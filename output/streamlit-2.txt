imp.png (1462×825)
https://blog.streamlit.io/content/images/2022/09/imp.png#border


new.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/new.gif


rerun.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/rerun.gif


test_1.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/test_1.gif#browser


require_file.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/require_file.gif#browser


shell.png (1900×896)
https://blog.streamlit.io/content/images/2022/09/shell.png#browser


create_repl.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/create_repl.gif#browser


app_demo.gif (1920×1032)
https://blog.streamlit.io/content/images/2022/09/app_demo.gif#browser


How to build Streamlit apps on Replit
https://blog.streamlit.io/how-to-build-streamlit-apps-on-replit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build Streamlit apps on Replit

Learn Streamlit by building the Beginner Template Tour

By Shruti Agarwal
Posted in Advocate Posts, September 29 2022
Build a Streamlit app on Replit
Step 1: Create a new Repl
Step 2: Install Streamlit
Step 3: Write libraries
Step 4: Write “Hello World!” 👋
Step 5: Run your Streamlit app
Step 6: Add more code! ✨
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Shruti Agarwal, and I’m a Streamlit Creator.

I love to use Streamlit when it comes to building and deploying beautiful apps in minutes.

I can vividly recall when I first tried to build a Streamlit app by using the online IDE (Integrated Development Environment) Replit. The Repl couldn’t load the app in a browser. It turned out that many Replit users have faced the same issue. So I built the Streamlit Beginner Template Tour (a guide for Streamlit basics) and created a Replit template. It successfully loaded in a browser! 🎉

In this post, I’ll show you how to do this step-by-step:

Step 1. Create a new Repl
Step 2. Install Streamlit
Step 3. Write libraries
Step 4. Write “Hello World!” 👋
Step 5. Run your Streamlit app
Step 6. Add more code! ✨

If you can’t wait to try it, here's the app and here’s the repo.

Build a Streamlit app on Replit
Step 1: Create a new Repl

If you don’t already have a Replit account, sign up for it and click on “+” to create a Repl. Select “Python template” and name it “streamlit_test”:

Step 2: Install Streamlit

Head to the “Shell” section of your Repl and type the following commands:

$ pip install streamlit
$ streamlit --version


Step 3: Write libraries

Add a new file as  requirements.txt  for writing libraries. Write these libraries to use inside your main code:

streamlit==1.12.2
pandas==1.4.4
numpy==1.23.2


It will look something like this:

Step 4: Write “Hello World!” 👋

Write the below code in main.py file:

import streamlit as st
st.title('Hello World!')
st.write('This is a simple text')


This imports the Streamlit library and adds a title along with the simple text. Your Repl will automatically save your work! Now, it’s time to run your app. 🤞

Step 5: Run your Streamlit app

Go back to “Shell”and type $streamlit run main.py. If it asks you to register your email, press the Enter key, and your app will open in a new browser:

Step 6: Add more code! ✨

Go ahead and add more code:

import pandas as pd
import numpy as np

# Expander section
with st.expander("About"):
  st.write("""Trying to add a data table, chart, sidebar button with 
          ballons, an image, text input & exploring tabs!""")

# Sidebar section
with st.sidebar:
  st.subheader('This is a Sidebar')
  st.write('Button with Balloons 🎈')
  if st.button('Click me!✨'):
    st.balloons()
  else:
    st.write(' ')

# Dataframe and Chart display section
st.subheader('Interactive Data Table')
df = pd.DataFrame(
    np.random.randn(50, 3),  # generates random numeric values!
    columns=["a", "b", "c"])
st.dataframe(df) 

st.subheader('Bar Chart 📊')
st.bar_chart(df)

# Image upload and text input section
st.subheader('An Image')
st.image('https://www.scoopbyte.com/wp-content/uploads/2019/12/tom-and-jerry.jpg')

st.subheader('Text Input')
greet = st.text_input('Write your name, please!')
st.write('👋 Hey!', greet)


# Tabs section
st.subheader('Tabs')
tab1, tab2 = st.tabs(["TAB 1", "TAB 2"])

with tab1:
  st.write('WOW!')
  st.image("https://i.gifer.com/DJR3.gif", width=400)

with tab2:
  st.write('Do you like ice cream? 🍨')
  agree = st.checkbox('Yes! I love it')
  disagree = st.checkbox("Nah! 😅")
  if agree:
    st.write('Even I love it 🤤')
  if disagree:
    st.write('You are boring 😒')


Here’s the code breakdown:

Libraries

pandas — for writing a dataframe

numpy — to generate random numbers

Containers

st.expander — to add an “About” section

st.sidebar — for passing Streamlit elements by using with notation

st.tabs — separated tabs to pass Streamlit elements by using with notation

Widgets

st.button and st.balloons — a button for throwing balloons 🎈

st.text_input — a single-line text input

st.checkbox — to select multiple options

Data Display Elements

st.dataframe — displays pandas dataframe as a data table

st.bar_chart— displays a beautiful bar chart

st.image — displays an image

Once you make these changes, the app will show a message in the top right corner. Click on “Always rerun” to see the changes:

Congratulations! 🥳

Now you know how to build an interactive and beautiful Streamlit app. It’ll look something like this:

Explore more with my Replit template in just two steps:

1. Click on “Use Template” to fork it and add your own flair!

2. In your forked Repl, go to “Shell” and type:

$ pip install -r requirements.txt
$ streamlit run streamlit_app.py


You’ll be able to view your app in a browser.

Wrapping up

Thank you for reading my post! I had so much fun building the Streamlit app and the Replit template. I hope you'll use it to create your own apps. If you want to share what you built or have any questions, please post them in the comments below or connect with me on Twitter, LinkedIn, or GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Classifier_A.gif (1726×840)
https://blog.streamlit.io/content/images/2022/01/Classifier_A.gif


Sequence_Sample_A.gif (1726×897)
https://blog.streamlit.io/content/images/2022/01/Sequence_Sample_A.gif


swarmplot.png (1672×858)
https://blog.streamlit.io/content/images/2022/01/swarmplot.png#browser


electrophorogram.png (1516×780)
https://blog.streamlit.io/content/images/2022/01/electrophorogram.png#browser


input-table.png (1532×816)
https://blog.streamlit.io/content/images/2022/01/input-table.png#border


How to diagnose blood cancer with Streamlit
https://blog.streamlit.io/how-to-diagnose-blood-cancer-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to diagnose blood cancer with Streamlit

Build a molecular pathology diagnostics app in 4 simple steps

By Eitan Halper-Stromberg
Posted in Advocate Posts, January 25 2022
What is the CloneRetriever app?
1. How to familiarize yourself with the data
2. How the “lymphoma” tissue differs from the “normal” tissue
3. How to combine samples in a SwarmPlot
4. How to make a classification algorithm
Wrapping up
References
Contents
Share this post
← All posts

Ever wondered how to use a bit of tissue (a biopsy, a surgery specimen, or a tube of blood) to diagnose lymphoma? Cue the CloneRetriever app!

In this post, you’ll learn:

How to familiarize yourself with the data
How the “lymphoma” tissue differs from the “normal” tissue
How to combine samples in a SwarmPlot
How to make a classification algorithm

Want to jump right in? Here's the CloneRetriever app.

What is the CloneRetriever app?

I’m a molecular pathologist. I look at a lot of tissue samples. Annotating them by hand as “lymphoma” or “normal” is very painful. I built the CloneRetriever app to diagnose blood cancer by using a semi-processed Excel file as data input.

How do you diagnose blood cancers like lymphoma? You examine the tissue under a microscope or sequence its DNA to look for abnormal cells. Alternatively, you can make algorithms look for dominant clonal expansions in DNA sequences consistent with lymphoma.

To interpret this DNA data, you’ll need heavy-duty lab equipment like a PCR machine, a DNA sequencer, or reagents. If you work in a place where it'd available (for example, a hospital by a university), you’ll get a file containing DNA information.

Let’s take a look at how to read it.

1. How to familiarize yourself with the data

Inside the app, take a look at the video tutorial. Below it, you’ll see the input table. That’s your DNA data:

This is four samples worth of data—sixty rows per sample. Each row represents the quantity of a particular piece of DNA.

In lymphoma, immune cells proliferate out of control. To diagnose it, you’d measure a piece of DNA that codes for a certain type of molecule known as immunoglobulin. This bit of DNA is specific to different populations of immune cells. Hence, lymphoma immunoglobulin will be different from the immune cells immunoglobulin.

Each row contains a measurement of a unique piece of immunoglobulin DNA. But how do you know if it’s lymphoma?

Look for the bits of DNA that are much more abundant. That’s lymphoma. See the column name descriptions? You’re solving a classification problem. This sample data is already annotated as lymphoma or not:

2. How the “lymphoma” tissue differs from the “normal” tissue

Below the input table, you’ll see a container titled “Reconstructed electropherogram.” It represents one single sample:

This plot is a histogram.

The X-axis shows the sequence length. The Y-axis shows the number of particular length DNA sequences. Each histogram has ten bars—one per sample “unit” sequence. The multicolored stacked bars represent the sequences of the same length.

Cancer looks like a tall bar—a spike. It’s because cancer sequences come from the cells that proliferate much faster than normal cells. Note: the sample FR1_R19-22-A_S21 is plotted by default. It shows cancer (indicated in the box 'sample to plot'). If you switch the view to the third one down the list, FR1_R19-23-A_S23, you’ll see normal cells. No massive spike!

3. How to combine samples in a SwarmPlot

What is a SwarmPlot? It’s the aggregated view of many DNA samples in one plot.

In the app, you’ll see a static SwarmPlot and an interactive SwarmPlot with two side-by-side panels. It has your data plotted together. Each point represents 1-3 sequences within a discrete unit of ten sequences.

For example, change the “Evaluate data at the level of” from “Every Sequence” to “Sample” and hit submit:

You’ll see one point for each of the four samples. It represents the piece of DNA that’s most likely derived from lymphoma.

And look! The two lymphoma samples in the left panel are within the "Yes" column! The app correctly classified them according to these three values: “fold change to 4th most prevalent sequence,” “frac of total,” and “sequence is present in replicate.”

So why use “Framework” or “Every Sequence” when you can view the four sample points?

Because sometimes there are many different DNA pieces that can indicate lymphoma. From a pathologist’s perspective, they should be reported in a test result. For a longer explanation of the SwarmPlot, read my manuscript.

4. How to make a classification algorithm

Use the three classifier buttons to predict lymphoma:

“Fold change to 4th most prevalent sequence.” The ratio of the DNA sequence to the fourth sequence, by rank.
“Frac of total.” The fraction of the DNA sequence relative to the combined total of every sequence in its measurement set.
“Sequence is present in replicate.” A boolean of “the sequence in question that’s also present in its replicate.”

You can also use these buttons to classify the data in the SwarmPlots. All points in the left SwarmPlot (the positives) appear in the “Yes” column.  All points in the right SwarmPlot (the negatives) appear in the “No” column.

The classifier function result is our diagnosis: “lymphoma,” “no-lymphoma”, or “unclear even after looking at the result manually.” To view it, scroll down to the “New Classifier True” output. Right below it, you’ll see the classifier output showing the values of the three most appropriate predictors.

Changing the evaluation level from “Every Sequence” to “Framework” or “Sample” will change the classifier result. At the sample level, the classifier sees only four data points (two positive and two negative), with each contributing one point to the most worrisome lymphoma sequence.

The default values “fold change to 4th most prevalent sequence” ≥4, “frac of total” ≥0, and “in replicate” separate the two positives from one negative but not from the other. In the plot on the right (with negatives), you’ll see one point in the first column (correctly classified as negative) and one point in the third column (incorrectly classified as positive).

How do you get the perfect separation of the positives and the negatives? The SwarmPlot on the left will show all data points in the third “Yes” column. The SwarmPlot on the right will show all data points in the first “No” column.

Wrapping up

Congratulations! Now you know how pathologists look at a file containing pieces of DNA and use it to diagnose lymphoma. You learned how to look at the data from the perspective of a single sample and in a combined view. You played around with the predictor threshold values to see how that changed the outcome of a classification.

Thank you for reading this article. I hope it helped you see how molecular pathologists think about blood cancer in the context of looking at someone’s DNA. If you have any questions or feedback, please leave them in the comments below or reach out to me at cloneRetriever@jhmi.edu.

References

Halper-Stromberg, Eitan (2021). CloneRetriever: An Automated Algorithm to Identify Clonal B and T Cell Gene Rearrangements by Next-Generation Sequencing for the Diagnosis of Lymphoid Malignancies

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

streamlit-st_labelling-2021-07-29-11-07-52.gif (1440×772)
https://blog.streamlit.io/content/images/2021/09/streamlit-st_labelling-2021-07-29-11-07-52.gif#browser


game-1-1.gif (1200×800)
https://blog.streamlit.io/content/images/2022/01/game-1-1.gif#border


Streamlit (Page 21)
https://blog.streamlit.io/page/21/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Adding beta and experimental “channels” to Streamlit

Introducing the st.beta and st.experimental namespaces

Product
by
TC Ricks
,
May 6 2020
Try Nightly Build for cutting-edge Streamlit

A new style of release for anyone who wants the most up-to-date Streamlit version

Product
by
TC Ricks
,
April 17 2020
The Streamlit roadmap—big plans for 2020!

Devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers

Product
by
Adrien Treuille
,
February 27 2020
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

py3dmol.png (1754×926)
https://blog.streamlit.io/content/images/2022/05/py3dmol.png#border


Streamlit (Page 20)
https://blog.streamlit.io/page/20/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Gravitational-wave apps help students learn about black holes

Exploring distant space with gravitational waves

Advocate Posts
by
Jonah Kanner
,
December 15 2020
Elm, meet Streamlit

A tutorial on how to build Streamlit components using Elm

Tutorials
by
Henrikh Kantuni
,
December 8 2020
Build knowledge graphs with the Streamlit Agraph component

A powerful and lightweight library for visualizing networks/graphs

Advocate Posts
by
Christian Klose
,
November 25 2020
Testing Streamlit apps using SeleniumBase

How to create automated visual tests

Tutorials
by
Randy Zwitch
,
November 23 2020
New UC Davis tool tracks California's COVID-19 cases by region

Regional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine

Advocate Posts
by
Pranav Pandit
,
November 19 2020
Introducing Streamlit Sharing

The new Streamlit platform for deploying, managing, and sharing your apps

Product
by
Adrien Treuille
,
October 15 2020
Deploying Streamlit apps using Streamlit sharing

A sneak peek into Streamlit's new deployment platform

Tutorials
by
Tyler Richards
,
October 15 2020
New layout options for Streamlit

Introducing new layout primitives—columns, containers, and expanders!

Product
by
Austin Chen
,
October 8 2020
Introducing Streamlit Components

A new way to add and share custom functionality for Streamlit apps

Product
by
Adrien Treuille
,
July 14 2020
Announcing Streamlit's $21M Series A

Developing new superpowers for the data science community

Product
by
Adrien Treuille
,
June 16 2020
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

plot.png (1850×730)
https://blog.streamlit.io/content/images/2022/05/plot.png#border


Graphical-Abstract.png (1886×2026)
https://blog.streamlit.io/content/images/2022/05/Graphical-Abstract.png#border


annotation-1.png (2000×807)
https://blog.streamlit.io/content/images/2022/05/annotation-1.png#browser


Roland Dunbrack - Streamlit
https://blog.streamlit.io/author/roland/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Roland Dunbrack
1 post
How to share scientific analysis through a Streamlit app

3 easy steps to share your study results with fellow scientists

Advocate Posts
by
Mitchell Parker and 
1
 more,
May 12 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mitchell Parker - Streamlit
https://blog.streamlit.io/author/mitchell/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mitchell Parker
1 post
How to share scientific analysis through a Streamlit app

3 easy steps to share your study results with fellow scientists

Advocate Posts
by
Mitchell Parker and 
1
 more,
May 12 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 19)
https://blog.streamlit.io/page/19/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Our $35 million Series B

We’re excited to announce a new funding round led by Sequoia 🌲

Product
by
Adrien Treuille
,
April 7 2021
Monthly rewind > March 2021

Your March look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 5 2021
Announcing Theming for Streamlit apps! 🎨

Try out the new dark mode and custom theming capabilities

Product
by
Abhi Saini
,
March 18 2021
Monthly rewind > February 2021

Your February look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 8 2021
How to use Roboflow and Streamlit to visualize object detection output

Building an app for blood cell count detection

Advocate Posts
by
Matt Brems
,
February 23 2021
Developing a streamlit-webrtc component for real-time video processing

Introducing the WebRTC component for real-time media streams

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
February 12 2021
Monthly rewind > January 2021

Your January look back at new features and great community content

Monthly Rewind
by
TC Ricks
,
February 8 2021
Streamlit ❤️ Firestore

Use Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!

Tutorials
by
Austin Chen
,
January 27 2021
Streamlit Components, security, and a five-month quest to ship a single line of code

The story of allow-same-origin

Tutorials
by
Tim Conkling
,
January 20 2021
Arup and New Story use data to help combat pandemic related evictions

Making data accessible to help address the eviction crisis

Advocate Posts
by
Jared Stock
,
January 7 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to share scientific analysis through a Streamlit app
https://blog.streamlit.io/how-to-share-scientific-analysis-through-a-streamlit-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to share scientific analysis through a Streamlit app

3 easy steps to share your study results with fellow scientists

By Mitchell Parker and Roland Dunbrack
Posted in Advocate Posts, May 12 2022
What is Rascore?
How to share an explorable scientific dataset
How to visualize the 3D structure of human proteins
How to make informative data plots
Wrapping up
Contents
Share this post
← All posts

Have you ever done an amazing scientific analysis and wanted to share it? We wanted the same. That’s why we built Rascore, a Streamlit app for sharing study results with fellow researchers to make new discoveries.

In this post, you’ll learn:

How to share an explorable scientific dataset
How to visualize the 3D structure of human proteins
How to make informative data plots

TLDR? Here is our app. Or jump straight into the repo code! 🧑‍💻

But before we get into the exciting stuff, let’s talk about...

What is Rascore?

Rascore is an app for analyzing the 3D structure of the tumor-associated RAS proteins (KRAS, NRAS, and HRAS—the most common cancer drivers). Rascore helps scientists explore and compare published structural models of RAS proteins in the Protein Data Bank (PDB), as well as simplify biological study and facilitate drug discovery.

Almost all RAS structures are determined by X-ray crystallography. Because of the experiment conditions like mutation status or bound inhibitors, the structures come out differently. In Rascore, we group similar structures by their 3D configuration to examine their properties and how they’re correlated with conditions.

How to share an explorable scientific dataset

You can download all RAS protein structural models from the PDB, but they’re not annotated. We wanted to automate the annotation of each RAS structure by its biological features (read more in our paper “Delineating The RAS Conformational Landscape”).

We also wanted to let researchers explore our annotated dataset and download subsets—like all RAS structures with a specific mutation or bound drugs at a certain site.

Here is the code to display datasets as a table and download it (replace st.table with st.dataframe to make it scrollable):

Use this code to display a table:

import streamlit as st

def show_st_table(df, st_col=None, hide_index=True):
		"""
		Show table in Streamlit application

		Parameters
		----------
		df: pandas.DataFrame
		st_col: st.columns object
		hide_index: bool
			Whether to display (True) or hide (False)
			the indices of the displayed pandas
			DataFrame
		"""
    if hide_index:
        hide_table_row_index = """
                <style>
                tbody th {display:none}
                .blank {display:none}
                </style>
                """
        st.markdown(hide_table_row_index, unsafe_allow_html=True)

    if st_col is None:
        st.table(df)
    else:
        st_col.table(df)


Use this code to download a table:

def encode_st_df(df):
		"""
		Encode pandas DataFrame in utf-8 format

		Parameters
		----------
		df: pandas.DataFrame
		"""
    return df.to_csv(sep="\\t", index=False).encode("utf-8")

def download_st_df(df, file_name, download_text, st_col=None):
		"""
		Download pandas DataFrame in Streamlit application

		Parameters
		----------
		df: pandas.DataFrame
		file_name: str
			Name of file (e.g., rascore_table.tsv)
		download_text: str
			Text on download button (e.g., Download Table)
		st_col: st.columns object
		"""
    if st_col is None:
        st.download_button(
            label=download_text,
            data=encode_st_df(df),
            file_name=file_name,
        )
    else:
        st_col.download_button(
            label=download_text,
            data=encode_st_df(df),
            file_name=file_name,
        )

How to visualize the 3D structure of human proteins

The data in Rascore relates only to the 3D structure of RAS proteins. We wanted researchers to compare structural models with different cancer-associated mutations or bound drugs.

Luckily, José Manuel Nápoles Duarte made a Streamlit plugin for visualizing protein structures by using Py3DMol. But Py3DMol doesn’t highlight protein structure parts like drug binding sites. So we created a highlighting function.

Below are the input values for parameters ending in “_lst”. They’re non-intuitive and relate to highlighting selected parts of protein structures. Each “_lst” takes a nested list as the input with a required object at each index of each sublist (see this doc for making a selection and coloring dictionaries):

Parameter	Purpose	Index 0	Index 1	Index 2
style_lst	To stylize parts of protein structures by changing 3D representation or coloring scheme	Selection Dictionary (e.g., {"chain":"A", "rest": "25-40"]\}	Coloring Dictionary (e.g., {"stick": {"colorscheme": "amino", "radius": 0.2}})	NA
label_lst	To apply custom labels to certain parts of protein structure	Label String	Coloring Dictionary	Selection Dictionary
reslabel_lst	To apply standard labels to residue (amino acid identity and linear position)	Selection Dictionary	Coloring Dictionary	NA
surface_lst	To add surface over 3D representation of protein structures	Coloring Dictionary	Selection Dictionary	NA

Here is the code:

import py3Dmol
from stmol import showmol

def show_st_3dmol(
    pdb_code,
    style_lst=None,
    label_lst=None,
    reslabel_lst=None,
    zoom_dict=None,
    surface_lst=None,
    cartoon_style="trace",
    cartoon_radius=0.2,
    cartoon_color="lightgray",
    zoom=1,
    spin_on=False,
    width=900,
    height=600,
):

"""
Show 3D view of protein structure from the 
Protein Data Bank (PDB)

Parameters
----------
pdb_code: str
	Four-letter code of protein structure in the PDB
	(e.g., 5P21)
style_lst: list of lists of dicts
	A nested list with each sublist containing a 
	selection dictionary at index 0 and coloring
	dictionary at index 1
label_lst: list of lists of dicts
	A nested list with each sublist containing a 
	label string at index 0, coloring dictionary
	at index 1, and selection dictionary at
	index 2
reslabel_lst: list of lists of dicts
	A nested list with each sublist containing a 
	selection dictionary at index 0 and coloring
	dictionary at index 1
zoom_dict: dict
surface_lst: list of lists of dicts
	A nested list with each sublist containing a 
	coloring dictionary at index 0 and selection
	dictionary at index 1
cartoon_style: str
	Style of protein structure backbone cartoon 
	rendering, which can be "trace", "oval", "rectangle", 
	"parabola", or "edged"
cartoon_radius: float
	Radius of backbone cartoon rendering
cartoon_color: str
	Color of backbone cartoon rendering
zoom: float
	Level of zoom into protein structure
	in unit of Angstroms
spin_on: bool
	Boolean specifying whether the visualized
	protein structure should be continually 
	spinning (True) or not (False)
width: int
	Width of molecular viewer
height: int
	Height of molecular viewer
"""
    view = py3Dmol.view(query=f"pdb:{pdb_code.lower()}", width=width, height=height)

    view.setStyle(
        {
            "cartoon": {
                "style": cartoon_style,
                "color": cartoon_color,
                "thickness": cartoon_radius,
            }
        }
    )

    if surface_lst is not None:
        for surface in surface_lst:
            view.addSurface(py3Dmol.VDW, surface[0], surface[1])

    if style_lst is not None:
        for style in style_lst:
            view.addStyle(
                style[0],
                style[1],
            )

    if label_lst is not None:
        for label in label_lst:
            view.addLabel(label[0], label[1], label[2])

    if reslabel_lst is not None:
        for reslabel in reslabel_lst:
            view.addResLabels(reslabel[0], reslabel[1])

    if zoom_dict is None:
        view.zoomTo()
    else:
        view.zoomTo(zoom_dict)

    view.spin(spin_on)

    view.zoom(zoom)
    showmol(view, height=height, width=width)


How to make informative data plots

Visual comparison of individual RAS structures is great, but there are hundreds of them to sift through.

We created an easy way to compare calculated metrics across RAS structures of different groups such as druggability score or pocket volumes. Possible data visualizations are scatterplots and box plots. You can make them with Matplotlib and load them into your app by using this function:

import streamlit as st
from io import BytesIO

def show_st_fig(fig, st_col=None):
    byt = BytesIO()
    fig.savefig(byt, format="png")
    if st_col is None:
        st.image(byt)
    else:
        st_col.image(byt)


Wrapping up

Rascore is an app for researchers to explore the 3D structural models of cancer-associated RAS proteins. Streamlit gave us easy dataset navigation, 3D protein structures visualization, and plotted data display. We hope more researchers use Streamlit to share their study results with the scientific community!

If you have questions, please post them in the comments below or reach out to us on Twitter at @Mitch_P and @RolandDunbrack or email us at mip34@drexel.edu and roland.dunbrack@gmail.com.

Thank you for reading our story, and happy app-building! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 18)
https://blog.streamlit.io/page/18/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to make a great Streamlit app: Part II

A few layout and style tips to make your apps look even more visually appealing!

Tutorials
by
Abhi Saini
,
June 22 2021
Easy monitoring of dbt Cloud jobs with Streamlit

How the Cazoo data science team built their dbt Cloud + Streamlit app

Advocate Posts
by
Martin Campbell
,
June 11 2021
Monthly rewind > May 2021

Your May look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2021
How to make a great Streamlit app

Designing an app your users will love

Tutorials
by
Abhi Saini
,
June 2 2021
Making apps for the Rasa research team (and open source community!)

Helping Rasa users understand their models

Advocate Posts
by
Vincent D. Warmerdam
,
May 12 2021
Monthly rewind > April 2021

Your April look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2021
Introducing Submit button and Forms 📃

We're releasing a pair of new commands called st.form and st.form_submit_button!

Tutorials
by
Abhi Saini
,
April 29 2021
Streamlit ❤️ Firestore (continued)

Aka the NoSQL sequel: Building a Reddit clone and deploying it securely

Tutorials
by
Austin Chen
,
April 22 2021
Build a Jina neural search with Streamlit

Use Jina to search text or images with the power of deep learning

Advocate Posts
by
Alex C-G
,
April 15 2021
Add secrets to your Streamlit apps

Use Secrets Management in Streamlit sharing to securely connect to private data sources

Tutorials
by
James Thompson
,
April 9 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 17)
https://blog.streamlit.io/page/17/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > August 2021

Your August look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2021
Deploying a cloud-native Coiled app

How Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users

Advocate Posts
by
Richard Pelgrim
,
September 7 2021
0.88.0 release notes

This release launches st.download_button as well as other improvements and bug fixes

Release Notes
by
Abhi Saini
,
September 3 2021
Labeling ad videos with Streamlit

How Wavo.me uses Streamlit’s Session State to create labeling tasks

Advocate Posts
by
Anastasia Glushko
,
September 2 2021
Streamlit gains a major new spell book

A tome to the magical fields of Python, algorithms, visualization, and machine learning

Product
by
Adrien Treuille
,
August 20 2021
Monthly rewind > July 2021

Your July look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 5 2021
All in on Apache Arrow

How we improved performance by deleting over 1k lines of code

Product
by
Henrikh Kantuni
,
July 22 2021
6 tips for improving your Streamlit app performance

Moving your Streamlit app from analysis to production

Tutorials
by
Randy Zwitch
,
July 20 2021
Monthly rewind > June 2021

Your June look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 5 2021
Session State for Streamlit 🎈

You can now store information across app interactions and reruns!

Product
by
Abhi Saini
,
July 1 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 16)
https://blog.streamlit.io/page/16/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > October 2021

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2021
☁️ Introducing Streamlit Cloud! ☁️

Streamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.

Product
by
Adrien Treuille
,
November 2 2021
Detecting parking spots with Streamlit

How to build a Streamlit parking spot app in 8 simple steps

Advocate Posts
by
Jeffrey Jex
,
October 26 2021
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
Launching a brand-new docs site 🥳

Improved layout, easier navigation, and faster search

Product
by
Snehan Kekre
,
October 13 2021
Monthly rewind > September 2021

Your September look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2021
Announcing Streamlit 1.0! 🎈

Streamlit used to be the simplest way to write data apps. Now it's the most powerful

Product
by
Adrien Treuille
,
October 5 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
New experimental primitives for caching (that make your app 10x faster!)

Help us test the latest evolution of st.cache

Product
by
Abhi Saini and 
1
 more,
September 22 2021
Common app problems: Resource limits

5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud

Tutorials
by
Johannes Rieke
,
September 9 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

markdown-1.gif (1200×800)
https://blog.streamlit.io/content/images/2022/01/markdown-1.gif#border


Streamlit (Page 15)
https://blog.streamlit.io/page/15/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to create interactive books with Streamlit in 5 steps

Use streamlit_book library to create interactive books and presentations

Advocate Posts
by
Sebastian Flores Benner
,
January 20 2022
How to master Streamlit for data science

The essential Streamlit for all your data science needs

Tutorials
by
Chanin Nantasenamat
,
January 18 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
Streamlit Cloud is now SOC 2 Type 1 compliant

We have completed a full external audit of our security practices

Product
by
Amanda Kelly
,
January 11 2022
Monthly rewind > December 2021

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 7 2022
Creating satellite timelapse with Streamlit and Earth Engine

How to create a satellite timelapse for any location around the globe in 60 seconds

Advocate Posts
by
Qiusheng Wu
,
December 15 2021
Deploy a private app for free! 🎉

And... get unlimited public apps

Product
by
Abhi Saini
,
December 9 2021
Monthly rewind > November 2021

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2021
Finding your look-alikes with semantic search

How Pinecone used Streamlit to create a Hacker News Doppelgänger app

Advocate Posts
by
Greg Kogan
,
December 1 2021
Forecasting with Streamlit Prophet

How Artefact built a Streamlit app to train time-series forecasting models

Advocate Posts
by
Maxime Lutel
,
November 10 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 14)
https://blog.streamlit.io/page/14/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
3 steps to fix app memory leaks

How to detect if your Streamlit app leaks memory and identify faulty code

Tutorials
by
George Merticariu
,
April 14 2022
Monthly rewind > March 2022

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 7 2022
30 Days of Streamlit

A fun challenge to learn and practice using Streamlit

Advocate Posts
by
Chanin Nantasenamat
,
April 1 2022
Sogeti creates an educational Streamlit app for data preprocessing

Learn how to use Sogeti’s Data Quality Wrapper

Advocate Posts
by
Tijana Nikolic
,
March 8 2022
Monthly rewind > February 2022

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 7 2022
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

Product
by
Adrien Treuille and 
2
 more,
March 2 2022
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

Advocate Posts
by
Nikolina Sarcevic and 
2
 more,
February 17 2022
Monthly rewind > January 2022

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 7 2022
How Delta Dental uses Streamlit to make lightning-fast decisions

From an idea to a prototype to production in just two weeks

Case study
by
Amanda Kelly
,
February 1 2022
How to diagnose blood cancer with Streamlit

Build a molecular pathology diagnostics app in 4 simple steps

Advocate Posts
by
Eitan Halper-Stromberg
,
January 25 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 13)
https://blog.streamlit.io/page/13/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How one finance intern launched his data science career from a coding bootcamp in Brazil

Learn how Marcelo Jannuzzi of iFood got his dream job in data science

Case study
by
Marcelo Jannuzzi and 
1
 more,
June 9 2022
Monthly rewind > May 2022

Your May look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2022
Introducing multipage apps! 📄

Quickly and easily add more pages to your Streamlit apps

Product
by
Vincent Donato
,
June 2 2022
How Streamlit uses Streamlit: Sharing contextual apps

Learn about session state and query parameters!

Tutorials
by
Tyler Richards
,
May 26 2022
Leverage your user analytics on Streamlit Community Cloud

See who viewed your apps, when, and how popular they are

Product
by
Diana Wang and 
1
 more,
May 17 2022
How to share scientific analysis through a Streamlit app

3 easy steps to share your study results with fellow scientists

Advocate Posts
by
Mitchell Parker and 
1
 more,
May 12 2022
Monthly rewind > April 2022

Your April look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2022
Wissam Siblini uses Streamlit for pathology detection in chest radiographs

Learn how Wissam detected thoracic pathologies in medical images

Case study
by
Wissam Siblini and 
1
 more,
May 3 2022
The Stable solves its data scalability problem with Streamlit

How Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days

Case study
by
Mark von Oven and 
1
 more,
April 28 2022
How to build a real-time live dashboard with Streamlit

5 easy steps to make your own data dashboard

Advocate Posts
by
AbdulMajedRaja RS
,
April 21 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

satschool-gif-2.gif (1918×1080)
https://blog.streamlit.io/content/images/2022/06/satschool-gif-2.gif#browser


Streamlit (Page 12)
https://blog.streamlit.io/page/12/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

Tutorials
by
Tyler Richards and 
2
 more,
August 18 2022
Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
Monthly rewind > July 2022

Your July look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 9 2022
The magic of working in open source

How we build our open-source library and release new features

Tutorials
by
Ken McGrady
,
August 4 2022
How to enhance Google Search Console data exports with Streamlit

Connect to the GSC API in one click and go beyond the 1,000-row UI limit!

Tutorials
by
Charly Wargnier
,
July 28 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Monthly rewind > June 2022

Your June look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 6 2022
JULO improves financial inclusion in Indonesia with Streamlit

Learn how JULO went from manual underwriting to automated credit scoring and a 22-member data team

Case study
by
Martijn Wieriks and 
1
 more,
June 30 2022
Make your st.pyplot interactive!

Learn how to make your pyplot charts interactive in a few simple steps

Tutorials
by
William Huang
,
June 23 2022
Observing Earth from space with Streamlit

Learn how Samuel Bancroft made the SatSchool app to teach students Earth observation

Advocate Posts
by
Samuel Bancroft
,
June 16 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

satschool-3.png (1914×857)
https://blog.streamlit.io/content/images/2022/06/satschool-3.png#browser


satschool-2.png (1663×857)
https://blog.streamlit.io/content/images/2022/06/satschool-2.png#browser


satschool-1.png (1665×860)
https://blog.streamlit.io/content/images/2022/06/satschool-1.png#browser


Observing Earth from space with Streamlit
https://blog.streamlit.io/observing-earth-from-space-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Observing Earth from space with Streamlit

Learn how Samuel Bancroft made the SatSchool app to teach students Earth observation

By Samuel Bancroft
Posted in Advocate Posts, June 16 2022
What is the SatSchool app?
Making a website with satellite data
Land
Oceans
Ice
Making a quiz
Step 1
Step 2
Step 3
Step 4
Step 5
Why use Streamlit?
Wrapping up
Contents
Share this post
← All posts

Have you ever tried teaching teenagers Earth observation and environmental science in an interactive way? That was my goal at SatSchool, and that's why I made the SatSchool app!

In this post, we’ll talk about:

Making a website with satellite data
Making a quiz
Why use Streamlit?

Can’t wait to dive in? Here is the SatSchool app and here is the repo code.

But before we get to the fun stuff…

What is the SatSchool app?

SatSchool is a school outreach program that introduces Earth observation concepts and career pathways to students (11-15 years old) in the UK. Students get their hands on satellite data to learn how satellites help us study the Earth from space.

As part of this program, I developed the SatSchool app— a “Hands on with Data” module—that introduces technical approaches to Earth observation challenges.

Making a website with satellite data

The SatSchool app features three themes: land, oceans, and ice (check out the source code here).

Land

This theme features an Amazon deforestation page.

Here I used:

Support Vector Machine (SVM) classifier to make a deforestation map
Input widgets for more forest/not-forest training points
st.session_state to remember the training points (as students label more data based on what they see)

Students can view Landsat-8 satellite imagery centered over a region in Brazil and compare the deforested areas with protected areas. Many explore machine learning and experiment with classifiers for the first time!

They learn how their training data makes the classifier better or worse, how satellite imagery can solve problems, and how to dynamically measure deforestation over thousands of square kilometers without installation, powerful computers, or technical know-how!

Oceans

This theme explores the relationship between the sea surface temperature and chlorophyll.

Here I used:

Altair charts to present simple graphs
st.number_input and st.form to make answering questions interactive (with st.balloons for correct answers! 🎈).

By using geemap with its split-panel map, students explore two global datasets and the connection between them.

Ice

This theme explores coding.

Here I used:

streamlit_ace for the terminal
st.session_state for step-by-step instructions in an st.warning box for aesthetic appeal

Students get introduced to radar satellite data and can bring up satellite images by using code before they realize they’re programming!

Making a quiz

I wanted to bring gamification into the learning process. Satellite imagery is cool, but how could I keep young people engaged so that they had fun learning?

Our quiz tests students on the SatSchool curriculum concepts. To get the best score, they have 30 seconds to answer as many questions as they can.

Here is how I did it in five steps:

Step 1

I did the following imports and variables:

import streamlit as st 
import time
import random
import math

TIME_LIMIT = 30 #seconds
row = None

Step 2

I randomly generated math questions using the following code:

def gen_question():
    #randomly choose two numbers in a range, alongside an operator
    operators = ['+','-','//','*']
    a,b = random.randint(1,10), random.randint(1,10)
    op = random.choice(operators)
    
    #construct the question text and evaluate the calculation
    ques = f"What is {a} {op} {b}?"
    ans = eval(f"{a}{op}{b}")

    #we create some purposely incorrect answer options
    option2 = eval(f"{b}{op}{a}")
    option3 = eval(f"{b-2}{op}{a+5}")
    option4 = eval(f"{b}{op}{a}-{a}")
    #we want to avoid duplicate answer options, so use this inelegant solution
    while option2 == ans:
       option2 += 1
    while option3 == ans or option3 == option2:
       option3 += 1
    while option4 == ans or option4 == option2 or option4 == option3:
       option4 += 1
    
    return {'question': ques,
            'options': {
            ans: True,
            option2: False,
            option3: False,
            option4: False
            }
            }

Step 3

I initialised values in st.session_state to keep track of timings, scores, etc.:

#initialise the session state if keys don't yet exist
if 'correct' not in st.session_state.keys():
    st.session_state['correct'] = None
if "quiz_active" not in st.session_state.keys():
    st.session_state["quiz_active"] = False

i,j,_ = st.columns([1,1,5])
if i.button("Start quiz", key='start_quiz', disabled=st.session_state['quiz_active']):
    st.session_state['quiz_active'] = True
    st.session_state['total_score'] = 0
    st.session_state['count'] = 0
    st.session_state['time_start'] = time.time()
    st.session_state['time_now'] = time.time()
    st.session_state['score'] = 0
    st.session_state['correct'] = None
    st.experimental_rerun()
if j.button("End quiz and reset", key='reset', disabled=not st.session_state['quiz_active']):
    st.session_state['total_score'] = 0
    st.session_state['count'] = 0
    st.session_state['correct'] = None
    st.session_state['quiz_active'] = False
    st.session_state['time_start'] = None
    st.experimental_rerun()

if not st.session_state['quiz_active']:
    st.write(f'\
 Welcome to the quiz! You have {TIME_LIMIT} seconds to answer as many questions as you can.')

Step 4

I controlled the button layout by using a mix of st.columns(), st.container(), and st.empty():

question_empty = st.empty()

d,e,_ = st.columns([2,2,6])
with d:
    total_score_empty = st.empty()
with e:
    st.write('')
    answer_empty = st.empty()

if st.session_state['quiz_active']:
    #check for time up upon page update
    if time.time() - st.session_state['time_start'] > TIME_LIMIT:
        st.info(f"Time's up! You scored a total of **{st.session_state['total_score']:.2f}** \\
                    and answered **{st.session_state['count']}** questions.")
    else:
        with question_empty:
            with st.container():
                #get a newly generated question with answer options
                row = gen_question()
                
                st.markdown(f"Question {st.session_state['count']+1}: {row['question']}")

                options = list(row['options'].keys())
                random.shuffle(options)

                a,b,_ = st.columns([2,2,6])
                #construct the answer buttons, and pass in whether the answer is correct or not in the args
                a.button(f"{options[0]}", on_click=answer, args=(str(row['options'][options[0]]),))
                a.button(f"{options[1]}", on_click=answer, args=(str(row['options'][options[1]]),))
                b.button(f"{options[2]}", on_click=answer, args=(str(row['options'][options[2]]),))
                b.button(f"{options[3]}", on_click=answer, args=(str(row['options'][options[3]]),))
                
                if st.session_state['correct']  == 'True' and st.session_state['count'] > 0:
                    answer_empty.success(f"Question {st.session_state['count']} correct!")
                elif st.session_state['correct'] == 'False' and st.session_state['count'] > 0:
                    answer_empty.error(f"Question {st.session_state['count']} incorrect!")
                total_score_empty.metric('Total score', value=f"{st.session_state['total_score']:.2f}", delta=f"{st.session_state['score']:.2f}")

Step 5

Finally, I defined a function that takes an answer and does something with the score:

def answer(ans):
    st.session_state['correct'] = ans
    if ans == 'True':
        #motivate quickfire answers with an exponential decay
        score = (1 * math.exp(-0.05*(time.time()-st.session_state['time_now'])))*10
        st.session_state['score'] = max(1, score)
    else:
        #penalise wrong answers with a negative score
        st.session_state['score'] = -10
    #update the score but prevent it from becoming negative
    st.session_state['total_score'] += st.session_state['score']
    st.session_state['total_score'] = max(0, st.session_state['total_score'])
    
    st.session_state['time_now'] = time.time()
    st.session_state['count'] += 1


I saved my completed script and viewed my app with streamlit run.

Also, if I wanted to, I could randomly choose from a selection of pre-written questions and implement a leaderboard functionality so that students could see their scores alongside others in the classroom.

Why use Streamlit?

Before I wrap this up…one more thing.

Why did I choose Streamlit? I’ve been using Streamlit for the past few years and knew it’d be perfect to make the SatSchool app because:

I can quickly deploy my Python code without any front-end headaches.
I can make a good-looking site and deploy my app to the Streamlit Community Cloud to share my work.
I can easily extend and modify the site. When my SatSchool colleagues want to add new content or demos, I can quickly prototype it (especially with the help of Sebastian Flores Benner’s fantastic streamlit_book library).
Streamlit’s amazing ecosystem of demos and third-party libraries make coding fast and fun. Third-party libraries are easy to integrate and offer lots of exciting functionality. Alongside the above-mentioned streamlit_book library, there is Qiusheng Wu’s geemap for the visualisation of satellite data, streamlit_timeline, streamlit_ace, streamlit-lottie, streamlit_juxtapose, and more.
Wrapping up

I had a lot of fun making the SatSchool app and I’m excited to hear what you think! There’s still a lot to do. Expect updated app demos as we take our program into schools and show people the power of satellite imagery.

I hope this post can show you how easy it is to make great apps with Streamlit. If you have any questions or want to share what you built, contact me on Twitter at @spiruel and check out SatSchool at @SatSchool_ and on our website.

Happy coding! 🛰️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 11)
https://blog.streamlit.io/page/11/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Discover and share useful bits of code with the 🪢 streamlit-extras library

How to extend the native capabilities of Streamlit apps

Tutorials
by
Arnaud Miribel
,
October 25 2022
Build a Streamlit Form Generator app to avoid writing code by hand

Learn how to make extendable starter Forms

by
Gerard Bentley
,
October 24 2022
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Monthly rewind > September 2022

Your September look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
How to build Streamlit apps on Replit

Learn Streamlit by building the Beginner Template Tour

Advocate Posts
by
Shruti Agarwal
,
September 29 2022
Streamlit App Starter Kit: How to build apps faster

Save 10 minutes every time you build an app

Tutorials
by
Chanin Nantasenamat
,
September 27 2022
How to build your own Streamlit component

Learn how to make a component from scratch!

Tutorials
by
Zachary Blackwood
,
September 15 2022
Monthly rewind > August 2022

Your August look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2022
Make dynamic filters in Streamlit and show their effects on the original dataset

Quickly and easily add dynamic filters to your Streamlit app

Tutorials
by
Vladimir Timofeenko
,
August 25 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 10)
https://blog.streamlit.io/page/10/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to quickly deploy and share your machine learning model for drug discovery

Share your ML model in 3 simple steps

Advocate Posts
by
Sebastian Ayala Ruano
,
December 15 2022
Find the top songs from your high school years with a Streamlit app

Use the Spotify API to generate 1,000+ playlists!

Advocate Posts
by
Robert Ritz
,
December 8 2022
Monthly rewind > November 2022

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2022
Streamlit-Authenticator, Part 1: Adding an authentication component to your app

How to securely authenticate users into your Streamlit app

Advocate Posts
by
Mohammad Khorasani
,
December 6 2022
Streamlit Quests: Getting started with Streamlit

The guided path for learning Streamlit

Tutorials
by
Chanin Nantasenamat
,
November 18 2022
Building robust Streamlit apps with type-checking

How to make type-checking part of your app-building flow

Advocate Posts
by
Harald Husum
,
November 10 2022
Monthly rewind > October 2022

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2022
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
Announcing the Figma-to-Streamlit plugin 🎨

Go from prototype to code as easy as 1-2-3 with our new community resource!

Product
by
Juan Martín García
,
November 1 2022
Prototype your app in Figma! 🖌️

Quickly and easily design your app with the Streamlit Design system

Tutorials
by
Jessi Shamis
,
October 27 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 9)
https://blog.streamlit.io/page/9/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

LLMs
by
Arvindra Sehmi
,
February 9 2023
Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component

How to add advanced functionality to your Streamlit app’s authentication component

Advocate Posts
by
Mohammad Khorasani
,
February 7 2023
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

Advocate Posts
by
Sven Koerner
,
February 2 2023
Host your Streamlit app for free

Learn how to transfer your apps from paid platforms to Streamlit Community Cloud

Tutorials
by
Chanin Nantasenamat
,
January 24 2023
Create a color palette from any image

Learn how to come up with the perfect colors for your data visualization

Advocate Posts
by
Siavash Yasini
,
January 19 2023
How to make a culture map

Analyze multidimensional data with Steamlit!

Tutorials
by
Michał Nowotka
,
January 12 2023
Build an image background remover in Streamlit

Skip the fees and do it for free! 🎈

Tutorials
by
Tyler Simons
,
January 10 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sidebar.gif (336×684)
https://blog.streamlit.io/content/images/2021/11/sidebar.gif


Streamlit-Prophet-1.gif (1200×675)
https://blog.streamlit.io/content/images/2021/11/Streamlit-Prophet-1.gif


Sogeti_compare-GIF.gif (799×397)
https://blog.streamlit.io/content/images/2022/03/Sogeti_compare-GIF.gif


Sogeti_topic-GIF-1.gif (799×397)
https://blog.streamlit.io/content/images/2022/03/Sogeti_topic-GIF-1.gif


image-copy.png (1188×879)
https://blog.streamlit.io/content/images/2022/03/image-copy.png#browser


Sogeti_preprocess-GIF.gif (799×397)
https://blog.streamlit.io/content/images/2022/03/Sogeti_preprocess-GIF.gif


audio-copy.png (1195×883)
https://blog.streamlit.io/content/images/2022/03/audio-copy.png#browser


polarity-copy.png (1193×885)
https://blog.streamlit.io/content/images/2022/03/polarity-copy.png#browser


pp-copy.png (1191×883)
https://blog.streamlit.io/content/images/2022/03/pp-copy.png#browser


te-copy.png (1908×892)
https://blog.streamlit.io/content/images/2022/03/te-copy.png#browser


Live-Data-Science-Dashboard-GIF-1.gif (1717×997)
https://blog.streamlit.io/content/images/2022/04/Live-Data-Science-Dashboard-GIF-1.gif


subsections-copy.png (1757×817)
https://blog.streamlit.io/content/images/2022/03/subsections-copy.png#browser


table-1.png (1600×217)
https://blog.streamlit.io/content/images/2022/04/table-1.png#browser


1_tNZWLX9aUinvjYYCxfih3g.png (951×734)
https://blog.streamlit.io/content/images/2022/03/1_tNZWLX9aUinvjYYCxfih3g.png#browser


Gerard Bentley - Streamlit
https://blog.streamlit.io/author/gerard/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Gerard Bentley
1 post
Build a Streamlit Form Generator app to avoid writing code by hand

Learn how to make extendable starter Forms

by
Gerard Bentley
,
October 24 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

local_decomposition.png (967×496)
https://blog.streamlit.io/content/images/2021/11/local_decomposition.png


external_regressors.png (657×343)
https://blog.streamlit.io/content/images/2021/11/external_regressors.png


dayofweek_perf.png (923×226)
https://blog.streamlit.io/content/images/2021/11/dayofweek_perf.png


overview.png (973×460)
https://blog.streamlit.io/content/images/2021/11/overview.png


detect_frame2_annotations.png (961×538)
https://blog.streamlit.io/content/images/2021/10/detect_frame2_annotations.png#browser


scatter.png (795×372)
https://blog.streamlit.io/content/images/2021/11/scatter.png


performance.png (957×498)
https://blog.streamlit.io/content/images/2021/11/performance.png


seasonality.png (718×317)
https://blog.streamlit.io/content/images/2021/11/seasonality.png


ruffa-jane-reyes-dlGhQPIstkQ-unsplash.jpg (2000×1333)
https://blog.streamlit.io/content/images/2021/10/ruffa-jane-reyes-dlGhQPIstkQ-unsplash.jpg


dataset-1.png (193×163)
https://blog.streamlit.io/content/images/2021/11/dataset-1.png


youtube_withWagon.png (969×636)
https://blog.streamlit.io/content/images/2021/10/youtube_withWagon.png#browser


-New--Hacker-News-Doppelga-nger-GIF--1920.1080-px--1.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/11/-New--Hacker-News-Doppelga-nger-GIF--1920.1080-px--1.gif


04_form-1.gif (856×833)
https://blog.streamlit.io/content/images/2022/10/04_form-1.gif#browser


parkingBoxes_frame2.jpeg (640×360)
https://blog.streamlit.io/content/images/2021/10/parkingBoxes_frame2.jpeg#browser


parkingBoxes_frame1.jpeg (640×360)
https://blog.streamlit.io/content/images/2021/10/parkingBoxes_frame1.jpeg#browser


jackson-hole.jpeg (1300×1014)
https://blog.streamlit.io/content/images/2021/10/jackson-hole.jpeg


03_parsing.gif (856×833)
https://blog.streamlit.io/content/images/2022/10/03_parsing.gif#browser


02_get_input.gif (857×832)
https://blog.streamlit.io/content/images/2022/10/02_get_input.gif#browser


05_demo.gif (853×715)
https://blog.streamlit.io/content/images/2022/10/05_demo.gif#browser


st-typing-playground.gif (1680×818)
https://blog.streamlit.io/content/images/2022/12/st-typing-playground.gif#browser


01_selection.gif (856×833)
https://blog.streamlit.io/content/images/2022/10/01_selection.gif#browser


Streamlit-quests-hero.png (2000×944)
https://blog.streamlit.io/content/images/2022/12/Streamlit-quests-hero.png#shadow


Content-analyzer-hero.png (2000×943)
https://blog.streamlit.io/content/images/2022/12/Content-analyzer-hero.png#shadow


FigmaPluginhero.png (2000×945)
https://blog.streamlit.io/content/images/2022/12/FigmaPluginhero.png#shadow


indexability-image.png (2160×1022)
https://blog.streamlit.io/content/images/2022/12/indexability-image.png#shadow


6-5.png (1146×328)
https://blog.streamlit.io/content/images/2021/08/6-5.png#border


5-7.png (1543×1226)
https://blog.streamlit.io/content/images/2021/08/5-7.png#browser


Cosmoracle_Sneak_Peak.gif (1820×1004)
https://blog.streamlit.io/content/images/2022/02/Cosmoracle_Sneak_Peak.gif


craig-mckay-p3dGOGBFbP4-unsplash.jpg (2000×1269)
https://blog.streamlit.io/content/images/2022/02/craig-mckay-p3dGOGBFbP4-unsplash.jpg


map.png (2000×1479)
https://blog.streamlit.io/content/images/2022/02/map.png


Cosmoracle-GIF--1920-px-.gif (1920×1080)
https://blog.streamlit.io/content/images/2022/02/Cosmoracle-GIF--1920-px-.gif


Sogeti creates an educational Streamlit app for data preprocessing
https://blog.streamlit.io/sogeti-creates-an-educational-streamlit-app-for-data-preprocessing/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Sogeti creates an educational Streamlit app for data preprocessing

Learn how to use Sogeti’s Data Quality Wrapper

By Tijana Nikolic
Posted in Advocate Posts, March 8 2022
What’s the purpose of DQW?
How to preprocess structured data
How to analyze one file
How to preprocess one file
How to compare two files
How to evaluate synthetic data
How to preprocess text data
How to preprocess data
How to do topic analysis with LDA
How to do sentiment analysis
How to preprocess audio data
How to analyze one file
How to augment one file
How to compare two files
How to preprocess image data
How to augment images
BONUS: A few handy tricks for app design
Wrapping up
Contents
Share this post
← All posts

Trying to find the best approach to improving data quality? We know this pain. That’s why we created an app to automate data preprocessing and educate aspiring and experienced data scientists about improving data quality. We chose Streamlit for its ease of development and called our app Sogeti’s Data Quality Wrapper (DQW).

In this post, you’ll learn how to use DQW to preprocess:

Structured data
Text data
Audio data
Image data

TL;DR? Go to the app straight away.🚀  Or jump into the code! 👩🏽‍💻

But before we dive into the fun stuff, let’s talk about...

What’s the purpose of DQW?

Sogeti is the Technology and Engineering Services Division of Capgemini. We’re an IT consultancy for testing, cloud, cyber security, and AI. Sogeti NL’s data science team (where I work) is always looking for ways to automate our workflow. We made DQW as part of ITEA IVVES—a project that focuses on testing AI models in various development phases.

Another product we’re developing is in this project is the Quality AI Framework. To ensure data quality, we use DQW in its initial Data Understanding and Data Preparation phases:

DQW is an accelerator for the Quality AI Framework. It demonstrates the functionality of useful packages and methods around data handling and preprocessing:

App (sub)section	Description	Visualization	Preprocessing	Package
Synthetic structured	x	x		table-evaluator
Structured	x	x		sweetviz
Structured	x	x		pandas-profiling
Structured, text			x	PyCaret
Text	x		x	NLTK
Text	x		x	spaCy
Text	x		x	TextBlob
Text	x	x		word_cloud
Text	x		x	Textstat
Image	x	x	x	Pillow
Audio	x	x		librosa
Audio	x	x		dtw
Audio			x	audiomentations
Audio	x	x		AudioAnalyzer
Report generation	x			FPDF
Report generation	x			wkhtmltopdf
Report generation	x			PDFKit

You can use DQW to preprocess structured data (synthetic data included!), text, images, and audio. Use the main selectbox to navigate through these data formats. Follow the Steps in the sidebar to navigate through the subsections:

How to preprocess structured data

The main page of the app offers structured data analysis. Structured data is data in a well-defined format used in various ML applications. The app offers one-file analysis and preprocessing, two-file comparison, and synthetic data evaluation.

How to analyze one file

Select one file analysis in Step 1, upload your file in Step 2, and select EDA in Step 3. You can also download the report in Step 4. Coding this was easy thanks to the Streamlit pandas-profiling component:

import streamlit as st
import pandas_profiling
from streamlit_pandas_profiling import st_profile_report

uploaded_data = st.sidebar.file_uploader('Upload dataset', type='csv')
data = pd.read_csv(uploaded_data)

# create the pandas profiling report
pr = data.profile_report()
st_profile_report(pr)
# optional, save to file
pr.to_file('pandas_prof.html')

How to preprocess one file

Select one file analysis in Step 1, upload your file in Step 2, select Preprocess and compare in Step 3. In Steps 4 and 5 you can download the report, the files, and the pipeline pickle:

In this subsection, we use PyCaret—a workflow automation package. Streamlit widgets make it easy to select which preprocessing steps you want to run. You can display these steps as a diagram, compare the original and the preprocessed file, and download the report, the files, and the pipeline pickle. The pipeline pickle helps you use PyCaret modeling functions, especially in the case of imbalanced class mitigation with SMOTE. The code used is in the structured_data.py script (see preprocess and show_pp_file functions).

How to compare two files

Select two file comparison in Step 1, upload your files in Step 2, and download the report in Step 3. In this subsection, we use  Sweetviz—an automated EDA library in Python. And to show the report on the app, we use the Streamlit HTML components function:

import streamlit as st
import streamlit.components.v1 as components
import sweetviz as sv

uploaded_ref = st.sidebar.file_uploader('Upload reference dataset', type='csv')
ref= pd.read_csv(uploaded_ref)

uploaded_comparison = st.sidebar.file_uploader('Upload comparison dataset', type='csv')
comparison = pd.read_csv(uploaded_comparison)

sw = sv.compare([ref, 'Reference'], [comparison, 'Comparison'])

sw.show_html(open_browser=False, layout='vertical', scale=1.0)

display = open('SWEETVIZ_REPORT.html', 'r', encoding='utf-8')

source_code = display.read()

# you can pass width as well to configure the size of the report
components.html(source_code, height=1200, scrolling=True)

How to evaluate synthetic data

Select synthetic data comparison in Step 1, upload your files in Step 2, choose one of the two package methods in Step 3, and download the report and the files in Step 4. Here we use the table-evaluator package. It checks all statistical properties (PCA included) and offers multiple model performance comparisons with the original and synthetic dataset:

The code is in the structured_data.py (table_evaluator_comparison), te.py, viz.py, and metrics.py. The report and the files are zipped. Here is the code used in the app:

def generate_zip_structured(original, comparison):
    """ A function to write files to disk and zip 'em """
    original.to_csv('pdf_files/synthetic_data/reference_file_dqw.csv', 
               index=False)
    comparison.to_csv('pdf_files/synthetic_data/comparison_file_dqw.csv', 
               index=False)
    # create a ZipFile object
    dirName = "pdf_files/synthetic_data"
    with ZipFile('pdf_files/synthetic_data.zip', 'w') as zipObj:
        # Iterate over all the files in directory
        for folderName, subfolders, filenames in os.walk(dirName):
        	for filename in filenames:
        		#create complete filepath of file in directory
        		filePath = os.path.join(folderName, filename)
        		# Add file to zip
        		zipObj.write(filePath, basename(filePath))

zip = generate_zip_structured(original, comparison)

# sidebar download, you can remove the sidebar api to have the normal download button
with open('pdf_files/synthetic_data/report_files_dqw.zip', 'rb') as fp:
	st.sidebar.download_button(
	'⬇️',
	data=fp,
	file_name='te_compare_files_dqw.zip',
	mime='application/zip'
	)

How to preprocess text data

Text data is unstructured and is used in NLP models. The text data section offers the flexibility of pasting a body of text or uploading a CSV/JSON file for analysis. Currently, it supports only English, but it offers a lot of analysis methods and automated data preprocessing.

How to preprocess data

Select your data input method in Step 1 and run preprocessing:

This subsection relies on multiple text-preprocessing functions like stemming, lemmatization, de-noising, and stop-word removal. These steps prepare text data in a machine-readable way. You can download the preprocessed file. The code is in the preprocessor.py script.

How to do topic analysis with LDA

Select your data input method in Step 1, run preprocessing (optional), and select topic modeling in Step 2. Choose the topics you want to run or calculate the optimal number of topics based on the u_mass coherence score. LDA topics are visualized in an interactive plot by using pyLDAvis. The code is in the lda.py script:

How to do sentiment analysis

Select your data input method in Step 1, run preprocessing (optional) and select sentiment in Step 2. You can do sentiment analysis with Vader and textblob. It’s an easy way to get the polarity of input text data. The code is in the polarity.py script:

How to preprocess audio data

Audio data is unstructured and used in audio signal processing algorithms such as music genre recognition and automatic speech recognition. The audio data section offers data augmentation, EDA, and comparison of two audio files.

How to analyze one file

Upload one file in Step 1 and select EDA only in Step 2. We use plots with librosa to describe the input audio file. The plot descriptions are in the app. The code is in the audio_data.py script, function audio_eda:

To upload and display the audio file widget, use this code:

import streamlit as st 

audio_file = st.sidebar.file_uploader(label="", 
type=[".wav", ".wave", ".flac", ".mp3", ".ogg"])

st.audio(audio_file , format="audio/wav", start_time=0)

How to augment one file

Upload one file in Step 1 and select Augmentation in Step 2. We use audiomentations—a library for the augmentation of audio files. It increases the robustness of the dataset in the case of a lack of training data. The app also runs EDA on the augmented file.

The code is in the audio_data.py script, function augment_audio. Pass the selected augmentation methods to this function with the Streamlit multiselect API, parse the user input as an expression argument, and evaluate it as a Python expression:

import streamlit as st 
from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift

audio_file = st.sidebar.file_uploader(label="", 
type=[".wav", ".wave", ".flac", ".mp3", ".ogg"])

augmentation_methods = st.multiselect('Select augmentation method:', 
['AddGaussianNoise', 
'TimeStretch', 
'PitchShift', 
'Shift'])  

# add p values to each method and eval parse all list elements
# so they are pushed to global environment as audiomentation methods
augmentation_list = [i + "(p=1.0)" for i in augmentation_methods]
augmentation_final = [eval(i) for i in augmentation_list]

# pass the list to augmentation
augment = Compose(augmentation_list)
How to compare two files

Upload two files in Step 1 and select Spectrum Compare or DTW in Step 2. We compare two files with Dynamic Time Warping (DTW), a method for analyzing the maximum path to the similarity of two time-series inputs. The code is in the audio_data.py script and the function is compare_files. Or we compare two files with audio analyser, a method that compares two spectrums with an applied threshold. The code is in the audio_data.py script and the function is audio_compare:

How to preprocess image data

The image data section offers data augmentation and EDA of your images.

How to augment images

Upload a dataset in Step 1, select Augmentations in Step 2, and download data in Step 3. here we use Pillow. It offers image resizing, noise application, and contrast and brightness adjustment. Thanks to session state you can apply multiple augmentations in a sequence and go back to the previous state if you need to. The code is in the augment.py script:

BONUS: A few handy tricks for app design

You can find a lot of helpful design functions in the helper_functions.py script. Here are a few tricks I use for all my Streamlit apps:

Local file as a background hack. This is a lifesaver. Simply use this code to pass a local file and open it as background:

def set_bg_hack(main_bg):
	'''
	A function to unpack an image from root folder and set as bg.
	
	Returns
	-------
	The background.
	'''
	# set bg name
	main_bg_ext = "png"
	  
	st.markdown(
	   f"""
	   <style>
	   .stApp {{
	       background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, "rb").read()).decode()});
	       background-size: cover
	   }}
	   </style>
	   """,
	   unsafe_allow_html=True
	)


Custom themes. Streamlit offers an easy way of secondary app styling through their UI. Check it out here.

The sidebar design. You can change the width of your sidebar with this simple code:

# set sidebar width
st.markdown(
"""
<style>
[data-testid="stSidebar"][aria-expanded="true"] > div:first-child {
    width: 300px;
}
[data-testid="stSidebar"][aria-expanded="false"] > div:first-child {
    width: 300px;
    margin-left: -300px;
}
</style>
""",
unsafe_allow_html=True,
)


I prefer to move all of the high-level user-defined steps to the sidebar: upload widgets, select boxes, etc. It’s very simple. Just add .sidebar to call the relevant API:

import streamlit as st

# add a logo to the sidebar
logo = Image.open("logo.png")
st.sidebar.image(logo, use_column_width=True)

# upload widget
file = st.sidebar.file_uploader("Upload file here")

# selectbox
add_selectbox = st.sidebar.selectbox(
    "What would you like to do?",
    ("EDA", "Preprocess", "Report")
)

Passing HTML code as text. Since this is a very robust app with a lot of components that need to be explained, in some cases it’s useful to pass HTML code as text. Use this function:

def sub_text(text):
    '''
    A function to neatly display text in app.
    Parameters
    ----------
    text : Just plain text.
    Returns
    -------
    Text defined by html5 code below.
    '''
    
    html_temp = f"""
    <p style = "color:#1F4E79; text_align:justify;"> {text} </p>
    </div>
    """
    
    st.markdown(html_temp, unsafe_allow_html = True)


Expanders for more information and references. The expanders are a space-saver in robust apps with a lot of text:

import streamlit as st

info = st.expander("Click here for more info on methods used")
with info:
  st.markdown("More information")
Wrapping up

Use the DQW app to automate your data preprocessing during AI model development. It’ll streamline your workflow and ensure transparency and quality. The app is still in development and is one of the many Streamlit apps we’ve created. We hope it’ll help educate the data science community about data-centric model development and data quality methods.

If you have questions, please leave them in the comments below or reach out to me at tia.nikolic@sogeti.com or on LinkedIn.

Thank you for reading this post, and happy app-building! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to build a real-time live dashboard with Streamlit
https://blog.streamlit.io/how-to-build-a-real-time-live-dashboard-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build a real-time live dashboard with Streamlit

5 easy steps to make your own data dashboard

By AbdulMajedRaja RS
Posted in Advocate Posts, April 21 2022
What’s a real-time live dashboard?
1. How to import the required libraries and read input data
2. How to do a basic dashboard setup
3. How to design a user interface
4. How to refresh the dashboard for real-time or live data feed
5. How to auto-update components
Wrapping up
Contents
Share this post
← All posts

Ever thought you could build a real-time dashboard in Python without writing a single line of HTML, CSS, or Javascript?

Yes, you can! In this post, you’ll learn:

How to import the required libraries and read input data
How to do a basic dashboard setup
How to design a user interface
How to refresh the dashboard for real-time or live data feed
How to auto-update components

Can’t wait and want to jump right in? Here's the code repo and the video tutorial.

What’s a real-time live dashboard?

A real-time live dashboard is a web app used to display Key Performance Indicators (KPIs).

If you want to build a dashboard to monitor the stock market, IoT Sensor Data, AI Model Training, or anything else with streaming data, then this tutorial is for you.

1. How to import the required libraries and read input data

Here are the libraries that you’ll need for this dashboard:

Streamlit (st). As you might’ve guessed, you’ll be using Streamlit for building the web app/dashboard.
Time, NumPy (np). Because you don’t have a data source, you’ll need to simulate a live data feed. Use NumPy to generate data and make it live (looped) with the Time library (unless you already have a live data feed).
Pandas (pd). You’ll use pandas to read the input data source. In this case, you’ll use a Comma Separated Values (CSV) file.

Go ahead and import all the required libraries:

import time  # to simulate a real time data, time loop

import numpy as np  # np mean, np random
import pandas as pd  # read csv, df manipulation
import plotly.express as px  # interactive charts
import streamlit as st  # 🎈 data web app development


You can read your input data in a CSV by using pd.read_csv(). But remember, this data source could be streaming from an API, a JSON or an XML object, or even a CSV that gets updated at regular intervals.

Next, add the pd.read_csv() call within a new function get_data() so that it gets properly cached.

What's caching? It's simple. Adding the decorator @st.experimental_memo will make the function get_data() run once. Then every time you rerun your app, the data will stay memoized! This way you can avoid downloading the dataset again and again. Read more about caching in Streamlit docs.

dataset_url = "https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv"

# read csv from a URL
@st.experimental_memo
def get_data() -> pd.DataFrame:
    return pd.read_csv(dataset_url)

df = get_data()


2. How to do a basic dashboard setup

Now let’s set up a basic dashboard. Use st.set_page_config() with parameters serving the following purpose:

The web app title page_title in the HTML tag <title> and in the browser tab
The favicon that uses the argument page_icon (also in the browser tab)
The layout = "wide" that renders the web app/dashboard with a wide-screen layout
st.set_page_config(
    page_title="Real-Time Data Science Dashboard",
    page_icon="✅",
    layout="wide",
)

3. How to design a user interface

A typical dashboard contains the following basic UI design components:

A page title
A top-level filter
KPIs/summary cards
Interactive charts
A data table

Let’s drill into them in detail.

Page title

The title is rendered as the <h1> tag. To display the title, use st.title(). It’ll take the string “Real-Time / Live Data Science Dashboard” and display it in the Page Title.

# dashboard title
st.title("Real-Time / Live Data Science Dashboard")


Top-level filter

First, create the filter by using st.selectbox(). It’ll display a dropdown with a list of options. To generate it, take the unique elements of the job column from the dataframe df. The selected item is saved in an object named job_filter:

# top-level filters
job_filter = st.selectbox("Select the Job", pd.unique(df["job"]))

Now that your filter UI is ready, use job_filter to filter your dataframe df.

# dataframe filter
df = df[df["job"] == job_filter]


KPIs/summary cards

Before you can design your KPIs, divide your layout into a 3 column layout by using st.columns(3). The three columns are kpi1, kpi2, and kpi3. st.metric() helps you create a KPI card. Use it to fill one KPI in each of those columns.

st.metric()’s label helps you display the KPI title. The value **is the argument that helps you show the actual metric (value) and add-ons like delta to compare the KPI value with the KPI goal.

# create three columns
kpi1, kpi2, kpi3 = st.columns(3)

# fill in those three columns with respective metrics or KPIs
kpi1.metric(
    label="Age ⏳",
    value=round(avg_age),
    delta=round(avg_age) - 10,
)

kpi2.metric(
    label="Married Count 💍",
    value=int(count_married),
    delta=-10 + count_married,
)

kpi3.metric(
    label="A/C Balance ＄",
    value=f"$ {round(balance,2)} ",
    delta=-round(balance / count_married) * 100,
)

Interactive charts

Split your layout into 2 columns and fill them with charts. Unlike the metric above, use the with clause to fill the interactive charts in the respective columns:

Density_heatmap in fig_col1
Histogram in fig_col2
# create two columns for charts
fig_col1, fig_col2 = st.columns(2)

with fig_col1:
    st.markdown("### First Chart")
    fig = px.density_heatmap(
        data_frame=df, y="age_new", x="marital"
    )
    st.write(fig)
   
with fig_col2:
    st.markdown("### Second Chart")
    fig2 = px.histogram(data_frame=df, x="age_new")
    st.write(fig2)

Data table

Use st.dataframe() to display the data frame. Remember, your data frame gets filtered based on the filter option selected at the top:

st.markdown("### Detailed Data View")
st.dataframe(df)

4. How to refresh the dashboard for real-time or live data feed

Since you don’t have a real-time or live data feed yet, you’re going to simulate your existing data frame (unless you already have a live data feed or real-time data flowing in).

To simulate it, use a for loop from 0 to 200 seconds (as an option, on every iteration you’ll have a second sleep/pause):

for seconds in range(200):

    df["age_new"] = df["age"] * np.random.choice(range(1, 5))
    df["balance_new"] = df["balance"] * np.random.choice(range(1, 5))
    time.sleep(1)

Inside the loop, use NumPy's random.choice to generate a random number between 1 to 5. Use it as a multiplier to randomize the values of age and balance columns that you’ve used for your metrics and charts.

5. How to auto-update components

Now you know how to do a Streamlit web app!

To display the live data feed with auto-updating KPIs/Metrics/Charts, put all these components inside a single-element container using st.empty(). Call it placeholder:

# creating a single-element container.
placeholder = st.empty()


Put your components inside the placeholder by using a with clause. This way you’ll replace them in every iteration of the data update. The code below contains the placeholder.container() along with the UI components you created above:

with placeholder.container():

    # create three columns
    kpi1, kpi2, kpi3 = st.columns(3)

    # fill in those three columns with respective metrics or KPIs
    kpi1.metric(
        label="Age ⏳",
        value=round(avg_age),
        delta=round(avg_age) - 10,
    )
    
    kpi2.metric(
        label="Married Count 💍",
        value=int(count_married),
        delta=-10 + count_married,
    )
    
    kpi3.metric(
        label="A/C Balance ＄",
        value=f"$ {round(balance,2)} ",
        delta=-round(balance / count_married) * 100,
    )

    # create two columns for charts
    fig_col1, fig_col2 = st.columns(2)
    
    with fig_col1:
        st.markdown("### First Chart")
        fig = px.density_heatmap(
            data_frame=df, y="age_new", x="marital"
        )
        st.write(fig)
        
    with fig_col2:
        st.markdown("### Second Chart")
        fig2 = px.histogram(data_frame=df, x="age_new")
        st.write(fig2)

    st.markdown("### Detailed Data View")
    st.dataframe(df)
    time.sleep(1)

And...here is the full code!

import time  # to simulate a real time data, time loop

import numpy as np  # np mean, np random
import pandas as pd  # read csv, df manipulation
import plotly.express as px  # interactive charts
import streamlit as st  # 🎈 data web app development

st.set_page_config(
    page_title="Real-Time Data Science Dashboard",
    page_icon="✅",
    layout="wide",
)

# read csv from a github repo
dataset_url = "https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv"

# read csv from a URL
@st.experimental_memo
def get_data() -> pd.DataFrame:
    return pd.read_csv(dataset_url)

df = get_data()

# dashboard title
st.title("Real-Time / Live Data Science Dashboard")

# top-level filters
job_filter = st.selectbox("Select the Job", pd.unique(df["job"]))

# creating a single-element container
placeholder = st.empty()

# dataframe filter
df = df[df["job"] == job_filter]

# near real-time / live feed simulation
for seconds in range(200):

    df["age_new"] = df["age"] * np.random.choice(range(1, 5))
    df["balance_new"] = df["balance"] * np.random.choice(range(1, 5))

    # creating KPIs
    avg_age = np.mean(df["age_new"])

    count_married = int(
        df[(df["marital"] == "married")]["marital"].count()
        + np.random.choice(range(1, 30))
    )

    balance = np.mean(df["balance_new"])

    with placeholder.container():

        # create three columns
        kpi1, kpi2, kpi3 = st.columns(3)

        # fill in those three columns with respective metrics or KPIs
        kpi1.metric(
            label="Age ⏳",
            value=round(avg_age),
            delta=round(avg_age) - 10,
        )
        
        kpi2.metric(
            label="Married Count 💍",
            value=int(count_married),
            delta=-10 + count_married,
        )
        
        kpi3.metric(
            label="A/C Balance ＄",
            value=f"$ {round(balance,2)} ",
            delta=-round(balance / count_married) * 100,
        )

        # create two columns for charts
        fig_col1, fig_col2 = st.columns(2)
        with fig_col1:
            st.markdown("### First Chart")
            fig = px.density_heatmap(
                data_frame=df, y="age_new", x="marital"
            )
            st.write(fig)
            
        with fig_col2:
            st.markdown("### Second Chart")
            fig2 = px.histogram(data_frame=df, x="age_new")
            st.write(fig2)

        st.markdown("### Detailed Data View")
        st.dataframe(df)
        time.sleep(1)


To run this dashboard on your local computer:

Save the code as a single monolithic app.py.
Open your Terminal or Command Prompt in the same path where the app.py is stored.
Execute streamlit run app.py for the dashboard to start running on your localhost and the link would be displayed in your Terminal and also opened as a new Tab in your default browser.
Wrapping up

Congratulations! You have learned how to build your own real-time live dashboard with Streamlit. I hope you had fun along the way.

If you have any questions, please leave them below in the comments or reach out to me at 1littlecoder@gmail.com or on Linkedin.

Thank you for reading, and Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Matthijs van der Wild - Streamlit
https://blog.streamlit.io/author/matthijs/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Matthijs van der Wild
1 post
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

Advocate Posts
by
Nikolina Sarcevic and 
2
 more,
February 17 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Marco Bonici - Streamlit
https://blog.streamlit.io/author/marco/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Marco Bonici
1 post
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

Advocate Posts
by
Nikolina Sarcevic and 
2
 more,
February 17 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Nikolina Sarcevic - Streamlit
https://blog.streamlit.io/author/nikolina/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Nikolina Sarcevic
1 post
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

Advocate Posts
by
Nikolina Sarcevic and 
2
 more,
February 17 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

markdown-2.gif (1200×800)
https://blog.streamlit.io/content/images/2022/01/markdown-2.gif#border


install-1.gif (1200×800)
https://blog.streamlit.io/content/images/2022/01/install-1.gif#border


content_analyzer_app.gif (3840×2160)
https://blog.streamlit.io/content/images/2022/11/content_analyzer_app.gif#browser


Forecasting with Streamlit Prophet
https://blog.streamlit.io/forecasting-with-streamlit-prophet/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Forecasting with Streamlit Prophet

How Artefact built a Streamlit app to train time-series forecasting models

By Maxime Lutel
Posted in Advocate Posts, November 10 2021
What is Streamlit Prophet?
Step 1. Data exploration
Step 2. Performance evaluation
Step 3. Error diagnosis
Step 4. Model optimization
Step 5. Forecast interpretability
Wrapping up
Contents
Share this post
← All posts

Need a baseline for your latest time-series forecasting project? Want to explain the decision-making process of a predictive model to a business audience? Struggling to understand if car prices are seasonal before buying a new one? I might have something for you...

A forecasting app Streamlit Prophet! 🔮

In this post, I'll teach you how to build it in 5 simple steps:

Step 1. Data exploration
Step 2. Performance evaluation
Step 3. Error diagnosis
Step 4. Model optimization
Step 5. Forecast interpretability

Want to jump right in? Test the app online or install the python package and run it locally.

But before we get to the fun stuff, let's talk about...

What is Streamlit Prophet?

Streamlit Prophet is a Streamlit app that helps data scientists create forecasting models without coding. Simply upload a dataset with historical values of the signal. The app will train a predictive model in a few clicks. And you get several visualizations to evaluate its performance and for further insights.

The underlying model is built with Prophet. Prophet is an open-source library developed by Facebook to forecast time-series data. The signal is broken down into several components such as trend, seasonalities, and holidays effects. The estimator learns how to model each of these blocks, then adds their contributions to produce an interpretable forecast. It performs better when the series has strong seasonal patterns and several cycles of historical data.

Let’s say, you want to predict the future sales for consumer goods in a store. Your historical data ranges from 2011 to 2015. A baseline model with default parameters fits on the data as you upload it. Your dataset will look like this:

First rows of the dataset with historical sales

Now, let's dive into building it!

Step 1. Data exploration

The first step in any forecasting project is to make sure that you know the dataset perfectly. Prophet provides a nice decomposition of the signal. The app has several charts to show you these insights at a glance.

The following graph gives you a global representation of the uploaded time series:

Global visualization showing the trend, the forecasts, and the true values
Actual sales. The black points are actual historical sales. Often they're between 75 and 225 units per day.
Outliers. At Christmas time, when the stores are closed, you can spot some outliers with low or no sales.
Trend. The red line shows the trend. It gives you a synthetic vision of the signal and helps visualize the global evolutions.
Prophet predictions. The blue line represents the forecasts made by a Prophet model that's automatically trained on your dataset. The model expects the sales to increase in 2016, following the growing trend that started in 2015.

These forecasts seem to be seasonal, but it's hard to distinguish between the different periodic components on this first plot. Let’s check another visualization to understand how these seasonal patterns affect the model output:

Prophet model's seasonal components

Two periodicities have been detected. They give you interesting insights into consumers’ habits:

Weekly cycle. The weekly cycle shows that most people shop on weekends, during which forecasts are increased by nearly 40 units per day.
Yearly seasonality. The graph also suggests that the sold products have a yearly seasonality, with more sales during the summer as compared to the rest of the year. The estimator will combine these periodic components and the global trend to produce the forecasts for future days.
Step 2. Performance evaluation

These plots synthesize the way data is modeled by Prophet. But is this representation reliable?

To answer this question, a section of the app is dedicated to the evaluation of the model quality. It shows a baseline forecasting performance. The time series is split into several parts. First, the model is fitted on a training set. Then it gets tested on a validation set. You also have advanced options like cross-validation.

There are different metrics to assess the model quality. Absolute metrics such as the root mean squared error (RMSE) calculate the magnitude of errors in the number of sales. At the same time, the relative metrics like the mean absolute percentage error (MAPE) are more interpretable. It's up to you to select the metric that's most relevant for your use case.

Because performance is unlikely to be uniform over all data points, getting a global indicator is not enough. Compute the metrics at a more detailed granularity to get a clear understanding of the model quality.

Let’s start with an in-depth analysis at the daily level, which is the lowest possible granularity in our case, as the model makes one prediction per day:

App's performance evaluation section

Observe an important variability. There are days when the error is bigger than 20% while some other forecasts are almost perfectly accurate. With this information in mind, you're probably wondering if there are patterns in the way the model makes mistakes. Are there days when it performs poorly? Fortunately, the app provides handy charts to satisfy our curiosity.

Step 3. Error diagnosis

The error diagnosis section is the most useful one. Here you can highlight the areas where forecasts could be improved and identify the challenges while building a reliable forecasting model. Use interactive visualizations to focus on specific areas.

For example, the scatter plot below represents each forecast made on the validation set by a single point (hover over the ones away from the red line to see which data points forecasts are far from the truth):

Scatter plot showing the true values versus the forecasted ones

In this example, hovering over the top right area shows that the points furthest from the red line are Saturdays and Sundays. This suggests that the model performs better during the week.

Let’s aggregate daily performance metrics to validate this intuition:

Model performance aggregated by days

On average, errors are bigger on the weekends. Keep this in mind when optimizing the model. Performance might also evolve over time. Select other levels of aggregation in the app to check it out. For example, compute metrics at a weekly or monthly granularity, or over the period of time when you suspect it to perform differently.

Step 4. Model optimization

Now you know the model’s main weaknesses. How do you improve it? In the app’s sidebar edit the default configuration and enter your specifications. All performance metrics and visualizations are updated each time you change settings, so you can get quick feedback.

User specifications entered in the sidebar

First, apply some customized pre-processing to your dataset. You have several alternatives to get around the challenges identified earlier. For example, a cleaning section lets you get rid of the outliers observed at Christmas. The outliers might confuse the model. You can also filter out particular days and train distinct models for the week and the weekends, as they seemed to be associated with different purchasing behaviors. Some other filtering and resampling options are available as well, in case they're relevant to your problem.

To help the model better fit the data, you can tune Prophet hyper-parameters. They influence how the estimator learns to represent the trend and the seasonalities from historical sales, and the relative weight of these components in the global forecast. Don’t worry if you’re not familiar with Prophet models. Tooltips explain the intuition behind each parameter and guide you through the tuning process.

In the modeling section, you can also feed the model with external information such as holidays or variables related to the signal to be forecasted (like the products’ selling price). These regressors are likely to improve performance as they provide the model with additional knowledge about a phenomenon that impacts sales.

Step 5. Forecast interpretability

Having an accurate forecasting model is nice, but being able to explain the main factors that contribute to its predictions is even better.

The last section of the app aims at helping you understand how your model makes decisions. You can look at a single component and see how its contribution to the overall forecasts evolves over time. Or you can take a single forecast and decompose it into the sum of contributions from several components.

Let’s start with the first option. The different components that influence forecasts are the trend, the seasonalities, and the external regressors. You already observed the impact of the weekly and the yearly seasonalities, so let’s take a look at the external regressors like the holidays and the products’ selling price:

Global impact of the external regressors in the model's forecasts

The impact of the public holidays is important. For example, every year at the beginning of September Labor Day increases forecasts by 50 sales. On Christmas, the dips show that the model has taken into account the fact that stores are closed on that day. And the price has increased year after year. So its impact on sales has shifted from positive to negative.

Let's take a look at how the model produces one specific forecast, especially when a particular event influences the prediction. This waterfall chart shows this decomposition for the forecast made on October 31st, 2012:

Contribution of the different forecast components on October 31st, 2012

In this example, the model ended up forecasting 96 sales. This is the sum of the contributions from five different components:

Global trend (+134): the most influential factor.
Halloween effect (-12): fewer sales on Halloween than on regular days.
Sell price (+2): the price must've been lower than on average that day.
Weekly seasonality (-23): this was a Wednesday, so not the weekend.
Yearly seasonality (-5): October is a low season for this product.

That kind of decomposition is not only useful for sharing insights with collaborators, it can also help analysts understand why their model doesn’t perform as expected. The app’s sidebar has parameters to increase or decrease the different components’ relative weights.

Wrapping up

I hope you add Streamlit Prophet to your forecasting toolbox. Look at the source code to learn more about how this app was built. And visit the Artefact tech blog for more information about our data science projects.

Have questions or improvement ideas? Please post them in the comments below or reach out to me through Medium or LinkedIn.

Thanks a lot for reading! ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Screen-Recording-2021-10-12-at-12.33.21-PM--1-.gif (4096×2304)
https://blog.streamlit.io/content/images/2021/11/Screen-Recording-2021-10-12-at-12.33.21-PM--1-.gif#shadow


Detecting parking spots with Streamlit
https://blog.streamlit.io/detecting-parking-spots-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Detecting parking spots with Streamlit

How to build a Streamlit parking spot app in 8 simple steps

By Jeffrey Jex
Posted in Advocate Posts, October 26 2021
1. How to stream from a YouTube camera to Streamlit with OpenCV
2. How to detect parking spots with Mask R-CNN
3. How to connect it all in Streamlit
Wrapping up
Contents
Share this post
← All posts

Struggling to find a parking spot in a busy place? Then keep reading!

In this post, I'll teach you how to build a Streamlit app that detects parking spots from a live YouTube stream. You'll learn:

How to stream from a YouTube camera to Streamlit with OpenCV
How to detect parking spots with Mask R-CNN
How to connect it all in Streamlit

Can't wait to try it for yourself? Here's a sample app and repo code.

Let's get started!

1. How to stream from a YouTube camera to Streamlit with OpenCV

Let's say, you're on a road trip. You've arrived in Jackson Hole, Wyoming. You want to take a selfie with the elk antler arches. But there are no parking spots. What do you do?

Iconic antler arch in Jackson Hole, Wyoming. Photo by Carol M Highsmith on Rawpixel.

This is 2021. There's data for that. 😉

Jackson Hole's town square has a public webcam. Let's analyze its camera's YouTube video stream. If Google can recognize faces in photographs, why not recognize cars in parking spots? 🚗

Step 1: Import libraries for pulling the video stream into Python

# Video getting and saving
import cv2  # open cvs, image processing
import urllib
import m3u8
import time
import pafy  # needs youtube_dl


For our next app, can we help horse-drawn wagons find parking? ☝️

Step 2: Make a function to grab and display a video clip

def watch_video(video_url, image_placeholder, n_segments=1, 
							 n_frames_per_segment=60):
    """Gets a video clip, 
        video_url: YouTube video URL
				image_placeholder: streamlit image placeholder where clip will display
				n_segments: how many segments of the YouTube stream to pull (max 7)
				n_frames_per_segment: how many frames each segment should be, max ~110
				"""
		####################################
		# SETUP BEFORE PLAYING VIDEO TO GET VIDEO URL
		####################################

		# Speed up playing by only showing every nth frame
    skip_n_frames=10
    video_warning = st.warning("showing a clip from youTube...")

    # Use pafy to get the 360p url
    url=video_url
    video = pafy.new(url)

    # best = video.getbest(preftype="mp4")  #  Get best resolution stream available
		# In this specific case, the 3rd entry is the 360p stream,
		#   But that's not ALWAYS true.
    medVid = video.streams[2]

    #  load a list of current segments for live stream
    playlist = m3u8.load(medVid.url)

    # will hold all frames at the end
    # can be memory intestive, so be careful here
    frame_array = []

    # Speed processing by skipping n frames, so we need to keep track
    frame_num = 0

		###################################
    # Loop over each frame of video, and each segment
		###################################
    #  Loop through all segments
    for i in playlist.segments[0:n_segments]:

        capture = cv2.VideoCapture(i.uri)

        # go through every frame in each segment
        for i in range(n_frames_per_segment):
					
						# open CV function to pull a frame out of video
            success, frame = capture.read()
            if not success:
                break

            # Skip every nth frame to speed processing up
            if (frame_num % skip_n_frames != 0):
                frame_num += 1
                pass
            else:
                frame_num += 1

								# Show the image in streamlit, then pause
                image_placeholder.image(frame, channels="BGR")
                time.sleep(0.5)
              

    # Clean up everything when finished
    capture.release()  # free the video

		# Removes the "howing a clip from youTube" message"
    video_warning.empty()  

    st.write("Done with clip, frame length", frame_num)

    return None


Wow, lots going on here! You've got some initial setup, then a loop where the video is being displayed one frame at a time, followed by closing the video and removing the messages the user saw while the video played.

NOTE: YouTube videos are copyrighted! YouTube terms were updated earlier in 2021 to restrict people from doing machine learning 'face harvesting' from videos. I received permission from the video poster I'm using in the example.

Step 3: Wrap this in another function to build a sidebar menu

def camera_view():
    # streamlit placeholder for image/video
    image_placeholder = st.empty()

    # url for video
    # Jackson hole town square, live stream
    video_url = "<https://youtu.be/DoUOrTJbIu4>"

    # Description for the sidebar options
    st.sidebar.write("Set options for processing video, then process a clip.")
    
		# Make a slider bar from 60-360 with a step size of 60
		#  st.sidebar is going to make this appear on the sidebar
    n_frames=60  # Frames per 'segment"
    n_segments = st.sidebar.slider("How many frames should this video be:",
        n_frames, n_frames*6, n_frames, step=n_frames, key="spots", help="It comes in 7 segments, 100 frames each")

		# We actually need to know the number of segments, 
		#   so convert the total number of frames to the number of segments we want
    n_segments = int(n_segments/n_frames)

		# Add a "go" button to show the clip in streamlit
    if st.sidebar.button("watch video clip"):
        watch_video(video_url=video_url,
                            image_placeholder=image_placeholder,
                            n_segments=n_segments,
                            n_frames=n_frames)


This wrapper lets the user control when to start the video and how long it should be.

NOTE: For Streamlit Cloud to work, you'll need a requirements.txt and a packages.txt files. This tells the server what Python libraries and packages you'll need. To make requirements.txt use pip list --format=freeze > requirements.txt. The packages file contains anything you needed to install (i.e., not from PyPi or conda). This is for openCV. Use virtual environments to manage Python packages. Here's my sample requirements file and packages file.

2. How to detect parking spots with Mask R-CNN

Before you find available spots, you'll need to map out where the spots are. Following Adam Geitgey's method, I've used a video clip during 'rush hour' when all spots were full and ran Mask R-CNN to detect vehicles. The idea was to identify cars that didn't move and assume they were parked.

In technical terms, you'll detect all the cars in the first frame and make a bounding box around them. A few frames later you'll detect where all the cars are again. If any bounding boxes from the first frame are still full in the later frame, it's a parking spot. Any boxes that weren't full are considered noise and are thrown away (i.e., a car driving on the road).

Step 4: Do imports for this section, which includes Mask R-CNN

# IMPORTS
# general
import numpy as np
import pandas as pd

# File handling
from pathlib import Path
import os
import pickle
from io import BytesIO
import requests
import sys

# Import mrcnn libraries
# I am using Matterport mask R-CNN modified for tensor flow 2, see source here:
# <https://raw.githubusercontent.com/akTwelve/Mask_RCNN/master>'
# You'll need a copy of mrcnn in your working directory, it can't be installed
#  from pypi yet.
import mrcnn.config
import mrcnn.utils
from mrcnn.model import MaskRCNN



Matterport's Mask R-CNN model hasn't been updated for TensorFlow 2. Luckily, someone else has done it. Use these installation instructions.

How is it going so far?

Look! You're standing on the shoulders of giants! Your tool includes code from Google (TensorFlow 2) and Facebook (Mask R-CNN). Next, you'll add pre-trained weights that enable the model to classify 80 different object categories. Two of those categories are cars and trucks. This is what will tell you if parking spots are vacant or not.

Time to make your model and load the weights.

Step 5: Create your Mask R-CNN model

def maskRCNN_model():
    """Makes a Mask R-CNN model, ideally save to cache for speed"""
    weights = get_weights()
    # Create a Mask-RCNN model in inference mode
    model = MaskRCNN(mode="inference", model_dir="model", config=MaskRCNNConfig())
    
    # Load pre-trained model
    model.load_weights(weights, by_name=True)
    model.keras_model._make_predict_function()
    
    return model

#Let's invoke it like so, with a nice message for streamlit users.
# Give message while loading weights
weight_warning = st.warning("Loading model, might take a few minutes, hold on...")

#Create model with saved weights
model = maskRCNN_model()

weight_warning.empty()  # Make the warning go away, done loading


Pass this model around to your other functions to do the object detection work.

Step 6: Combine your YouTube streaming code with the model

def detectSpots(video_file, model,
                 show_video=True, initial_check_frame_cutoff=10):
    '''detectSpots(video_file, initial_check_frame_cutoff=10)
    Returns: np 2D array of all bounding boxes that are still occupied
    after initial_check_frame_cutoff frames.  These can be considered "parking spaces".
    An update might identify any spaces that get occupied at some point and stay occupied 
    for a set length of time, in case some areas start off vacant.
		video_file: saved or online video file to detect on
		model: mask R-CNN model
		show_video: Boolean, display to streamlit or not
		initial_check_frame_cutoff: The frame number to compare against the 1st. Frames
					After this number will be ignored. i.e. =55, then frame 55 will have detection
					run to compare which boxes from the first frame still have a car in them.
		'''
    # Load the video file we want to run detection on
    video_capture = cv2.VideoCapture(video_file)

    # Store the annotated frames for output to video/counting how many frames we've seen
    frame_array = []

    # Will contain bounding boxes of parked cars to identify 'parkable spots'
    parked_car_boxes = []
		# same as above, but for final pass
    parked_car_boxes_updated = []

    # Make image appear in streamlit
    image_placeholder_processing = st.empty()

    # Loop over each frame of video
    while video_capture.isOpened():
        success, frame = video_capture.read()
        if not success:
            st.write(f"Processed {len(frame_array)} frames of video, exiting.")
            return parked_car_boxes

        # Convert the image from BGR color (which OpenCV uses) to RGB color
        rgb_image = frame[:, :, ::-1]

        # ignore the inbetween frames 0 to x, don't run the model on them and save processing time
        if 0 < len(frame_array) < initial_check_frame_cutoff:
            print(f"ignore this frame for processing, #{len(frame_array)}")
        else:
            print(f"Processing frame: #{len(frame_array)}")
            # Run the image through the Mask R-CNN model to get results

            results = model.detect([rgb_image], verbose=0)

            # Mask R-CNN assumes we are running detection on multiple images.
            # We only passed in one image to detect, so only grab the first result.
            r = results[0]

            # The r variable will now have the results of detection:
            # - r['rois'] are the bounding box of each detected object
            # - r['class_ids'] are the class id (type) of each detected object
            # - r['scores'] are the confidence scores for each detection
            # - r['masks'] are the object masks for each detected object (which gives you the object outline)

            if len(frame_array) == 0:
                # This is the first frame of video,
                # Save the location of each car as a parking space box and go to the next frame of video.
                # We check if any of those cars moved in the next 5 frames and assume those that don't are parked
                parked_car_boxes = get_car_boxes(r['rois'], r['class_ids'])
                parked_car_boxes_init = parked_car_boxes
                print('Parking spots 1st frame:', len(parked_car_boxes))

            # If we are past the xth initial frame, already know where parked cars are, then check if any cars moved:
            else:
                # We already know where the parking spaces are. Check if any are currently unoccupied.

                # Get where cars are currently located in the frame
                car_boxes = get_car_boxes(r['rois'], r['class_ids'])

                # See how much those cars overlap with the known parking spaces
                overlaps = mrcnn.utils.compute_overlaps(parked_car_boxes, car_boxes)

                # Loop through each known parking space box
                for row, areas in enumerate(zip(parked_car_boxes, overlaps)):
                    parking_area, overlap_areas = areas
                    # For this parking space, find the max amount it was covered by any
                    # car that was detected in our image (doesn't really matter which car)
                    max_IoU_overlap = np.max(overlap_areas)

                    # Get the top-left and bottom-right coordinates of the parking area
                    y1, x1, y2, x2 = parking_area

                    # Check if the parking space is occupied by seeing if any car overlaps
                    # it by more than x amount using IoU
                    if max_IoU_overlap < 0.20:
                        # In the first few frames, remove this 'spot' and consider it as a moving car instead
                        # Transient event, draw green box
                        cv2.rectangle(frame, (x1, y1),
                                      (x2, y2), (0, 255, 0), 3)
                    else:
                        # Consider this a parking spot, car is still in it!
                        # Dangerous to mutate array while using it! So using a copy
                        parked_car_boxes_updated.append(list(parking_area))

                        # Parking space is still occupied -> draw a red box around it
                        cv2.rectangle(frame, (x1, y1),
                                      (x2, y2), (0, 0, 255), 1)

                    # Write the top and bottom corner locations in the box for ref
                    font = cv2.FONT_HERSHEY_DUPLEX
                    cv2.putText(frame, str(parking_area),
                                (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255))

                parked_car_boxes = np.array(
                    parked_car_boxes_updated)  # only happens once

        # print number of frames
        font = cv2.FONT_HERSHEY_COMPLEX_SMALL
        cv2.putText(frame, f"Frame: {len(frame_array)}",
                    (10, 340), font, 0.5, (0, 255, 0), 2, cv2.FILLED)

        # Show the frame of video on the screen
        if show_video:
            image_placeholder_processing.image(frame, channels="BGR")
            time.sleep(0.01)

        # Append frame to outputvideo
        frame_array.append(frame)

        # stop when cutoff reached
        if len(frame_array) > initial_check_frame_cutoff:
            print(f"Finished, processed frames: 0 - {len(frame_array)}")
            break

    # Clean up everything when finished
    video_capture.release()
    #write_frames_to_file(frame_array=frame_array, file_name=video_save_file)

    # Show final image in matplotlib for ref
    return parked_car_boxes


The machine learning toolbox has something called "intersection over union" or IoU. IoU calculates the number of pixels where two bounding boxes overlap. IoU is used heavily inside of CNNs to find the 'best' bounding box around detected objects, removing all the initial overlapping guesses.

Set a cutoff to determine how 'occupied' a box needs to be, to be considered full. Think of this as a 'fraction of overlap' where 1 = perfect overlap and 0 = no overlap.

Here is what it looks like.

First frame:

Identify ALL cars in this frame and save their bounding box.

Nth frame:

Compare IoU on the boxes from the first frame with the cars in this frame. If IoU is high (i.e., there's still a car in the spot), count it as a parking spot. Red = 'parking spot' (had a car in both frames). Green = 'not a valid spot' (didn't have cars in the nth frame).

But look...

This method isn't perfect! Notice the red boxes that aren't parking spots? One was never a car. And in another one, two separate cars happened to be in the same place. This caused the algorithm to consider it a parking spot.

Step 7: Combine your YouTube clip with the detected parking spot to see which spots are full or empty

Detect all cars in the given frame.
For each car, see if it's overlapping a parking spot.
Color the parking spot boxes red if filled and green if vacant.
Repeat for all frames.

This code is similar to the detect spots function. It's doing the same things but on all processed frames instead of only two. Need help? See the function countSpots() in my repo code linked at the top.

3. How to connect it all in Streamlit

Well done! You've built the framework for a solid app.

Step 8: Tie all these functions together with a Streamlit interface

st.title("Title: Spot Or Not?")
st.write("Subtitle: Parking Spot Vacancy with Machine Learning")

# Render the readme as markdown using st.markdown as default
#  This loads a markdown file from your github.  Much cleaner than putting loads
#  of text in your Streamlit app! Easier to edit too!
readme_text = st.markdown(get_file_content_as_string("instructions.md"))

 # Add a menu selector for the app mode on the sidebar.
st.sidebar.title("Settings")
app_mode = st.sidebar.selectbox("Choose the app mode",
    ["Show instructions", "Live data", "Camera viewer","Show the source code"])
if app_mode == "Show instructions":
    st.sidebar.success('Next, try selecting "Camera viewer".')
# Loads the source code and displays it
elif app_mode == "Show the source code":
    readme_text.empty()
    st.code(get_file_content_as_string("streamlit_app.py"))
elif app_mode == "Live data":
    # Add horizontal line to sidebar
    st.sidebar.markdown("___")
    readme_text.empty()
    live_mode()
elif app_mode == "Camera viewer":
    # Add horizontal line to sidebar
    st.sidebar.markdown("___")
    readme_text.empty()
    camera_view()


I didn't go over the live_mode() function, but it's essentially a wrapper for detectSpots() and countSpots(), similar to what you've built for the camera viewer. Streamlit reruns the ENTIRE app every time you make a menu selection. Newer versions of Streamlit (such as 1.0 and later) can store session state. To keep things simple, stick with this menu.

Let's walk through what happens here.

After the app loads, it'll display the "show instructions" (the first item in your selectbox). If a user selects a different option, you need to clear the readme_text using readme_text.empty().  .empty()tells Streamlit to clear that particular object. Then you run your wrapper functions to show whatever it is the user selected in the main area. Since you don't have a session state, you don't need to 'erase' the previous choice, except the default- readme_text.

Wrapping up
Photo by Ruffa Jane Reyes on Unsplash

Congratulations! You did it. You got a video stream from YouTube, used unsupervised machine learning to detect parking spaces from a video, identified which of those spaces are vacant, and tied it all together in Streamlit.

If you plan to deploy to Streamlit Cloud, do it early and test it regularly to avoid package dependency despair. Keep building and don't give up!

What's next?

Parking lot vacancy data has many potential uses:

Vehicle routing—send vehicles to open spots via navigation app or digital signage.
Proxy for how busy an area is (shopping malls, airports, conference centers, etc.) to inform visitors.
Identification of the commonly used "parking spots" that aren't allowed (illegal parking).

If you found working with these tools amazing and frustrating at the same time, let's agree to ride a bike next time and avoid the parking hassle! 🚲

Got questions? Let me know in the comments below or message me on LinkedIn.

Happy Streamlit-ing! ❤️

References:

Acharya, Yan. Real-time image-based parking occupancy detection using deep learning.
Cazamias, Marek (2016). Parking Space Classification using Convolutional Neural Networks.
Amato, Giuseppe; Carrara, Fabio; Falchi, Fabrizio; Gennaro, Claudio; Vairo, Claudio. Accessed June 2021, CNR park.
COCO dataset, used to train Matterport model.
Dwivedi, Priya (2018). Find where to park in real time using OpenCV and Tensorflow.
Geitgey, Adam (2019). Snagging parking spaces with mask R CNN and Python.
He, Kaiming; Gkioxari, Georgia; Dollár, Piotr; Girshick, Ross (2017). Original Mask R-CNN paper by Facebook research.
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Finding your look-alikes with semantic search
https://blog.streamlit.io/finding-your-look-alikes-with-semantic-search/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Finding your look-alikes with semantic search

How Pinecone used Streamlit to create a Hacker News Doppelgänger app

By Greg Kogan
Posted in Advocate Posts, December 1 2021
Why a Doppelgänger app?
Step 1. Create a vector database of Hacker News users
1. Create a database in Pinecone
2. Retrieve the data
3. Prepare and embed the data
4. Insert the data
5. Query Pinecone
Step 2.  Build the app in Streamlit
1. Install Streamlit
2. Create a base Streamlit app
3. Create Store and Effects
4. Layout the page
Step 3. Combine the two together
Wrapping up
Contents
Share this post
← All posts

Do you want to find like-minded people on Hacker News with a similar commenting history?

We've got you covered!

In this post, you'll learn how to build a Doppelgänger app in three simple steps:

Create a vector database in Pinecone.
Build an app in Streamlit.
Combine the two together.

Can't wait and want to see how it works? Try the app right here.

But before we get into building it, let's answer one question...

Why a Doppelgänger app?

Searching for your celebrity doppelgänger isn’t a new idea. In fact, it’s so unoriginal that no one has updated the celebrity-face dataset in three years!

But we weren't looking for celebrities. We were looking for users with matching comment histories—Hacker News "celebrities" like patio11, tptacek, and pc.

At Pinecone, we've built a vector database that makes it easy to add semantic search to production applications. We were intrigued by the idea of making a semantic search app for Hacker News. Could it compare the semantic meaning of your commenting history with the histories of all the other users?

So we thought, "How about the doppelgänger idea but for Hacker News?"

It took only a few hours to build it, with most of that time being spent on converting raw data into vector embeddings (more below) and debating which users to feature as examples. The app got a lot of attention on Hacker News (Surprise!), getting thousands of visitors and 215 comments. Many people asked how it works, so here's an inside look at how we made it and how you can make your own version.

Step 1. Create a vector database of Hacker News users
1. Create a database in Pinecone

Create a new vector index for storing and retrieving data by semantic similarity. We use cosine similarity as it's more intuitive and widely used with word vectors.

!pip install -qU pinecone-client
!pip install -qU sentence-transformers
!pip install -qU google-cloud-bigquery
!pip install -q pyarrow pandas numpy

import pinecone
import os

# Load Pinecone API key
api_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'
# Set Pinecone environment. Default environment is us-west1-gcp
env = os.getenv('PINECONE_ENVIRONMENT') or 'us-west1-gcp'
pinecone.init(api_key=api_key, environment=env)

index_name = 'hackernews-doppel-demo'
pinecone.create_index(index_name, dimension=300, metric="cosine", shards=1)
index = pinecone.Index(index_name)

2. Retrieve the data

Create a class to collect the data from the publicly available dataset on BigQuery. Get every comment and story from every user that hasn't been deleted or labeled as "dead" in the last three years (stories and comments killed by software, moderators, or user flags).

from google.cloud.bigquery import Client

class BigQueryClient:
    __client = None

    @classmethod
    def connect(cls):
        os.environ[
            'GOOGLE_APPLICATION_CREDENTIALS'] = '<file_name>'
        cls.__client = Client()

    @classmethod
    def get_client(cls):
        if cls.__client is None:
            cls.connect()
        return cls.__client

3. Prepare and embed the data

Collect and merge all available data for each user—with no additional processing steps and no weights added to comments or stories.

You'll face two limitations:

Caring about all comments and stories equally.
Capturing exactly why a user was matched with someone else if they've changed interests in the last three years.

Next, create a single embedding for each user with the help of the average word embeddings of Komninos and Manandhar (about three hours). This algorithm works much faster when compared to other state-of-the-art approaches (such as the commonly used BERT model).

from sentence_transformers import SentenceTransformer
MODEL = SentenceTransformer('average_word_embeddings_komninos')

import pandas as pd
import numpy as np
from typing import List

class NewsDataPrep():

    def load_data(self, interval: int) -> pd.DataFrame:
        news_data = pd.DataFrame()

        try:
            print('Retrieving data from bigquery..')
            query = f"""
                    SELECT distinct b.by as user, b.title, b.text
                    FROM `bigquery-public-data.hacker_news.full` as b
                    WHERE b.timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(),
                    INTERVAL {interval} DAY) 
                    AND b.timestamp <= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), 
                    INTERVAL {interval - 90} DAY)
                    AND (b.deleted IS NULL AND b.dead IS NULL)
                    AND (b.title IS NOT NULL OR b.text IS NOT NULL)
                    AND b.type in ('story', 'comment');
                    """

            query_job = BigQueryClient.get_client().query(query)
            news_data = query_job.to_dataframe()
        except Exception as e:
            if '403' in str(e):
                print('Exceeded quota for BigQuery! (403)')

            if '_InactiveRpcError' in str(e):
                print('Pinecone service is not active '
                          'at the moment (_InactiveRpcError)')
            print(e)
        return news_data

    def get_embeddings(self, news_data: pd.DataFrame) -> List:
        def func(x):
            return [*map(np.mean, zip(*x))]

        news_data.title.fillna(value='', inplace=True)
        news_data.text.fillna(value='', inplace=True)
        news_data['content'] = news_data.apply(
            lambda x: str(x.title) + " " + str(x.text), axis=1)
        vectors = MODEL.encode(news_data.content.tolist())
        news_data['vectors'] = vectors.tolist()
        news_data_agg = (news_data.groupby(['user'], as_index=False)
                         .agg({'vectors': func}))

        # return user_embeddings
        return list(zip(news_data_agg.user, news_data_agg.vectors))

4. Insert the data

Now insert the data (as vector embeddings) into the Pinecone database. Our total index size (the number of inserted embeddings) was around 230,000. Each data point was represented as a single tuple that contained a user ID and a corresponding vector. Each vector contained 300 dimensions.

import itertools

def chunks(iterable, batch_size=100):
    it = iter(iterable)
    chunk = tuple(itertools.islice(it, batch_size))
    while chunk:
        yield chunk
        chunk = tuple(itertools.islice(it, batch_size))

data_days_download= 1100

news_data_prep = NewsDataPrep()

for i in range(data_days_download, 0, -90):
    print(f'Loading data from {i - 90} to {i} days in the past')
    news_data = news_data_prep.load_data(interval=i)
    print('Creating embeddings. It will take a few minutes')
    embeddings = news_data_prep.get_embeddings(news_data)
    print('Starting upsert')
    for batch in chunks(embeddings, 500):
        index.upsert(vectors=batch)
    print('Upsert completed')

5. Query Pinecone

Your database is ready to be queried for the top 10 similar users given any user ID (represented as a vector embedding). Let's build a Streamlit app so that anyone can do this through their browser.

Step 2.  Build the app in Streamlit

The above summarized the data preparation and the database configuration steps (see the Pinecone quickstart guide for instructions). With the data vectorized and loaded into Pinecone, you can now build a Streamlit app to let anyone query that database through the browser.

1. Install Streamlit

Install Streamlit by running:

pip install streamlit


To see some examples of what Streamlit is capable of, run:

streamlit hello

2. Create a base Streamlit app

Create a base class to represent your Streamlit app. It'll contain a store and an effect object. You'll use the effect object to initialize Pinecone and to save the index name in the store. Next, add a render method to handle the page layout.

In a Streamlit app, each user action prompts the screen to be cleared and the main function to be run. Create the app and call render. In render, use st.title to display a title, then call render on the home page.

import streamlit as st

class App:
	title = "Hacker News Doppelgänger"

	def __init__(self):
		self.store = AppStore()
		self.effect = AppEffect(self.store)
		self.effect.init_pinecone()

	def render(self):
		st.title(self.title)
		PageHome(self).render()

if __name__ == "__main__":
	App().render()

3. Create Store and Effects

The store will be used to hold all the data needed to connect to Pinecone. To connect to a Pinecone index, you'll need your API key and the name of your index. You'll take this data from environment variables.

To set these locally, run:

export PINECONE_API_KEY=<api-key> && export PINECONE_INDEX_NAME=<index-name>


These can be set in a published Streamlit app during the creation process or by changing the settings on a running app:

import os
from dataclasses import dataclass

API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

@dataclass
class AppStore:
	api_key = API_KEY
	index_name = INDEX_NAME


Use the AppEffect class to connect your app to Pinecone (with init) and to the index (docs):

class AppEffect:
	
	def __init__(self, store: AppStore):
		self.store = store

	def init_pinecone(self):
		pinecone.init(api_key=self.store.api_key)

	def init_pinecone_index(self):
		return pinecone.Index(self.store.index_name)

4. Layout the page

Create and fill out the render method of the PageHome class.

First, use st.markdown to display instructions. Under it, display the buttons for suggested usernames. Use st.beta_columns to organize Streamlit elements in columns and st.button to place a clickable button on the page.

If the app's last action was clicking on that button, then st.button will return True. Save the value of that user in st.session_state (to save and use this value between renderings):

def render_suggested_users(self):
	st.markdown("Try one of these users:")
	columns = st.beta_columns(len(SUGGESTED_USERNAMES))
	for col, user in zip(columns, SUGGESTED_USERNAMES):
		with col:
			if st.button(user):
				st.session_state.username = user


Below the suggested users, show a text entry where the user can enter any username and a submit button which they can click on, to search.

To do this, use st.form with st.text_input and st.form_submit_buttonm. If you have a selected username saved in st.session_state.markdown, put that value in the text box. Otherwise, leave it empty for user input.

Now, return the value from st.form_submit_button . It'll return true if the user clicked the submit button on the last run:

def render_search_form(self):
	st.markdown("Or enter a username:")
	with st.form("search_form"):
		if st.session_state.get('username'):
			st.session_state.username = st.text_input("Username", value=st.session_state.username)
		else:
			st.session_state.username = st.text_input("Username")
		return st.form_submit_button("Search")


Once the user searches, render the results. Use st.spinner to show a progress indicator to the user while loading the results. Because of Pinecone's blazing-fast search speeds, the loading icon won't be visible for long!

To complete the search, fetch the user from your Pinecone index using the entered username as the ID. No vector for the user? That means they didn't have any activity on Hacker News in the last three years, so you'll see an error message.

If you find a user, query Pinecone for the closest matches. Use a Markdown table to display the results and include a link to their Hacker News comment history as well as the proximity score for each result:

def render_search_results(self):
	with st.spinner("Searching for " + st.session_state.username):
		result = self.index.fetch(ids=st.session_state.username)
		has_user = len(result.vector) != 0
	if !has_user:
		return st.markdown("This user does not exist or does not have any recent activity.")
	with st.spinner("Found user history, searching for doppelgänger"):
		closest = self.index.queries(queries=result.vector, top_k=11)
	results = [{'username': id, 'score': round(score, 3)}
			for id, score in zip(closest.ids, closest.scores)
			if id != st.session_state.username][:10]
	result_strings = "\
".join([

f"|[{result.get('username')}](<https://news.ycombinator.com/threads?id={result.get('username')}>)|{result.get('score')}|" for result in results
])
	markdown = f"""
	| Username | Similarity Score |
	|----------|------:|
	{result_strings}
	"""
	with st.beta_container():
		st.markdown(markdown)

Step 3. Combine the two together

You're almost done! All that's left is to tie it all together in a single render method:

class PageHome:

	def __init__(self, app):
		self.app = app
	
	@property
	def index(self):
		return self.app.effect.init_pinecone_index()

	def render(self):
		self.render_suggested_users()
		submitted = self.render_search_form()
		if submitted:
			self.render_search_results()```


Congratulations! 🥳

You now have a fully functioning Hacker News Doppelgänger app. Run streamlit.app.py and navigate to localhost:8051 to see your app in action.

Wrapping up

Thank you for reading this post. We're very excited to have shared this with you and we hope this inspires you to build your own semantic search application with Pinecone and Streamlit.

Have questions or improvement ideas? Please leave them in the comments below or send them to info@pinecone.io or @pinecone.

Happy app-building! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Spot-Or-Not-GIF.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/11/Spot-Or-Not-GIF.gif#border


Cosmoracle-social.gif (1266×636)
https://blog.streamlit.io/content/images/2022/02/Cosmoracle-social.gif#browser


Timelapse--Original-Res-1452-px----1--1.gif (1452×844)
https://blog.streamlit.io/content/images/2021/12/Timelapse--Original-Res-1452-px----1--1.gif#shadow


empty-1.png (1920×1080)
https://blog.streamlit.io/content/images/2022/01/empty-1.png#border


Streamlit-Prophet.gif (1200×675)
https://blog.streamlit.io/content/images/2021/12/Streamlit-Prophet.gif#shadow


Build a Streamlit Form Generator app to avoid writing code by hand
https://blog.streamlit.io/build-a-streamlit-form-generator-app-to-avoid-writing-code-by-hand/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build a Streamlit Form Generator app to avoid writing code by hand

Learn how to make extendable starter Forms

By Gerard Bentley
October 24 2022
What is Streamlit Form Generator app?
1. Get an OpenAPI Specification (OAS) from the user by upload, text box, or URL.
2. Generate Python code with Pydantic BaseModel classes from objects in the OAS API schema.
3. Create a ZIP archive of the generated code for users to download and make demos with
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Gerard Bentley. I’m a backend engineer at Sensible Weather and a Streamlit Creator.

I made lots of Streamlit demos to understand our web API endpoints—that took writing a bunch of similar code. So instead of writing Python models and starter Streamlit code by hand, I combined Streamlit-Pydantic, Datamodel Code Generator, and the OpenAPI Specification and built the Streamlit Form Generator app.

In this post, I’ll show you how to build it step-by-step:

Get an OpenAPI Specification (OAS) from the user by upload, text box, or URL.
Generate Python code with Pydantic BaseModel classes from objects in the OAS API schema.
Create a ZIP archive of the generated code for users to download and make demos with.

Can’t wait to get started? Here's the app and the repo.

What is Streamlit Form Generator app?

The Streamlit Form Generator app makes code that accepts and validates user inputs according to an API spec. It will let you:

Get multiple user inputs of an API spec.
Parse it into Pydantic models.
Templatize a new Streamlit-Pydantic form-based app with user-selected options.

Here is the template generated by the example OAS:

In an internal dogfooding demo, I use this form input for making HTTP requests to my development server. I’ve included Streamlit-Folium to let users select the latitude and the longitude with a pin on a map.

Let’s skip writing similar Streamlit Forms and build this app together:

1. Get an OpenAPI Specification (OAS) from the user by upload, text box, or URL.

You can get user input for the app by doing the following:

Use an example OAS (from the Weather Insurance Quote API that I work on at Sensible Weather).
Upload a file containing an OAS.
Manually enter an OAS in a text box.
Fetch a file containing an OAS from a URL.

To start, install Streamlit. Use pip install streamlit in your Python environment of choice or see the detailed Get started guide for more options. After a conventional import streamlit as st, use a radio button to let the user choose their input method:

import streamlit as st

use_example = "Example OpenAPI Specification"
use_upload = "Upload an OpenAPI Specification"
use_text_input = "Enter OpenAPI Specification in Text Input"
use_url = "Fetch OpenAPI Specification from a URL"

st.header('OAS -> Pydantic -> Streamlit Form Code Generator')
input_method = st.radio(
    label="How will you select your API Spec",
    options=[use_example, use_upload, use_text_input, use_url],
)

st.subheader(input_method)



Your app will look something like this:

Change st.radio() to st.sidebar.radio() to see if the app looks better with this option in the sidebar. Or change it to st.selectbox() if you prefer the look of select boxes.

To get the raw OAS text, present the data at each step in an st.expander() for the user to inspect. The expander will hide the content if the user doesn't care:

from pathlib import Path

import httpx
from pydantic import BaseModel, HttpUrl
from streamlit.runtime.uploaded_file_manager import UploadedFile

# ...

@st.experimental_memo
def decode_uploaded_file(oas_file: UploadedFile) -> str:
    return oas_file.read().decode()

@st.experimental_memo
def decode_text_from_url(oas_url: str) -> str:
    try:
        response = httpx.get(oas_url, follow_redirects=True, timeout=10)
        return response.text
    except Exception as e:
        print(repr(e))
        return ""

class ValidURL(BaseModel):
    url: HttpUrl

def get_raw_oas(input_method: str) -> str:
    if input_method == use_example:
        st.write("This will demo how the app works!")
        oas_file = Path("quote-oas.json")
        raw_oas = oas_file.read_text()
    elif input_method == use_upload:
        st.write("This will let you use your own JSON or YAML OAS!")
        oas_file = st.file_uploader(
            label="Upload an OAS",
            type=["json", "yaml", "yml"],
            accept_multiple_files=False,
        )
        if oas_file is None:
            st.warning("Upload a file to continue!")
            st.stop()
        raw_oas = decode_uploaded_file(oas_file)
    elif input_method == use_text_input:
        st.write("This will parse raw text input into JSON or YAML OAS!")
        raw_oas = st.text_area(label="Enter OAS JSON or YAML text")
        if not len(raw_oas):
            st.warning("Enter OAS text to continue!")
            st.stop()
    elif input_method == use_url:
        st.write("This will fetch text from the URL containing a JSON or YAML OAS!")
        raw_oas_url = st.text_input(label="Enter the URL that hosts the OAS")
        try:
            oas_url = ValidURL(url=raw_oas_url)
        except Exception as e:
            print(repr(e))
            st.warning("Enter a valid HTTP(S) URL to continue!")
            st.stop()
        raw_oas = decode_text_from_url(oas_url.url)
    else:
        raise Exception("Unknown input_method")
    return raw_oas

raw_oas = get_raw_oas()
with st.expander("Show input OAS"):
    st.code(raw_oas)



Now your app should look something like this:

The point of this get_raw_oas block is to get the raw OAS text regardless of the input method:

In the "Example" branch, use the built-in Path class to read_text() from a file in the same folder as your Streamlit app (this works when the app is deployed to Streamlit Community Cloud and the file is in GitHub).
In the "Upload" branch, use st.file_uploader() to let the user drag and drop or choose their file. This provides None if the user hasn’t uploaded anything and a BytesIO-like object if they have.
In the "Text Input" branch, use st.text_area() to allow free text input. You can opt for something fancier such as Streamlit Quill for a rich text editor.
In the "URL" branch, let the user input URL as text, validate it with Pydantic's HttpURL class, and 'GET' the file from the URL using [httpx](<https://github.com/encode/httpx/>).
For the "Upload" and the "URL" branches, use st.experimental_memo to cache the results between re-runs. The "Example" branch runs fast enough, and the "Text Input" branch is already cached until changed (because that’s how Streamlit works).

In all of the user upload branches warn the user and stop the script if they haven't added anything. You can use this technique with Streamlit forms as well.

2. Generate Python code with Pydantic BaseModel classes from objects in the OAS API schema.

Now, let’s go over using Datamodel Code Generator to parse the raw input text into the Python Pydantic classes. It’s super convenient for making Python clients that communicate with APIs. You can do the following:

Get type validation and autocomplete.
Dump Pydantic models into the JSON format to call your API.
Parse the JSON response into Pydantic models for validation and further handling.

Dictionaries are more convenient for hacking around, but one misspelled key can make bug-hunting hard. Import the module and follow this documentation example of calling it a module. Use a TemporaryDirectory so as not to pollute the Streamlit Community Cloud app (you’ll allow downloading at the end).

If the OAS is very large, cache this function:

import ast
from dataclasses import dataclass
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import List

from datamodel_code_generator import InputFileType, generate

# ...

@dataclass
class ModuleWithClasses:
    name: str
    code: str
    classes: List[str]

@st.experimental_memo()
def parse_into_modules(raw_oas: str) -> List[ModuleWithClasses]:
    with TemporaryDirectory() as temporary_directory_name:
        temporary_directory = Path(temporary_directory_name)
        module_files = generate_module_or_modules(raw_oas, temporary_directory)

        modules = []
        for module in module_files:
            module_code = module.read_text()

            module_ast = ast.parse(module_code)
            module_class_names = [
                x.name for x in module_ast.body if isinstance(x, ast.ClassDef)
            ]
            modules.append(
                ModuleWithClasses(
                    name=module.stem,
                    code=module_code,
                    classes=module_class_names,
                )
            )
    return modules

def generate_module_or_modules(raw_oas: str, output_directory: Path) -> List[Path]:
    output = Path(output_directory / "models.py")
    try:
        generate(
            raw_oas,
            input_file_type=InputFileType.OpenAPI,
            output=output,
        )
        return [output]
    except Exception as e:
        print(repr(e))
        try:
            generate(
                raw_oas,
                input_file_type=InputFileType.OpenAPI,
                output=output_directory,
            )
            return list(output_directory.iterdir())
        except Exception as e:
            print(repr(e))
            return []

modules = parse_into_modules(raw_oas)
if not len(modules):
    st.error("Couldn't find any models in the input!")
    st.stop()

st.success(f"Generated {len(modules)} module files")

all_module_models = []
for module in modules:
    import_name = module.name
    if import_name != "models":
        import_name = f"models.{import_name}"
    with st.expander(f"Show Generated Module Code: {import_name}"):
        st.code(module.code)

    for model_name in module.classes:
        all_module_models.append((module.name, model_name))



To let the user pick classes for Streamlit form inputs, use Python's Abstract Syntax Tree module to parse (ast.parse()) each generated module and grab them into a list.

Try the generate() function with an output target of a single file Path and a directory Path. Showing the full error text to the user might reveal sensitive data, so printing it to the console is good enough.

There are more full-featured code generators such as this open-source OpenAPITools project with over 30 languages. I used Datamodel Code Generator to make a flexible Streamlit + Pydantic template. It also produces idiomatic Pydantic code and has the flexibility to handle JSON schema, raw JSON/CSV/YAML data, and even a Python dictionary. (It’s a Python library, and I don't know how to run OpenAPI generator as an npm package or Java jar on Streamlit Community Cloud 😅.)

After you generate Pydantic models from the OAS, your app will look something like this:

3. Create a ZIP archive of the generated code for users to download and make demos with

Generate code for a Streamlit form with Streamlit-Pydantic and whatever generated classes the user selects as input possibilities.

Since most API specs will have some models that are only used within other models, let the user decide what they want to use for their Streamlit forms. You can further utilize the OpenAPI specification documentation to check what schema models are used as request bodies, but this will take a lot of modifications to the Datamodel Code Generator.

Use Python string manipulation to make a starting point for using the selected models in Streamlit. The guts are not very exciting and should be replaced by a templating library such as jinja2 if you want to make more complicated starter code:

if len(all_module_models) > 1:
    selections = st.multiselect(
        label="Select Models that will be Form Inputs",
        options=all_module_models,
        default=all_module_models[0],
        format_func=lambda x: f"{x[0]}.{x[1]}",
    )
else:
    selections = list(all_module_models)

def generate_header(models_with_modules: List[Tuple[str, str]]) -> str:
# ...

def generate_single_model_form(model: str) -> str:
# ...

def generate_multi_model_form(models: List[str]) -> str:
# ...

@st.experimental_memo
def generate_streamlit_code(selected_module_models: List[Tuple[str, str]]) -> str:
    streamlit_code = generate_header(selected_module_models)
    if len(selected_module_models) == 1:
        model_module, model = selected_module_models[0]
        streamlit_code += generate_single_model_form(model)
    else:
        models = [model for _, model in selected_module_models]
        streamlit_code += generate_multi_model_form(models)
    return streamlit_code

streamlit_code = generate_streamlit_code(selections)
with st.expander("Show Generated Streamlit App Code", True):
    st.code(body=streamlit_code, language="python")



One lacking aspect of templating is repetitive imports. Do a quick run of isort on the generated code to clean this up.

For the user to choose multiple models as form inputs, give the template a radio selector in the sidebar. You can adapt it to multi-page apps (I kept it in one page).

At this point, your app should look something like this:

Build the models.py if it’s contained in a single module, or build the models directory structure with all of the modules.

Streamlit's download_button works great with a bytes object:

@st.experimental_memo()
def zip_generated_code(modules: List[ModuleWithClasses], streamlit_code: str) -> bytes:
# ...

zip_bytes = zip_generated_code(modules, streamlit_code)
st.download_button(
    label="Download Zip of Generated Code",
    data=zip_bytes,
    file_name="generated_code.zip",
)

Wrapping up

Congratulations! You did it. You learned how to build your own Streamlit Form Generator app. I hope it will help you make user-friendly demos of your favorite APIs!

If you have any questions, please drop them below in the comments or reach out to me on GitHub, LinkedIn, or Twitter with your ideas or inspiring projects!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

memory.png (2000×1125)
https://blog.streamlit.io/content/images/2021/11/memory.png#border


image-1.png (2000×755)
https://blog.streamlit.io/content/images/2021/12/image-1.png#browser


session_state_0x7fedb68992c8.png (815×1668)
https://blog.streamlit.io/content/images/2021/12/session_state_0x7fedb68992c8.png#browser


Untitled.png (1588×952)
https://blog.streamlit.io/content/images/2021/12/Untitled.png#browser


4.png (700×295)
https://blog.streamlit.io/content/images/2021/12/4.png


5.png (472×506)
https://blog.streamlit.io/content/images/2021/12/5.png


10.png (880×424)
https://blog.streamlit.io/content/images/2021/12/10.png


9-1.png (800×158)
https://blog.streamlit.io/content/images/2021/12/9-1.png


8-1.png (800×175)
https://blog.streamlit.io/content/images/2021/12/8-1.png


7.png (480×980)
https://blog.streamlit.io/content/images/2021/12/7.png


6.png (494×590)
https://blog.streamlit.io/content/images/2021/12/6.png


3-1.png (496×362)
https://blog.streamlit.io/content/images/2021/12/3-1.png


Streamlit (Page 8)
https://blog.streamlit.io/page/8/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing Streamlit to the Polish Python community

My Streamlit presentation at PyWaW #103

Product
by
Michał Nowotka
,
April 4 2023
Building an Instagram hashtag generation app with Streamlit

5 simple steps on how to build it

Advocate Posts
by
William Mattingly
,
March 29 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Hackathon 101: 5 simple tips for beginners

Prepare to win your first hackathon!

Tutorials
by
Chanin Nantasenamat
,
March 16 2023
Create a search engine with Streamlit and Google Sheets

You’re sitting on a goldmine of knowledge!

Advocate Posts
by
Sebastian Flores Benner
,
March 14 2023
10 most common explanations on the Streamlit forum

A guide for Streamlit beginners

Advocate Posts
by
Debbie Matthews
,
March 9 2023
Building a PivotTable report with Streamlit and AG Grid

How to build a PivotTable app in 4 simple steps

Advocate Posts
by
Pablo Fonseca
,
March 7 2023
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

Product
by
Lukas Masuch and 
2
 more,
February 28 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

Product
by
Tim Conkling and 
2
 more,
February 14 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1.png (939×599)
https://blog.streamlit.io/content/images/2021/12/1.png


Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/7/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Build a Jina neural search with Streamlit

Use Jina to search text or images with the power of deep learning

Advocate Posts
by
Alex C-G
,
April 15 2021
How to use Roboflow and Streamlit to visualize object detection output

Building an app for blood cell count detection

Advocate Posts
by
Matt Brems
,
February 23 2021
Developing a streamlit-webrtc component for real-time video processing

Introducing the WebRTC component for real-time media streams

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
February 12 2021
Arup and New Story use data to help combat pandemic related evictions

Making data accessible to help address the eviction crisis

Advocate Posts
by
Jared Stock
,
January 7 2021
Gravitational-wave apps help students learn about black holes

Exploring distant space with gravitational waves

Advocate Posts
by
Jonah Kanner
,
December 15 2020
Build knowledge graphs with the Streamlit Agraph component

A powerful and lightweight library for visualizing networks/graphs

Advocate Posts
by
Christian Klose
,
November 25 2020
New UC Davis tool tracks California's COVID-19 cases by region

Regional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine

Advocate Posts
by
Pranav Pandit
,
November 19 2020
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Jeffrey Jex - Streamlit
https://blog.streamlit.io/author/jeffrey/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jeffrey Jex
1 post
Detecting parking spots with Streamlit

How to build a Streamlit parking spot app in 8 simple steps

Advocate Posts
by
Jeffrey Jex
,
October 26 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Download_Button--Full-Screen--1.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/10/Download_Button--Full-Screen--1.gif#browser


Maxime Lutel - Streamlit
https://blog.streamlit.io/author/maxime/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Maxime Lutel
1 post
Website
Forecasting with Streamlit Prophet

How Artefact built a Streamlit app to train time-series forecasting models

Advocate Posts
by
Maxime Lutel
,
November 10 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

2.png (967×178)
https://blog.streamlit.io/content/images/2021/12/2.png


Greg Kogan - Streamlit
https://blog.streamlit.io/author/greg/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Greg Kogan
1 post
Finding your look-alikes with semantic search

How Pinecone used Streamlit to create a Hacker News Doppelgänger app

Advocate Posts
by
Greg Kogan
,
December 1 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Eitan Halper-Stromberg - Streamlit
https://blog.streamlit.io/author/eitan/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Eitan Halper-Stromberg
1 post
How to diagnose blood cancer with Streamlit

Build a molecular pathology diagnostics app in 4 simple steps

Advocate Posts
by
Eitan Halper-Stromberg
,
January 25 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

soc-2-small-long-1.png (1500×506)
https://blog.streamlit.io/content/images/2022/01/soc-2-small-long-1.png


coiled2-3.gif (1600×900)
https://blog.streamlit.io/content/images/2021/10/coiled2-3.gif#browser


Balloons_Snowflake.gif (1515×852)
https://blog.streamlit.io/content/images/2022/03/Balloons_Snowflake.gif#shadow


design---six-tips-1.png (1594×1300)
https://blog.streamlit.io/content/images/2021/08/design---six-tips-1.png#shadow


cache-1.png (1840×1158)
https://blog.streamlit.io/content/images/2021/10/cache-1.png#border


rsource-limits-3.png (1284×1048)
https://blog.streamlit.io/content/images/2021/10/rsource-limits-3.png#border


image.png (2000×2017)
https://blog.streamlit.io/content/images/2021/08/image.png#border


Share-GIF--30-FPS-.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/12/Share-GIF--30-FPS-.gif#shadow


favicon-transparent-1.png (256×256)
https://blog.streamlit.io/content/images/size/w256h256/2021/03/favicon-transparent-1.png


MicrosoftTeams-image--23---2-.png (2000×2467)
https://blog.streamlit.io/content/images/2021/08/MicrosoftTeams-image--23---2-.png#shadow


Sogeti_feature-GIF-1.gif (1912×955)
https://blog.streamlit.io/content/images/2022/03/Sogeti_feature-GIF-1.gif#shadow


wavo-2.png (1478×1206)
https://blog.streamlit.io/content/images/2021/10/wavo-2.png#browser


CloneRetriever--1726-px---2-.gif (1726×842)
https://blog.streamlit.io/content/images/2022/02/CloneRetriever--1726-px---2-.gif#shadow


Delta_dental_hero.png (2554×1302)
https://blog.streamlit.io/content/images/2022/02/Delta_dental_hero.png#browser


Radius-Explorer--Static-.png (1894×932)
https://blog.streamlit.io/content/images/2022/05/Radius-Explorer--Static-.png#border


Live-Data-Science-Dashboard-GIF.gif (1717×997)
https://blog.streamlit.io/content/images/2022/05/Live-Data-Science-Dashboard-GIF.gif#border


Memory-Leak.png (2000×755)
https://blog.streamlit.io/content/images/2022/05/Memory-Leak.png#border


supabase.png (1664×796)
https://blog.streamlit.io/content/images/2022/05/supabase.png#browser


OSS-podcast-1.png (1986×1656)
https://blog.streamlit.io/content/images/2022/04/OSS-podcast-1.png#shadow


30DaysOfStreamlit--1-.png (2000×981)
https://blog.streamlit.io/content/images/2022/05/30DaysOfStreamlit--1-.png#shadow


Vincent D. Warmerdam - Streamlit
https://blog.streamlit.io/author/vincent/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Vincent D. Warmerdam
1 post
Making apps for the Rasa research team (and open source community!)

Helping Rasa users understand their models

Advocate Posts
by
Vincent D. Warmerdam
,
May 12 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

pasted-image-0--3-.png (1170×772)
https://blog.streamlit.io/content/images/2021/08/pasted-image-0--3-.png#browser


5--2-.png (1200×690)
https://blog.streamlit.io/content/images/2021/08/5--2-.png#border


4--4-.png (1200×271)
https://blog.streamlit.io/content/images/2021/08/4--4-.png#border


3-2--1-.png (1200×631)
https://blog.streamlit.io/content/images/2021/08/3-2--1-.png#border


2--1-.png (1200×901)
https://blog.streamlit.io/content/images/2021/08/2--1-.png#border


Intro-Rascore-new-1.gif (960×544)
https://blog.streamlit.io/content/images/2022/06/Intro-Rascore-new-1.gif#browser


wissam-hero.png (2000×1069)
https://blog.streamlit.io/content/images/2022/06/wissam-hero.png#shadow


jina.gif (1280×720)
https://blog.streamlit.io/content/images/2022/09/jina.gif#border


streamlit-streamlit.gif (1092×641)
https://blog.streamlit.io/content/images/2022/05/streamlit-streamlit.gif#browser


HappyBirds--1920-.gif (1920×1080)
https://blog.streamlit.io/content/images/2022/02/HappyBirds--1920-.gif#shadow


workspace-analytics-dotted-5.gif (1000×755)
https://blog.streamlit.io/content/images/2022/05/workspace-analytics-dotted-5.gif#browser


streamlit-spell-book.png (1920×1080)
https://blog.streamlit.io/content/images/2021/08/streamlit-spell-book.png


1-3-1.png (1200×420)
https://blog.streamlit.io/content/images/2021/08/1-3-1.png


GIF_in_the_Browser_4--1--3.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/12/GIF_in_the_Browser_4--1--3.gif#shadow


arup.gif (2364×1134)
https://blog.streamlit.io/content/images/2022/08/arup.gif#browser


How-to-master-Streamlit.png (2000×858)
https://blog.streamlit.io/content/images/2022/02/How-to-master-Streamlit.png#border


Alex C-G - Streamlit
https://blog.streamlit.io/author/alexcg/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Alex C-G
Dev Rel Lead at Jina AI, building AI-powered search. Open source evangelist and caffeine addict. Python in my 🧠, Vim in my 🖐️
1 post
Website
Twitter
Build a Jina neural search with Streamlit

Use Jina to search text or images with the power of deep learning

Advocate Posts
by
Alex C-G
,
April 15 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Camera-input-release-demo-1.png (1920×1080)
https://blog.streamlit.io/content/images/2022/02/Camera-input-release-demo-1.png#shadow


3-3.png (2000×1266)
https://blog.streamlit.io/content/images/2021/08/3-3.png#browser


search-console-hero.gif (1234×868)
https://blog.streamlit.io/content/images/2022/07/search-console-hero.gif#browser


2-5.png (2000×1266)
https://blog.streamlit.io/content/images/2021/08/2-5.png#browser


Improved-charts.gif (750×424)
https://blog.streamlit.io/content/images/2022/09/Improved-charts.gif#browser


Jessica Smith - Streamlit (Page 3)
https://blog.streamlit.io/author/jessica/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jessica Smith
26 posts
Monthly rewind > July 2021

Your July look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 5 2021
Monthly rewind > June 2021

Your June look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 5 2021
Monthly rewind > May 2021

Your May look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2021
Monthly rewind > April 2021

Your April look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2021
Monthly rewind > March 2021

Your March look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 5 2021
Monthly rewind > February 2021

Your February look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 8 2021
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

autofilter-dataframes.gif (782×609)
https://blog.streamlit.io/content/images/2022/09/autofilter-dataframes.gif#browser


Screen-Shot-2022-07-20-at-1.44.49-PM.png (2000×1251)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-20-at-1.44.49-PM.png#browser


qiusheng-hero.jpeg (1480×700)
https://blog.streamlit.io/content/images/2022/07/qiusheng-hero.jpeg#border


Jared Stock - Streamlit
https://blog.streamlit.io/author/jared/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jared Stock
Jared is a Digital Consultant at Arup
1 post
Twitter
Arup and New Story use data to help combat pandemic related evictions

Making data accessible to help address the eviction crisis

Advocate Posts
by
Jared Stock
,
January 7 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

open-source-1.png (2000×944)
https://blog.streamlit.io/content/images/2022/09/open-source-1.png#border


dynamic-filters.png (1105×592)
https://blog.streamlit.io/content/images/2022/09/dynamic-filters.png#border


Screen-Shot-2022-07-29-at-4.53.15-PM-1.png (1013×201)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-29-at-4.53.15-PM-1.png#shadow


starterkithero.png (2000×946)
https://blog.streamlit.io/content/images/2022/10/starterkithero.png


streamlitformgenerator.gif (853×715)
https://blog.streamlit.io/content/images/2022/11/streamlitformgenerator.gif#browser


Create-your-own-custom-component.png (2000×941)
https://blog.streamlit.io/content/images/2022/10/Create-your-own-custom-component.png


uplanner.png (1512×882)
https://blog.streamlit.io/content/images/2022/11/uplanner.png#browser


Beginner-Template-Tour.gif (640×362)
https://blog.streamlit.io/content/images/2022/10/Beginner-Template-Tour.gif#browser


Monthly rewind > November 2022
https://blog.streamlit.io/monthly-rewind-november-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > November 2022

Your November look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, December 7 2022
🏆 App of the month 🏆
Streamlit November updates
🔍 Current release: 1.15.1
🔮 Upcoming
🔍 Indexability
🖌️ Figma-to-Streamlit Plugin
Featured Streamlit content
💎 Streamlit Quests: Getting started with Streamlit
📺 Make a video content analyzer app with Streamlit and AssemblyAI
✋ Building robust Streamlit apps with type-checking
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our November featured app of the month is... 🥁🥁🥁🥁🥁

Song Describer by researchers Ilaria Manco, Benno Weck, Philip Tovstogan, Dmitry Bogdanov, and Minz Won.

This app is an open-source data collection platform for annotating music with textual descriptions. By collecting user inputs, the team hopes to create the first research dataset of music-caption pairs. This will give insight into how people describe music and support the development of audio-text ML models. [code]

Streamlit November updates

Check out the latest updates and releases from November.

🔍 Current release: 1.15.1

The latest release is 1.15.1. Recent updates include the ability for widget labels to contain inline Markdown and for st.​audio to properly play audio data from NumPy arrays. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

We're currently working on these new features:

Streamlit theme for 3rd party charting libraries
Colored text
St commands work with Snowpark + PySpark

Visit our roadmap app to see what else we're working on. 🥳

🔍 Indexability
All public Community Cloud apps are now indexable by search engines, making them easier to discover.

🖌️ Figma-to-Streamlit Plugin
After prototyping your apps in Figma, you can turn the designs into code with the Figma-to-Streamlit plugin!


Featured Streamlit content

💎 Streamlit Quests: Getting started with Streamlit
Check out Streamlit Quests—a guide from the Data Professor to help you navigate getting started with Streamlit.

📺 Make a video content analyzer app with Streamlit and AssemblyAI
Learn how to build an analyzer app with Mısra Turp, using Streamlit and AssemblyAI, to easily screen videos for harmful or sensitive content.

✋ Building robust Streamlit apps with type-checking
Quickly find and eliminate defects in your code with type-checking! Harald explains how to use this technique and make it part of your app-building flow.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

See if the first stage of Falcon 9 will land with Hakan's Booster Landing Prediction App.
Santiago's Volve Production Dashboard app visualizes the production dashboard of an oil field and performs exploratory data analysis.
Use Lee's SERP Keyword Extractor to extract the top 10 keywords from each page ranking in the SERPs.
Quickly get started with Streamlit with Misra's Streamlit project template.
Test your geography skills with Gerard's app Guess the Country 'Worldle' Edition.
Kota created a custom component for interactive board games and an app to play the Doubutsu Shogi (Animal Chess) game.
Hubert's SurViZ app lets you quantitatively compare various galaxy missions, such as Euclid, Rubin/LSST, or JWST.
Choe's app gives a 3D Model Reconstruction using FORESIGHT STEREO Technology.
Get EDA, flexible data wrangling, and swift ML modeling using EasyDS—a free app tool by Huijun.
Ayoub's Brainstorming Buddy, built on GPT-3, helps to generate ideas on a given topic.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
October 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to Create Automated Visual Tests [SeleniumBase Tutorial]
https://blog.streamlit.io/testing-streamlit-apps-using-seleniumbase/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Testing Streamlit apps using SeleniumBase

How to create automated visual tests

By Randy Zwitch
Posted in Tutorials, November 23 2020
Case Study: streamlit-folium
Setting up a test harness
Defining test success
Testing using SeleniumBase
Testing using OpenCV
Automating tests using GitHub actions
Writing tests saves work in the long run
Contents
Share this post
← All posts

In the time I’ve worked at Streamlit, I’ve seen hundreds of impressive data apps ranging from computer vision applications to public health tracking of COVID-19 and even simple children’s games. I believe the growing popularity of Streamlit comes from the fast, iterative workflows through the Streamlit “magic” functionality and auto-reloading the front-end upon saving your Python script. Write some code, hit ‘Save’ in your editor, then visually inspect the correctness of each code change. And with the unveiling of Streamlit sharing for easy deployment of Streamlit apps, you can go from idea to coding to deploying your app in just minutes!

Once you've created a Streamlit app, you can use automated testing to future-proof it against regressions. In this post, I'll be showing how to programmatically validate that a Streamlit app is unchanged visually using the Python package SeleniumBase.

Case Study: streamlit-folium

To demonstrate how to create automated visual tests, I’m going to use the streamlit-folium GitHub repo, a Streamlit Component I created for the Folium Python library for leaflet.js. Visual regression tests help detect when the layout or content of an app changes, without requiring the developer to manually visually inspect the output each time a line of code changes in their Python library. Visual regression tests also help with cross-browser compatibility of your Streamlit apps and provide advanced warning about new browser versions affecting how your app is displayed.

Setting up a test harness

The streamlit-folium test harness has three files:

tests/requirements.txt: the Python packages only needed for testing
tests/app_to_test.py: the reference Streamlit app to test
tests/test_package.py: the tests to demonstrate the package works as intended

The first step is to create a Streamlit app using the package to be tested and use that to set the baseline. We can then use SeleniumBase to validate that the structure and visual appearance of the app remains unchanged relative to the baseline.

This post focuses on describing test_package.py since it’s the file that covers how to use SeleniumBase and OpenCV for Streamlit testing.

Defining test success

There are several ways to think about what constitutes looking the same in terms of testing. I chose the following three principles for testing my streamlit-folium package:

The Document Object Model (DOM) structure (but not necessarily the values) of the page should remain the same
For values such as headings, test that those values are exactly equal
Visually, the app should look the same

I decided to take these less strict definitions of “unchanged” for testing streamlit-folium, as the internals of the Folium package itself appear to be non-deterministic. Meaning, the same Python code will create the same looking image, but the generated HTML will be different.

Testing using SeleniumBase

SeleniumBase is an all-in-one framework written in Python that wraps the Selenium WebDriver project for browser automation. SeleniumBase has two functions that we can use for the first and second testing principles listed above: check_window, which tests the DOM structure and assert_text, to ensure a specific piece of text is shown on the page.

To check the DOM structure, we first need a baseline, which we can generate using the check_window function. The check_window has two behaviors, based on the required name argument:

If a folder <name> within the visual_baseline/<Python file>.<test function name> path does not exist, this folder will be created with all of the baseline files
If the folder does exist, then SeleniumBase will compare the current page against the baseline at the specified accuracy level

You can see an example of calling check_window and the resulting baseline files in the streamlit-folium repo. In order to keep the baseline constant between runs, I committed these files to the repo; if I were to make any substantive changes to the app I am testing (app_to_test.py), I would need to remember to set the new baseline or the tests would fail.

With the baseline folder now present, running check_window runs the comparison test. I chose to run the test at Level 2, with the level definitions as follows:

Level 1 (least strict): HTML tags are compared to tags_level1.txt
Level 2: HTML tags and attribute names are compared to tags_level2.txt
Level 3 (most strict): HTML tags, attribute names and attribute values are compared to tags_level3.txt    

As mentioned in the “Defining Test Success” section, I run the check_window function at Level 2, because the Folium library adds an GUID-like id value to the attribute values in the HTML, so the tests will never pass at Level 3 because the attribute values are always different between runs.

For the second test principle (“check certain values are equal”), the assert_text method is very easy to run:

self.assert_text("streamlit-folium")

This function checks that the exact text “streamlit-folium” is present in the app, and the test passes because it’s the value of the H1 heading in this example.

Testing using OpenCV

While checking the DOM structure and presence of a piece of text provides some useful information, my true acceptance criterion is that the visual appearance of the app doesn’t change from the baseline. In order to test that the app is visually the same down to the pixel, we can use the save_screenshot method from SeleniumBase to capture the current visual state of the app and compare to the baseline using the OpenCV package:

	from seleniumbase import BaseCase
	import cv2
	import time
	

	

	class ComponentsTest(BaseCase):
	    def test_basic(self):
	

	        # open the app and take a screenshot
	        self.open("http://localhost:8501")
	

	        time.sleep(10)  # give leaflet time to load from web
	        self.save_screenshot("current-screenshot.png")
	

	        # test screenshots look exactly the same
	        original = cv2.imread(
	            "visual_baseline/test_package.test_basic/first_test/screenshot.png"
	        )
	        duplicate = cv2.imread("current-screenshot.png")
	

	        assert original.shape == duplicate.shape
	

	        difference = cv2.subtract(original, duplicate)
	        b, g, r = cv2.split(difference)
	        assert cv2.countNonZero(b) == cv2.countNonZero(g) == cv2.countNonZero(r) == 0
view raw
seleniumbase.py hosted with ❤ by GitHub

Using OpenCV, the first step is to read in the baseline image and the current snapshot, then compare that the size of the pictures are identical (the shape comparison checks that the NumPy ndarrays of pixels have the same dimensions). Assuming the pictures are both the same size, we can then use the subtract function from OpenCV to calculate the per-element difference between pixels by channel (blue, green and red). If all three channels have no differences, then we know that the visual representation of the Streamlit app is identical between runs.

Automating tests using GitHub actions

With our SeleniumBase and OpenCV code set up, we can now feel free to make changes to our Streamlit Component (or other Streamlit apps) and not worry about things breaking unintentionally. In my single-contributor project, it’s easy to enforce running the tests locally, but with tools such as GitHub Actions available for free for open-source projects, setting up a Continuous Integration pipeline guarantees the tests are run for each commit.

The streamlit-folium has a workflow run_tests_each_PR.yml defined that does the following:

Sets up a test matrix for Python 3.6, 3.7, 3.8
Installs the package dependencies and test dependencies
Lints the code with flake8
Install Chrome with seleniumbase
Run the Streamlit app to test in the background
Run the SeleniumBase and OpenCV tests in Python
	# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
	# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions
	

	name: Run tests each PR
	

	on:
	  push:
	    branches: [ master ]
	  pull_request:
	    branches: [ master ]
	

	jobs:
	

	  build:
	    runs-on: ubuntu-latest
	    strategy:
	      matrix:
	        python-version: [3.6, 3.7, 3.8]
	

	    steps:
	    - uses: actions/checkout@v2
	    - name: Set up Python ${{ matrix.python-version }}
	      uses: actions/setup-python@v2
	      with:
	        python-version: ${{ matrix.python-version }}
	    - name: Install dependencies
	      run: |
	        python -m pip install --upgrade pip
	        pip install flake8 pytest
	        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
	        if [ -f tests/requirements.txt ]; then pip install -r tests/requirements.txt; fi
	    - name: Lint with flake8
	      run: |
	        # stop the build if there are Python syntax errors or undefined names
	        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
	        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
	        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
	    - name: Install chromedriver
	      run: |
	        seleniumbase install chromedriver latest
	    - name: Start Streamlit app
	      run: |
	        streamlit run tests/app_to_test.py &
	    - name: Test with pytest
	      run: |
	        pytest
view raw
run_tests_each_PR.yml hosted with ❤ by GitHub

By having this workflow defined in your repo, and required status checks enabled on GitHub, every pull request will now have the following status check appended to the bottom, letting you know the status of your changes:

Writing tests saves work in the long run

Having tests in your codebase has numerous benefits. As explained above, automating visual regression tests allows you to maintain an app without having to have a human in the loop looking for changes. Writing tests is also a great signal to potential users that you care about stability and long-term maintainability of your projects. It’s not only easy to write tests for a Streamlit app and have them automatically run on each GitHub commit, but that the extra work of adding tests to your Streamlit project will save you time in the long run.  

Have questions about this post or Streamlit in general? Stop by the Streamlit Community forum, start a discussion, meet other Streamlit enthusiasts, find a collaborator in the Streamlit Component tracker or share your Streamlit project! There are plenty of ways to get involved in the Streamlit community and we look forward to welcoming you 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Calculating distances in cosmology with Streamlit
https://blog.streamlit.io/calculating-distances-in-cosmology-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

By Nikolina Sarcevic, Matthijs van der Wild and Marco Bonici
Posted in Advocate Posts, February 17 2022
What is CosmΩracle?
How it all started
How we worked together in a virtual setting
Where to go from here?
Wrapping up
Contents
Share this post
← All posts

Ever looked at the night sky and wondered, "Hey, how far away is that star?" We wondered about it too, and we wanted an app that could calculate this distance in one click. So we made CosmΩracle!

In this post, you’ll learn:

What is CosmΩracle?
How it all started
How we worked together in a virtual setting
Where to go from here?

Want to jump right in? Here's the app and the source code.

But first, let’s talk about...

What is CosmΩracle?

CosmΩracle is an app that calculates distances for different sets of cosmological parameters. Go to www.cosmoracle.com and click on Cosmological Distances. You’ll see a menu in the sidebar with fields for redshift, the Hubble constant, the energy content in the Universe from matter, radiation, dark energy, and different equation of state parameters.

Some sets of parameters are more popular than others (they’re favored by different observational datasets), so we have pre-programmed some values for you. The only thing you need to get started is the value of the redshift. Want to know a certain distance of those galaxies at redshift 2? Simply plug that value into the redshift field, and CosmΩracle will get cracking!

It’s important in science that everyone agrees on what the input parameters represent, and CosmΩracle is no exception. As you can see above, we’ve included a Definitions page where you can see how each parameter is defined and how CosmΩracle uses these parameters to compute quantities that are used in cosmology.

How it all started

There are plenty of “somethings” in the field of physics. If you’re working in cosmology and/or astronomy, at some point you’ll need to use one of the many “distances.” The need for many different types of distances is due to the Universe being very complicated—space expanding, objects moving, interacting, and mixing, things getting shifted and distorted. Depending on the problem you’re working on, you’ll need a certain distance calculated.

On top of this, these distances will depend on the kind of Universe you’re working with and on how much matter it contains. It probably sounds weird at first, but give it some thought. The amount of matter affects the expansion rate of the Universe, so different quantities lead to different increases in distance.

Other things that affect distances are the curvature of the Universe and dark matter or dark energy. If you have all the necessary ingredients, it’s possible to calculate these distances. Most of the time, you just “want the thing calculated” without going through the process of installing Python libraries and writing the code just to get that number.

Until recently, we calculated these distances with the first online version of the famous Ned Wright’s Cosmo Calculator from 1999 or with the astropy cosmology package. Although these are great tools, we felt that there was a need for an improved and modern version.

The three of us met online and decided that our app needs to:

be user-friendly (everyone should be able to use it without spending a lot of time on learning it)
work across all devices: desktop, tablet, and mobile (a la “astronomy on the go”)
be fast and accessible
be correct in terms of physics
be designed in a modern way
represent the results in a numerical and illustrative way (think plots)
have an option to download the calculated data (to be used for further analysis)
be designed in a way that’s easy to maintain and upgrade
be open-source

How to achieve it?

All three of us have a programming background. We work in physics and love exploring the latest advancements in the field of data visualization. Sure, we could “code up” functions to calculate all sorts of things. But we wanted a simple and neat way of putting apps online. Every option we found needed knowledge beyond our skillset or wasn't open-source.

In the spring of 2021, Niko’s best friend Robert (also an astrophysicist) shared “a super cool new way of making apps online called Streamlit🎈.” While transitioning from research into data science, Robert made an incredible app called “Distribution Analyser”. It analyses all scipy.stats functions. You can read more about it in this Towards Data science article. When the time came to test the best way to combine Python code and make it into an app, Streamlit was one of our first choices.

How we worked together in a virtual setting

Since we’re all researchers at different institutions, it was hard for us to find the time to work together. We dedicated a few days during the holidays to exclusively work on CosmΩracle. After we decided on the functions to start with, we distributed the tasks, finalized the first Python code, and discussed how to deploy it.

CosmΩracle calculates the time that passed between a distant object emitting a photon and that same photon reaching Earth. Functions like this form the backbone of CosmΩracle but need to be deployed in order to be useful.

def get_lookback_time(z, H0=constants['Hubble0'], ΩM=constants['matter-density'],
                      ΩDE=constants['DE-density'], ΩR=constants['rad-density'],
                      w0=constants['w0'], wa=constants['wa']):
    """
    Method to compute the lookback time in Gyrs
    """
    integrand = lambda x: 1/(get_E_z(x, ΩM, ΩDE, ΩR, w0, wa)*(1+x))
    if isinstance(z, float) or isinstance(z, int):
        if z < 0:
            raise ValueError("Enter a non-negative redshift.")
        result, _ = integrate.quad(integrand, 0, z)
    elif isinstance(z, np.ndarray):
        if any(t < 0 for t in z):
            raise ValueError("Enter a non-negative redshift.")
        result = np.vectorize(lambda x: integrate.quad(integrand, 0, x)[0])(z)
    else:
        raise TypeError(f'Expected "Union[float, np.ndarray]", got {type(z)}')
    c0 = constants['speed-of-light']
    return result*hubble_time(H0)


That first day we worked until midnight. Too tired to keep going, we decided to talk later. But Niko stayed up and tried Streamlit. It worked! She immediately wrote to Matthijs and Marco, “We need to meet. This is wonderful. This is like magic." So we met on Zoom and continued working into the morning. In a week, we had the first working version of CosmΩracle!

The unofficial fourth member of the gang: coffee. It supported every one of us during the small hours of the morning. Photo by Craig McKay on Unsplash

What worked well is the combination of several important factors:

We had a clear goal in mind
We had a lot of enthusiasm
We were having fun

Also, the crucial part was that our communication was wonderful. We’re friends in real life. That makes everything easier.

Last but definitely not least, the Streamlit platform was essential. It’s very intuitive, powerful, customizable, and very easy to use. As scientists, we’re used to computations but have no experience in deploying web apps. Without Streamlit, we wouldn't have been able to build the momentum to finish this project.

We also used:

Slack for creating tasks/private discussions or sending messages.
GitHub for version control, keeping track of issues that have to be dealt with, and for hosting a GitHub Page that links to the page hosted on Streamlit.
Zoom for in-person discussions.
Python for coding.
Where to go from here?

Thanks to Streamlit, the code implementation was very easy. We have organized the code into blocks dedicated to configuration, computation, or presentation. The result is that CosmΩracle is structured in a way that makes it easy to create extensions.

For example, here is the script that’s at the center of the Streamlit app:

import streamlit as st

import page_introduction as pi
import page_distances as pd
import page_documentation as doc

st.sidebar.write(" ")

pages = {
        "Introduction": pi,
        "Cosmological Distances": pd,
        "Definitions": doc,
    }

st.sidebar.title("Main options")

# Radio buttons to select desired option
page = st.sidebar.radio("", tuple(pages.keys()))

pages[page].show_page()


The parts of CosmΩracle that do all the work (Introduction, Cosmological Distances, and Definitions) are stored in separate scripts. This makes CosmΩracle modular and extensible.

We plan to build a new feature that will let CosmΩracle compute the growth of matter density fluctuations in inflationary cosmology. After we finish the script, adding this new functionality to the app will be a simple matter of adding a line like import page_perturbations to the code above. This will make the corresponding adjustments to the list of pages and will be available on the website.

We also plan to extend the existing features of CosmΩracle.

For example, the current pre-set parameters are based on the latest analysis of the data of the Planck satellite. They’re hardcoded into the app in page_distances.py above. People from different parts of astronomy and cosmology will have different preferences for CosmΩracle’s parameters. We’ll be adding more sets (based on WMAP or the astronomical 70-30-70 cosmology).

Here is a sneak peek for you:

We want to give CosmΩracle more functionality for quantities that are directly calculated from the cosmological distance measures. Currently, it can calculate angular sizes and convert between these angular sizes and physical sizes. But it can only give you an indication of the physical size of an object for a fixed angular extension of 1’’ (1 arcsecond or 1/3600th of a degree). We plan to add an option for you to set the angular size yourself. So if you want to know the real size of those 0.3’’ galaxy lobes, stay tuned!

Wrapping up

With the increasing amount of knowledge and data, we need to develop ways of communicating findings and creating better and more accessible ways to disseminate knowledge. Accessibility and open-source are key to achieving it.

We’re very impressed with how easy it is to use Streamlit. And it’s open-source! For us, to have the code open to the public is extremely valuable because the community can inspect our work, point out our mistakes, and give suggestions for improvements.

We believe that science belongs to everyone. If our app helps the astronomy community with research and teaching, then we’ve achieved a great deal. And all of this was possible because Streamlit is so easy to use.

We hope you enjoyed our story. If you have any questions, please leave them in the comments below or reach out to Niko at nikolina.sarcevic@gmail.com, Matthijs at primarius@gmail.com, and Marco at bonici.marco@gmail.com.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to create interactive books with Streamlit in 5 steps
https://blog.streamlit.io/how-to-create-interactive-books-with-streamlit-and-streamlit-book-in-5-steps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to create interactive books with Streamlit in 5 steps

Use streamlit_book library to create interactive books and presentations

By Sebastian Flores Benner
Posted in Advocate Posts, January 20 2022
Install the library
Create the main file
Create content in Markdown files
Create content in a Python file
Let's make it fun!
Share your app!
Wrapping up
Contents
Share this post
← All posts

I love Streamlit. I moved most of my content, websites, and code to Streamlit to make it more interactive. Recently, I presented at PyCon Chile using Streamlit as a PowerPoint substitute. But as my content grew, handling different "pages" got complicated. I had to copy-paste the page handling code from one repository to another. I wanted a simple solution that any person, especially teachers, could use to create an interactive app for teaching or self-learning. Let people focus on the content and the technology takes care of the rest!

After some thought, I realized that the best way to solve this was to create a companion library for Streamlit. I called it streamlit_book. I coded it to take all of the bookkeeping (pun intended!). I even defined some Markdown formats and Python functions so you can do quizzes more easily: true/false questions, multiple-choice, single-choice, and others. You can put your content on plain Markdown, or you take advantage of the interactivity provided by Streamlit and Python, it's up to you!

In this post, I'll show you how to use the library to create interactive books or presentations with Streamlit:

Install the library
Create the main file
Create content in Markdown files
Create content in Python file
Share your app!

You can take a sneak peek at the app here and see the docs here.

Install the library

Let's build a short tutorial called "Happy Birds". It'll teach you how to win at a game involving flying birds, pigs, and trajectories. As usual, store all the required libraries in your app in a file requirements.txt:

streamlit
streamlit_book
matplotlib
numpy


Install these libraries using pip (or use a different virtual environment):

pip install -r requirements.txt


Create the main file

Create a file happy_birds.py and define the properties:

import streamlit as st
import streamlit_book as stb

# Streamlit webpage properties
st.set_page_config(page_title="HappyBirds", page_icon="🐦")

# Streamlit book properties
stb.set_book_config(path="HappyBirds")

Lines 1 and 2 are regular library imports.
Line 5 sets up the Streamlit app. You can use all the regular Streamlit magic here: set up the layout, the page title (on your browser's tab), and even a small icon.
Line 8 sets up Streamlit Book by indicating a folder where it should look for content files. Make sure to create the folder HappyBirds.

If you run the file happy_birds, it'll show a warning message of having no content files:

streamlit run happy_birds.py


Create content in Markdown files

Create content by adding files into the folder HappyBirds. Notice that Streamlit Book will sort the files using lexicographic (alphabetic) order, so it can be helpful to put numbers before the names to have the desired ordering.

First, create "00 Cover.md." This file will be a cover image for the book, with a big image and some funny text to engage readers:

# Happy Birds

This web app illustrates the use of [streamlit_book](<https://streamlit-book.readthedocs.io/en/latest/>) for teaching and learning. In this particular web app, we will explain motion trajectories.

<img src="<https://github.com/sebastiandres/streamlit_happy_birds/blob/main/images/happybird.png?raw=true>" alt="happy Birds" width="700">

Happy Birds uses streamlit, streamlit book, numpy and matplotlib libraries.


Notice that you need to insert the URL of the image, not the local path.

Next, create "01 The Theory.md". This is the file with the (hopefully not boring) explanations of how projectile motion works. Notice how we put a quiz at the end!

# Projectile Motion

## The question

Considering no air resistance, what is the trajectory followed by a projectile thrown with initial velocity $v_0$ at an angle $\\theta$?

<img src="<https://github.com/sebastiandres/streamlit_happy_birds/blob/main/images/definition.png?raw=true>" alt="Parameter Definition" width="700">

## The short answer

The trajectory followed by a projectile thrown with initial velocity $v_0$ at an angle $\\theta$, without air resistance, is:

$$
x(t) = v_0 \\cos(\\theta)t \\\\\\\\
y(t) = v_0 \\sin(\\theta)t - \\frac{1}{2} g t^{2}
$$

where $x$ and $y$ are the horizontal and vertical directions, and $g$ is the acceleration due to gravity.

## The long answer

To obtain the trajectory we start with the equations for the acceleration as given by Newton's Laws:

$$
m \\frac{d^{2}}{dt^{2}} x = 0 \\\\\\\\
m \\frac{d^{2}}{dt^{2}} y = - mg
$$

Initial condition for the position: $x(t=0)=0$ and $y(t=0)=0$.

Initial condition for the velocity: $v_x(t=0) = \\cos(\\theta)$ and $v_y(t=0) = \\sin(\\theta)$.

After simplifying for the mass $m$, we can solve by integrating and considering the conditions for velocity: 

$$
\\frac{d}{dt}x = v_x(t)= v_0 \\cos(\\theta) \\\\\\\\
\\frac{d}{dt}y = v_y(t)= v_0 \\sin(\\theta) - g t
$$

Integrating again and considering the initial conditions for $x$ and $y$, we obtain:

$$
x(t) = v_0 \\cos(\\theta)t \\\\\\\\
y(t) = v_0 \\sin(\\theta)t - \\frac{1}{2} g t^{2}
$$

## Quiz time

Following the equation above, answer the following question:

stb.single_choice
What is the trajectory of a projectile without considering air resistance?
- A straight line
+ A parabola
- A circle
- A hyperbola


The Streamlit + Streamlit Book app will update automatically, and you'll be able to navigate the created pages.

Create content in a Python file

Now, let's add an interactive page for people so people can try different parameters. You can even use questions from streamlit_book! Call this file "02 The practice.py":

import streamlit as st
import streamlit_book as stb
import numpy as np

from code.trajectory import get_trajectory, fig_from_list

if "trayectory_list" not in st.session_state:
    st.session_state["trayectory_list"] = []

# Title
st.title("Trajectory of a projectile")
st.subheader("Equations of motion of a projectile")
st.latex("x(t) = v_0 \\\\cos(\\\\theta)t")
st.latex("y(t) = v_0 \\\\sin(\\\\theta)t - \\\\frac{1}{2} g t^{2}")

# Parameters
st.subheader("Simulation parameters")
c1, c2, c3 = st.columns(3)
dv0 = 1
v0 = c1.slider("Initial Velocity [meters/second]", 
                        min_value=dv0, max_value=100*dv0, 
                        value=10*dv0, step=dv0, help="Initial velocity for the projectile")
dtheta = 1
theta_deg = c2.slider("Initial Angle [degrees]", 
                        min_value=5, max_value=90, 
                        value=45, step=5, help="Initial velocity for the projectile")
# options for gravity: earth, moon, mars, jupiter
gravity_dict = {'Earth': 9.8, 'Moon': 1.6, 'Mars': 3.7, 'Jupiter': 24.8}
gravity_label = c3.selectbox("Gravity", gravity_dict.keys(), index=0)
gravity = gravity_dict[gravity_label]

# Compute the plot
c1, c2 = st.columns([.5, .1])
if c1.button("Add plot"):
    traj_dict = get_trajectory(v0, theta_deg, gravity, gravity_label)
    st.session_state["trayectory_list"].append(traj_dict)

if c2.button("Clear plots"):
    st.session_state["trayectory_list"] = []

if len(st.session_state["trayectory_list"]) > 0:
    fig = fig_from_list(st.session_state["trayectory_list"])
    st.pyplot(fig)

# The quizz
st.subheader("Quizz time!")

stb.single_choice("At what angle is obtained the maximal distance?",
                options=["15", "30", "45", "60", "75"], answer_index=2)

stb.true_or_false("On the moon, the horizontal distance is always larger than on the earth under the same initial velocity and angle.",
                    answer=True)


This makes use of some helper functions in code/trajectories.py. Everything is on the GitHub repo.

Let's make it fun!

We can even make a game out of it, to further test people's understanding of the motion equations.

Let's create a "03 The game.py" file with the content.

import streamlit as st
import numpy as np

from code.trajectory import get_trajectory, fig_from_list, check_solution

# Fill up the page
c1, c2 = st.columns([8,1])
c1.title("The Game")
restart = c2.button("Restart")

# Gravity constants by planet
GRAVITY_DICT = {'Earth': 9.8, 'Moon': 1.6, 'Mars': 3.7, 'Jupiter': 24.8}

# Setup the session_state variables
if restart or "remaining_guesses" not in st.session_state:
    st.session_state["remaining_guesses"] = 3

if restart or"guess_list" not in st.session_state:
    st.session_state["guess_list"] = []

if restart or"game_gravity_index" not in st.session_state:
    st.session_state["game_gravity_index"] = np.random.randint(0, len(GRAVITY_DICT))
planet_list = list(GRAVITY_DICT.keys())
game_planet = planet_list[st.session_state["game_gravity_index"]]
game_gravity = GRAVITY_DICT[game_planet]

if restart or "solution" not in st.session_state:
    v0_sol = np.random.randint(30, 60)
    theta_deg_sol = 45
    theta_rad_sol = theta_deg_sol * np.pi / 180
    t_max_sol = 2*v0_sol*np.sin(theta_rad_sol)/game_gravity
    x_max_sol = v0_sol*np.cos(theta_rad_sol)*t_max_sol
    pig_position = [x_max_sol, 0]
    st.session_state["solution"] = {
                                    "pig_position":pig_position, 
                                    "v0_sol": v0_sol, 
                                    "theta_deg_sol": theta_deg_sol,
                                    }

article_dict = {'Earth': "", 'Moon': "the", 'Mars': "", 'Jupiter': ""}
c1.subheader(f"Can you hit the target on {article_dict[game_planet]} {game_planet}?")

# Pig position
x_text = f"x = {st.session_state.solution['pig_position'][0]:.3f} meters"
y_text = f"y = {st.session_state.solution['pig_position'][1]:.3f} meters"
st.write(f"The target is at **{x_text}** and **{y_text}**")
# Get the parameters
st.subheader("Enter the parameters")
c1, c2, c3, c4 = st.columns([3,3,3,1])
dv0 = 1
v0 = c1.slider("Initial Velocity [meters/second]", 
                        min_value=dv0, max_value=100*dv0, 
                        value=50, step=dv0, help="Initial velocity for the projectile")
dtheta = 1
theta_deg = c2.slider("Initial Angle [degrees]", 
                        min_value=5, max_value=90, 
                        value=30, step=5, help="Initial velocity for the projectile")
# options for gravity: earth, moon, mars, jupiter
c3.metric(value=game_gravity, label=f"{game_planet}'s gravity in m/s^2")

# Shoooooot
if st.session_state["remaining_guesses"] > 0:
    if c4.button("Shoot!"):
        st.session_state["remaining_guesses"] -= 1
        traj_dict = get_trajectory(v0, theta_deg, game_gravity, game_planet)
        st.session_state["guess_list"].append(traj_dict)

# Placeholder for information
placeholder = st.empty()

# Always plot, to show the target
fig = fig_from_list(st.session_state["guess_list"], st.session_state.solution["pig_position"])
st.pyplot(fig)

# We check if we hit the pig after the shoot we have guesses left
if check_solution(st.session_state.solution["pig_position"], st.session_state["guess_list"]):
    placeholder.success("You hit the pig... I mean, the target!")
elif st.session_state["remaining_guesses"] == 0:
    line1 = "You're out of guesses! :("
    v0_sol = st.session_state.solution["v0_sol"]
    theta_deg_sol = st.session_state.solution["theta_deg_sol"]
    line2 = f"One possible solution was $v_0$={v0_sol} [m/s^2] and $\\\\theta$={theta_deg_sol} [deg]"
    placeholder.error(line1 + line2)
else:
    # Say to keep trying, but only if at least tried once
    if st.session_state['remaining_guesses']==2:
        text = f"Keep trying! You have {st.session_state['remaining_guesses']} guesses remaining. Have you tried solving the equations?"
        placeholder.warning(text)
    if st.session_state['remaining_guesses']==1:
        text = f"Use carefully the last guess!"
        placeholder.warning(text)


Share your app!

Finally, you can share your app with the world (and your students!). It's as easy as sharing any Streamlit app, because streamlit_book is just another required library.

Wrapping up

I had a lot of fun creating the streamlit_book library. I hope you'll use it to create awesome books, courses, or presentations, and extend the ideas we started on the happy birds app. I'll keep updating the library and adding new features. You can check the documentation here and the source code here. If you create an app or want a new feature, reach out to me (in Spanish, English, or French)! Find me as @sebastiandres on Twitter and GitHub, or comment below!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > October 2021
https://blog.streamlit.io/monthly-rewind-october-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > October 2021

Your October look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, November 8 2021
🏆 App of the month 🏆
Streamlit October updates
🔍 Current release: 1.1.0
🔮 Upcoming
🎈 Streamlit 1.0 release
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our October featured app of the month is... 🥁🥁🥁🥁🥁

Streamlit-geospatial by Qiusheng Wu!

This multi-page web app demonstrates various interactive geospatial applications created with open-source mapping libraries such as leafmap, geemap, pydeck, and kepler.gl. Create timelapses, upload vector data, search basemaps, and more! This is an open-source project. [code].

Streamlit October updates

Let's take a look back at all that happened during the month of October!

🔍 Current release: 1.1.0

The latest release is 1.1.0. Recent updates include memory improvements and semantic versioning. See the memory usage over time of one of our internal apps below. The lines on the left are using older Streamlit versions and the line on the right is using 1.1.0!

Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming

Some new upcoming features and updates to get excited about:

Webcam input
Better "Argh" error page
Favorite an app

Check out our roadmap app to see what else lies ahead!

🎈 Streamlit 1.0 release

Streamlit 1.0 was officially released. 🥳 Thank you community for all the feedback and pull requests which made reaching this milestone possible!



Featured Streamlit content


We launched a brand-new docs site! Easily navigate between API references (now also more discoverable in searches), Cloud deployment, tutorials, and other resources.

In Detecting parking spots with Streamlit, Jeffrey described how to build an app that can detect parking spaces from a livestream with OpenCV and Mask R-CNN.


Featured community content

Some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.


Zakaria's Clean Speech Audio with IBM Watson STT and Streamlit App lets you clean up your Audio recordings by removing unwanted spaces and words
Qiusheng incorporated leafmap apps to his open-source project
In his video tutorial, PartTimeLarry showed how to use Streamlit to build a simple UI on top of OpenSea NFT API
Akshanh made a Streamlit Playground app where you can learn, experiment, and play with Streamlit components on the fly
In the video Quantum Tic Tac Toe with IBM Qiskit and Streamlit, Jay showed how to build a Quantum version of the classic Tic Tac Toe game
Loren created Dashboard Evolution Covid19 en France to show different views of the evolution of the Covid-19 in France
José's STL files from molecules app returns molecular models in STL format for 3D printing
Sean created xPlot, a data explorer to plot data from files for analysis
Find rankings, result types, and ranking URLs for a bulk list of keywords in Michael's Keyword rankings app
Aditya made a Triple Combo Plotter app to plot LAS file data
You can quickly clean up your keywords lists with Alexis' SEO Keywords Spelling Checker

Thanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

3 steps to fix app memory leaks
https://blog.streamlit.io/3-steps-to-fix-app-memory-leaks/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
3 steps to fix app memory leaks

How to detect if your Streamlit app leaks memory and identify faulty code

By George Merticariu
Posted in Tutorials, April 14 2022
1. Identify the memory leak
2. Identify leaking objects
3. Identify the code that's allocating the leaking objects
Wrapping up
Contents
Share this post
← All posts

Does your Streamlit app crash after long use or constantly runs out of memory? Chances are, it has a memory leak.

In this post, you’ll learn how to find and fix memory leaks in three simple steps:

Identify the memory leak
Identify the leaking objects
Identify the code that's allocating the leaking objects

NOTE: Streamlit is used in a variety of settings, from short-lived research projects to live company dashboards. This post is primarily aimed at developers deploying Streamlit apps with very long uptimes. We at Streamlit strive to keep the Streamlit framework itself free of leaks. Developers with long-running apps also need to be mindful of memory usage over time. Even a tiny memory leak can compound. Eventually, even the beefiest machine will fall over if memory usage isn’t kept in check.

Let’s dive right in.

1. Identify the memory leak

A leak happens when your app acquires memory resources and never releases them. It just consumes more and more memory. Since it's a finite resource, eventually it gets exhausted, and the OS terminates your application.

As an example, we'll use a Streamlit library memory leak that we investigated and patched as part of this pull request. After we found the faulty code and released a fix, apps started using one-tenth to one-twentieth of the memory used before!

To find out if your app leaks memory, use a memory profiler (we used mprof ):

# make sure mprof is installed
pip install memory_profiler

# find the pid of the Streamlit app (the next number after the user)
ps aux | grep "streamlit run" | grep -v grep

# start profiling the memory of the app
mprof run --attach <pid>


Once the profiler is attached, use your app to simulate a higher load (multiple sessions, more complex operations, etc.). After a few minutes, plot a memory graph:

mprof plot

# if the above gives you an error related to GUI you can try fixing it with
pip install PyQt5


If the memory usage doesn't plateau, your app is leaking memory:

2. Identify leaking objects

Now identify which objects are allocated and never released. Use the tracemalloc Python library. Take snapshots between executions (after forcing the garbage collection).

If an object persists in a snapshot, it means it's never collected:

import tracemalloc, json
import streamlit as st
import gc

@st.experimental_singleton
def init_tracking_object():
  tracemalloc.start(10)

  return {
    "runs": 0,
    "tracebacks": {}
  }


_TRACES = init_tracking_object()

def traceback_exclude_filter(patterns, tracebackList):
    """
    Returns False if any provided pattern exists in the filename of the traceback,
    Returns True otherwise.
    """
    for t in tracebackList:
        for p in patterns:
            if p in t.filename:
                return False
        return True


def traceback_include_filter(patterns, tracebackList):
    """
    Returns True if any provided pattern exists in the filename of the traceback,
    Returns False otherwise.
    """
    for t in tracebackList:
        for p in patterns:
            if p in t.filename:
                return True
    return False


def check_for_leaks(diff):
    """
    Checks if the same traceback appears consistently after multiple runs.

    diff - The object returned by tracemalloc#snapshot.compare_to
    """
    _TRACES["runs"] = _TRACES["runs"] + 1
    tracebacks = set()

    for sd in diff:
        for t in sd.traceback:
            tracebacks.add(t)

    if "tracebacks" not in _TRACES or len(_TRACES["tracebacks"]) == 0:
        for t in tracebacks:
            _TRACES["tracebacks"][t] = 1
    else:
        oldTracebacks = _TRACES["tracebacks"].keys()
        intersection = tracebacks.intersection(oldTracebacks)
        evictions = set()
        for t in _TRACES["tracebacks"]:
            if t not in intersection:
                evictions.add(t)
            else:
                _TRACES["tracebacks"][t] = _TRACES["tracebacks"][t] + 1

        for t in evictions:
            del _TRACES["tracebacks"][t]

    if _TRACES["runs"] > 1:
        st.write(f'After {_TRACES["runs"]} runs the following traces were collected.')
        prettyPrint = {}
        for t in _TRACES["tracebacks"]:
            prettyPrint[str(t)] = _TRACES["tracebacks"][t]
        st.write(json.dumps(prettyPrint, sort_keys=True, indent=4))


def compare_snapshots():
    """
    Compares two consecutive snapshots and tracks if the same traceback can be found
    in the diff. If a traceback consistently appears during runs, it's a good indicator
    for a memory leak.
    """
    snapshot = tracemalloc.take_snapshot()
    if "snapshot" in _TRACES:
        diff = snapshot.compare_to(_TRACES["snapshot"], "lineno")
        diff = [d for d in diff if
                d.count_diff > 0 and traceback_exclude_filter(["tornado"], d.traceback)
                and traceback_include_filter(["streamlit"], d.traceback)
                ]
        check_for_leaks(diff)

    _TRACES["snapshot"] = snapshot


gc.collect()
compare_snapshots()


NOTE: Call compare_snapshots() always after the gc collection is forced, to make sure you track only the objects for which memory can’t be reclaimed.

Run the above in the Streamlit app, version lower than 1.0, and you'll get the following output:


    "<attrs generated init streamlit.state.session_state.SessionState>:17": 22,


SessionState is the object that leaked.

Now let's identify the part of the code that allocates the SessionState and never releases it.

3. Identify the code that's allocating the leaking objects

Track which object is not releasing the memory using the objgraph library. To install it, run pip install objgraph. Track the holder of the SessionState after the gc collection is forced:

import gc
import objgraph

for o in gc.get_objects():
    if 'session_state.SessionState' in str(type(o)) and o is not st.session_state:
        filename = f'/tmp/session_state_{hex(id(o))}.png'
        objgraph.show_chain(
            objgraph.find_backref_chain(
                 o,
                 objgraph.is_proper_module),
            backrefs=False,
            filename=filename)

        st.write("SessionState reference retained by: ", type(o))
        st.image(filename)


In our case, when we run on Streamlit versions below 1.0.0, you can see that the SessionState object is held by the Signal class:

The allocation of SessionState happens in the streamlit.config module. The object holding the resource is Signal. It holds the resources that need to be released (check out our pull request).

After we applied the fix and updated the app, the memory usage plateaued:

Wrapping up

Now you know how to detect if your Streamlit app is leaking memory and how to fix it! If you have any questions, please let us know in the comments below or on the forum. We'd be happy to help! ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Counties_v2.gif (1916×945)
https://blog.streamlit.io/content/images/2022/08/Counties_v2.gif#browser


Streamlit-testapp--1-.gif (3446×2038)
https://blog.streamlit.io/content/images/2022/11/Streamlit-testapp--1-.gif#browser


extras-1.gif (800×668)
https://blog.streamlit.io/content/images/2022/11/extras-1.gif


multipageapps--3--1.gif (1034×552)
https://blog.streamlit.io/content/images/2022/06/multipageapps--3--1.gif#browser


Roadmap-hero.png (2000×945)
https://blog.streamlit.io/content/images/2022/11/Roadmap-hero.png


Creating satellite timelapse with Streamlit and Earth Engine
https://blog.streamlit.io/creating-satellite-timelapse-with-streamlit-and-earth-engine/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Creating satellite timelapse with Streamlit and Earth Engine

How to create a satellite timelapse for any location around the globe in 60 seconds

By Qiusheng Wu
Posted in Advocate Posts, December 15 2021
How to create a satellite timelapse without coding
1. Draw a Region of Interest (ROI) on the map
2. Upload a GeoJSON file to the web app
3. Select a satellite image collection
4. Select a band combination
5. Select an administrative boundary
6. Customize timelapse parameters
7. Download your timelapse in GIF and MP4 formats
How to deploy an Earth Engine app to Streamlit Cloud
1. Fork the streamlit-geospatial repo
2. Sign up for an Earth Engine account
3. Install GeoPandas, geemap, and Streamlit
4. Get an Earth Engine token
5. Deploy your app to Streamlit Cloud
Wrapping up
Contents
Share this post
← All posts

Want to create a satellite timelapse for any location around the globe to see how the Earth has changed? You've come to the right place. I present to you...

An interactive web app for creating satellite timelapse without coding!

In this post, you'll learn how to create a satellite timelapse for any location and to deploy an Earth Engine app to Streamlit Cloud:

How to create a satellite timelapse without coding:

Draw a Region of Interest (ROI) on the map
Upload a GeoJSON file to the web app
Select a satellite image collection
Select a band combination
Select an administrative boundary
Customize timelapse parameters
Download your timelapse in GIF and MP4 formats

How to deploy an Earth Engine app to Streamlit Cloud:

Fork the streamlit-geospatial repo
Sign up for an Earth Engine account
Install GeoPandas, geemap, and Streamlit
Get an Earth Engine token
Deploy your app to Streamlit Cloud

Want to jump right in? Here's the web app and repo code.

Let's get started!

How to create a satellite timelapse without coding
1. Draw a Region of Interest (ROI) on the map

First, navigate to the web app at https://streamlit.gishub.org. Once the app opens in the web browser, click "Create Timelapse" on the left sidebar menu. You should see a map on the left and a list of options on the right. Pan and zoom the map to find your Region of Interest (ROI).

Next, click the rectangle tool to draw a rectangle on the map:

Draw a Region of Interest (ROI) on the map and export the ROI as a GeoJSON

Due to the limitation of the folium Python package, there is no way to retrieve the coordinates of drawn shapes on the map. So you'll need to export the ROI manually and upload it back to the web app for subsequent steps.

Click the "Export" button in the upper-right corner of the map to export the ROI as a GeoJSON file to your computer. The GeoJSON file is a plain text file containing the coordinates of the geometries drawn on the map. Use any Text Editor to open and inspect the file.

2. Upload a GeoJSON file to the web app

Click "Browser files" to locate the GeoJSON file exported in the previous step and upload it to the web app. Or you can use other existing GeoJSON. Once you click the "Open" button on the open file dialog, the selected GeoJSON file should be uploaded to the app momentarily with the file name listed under "Browsers files":

Upload the exported GeoJSON back to the web app
3. Select a satellite image collection

Select "Landsat TM-ETM-OLI" from the dropdown list:

Select a satellite image collection to create timelapse

I chose Landsat program because it's the longest-running enterprise for acquiring satellite imagery of the Earth since 1972. The most recent satellite, Landsat 9, was successfully launched on 27 September 2021. The "Landsat TM-ETM-OLI" image collection contains all available Landsat imagery acquired since 1984.

If you want to choose a different image collection from the Earth Engine Data Catalog, select "Any Earth Engine ImageCollection."

4. Select a band combination

Landsat imagery comes with multi-spectral bands. Here are the common spectral bands produced by Landsat TM, ETM+, and OLI sensors:

Spectral bands of Landsat satellite imagery

To display and visualize an image on a computer screen, decide which three spectral bands you want to use for the Red, Green, and Blue (RGB) channels. Some of the commonly used band combinations include Natural Color (Red/Green/Blue), Color Infrared (NIR/Red/Green), Short-Wave Infrared (SWIR2/SWIR1/Red), Agriculture (SWIR1/NIR/Blue), and Geology (SWIR2/SWIR1/Blue). Read more about Landsat band combinations here and here.

Select a band combination from the dropdown list. Or just enter the title you want to see on the resulting Landsat timelapse:

Select an RGB band combination
5. Select an administrative boundary

If your timelapse covers a large region, overlay it with an administrative boundary. Select it from the built-in datasets (e.g., Continents, Countries, US States) or select "User-defined," then enter an HTTP URL for the GeoJSON file:

You can customize the boundary's color, line width, and opacity.

6. Customize timelapse parameters

The "Frames per second" parameter controls the speed of the timelapse. The smaller the number, the slower the timelapse, and vice versa.

For example, a timelapse of annual Landsat imagery (1984-2021) has 38 frames. At 5 frames per second, the timelapse would last 7.6 seconds. By default, the app uses all available imagery to create the composite. You can change the start year, the end year, the start month, and the end month if you want a specific time range.

Since Landsat satellites carry optical sensors, you might see clouds in Landsat imagery (especially in the tropics). By default, the app applies the fmask algorithm to remove clouds, shadows, and snow. This can create black spots (nodata) in your timelapse. You can also change the font type, size, and color to customize the animated timestamps.

When finished with all parameters, click "Submit":

Under the hood, the app will collect the parameters and pass them to the landsat_timelapse function. It will utilize the GEE cloud computing platform to process the satellite imagery and generate the timelapse. The initial timelapse generated by Earth Engine will be further processed by geemap to add a progress bar, animated timestamps, etc.

7. Download your timelapse in GIF and MP4 formats

You can now download your timelapse as a GIF or MP4 animation in 60 seconds. 🚀 Right-click the image/video to save it, then share it with your friends, family, or on social media! 😇

Here are a few timelapse examples:

River dynamics of Ucayali River, Peru - Created using Landsat imagery (SWIR1/NIR/Red)

Vegetation dynamics in Africa - created using monthly MODIS NDVI data

Northeast Pacific bomb cyclone in October 2021 - Created using GOES-17

Creek Fire in California in September 2020 - Created using GOES-17

Temperature dynamics at the global scale - Created using MODIS Land Surface Temperature

Want more examples? Use hashtags #geemap and #streamlit to search on Twitter and LinkedIn.

How to deploy an Earth Engine app to Streamlit Cloud
1. Fork the streamlit-geospatial repo

Fork the streamlit-geospatial repo to your GitHub account. It contains the source code (>1000 lines) of the multi-page web app for various geospatial applications. You can find it here.

2. Sign up for an Earth Engine account

Sign up for a Google Earth Engine account. Once you get the email, log in to the Earth Engine Code Editor to verify that your account has been authorized to use Earth Engine.

3. Install GeoPandas, geemap, and Streamlit

Install GeoPandas, geemap, and Streamlit Python packages. If you have Anaconda or Miniconda installed on your computer, you can create a fresh conda environment to install the required packages using the following commands:

conda create -n gee python=3.8
conda activate gee
conda install geopandas
pip install geemap

4. Get an Earth Engine token

Type python into the terminal and press "Enter" to get into the Python interactive shell. Then type import ee and ee.Authenticate() and press "Enter":

import ee
ee.Authenticate()

Authenticate Google Earth Engine

Log in to your Google account to obtain the authorization code and paste it back into the terminal. Once you press "Enter," an Earth Engine authorization token will be saved to your computer under the following file path (depending on your operating system):

Windows: C:\\Users\\USERNAME\\.config\\earthengine\\credentials
Linux: /home/USERNAME/.config/earthengine/credentials
MacOS: /Users/USERNAME/.config/earthengine/credentials


Navigate to the above file path and open the credentials file using a Text Editor. Copy the token wrapped within the double quotes to the clipboard:

The Earth Engine token
5. Deploy your app to Streamlit Cloud

To deploy your app, click the “New app” button in the upper-right corner of your Streamlit workspace at https://share.streamlit.io, then fill in your repo path (e.g., USERNAME/streamlit-geospatial), branch (e.g., master), and main file path (e.g., app.py). Go to App settings - Secrets and set EARTHENGINE_TOKEN as an environment variable for your web app. Click "Save":

Depending on the number of dependencies specified in requirements.txt, the app might take a few minutes to install and deploy. Once deployed, you'll see its URL in your Streamlit Cloud workspace. It'll follow a standard structure based on your GitHub repo, such as:

<https://share.streamlit.io/>[user name]/[repo name]/[branch name]/[app path]


For example, here is the long app URL: https://share.streamlit.io/giswqs/streamlit-geospatial/app.py. And here is the short app URL: https://streamlit.gishub.org

Wrapping up

Congratulations! 👏 You did it! You have built an interactive web app for creating a satellite timelapse. You're welcome to contribute your comments, questions, resources, and apps as issues or pull requests to the streamlit-geospatial repo.

Want to learn more about GEE and geemap? Check out https://geemap.org , my article, and my YouTube channel for video tutorials. Or get in touch with me on Twitter or LinkedIn.

Thanks for reading, and happy coding! 🍺

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly Rewind - Streamlit (Page 3)
https://blog.streamlit.io/tag/monthly-rewind/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Monthly Rewind
27 posts
Monthly rewind > July 2021

Your July look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 5 2021
Monthly rewind > June 2021

Your June look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 5 2021
Monthly rewind > May 2021

Your May look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2021
Monthly rewind > April 2021

Your April look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2021
Monthly rewind > March 2021

Your March look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 5 2021
Monthly rewind > February 2021

Your February look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 8 2021
Monthly rewind > January 2021

Your January look back at new features and great community content

Monthly Rewind
by
TC Ricks
,
February 8 2021
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

satschool-static-1.png (2000×1249)
https://blog.streamlit.io/content/images/2022/06/satschool-static-1.png#browser


ampredst-app.gif (960×519)
https://blog.streamlit.io/content/images/2022/12/ampredst-app.gif#browser


matplotlib_gif-1.gif (932×772)
https://blog.streamlit.io/content/images/2022/06/matplotlib_gif-1.gif#shadow


ifood.jpeg (2000×946)
https://blog.streamlit.io/content/images/2022/06/ifood.jpeg#shadow


typing-playground-1.gif (1680×818)
https://blog.streamlit.io/content/images/2022/11/typing-playground-1.gif#browser


ampredst-app.gif (960×519)
https://blog.streamlit.io/content/images/2023/01/ampredst-app.gif#browser


not-active-amp.png (672×139)
https://blog.streamlit.io/content/images/2022/12/not-active-amp.png#border


active-amp.png (666×135)
https://blog.streamlit.io/content/images/2022/12/active-amp.png#border


molecular-descriptors.png (1244×172)
https://blog.streamlit.io/content/images/2022/12/molecular-descriptors.png#border


aminoacid-composition.png (859×528)
https://blog.streamlit.io/content/images/2022/12/aminoacid-composition.png#border


ampredst-app-input-peptide.png (1476×384)
https://blog.streamlit.io/content/images/2022/12/ampredst-app-input-peptide.png#border


ampredst-app-2.png (1482×343)
https://blog.streamlit.io/content/images/2022/12/ampredst-app-2.png#border


ampredst-app-1.png (1486×392)
https://blog.streamlit.io/content/images/2022/12/ampredst-app-1.png#border


attribute-error.png (2000×571)
https://blog.streamlit.io/content/images/2022/11/attribute-error.png#border


subtle-defect.png (1926×644)
https://blog.streamlit.io/content/images/2022/11/subtle-defect.png#border


4.1-2.png (1200×848)
https://blog.streamlit.io/content/images/2021/08/4.1-2.png#browser


3-8.png (1200×863)
https://blog.streamlit.io/content/images/2021/08/3-8.png#border


2-11.png (1200×724)
https://blog.streamlit.io/content/images/2021/08/2-11.png#border


1-10.png (1200×424)
https://blog.streamlit.io/content/images/2021/08/1-10.png#border


Streamlit-authenticator-part1-social.png (1200×675)
https://blog.streamlit.io/content/images/2023/01/Streamlit-authenticator-part1-social.png


Spotify-playlists-1.png (2000×1251)
https://blog.streamlit.io/content/images/2023/01/Spotify-playlists-1.png#browser


social-share-preview.png (2000×1597)
https://blog.streamlit.io/content/images/2023/01/social-share-preview.png


1.2.gif (594×445)
https://blog.streamlit.io/content/images/2022/08/1.2.gif#browser


3.gif (800×900)
https://blog.streamlit.io/content/images/2022/08/3.gif#browser


Untitled--3--1.png (1682×1518)
https://blog.streamlit.io/content/images/2022/08/Untitled--3--1.png#browser


Untitled--2-.png (779×806)
https://blog.streamlit.io/content/images/2022/08/Untitled--2-.png


2.gif (800×900)
https://blog.streamlit.io/content/images/2022/08/2.gif#browser


Untitled--1-.png (603×604)
https://blog.streamlit.io/content/images/2022/08/Untitled--1-.png


Untitled-1.png (1105×592)
https://blog.streamlit.io/content/images/2022/08/Untitled-1.png


Streamlit (Page 7)
https://blog.streamlit.io/page/7/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Chat with the Cat Generative Dialogue Processor (CatGDP)

Build your own catbot with a quirky persona!

Advocate Posts
by
Tianyi Pan
,
May 3 2023
Introducing st.connection!

Quickly and easily connect your app to data and APIs

Product
by
Joshua Carroll and 
1
 more,
May 2 2023
The ultimate athlete management dashboard for biomechanics

Learn how to measure jump impulse, max force, and asymmetry with Python and Streamlit

Advocate Posts
by
Hansen Lu
,
April 27 2023
Creating a Time Zone Converter with Streamlit

6 steps on how to build your own converter

Advocate Posts
by
Vinícius Oviedo
,
April 25 2023
Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Create an animated data story with ipyvizzu and Streamlit

A tutorial on using ipyvizzu and ipyvizzu-story

Advocate Posts
by
Peter Vidos
,
April 20 2023
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

Advocate Posts
by
Dmitry Kosarevsky
,
April 18 2023
Introducing a chemical molecule component for your Streamlit apps

Integrate a fully featured molecule editor with just a few lines of code!

Product
by
Michał Nowotka
,
April 13 2023
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

Advocate Posts
by
Kanak Mittal
,
April 11 2023
Building GPT Lab with Streamlit

12 lessons learned along the way

LLMs
by
Dave Lin
,
April 6 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/6/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
How to diagnose blood cancer with Streamlit

Build a molecular pathology diagnostics app in 4 simple steps

Advocate Posts
by
Eitan Halper-Stromberg
,
January 25 2022
How to create interactive books with Streamlit in 5 steps

Use streamlit_book library to create interactive books and presentations

Advocate Posts
by
Sebastian Flores Benner
,
January 20 2022
Creating satellite timelapse with Streamlit and Earth Engine

How to create a satellite timelapse for any location around the globe in 60 seconds

Advocate Posts
by
Qiusheng Wu
,
December 15 2021
Finding your look-alikes with semantic search

How Pinecone used Streamlit to create a Hacker News Doppelgänger app

Advocate Posts
by
Greg Kogan
,
December 1 2021
Forecasting with Streamlit Prophet

How Artefact built a Streamlit app to train time-series forecasting models

Advocate Posts
by
Maxime Lutel
,
November 10 2021
Detecting parking spots with Streamlit

How to build a Streamlit parking spot app in 8 simple steps

Advocate Posts
by
Jeffrey Jex
,
October 26 2021
Deploying a cloud-native Coiled app

How Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users

Advocate Posts
by
Richard Pelgrim
,
September 7 2021
Labeling ad videos with Streamlit

How Wavo.me uses Streamlit’s Session State to create labeling tasks

Advocate Posts
by
Anastasia Glushko
,
September 2 2021
Easy monitoring of dbt Cloud jobs with Streamlit

How the Cazoo data science team built their dbt Cloud + Streamlit app

Advocate Posts
by
Martin Campbell
,
June 11 2021
Making apps for the Rasa research team (and open source community!)

Helping Rasa users understand their models

Advocate Posts
by
Vincent D. Warmerdam
,
May 12 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > December 2021
https://blog.streamlit.io/monthly-rewind-december-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > December 2021

Your December look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, January 7 2022
🏆 App of the month 🏆
Streamlit December updates
🔍 Current release: 1.3.0
🔮 Upcoming
☁️ Streamlit Cloud free tier expansion
🔒 SOC 2 Certification
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our December featured app of the month is... 🥁🥁🥁🥁🥁

CosmΩracle by Niko, Marco and Matthijs!

CosmΩracle is an app that calculates distances in cosmology. It was created to be a useful and convenient computing aid for people who have a need to know the structure of the universe. Calculate anywhere, anytime! [code].

Streamlit December updates

Let's take a look back at all that went on during December.

🔍 Current release: 1.3.0

The latest release is 1.3.0. Recent updates include further support for PyDeck and Plotly charts, and st.caption supporting HTML in text with unsafe_allow_html parameter. Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming

Upcoming new features and updates to get excited about:

Sign in with email (coming VERY soon 👀)
Camera integration
In-app share feature

Check out our roadmap app to get a scope of what else we're working on. 🥳

☁️ Streamlit Cloud free tier expansion
The free tier of Streamlit Cloud got some major updates! You can now deploy unlimited public apps and a private app for free.

🔒 SOC 2 Certification
We're committed to meeting industry standards and are now SOC 2 Type 1 certified. Read more about securely sharing your apps using Streamlit Cloud.


Featured Streamlit content
Learn how to create a satellite timelapse for any location in the world and deploy your own Earth Engine Streamlit app in this blog post by Qiusheng!


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Explore civic data in Jenna's City of Boston: CityScore app.
Learn How to Embed Tweets on Streamlit Web Application with Avra's tutorial.
Vivek's Face Detection and Analysis app uses DeepFace and OpenCV to determine age, gender, and emotion.
In this live coding, Nicholas shows how to Build a TikTok Data Science App with Streamlit and Python.
Pair up Pokémon and see who would win in Marshall's Pokemon Battle Simulator.
Calculate elite qualifying miles and dollars with King's Air Canada Aeroplan miles and dollars calculator.
Sven's video tutorial teaches you how to Add a snowflake animation to your Streamlit web app.
In her article Streamlit Hands-On: Features and Tips For Enhanced App User Experience, Sharone guides you through the code of a real-world use case.
Javier's Portfolio Optimization Tool lets you apply the Efficient Frontier implementation using MonteCarlo simulations to define and optimize two portfolio examples.
In the Summarizing my favorite podcasts with Python tutorial by AssemblyAI, you can learn how to build an app that summarizes podcasts to text by chapter.
Learn how to make an ML Hyperparameter Optimization App using Streamlit in Nandan's step-by-step tutorial.
Analyze and visualize your tweets from the last year in Roberto's app Tu año en Twitter.
Another tutorial from Sven shows you how to Build a Website in only 12 minutes using Python & Streamlit.
Yash and Yash created a new Streamlit-Chat component for a chat-bot UI.

Thanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tijana Nikolic - Streamlit
https://blog.streamlit.io/author/tijana/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Tijana Nikolic
1 post
Sogeti creates an educational Streamlit app for data preprocessing

Learn how to use Sogeti’s Data Quality Wrapper

Advocate Posts
by
Tijana Nikolic
,
March 8 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > September 2021
https://blog.streamlit.io/monthly-rewind-september-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > September 2021

Your September look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, October 7 2021
🏆 App of the month 🏆
Streamlit September updates
🔍 Current release: 1.0.0
🔮 Upcoming features
🎮 Discord
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our September featured app of the month is......🥁🥁🥁🥁🥁

Streamlit Prophet by Maxime Lutel.

This app helps data scientists to visually train, evaluate and optimize a Prophet forecasting model. [code]

Streamlit September updates

Let's take a look back at all that happened during the month of September!

🔍 Current release: 1.0.0

The latest release is 1.0.0. Recently added features from this month include configurable hamburger menu and experimental cache primitives. Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming features

Some new features coming soon:

Text dials
Support for cards

Check out our roadmap app for more on what's on the horizon!

🎮 Discord

We now have a community Discord. This new space allows you to meet and have real time chat with others in the community. Connect with peers in your industry, with similar topic interests, and that speak the same language! Read more here.  

Featured Streamlit content

The team at Wavo discussed how their creative labeling app provides them with data-driven insights to build better music marketing campaigns.

Coiled showed how to make a Streamlit-on-Coiled app to present multi-GBs of data seamlessly.

Johannes wrote a great blog post with helpful tips on how to make your resource hungry apps more efficient.

Release 0.89.0 brought new experimental cache primitives that help you cache your data up to 10x faster.

Our release notes expanded into blog posts! Get a more detailed insight on each release. This month included. 0.88.0 and 0.89.0


Featured community content

Some great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.


Jesse's YouTube tutorial showed how to perform File Downloads with Streamlit Download Button
Data Professor chatted with Tyler about his new book on his podcast Data Science Podcast with Tyler Richards (Facebook Data Scientist)
Mohammad launched DummyLearn, a free online machine learning platform
Nelson created an app called explainMyModel that explains different models and their feature contributions
TeFIRE Calculator by Jimmy let you play with crypto projections and monthly income
Spidy showed how to make a vaccination center and availability checker app in his video Py-Cowin | Find Vaccination Center using Python Streamlit
Pavan came out with another great CV app that lets you convert a Video to GIF
Marshall's app visualized US State Migration data from 2019
Data Professor answered the question Is it Possible to Add a Navigation Bar to Streamlit Apps? in his video tutorial
Teddy created a fun PvP Gomoku game app
Tunahan wrote a tutorial Control the Flow: Streamlit showing how to build an app and use session state
Harshit came out with a popular video tutorial Building a buzzing stocks news feed using NLP and Streamlit | Named Entity Recognition & Linking
Avra's SciLit app let you search through scientific publications based on keywords
Lucas and his team created a clustering strategy app iRaPCA clustering, with downloadable validation metrics and graphs

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > November 2021
https://blog.streamlit.io/monthly-rewind-november-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > November 2021

Your November look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, December 7 2021
🏆 App of the month 🏆
Streamlit November updates
🔍 Current release: 1.2.0
🔮 Upcoming
☁️ Streamlit Cloud
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our November featured app of the month is... 🥁🥁🥁🥁🥁

Streamlit as a PowerPoint replacement by Sebastian Flores!

Sebastian created a Streamlit app to give a presentation about Streamlit at PyCon Chile! The app uses slide pagination and goes over features of Streamlit, providing examples and code. [code].

Streamlit November updates

Let's take a look back at all that's happened during November!

🔍 Current release: 1.2.0

The latest release is 1.2.0. Recent updates include the ability to set custom placeholder text and to resize the input box in st.text_area. Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming

Some new upcoming features and updates to get excited about:

Favorite an app
Sign in with an email
Custom subdomains

Check out our roadmap app to see what else we're working on!

☁️ Streamlit Cloud
Streamlit Cloud launched. Check out the different packages geared for everything from personal projects to team workflows needing enterprise-grade features!



Featured Streamlit content

Maxime shared how he and his team built an app to train time-series forecasting models in Forecasting with Streamlit Prophet.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Greg's Google Trends For Top GSC Keywords app lets you label your top-performing GSC keywords with Google Trends.
William gives a tutorial on how to build An App to Update Excel File with Pandas and Streamlit.
Analyze the sales dynamics of an NFT collection published on the Algorand blockchain in Vilijan's NFT Analytics App.
Bek teaches how to create a Stock Prices Web App with Streamlit Framework in his informative YouTube video.
Fanilo's Twittorial: Using Seaborn in Streamlit gives a quick breakdown on plotting Seaborn plots in Streamlit.
Bogdan recorded a demo of his awesome Stock Beta Calculator app.
Kabilan's Mask Detection and Social Distancing Detection app finds COVID risk in an area based on live video input.
In his video tutorial, Siddhardhan shows how to Deploy Machine Learning Model using Streamlit in Python.
Rahman's DataCo Smart Supply Chain Dashboard provides nice graphics for data visualization.
GPT-3 for SEO by Andrea is an app to test all the possible SEO use-cases for GPT-3.
Learn how to Build a Personal Webpage with Streamlit in Alan's article.

Thanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

AbdulMajedRaja RS - Streamlit
https://blog.streamlit.io/author/abdulmajedraja/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by AbdulMajedRaja RS
1 post
How to build a real-time live dashboard with Streamlit

5 easy steps to make your own data dashboard

Advocate Posts
by
AbdulMajedRaja RS
,
April 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Samuel Bancroft - Streamlit
https://blog.streamlit.io/author/samuel/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Samuel Bancroft
1 post
Twitter
Observing Earth from space with Streamlit

Learn how Samuel Bancroft made the SatSchool app to teach students Earth observation

Advocate Posts
by
Samuel Bancroft
,
June 16 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Shruti Agarwal - Streamlit
https://blog.streamlit.io/author/shruti/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Shruti Agarwal
1 post
How to build Streamlit apps on Replit

Learn Streamlit by building the Beginner Template Tour

Advocate Posts
by
Shruti Agarwal
,
September 29 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

scienceio.png (2000×942)
https://blog.streamlit.io/content/images/2023/03/scienceio.png


cloud-deploy-1.png (2000×943)
https://blog.streamlit.io/content/images/2023/03/cloud-deploy-1.png


culture-map-hero.png (2000×947)
https://blog.streamlit.io/content/images/2023/03/culture-map-hero.png


Pranav Pandit - Streamlit
https://blog.streamlit.io/author/pranavpandit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Pranav Pandit
1 post
New UC Davis tool tracks California's COVID-19 cases by region

Regional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine

Advocate Posts
by
Pranav Pandit
,
November 19 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

George Merticariu - Streamlit
https://blog.streamlit.io/author/george/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by George Merticariu
1 post
3 steps to fix app memory leaks

How to detect if your Streamlit app leaks memory and identify faulty code

Tutorials
by
George Merticariu
,
April 14 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/page/6/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
Add secrets to your Streamlit apps

Use Secrets Management in Streamlit sharing to securely connect to private data sources

Tutorials
by
James Thompson
,
April 9 2021
How to use Roboflow and Streamlit to visualize object detection output

Building an app for blood cell count detection

Advocate Posts
by
Matt Brems
,
February 23 2021
Streamlit ❤️ Firestore

Use Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!

Tutorials
by
Austin Chen
,
January 27 2021
Streamlit Components, security, and a five-month quest to ship a single line of code

The story of allow-same-origin

Tutorials
by
Tim Conkling
,
January 20 2021
Elm, meet Streamlit

A tutorial on how to build Streamlit components using Elm

Tutorials
by
Henrikh Kantuni
,
December 8 2020
Testing Streamlit apps using SeleniumBase

How to create automated visual tests

Tutorials
by
Randy Zwitch
,
November 23 2020
Deploying Streamlit apps using Streamlit sharing

A sneak peek into Streamlit's new deployment platform

Tutorials
by
Tyler Richards
,
October 15 2020
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

colorpalette.png (2000×948)
https://blog.streamlit.io/content/images/2023/03/colorpalette.png


background-removal-hero-1.png (2000×942)
https://blog.streamlit.io/content/images/2023/03/background-removal-hero-1.png


Monthly rewind > April 2022
https://blog.streamlit.io/monthly-rewind-april-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > April 2022

Your April look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, May 5 2022
🏆 App of the month 🏆
Streamlit April updates
🔍 Current release: 1.9.0
🔮 Upcoming
🗓 30 days of Streamlit
🎙 Tech Twitter Space
📄 New docs tutorials
🎈 New Streamlit Creators
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our April featured app of the month is... 🥁🥁🥁🥁🥁

SatSchool - Hands on with Data by Samuel Bancroft.

SatSchool is an earth observation outreach program for getting hands-on with satellite data. This interactive app teaches you how to work with different types of data in order to visualize and understand environmental concepts. [code]

Streamlit April updates

Let's take a look at what happened in April.

🔍 Current release: 1.9.0

The latest release is 1.9.0. Recent updates include support for keyword-only argument expanded on st.json and the maintaining of the widgets' value when disabled is set/unset. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

Some exciting new features coming soon:

Workspace analytics and app viewers data
Multipage apps
New dataframe UI

Check out our roadmap app to see what else we're working on. 🥳

🗓 30 days of Streamlit
April kicked off #30DaysofStreamlit, a fun social challenge to practice building and deploying Streamlit apps. Get started and follow the daily tasks here.

🎙 Tech Twitter Space
As part of #30DaysofStreamlit, Francesco hosted a Tech Twitter Space with Amanda, Randy, and Chanin (Dataprofessor). Listen to the recording to hear the discussion—including questions from the community!
📄 New docs tutorials
We added two new database connection tutorials to the docs. Learn how to connect Streamlit to Deta Base and Supabase.

🎈 New Streamlit Creators
Welcome our newest Streamlit Creators—Mısra, Gerard, and Franky! 🥳
Featured Streamlit content
Learn how to find memory leaks and fix them with these three steps from George.

1littlecoder teaches you how to build your own real-time live data dashboard for quick insights into finance, marketing, and data analytics.

See how Streamlit helped the analytics team at The Stable scale and democratize their data and go from idea to app in just a few days.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Get insight into energy production and use Benedict's Energytics app.
Anna's Hiking Upward Recommender app can help you find your next similar hike.
Sharone's article shows how to Make Dataframes Interactive by using Streamlit-Aggrid.
In his Introduction to Streamlit and Streamlit Components, Arvindra details how to build not only a Streamlit app but also custom components to expand its functionality.
Christoph's Prettymapp app lets you create beautiful artistic maps in your browser for any address.
Learn how to add keyboard shortcuts to navigate through the Streamlit apps with Fanilo's video tutorial.
The Striking Distance Creator app by Lee blends keyword & crawl data to provide actionable insights for keywords close to ranking.
Japan created an app for cryptocurrency prediction called CryptoBase.
Jina AI released Jina NOW—a no-code solution for creating a neural search with a built-in Streamlit interface!
spacy-streamlit got an upgrade with improved NER visualization, UX and layout enhancements, and two new example scripts.
Joan teaches how to Deploy Machine Learning Web Applications with Streamlit and seamlessly interact with models in a creative and easy way.
Okld released an awesome new component Streamlit Elements that allows you to build draggable and resizable dashboards with Material UI, Nivo charts, and more!
Learn how to Generate Images with Your Voice Using DALL-E in Assembly AI's tutorial by Patrick.
Another article by Sharone dives into how to Create a Data Profiling App Using Pandas_Profiling and Streamlit.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

kedrobloghero.png (2000×945)
https://blog.streamlit.io/content/images/2023/03/kedrobloghero.png


Streamlit
https://blog.streamlit.io/december-rewind/https://blog.streamlit.io/create-a-color-palette-from-any-image/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
404
Page not found
← Go to the front page
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Screenshot-2023-02-01-at-9.33.30-PM-2.png (2000×943)
https://blog.streamlit.io/content/images/2023/03/Screenshot-2023-02-01-at-9.33.30-PM-2.png


authenticator2-1.png (2000×943)
https://blog.streamlit.io/content/images/2023/03/authenticator2-1.png


gif-editor-game.gif (1542×812)
https://blog.streamlit.io/content/images/2023/03/gif-editor-game.gif


columnsdemo.png (2000×942)
https://blog.streamlit.io/content/images/2023/03/columnsdemo.png


Making Apps for the Rasa Research Team & Open Source Community
https://blog.streamlit.io/rasalit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Making apps for the Rasa research team (and open source community!)

Helping Rasa users understand their models

By Vincent D. Warmerdam
Posted in Advocate Posts, May 12 2021
Challenges
Enter Rasalit
GridSearch Overview
Rasa NLU Playground
Spelling Effects
Text Clustering
Steps Going Forward
Resources
Contents
Share this post
← All posts

To learn more about Rasalit, check out the article on the Rasa blog.

Rasa Open Source is a machine learning framework for building text- and voice-based virtual assistants. It’s a Python library with tools that can understand messages, reply to users, and connect to different messaging channels and APIs.

In this post, we'll do a deep dive into the Rasalit project, which is an integration between Rasa and Streamlit, but here's a sample app if you want to test it out right away!

Challenges

Rasa actively researches and shares practical algorithms that can handle natural language tasks, but exploring algorithms in this space brings a few unique challenges.

For starters, we can only benchmark on datasets that are openly available. If there is any private data in a conversation, it can’t be shared - which excludes a lot of meaningful datasets.

Privacy isn’t the only constraint we face, another limitation is the languages we can use in our benchmarking datasets. We’ve done our best to integrate many open-source tools for non-English deployments, but we still actively rely on our community for feedback.

To address this, we’ve been looking for a meaningful tool to give to our community that makes it easy to explore and investigate trained Rasa models interactively. If we can make it easy for users to inspect their pipelines, we also make it easier for people to give feedback on specific parts.

Enter Rasalit

Rasalit’s first iteration was a simple demo in a Jupyter notebook. To use it, you would declare an utterance text for a pre-trained Rasa pipeline to classify and then you could see the prediction’s confidence values in a bar chart.

In theory, we had a meaningful visualization. But it became clear this approach wouldn’t work in practice for many reasons:

Sharing notebooks over GitHub tends to be a painful experience.
Not every Rasa user is familiar with Python, which means a Jupyter notebook can be intimidating for some.
Hosting a Jupyter notebook on a server involves security risks. You can’t run a Jupyter notebook in read-only mode and still allow users to change settings, which means you can’t host our visualizations securely on a private server.
Jupyter is excellent when writing code, but code can distract us from our visualizations. We wanted users to focus on the model views.

That’s why, instead of going with Jupyter, we decided to package our views with Streamlit. Streamlit allows us to control what users can interact with and keep distractions away.

We ended up creating several Streamlit apps that proved valuable, and we bundled them all together into a package called Rasalit.

Here’s what it looks like when you run it from the terminal.

> python -m rasalit
Options:
  --help  Show this message and exit.

Commands:
  overview       Gives an overview of all `rasa train nlu` results.
  live-nlu       Select a trained Rasa model and interact with it.
  spelling       Check the effect of spelling on NLU predictions.
  nlu-cluster    Cluster a text file to look for clusters of intents.
  version        Prints the current version


Each command represents a separate Streamlit application. When passing arguments in Rasalit, we can translate them into appropriate arguments for Streamlit.

GridSearch Overview

One tool in Rasalit handles visualizing grid-search results. You can run cross-validation from the command line in Rasa, but our plugin now makes it easy to get an overview of the scores too.

Rasa NLU Playground

The second app allows users to interact directly with a pre-trained Rasa model. You get an overview of the intent confidence and any detected entities.

We’ve also added charts that visualize the classifier’s internal attention mechanism.

To keep the overview simple, we’ve hidden these details. An excellent feature from Streamlit is you can hide details via the expander component. That means we can add detailed features for our Research team while still keeping the app distraction-free for the general community.

Spelling Effects

There’s also a spelling robustness checker in Rasalit, which simulates spelling errors on a text that you give it. It will show you how robust your trained models are against typos.

Text Clustering

Finally, we’ve also added a tool for folks who are just getting started with their virtual assistants. Some users might already have some unlabelled training data and might just be curious to explore the clusters in them.

For this use case, we’ve built a text clustering demo. It uses a light version of the universal sentence encoder to cluster text together. This result can be explored interactively.

Steps Going Forward

Streamlit has turned out to be a surprisingly flexible communication tool. Our research team and community members all use Rasalit. It’s also been getting us great feedback! We’re proud to report it has surpassed 100 Github stars.

Rasalit has been so successful we’ve also started using it in other places. We host a training data repository where users can find training data to help get them started with their first virtual assistant.

Before, users had to search inside the many YAML files to find the training data that fits their use cases. Now we’ve simply attached a hosted Streamlit app that makes it easy for users to find the relevant training data. It’s a great experience!

We’re excited to see what you all think of Rasalit and what new applications we find for Streamlit, both for our Research team and across the Rasa open source community.

Resources

Rasa

Homepage
Docs
Rasalit repo
Forum

Streamlit

GitHub
Docs
Forum

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > May 2022
https://blog.streamlit.io/monthly-rewind-may-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > May 2022

Your May look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, June 7 2022
🏆 App of the month 🏆
Streamlit May updates
🔍 Current release: 1.10.0
🔮 Upcoming
📈 Workspace analytics and app viewers
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our May featured app of the month is... 🥁🥁🥁🥁🥁

BERT Semantic Interlinking Tool by Lee Foot!

Upload an internal_html.csv from Screaming Frog and discover pages to interlink. This app allows you to find related products and blogs, supporting content for category pages, and related pages to crosslink. It also lets you group similar pages. You can filter the source and the destination URL type, select cluster size and accuracy, and cluster any column. [code]

*Learn about the latest updates here as of Dec. 2023

Streamlit May updates

A lot of exciting things happened in May! Let's take a look.

🔍 Current release: 1.10.0

The latest release is 1.10.0. Recent updates include native support for multipage apps and redesigned st.dataframe. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

These new features are currently on the horizon:

Custom and unique subdomains
Replay of cached st calls

Check out our roadmap app to see what else we're working on. 🥳

📈 Workspace analytics and app viewers
Get insight into the traffic of your Streamlit apps without having to use JS hacks! Try the new built-in Analytics Modal in Community Cloud.

Featured Streamlit content
Learn how the Streamlit data science team uses st.session_state and query parameters to share insights through contextual apps.

Need a quick way to interactively share your project with your team? See how Wissam brought his AI4Health work to life with Streamlit!

Read how Mitch and Roland visualized their research with Rascore, an app for analyzing the 3D structural models of RAS proteins.

Learn how to make a select box in the latest Streamlit short.

Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Visualize Chemical Space with the ChemPlot app by Dajt and team.
Submit a Wind Simulation — Request Offer with Johannes' interactive app.
Add Depth-aware text to an image using Vivien's app.
Watch Andy's step-by-step video on Getting Started With Streamlit in Python.
Ayoub's Text data extractor lets you upload a PDF and receive a text file or zip of the text from it.
José's app demonstrates Ley de Snell to teach refraction and Total Internal Reflection.
Anuraag's app uses ML to find soccer Players of Similar Profiles.
Create Speck molecular structures in Streamlit apps with Avra's Specklit component.
Yuichiro started the stlite, a project allowing Streamlit to run completely on browsers with Wasm by using Pyodide!
Felipe's tutorial teaches you how to make Magic Data Apps with Snowflake, Streamlit, and DuckDB.
Learn how to make a Streamlit Keyword Density Checker Web App in this video tutorial from Nileg Production.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > July 2021
https://blog.streamlit.io/monthly-rewind-july-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > July 2021

Your July look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, August 5 2021
🏆 App of the month 🏆
Streamlit July updates
🔍 Current release: 0.85.1
🔮 Upcoming features
🎈 New blog design
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our July featured app of the month is......🥁🥁🥁🥁🥁

Chef Transformer by Mehrdad, Kartik, Haswanth, Deepak and Nicholas.

Discover new and unique recipes from Chef Scheherazade and Chef Giovanni, trained NLP models/"chefs", by entering a type of cuisine or a custom ingredient list. [code]

Streamlit July updates

Let's take a look at all that happened during the month of July!

🔍 Current release: 0.85.1

The latest release is 0.85.1. Notably, Streamlit now uses Apache Arrow to serialize DataFrames but make sure to check out the changelog to see all the latest features and fixes.

🔮 Upcoming features

Be on the lookout for these new features on the horizon:

Text dials
Download button
Support for cards
🎈 New blog design

If you hadn't noticed, we made some fun changes to our blog site! Take a look around and explore our other posts.

Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Randy outlines some tips on improving your app performance in the final part of the Designing Streamlit Apps for the User series.

Henrikh dives into how we now use Arrow as our internal serialization format and how it helped delete over 1,000 lines of code from our codebase here.

Listen to Amanda talk about her career journey in this Founders You Should Know interview with Renaissance Collective
Ken gives an in depth interview and answers Streamlit questions in dataroots' Tour de Tools.
Featured community content

Some great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Dr. WJB Mattingly kicks off his Streamlit tutorial series with a video on How to use Streamlit in 30 Minutes!
Upload an image to Ujjayanta's app and it will Generate ASCII images using GAN
This Cloud Optimized GeoTIFF Viewer by Mykola lets you view your COG files with an interactive web map
Anmol's Nightingale Chart Plotter app lets you create, customize and download Nightingale charts with mplsoccer
Matthew and his colleagues created a dashboard to Visualize your EED data which can generate six unique visualizations as seen from a variety of different perspectives
The Python Engineer gives a speedy tutorial in his Machine Learning Web App In Python In 60 Seconds! #Shorts video
Gain insight into, measure the quality of, and repair your data with the DataQtor app by Beytullah
Lukas' Animation des Rheinabflusses seit Juni 2020 consecutively displays the precipitation and runoff data of the Rhine in Basel
Yuichiro forked Kazuhito's Tokyo2020 Pictogram using MediaPipe code and made a Streamlit app so anyone can try it!
Cassie developed a Dungeons & Dragons Currency Converter to calculate which and how many coins to use when spending in the game
Check out Kieran's simple web publishing system which can display both text based and interactive code-based posts

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > August 2021
https://blog.streamlit.io/monthly-rewind-august-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > August 2021

Your August look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, September 7 2021
🏆 App of the month 🏆
Streamlit August updates
🔍 Current release: 0.88.0
🔮 Upcoming features
🎈 New careers page
🎨 New creators
🍎 Education poll
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our August featured app of the month is......🥁🥁🥁🥁🥁

Parking Spot Vacancy by Jeffrey Jex.


Wanna know how many parking spots are available at the Jackson Hole Wyoming Town Square before making the trip? This app takes a live stream of the area and and processes it with Mask R-CNN to detect cars and trucks. It then compares locations with parking places to see which are available. [code]

Streamlit August updates

Let's take a look at all that happened during the month of August!

🔍 Current release: 0.88.0

The latest release is 0.88.0 with st.download_button being the notable feature. August also brought st.metric! Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming features

Check these new features coming soon:

Improved hamburger menu
Next gen cache
Text dials
Support for cards
🎈 New careers page

Want to know what it's like to work at Streamlit? Check out the new and improved careers page to learn more about our values and see what roles are open!

🎨 New creators

We're excited to welcome 3 new members to the Streamlit creators program. Shout out to AbdulMajed, Pablo, and Yuichiro. Get to know them and the rest of our awesome creators here.

🍎 Education poll

We're looking to develop an Education Program and would love your insight and feedback on Streamlit in the classroom. If you're a teacher or student, take a sec to fill out our form and let us know how we can support you.

Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Streamlit creator Tyler Richards released his book "Getting Started with Streamlit for Data Science", a great comprehensive guide on learning Streamlit!

Check out more details about this awesome new resource in Adrien's review piece.
Streamlit Gains a Major New Spell Book
A tome to the magical fields of Python, Algorithms, Visualization, and Machine learning.
Streamlit
Adrien Treuille

Featured community content

Some great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

1littlecoder showed how to make an app that can extract text from images in his video Python Tutorial to build Image to Text App using EasyOCR & Streamlit
Python Engineer created a Stock Prediction Web App In Python In 60 Seconds! in his short
Yaniss' Broken Links Finder app takes a domain and finds every broken link in any of the pages present on the website
See how Streamlit is used for Sharing micro-computed tomography(MCT) image analysis in dentistry in Jongki's app
Avra gave tips on how to design your apps in his video STREAMLIT Python TRICKS - make your WEB APP look BETTER in 6 Minutes
SEOgre by Colt, is a tool that lets you layer Google updates, GSC traffic, and SEO tool data exports to better analyze data
Jose made an app SMILES + RDKit + Py3DMOL to learn about Simplified Molecular Input Line Entry System
In his video Streamlit Tutorial for Building Analytics Metric Dashboard in Python 1littlecoder went over the new st.metric feature
Jenna created Interact with Gapminder Data, an app that students can learn how to make in her workshop on interactive data visualizations in python
Ahmed wrote an overview outlining 7 Reasons Why You Should Use the Streamlit AgGrid Component
Misra came back with another great YouTube tutorial showing How to use Streamlit session states and callback functions | Make your apps remember things!

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > February 2022
https://blog.streamlit.io/monthly-rewind-february-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > February 2022

Your February look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, March 7 2022
🏆 App of the month 🏆
Streamlit February updates
🔍 Current release: 1.7.0
🔮 Upcoming
📺 Streamlit and Pinecone Webinar
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our February featured app of the month is... 🥁🥁🥁🥁🥁

Streamlit with AssemblyAI by Ahmed Besbes!

This app uses AssemblyAI to transcribe YouTube videos and extract topics from them. It breaks the video into portions and highlights the detected topics from each. [code]

Streamlit February updates

Let's take a look at all that happened in February.

🔍 Current release: 1.7.0

The latest release is 1.7.0. Recent updates include WebSocket compression being disabled by default, and radio and checkboxes improving focus on keyboard navigation. Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming

Below are some upcoming features to get excited about:

App viewers data
Email alert when app is over the resource limits
Multipage apps

Check out our roadmap app to get a bigger scope of what else we're working on. 🥳

📺 Streamlit and Pinecone Webinar
Randy joined James and Pinecone to show how to build a custom Q&A app to revolutionize your search systems!

Featured Streamlit content
See how Delta Dental uses Streamlit to make lightning-fast decisions and empower their data team.

Calculate distances in cosmology with just one click! Learn how researchers Niko Marco and Matthijs came together to create the CosmΩracle app.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Lucas made a financial data visualization app with Raindrop Charts using Yfinance, Streamlit, & Plotly.
Fanilo showed 4 ways to display Seaborn charts in Streamlit in his YouTube tutorial.
James wrote about how to build production-ready NLP apps in his article Getting Started with Streamlit for NLP.
In his article Build Your Own SEO AnswerBox With GPT-3 Codex & Streamlit, Vincent explained how to gain productivity by configuring your own smart SEO dashboards using phrases and questions.
Learn How to build a Cryptocurrency Price Index [with Python using SQL,APIs and Streamlit] in Algovibes' video tutorial.
See visualizations for the last 500 recorded earthquakes on a Turkish map in Doğukan's Turkey Earthquake Dashboard .
Alan wrote a great step-by-step guide on how to Publish Your Streamlit Apps in the Cloud.
In his helpful tutorial, Fanilo went over 10 tips to become a Streamlit Cloud Master.
Explore Oscar nominees and make predictions on the winners in Bogdan's 2022 Oscars Predictions app.
Ammar's app summarized the most common Activation Functions—an important topic in deep learning.
Create pallettes from colormaps with Robert's Color Extractor and use them in your projects.
Brydon created NorthDash: Canada’s Status Dashboard to visualize how Canada is faring based on data.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > March 2022
https://blog.streamlit.io/monthly-rewind-march-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > March 2022

Your March look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, April 7 2022
🏆 App of the month 🏆
Streamlit March updates
🔍 Current release: 1.8.0
🔮 Upcoming
❄️ Snowflake
🎙 Open source podcast
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our March featured app of the month is... 🥁🥁🥁🥁🥁

Rascore by Mitchell Parker and Roland Dunbrack!

Rascore is a tool for analyzing structures of the RAS protein family. It can also be used to build an updatable database of all available RAS structures in the Protein Data Bank. This app aims to simplify the biological study of RAS proteins in cancer and other diseases and facilitate RAS drug discovery. [code]

Streamlit March updates

Let's take a look at what happened in March.

🔍 Current release: 1.8.0

The latest release is 1.8.0. Recent updates include improved performance for dataframes and design improvements to our header. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

Below are some exciting upcoming features:

App viewers data
Workspace analytics
Multipage apps

Check out our roadmap app to get a bigger scope of what else we're working on. 🥳

❄️ Snowflake
Streamlit was acquired by Snowflake! We're excited to join forces and open new frontiers in data science and data application development. Read more.

🎙 Open source podcast
Adrien joined the folks at Open Source Startup Podcast to chat about the launch and cultivation of Streamlit.

Featured Streamlit content
See how Sogeti created Data Quality Wrapper, an educational Streamlit app for data preprocessing.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Avra demonstrated how to use STREAMLIT with AG Grid Table - Interactive Table in his YouTube video.
In his article, James went over How to Create Custom Streamlit Components!
Andrea built an app for Web Scraping Made Easy using the AutoScraper library.
Qiusheng added an interactive Split-Map feature to his geospatial app.
Alan detailed how he used Streamlit for Visualizing the Economic Impact of the Pandemic.
Gerard created an app to explore Interactive Timeseries Forecasting with Darts.
Fidelity Account Overview is another app from Gerard that visualizes your Fidelity data.
In an AssemblyAI YouTube video, Misra showed how to make an app for Auto-generating meeting notes with Python.
Enter a keyword and see the list of Google questions with Mihir's Realtime - People Also Ask Tool.
Pythonology's beginner Python tutorial teaches how to make a Currency Converter - Streamlit app.
Get help figuring out daily Worlde puzzles by using Siddhant's Wordle Solver app.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
February 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Screenshot-2023-02-13-at-11.12.40-AM-1.png (2000×947)
https://blog.streamlit.io/content/images/2023/03/Screenshot-2023-02-13-at-11.12.40-AM-1.png


Build a Neural Search | Use Jina to Search Text or Images
https://blog.streamlit.io/streamlit-jina-neural-search/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build a Jina neural search with Streamlit

Use Jina to search text or images with the power of deep learning

By Alex C-G
Posted in Advocate Posts, April 15 2021
Why use Jina to build a neural search?
Why use Streamlit with Jina?
How does it work?
Example code
Behind the scenes
Set configuration variables
Render component
Use it in your project
What to do next
Jina
Streamlit
A big thank you!
Contents
Share this post
← All posts

Do you ever think, “Darn this stupid cloud. Why can’t there be an easier way to build a neural search on it?”

Well, if you have, this article is for you. I’m going to walk through how to use Jina's new Streamlit component to search text or images to build a neural search front end.

Want to jump right in? Check out the component's repo.

Why use Jina to build a neural search?

Jina is an open-source deep learning-powered search framework for building cross-/multi-modal search systems (e.g. text, images, video, audio) on the cloud. Essentially, it lets you build a search engine for any kind of data with any kind of data.

So you could build your own text-to-text search engine ala Google, a text-to-image search engine ala Google Images, an audio-to-audio search engine and so on. Companies like Facebook, Google, and Spotify build these searches powered by state-of-the-art AI-powered models like FAISS, DistilBERT and Annoy.

Why use Streamlit with Jina?

I've been a big fan of Streamlit since before I even joined Jina. I used it on a project to create terrible Star Trek scripts that later turned into a front end for text generation with Transformers. So I'm over the moon to be using this cool framework to build something for our users.

Building a Streamlit component helps the data scientists, machine learning enthusiasts, and all the other developers in the Streamlit community build cool stuff powered by neural search. It offers flexibility and, being written in Python, it can be easier for data scientists to get up to speed.

Out of the box, the streamlit-jina component has text-to-text and image-to-image search, but Jina offers a rich search experience for any kind of data with any kind of data so there's plenty more to add to the component!

How does it work?

Every Jina project includes two Flows:

Indexing: for breaking down and extracting rich meaning from your dataset using neural network models

Querying: for taking a user input and finding matching results

Our Streamlit component is a front end for end users, so it doesn't worry about the indexing part.

Admin spins up a Jina Docker image: docker run -p 45678:45678 jinahub/app.example.wikipedia-sentences-30k:0.2.9-1.0.1
User enters a query into the Streamlit component (currently either a text input or an image upload) and hits 'search'
The input query is wrapped in JSON and sent to Jina's query API
The query Flow does its thing and returns results in JSON format (along with lots of metadata)
The component parses out the useful information (e.g. text or image matches) and displays them to the user
Example code

Let's look at our text search example, since it's easier to see what's going on there:

import streamlit as st
from streamlit_jina import jina
st.set_page_config(page_title="Jina Text Search",)

endpoint = "http://0.0.0.0:45678/api/search"

st.title("Jina Text Search")
st.markdown("You can run our [Wikipedia search example](https://github.com/jina-ai/examples/tree/master/wikipedia-sentences) to test out this search")

jina.text_search(endpoint=endpoint)


As you can see, the above code:

Imports streamlit and streamlit_jina
Sets the REST endpoint for the search
Sets the page title
Displays some explanatory text
Displays the Jina text search widget with endpoint defined

For the Jina Streamlit widgets you can also pass in other parameters to define number of results you want back or if you want to hide certain widgets.

Behind the scenes

The source code for our module is just one file, __init__.py. Let's just look at the high-level functionality for our text search example for now:

Set configuration variables
headers = {
    "Content-Type": "application/json",
}

# Set default endpoint in case user doesn't specify and endpoint
DEFAULT_ENDPOINT = "http://0.0.0.0:45678/api/search"

Render component
class jina:
    def text_search(endpoint=DEFAULT_ENDPOINT, top_k=10, hidden=[]):
        container = st.beta_container()
        with container:
            if "endpoint" not in hidden:
                endpoint = st.text_input("Endpoint", endpoint)

            query = st.text_input("Enter query")

            if "top_k" not in hidden:
                top_k = st.slider("Results", 1, top_k, int(top_k / 2))

            button = st.button("Search")

            if button:
                matches = text.process.json(query, top_k, endpoint)
                st.write(matches)

        return container


In short, the jina.text_search() method:

Creates a Streamlit container to hold everything, with sane defaults if not specified
If widgets aren't set to hidden, present them to user
[User types query]
[User clicks button]
Sends query to Jina API and returns results
Displays results in the component

Our method's parameters are:

jina.text_search() calls upon several other methods, all of which can find in __init__.py. For image search there are some additional ones:

image.encode.img_base64() encodes a query image to base64 and wraps it in JSON before passing to Jina API
Jina's API returns matches in base64 format. The image.render.html() method wraps these in <IMG> tags so they'll display nicely
Use it in your project

In your terminal:

Create a new folder with a virtual environment and activate it. This will prevent conflicts between your system libraries and your individual project libraries:

mkdir my_project
virtualenv env
source env/bin/activate


Install the Streamlit and Streamlit-Jina packages:

pip install streamlit streamlit-jina


Index your data in Jina and start a query Flow. Alternatively, use a pre-indexed Docker image:

docker run -p 45678:45678 jinahub/app.example.wikipedia-sentences-30k:0.2.9-1.0.1


Create your app.py:

import streamlit as st
from streamlit_jina import jina
st.set_page_config(page_title="Jina Text Search",)

endpoint = "http://0.0.0.0:45678/api/search" # This is Jina's default endpoint. If your Flow uses something different, switch it out

st.title("Jina Text Search")

jina.text_search(endpoint=endpoint)


Run Streamlit:

streamlit run app.py


And there you have it – your very own text search!

For image search, simply swap out the text code above for our image example code and run a Jina image (like our Pokemon example).

What to do next

Thanks for reading the article and looking forward to hearing what you think about the component! If you want to learn more about Jina and Streamlit here are some helpful resources:

Jina
Streamlit-Jina component
Jina docs
Jina Fundamentals
Jina hello-world demos
Streamlit
Streamlit docs
Components gallery
App gallery
Community forum
A big thank you!

Major thanks to Randy Zwitch, TC Ricks and Amanda Kelly for their help getting our component live. And thanks to all my colleagues at Jina for building the backend that makes this happen!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > January 2022
https://blog.streamlit.io/monthly-rewind-january-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > January 2022

Your January look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, February 7 2022
🏆 App of the month 🏆
Streamlit January updates
🔍 Current release: 1.5.0
🔮 Upcoming
📸 Camera integration
🎈 What's new in Streamlit
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our January featured app of the month is... 🥁🥁🥁🥁🥁

gitlit by Vincent Warmerdam!

This app is a GitHub Actions scraper for popular open-source repositories. Get insights into testing times and costs needed to run these projects. [code]

Streamlit January updates

Let's take a look at what happened in January.

🔍 Current release: 1.5.0

The latest release is 1.5.0. Recent updates include the favicon defaulting to PNG for transparency and better support for nested HTML. Be sure to check out the changelog to learn more about all of the latest features and fixes.

🔮 Upcoming

Here are some upcoming features to get excited about:

In-app share menu
St.user
Multipage apps

Check out our roadmap app to get a bigger scope of what else we're working on. 🥳

📸 Camera integration
You can now upload images to your apps straight from your camera with the new st.camera_input feature. Try it out with the release demo.

🎈 What's new in Streamlit
Recent updates to Streamlit Cloud include the sign-in with email and the ability to share apps with any email address. Read more about these features here.



Featured Streamlit content
Learn Streamlit essentials in this guide by Data Professor outlining all the steps from start to deployment!

Create interactive presentations with Streamlit using Sebatian's streamlit_book library.

See how to diagnose blood cancer from DNA data with Streamlit! Check out CloneRetriever by Eitan Halper-Stromberg, the molecular pathologist at Johns Hopkins.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Daniel's Carbon Intensity App lets you visualize the carbon intensity for the UK by date and can forecast up to 2 days ahead.
In his video Pseudo Grid Layout using Streamlit columns, Fanilo shows how to use horizontal layout and columns to improvise a grid layout.
Say or write a pickup line, and Paul's Pickup Line Prediction Using K-Nearest Neighbors app will determine if it's a success or a failure.
Simulate and study emergent properties of teamwork with Michael and Chris's SuperScript app.
Learn all about these 5 Streamlit Components To Build Better Applications in Ahmed's breakdown.
Sebastian's app teaches about the Confusion Matrix in an interactive and fun way.
Learn how to build a Real-Time Live Finance/Marketing/Data Science Dashboard in Python from 1littlecoder's YouTube tutorial.
Sejal does A Deep Dive into Wordle, the New Pandemic Puzzle Craze with an app to analyze how effective your guesses are.
Siavash created Koffee of the world—a simple web app to explore, visualize, and compare coffee quality profiles of different countries.
Find Google Earth Engine apps by their location, filter by zoom level, and open the app directly in your browser with Philipp's Earth Engine App Finder.
Use Antoine's Download data from archive.org app to get old URLs for a domain—a necessary step for fixing migrations.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Screenshot-2023-03-27-at-11.41.31-PM-1.png (2000×945)
https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-27-at-11.41.31-PM-1.png


hackathonhero.png (2000×947)
https://blog.streamlit.io/content/images/2023/05/hackathonhero.png


Streamlit
https://blog.streamlit.io/december-rewind/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
404
Page not found
← Go to the front page
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

ezgif.com-resize.gif (753×461)
https://blog.streamlit.io/content/images/2023/05/ezgif.com-resize.gif#browser


Screenshot-2023-03-07-at-10.56.55-AM.png (2000×945)
https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-07-at-10.56.55-AM.png


Screenshot-2023-03-13-at-9.05.07-PM.png (2000×943)
https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-13-at-9.05.07-PM.png


10forumquestions-1.png (2000×945)
https://blog.streamlit.io/content/images/2023/05/10forumquestions-1.png


GIF-1.2.gif (1000×600)
https://blog.streamlit.io/content/images/2021/08/GIF-1.2.gif


Main_Secrets_Management_1.gif (1280×872)
https://blog.streamlit.io/content/images/2021/08/Main_Secrets_Management_1.gif#border


_low__2d_balloons-1.gif (800×800)
https://blog.streamlit.io/content/images/2021/08/_low__2d_balloons-1.gif#border


GIF-321.gif (796×386)
https://blog.streamlit.io/content/images/2021/08/GIF-321.gif#border


1-1-1.png (1200×607)
https://blog.streamlit.io/content/images/2021/08/1-1-1.png


3-1-1.png (1200×700)
https://blog.streamlit.io/content/images/2021/08/3-1-1.png#border


2-2-1.png (1200×700)
https://blog.streamlit.io/content/images/2021/08/2-2-1.png#border


3-7.png (1200×700)
https://blog.streamlit.io/content/images/2021/08/3-7.png#border


5-8.png (1200×512)
https://blog.streamlit.io/content/images/2021/08/5-8.png#border


4-5.png (1200×500)
https://blog.streamlit.io/content/images/2021/08/4-5.png#border


6-2.png (1202×729)
https://blog.streamlit.io/content/images/2021/08/6-2.png#browser


2-10.png (1200×512)
https://blog.streamlit.io/content/images/2021/08/2-10.png#border


1-9.png (1200×775)
https://blog.streamlit.io/content/images/2021/08/1-9.png#border


3-6.png (1200×727)
https://blog.streamlit.io/content/images/2021/08/3-6.png#browser


topics-discussed.png (2000×604)
https://blog.streamlit.io/content/images/2022/11/topics-discussed.png#border


Jessica Smith - Streamlit (Page 2)
https://blog.streamlit.io/author/jessica/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jessica Smith
26 posts
Monthly rewind > May 2022

Your May look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2022
Monthly rewind > April 2022

Your April look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2022
Monthly rewind > March 2022

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 7 2022
Monthly rewind > February 2022

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 7 2022
Monthly rewind > January 2022

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 7 2022
Monthly rewind > December 2021

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 7 2022
Monthly rewind > November 2021

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2021
Monthly rewind > October 2021

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2021
Monthly rewind > September 2021

Your September look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2021
Monthly rewind > August 2021

Your August look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Anastasia Glushko - Streamlit
https://blog.streamlit.io/author/anastasia/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Anastasia Glushko
1 post
Labeling ad videos with Streamlit

How Wavo.me uses Streamlit’s Session State to create labeling tasks

Advocate Posts
by
Anastasia Glushko
,
September 2 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

video-summary-function.png (2000×782)
https://blog.streamlit.io/content/images/2022/11/video-summary-function.png#border


sensitive-content-detected.png (2000×362)
https://blog.streamlit.io/content/images/2022/11/sensitive-content-detected.png#border


no-sensitive-content.png (2000×212)
https://blog.streamlit.io/content/images/2022/11/no-sensitive-content.png#border


take2.gif (856×464)
https://blog.streamlit.io/content/images/2021/08/take2.gif#border


2-9.png (1200×596)
https://blog.streamlit.io/content/images/2021/08/2-9.png#border


video-summary.png (1514×385)
https://blog.streamlit.io/content/images/2022/11/video-summary.png#border


gif-1-3.gif (1102×594)
https://blog.streamlit.io/content/images/2021/08/gif-1-3.gif#browser


selected-video-analysis.png (2000×574)
https://blog.streamlit.io/content/images/2022/11/selected-video-analysis.png#border


1-8.png (1174×634)
https://blog.streamlit.io/content/images/2021/08/1-8.png#border


analyze-youtube-channel-content.png (2000×785)
https://blog.streamlit.io/content/images/2022/11/analyze-youtube-channel-content.png#border


balloons-1.gif (1280×720)
https://blog.streamlit.io/content/images/2021/09/balloons-1.gif#browser


content-analyzer-app-topics.png (2000×1132)
https://blog.streamlit.io/content/images/2022/11/content-analyzer-app-topics.png#browser


content-analyzer-app-thumbnails.png (2000×1049)
https://blog.streamlit.io/content/images/2022/11/content-analyzer-app-thumbnails.png#browser


app_layout_progress_bar-1.png (1478×1206)
https://blog.streamlit.io/content/images/2021/09/app_layout_progress_bar-1.png#browser


sample_output-1.png (1564×88)
https://blog.streamlit.io/content/images/2021/09/sample_output-1.png#border


app_layout-1.png (1594×1596)
https://blog.streamlit.io/content/images/2021/09/app_layout-1.png#browser


high_medium_low--1-.png (1522×1228)
https://blog.streamlit.io/content/images/2021/09/high_medium_low--1-.png#browser


gif-1.gif (709×693)
https://blog.streamlit.io/content/images/2021/08/gif-1.gif#border


5.png (1212×532)
https://blog.streamlit.io/content/images/2021/08/5.png#border


4.png (1212×489)
https://blog.streamlit.io/content/images/2021/08/4.png#border


3-1.png (1212×648)
https://blog.streamlit.io/content/images/2021/08/3-1.png#border


1-3.png (1212×728)
https://blog.streamlit.io/content/images/2021/08/1-3.png#browser


2-1.png (1212×592)
https://blog.streamlit.io/content/images/2021/08/2-1.png#border


13.1.png (1200×923)
https://blog.streamlit.io/content/images/2021/08/13.1.png#border


Screen-Recording-2020-12-08-at-03.30.25-PM.gif (720×354)
https://blog.streamlit.io/content/images/2021/08/Screen-Recording-2020-12-08-at-03.30.25-PM.gif#border


Qiusheng Wu - Streamlit
https://blog.streamlit.io/author/qiusheng/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Qiusheng Wu
2 posts
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Creating satellite timelapse with Streamlit and Earth Engine

How to create a satellite timelapse for any location around the globe in 60 seconds

Advocate Posts
by
Qiusheng Wu
,
December 15 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

12.1.png (1200×344)
https://blog.streamlit.io/content/images/2021/08/12.1.png#border


10.1.png (1200×1056)
https://blog.streamlit.io/content/images/2021/08/10.1.png#border


11.1.png (1200×418)
https://blog.streamlit.io/content/images/2021/08/11.1.png#border


9.1.png (1200×635)
https://blog.streamlit.io/content/images/2021/08/9.1.png#border


8.1.png (1200×681)
https://blog.streamlit.io/content/images/2021/08/8.1.png#border


5.1.png (1200×1036)
https://blog.streamlit.io/content/images/2021/08/5.1.png#browser


6.1.png (1200×765)
https://blog.streamlit.io/content/images/2021/08/6.1.png#border


GIF_2-1.gif (952×806)
https://blog.streamlit.io/content/images/2021/08/GIF_2-1.gif#browser


GIF_3.gif (952×544)
https://blog.streamlit.io/content/images/2021/08/GIF_3.gif#browser


4.1.png (1179×753)
https://blog.streamlit.io/content/images/2021/08/4.1.png#browser


GIF_1.gif (952×671)
https://blog.streamlit.io/content/images/2021/08/GIF_1.gif#browser


3.1.png (1197×769)
https://blog.streamlit.io/content/images/2021/08/3.1.png#browser


2.1.png (1200×455)
https://blog.streamlit.io/content/images/2021/08/2.1.png#browser


2-7.png (1200×1043)
https://blog.streamlit.io/content/images/2021/08/2-7.png#border


1.2.png (1200×1085)
https://blog.streamlit.io/content/images/2021/08/1.2.png#border


Using Data to Combat Pandemic-Related Evictions
https://blog.streamlit.io/open-source-eviction-data/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Arup and New Story use data to help combat pandemic related evictions

Making data accessible to help address the eviction crisis

By Jared Stock
Posted in Advocate Posts, January 7 2021
Empowering users, no code required
Mapping it out
Making data accessible
Contents
Share this post
← All posts

Written by Jared Stock, a digital consultant at Arup.

One of the many consequences of COVID-19 in the US is that countless families are having trouble keeping up with rent. According to Moody’s, nearly 12 million renters will owe an average of $5,850 in back rent by January 2021. These renters will face the terrifying prospect of losing their homes in the middle of winter during a pandemic.

In early September, the Centers for Disease Control issued a national moratorium on evictions until December 31st, 2020. Under this order, tenants can provide a declaration to their landlord that shows that they meet certain conditions, which then prevents the landlord from evicting them. However, as the CDC notes in the order, “this Order does not relieve any individual of any obligation to pay rent, make a housing payment, or comply with any other obligation…” So, while tenants were protected until then, their rent payments continued to accumulate and they could face eviction if they aren’t able to pay all of the rent back.

One way to help these families is to simply pay their rent, and that’s exactly what New Story, a charity based in the Bay Area, set out to do in the spring of 2020. Our team at Arup helped New Story analyze data in the Bay Area to help them make decisions about how to distribute direct payments to families. We looked at a variety of data sources ranging from economic indicators to how vulnerable a county was to COVID-19. We were able to come up with a relative risk index that we used to compare counties and show where the greatest need was. After that work was done, the next logical step was to share our code and data with the community.

While some NGOs have people with software and data experience, many more don’t have the skills required to use the open data that exists. Our goal is to make that data and the analysis that we’ve done around evictions available to as many people as possible to help drive decisions and ultimately keep more people in their homes.

Empowering users, no code required

When we initially published our repository and data, users had to have some coding experience in order to use it - which can be intimidating! That immediately reduced the number of people who could potentially use the data. This was a clear opportunity to use Streamlit to make the data accessible to everyone without needing to understand SQL queries or how to run Python code. Since our initial release was just vanilla Python code, it was easy to start adding functionality by just marking up our code with Streamlit functions. We were able to adapt our existing Python workflows in just a couple hours and deploy using Streamlit sharing in just a couple minutes.

Right off the bat, we were able to turn what were originally command line prompts into visual inputs for users that update as they go through the process. Since this analysis follows a linear sequence of user inputs, Streamlit inputs allow for a much better user experience and allow us to show data at various steps during the analysis in a much nicer format than writing to a command line. We also provide explanation and details right alongside the inputs and data, rather than in a separate README file.

Streamlit’s input components allowed us to give users a lot more control over what they want to analyze. Instead of making assumptions about housing stock distributions or what features are important to risk, we could surface those assumptions to the user so they could make their own decisions. This level of transparency and control was the inspiration for the other page in the app: the Data Explorer.

This page is meant to give the users the ability to dive into the data in more detail - allowing a user to look at the values of a single feature in the database and compare the values of two different features. Users can also download the raw data as an Excel document and do whatever they want with it. We’re hoping to add more flexibility to this page in the future, like being able to compare counties in multiple different states. If you have ideas you’d like to see implemented, please add a feature request or better yet, contribute a pull request on our GitHub.

Mapping it out

We’ve found that the clearest way for people to understand comparative risk between counties is with a map. Streamlit already has support for simple maps, however, we didn’t just want to plot points; we wanted to show the county shapes. County shapefiles are common and we could get them into our database relatively easily using PostGIS. They get stored in a format called Well Known Binary (WKB), so we need to get them into a format that can be read by pydeck.  

First, I load in the geometry data using Shapely with shapely.wkb.loads(). But immediately we have a problem: the data comes in as a Well Known Text (WTK) object, not as geojson that we can parse. GIS isn't my specialty, so like a good programmer, I looked around Stack Overflow and eventually found some snippets that convert it into sets of coordinates that Python could parse as a dict and then clean it up a bit.

geo_df['geom'] = geo_df.apply(lambda row: row['geom'].buffer(0), axis=1)
geo_df['geom'] = geo_df.apply(lambda row: gpd.GeoSeries(row['geom']).__geo_interface__, axis=1)
geo_df['coordinates'] = geo_df.apply(lambda row: clean_coordinates(row), axis=1)
I later found a better way to do this using Shapely
def clean_coordinates(row: pd.Series) -> list:
    # combine multipolygon into one object as a single polygon
    for f in row['geom']['features']:
        if f['geometry']['type'] == 'MultiPolygon':
            f['geometry']['type'] = 'Polygon'
            combined = []
            for i in range(len(f['geometry']['coordinates'])):
                combined.extend(list(f['geometry']['coordinates'][i]))
            f['geometry']['coordinates'] = combined

        # flatten coordinates
        f['geometry']['coordinates'] = f['geometry']['coordinates'][0]
    return row['geom']

Now we have coordinates that define each shape! I add those to our DataFrame and then I can turn everything into nice, friendly geojson for our map. I create a feature collection with each county's name, shape coordinates, and the values (in our case just one) that we want to display.

def make_geojson(geo_df: pd.DataFrame, features: list) -> dict:
    geojson = {"type": "FeatureCollection", "features": []}
    for i, row in geo_df.iterrows():
        feature = row['coordinates']['features'][0]
        props = {"name": row['County Name']}
        for f in features:
            props.update({f: row[f]})
        feature["properties"] = props
        del feature["id"]
        del feature["bbox"]
        feature["geometry"]["coordinates"] = [feature["geometry"]["coordinates"]]
        geojson["features"].append(feature)

    return geojson
This function creates a new geojson object with our data and the specific features/columns that we want to display

Now we can finally show this data in our Streamlit app. In this case, I want to give pydeck a DataFrame with only what we want to show on the map. I turn this geojson into a DataFrame and add fill colors as another column, and then I can create a layer for our shapes and pass that into the st.pydeck_chart() function along with a tooltip to show the value of the feature.

    polygon_layer = pdk.Layer(
        "PolygonLayer",
        geo_df,
        get_polygon="coordinates",
        filled=True,
        stroked=False,
        opacity=0.5,
        get_fill_color='fill_color',
        auto_highlight=True,
        pickable=True,
    )
    # The brackets here are expected for pdk, so string formatting is less friendly
    tooltip = {"html": "<b>County:</b> {name} </br>" + "<b>" + str(map_feature) + ":</b> {" + str(map_feature) + "}"}

    r = pdk.Deck(
        layers=[polygon_layer],
        initial_view_state=view_state,
        map_style=pdk.map_styles.LIGHT,
        tooltip=tooltip
    )
    st.pydeck_chart(r)

Once our data is properly formatted, turning it into a map is pretty straightforward:

Because the functions to create the map are generalized, it makes expanding on them much easier. For example, if we want to create a map with multiple layers, we could update the existing make_map function to accept a list of features to map and then create multiple of Layers instead of just one. We can use these functions like a component in React or Angular to show different information in multiple places. In fact, I did exactly that to create map view on the Data Explorer page to allow users to see any feature by just changing the inputs to the functions.

Making data accessible

This project initially aimed to collect disparate data sources and make our analysis easier for anyone, but not everyone has the Python and data skills to use it on its own. Streamlit allowed us to replace a scary command line interface with a more familiar and functional web app, hopefully allowing more users to interact with data. It also gave us the ability to show the data in more interactive and intuitive ways than we could if users had to run the project locally. While we hope this will help people address the eviction crisis, we think this data also can help address other social problems in policy-making, planning, and other fields.

Arup decided to make this project open because we believe we can have a bigger impact by working together, so we’re eager to work with the community to make this tool more useful. If you have ideas for new functionality, let us know or better yet, contribute to the repository. You can find the app here. The coming months could be a very scary time for lots of people, and we hope that this tool may help in some way to keep more people in their homes.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

gif-2.gif (947×633)
https://blog.streamlit.io/content/images/2021/08/gif-2.gif#browser


Monthly rewind > July 2022
https://blog.streamlit.io/monthly-rewind-july-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > July 2022

Your July look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, August 9 2022
🏆 App of the month 🏆
Streamlit July updates
🔍 Current release: 1.11.1
🔮 Upcoming
💻 #30DaysofStreamlit in French
⭐️ Streamlit reached 20k GitHub Stars!
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our July featured app of the month is... 🥁🥁🥁🥁🥁

Visualizing your non-linear career by Michael Condon!

This app lets you create a timeline of your career, highlighting the top skills used within a job role. The generated polar graph visualizes the amount of experience in each skill set. [code]

Streamlit July updates

Here are some new features and highlights from July!

🔍 Current release: 1.11.1

The latest release is 1.11.1. Recent updates include tab containers for your app via st.tabsand the ability to set a gap size between columns. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

These new features are currently on the horizon:

Unique and custom subdomains
Replaying of cached st. calls
User-resizable st.sidebar

Check out our roadmap app to see what else we're working on. 🥳

💻 #30DaysofStreamlit in French
The 30 Days of Streamlit app is now available in French!

⭐️ Streamlit reached 20k GitHub Stars!

Featured Streamlit content
Check out the backstory behind Qiusheng's popular geospatial app and learn how Streamlit helped optimize it.

Get past Google Search Console export limitations and get more out of your data with Charly's new Streamlit app!

Learn about the fundamental st.write and magic commands with Chanin (Data Professor) in this new tutorial.

Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Saša explains how to Use Streamlit to Visualise your ML Models Running on Snowflake in his Medium article.
Get quantitative analysis of the Brazilian financial market with Roberto's AnáliseQuant app.
Learn How to Add a Background Image to Your Streamlit App by reading Sharone's tutorial.
Bogdan's Options Calculator lets you scrape live option chains from Yahoo Finance and visualize the bid/ask/last traded price for different strike prices.
In Khuyen's tutorial you'll learn how to Build a Robust Workflow to Visualize Trending GitHub Repositories in Python filtered by your favorite language.
Use Mykola's STAC Discovery app to browse and discover data from the Microsoft Planetary Computer Catalog.
Arvindra created a Media Explorer App—a hybrid architecture media server, media service, and Streamlit client app by using FastAPI and Python.
The Renewcast app by Giannis provides forecasts for renewable electricity generation in European countries.
Danny goes over Using Streamlit and Plotly to Create Interactive Candlestick Charts in his tutorial.
Billy's Surfboard Volume Calculator predicts the best surfboard volume for you based on your user input.
The Global Fund API Explorer from Adrien visualizes disbursements information from the GF API with several filter options.
David's Espectro app allows users to query spectral indices and visualize the required bands for an index computation.
The Gene Updater app by Kuan and team autocorrects and updates Excel misidentified gene names.
Select two different emojis and get a combination of them with Zachary's Emoji Kitchen Streamlit app.


Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > August 2022
https://blog.streamlit.io/monthly-rewind-august-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > August 2022

Your August look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, September 7 2022
🏆 App of the month 🏆
Streamlit August updates
🔍 Current release: 1.12.0
🔮 Upcoming
📊 New and improved built-in charts
🎙 Take a deep dive into data science with Adrien
Featured Streamlit content
✨ The magic of working in open source
🔽 Auto-generate a dataframe filtering UI in Streamlit
💻 Make dynamic filters in Streamlit and show their effects on the original dataset
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our August featured app of the month is... 🥁🥁🥁🥁🥁

What songs were popular when I was in high school? by Robert Ritz!

Having a hard time discovering playlists on Spotify? Robert's app lets you choose a time period and then auto-generates a playlist of the top songs from those years. Check it out to discover new music or if you want to go on a trip down memory lane. [code]

Streamlit August updates

Take a look at these updates and highlights from August!

🔍 Current release: 1.12.0

The latest release is 1.12.0. Recent updates include improved built-in charts, resizable side bar, and the ability to put static elements into functions that are cached via st.experimental_memo or st.experimental_singleton. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

These new features are currently on the horizon:

Unique and custom subdomains
Ability to hide space for an empty label
Apps are indexed by Google

Visit our roadmap app to see what else we're working on. 🥳

📊 New and improved built-in charts
Release 1.12.0 brings newly improved built-in charts. Line, area, and bar charts have gained a new design as well as parameters 'x' and 'y' for more control.

🎙 Take a deep dive into data science with Adrien
A Deep Dive into Data Science with Adrien Treuille, CEO and Co-founder of Streamlit (Acquired by Snowflake) - The Data Cloud Podcast
In this episode, Adrien Treuille, Co-founder of Streamlit (acquired by Snowflake), shares how business people can use data science and machine learning, how you should incorporate data scientists into your organizations, and so much more.
Snowflake
Featured Streamlit content

✨ The magic of working in open source
See what it's like to work in open source! Ken writes about how new features are implemented, the importance of community, and how you can contribute.

🔽 Auto-generate a dataframe filtering UI in Streamlit
Want to add a filtering UI to your dataframe without a bunch of code iterations? Learn from Tyler, Arnaud, and Zachary about the filter_dataframe function.

💻 Make dynamic filters in Streamlit and show their effects on the original dataset
Vladimir guides you how to quickly and easily add dynamic filters to your Streamlit app.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Avery walks you through how to make a Sports Analytics Project With Python & Streamlit that will look great on your data science portfolio.
Learn from Jesse how to Build a Static Code Analysis App with Streamlit and Python in his YouTube tutorial.
Timotius' app visualizes various aspects of Gender Equality in Indonesia.
Explore the data of the Vietnam National Team with Daryl's app.
Eugenia teaches you How to build a Web App to Transcribe and Summarize audio with Python in her article.
The Agriculture, Water, and Climate Group at the University of Manchester created AquaPlan to enable farmers, businesses, and governments to make more informed decisions about water.
Add a Custom Streamlit Background Image/Color Gradient through CSS to your apps by following Fanilo's video tutorial.
Get quick overview of your Netflix data with Sebastian's Netflix Profile Analysis app.
Generate unique QR codes with advanced customization using the Flash QR app by Gerard.
With William's Shakespeare Search Engine app, you can semantically search the works of Shakespeare.
Learn and share more about your personality by taking the tests in Rinku's Findself app.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > September 2022
https://blog.streamlit.io/monthly-rewind-september-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > September 2022

Your September look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, October 7 2022
🏆 App of the month 🏆
Streamlit September updates
🔍 Current release: 1.13.0
🔮 Upcoming
Featured Streamlit content
🧩 How to build your own Streamlit component
📦 Streamlit App Starter Kit: How to build apps faster
💻 How to build Streamlit apps on Replit
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our September featured app of the month is... 🥁🥁🥁🥁🥁

Abstractonator by Crossref Labs!

This app is a game that tests if you can distinguish machine-generated content (MGC) from human-generated content (HGC). Try to guess which scholarly abstracts are the human-generated originals and which are AI-generated! [code]

Streamlit September updates

Let's take a look at some updates and highlights from September.

🔍 Current release: 1.13.0

The latest release is 1.13.0. Recent updates include the ability to optionally hide widget labels and to stretch st.dataframe across the full container width. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

We're currently working on these new features:

Streamlit theme for 3rd party charting libraries
Apps are indexed by Google

Visit our roadmap app to see what else we're working on. 🥳

Featured Streamlit content

🧩 How to build your own Streamlit component
Learn how to create your own custom component in this tutorial from Zachary.

📦 Streamlit App Starter Kit: How to build apps faster
Save 10 minutes every time you build an app with the Data Professor's Streamlit App Starter Kit—a GitHub template to kick-start your creations!

💻 How to build Streamlit apps on Replit
Easily build apps on Replit and explore Streamlit elements with the Beginner Template Tour from Streamlit Creator Shruti.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Fanilo tackles a bunch of community questions in his video Streamlit Pro answers YOUR questions & messages • 1k AMA.
Sephora's NLLB-200 translator for lyrics demo app lets you translate lyrics from popular songs to 200 languages, including low-resource languages.
Roll the dice and see the distribution of the sum of die results with Bruno's Playing dice! app.
With Koen's SEO A/B Test Analyzer you can validate almost all SEO changes and optimizations before they're implemented on your website.
Valeria's Tarot Reading App will give you a three-card spread on your past, present, and future.
Use Mykola's app to discover Land Surface Temperature - River Basins in Europe and the USA.
Nicholas attempts a fun coding challenge in his YouTube video: I tried to build a Python Machine Learning Streamlit App in 7 Minutes.
Yuichiro's stlite sharing lets you write and share Streamlit apps online serverlessly!
Sam's tutorial Python-Based Data Viz (With No Installation Required) walks you through setting up a Streamlit app to share with stlite.
With Zakaria's Distillation Column app, you can estimate the required performance of a given system for a specified reflux rate and then use the values to help size a complete distillation column.
In his YouTube tutorial, Fede teaches you How to code and deploy a Streamlit App in less than 20 minutes!
Arvindra made a mini-tutorial app to help with Initializing widget values and getting them to stick without double presses.
Arjune's first Streamlit app, HR Analytics, visualizes a summary of each department and each employee in the organization.
Use Michael's Drug Target Prediction app to find molecules with similar structural properties that can serve as a source for new drugs.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > October 2022
https://blog.streamlit.io/monthly-rewind-october-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > October 2022

Your October look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, November 8 2022
🏆 App of the month 🏆
Streamlit October updates
🔍 Current release: 1.14.0
🔮 Upcoming
📦 The next frontier for Streamlit
🪢  Discover and share useful bits of code with the streamlit-extras library
🖌️ Prototype your app in Figma!
Featured Streamlit content
💻 How to build Streamlit apps on Replit
🧩 uPlanner fosters data processing innovation with Streamlit
📄 Build a Streamlit Form Generator app to avoid writing code by hand
📄 How to make a Streamlit links page
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our October featured app of the month is... 🥁🥁🥁🥁🥁

Sophisticated Palette by Siavash Yasini!

This app creates color palettes from famous artwork or any image of your choice! Simply choose an image and get inspiration for the colors of your projects. You can adjust the colors of the image and customize the palette and sample pixel size. Copy the code snippet provided in the app to adopt the colors into your matplotlib and plotly charts. [code]

Streamlit October updates

Check out the latest updates and releases from October.

🔍 Current release: 1.14.0

The latest release is 1.14.0. Recent updates include the ability to set st.button and st.form_submit_button as primary or secondary, and the ability to limit the number of options selectable for st.multiselect. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

We're currently working on these new features:

Apps are indexed by Google
Markdown in widget labels
Streamlit theme for 3rd party charting libraries

Visit our roadmap app to see what else we're working on. 🥳

📦 The next frontier for Streamlit
We're excited to announce our feature roadmap for 2023 and beyond! What lies ahead for Streamlit? We’re thinking bigger than ever.

🪢  Discover and share useful bits of code with the streamlit-extras library
Check out the streamlit-extras library by Arnaud—a collection of small but mighty custom components to give your apps that extra something!

🖌️ Prototype your app in Figma!
Want to make sure your app's design is on point before jumping into the code? Check out the Streamlit Design System in Figma to prototype ahead of time.


Featured Streamlit content

💻 How to build Streamlit apps on Replit
Easily build apps on Replit and explore Streamlit elements with the Beginner Template Tour from Streamlit Creator Shruti.

🧩 uPlanner fosters data processing innovation with Streamlit
See how Streamlit enables Sebastian's team at uPlanner to build apps for custom scripts and simplify the user experience, resulting in faster innovation.

📄 Build a Streamlit Form Generator app to avoid writing code by hand
Learn from Gerard how to build a Streamlit Form Generator—an app for making code that accepts and validates user inputs according to an API spec.

📄 How to make a Streamlit links page
Share all of your social links in one place! Learn how to make a Streamlit links page in this tutorial from the Data Professor. You can also add a custom subdomain!

Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

You can explore Cycling Rates in London and customize your maps with Lisa's app.
Vansh's app lets you visualize Solar Flux Data from the Canadian Space Agency.
In his video tutorial, Andrej explains how to go about the Streamlit Python App Setup.
Create beautiful timelines with bi-directional communication using Qiusheng's New Component: Streamlit Timeline.
Input a YouTube link to Batuhan's Auto Subtitled Video Generator and it will display the video with subtitles.
Intellipaat's video tutorial takes you through Building a Machine Learning Web Application using Streamlit.
Generate synthetic walking trials of healthy synthetic subjects using Luca's app GaitGAN: Generating Gait Data by Generative Adversarial Networks for Data Augmentation in Biomechanics.
See Vizzu's animated story in a Streamlit app showcasing Data Scientists’ Presentation Tools.
Carlos created a simple Toggle Switch Component using React/Material-UI.
Shruti's Heart-shaped toggle switch component adds a fun shape to the toggle switch.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > June 2022
https://blog.streamlit.io/monthly-rewind-june-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > June 2022

Your June look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, July 6 2022
🏆 App of the month 🏆
Streamlit June updates
🔍 Current release: 1.10.0
🔮 Upcoming
📈 Multipage apps
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our June featured app of the month is... 🥁🥁🥁🥁🥁

Learning log by Tyler Hillery!

Inspired by GitHub's contribution graphs, Tyler created this app to track the time he spends on learning. You can see the date of the learning session, the length of time, what was studied and where, along with his contextual notes. The app keeps Tyler accountable to his learning goals and helps visualize his progress! [code]

Streamlit June updates

A lot of exciting things happened in June! Let's take a look.

🔍 Current release: 1.10.0

The latest release is 1.10.0. Recent updates include native support for multipage apps and the redesigned st.dataframe. Be sure to check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

These new features are currently on the horizon:

Unique and custom subdomains
Replaying of cached st. calls
User-resizable st.sidebar

Check out our roadmap app to see what else we're working on. 🥳

📈 Multipage apps
The long-awaited multipage feature has finally arrived! Now you can quickly and easily add more pages to your Streamlit apps.

Featured Streamlit content
Make your static charts interactive using st.pyplot.

Read how Marcelo started his career in data science and how Streamlit helps further his projects at iFood.

Learn how Samuel made an interactive app to teach students about Earth observation—using multiple Streamlit features and community components!


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Get a glimpse at reported UFO sightings with Tyler's UFO Sightings Dashboard in Streamlit.
With Subham's Data Pre-processing and Model Selection Web App you can see the impact of different parameters on different models and select the best combination.
Learn how to Build your own Fast Text Reader with Streamlit and Spacy in Ramsri's YouTube tutorial.
Get started with Streamlit using Shruti's Streamlit Beginner Template Tour.
Anirudh's app allows you to Visualize Sound in an image—including in real-time!
Follow Dr. W.J.B. Mattingly's video to learn How to Make a Multi-Page App with Streamlit Pages in Python—The BEST Streamlit Update.
Create network graphs with data from Genshin Impact in MiracleXYZ's Genshin Analysis app.
Shreya teaches ways to customize your apps in her Microblog: Customising Python Streamlit Dashboards with Custom Themes and More!
Banjo walks through creating a Streamlit app and showcases the flexibility for building solutions in his talk Building Interactive Apps in Python with Streamlit.
Get help with cheminformatics tasks using Suneel's rdKit CheatSheet.
Easily create apps with Sebastian's Template for a multipaging Streamlit app.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mark von Oven - Streamlit
https://blog.streamlit.io/author/vonoven/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mark von Oven
1 post
Twitter
The Stable solves its data scalability problem with Streamlit

How Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days

Case study
by
Mark von Oven and 
1
 more,
April 28 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Wissam Siblini - Streamlit
https://blog.streamlit.io/author/wissam/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Wissam Siblini
1 post
Wissam Siblini uses Streamlit for pathology detection in chest radiographs

Learn how Wissam detected thoracic pathologies in medical images

Case study
by
Wissam Siblini and 
1
 more,
May 3 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly Rewind - Streamlit (Page 2)
https://blog.streamlit.io/tag/monthly-rewind/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Monthly Rewind
27 posts
Monthly rewind > May 2022

Your May look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
June 7 2022
Monthly rewind > April 2022

Your April look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
May 5 2022
Monthly rewind > March 2022

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 7 2022
Monthly rewind > February 2022

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 7 2022
Monthly rewind > January 2022

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 7 2022
Monthly rewind > December 2021

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 7 2022
Monthly rewind > November 2021

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2021
Monthly rewind > October 2021

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2021
Monthly rewind > September 2021

Your September look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2021
Monthly rewind > August 2021

Your August look back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

gif-1-2.gif (957×634)
https://blog.streamlit.io/content/images/2021/08/gif-1-2.gif#browser


Radius-Explorer-GIF-1.gif (1920×1119)
https://blog.streamlit.io/content/images/2022/04/Radius-Explorer-GIF-1.gif#border


Chest-Radiographs-Classifier-GIF.gif (1724×729)
https://blog.streamlit.io/content/images/2022/05/Chest-Radiographs-Classifier-GIF.gif#border


st.camera_input.png (2514×1510)
https://blog.streamlit.io/content/images/2022/01/st.camera_input.png


trs_mark-1.jpeg (1368×470)
https://blog.streamlit.io/content/images/2022/04/trs_mark-1.jpeg#border


hack_day2-1.jpeg (1049×612)
https://blog.streamlit.io/content/images/2022/04/hack_day2-1.jpeg#border


patent_pending-5.jpeg (800×800)
https://blog.streamlit.io/content/images/2022/04/patent_pending-5.jpeg#border


Gallery--10-FPS-.gif (1700×991)
https://blog.streamlit.io/content/images/2022/05/Gallery--10-FPS-.gif#border


Image-13.01.22.png (1247×951)
https://blog.streamlit.io/content/images/2022/01/Image-13.01.22.png


sign-in-2.png (2000×1112)
https://blog.streamlit.io/content/images/2022/01/sign-in-2.png


gravitational.gif (2878×1530)
https://blog.streamlit.io/content/images/2022/08/gravitational.gif#browser


How to quickly deploy and share your machine learning model for drug discovery
https://blog.streamlit.io/how-to-quickly-deploy-and-share-your-machine-learning-model-for-drug-discovery/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to quickly deploy and share your machine learning model for drug discovery

Share your ML model in 3 simple steps

By Sebastian Ayala Ruano
Posted in Advocate Posts, December 15 2022
How to create a user-friendly interface for retrieving molecular inputs
How to display important data from the input molecules
How to deploy and use an ML model for predicting potential drugs
Wrapping up
Contents
Share this post
← All posts

Drug discovery is a long and complicated process. Bringing a new drug to the market can take decades and cost millions. Antimicrobial peptides or AMPs (bioactive drugs that control infectious diseases caused by superbugs) and ML algorithms can help, but it’s hard to deploy computational models for public feedback.

That’s why I built AMPredST—a web application that predicts antimicrobial activity and general AMP properties (based on my previous project).

In this post, I’ll show you how Streamlit can extend your ML models’ use for drug discovery. You’ll learn:

How to create a user-friendly interface for retrieving molecular inputs
How to display important data from the input molecules
How to deploy and use an ML model for predicting potential drugs
👉
Can’t wait to try it out? Here’s the app and the repository with all the code.
How to create a user-friendly interface for retrieving molecular inputs

First, use the st.session_state to store the sequence input and share this value between the app’s reruns. Then add functions for defining default values of active and inactive peptides and removing the input sequence.

Include your app’s inputs and default functionalities in the sidebar:

An input widget ( st.sidebar.text_input) for entering the peptide sequence
Three buttons (st.sidebar.button) for implementing the previously defined functions

After getting the input, the main page of the app will show a sub-header (st.subheader) and an informational message (st.info) of the input sequence (these values change depending on the information provided in the input widget from the sidebar):

if 'peptide_input' not in st.session_state:
  st.session_state.peptide_input = ''

# Input peptide
st.sidebar.subheader('Input peptide sequence')

def insert_active_peptide_example():
    st.session_state.peptide_input = 'LLNQELLLNPTHQIYPVA'

def insert_inactive_peptide_example():
    st.session_state.peptide_input = 'KSAGYDVGLAGNIGNSLALQVAETPHEYYV'

def clear_peptide():
    st.session_state.peptide_input = ''

peptide_seq = st.sidebar.text_input('Enter peptide sequence', st.session_state.peptide_input, key='peptide_input', help='Be sure to enter a valid sequence')
st.sidebar.button('Example of an active AMP', on_click=insert_active_peptide_example)
st.sidebar.button('Example of an inactive peptide', on_click=insert_inactive_peptide_example)
st.sidebar.button('Clear input', on_click=clear_peptide

if st.session_state.peptide_input == '':
  st.subheader('Welcome to the app!')
  st.info('Enter peptide sequence in the sidebar to proceed', icon='👈')
else:
  st.subheader('⚛️ Input peptide:')
  st.info(peptide_seq)


How to display important data from the input molecules

After getting the input sequence, you can calculate the properties that characterize the molecule. Analyze the protein sequences from the Biopython library with functions like molecular_weight, gravy, or aromaticity (read more here).

Use st.metric with st.columns to display these quantities:

if st.session_state.peptide_input != '':
		# General properties of the peptide
		st.subheader('General properties of the peptide')
		analysed_seq = ProteinAnalysis(peptide_seq)
		mol_weight = analysed_seq.molecular_weight()
		aromaticity = analysed_seq.aromaticity()
		instability_index = analysed_seq.instability_index()
		isoelectric_point = analysed_seq.isoelectric_point()
		charge_at_pH = analysed_seq.charge_at_pH(7.0)
		gravy = analysed_seq.gravy()
		
		col1, col2, col3 = st.columns(3)
		col1.metric("Molecular weight (kDa)", millify(mol_weight/1000, precision=3))
		col2.metric("Aromaticity", millify(aromaticity, precision=3))
		col3.metric("Isoelectric point", millify(isoelectric_point, precision=3))
		
		col4, col5, col6 = st.columns(3)
		col4.metric("Instability index", millify(instability_index, precision=3))
		col5.metric("Charge at pH 7", millify(charge_at_pH, precision=3))
		col6.metric("Gravy index", millify(gravy, precision=3))


Streamlit has functions for creating interactive plots using Python libraries like Matplotlib, Altair, or Plotly. Create an interactive bar plot of the input peptide’s amino acid composition with Plotly (st.plotly_chart):

# Bar plot of the aminoacid composition
count_amino_acids = analysed_seq.count_amino_acids()

st.subheader('Aminoacid composition')

df_amino_acids =  pd.DataFrame.from_dict(count_amino_acids, orient='index', columns=['count'])
df_amino_acids['aminoacid'] = df_amino_acids.index
df_amino_acids['count'] = df_amino_acids['count'].astype(int)

plot = px.bar(df_amino_acids, y='count', x='aminoacid',
        text_auto='.2s', labels={
                "count": "Count",
                "aminoacid": "Aminoacid"
            })
plot.update_traces(textfont_size=12, textangle=0, textposition="outside", showlegend=False)
st.plotly_chart(plot)


Finally, calculate the frequency of the amino acids from the peptide sequence and show the dataframe with st.write. The ML algorithm will use this feature matrix as input to predict the activity of the peptide sequence:

# Calculate Amino acid composition feature from the AMP
amino_acids_percent = analysed_seq.get_amino_acids_percent()

amino_acids_percent = {key: value*100 for (key, value) in amino_acids_percent.items()}

df_amino_acids_percent =  pd.DataFrame.from_dict(amino_acids_percent, orient='index', columns=['frequencies']).T

st.subheader('Molecular descriptors of the peptide (Amimoacid frequencies)')

st.write(df_amino_acids_percent)


How to deploy and use an ML model for predicting potential drugs

Now, you can use your ML to predict promising new drugs. But first, ensure your model’s file size doesn’t exceed GitHub limits (to deploy to Streamlit Community Cloud).

Compress the model’s bin file into a zip file and use the zip file library to load the model into the script (or use the Git Large File Storage extension):

# Function to unzip the model
@st.experimental_singleton
def unzip_model(zip_file_name):
    # opening Zip using 'with' keyword in read mode
    with zipfile.ZipFile(zip_file_name, 'r') as file:
        # printing all the information of archive file contents using 'printdir' method
        print(file.printdir())
        # extracting the files using 'extracall' method
        print('Extracting all files...')
        file.extractall()
        print('Done!') # check your directory of a zip file to see the extracted files

# Assigning zip filename to a variable
zip_file_name = 'ExtraTreesClassifier_maxdepth50_nestimators200.zip'

# Unzip the file with the defined function
unzip_model(zip_file_name)


Next, load the ML model using the pickle library and predict the peptide activity:

# Function load the best ML model
@st.experimental_singleton 
def load_model(model_file):
  with open(model_file, 'rb') as f_in:
      model = pickle.load(f_in)
  return model

# Load the model
model_file = 'ExtraTreesClassifier_maxdepth50_nestimators200.bin'
model = load_model(model_file)

y_pred = model.predict_proba(df_amino_acids_percent)[0, 1]
active = y_pred >= 0.5


I recommend you create cache-decorated functions for the previous tasks using the st.experimental_singleton decorator, which will store an object for each function and share it across all users connected to the app.

Finally, show the model’s results on the app’s main page. If you want, color the text to highlight the active and inactive resulting peptides using the amazing annotated_text Streamlit component:

# Print results
st.subheader('Prediction of the AMP activity using the machine learning model')
if active == True:
  annotated_text(
  ("The input molecule is an active AMP", "", "#b6d7a8"))
  annotated_text(
  "Probability of antimicrobial activity: ",
  (f"{y_pred}","", "#b6d7a8"))
else:
  annotated_text(
  ("The input molecule is not an active AMP", "", "#ea9999"))
  annotated_text(
  "Probability of antimicrobial activity: ",
  (f"{y_pred}","", "#ea9999"))


Wrapping up

Congratulations! You’ve learned how to deploy your ML models for drug discovery in an app that can help the entire scientific community!

If you have any comments or suggestions, please leave them below in the comments, email me, DM me on Twitter, or create an issue in the repo.

Thank you for reading, and happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

how-to-use-roboflow.gif (1052×624)
https://blog.streamlit.io/content/images/2022/08/how-to-use-roboflow.gif#browser


Untitled--1-.png (569×651)
https://blog.streamlit.io/content/images/2022/04/Untitled--1-.png#border


scienceio-app-1.gif (850×1090)
https://blog.streamlit.io/content/images/2023/01/scienceio-app-1.gif#border


dashboard_model_2.PNG (1843×884)
https://blog.streamlit.io/content/images/2022/05/dashboard_model_2.PNG#browser


csv2excel.gif (1660×800)
https://blog.streamlit.io/content/images/2022/10/csv2excel.gif#browser


Building robust Streamlit apps with type-checking
https://blog.streamlit.io/building-robust-streamlit-apps-with-type-checking/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building robust Streamlit apps with type-checking

How to make type-checking part of your app-building flow

By Harald Husum
Posted in Advocate Posts, November 10 2022
The methods of detecting defects
Static analysis vs. dynamic analysis
Type-checking
What does this have to do with Streamlit anyway?
How to make type-checking part of your app-building flow
Step 1. Understanding type hints
Step 2. Installing mypy
Step 3. Running mypy
An example from the world of Streamlit
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Harald Husum. I’m a Machine Learning Engineer at Intelecy and a Streamlit Creator.

Have you ever come across an app that promises to solve your problem, only to have it crash as you try it out? Nothing ruins the experience like encountering defects. As app developers, we’re painfully aware of this. Defects are an ever-present concern, so having efficient routines for weeding them out is a must.

In this post, I’ll share a powerful technique for eliminating defects—type-checking—and how to make it a part of your app-building flow.

Let’s get started!

💡
If you want to get a feel for how type-checking works, take a look at my app which lets you type-check Streamlit code directly from your browser (without installing the required tooling). If you’re interested in the code, here’s a repo as well.
The methods of detecting defects

For some classes of software defects, the only cure is meticulous testing.

When creating an app, writing and running tests should always be part of your toolbox. Not sure about how to test Streamlit apps? These tutorials can help you get started. But testing can be time-consuming, and covering various edge cases of a large code base isn’t always feasible. As software developers, we’re often time-constrained, so we want fast and efficient methods for detecting defects.

Luckily, testing isn’t the only technique that can surface problems in your code.

Static analysis vs. dynamic analysis

When you want to uncover as many issues as possible with as little work as possible, static analysis tools are your friends.

Static analysis is the process of analyzing code without actually running it. Compare it to testing—or dynamic analysis—where to gain any insight into the code, you have to execute it. Of course, static analysis tools can’t discover all your problems, so you should still write tests. But you can catch a lot of issues without investing much effort.

Within the context of Python, linting is probably the most common form of static analysis. You might’ve come across linting tools like Pylint and Flake8. They help detect potential quality issues in your code. Code formatters like YAPF and Black also rely on static analysis. However, automatic linting and formatting only scratch the surface of what static analysis can do for you, as you’ll see shortly. 😊

Type-checking

In the last decade, Python has seen a popularity boom. As a result, Python increasingly sees use in large and complex projects, where demands for code quality are higher. This trend drives the development and adoption of static analysis techniques. Type-checking is one such technique that’s gaining traction in the Python community.

Type-checking ensures that interacting with objects doesn’t lead to obvious errors. For example, calling x.lower() will work if x is of type str. But if x is an int, you’ll trigger the following exception:

AttributeError: 'int' object has no attribute 'lower’

You might ask, “Who would try to call the lower method on an integer?” But variants of this problem happen all the time—by accident—when you write or refactor Python code. Embracing type-checking will let you avoid such errors.

Like other forms of static analysis, type-checking is usually performed by a tool—a type checker. There are many type checkers in the Python ecosystem. Of note are PyType, Pyright, and Pyre. But the most popular is probably mypy. It’s been around the longest and supports most of the latest typing features in Python. It’ll help you make your Streamlit projects less prone to failure.

What does this have to do with Streamlit anyway?

Good question! Although type checkers can identify issues in your regular old Python code, they work best when your code is annotated with type hints. Type hints inform the type checker about the intention behind the code and allow the type checker to verify that the code functions as intended.

Over the last few months, the community has done a lot of work annotating Streamlit’s public APIs with type hints. Type checkers can now correctly understand what types are expected by Streamlit functions and what types are returned. Consequently, type checkers have gone from being somewhat useful to really useful for checking code that interacts with Streamlit, like your apps. (I expect the Streamlit code base annotations to improve with time).

If you're not already type-checking your Streamlit apps, now is a good time to start!

How to make type-checking part of your app-building flow

To get a feel for type-checking without committing to anything, use my app to type-check Streamlit code snippets from a browser:

Go to https://typing-playground.streamlitapp.com/
Paste in a code snippet
Press “Type-check with mypy”
Observe how mypy feels about your code

The process should look something like this:

The app internals are fairly simple. The user input is passed on to mypy for type checking. The resulting type-checking report is somewhat prettified before being presented to the user (check out the repo for details).

Now, let’s make type-checking a part of your Streamlit app-building workflow.

Step 1. Understanding type hints

Earlier, I mentioned that type hints are essential to get the most out of a type checker. Let’s have a closer look at Python-type hints and what they express (if you’re using Python < 3.10, some examples won't work because of the union operator for types (|) addition, unless you add from __future__ import annotations to the top of your file).

The first is a variable hint. It looks something like this:

n: float = 42.42

The variable n should always have a float value.

By adding : float after your variable name, you inform the type checker that you intend that n should always remain a float. Should you make a mistake and write something like n = “I’m no float” later in your code, mypy will inform you that you made a mistake:

error: Incompatible types in assignment (expression has type "str", variable has type "float")

In practice, you won't have to annotate n like this because mypy implicitly assumes that variables have the type of whatever value you assign them first. But knowing about variable hints will let you override mypy when this assumption doesn't hold.

For instance, imagine that we want to turn n into a constant and disallow changing it at all. We can achieve this by way of a special variable hint:

from typing import Final

N: Final = 42.42
N is annotated as a constant.

If we now try to change N in any way, it will lead to mypy complaining:

N = 42  # error: Cannot assign to final name "N"
Mypy stops us from overriding a constant.

Another form of type hints is the one used on functions.

def greet(name: str | None) -> str:
    if name is None:
        name = "mysterious stranger"
    return f"Happy Streamlit-ing, {name}! 🎈"


Here, name: str | None means that the function has a parameter, name, and that it’s expected to be passed an argument that’s either an instance of str or None. The function also has a return type annotation -> str, which implies that it’ll always return an instance of str.

Adding these hints to the function means that mypy will protect you against two potential mistakes:

# Trying to pass in a type the function isn't intended to support
# Mypy - error: Argument 1 to "greet" has incompatible type "int"; 
#   expected "Optional[str]"
greet(42)

# Misusing the returned greeting
# Mypy - error: "str" has no attribute "a_method_strings_dont_have"
greet("John Doe").a_method_strings_dont_have()


Let’s say you make the following change to the greet function:

def greet(name: str | None) -> str:
    if name is None:
        # No greetings for secretive people
        return None
    return f"Happy Streamlit-ing, {name}! 🎈"


Mypy will notice that you say you’re going to return a str. But in practice, you can also return a None. Mypy will warn you: error: Incompatible return value type (got "None", expected "str").

This is helpful because it forces you to think carefully about your change. Either greet should always return a str, in which case you need to take a step back. Or, if you intended to create this new behavior, update the function hints to def greet(name: str | None) -> str | None. As a result, all usages of greet that don't gracefully handle the function returning None will need to be updated.

This might sound scary, but mypy will point out any locations in your code base where you might be using the return value from greet in a way that assumes it’s a str.

💡
There’s more to type hints. Take a look at PEP484—the main specification for type hints in Python.
Step 2. Installing mypy

To type-check your Streamlit project, you need to install mypy. It should be easy, as it's just another Python dependency. Just use pip:

$ python3 -m pip install mypy


Or use Poetry to manage your dependencies:

$ poetry add mypy --group dev

Step 3. Running mypy

Mypy can analyze a single file:

$ mypy program.py


Or it can analyze all Python code in a directory:

$ mypy my_project


Try running it on your code and see if it reports any errors. If it doesn't, you're a better developer than me. 😉

An example from the world of Streamlit

To move from theoretical to practical, I want to share a type of error I experienced in one of my Streamlit apps. It could've been avoided with the type hints added in the latest Streamlit version. I can't share the actual code, so I'll share an imagined example.

Here is a simple app to explore the pastry collections of a few local bakeries. It lets me select a bakery and one of its pastries to see the information about it:

"""This is the code for our bakery app"""
from dataclasses import dataclass

import streamlit as st

@dataclass
class Pastry:
    name: str
    description: str

@dataclass
class Baker:
    name: str
    pastries: list[Pastry]

bakers = [
    Baker(
        name="Eager Bakery",
        pastries=[
            Pastry(
                name="Cinnamon Bun",
                description="The best there is",
            ),
            Pastry(
                name="Magic Muffin",
                description="Putting sparkles back in your day.",
            ),
            Pastry(
                name="Dreamy Donut",
                description="Never drink your coffee without it.",
            ),
        ],
    ),
    Baker(
        name="Lazy Bakin'",
        pastries=[],
    ),
]

baker = st.sidebar.selectbox(
    label="Select a bakery:",
    options=bakers,
    format_func=lambda b: b.name,
)

pastry = st.sidebar.selectbox(
    label=f"Select one of {baker.name}'s pastries:",
    options=baker.pastries,
    format_func=lambda p: p.name,
)
st.write(pastry.name)
st.write(pastry.description)


Take a look at the code. Can you spot a subtle defect? As you interact with the app, it seems there are no problems:

But when you switch the bakeries, an error strikes:

Instead of information about a pastry, you get an AttributeError. How annoying!

When I encountered this error in my app, I was surprised to learn the cause. It turns out that streamlit.selectbox will return None if the options sequence you pass to it is empty. In this case, since Lazy Bakin' disappointingly hasn't developed any pastries yet, there are no options to select in the pastry selectbox. So instead of a pastry to render, you get None.

I found this through a runtime error—in production, no less. But with the latest version of Streamlit and the support of mypy, you won't have to. Running mypy yields four errors:

pastries.py:50: error: Item "None" of "Optional[Baker]" has no attribute "name"
pastries.py:51: error: Item "None" of "Optional[Baker]" has no attribute "pastries"
pastries.py:54: error: Item "None" of "Optional[Any]" has no attribute "name"
pastries.py:55: error: Item "None" of "Optional[Any]" has no attribute "description"


To summarize, mypy is alerting you to the fact that both your baker and pastry variables might be set to None. So if mypy is part of your project's CI/CD pipeline, you'll avoid releasing code like this to your users.

Here is how to adapt your code to avoid these errors:

baker = st.sidebar.selectbox(
    label="Select a bakery:",
    options=bakers,
    format_func=lambda b: b.name,
)

# Since we know that `bakers` is a non-empty list, the errors mypy points out 
# for `baker` are technically false positives. This is easy enough to deal with.
# By asserting that baker is not None, mypy is placated. And, in practice, the 
# assert should never fail.
assert baker is not None

pastry = st.sidebar.selectbox(
    label=f"Select one of {baker.name}'s pastries:",
    options=baker.pastries,
    format_func=lambda p: p.name,
)

# For the `pastry` case, we must show a bit more care. We now know that we can 
# actually get a None value here. What you want to do when that happens is up 
# to you. In my case, I just choose not to print anything. Moving the 
# `st.write` calls behind a conditional is all that is needed to make mypy happy.
if pastry is not None:
    st.write(pastry.name)
    st.write(pastry.description)


That's it!

Wrapping up

You've now seen how type-checking can help you build more robust Streamlit apps! 🎉

Thank you for taking the time to read this article. I hope that it'll help you with your current and future Streamlit projects. Should you have any thoughts, comments, or questions about type annotations and type-checking, please post them in the comments below or connect with me on Twitter, LinkedIn, or GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Building Your Reddit Clone | Streamlit & Firestore
https://blog.streamlit.io/streamlit-firestore-continued/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit ❤️ Firestore (continued)

Aka the NoSQL sequel: Building a Reddit clone and deploying it securely

By Austin Chen
Posted in Tutorials, April 22 2021
Recap
Part 3: Building your Reddit clone
Reading one post
Writing one post
Reading ALL posts
Streamlit Widgets and Firestore
Part 4: Securely deploying on Streamlit sharing
Convert our JSON key into a secrets.toml file
Ignoring our local secret files in git
Adding the secret to Streamlit sharing
Wrapping up
Resources
Contents
Share this post
← All posts
Recap

Last time, in Parts 1 & 2, we walked through all the necessary steps to set up Streamlit, Streamlit sharing, and Firestore as well as went over what Firestore is and how it can help in your Streamlit apps.

Today, we'll dive into the exciting stuff: In Part 3 we'll code up the Firestore Reddit app, then in Part 4 we'll add secrets and make it live for the whole world!

Part 3: Building your Reddit clone
Reading one post

Let's start by replacing our streamlit_app.py , to check that Firestore is indeed set up correctly:

import streamlit as st
from google.cloud import firestore

# Authenticate to Firestore with the JSON account key.
db = firestore.Client.from_service_account_json("firestore-key.json")

# Create a reference to the Google post.
doc_ref = db.collection("posts").document("Google")

# Then get the data at that reference.
doc = doc_ref.get()

# Let's see what we got!
st.write("The id is: ", doc.id)
st.write("The contents are: ", doc.to_dict())(


What is doc_ref here? It's short for "document reference". You can think of a reference to be like the title of a book that you want to get from the library. Creating a reference is really fast, but there's no data in the reference itself. In order to download the data that the reference is talking about, you have to call .get() on that reference — which, going by the analogy, is when the librarian takes the title and fetches the corresponding book. Creating a doc_ref is really fast; calling .get() can be a lot slower!

💡 Pro-tip: Remember, it's $ streamlit run streamlit_app.py to run your code locally, and you will see it on http://localhost:8501. I'd encourage you to turn on "Always rerun" in the top right hamburger menu of your app at this point, so that Streamlit will automatically update each time you save your Python code.

If this is what you see after running through the above:

Then congrats! Your Streamlit app has successfully read from the Firestore database.

Writing one post

Next up, let's create a new document directly in Python. Start the same way — by making a doc_ref, but this time filling in the ID you want to use for your new document. Then, you can call .set() with a Python dictionary containing the data you wanted. Here's what that looks like:

# This time, we're creating a NEW post reference for Apple
doc_ref = db.collection("posts").document("Apple")

# And then uploading some data to that reference
doc_ref.set({
	"title": "Apple",
	"url": "www.apple.com"
})

Reading ALL posts

If we don't know the ID of the post we want, or want to list all of the posts, that's straightforward too! Form a reference to the whole collection of posts, and call .stream() to get a list of docs you can iterate through.

You may have noticed already, but for each doc we get, we convert it to a Python dict using .to_dict() so that we can work with it. As with any Python dict, you can then get its values with bracket notation (doc.to_dict()["url"]).

# Now let's make a reference to ALL of the posts
posts_ref = db.collection("posts")

# For a reference to a collection, we use .stream() instead of .get()
for doc in posts_ref.stream():
	st.write("The id is: ", doc.id)
	st.write("The contents are: ", doc.to_dict())


That's all the database operations we'll need to build our Reddit clone! Now we can start diving into combining these database calls with our Streamlit widgets to build the full app.

Streamlit Widgets and Firestore

To let our users create new posts, we can use 3 widgets:

A st.text_input to write in the title of the post
A st.text_input for the url
And a st.button to allow the user to submit

Then, let's clean up the rendering of the posts to use some nice Markdown formatting. Here's what we have so far in streamlit_app.py

import streamlit as st
from google.cloud import firestore

db = firestore.Client.from_service_account_json("firestore-key.json")

# Streamlit widgets to let a user create a new post
title = st.text_input("Post title")
url = st.text_input("Post url")
submit = st.button("Submit new post")

# Once the user has submitted, upload it to the database
if title and url and submit:
	doc_ref = db.collection("posts").document(title)
	doc_ref.set({
		"title": title,
		"url": url
	})

# And then render each post, using some light Markdown
posts_ref = db.collection("posts")
for doc in posts_ref.stream():
	post = doc.to_dict()
	title = post["title"]
	url = post["url"]

	st.subheader(f"Post: {title}")
	st.write(f":link: [{url}]({url})")


Which turns into this Streamlit app:

Isn't that cool? In less than 30 lines of code, you've made a web app that creates new posts and saves them. Your database means that all this data is backed up. Run this Streamlit app on another browser tab (or even another computer entirely) and you'll see the exact same data! And changes on one app will be shared to EVERY app 😮

Part 4: Securely deploying on Streamlit sharing

💡 Pro-tip: To use secrets in your Streamlit deployed apps you'll first need an invite to sharing. Request an invite here if you're not already in the beta!

All right. Before we deploy this lovely Reddit clone to Streamlit sharing so that the whole world can access it, there's one thing we need to take care of. Remember the firestore-key.json file, the password that our Python code uses to sign in to Firestore? If we commit and push that file onto GitHub, it would be like sharing your password with the entire internet...

This is where Secrets comes in! Secrets are a way to pass in information that your app needs to know, but you don't want to publish on GitHub. Here's how we can securely upload our Firestore key:

Convert our JSON key into a secrets.toml file

JSON and TOML are two different file formats, but the core idea is pretty similar - they both make it easy to pass around a bunch of string keys and their corresponding values. (One way to think of JSON & TOML are as representations of a Python dictionary, but written as a file.) Firestore gave us our secrets as a JSON file, but Streamlit secrets expect a TOML; let's convert between them with a Python script!

Go ahead and copy this code into a new script, key-to-toml.py

import toml

output_file = ".streamlit/secrets.toml"

with open("firestore-key.json") as json_file:
    json_text = json_file.read()

config = {"textkey": json_text}
toml_config = toml.dumps(config)

with open(output_file, "w") as target:
    target.write(toml_config)


And then run the script:

$ python key-to-toml.py


Your  firestore-key.json has now been written out to .streamlit/secrets.toml! Now we can update our Streamlit app to use this new TOML file when initializing the Firestore library:

# Replace:
db = firestore.Client.from_service_account_json("firestore-key.json")

# With:
import json
key_dict = json.loads(st.secrets["textkey"])
creds = service_account.Credentials.from_service_account_info(key_dict)
db = firestore.Client(credentials=creds, project="streamlit-reddit")


When you're done, double-check your Streamlit app — everything should work the same, reading and writing from Firestore. That's because the new st.secrets knows to look for a file called .streamlit/secrets.toml when Streamlit is running on your local machine!

Ignoring our local secret files in git

Now that we've converted our secret key, we're almost ready to push the code to GitHub. We just need to configure git to ignore our secret files, done by the handy-dandy .gitignore file:

secrets.toml
firestore-key.json


Now, all changes in these two secret files will be safely excluded when you push your code. Let's do that now. From a command line:

$ git commit -am 'Read and write to Firestore, securely!'
$ git push


You can look on GitHub to see that your code is all there, minus the two secret files~

Adding the secret to Streamlit sharing

Normally, a git push is all we need to update any Streamlit sharing app. But because we're adding secrets, we need paste those secrets to the sharing dashboard. You can do this by copying the entire contents of .streamlit/secrets.toml, and pasting it here:

Aaaand that's it! Go to your app; you should see it deployed to the entire world. Congrats🎈

Wrapping up

That's the end of this tutorial... but it doesn't have to be the end of your app! There's a lot of other cool things you can do with Firestore in Streamlit, such as:

Nesting documents inside of documents (e.g. to implement upvotes and comments)
Forming complex queries (e.g. getting the 10 most recent posts created by Alice)
Listening for updates in REAL TIME (e.g. to make a chat app)

There's so many awesome places to go from here — I can't wait to see what you make. Share your work with the entire community by posting below in the comments!

Resources
Firestore
Streamlit docs
Streamlit Github
Streamlit Forum
Sharing sign-up
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > December 2022
https://blog.streamlit.io/monthly-rewind-december-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > December 2022

Your December look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, January 9 2023
🏆 App of the month 🏆
Streamlit December updates
🔍 Current release: 1.16.0
🔮 Upcoming
📊 Streamlit theme for Altair and Plotly charts
🖼️ Social share preview
⭐️ Streamlit Education program
Featured Streamlit content
💊 How to quickly deploy and share your machine learning model for drug discovery
🎵 Find the top songs from your high school years with a Streamlit app
🔐 Streamlit-Authenticator, Part 1: Adding an authentication component to your app
📺 New tutorials on Streamlit YouTube:
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our December featured app of the month is... 🥁🥁🥁🥁🥁

Feather AI by Josh Cadorette and David Hoffman.

Feather AI is an audio-to-text summarizer app built in 1 week during a hackathon! To use the app, all you need to do is add a link to a podcast or YouTube video, enter your email, and get a polished summary in your inbox in 5-10 minutes. It can be used for many use cases, such as summarizing earnings calls, meetings, or webinars.

Streamlit December updates

Below are the latest updates and releases from December.

🔍 Current release: 1.16.0

The latest release is 1.16.0. Recent updates include a new Streamlit theme for Altair and Plotly charts and colored text in st.markdown. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

New features to get excited about:

De-experimentalize faster rerun
Ability to serve static files
Editable dataframes

Visit our roadmap app to see what else we're working on. 🥳

📊 Streamlit theme for Altair and Plotly charts
There's a new Streamlit theme for Altair and Plotly charts! Now your charts will integrate better with the rest of your app's design.
🖼️ Social share preview
Introducing share previews for Community Cloud apps. You'll see a generated image and description previewing your app when you share it.

⭐️ Streamlit Education program
We're launching two new education programs!
🍎 Educator Ambassadors—supports educators using Streamlit in the classroom.
🎒 Student Ambassadors—guides students from building apps to mastering Streamlit.
Featured Streamlit content

💊 How to quickly deploy and share your machine learning model for drug discovery
Learn how to deploy and share an ML model for drug discovery! Sebastián goes over how to create the UI, display important data, and use the model for prediction.

🎵 Find the top songs from your high school years with a Streamlit app
Want to rediscover music from your high school days? Learn from Robert how to build an app to generate Spotify playlists of top songs from selected years.

🔐 Streamlit-Authenticator, Part 1: Adding an authentication component to your app
Check out Streamlit-Authenticator—a custom component by Mohammad to help with apps that require user authentication and privileges.

📺 New tutorials on Streamlit YouTube:

Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

See and navigate awesome components with Johannes' Streamlit components hub app.
Using GPT3 and DALL-E, Nikolas's Generate Tweets app will create Tweets for you and upload them to your account!
Luca's Molecular icons generator lets you generate and customize icons from SMILES, Name, Cas-number, Inchi, InChIKey, loaded molecule file, or SMILES list.
Learn about the streamlit-pandas component and how to Easily Make DataFrame App with Streamlit Pandas (Only 2 lines of Python!) from William.
Experiment with Ultralytics Yolov5 models using Robin's yolov5-ui app.
Ozgur's Euro league stats app decides the most productive 5 team members in selected matches.
With Ali's Cross Chain Monitoring Tool, you can compare +10 blockchains in different sectors and view the performance of each blockchain.
Indraneel created his Curated app to provide awesome resources for learning new skills.
The Psychiatric Medications Side Effects app by Nadya and team lets you input a person's psychiatric medications and conveniently displays all their side effects for clearer understanding.
Get insights about the impact of 5G on the top-level management of Atliquo with Jegadheesh's Atliquo Telecom - Performance app.
Listen to the Streamlit Community Song by Tom John, with lyrics written by ChatGPT.
Prit's Constellation Explorer computes a transit schedule from the biggest satellite constellations to see which are passing over your location.
Catch up on some Streamlit features you didn't know existed…in this video from Coding is Fun.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
October 2022
November 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Untitled.gif (800×567)
https://blog.streamlit.io/content/images/2022/08/Untitled.gif#browser


Untitled.png (1115×556)
https://blog.streamlit.io/content/images/2022/04/Untitled.png#border


ezgif.com-gif-maker.gif (800×602)
https://blog.streamlit.io/content/images/2022/08/ezgif.com-gif-maker.gif#browser


Raunaq Malhotra - Streamlit
https://blog.streamlit.io/author/raunaq/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Raunaq Malhotra
1 post
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

scienceio-journal.gif (840×420)
https://blog.streamlit.io/content/images/2023/01/scienceio-journal.gif#border


Gaurav Kaushik - Streamlit
https://blog.streamlit.io/author/gaurav/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Gaurav Kaushik
1 post
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

4.png (2354×1840)
https://blog.streamlit.io/content/images/2022/08/4.png#browser


text-search.png (736×986)
https://blog.streamlit.io/content/images/2023/01/text-search.png#border


text-search-Oct-05-2022-13-37-42.gif (858×1040)
https://blog.streamlit.io/content/images/2022/12/text-search-Oct-05-2022-13-37-42.gif#browser


3.png (2355×1849)
https://blog.streamlit.io/content/images/2022/08/3.png#browser


Untitled--5-.png (442×96)
https://blog.streamlit.io/content/images/2022/08/Untitled--5-.png


Untitled--4-.png (458×170)
https://blog.streamlit.io/content/images/2022/08/Untitled--4-.png


concatenate.gif (1660×800)
https://blog.streamlit.io/content/images/2022/10/concatenate.gif#browser


2.png (2356×1833)
https://blog.streamlit.io/content/images/2022/08/2.png#browser


1.png (2357×1831)
https://blog.streamlit.io/content/images/2022/08/1.png#browser


gif-1-1.gif (1241×821)
https://blog.streamlit.io/content/images/2021/08/gif-1-1.gif#browser


Make dynamic filters in Streamlit and show their effects on the original dataset
https://blog.streamlit.io/make-dynamic-filters-in-streamlit-and-show-their-effects-on-the-original-dataset/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Make dynamic filters in Streamlit and show their effects on the original dataset

Quickly and easily add dynamic filters to your Streamlit app

By Vladimir Timofeenko
Posted in Tutorials, August 25 2022
1. Build dynamic filters
2. Transform data
3. Use Sankey chart
4. Show generated SQL statements
Wrapping up
Contents
Share this post
← All posts

In this tutorial, I'll show you how to add dynamic filters to your Streamlit app with a bit of functional Python programming. As an example, we'll be using a Streamlit demo app that connects to Snowflake and retrieves the data by using Snowpark Python, pushing the computations into a Snowflake warehouse.

You’ll learn how to:

Build dynamic filters
Transform the data
Use Sankey chart
Show generated SQL statements

Want to dive right in? Here's the repo code that also contains a set of SQL statements to seed the tables with the data.

1. Build dynamic filters

First, let’s take a look at the app’s architecture:

Your data lives in Snowflake. To present it in Streamlit:

Install Streamlit;
Set up Streamlit secrets;
Fill in the boilerplate that connects Streamlit to Snowflake Snowpark:
# main.py
# Initialize connection.
def init_connection() -> Session:
    return Session.builder.configs(st.secrets["snowpark"]).create()

if __name__ == "__main__":
    # Initialize the filters
    session = init_connection()


To make your app look beautiful, use the sidebar and the main area to visually separate the filters. Build out the UI with draw_sidebar and draw_main_ui:

Next, implement the dynamic filters in the sidebar. The data is retrieved from Snowflake and transformed by using dataframes, so your filter class will be the bridge between the Streamlit framework and the Snowpark dataframes.

Let’s use Python’s awesome dataclasses as the basis for the class. It provides a great framework for writing a class and has some convenient methods you’ll be using.

To control the widget presentation, you’ll need a human_name field (when the page prompts for inputs and it’s showing information in the charts) and a widget_type field (to create an interactive element).

To access your filter’s state across the application, give it a widget_id.

Since the filters can't be both static (an on/off checkbox) and dynamic and depend on the source data (slider), you need a way to store the maximum value that’s retrieved from the table - instance field _max_value.

I used a checkbox and slider widgets as an example, but you can extend the code to work with other Streamlit components.

Your starting class will look like this:

@dataclass
class OurFilter:
    """This dataclass represents the filter that can be optionally enabled.

    It is created to parametrize the creation of filters from Streamlit and to keep the state."""
    human_name: str
    widget_type: callable  # Should be one of st.checkbox or st.select_slider. Other elements could be implemented similarly
    widget_id: str
    is_enabled: bool = False  # Controls whether the filter has been enabled. Useful for filtering the list of filters
    _max_value: int = 0


Next, add the other side—the one that will work with Snowpark to filter the data.

You’ll need a way to connect to Snowflake so the Snowpark session could be shared among all instances of class. Cue class variables to the rescue!

I’m working with a single table in Snowflake, so I’ll keep the table_name as yet another class variable. Each filter represents its own column in the table, so it’ll be table_column.

For the OurFilter class to have a generic interface, keep the name of the dataframe’s filtering method as an internal variable:

from dataclasses import dataclass
from typing import ClassVar
from snowflake.snowpark.session import Session

@dataclass
class OurFilter:
    """This dataclass represents the filter that can be optionally enabled.

    It is created to parametrize the creation of filters from Streamlit and to keep the state."""
    # Class variables
    table_name: ClassVar[str]
    session: ClassVar[Session]

    # The name to display in UI
    human_name: str
    # Column in the table which will be used for filtering
    table_column: str
    # ID of the streamlit widget
    widget_id: str
    # The type of streamlit widget to generate
    widget_type: callable
    # Field to track if the filter is active. Can be used for filtering the list of filters
    is_enabled: bool = False
    # max value
    _max_value: int = 0
    # dataframe method that will be used for filtering the data
    _df_method: str = ""


You can define the __post_init__ dataclass method to automatically populate the value of _max_value and control the value of _df_method. This hides the internals of implementation from the class interface:

def __post_init__(self):
        if self.widget_type not in (st.select_slider, st.checkbox):
            raise NotImplemented

        if self.widget_type is st.select_slider:
            self._df_method = "between"
            self._max_value = (
                self.session.table(MY_TABLE)
                .select(max(col(self.table_column)))
                .collect()[0][0]
            )
        elif self.widget_type is st.checkbox:
            self._df_method = "__eq__"


Let’s add a way to render OurFilter in Streamlit and show some text to the user:

def create_widget(self):
        if self.widget_type is st.select_slider:
            base_label = "Select the range of"
        elif self.widget_type is st.checkbox:
            base_label = "Is"
        else:
            base_label = "Choose"
        widget_kwargs = dict(label=f"{base_label} {self.widget_id}", key=self.widget_id)
        if self.widget_type is st.select_slider:
            widget_kwargs.update(
                dict(
                    options=list(range(self.max_value + 1)),
                    value=(0, self.max_value),
                )
            )
        # Invocation of the streamlit method to place the widget on the page
				# e.g. st.checkbox(**widget_kwargs)
        self.widget_type(**widget_kwargs)


To apply the filters to a sequence of dataframes, let’s make the class callable. This will allow us to use them more expressively:

def __call__(self, _table: Table):
        """This method turns this class into a functor allowing to filter the dataframe.

        This allows to call it like so:

        f = OurFilter(...)
        new_table = last_table[f(last_table)]"""
        return methodcaller(self.df_method, **(self._get_filter_value()))(
            _table[self.table_column.upper()]
        )

def _get_filter_value(self):
        """Custom unpack function that retrieves the value of the filter
        from session state in a format compatible with self._df_method"""
        _val = st.session_state.get(self.widget_id)
        if self.widget_type is st.checkbox:
            # For .eq
            return dict(other=_val)
        elif self.widget_type is st.select_slider:
            # For .between
            return dict(lower_bound=_val[0], upper_bound=_val[1])
        else:
            raise NotImplemented



If you rewrite the main.py to import the filter class and draw the sidebar, you’ll see the dynamic filters on the page:

from typing import Iterable

import streamlit as st
from lib.filterwidget import OurFilter
from toolz import pluck

MY_TABLE = "CUSTOMERS"

def _get_active_filters() -> filter:
    return filter(lambda _: _.is_enabled, st.session_state.filters)

def _is_any_filter_enabled() -> bool:
    return any(pluck("is_enabled", st.session_state.filters))

def _get_human_filter_names(_iter: Iterable) -> Iterable:
    return pluck("human_name", _iter)

def draw_sidebar():
    """Should include dynamically generated filters"""

    with st.sidebar:
        selected_filters = st.multiselect(
            "Select which filters to enable",
            list(_get_human_filter_names(st.session_state.filters)),
            [],
        )
        for _f in st.session_state.filters:
            if _f.human_name in selected_filters:
                _f.enable()

        if _is_any_filter_enabled():
            with st.form(key="input_form"):

                for _f in _get_active_filters():
                    _f.create_widget()
                st.session_state.clicked = st.form_submit_button(label="Submit")
        else:
            st.write("Please enable a filter")

if __name__ == "__main__":
    # Initialize the filters
    session = init_connection()
    OurFilter.session = session
    OurFilter.table_name = MY_TABLE

    st.session_state.filters = (
        OurFilter(
            human_name="Current customer",
            table_column="is_current_customer",
            widget_id="current_customer",
            widget_type=st.checkbox,
        ),
        OurFilter(
            human_name="Tenure",
            table_column="years_tenure",
            widget_id="tenure_slider",
            widget_type=st.select_slider,
        ),
        OurFilter(
            human_name="Weekly workout count",
            table_column="average_weekly_workout_count",
            widget_id="workouts_slider",
            widget_type=st.select_slider,
        ),
    )

    draw_sidebar()


This will produce something like this:

Now that you have the filter presentation working, let’s see how those filters apply to the data and how it’s presented to the user.

2. Transform data

To show what effects the filters have on the dataset, preserve some data references. Since the OurFilter class already contains the logic to perform the filtering and hides it behind an interface, the transformation will be pretty light:

# main.py

def draw_main_ui(_session: Session):
    """Contains the logic and the presentation of the main section of the UI"""
    if _is_any_filter_enabled():  # Do not run any logic if no filters are actually enabled

        customers: Table = _session.table(MY_TABLE)
        table_sequence = [customers]

        _f: MyFilter
        for _f in _get_active_filters():
            # This block generates the sequence of dataframes as continually applying AND filtering set by the sidebar
            # The dataframes are to be used in the Sankey chart.

            # First, get the last dataframe in the list
            last_table = table_sequence[-1]
            # Apply the current filter to it
            new_table = last_table[
                # At this point the filter will be applied to the dataframe using the __call__ method
                _f(last_table)
            ]
            # And save it in the sequence
            table_sequence += [new_table]
        
        st.header("Dataframe preview")

        st.write(table_sequence[-1].sample(n=5).to_pandas().head())
    else:
        st.write("Please enable a filter in the sidebar to show transformations")


With this bit of code in the main.py file, the preview of all applied filters will show up in Streamlit:

3. Use Sankey chart

Sankey chart (or Sankey diagram) shows the user how the data flows through the filter (the filtering effect). To dynamically visualize it as a graph, use the Sankey class from plotly.graph_objects and Streamlit’s built-in integration with Plotly library.

The interface for plotly.graph_objects.Sankey looks like this (color-coded):

The source and the target lists describe which labels are connected, and the value describes the size of the flow.

To generate the labels as well as the link, create a few helper functions that’ll take the table_sequence from the code above and produce the needed values for the visualization:

# lib/chart_helpers.py
from typing import Iterable, List, Tuple

from snowflake.snowpark.table import Table

def mk_labels(_iter: Iterable[str]) -> Tuple[str, ...]:
    """Produces the labels configuration for plotly.graph_objects.Sankey"""
    first_label = "Original data"
    return (first_label,) + tuple(map(lambda _: f"Filter: '{_}'", _iter)) + ("Result",)

def mk_links(table_sequence: List[Table]) -> dict:
    """Produces the links configuration for plotly.graph_objects.Sankey"""
    return dict(
        source=list(range(len(table_sequence))),
        target=list(range(1, len(table_sequence) + 1)),
        value=[_.count() for _ in table_sequence],
    )


Now go back to main.py to show the Sankey chart on the page:

# main.py
import plotly.graph_objects as go
# ...
 
def draw_main_ui(_session: Session):
		if _is_any_filter_enabled():
				# ...
				# Generate the Sankey chart
				fig = go.Figure(
				    data=[
				        go.Sankey(
				            node=dict(
				                pad=15,
				                thickness=20,
				                line=dict(color="black", width=0.5),
				                label=mk_labels(_get_human_filter_names(_get_active_filters())),
				            ),
				            link=mk_links(table_sequence),
				        )
				    ]
				)
				st.header("Sankey chart of the applied filters")
				st.plotly_chart(fig, use_container_width=True)


The visualization shows how the filters are applied to the original dataset:

4. Show generated SQL statements

Since the program already tracks the sequence of transformations, you can show which SQL statements will produce the same results. To generate them from the table_sequence and show them in Streamlit, use st.markdown and build the table element with this code:

# main.py
# ...
 
def draw_main_ui(_session: Session):
		if _is_any_filter_enabled():
				# ...
				# Add the SQL statement sequence table
				statement_sequence = """
				| number | filter name | query, transformation |
				| ------ | ----------- | --------------------- |"""
				st.header("Statement sequence")
				for number, (_label, _table) in enumerate(
				    zip(
				        mk_labels(_get_human_filter_names(_get_active_filters())),
				        table_sequence,
				    )
				):
				    statement_sequence += f"""\
| {number+1} | {_label} | ```{_table.queries['queries'][0]}``` |"""
				
				st.markdown(statement_sequence) 



As the filters are applied, this code runs and maintains the table of statements:

Wrapping up

And that’s how you can implement dynamic filters in Streamlit! It takes only a few lines of code to add them to your app and let the user see their effects.

Have any questions or want to share a cool app you made? Join us on the forum, tag us on Twitter, or let us know in the comments below.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Screen-Shot-2021-07-16-at-4.14.55-PM.png (1186×296)
https://blog.streamlit.io/content/images/2021/10/Screen-Shot-2021-07-16-at-4.14.55-PM.png#border


Randy Zwitch - Streamlit
https://blog.streamlit.io/author/randyzwitch/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Randy Zwitch
2 posts
Twitter
6 tips for improving your Streamlit app performance

Moving your Streamlit app from analysis to production

Tutorials
by
Randy Zwitch
,
July 20 2021
Testing Streamlit apps using SeleniumBase

How to create automated visual tests

Tutorials
by
Randy Zwitch
,
November 23 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

screen-shot-2021-07-16-at-3.47.33-pm--1-.png (866×146)
https://blog.streamlit.io/content/images/2021/10/screen-shot-2021-07-16-at-3.47.33-pm--1-.png#border


metric.jpeg (1063×588)
https://blog.streamlit.io/content/images/2022/05/metric.jpeg#browser


julo-gif-3.gif (1050×596)
https://blog.streamlit.io/content/images/2022/06/julo-gif-3.gif#browser


checkout_form-1.gif (1407×1045)
https://blog.streamlit.io/content/images/2023/05/checkout_form-1.gif#browser


marcelo.jpeg (800×800)
https://blog.streamlit.io/content/images/2022/05/marcelo.jpeg#border


Marcelo Jannuzzi - Streamlit
https://blog.streamlit.io/author/marcelo/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Marcelo Jannuzzi
1 post
How one finance intern launched his data science career from a coding bootcamp in Brazil

Learn how Marcelo Jannuzzi of iFood got his dream job in data science

Case study
by
Marcelo Jannuzzi and 
1
 more,
June 9 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

julo-3-1.jpeg (1700×734)
https://blog.streamlit.io/content/images/2022/06/julo-3-1.jpeg#browser


julo-2.jpeg (2000×1121)
https://blog.streamlit.io/content/images/2022/06/julo-2.jpeg


TZC-streamlit.gif (682×247)
https://blog.streamlit.io/content/images/2023/04/TZC-streamlit.gif#browser


julo-1-1.jpeg (2000×1118)
https://blog.streamlit.io/content/images/2022/06/julo-1-1.jpeg


Martijn Wieriks - Streamlit
https://blog.streamlit.io/author/martijn/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Martijn Wieriks
1 post
JULO improves financial inclusion in Indonesia with Streamlit

Learn how JULO went from manual underwriting to automated credit scoring and a 22-member data team

Case study
by
Martijn Wieriks and 
1
 more,
June 30 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 6)
https://blog.streamlit.io/page/6/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Semantic search, Part 1: Implementing cosine similarity

Wrangling Foursquare data and implementing semantic search in Snowflake

Snowflake powered ❄️
by
Dave Lin
,
May 17 2023
Analyzing real estate properties with Streamlit

A 7-step tutorial on how to make your own real estate app

Advocate Posts
by
Vinícius Oviedo
,
May 16 2023
Streamlit wizard and custom animated spinner

Improve user experience with simplified data entry and step-by-step guidance

Snowflake powered ❄️
by
Andrew Carson
,
May 15 2023
Learn Morse code with a Streamlit app

5 steps to build your own Morse code tutor!

Advocate Posts
by
Alice Heiman
,
May 12 2023
The ultimate Wordle cheat sheet

Learn how to beat Wordle with Streamlit

Advocate Posts
by
Siavash Yasini
,
May 11 2023
How to build an LLM-powered ChatBot with Streamlit

A step-by-step guide using the unofficial HuggingChat API

LLMs
by
Chanin Nantasenamat
,
May 10 2023
Build a Snowflake DATA LOADER on Streamlit in only 5 minutes

Drag and drop your Excel data to Snowflake with a Streamlit app

Snowflake powered ❄️
by
Sasha Mitrovich
,
May 9 2023
Convert images into pixel art

A 5-step tutorial for making a pixel art converter app

Advocate Posts
by
soma noda
,
May 8 2023
Accessible color themes for Streamlit apps

Control your app’s color scheme and visual accessibility

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
May 5 2023
Collecting user feedback on ML in Streamlit

Improve user engagement and model quality with the new Trubrics feedback component

Advocate Posts
by
Jeff Kayne and 
1
 more,
May 4 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

step3.PNG.png (1324×1282)
https://blog.streamlit.io/content/images/2023/04/step3.PNG.png#browser


Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/5/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Streamlit-Authenticator, Part 1: Adding an authentication component to your app

How to securely authenticate users into your Streamlit app

Advocate Posts
by
Mohammad Khorasani
,
December 6 2022
Building robust Streamlit apps with type-checking

How to make type-checking part of your app-building flow

Advocate Posts
by
Harald Husum
,
November 10 2022
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
How to build Streamlit apps on Replit

Learn Streamlit by building the Beginner Template Tour

Advocate Posts
by
Shruti Agarwal
,
September 29 2022
Observing Earth from space with Streamlit

Learn how Samuel Bancroft made the SatSchool app to teach students Earth observation

Advocate Posts
by
Samuel Bancroft
,
June 16 2022
How to share scientific analysis through a Streamlit app

3 easy steps to share your study results with fellow scientists

Advocate Posts
by
Mitchell Parker and 
1
 more,
May 12 2022
How to build a real-time live dashboard with Streamlit

5 easy steps to make your own data dashboard

Advocate Posts
by
AbdulMajedRaja RS
,
April 21 2022
30 Days of Streamlit

A fun challenge to learn and practice using Streamlit

Advocate Posts
by
Chanin Nantasenamat
,
April 1 2022
Sogeti creates an educational Streamlit app for data preprocessing

Learn how to use Sogeti’s Data Quality Wrapper

Advocate Posts
by
Tijana Nikolic
,
March 8 2022
Calculating distances in cosmology with Streamlit

Learn how three friends made the cosmology on-the-go app CosmΩracle

Advocate Posts
by
Nikolina Sarcevic and 
2
 more,
February 17 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Sebastian Ayala Ruano - Streamlit
https://blog.streamlit.io/author/sebastian-ayala-ruano/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Sebastian Ayala Ruano
1 post
How to quickly deploy and share your machine learning model for drug discovery

Share your ML model in 3 simple steps

Advocate Posts
by
Sebastian Ayala Ruano
,
December 15 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

custom_steps-1.gif (1407×866)
https://blog.streamlit.io/content/images/2023/05/custom_steps-1.gif#browser


Dropjumptextfile.PNG-1.png (2255×1375)
https://blog.streamlit.io/content/images/2023/04/Dropjumptextfile.PNG-1.png#browser


custom_spinner-1-1.gif (1407×866)
https://blog.streamlit.io/content/images/2023/05/custom_spinner-1-1.gif#browser


step2.PNG.png (1310×914)
https://blog.streamlit.io/content/images/2023/04/step2.PNG.png#browser


wizard-form.gif (1407×749)
https://blog.streamlit.io/content/images/2023/05/wizard-form.gif#browser


wizard_frame_2-1-1.gif (1407×674)
https://blog.streamlit.io/content/images/2023/05/wizard_frame_2-1-1.gif#browser


mcs_select.png (2378×1624)
https://blog.streamlit.io/content/images/2023/06/mcs_select.png#browser


sis_mc_connect.png (2000×1324)
https://blog.streamlit.io/content/images/2023/06/sis_mc_connect.png#browser


sis_mc_app.png (2356×1618)
https://blog.streamlit.io/content/images/2023/06/sis_mc_app.png#browser


pyplot-5-1.gif (600×519)
https://blog.streamlit.io/content/images/2022/06/pyplot-5-1.gif#browser


sis_mc_app_result.png (1554×663)
https://blog.streamlit.io/content/images/2023/06/sis_mc_app_result.png#browser


remove-background-from-your-image.png (1592×1025)
https://blog.streamlit.io/content/images/2022/12/remove-background-from-your-image.png#border


pyplot-4.png (1452×1308)
https://blog.streamlit.io/content/images/2022/06/pyplot-4.png#browser


pyplot-3-1.png (1484×1381)
https://blog.streamlit.io/content/images/2022/06/pyplot-3-1.png#browser


simple_matplot_gif-1.gif (850×606)
https://blog.streamlit.io/content/images/2022/06/simple_matplot_gif-1.gif#browser


Screenshot-2023-02-08-at-6.45.33-AM.png (2000×965)
https://blog.streamlit.io/content/images/2023/02/Screenshot-2023-02-08-at-6.45.33-AM.png#browser


pyplot-1-1.png (1651×1310)
https://blog.streamlit.io/content/images/2022/06/pyplot-1-1.png#browser


UC Davis Dashboard That Tracks California's COVID-19 Cases By Region
https://blog.streamlit.io/uc-davis-tool-tracks-californias-covid-19-cases-by-region/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
New UC Davis tool tracks California's COVID-19 cases by region

Regional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine

By Pranav Pandit
Posted in Advocate Posts, November 19 2020
Building a tool to understand COVID-19 at a regional level
Creating the tool in Streamlit
Compiling and presenting the data
Sharing our app with the rest of the world
Getting the app to shine
Improving app stability
Improving app performance
In closing
Contents
Share this post
← All posts

Written by Pranav Pandit - Postdoctoral Research Fellow at One Health Institute at UC Davis

Building a tool to understand COVID-19 at a regional level

The COVID-19 pandemic has highlighted the importance of constant and real-time disease surveillance to better control unprecedented local outbreaks. Early on in the pandemic, the state of California came up with its own distinct phases for opening the economy and community activities. These phases were based on county-wide thresholds of daily COVID-19 incidence, test positivity, and availability of ICU beds in the county. While county-wide data is continuously available, a composite picture of a group of counties is not as easy to understand. Such information is especially useful for bigger organizations that serve regional communities, such as universities, corporate companies, or dense urban regions that are highly connected.

At the UC Davis School of Veterinary Medicine, faculty, staff, and students come from many parts of California, but primarily from three counties—Yolo, Solano and Sacramento. University students, staff, and faculty commute from these nearby communities, so it's key to not just follow up on the COVID-19 statistics in Yolo County but rather look at a combination of the three counties to make informed decisions on reopening policies. Similarly, larger metropolitan areas such as the San Francisco Bay Area can benefit greatly from composite COVID-19 statistics for the region.

My team in the Epicenter for Disease Dynamics within the school’s One Health Institute was tasked with developing a tool that would go beyond county data and help us understand regional risk. Drawing from statistics already provided by the California Department of Public Health, John Hopkins University and CovidActNow.org, we have created a COVID-19 tool that lets the university better understand our regional data and plan our day-to-day decisions.

Creating the tool in Streamlit

By using Streamlit we were able to convert our Python code into an interactive tool that enables users to select counties of interest and get composite COVID-19 intelligence for the combined region. You can check out the tool here and the source code for the tool here.

Compiling and presenting the data

We already had a lot of code developed to track and visualize cases in California with live data provided through the state's data portal. We also started capturing daily case data from the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University and decided to highlight informational snippets related to positive test rates and ICU rooms which is available through CovidActNow.org.

The application starts with displaying a dashboard that shows composite data of three counties surrounding the University of California Davis (Yolo, Sacramento and Solano). Users can either choose a single county or a combination of multiple counties of their choice to get trends on A) daily new cases per 100,000 population (averaged over the last seven days), (B) daily incidence (new cases), (C) Cumulative cases and deaths and (D) Daily new tests (testing data is available only for a few counties in California).

If you'd like to learn more about how to interpret these metrics, covidlocal.org is a good resource - but in short, the initial reopening of a community is indicated when daily cases decline for 21 consecutive days and estimates for new cases per 100,000 are below 25 cases per day (Phase 2). Similarly, Phase 3 economic recovery opening is indicated when estimates for new cases per 100,000 are below 10 cases per day.

Sharing our app with the rest of the world  

Once the initial version of the app was ready, I wanted my teammates at UC Davis to access the app, but I didn't want to put them through the ordeal of downloading the source code, setting up the development environment, and running the app locally. We hosted it on Heroku, but soon realized its limitations in terms of the number of users that can access it simultaneously.

Around the same time, Streamlit had launched a beta version of their sharing platform. The Streamlit sharing platform allows deploying Streamlit apps directly from a public repository in Github and sharing with external shareholders for free (as of writing this sharing is currently invite only, but you can sign-up here).

Once I was enrolled in the beta program, I was able to deploy the app in just a couple of clicks. All I had to do was host the app source code in a public Github repository.

After the app was deployed to the Streamlit sharing platform, I shared the URL with the Dean's office, and the URL shortly made it's way to school's internal portal and became a tool used in planning field operations while minimizing the risk of exposing students and researchers to Covid-19.

Getting the app to shine  

Around this time we noticed a couple of issues related to the performance and stability of the app. I will explain the issues along with the solutions below.

Improving app stability  

During one of my team meetings, when some of my teammates tried to access the app simultaneously, we noticed the app crashed and restarted. With the help of the engineering team at Streamlit, we narrowed down these issues to the use of matplotlib as the library for rendering charts based on data frames in the app. It turned out that matplotlib is not best suited for applications running in a highly concurrent environment, in fact, the official matplotlib documentation claims that the library is not thread-safe. As a result, when multiple users tried to access the app simultaneously, the app would crash and restart. We were able to solve this problem by eliminating the use of matplotlib 's global API, and more importantly, by using explicit synchronization semantics when accessing matplotlib figures in the Python code (see example code).

Improving app performance  

As the app was becoming more popular within UC Davis, another concern I had was around the noticeably high latency of the charts loading when a user visits the app. I was already using Streamlit's built-in caching using st.cache() annotations, so I knew the app was not doing the heavy-lifting of downloading datasets and computing aggregates every time. We profiled the app's performance using Python's built-in cProfile package and observed that most of the time was spent plotting the charts inside matplotlib.  Taking inspiration from a Streamlit user community forum thread, which recommends altair as a more performant library for plotting, I ported the visualizations from matplotlib to altair , and noticed up to 3x improvement in the time it takes to run the app.

Here's a code snippet showing how easy it is to plot Altair charts in Streamlit:


# We use a custom scale to modify legend colors.
scale = alt.Scale(domain=["cases", "deaths"], range=['#377eb8', '#e41a1c'])

# Create a base chart layer using the dataframe.
base = alt.Chart(cases_and_deaths, title='(C) Cumulative cases and deaths'
    ).transform_calculate(
    cases_="'cases'", deaths_="'deaths'")
    
# Overlay a plot of number of cases.
c = base.mark_line(strokeWidth=3).encode(
    x=alt.X("Datetime", axis=alt.Axis(title = 'Date')),
    y=alt.Y("cases", axis=alt.Axis(title = 'Count')),
    color=alt.Color("cases_:N", scale=scale, title=""))
    
# Overlay a plot of number of deaths.
d = base.mark_line(strokeWidth=3).encode(
    x=alt.X("Datetime", axis=alt.Axis(title='Date')),
    y=alt.Y("deaths", axis=alt.Axis(title = 'Count')),
    color=alt.Color("deaths_:N", scale=scale, title=""))
    
# Finally, render the chart.
st.altair_chart(c + d, use_container_width=True)


In closing

At the UC Davis School of Veterinary Medicine, the tool is being used to track metrics to better inform campus safety services and communications, but we believe the tool can be used for a wider audience as well.

At this stage of the pandemic, many individuals, families, and smaller organizations are debating their own decisions - whether to take a vacation, if they can visit family during holidays, when to make a work trip, or if it's too much of a risk to conduct daily activities such as grocery shopping. Such activities often require an additional aspect of regional risk assessment, and we hope that our tool can help provide that context. Please try it out, send feedback, and continue to share your own data and create your own great tools. We'll all get through this together.

Special thanks to Amey Deshpande and the Streamlit team for helping to optimize the app code and to Kat Kerlin, Tom Hinds and the UC Davis marketing team for the help editing the blog post.


About the author

Pranav Pandit, BVSc & AH, MPVM, Ph.D. is a postdoctoral scholar at the EpiCenter for Disease Dynamics, part of the One Health Institute at the UC Davis School of Veterinary Medicine. A veterinary epidemiologist specializing in mathematical modeling, Pranav is interested in understanding transmission diseases in animal populations and factors affecting spillover to humans. After completing his MPVM from UC Davis, Pranav completed his Ph.D. from École nationale vétérinaire de Nantes, in France. Follow Pranav and the EpiCenter for Disease Dynamics on Twitter: @PanditPranav and @EpiCenterUCD.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

4-1.png (1200×760)
https://blog.streamlit.io/content/images/2021/08/4-1.png#border


intro.gif (1660×800)
https://blog.streamlit.io/content/images/2022/10/intro.gif#browser


hello-upload-3.png (789×700)
https://blog.streamlit.io/content/images/2022/12/hello-upload-3.png#border


hello-upload-2.png (745×364)
https://blog.streamlit.io/content/images/2022/12/hello-upload-2.png#border


hello-upload-1.png (1558×508)
https://blog.streamlit.io/content/images/2022/12/hello-upload-1.png#border


user-privileges.gif (824×506)
https://blog.streamlit.io/content/images/2022/12/user-privileges.gif#browser


logged-in.png (776×462)
https://blog.streamlit.io/content/images/2022/11/logged-in.png#border


incorrect-login.png (818×495)
https://blog.streamlit.io/content/images/2022/11/incorrect-login.png#border


streamlit-spotify-playlists-app.gif (942×676)
https://blog.streamlit.io/content/images/2022/11/streamlit-spotify-playlists-app.gif#border


csv-file-playlists.png (1768×988)
https://blog.streamlit.io/content/images/2022/11/csv-file-playlists.png#browser


authentication-code.png (1504×764)
https://blog.streamlit.io/content/images/2022/11/authentication-code.png#browser


top-songs-dataframe-table.png (1609×904)
https://blog.streamlit.io/content/images/2022/11/top-songs-dataframe-table.png#browser


billboard-hot-100-top-ten-singles.png (2042×1178)
https://blog.streamlit.io/content/images/2022/11/billboard-hot-100-top-ten-singles.png#browser


plotly-code.png (698×928)
https://blog.streamlit.io/content/images/2023/01/plotly-code.png#border


color-picker.gif (600×293)
https://blog.streamlit.io/content/images/2023/01/color-picker.gif#border


3-4.png (1200×305)
https://blog.streamlit.io/content/images/2021/08/3-4.png#border


2-6.png (1200×890)
https://blog.streamlit.io/content/images/2021/08/2-6.png#border


mona-lisa-pixels-clustered.png (822×510)
https://blog.streamlit.io/content/images/2023/01/mona-lisa-pixels-clustered.png#border


mona-lisa-pixels.png (822×510)
https://blog.streamlit.io/content/images/2023/01/mona-lisa-pixels.png#border


sophisticated-palette-app-demo.gif (600×481)
https://blog.streamlit.io/content/images/2023/01/sophisticated-palette-app-demo.gif#browser


mona-lisa.png (982×194)
https://blog.streamlit.io/content/images/2023/01/mona-lisa.png#border


Untitled-1.png (1838×1119)
https://blog.streamlit.io/content/images/2021/08/Untitled-1.png#browser


8.png (1200×634)
https://blog.streamlit.io/content/images/2021/08/8.png#border


5-1.png (1200×701)
https://blog.streamlit.io/content/images/2021/08/5-1.png#border


7.png (1200×566)
https://blog.streamlit.io/content/images/2021/08/7.png#border


6.png (1200×557)
https://blog.streamlit.io/content/images/2021/08/6.png#border


3.png (1826×1159)
https://blog.streamlit.io/content/images/2021/08/3.png#browser


2.png (1820×1157)
https://blog.streamlit.io/content/images/2021/08/2.png#browser


Matt Brems - Streamlit
https://blog.streamlit.io/author/matt/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Matt Brems
1 post
How to use Roboflow and Streamlit to visualize object detection output

Building an app for blood cell count detection

Advocate Posts
by
Matt Brems
,
February 23 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

streamlit-figma-design-system-1.gif (3446×2038)
https://blog.streamlit.io/content/images/2022/10/streamlit-figma-design-system-1.gif#browser


0_di0AIuD61ak2AfQC.png (1400×707)
https://blog.streamlit.io/content/images/2021/08/0_di0AIuD61ak2AfQC.png#browser


streamlit-figma-app-base.gif (3446×2038)
https://blog.streamlit.io/content/images/2022/10/streamlit-figma-app-base.gif#browser


0_zL052KWOPvXxGOo6.png (1400×964)
https://blog.streamlit.io/content/images/2021/08/0_zL052KWOPvXxGOo6.png#browser


Jonah Kanner - Streamlit
https://blog.streamlit.io/author/jonah/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jonah Kanner
1 post
Website
Gravitational-wave apps help students learn about black holes

Exploring distant space with gravitational waves

Advocate Posts
by
Jonah Kanner
,
December 15 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

0_bLyrD_iCLEzljxfg.png (1400×323)
https://blog.streamlit.io/content/images/2021/08/0_bLyrD_iCLEzljxfg.png#browser


cazoo-1.png (2000×1777)
https://blog.streamlit.io/content/images/2022/09/cazoo-1.png#browser


streamlit-2-1.gif (640×480)
https://blog.streamlit.io/content/images/2021/09/streamlit-2-1.gif#browser


files.png (1088×744)
https://blog.streamlit.io/content/images/2022/10/files.png#border


reset_password.PNG-2.png (845×556)
https://blog.streamlit.io/content/images/2023/01/reset_password.PNG-2.png#border


0_7XCiGHafU_Cy1xc0--3-.png (652×738)
https://blog.streamlit.io/content/images/2021/08/0_7XCiGHafU_Cy1xc0--3-.png#browser


Martin Campbell - Streamlit
https://blog.streamlit.io/author/martin/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Martin Campbell
1 post
Easy monitoring of dbt Cloud jobs with Streamlit

How the Cazoo data science team built their dbt Cloud + Streamlit app

Advocate Posts
by
Martin Campbell
,
June 11 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

streamlit-secrets-2.gif (1600×900)
https://blog.streamlit.io/content/images/2021/09/streamlit-secrets-2.gif#brower


streamlit-components.png (1894×1454)
https://blog.streamlit.io/content/images/2022/10/streamlit-components.png#border


streamlit-figma-design-system.png (574×380)
https://blog.streamlit.io/content/images/2022/10/streamlit-figma-design-system.png#border


visualization-1.png (696×834)
https://blog.streamlit.io/content/images/2023/01/visualization-1.png#border


Theming--3-.gif (1280×821)
https://blog.streamlit.io/content/images/2021/08/Theming--3-.gif


The magic of working in open source
https://blog.streamlit.io/the-magic-of-working-in-open-source/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The magic of working in open source

How we build our open-source library and release new features

By Ken McGrady
Posted in Tutorials, August 4 2022
How we prioritize new features
What goes into implementing new features
How we keep in touch with the community
How you can contribute to the open-source community
Does this make you excited?
Contents
Share this post
← All posts

Wondering what it’s like to work on the Streamlit open-source project? There are many reasons why we all love it. But the most exciting one is our focus on becoming the best tool in every data scientist’s toolchain.

Open source involves lots of stakeholders yet offers limited resources. So our biggest challenge is to prioritize and implement the most useful features.

In this post, you’ll learn:

How we prioritize new features
What goes into implementing new features
How we keep in touch with the community
How you can contribute to the open-source community
How we prioritize new features

We prioritize new features every quarter:

Our product team decides which features will evolve the product and its audience.
Our engineering team builds solutions to GitHub issues.
Our community team monitors our social channels and advocates for the community’s needs.

For example, these features took countless hours of brainstorming, prototyping, and testing: multipage apps, new caching primitives, camera input, and updated dataframes.

In between the larger features, we tackle small delightful experiences, fix bugs, improve built-in charts, and add parameters to our APIs (tooltips, gap sizes, disabled widgets, etc.).

What goes into implementing new features

Before we start working on a new feature, we talk to our Data Science team and Streamlit Creators. Together, we decide which feature has the right amount of complexity and the most intuitive API (though there’s rarely a single solution for everyone’s use case).

A feature typically starts out as a simple “couple-of-lines change” that grows into a discussion on how it’ll impact the users, how it could be misused, and if it’ll keep our software resilient. We sort through lots of community feedback before finally pulling the trigger.

Once we build and release the feature, we move forward super-fast by:

Unit-testing it to narrow down bugs in code;
End-to-end testing to test the full functionality of a feature;
And screenshot-testing it to make the visuals pixel-perfect.

From an engineering standpoint, we try to not break our API while keeping a semantic versioning promise. We work with our product and design teams to give our users the best experience by looking at the common data use cases and designing solutions that have room for change. Plus, all external contributors' code gets assigned a code reviewer. Often we assign two code reviewers because we’re not familiar with the context!

If you’re curious to learn more about how we implement new features, check out these posts:

How to enhance Google Search Console data exports with Streamlit
How Streamlit uses Streamlit: sharing contextual apps
New experimental primitives for caching
Announcing theming for Streamlit apps!
How we keep in touch with the community

It can be a challenge for engineers to balance delivering features and talking to the community. We want to deliver our code on time, so our community conversations have a “context switching” tax. Our focus tends to be more on the quality of our product and less on the use cases, so our attention goes to the GitHub issues and bugs. We try to understand the issue, reliably reproduce it, and guesstimate its impact. Often, due to timing, we can’t fix the bug, but we get enough knowledge to help an external contributor solve the problem.

But we’re out there:

Our Engineering team posts release notes and responds to many posts on the forum.
Our Data Science team always has new ideas based on their Streamlit dogfooding.
Our Developer Relations team works with the community to produce rich content like 30 days of Streamlit.

We’re now part of Snowflake, and Snowflake’s mission is to mobilize the world’s data. Our community plays a big role in it. We believe in the full-employment theorem so we can always make Streamlit a better product for data scientists!

How you can contribute to the open-source community

Contributing to the open-source community is very rewarding. The software is free. And you can improve a single function or a whole discipline! But getting involved may seem daunting as most conversations are asynchronous. It takes time, patience, and fortitude.

If you want to get involved and help us make a stronger product, we’d love for you to do so! Here is how to get started:

Use Streamlit! Building software requires domain experience. Read more about Streamlit’s main concepts.
File bugs if you see them. Implement small, reliable, reproducible cases, and include as many details as possible. Many issues take time to understand because messages get lost in translation.
Help the community. Simple explanations help people understand Streamlit better. Better yet, turn it into content (for example, a YouTube channel or a Medium blog.)
Improve our documentation. We have amazing documentation and we value your input!
Share your apps on the forum and social!

Spend a month or two focused on the above—it’ll clarify for you how to help out in code. When ready, follow our contributing guidelines and take on a bug from our GitHub issues. Bugs are understandable, reproducible, and have a desired outcome. We identified some good first issues, but there are many more to choose from.

And finally, follow good software engineering practices in designing your solution and write tests (it saves the first comment in a code review). 🧑‍💻

Does this make you excited?

Want to work on open source as a job? Join our team! Our jobs require a unique skill set because Streamlit’s main value is delivering a clean and interactive user interface for developers, so we rely on strong frontend skills with TypeScript/React. And our developers interface with Streamlit using a simple Python API and server.

Here are our current job openings:

Senior Software Engineer and Senior Product Manager on our Open Source team.
Software Engineer on our Community Cloud team (if you have experience building full-stack services in the cloud).

Thank you for being part of our community. If you have questions, please post them in the comments below, and you may see them answered in future blog posts. 😉

Happy coding! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sample-code-github.png (1209×630)
https://blog.streamlit.io/content/images/2023/01/sample-code-github.png#browser


Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/page/5/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
Make your st.pyplot interactive!

Learn how to make your pyplot charts interactive in a few simple steps

Tutorials
by
William Huang
,
June 23 2022
How Streamlit uses Streamlit: Sharing contextual apps

Learn about session state and query parameters!

Tutorials
by
Tyler Richards
,
May 26 2022
3 steps to fix app memory leaks

How to detect if your Streamlit app leaks memory and identify faulty code

Tutorials
by
George Merticariu
,
April 14 2022
How to master Streamlit for data science

The essential Streamlit for all your data science needs

Tutorials
by
Chanin Nantasenamat
,
January 18 2022
Common app problems: Resource limits

5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud

Tutorials
by
Johannes Rieke
,
September 9 2021
6 tips for improving your Streamlit app performance

Moving your Streamlit app from analysis to production

Tutorials
by
Randy Zwitch
,
July 20 2021
How to make a great Streamlit app: Part II

A few layout and style tips to make your apps look even more visually appealing!

Tutorials
by
Abhi Saini
,
June 22 2021
How to make a great Streamlit app

Designing an app your users will love

Tutorials
by
Abhi Saini
,
June 2 2021
Introducing Submit button and Forms 📃

We're releasing a pair of new commands called st.form and st.form_submit_button!

Tutorials
by
Abhi Saini
,
April 29 2021
Streamlit ❤️ Firestore (continued)

Aka the NoSQL sequel: Building a Reddit clone and deploying it securely

Tutorials
by
Austin Chen
,
April 22 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Ken McGrady - Streamlit
https://blog.streamlit.io/author/ken/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Ken McGrady
1 post
The magic of working in open source

How we build our open-source library and release new features

Tutorials
by
Ken McGrady
,
August 4 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

flow.png (1806×1008)
https://blog.streamlit.io/content/images/2022/10/flow.png#border


Vladimir Timofeenko - Streamlit
https://blog.streamlit.io/author/vladimir_t/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Vladimir Timofeenko
1 post
Make dynamic filters in Streamlit and show their effects on the original dataset

Quickly and easily add dynamic filters to your Streamlit app

Tutorials
by
Vladimir Timofeenko
,
August 25 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Jessi Shamis - Streamlit
https://blog.streamlit.io/author/jessi/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jessi Shamis
1 post
Prototype your app in Figma! 🖌️

Quickly and easily design your app with the Streamlit Design system

Tutorials
by
Jessi Shamis
,
October 27 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Harald Husum - Streamlit
https://blog.streamlit.io/author/harald/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Harald Husum
1 post
Building robust Streamlit apps with type-checking

How to make type-checking part of your app-building flow

Advocate Posts
by
Harald Husum
,
November 10 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Theming-2--1-.png (1964×1116)
https://blog.streamlit.io/content/images/2021/08/Theming-2--1-.png


Screen_Shot_2021-03-15_at_10.43.38_AM.png (2000×1055)
https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-03-15_at_10.43.38_AM.png#browser


update_user_details.PNG.png (853×464)
https://blog.streamlit.io/content/images/2023/01/update_user_details.PNG.png#border


forgot_username.PNG.png (856×372)
https://blog.streamlit.io/content/images/2023/01/forgot_username.PNG.png#border


5-3.png (1200×634)
https://blog.streamlit.io/content/images/2021/08/5-3.png#browser


forgot_password.PNG.png (857×369)
https://blog.streamlit.io/content/images/2023/01/forgot_password.PNG.png#border


register_user.PNG.png (868×756)
https://blog.streamlit.io/content/images/2023/01/register_user.PNG.png#border


Richard Pelgrim - Streamlit
https://blog.streamlit.io/author/richard/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Richard Pelgrim
1 post
Deploying a cloud-native Coiled app

How Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users

Advocate Posts
by
Richard Pelgrim
,
September 7 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1.1.png (1200×688)
https://blog.streamlit.io/content/images/2021/08/1.1.png#border


2.2.png (1076×567)
https://blog.streamlit.io/content/images/2021/08/2.2.png#browser


10.png (1884×1255)
https://blog.streamlit.io/content/images/2021/08/10.png#border


11.png (1920×1195)
https://blog.streamlit.io/content/images/2021/08/11.png#border


9.png (1600×985)
https://blog.streamlit.io/content/images/2021/08/9.png#border


7-1.png (1881×894)
https://blog.streamlit.io/content/images/2021/08/7-1.png#border


8-1.png (1600×938)
https://blog.streamlit.io/content/images/2021/08/8-1.png#border


6-1.png (1920×1114)
https://blog.streamlit.io/content/images/2021/08/6-1.png#border


5-6.png (1772×1879)
https://blog.streamlit.io/content/images/2021/08/5-6.png#border


4-3.png (1200×835)
https://blog.streamlit.io/content/images/2021/08/4-3.png#border


3-5.png (1200×705)
https://blog.streamlit.io/content/images/2021/08/3-5.png#border


2-8.png (1920×1180)
https://blog.streamlit.io/content/images/2021/08/2-8.png#border


5-5.png (1720×993)
https://blog.streamlit.io/content/images/2021/08/5-5.png#border


3-9.png (996×492)
https://blog.streamlit.io/content/images/2021/08/3-9.png#browser


2-12.png (996×588)
https://blog.streamlit.io/content/images/2021/08/2-12.png#browser


1-11.png (996×588)
https://blog.streamlit.io/content/images/2021/08/1-11.png#browser


Monthly rewind > January 2023
https://blog.streamlit.io/monthly-rewind-january-2023/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > January 2023

Your January look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, February 16 2023
🏆 App of the month 🏆
Streamlit January updates
🔍 Current release: 1.18.0
🔮 Upcoming
❄️ Snowpark and PySpark support
🎈 Streamlit Tutorial-a-thon
Featured Streamlit content
🖼️️ Build an image background remover in Streamlit
🎨️ Create a color palette from any image
🗺️ How to make a culture map
☁️ Host your Streamlit app for free
📄 ScienceIO manages billions of rows of training data with Streamlit
📺 New YouTube tutorial: How to Create an Interactive Research Article using Streamlit
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our January featured app of the month is... 🥁🥁🥁🥁🥁

MathGPT by José Manuel Nápoles Duarte!

MathGPT is an app that uses Open AI's GPT-3 and NumPy to handle mathematical operations—such as vectors, matrices, and even college physics problems. Enter a prompt involving a math operation and get the result, explanation, and code behind the solution.

Streamlit January updates

Below are the latest updates and releases from January.

🔍 Current release: 1.18.0

The latest release is 1.18.0. Recent updates include new caching commands to replace st.cache and columns inside columns. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

New features to get excited about:

Cleaner hamburger menu
Improved st.help
Tooltips on text elements

Visit our roadmap app to see what else we're working on. 🥳

❄️ Snowpark and PySpark support
Snowflake's Snowpark and Apache's PySpark dataframes are now supported out of the box! Just pass them into any st command that accepts pandas dataframes. See the demo app.
🎈 Streamlit Tutorial-a-thon

Congratulations to the winners of the first Streamlit Tutorial-a-thon: Lisa, Andy, Vinícius, and Hansen! Check out their awesome tutorials. 🎉

Lisa's blog post
Andy's blog post
Vinícius's blog post
Hansen's video
Featured Streamlit content

🖼️️ Build an image background remover in Streamlit
Want to isolate an image from its background while still maintaining the quality? Learn how to build a background remover app from Tyler Simons.

🎨️ Create a color palette from any image
Want to enhance your data visualization with a custom color palette? Build an app with Siavash Yasini to generate colors from any image.

🗺️ How to make a culture map
Learn from Michał Nowotka how to create a culture map app while adding dynamic components for visual analysis.

☁️ Host your Streamlit app for free
Want to rediscover music from your high school days? Learn from Robert how to build an app to generate Spotify playlists of top songs from selected years.

📄 ScienceIO manages billions of rows of training data with Streamlit
Learn from Gaurav Kaushik how ScienceIO searches and interacts with its training dataset for large healthcare language models—using a Streamlit app connected to a Snowflake database!

📺 New YouTube tutorial: How to Create an Interactive Research Article using Streamlit
Learn from the Data Professor how to turn your research article into an interactive Streamlit app that can easily be shared with anyone!

Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Try out and explore Hugging Face diffusers with Abhishek's Diffuzers app.
Learn how to Use chatGPT to build a Machine Learning Web App in Python in Avra's YouTube tutorial.
Bo's TrainAnimeAI app lets you train like your favorite anime character! Input a character's name and get workout and diet plans based on them.
Carlos teaches how to Create repeatable items in Streamlit in his blog post.
Convert any question to an SQL query with Idriss' Text to SQL app.
Ben's Soccer Prospect Research & Radar Creation helps you find players that meet specific criteria.
Maciej's Ask my PDF app is a question-answering system built on GPT3.
Dash wrote a technical guide for building image recognition applications in Snowflake.
Search directly in Google Scholar with Ayoub's Scholar Scrap app.
Whom does your child look like is Daisy's image comparison app that provides similarity scores of a child to parents.
Stuck learning Python? Make it fun with Streamlit—Sasha's video shows his development flow and how it helped his sales engineering job.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
October 2022
November 2022
December 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > February 2023
https://blog.streamlit.io/monthly-rewind-february-2023/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > February 2023

Your February look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, March 22 2023
🏆 App of the month 🏆
Streamlit February updates
🔍 Current release: 1.20.0
🔮 Upcoming
⚡️ New caching commands
🪆 Columns inside columns
🚀 Editable dataframes
Featured Streamlit content
🦾 Using Streamlit for semantic processing with semantha
🔐 Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component
🤖 Using ChatGPT to build a Kedro ML pipeline
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our February featured app of the month is... 🥁🥁🥁🥁🥁

The Dungeon by Tomasz Hasiów!

Explore the depths of an ancient dungeon in this Streamlit-based dungeon crawler game. [code]

Streamlit February updates

Below are the latest updates and releases from February.

🔍 Current release: 1.20.0

The latest release is 1.20.0. Recent updates include granular control over app embedding behavior, a cleaner hamburger menu, and the de-experimentalization of faster reruns. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

New features to get excited about:

Step parameter for st.time_input
Tooltips on text elements
Improved st.help

Visit our roadmap app to see what else we're working on. 🥳

⚡️ New caching commands
Introducing st.cache_data and st.cache_resource—two new simpler and faster commands to replace st.cache! Learn more about them here.

🪆 Columns inside columns
st.columns now supports up to one level of column nesting! Check out the demo app for examples.

🚀 Editable dataframes
Editable dataframes are here! With st.experimental_data_editor you can now interact with the dataframes in your apps.

Featured Streamlit content

🦾 Using Streamlit for semantic processing with semantha
Learn how to integrate semantic AI processing into your apps and use cases. Sven Koerner outlines the steps using Streamlit and semantha.

🔐 Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component
Learn from Mohammad Khorasani how to add advanced features to the Streamlit-Authenticator component in Part 2 of the blog series.

🤖 Using ChatGPT to build a Kedro ML pipeline
Want to learn from ChatGPT how to deploy and manage ML models with Kedro and Streamilt? See how Arvindra Sehmi asked it to teach him just that.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Sasmitha's KnowledgeGPT app allows you to ask questions about your documents and get accurate answers with instant citations.
Get scientific searches and insights all in one place with Avra's PubLit—now using GPT3.
Jacob outlines How to Build a Dividend Investing Dashboard in Python and Streamlit in his Medium article.
Build a ChatGPT-like chatbot using LangChain, GPT-3, and Streamlit in echohive's video: Langchain ChatGPT your documents challenge with Gpt 3 and OpenAI embeddings and Streamlit UI.
Present your portfolio and projects as visually pleasing cards with Tom's Streamlit Cardfolio: A Portfolio Presentation App!
Tyler's Caltrain Platform app shows the real-time status of trains to improve your commute.
Stephan's GPTFlix is like a ChatGPT for movie reviews—ask it questions on movie knowledge, reviews, and recommendations.
Alexander shows how to become one of the top managers in Fantasy Premier League With Snowflake, Streamlit & Python.
Culture Biosciences created InSiliCHO, a mechanistic model of CHO cell dynamics for exploration of model-assisted DOE, forecasting, and more.
In Aleksa's video Building web apps using Streamlit | Streamlit crash course | MLOps series #2, you'll learn how to build a fully-fledged web app using Streamlit + HuggingFace Inference API.
Nicholas goes over How to Code a Machine Learning Lip Reading App with Python Tensorflow and Streamlit in his YouTube tutorial.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2023
January 2023
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
October 2022
November 2022
December 2022
2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > March 2023
https://blog.streamlit.io/monthly-rewind-march-2023/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > March 2023

Your March look-back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, April 24 2023
🏆 App of the month 🏆
Streamlit March updates
🔍 Current release: 1.21.0
🔮 Upcoming
✨ New Docs feature
Featured Streamlit content
🙋 10 most common explanations on the Streamlit forum
🔍 Create a search engine with Streamlit and Google Sheets
💻 Building a PivotTable report with Streamlit and AG Grid
🧑‍💻 Hackathon 101: 5 simple tips for beginners
#️⃣ Building an Instagram hashtag generation app with Streamlit
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our March featured app of the month is... 🥁🥁🥁🥁🥁

Music Source Splitter by Fabio Grasso!

This app allows you to extract vocal and instruments from any audio. It uses a pre-trained model called Hybrid Spectrogram and Waveform Source Separation. You can upload an audio file, paste in a URL, or even record your own. [code]

Streamlit March updates

Below are the latest updates and releases from March.

🔍 Current release: 1.21.0

The latest release is 1.21.0. Recent updates include improved st.help, support of global secrets.toml file, and a step parameter for st.time_input. Check out the changelog to learn more about the latest features and fixes.

🔮 Upcoming

New features to get excited about:

Easily connect to data sources with st.experimental_connection
Customize the visibility items in the toolbar, options menu, and the settings dialog
Ability to embed Streamlit apps

Visit our roadmap app to see what else we're working on. 🥳

✨ New Docs feature
View the source code of any Streamlit command by clicking the [source] button next to the function signature in the API reference. You can also go back and see the code from previous Streamlit versions.

Featured Streamlit content

🙋 10 most common explanations on the Streamlit forum
Are you new to Streamlit and the community forum? Check out this guide from moderator Debbie Matthews explaining the 10 most common questions with helpful tips.

🔍 Create a search engine with Streamlit and Google Sheets
Learn from Sebastian Flores Benner how to build a search engine app using Streamlit, pandas, and a Google Sheets database—the use cases are endless.

💻 Building a PivotTable report with Streamlit and AG Grid
Make a PivotTable report with Streamlit and AG Grid! Creator of the popular AgGrid component Pablo Fonseca shows you how in 4 simple steps.

🧑‍💻 Hackathon 101: 5 simple tips for beginners
Interested in joining a hackathon? In this post, the Data Professor shares how to get started and 5 simple tips for a successful experience.

#️⃣ Building an Instagram hashtag generation app with Streamlit
Build an Instagram hashtag generation app with Dr. William Mattingly. Learn how to scrape the data, create dynamic components with custom keys, and display a nice visualization.


Featured community content

Here are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.

Use ChatGPT to assist you in your job search with Vaibhav's Careermocha app.
Guillaume and team created a ChatGPT-powered medical diagnostic and symptom-checking tool—Medical Diagnosis Assistant.
Interact with or create your own AI assistant with Dave's GPT Lab.
Bryan writes how to create a Streamlit app for Monitoring Sea Surface Temperature at the global level with GEE.
Easily generate sales copy for a product using Josiah's app Salesy - The Product Copy Assistant.
In Fanilo's video, Streamlit App Showcase | Tips for making your app stand out, he reviews awesome apps and gives tips on how to make yours awesome too!
Create visually appealing and accessible color themes for your apps with Yuichiro's Streamlit color theme editor.
Vinícius' LaTeX Longtable Generator app lets you generate LaTeX longtables quickly and conveniently.
Turn images into pixel art with customizable colors and settings using the Pixel Art Converter app by Akaz.
Learn how to Code Your Own ChatGPT Article Generator with Python & Streamlit in this video by Augmented Startups.
Emily provides a step-by-step walkthrough of a hacky approach for SiS usage tracking in her article Tracking SiS application usage: An interim solution.
With Alex's Cybersyn Financial Data App you can get financial data covering bank financials, locations, corporate structures, and more.
Input an Ethereum smart contract into Kofi's Contract Wizard app and it will describe to you what that contract does.
In Roel's Creating a Marketing Segmentation App with Streamlit & Snowpark article, you'll learn how to create a customer segmentation tool that can sync its selection back to the data warehouse.

Thanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.

Reach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

2023
January 2023
February 2023
2022
January 2022
February 2022
March 2022
April 2022
May 2022
June 2022
July 2022
August 2022
September 2022
October 2022
November 2022
December 2022
2021
January 2021 
February 2021
March 2021
April 2021
May 2021
June 2021
July 2021
August 2021
September 2021
October 2021
November 2021
December 2021
January 2022
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > May 2021
https://blog.streamlit.io/monthly-rewind-may-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > May 2021

Your May look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, June 7 2021
🏆 App of the month 🏆
Streamlit May updates
🔍 Current release: 0.82.0
🔮 Upcoming features
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our May featured app of the month is......🥁🥁🥁🥁🥁

Worst-Case Analysis for Feature Rollouts by The Crosstab Kite team.

This app shows a Bayesian analysis of a simulated staged rollout experiment. It was built as a demo to arguably show a method better than confidence intervals for this process. [code]

Streamlit May updates

Here are some updates on happenings at Streamlit this month.

🔍 Current release: 0.82.0

The latest release is 0.82.0. This update helps with memory management by running more aggressive garbage collection between scripts. If you haven't updated in a while, make sure to check out the changelog since we're continually releasing new features and fixes.

🔮 Upcoming features

Here are some new features to be on the lookout for:

st.download
st.card
Session State - coming VERY soon 👀
Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Listen to CEO Adrien and COO Amanda talk Streamlit with Chris Chinchilla on his podcast Chinchilla Squeaks.
Is a career in tech right for you? Amanda discusses with Aprés about career options in the tech field for women returning to work after caregiving.
Read how the Rasa team are using Streamlit with Rasalit - a tool to interactively explore and investigate trained Rasa models.
The latest Streamlit short in the core functions series goes over how to make a double-ended slider. Read more here.
Featured community content

Some great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

In his video tutorial Working with Streamlit Forms, Jesse goes over how to use the latest feature release
A Binance Premiums app from Robert tracks premia on the Binance exchange and provides tutorials on trades
Fanilo demonstrated the new st.form feature in his Choose your favorite cat app
André and Karthik trained a model in Detecting deforestation through satellite imagery and created an awesome dashboard
You can create APIs from Excel XLSX and CSV data files in Arvindra's APINESS app
Compare different battery dispatch methods and their impact on carbon and costs in this Designing a Building Battery Dispatch Strategy app by Obed
Jeff and team created Power Density, an app for initial geothermal exploration scenarios and educational use
In his article Productivity Tracking with the Notion API and Python Lucas shows how he built a simple project tracker dashboard with the newly released Notion API
Jesse teaches how to make a Text Classifier App with Streamlit and River Python (Online Machine Learning) in his video
Orit made her first ever Streamlit app The G-Trendalyser which gives you top & rising trends for 5 keywords, directly from Google Trends
Okkar's Analyzing LinkedIn Connections app provides data visualization of your network on LinkedIn
The Data Professor, known for his video tutorials, wrote a blog tutorial on How to Build a Machine Learning App in Python

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > April 2021
https://blog.streamlit.io/monthly-rewind-april-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > April 2021

Your April look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, May 5 2021
🏆 App of the month 🏆
Streamlit April updates
🔍 Current release: 0.81.0
🔮 Upcoming features
🌲 Series B
🎈 New Streamlit creators
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our April featured app of the month is......🥁🥁🥁🥁🥁

Playground by Ahmed Besbes.

This app allows you to interactively play with machine learning models directly from the browser. You can use it to test different models with varying hyper-parameters on a set of non-linear classification problems. [code]

Streamlit April updates

Here are some of the many exciting things that happened at Streamlit in April.

🔍 Current release: 0.81.0

April brought two big releases, the newest being 0.81.0. Many notable features were introduced in these releases including secrets management, forms, and submit button. If you haven't updated in a while, make sure to check out the changelog since we're continually releasing new features and fixes.

🔮 Upcoming features

Programmable state will be landing in the near future!

🌲 Series B

We announced our Series B round of funding led by Sequoia and backed by previous investors Gradient Ventures and GGV Capital. Read more from CEO Adrien in his blog post.

🎈 New Streamlit creators

We welcomed four new awesome community members to the Streamlit Creators program. Read more about them below and check out our creators page here.

Introducing the Q2 2021 Streamlit Creators!
It’s been a wild Q1 2021 for Streamlit, with the Series B announcement, the release of Secrets management and Theming for app customization! But we’re never too busy to recognize members of the Streamlit community, whose contributions breathe life into the community, demonstrate new and exciting use…
Streamlit
randyzwitch
Featured Streamlit content

Videos and articles written by or featuring the Streamlit team for your viewing and reading pleasure.

James outlines how to use Secrets Management in Streamlit sharing to securely connect to private data sources.

The new commands st.form and st.form_submit_button are explained by Abhi in his latest blog post.

Alex C-G wrote about the Jina component and how to build a Jina neural search within your app.

In Austin's conclusion to his Firestore tutorial, he shows how to build a Reddit clone and deploy it with secrets.

We released two more Streamlit Shorts this month in the core function series: How to make a slider and a select slider. Read more about Shorts here.
Featured community content

Some great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Robert's Distribution Analyser app allows you to interactively explore continuous distributions from SciPy and fit distribution(s) to your data
Ethan walks through how he deployed his app in Brewing a Coffee Recommender (Part 2)
Bhavesh shows how you can track prices of cryptocurrencies in his video tutorial Bitcoin Tracking with 20 lines of Python code
In Misra's video, she goes into How to Make Pie Charts in Streamlit Using Plotly
The Python Data Visualization Tour app by Jeff visualizes plots and code for 6 Python plotting libraries at once: bokeh, altair, matplotlib, seaborn, plotly (express) & pandas
Charly released a beta version of his app StreamProphet that lets you visualize your forecasted SEO traffic
Another great tutorial from 1littlecoder shows how to Build Streamlit Dashboard Template for Python Data Science | Tableau Alternative in Python
In his video How to deploy ML app in 2 mins | Streamlit Sharing, Anuj walks you through how to deploy your app on sharing
Gareth made interactive character interaction graphs in his Star Trek Script Analysis Dashboard
Aaron's app NFL Mock Draft Database determines probabilities of the draft results with nice visualizations
Justin outlines how Reverie Labs is Building Web Applications From Python Scripts with Streamlit in his blog post

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > March 2021
https://blog.streamlit.io/monthly-rewind-march-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > March 2021

Your March look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, April 5 2021
🏆 App of the month 🏆
Streamlit March updates
🔍 Current release: 0.79.0
🔮 Upcoming features
🖼 New website and logo
🐯 Tigergraph hackathon continued and ended
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our March featured app of the month is......🥁🥁🥁🥁🥁

OmicLearn by  OmicEra.

This app is a transparent exploration of machine learning for biomarker discovery from proteomics and omics data. It was developed to enable easy access to ML without the requirement for any programming or bioinformatic skills. [code]

Streamlit March updates

Here are some of the exciting things that happened at Streamlit in March.

🔍 Current release: 0.79.0

The newest Streamlit release is 0.79.0, which featured custom theming and tooltips. If you haven't updated in a while, make sure to check out the change log since we're continually releasing new features and fixes.

🔮 Upcoming features

A quick look at some of the upcoming features planned for Q2:

Programmable state
Submit button
Anchor links
Secrets—coming very soon, keep a lookout 👀
🖼 New website and logo

Our website got a full redesign to put even more focus on the contributions from the community. You can view a number of community apps in the new gallery and read more about our awesome creators.

Our logo also got a little polishing and we have some fun new stickers for the community.

🐯 Tigergraph hackathon continued and ended

Streamlit cohosted a hackathon with Tigergraph and Graphistry inspiring developers from around the world to connect, build apps and win prizes. Read more about it and view the winners here.

Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Read all about custom theming in the launch article written by Abhi. He goes over the details of theming as well as the new dark mode feature.

Marisa walks you through how to use custom themes and dark mode in our theming tutorial.

We released a number of new Streamlit Shorts this month primarily revolving around core functions. Read more about Shorts here and be on the lookout for future series!

Featured community content

Some great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Bolei shared a preview of an app exploring SeFa: Closed Form Factorization of Latent Semantics in GANs
Mala wrote a helpful tutorial on how to Build Your First Data Visualization Web App in Python Using Streamlit
Bone segmentator by William is an app made to help biodesigners in fitting internal prostheses
In his article, Yong shows how to Build Your First Interactive Data Science Web App with Streamlit
Jesse gave a tutorial on How to Add File Downloads To Streamlit Apps (Custom Functions) in his Youtube video
Jesse also demonstrated how to use the new theming feature in his video Streamlit Themes & How to Customize Streamlit Apps
Ken Jee began a project March Madness Solved With Machine Learning? (Can I do it?) where he'll see if his ML model can dominate March madness brackets
Simon shared that his open source PyMedPhys app had a major release update
Harsh gave a helpful introduction to custom themes in his video tutorial Change Colour Scheme of Your Streamlit App!
Felipe's app APOD Project brought an astronomy picture of the day provided by NASA API
Another helpful Data Professor tutorial came out on How to build an app for combining the contents of multiple spreadsheets | Streamlit

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Monthly rewind > February 2021
https://blog.streamlit.io/monthly_rewind_february_2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > February 2021

Your February look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, March 8 2021
🏆 App of the month 🏆
Streamlit February updates
🔍 Current releases: 0.78.0
🕹 Closed betas
🐯 Tigergraph hackathon began and is currently underway
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our February featured app of the month is......🥁🥁🥁🥁🥁

Bayesian Deep Learning for Galaxy Zoo DECaLS by Mike Walmsley.

From 2015-2020, Galaxy Zoo volunteers made 1.8 million detailed classifications of 314,000 DECaLS galaxies. This app allows you to explore that data and predicts morphology posteriors. [code]

Streamlit February updates

A lot can happen in a month- even the shortest one of the year. Here's what you might have missed in February.

🔍 Current releases: 0.78.0

The newest Streamlit release is 0.78.0, but make sure to check out the change log if you haven't updated in a while to see what's been going on.

🕹 Closed betas

We currently have a number of coming soon features that are being tested in closed beta, including:

Secrets management
Programmable state
Customizable theming — coming very soon, check out this sneak peek:

If you're interested in testing any of these send us a message on the forum and we'll add you to the beta!

🐯 Tigergraph hackathon began and is currently underway
Streamlit is cohosting a hackathon with Tigergraph and Graphistry through March 22nd! Connect with developers around the world and build a web app with a chance to win $15,000+ in prizes. Check out the details and sign up here.
Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Listen to CEO Adrien discuss his journey from undergrad to Streamlit on Ken Jee's podcast Ken’s Nearest Neighbors.

COO Amanda's fireside chat with Glenn Solomon of GGV Capital from Open Core Summit 2020 is now available to stream. Listen in as they chat about the founding of Streamlit, community, and plans for the future.

Matt Brems wrote How to Use Roboflow and Streamlit to Visualize Object Detection Output showing how to create a blood cell count detection app.

Featured community content

Some great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Yuichiro Tachibana wrote in depth article about how he developed the real time video processing component WebRTC within Streamlit
Johannes put together a best-of-streamlit list featuring great community apps ranked by GitHub stars
Mohammad outlines how to Develop and deploy a UI with Python in under 15 minutes with Streamlit
In another one of his helpful tutorial videos, Jesse goes over Building A Course Recommender App with Streamlit & Udemy Dataset
Yong offers 8 Simple and Useful Streamlit Tricks You Should Know in this article to help better your apps
The Python Engineer made a video tutorial on how build a stock prediction web app in Python using Streamlit, Yahoo Finance, and Facebook Prophet
An NLP app by Charly, Wiki Topic Grapher, retrieves entity relationships from any Wikipedia seed topic. You can then get a network graph of these connected entities, save the graph as jpg or export the results ordered by salience to CSV
Yash's article A Guide to Streamlit — Frontend for Data Science Made Simpler walks through the important steps in making your project great
Part Time Larry demonstrates how to build Financial Dashboards with Python in this thorough video
An interactive Game of Thrones dashboard by Mario depicts interesting character analysis based on the TV scripts of the show
Rahul wrote an article on how to make your GitHub profile stand out by using his GitHub Profile README app
See and explore beautifully displayed Norwegian Meteorological data in this app by Gregoire

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit
https://blog.streamlit.io/p/efb2b033-a8e4-47ec-bb60-f1313d84052a/discuss.streamlit.io/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
404
Page not found
← Go to the front page
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image-2.png (1200×1041)
https://blog.streamlit.io/content/images/2021/08/image-2.png#border


appspart2.png (2000×1628)
https://blog.streamlit.io/content/images/2021/08/appspart2.png#border


databasesnew.png (1488×930)
https://blog.streamlit.io/content/images/2021/07/databasesnew.png


Teaser-GIF-2.gif (1920×1080)
https://blog.streamlit.io/content/images/2021/08/Teaser-GIF-2.gif#browser


Jessica Smith - Streamlit
https://blog.streamlit.io/author/jessica/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jessica Smith
26 posts
Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Monthly rewind > November 2022

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2022
Monthly rewind > October 2022

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2022
Monthly rewind > September 2022

Your September look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2022
Monthly rewind > August 2022

Your August look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2022
Monthly rewind > July 2022

Your July look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 9 2022
Monthly rewind > June 2022

Your June look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 6 2022
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1.png (1061×601)
https://blog.streamlit.io/content/images/2021/08/1.png#browser


group_selection_example.gif (1356×704)
https://blog.streamlit.io/content/images/2021/08/group_selection_example.gif#browser


Labeling ad videos with Streamlit
https://blog.streamlit.io/labeling-ad-videos-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Labeling ad videos with Streamlit

How Wavo.me uses Streamlit’s Session State to create labeling tasks

By Anastasia Glushko
Posted in Advocate Posts, September 2 2021
1. Why does Wavo label ad videos?
2. What makes Streamlit a great labeling solution?
3. How to build a creative labeling app using Streamlit's SessionState
Come up with a simple app layout
Use Session State to remember users and labeled videos
Make the app shine with some fun add-ons
Wrapping up
Contents
Share this post
← All posts

The clips labeled in the app preview are sample cutouts from a music video "Pony" by a Montreal-based hip-hop artist SLM.

Marketing ROI (return-on-investment) in the music industry is unpredictable. To break through, new artists have to invest in advertising. But will it succeed? A lot depends on the ad creative's quality.

There are many tips out there on how to make a video ad stand out. But advertising music is different from advertising physical products, like clothing and house decor: music can be consumed directly in the ad.

At Wavo, we use the latest machine learning technology to make investing in artists more predictable and scalable. Our unique combination of technology and music industry expertise helps develop data-driven solutions to understand and optimize the quality of music marketing.

In this post, we'll cover:

Why does Wavo label ad videos?
What makes Streamlit a great labeling solution?
How to build a creative labeling app using Streamlit's SessionState

Let's jump right in!

1. Why does Wavo label ad videos?

Ads perform according to the features of the promotional materials used in them, or Ad Creatives (AC for short).

Labeling AC features with Streamlit makes it possible to link them to performance metrics such as click-through rate, impressions, and views. By analyzing labeled data, we can better understand which ad features drive these performance metrics and build more effective campaigns.

To test this, we created a protocol for a proof-of-concept data science project. Our goal was to link metrics like click-through rate and video views to specific AC features and best practices.

We had three big resources:

500 music video clips
10 creative best practices from 5 years of running music advertising campaigns
22 media analysts with vast experience running music marketing campaigns
2. What makes Streamlit a great labeling solution?

At first, we looked into using AWS Ground Truth and LimeSurvey. Both tools were quite powerful.  But for our purposes, most features would go unused and only add complexity to the labeling process. We wanted a fast, simple, and inexpensive prototype, and Streamlit fit the bill!

I knew about Streamlit from my Insight Fellows days. While I spent days struggling with editing Bootstrap templates in html (ugh!), other “fellows” from my cohort used Streamlit. Their first machine learning apps were up in hours, all made in Python. The code looked impressively straightforward. I made a mental note to use only Streamlit going forward.

A year later, I had the opportunity to try it. I crossed my fingers that Streamlit could help me create a cohesive questionnaire: present a creative labeling form, store the labeler's response, and move on to the next AC. As it turned out, it could.

3. How to build a creative labeling app using Streamlit's SessionState

Come up with a simple app layout

We built the labeling questionnaire in one sprint, with 150 lines of Python code.

You can do it, too! Here's how it worked.

The user (our in-house marketing expert) selected their name from a dropdown of user IDs. This triggered an individual list of ACs to be loaded by the app. The first video creative appeared on the screen with the corresponding labeling tasks. The user then watched the video and rated the creative quality (low, medium, or high).

Next, the user entered the length of the video. Several checkboxes appeared—one for each creative best practice we recommend our clients to follow. The user had then selected all the best practices. This also included flags for the creative type: whether the video was a static visualizer or an animation.

Use Session State to remember users and labeled videos

Without the use of Session State, a simple Streamlit app would run your Python script from top to bottom. With every rerun of the script, it would lose any changes the user had made in the browser. The SessionState feature enables the simple persistence of these browser state changes.

We needed our app to retain two things in Session State, so that it could progress through our list of questions: the user ID (the name of the labeling expert) and the current question number. The user ID was taken from the user’s selection in the dropdown menu:

id_provided = st.selectbox('Hello! Who is this?', user_ids) 

# user_ids is a list defined above


To allow our labelers to start the questionnaire, take a break, close the app, and return to it later, we stored the answer to every question in a .csv file.

On every run of the script, Session State would be updated with the index of the next unanswered question. So whenever the user would return, they'd be exactly where they left off.

The same code ensured that the question number was updated on every consecutive run of the app (corresponding to all the labeling tasks for one of the video creatives):

import os
import pandas as pd
import streamlit as st

# check if the user is new or returning
output_filename = ‘./results_’ + str(id_provided) + ‘.csv’
	df = pd.DataFrame()
	if os.path.isfile(output_filename):
		df = pd.read_csv(output_filename)
	
	if df.shape[0] > 0:
		last_row = df.shape[0] - 1
		question_number = int(df.iloc[last_row].q_num) + 1
	else:
		question_number = 0

# defining our Session State
st.session_state.user_id = id_provided
st.session_state.question_number = question_number


When the user clicked through the labeling tasks for a given video and the script reruns, the Session State was updated: the id_provided didn’t change (unless the user selected a different ID in the dropdown menu) and the question_number increased by one. For every video labeling iteration, the question_number was used to access the right link from the list of the videos to label.

Make the app shine with some fun add-ons

To showcase some more fun use cases of Session State, we also had a progress bar at the top of the page:

st.progress((st.session_state.question_number)/(len(creatives))) 

# creatives is a list of links to the labelled video ads


And then there’s one final reward:

if st.session_state.question_number >= len(creatives):
		st.text('THANK YOU, YOU ARE DONE!')
		st.balloons()
		st.stop()


Wrapping up

Over the course of one week, 23 marketing experts labeled 500 creatives. This allowed us to report data-driven insights about creative quality to our clients and build better marketing campaigns.

With Streamlit, developing a custom and user-friendly labeling questionnaire for this project took just a few hours of work for one data scientist. 10/10 we would use Streamlit again.

Got questions? Let me know in the comments or via email.

Article by
Anastasia Glushko
Machine Learning Researcher

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Make a video content analyzer app with Streamlit and AssemblyAI
https://blog.streamlit.io/make-a-video-content-analyzer-app-with-streamlit-and-assemblyai/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

By Misra Turp
Posted in Advocate Posts, November 3 2022
What's a content analyzer app?
Step 1. Create an AssemblyAI account
Step 2. Collect user input
Step 3. Submit the video to AssemblyAI for analysis
Step 4. Receive analysis results from AssemblyAI
Step 5. Display analysis results
Video Summary
Sensitive Topics
Topic Detection
Wrapping up
Contents
Share this post
← All posts

Hello, community! 👋

My name is Mısra Turp, and I work as a developer educator at AssemblyAI.

We partner with YouTube creators to make helpful content for the AI community. Our creators make videos about technology, AI, deep learning, and machine learning. We screen the videos for sensitive or harmful content, which can take a long time. So I made an AI-powered app that can analyze video content automatically (yay!).

In this post, I'll share with you how to build and use the content analyzer app step-by-step:

Create an AssemblyAI account
Collect user input with Streamlit
Submit the video to AssemblyAI for analysis
Receive analysis results from AssemblyAI
Present analysis results in 3 sections: summary, topics, sensitive content

But before we start, let's talk about...

💡
Can’t wait to see it? Here’s the app and the Github repo with all the code. And check out this video tutorial on the same topic!
What's a content analyzer app?

The content analyzer app helps you evaluate the videos of a given channel very quickly. Given a list of videos in a TXT file, the app gives the user the option to select a video to analyze by visualizing the video's thumbnails:

Once a video is selected, the app reports a summary of the video, a list of sensitive topics (if any), and all the topics that were discussed:

Step 1. Create an AssemblyAI account

To start, create a free AssemblyAI account and get a Free API key. Then install Streamlit, pandas, requests, and pytube Python libraries:

pip install streamlit
pip install pandas
pip install requests
pip install pytube

Step 2. Collect user input

Next, build the skeleton of the Streamlit app by including instructions and headlines:

import streamlit as st

st.title("Analyze a YouTube channel's content")
st.markdown("With this app you can audit a Youtube channel to see if you'd like to sponsor them. All you have to do is to pass a list of links to the videos of this channel and you will get a list of thumbnails. Once you select a video by clicking its thumbnail, you can view:")
st.markdown("1. a summary of the video,") 
st.markdown("2. the topics that are discussed in the video,") 
st.markdown("3. whether there are any sensitive topics discussed in the video.")
st.markdown("Make sure your links are in the format: <https://www.youtube.com/watch?v=HfNnuQOHAaw> and not <https://youtu.be/HfNnuQOHAaw>")


Now you need a list of URLs from the user. They can upload it through the Streamlit file_uploader. If the user doesn't have anything yet and wants to play around with the app, they can select a checkbox to use a default TXT file:

default_bool = st.checkbox('Use default example file', )

if default_bool:
	file = open('./links.txt')
else:  
	file = st.file_uploader('Upload a file that includes the video links (.txt)')


Next, download the audio of this video from YouTube by casting the given TXT file into a Python list and passing each URL through a function called save_audio:

if file is not None:
	print(file)
	dataframe = pd.read_csv(file, header=None)
	dataframe.columns = ['video_url']
	urls_list = dataframe["video_url"].tolist()
	
	titles = []
	locations = []
	thumbnails = []
	
	for video_url in urls_list:
	  video_title, save_location, thumbnail_url = save_audio(video_url)
	  titles.append(video_title)
	  locations.append(save_location)
	  thumbnails.append(thumbnail_url)

#	... (continued if file is not None:)


This function returns the video title, the location where the audio is saved, and the thumbnail. The information is saved in separate lists:

from pytube import YouTube
import os

@st.experimental_memo
def save_audio(url):
	yt = YouTube(url)
	video = yt.streams.filter(only_audio=True).first()
	out_file = video.download()
	base, ext = os.path.splitext(out_file)
	file_name = base + '.mp3'
	os.rename(out_file, file_name)
	print(yt.title + " has been successfully downloaded.")
	print(file_name)
	return yt.title, file_name, yt.thumbnail_url


Now that you have the information on every video, you can display it in a grid for the user to select one. There is no default Streamlit widget for this, but Streamlit user vivien made a custom component that displays clickable images in a grid.

To create this widget, pass the list of thumbnails, the list of video names (to show when hovered over the thumbnail), and the styling details to the constructor. If there are too many URLs, the display might take up too much space. To avoid that, in the div_style set "overflow-y":"auto" to have a scroll bar.

Whichever thumbnail the user clicks, that video will be selected for the analysis:

from st_clickable_images import clickable_images

#	... (continued if file is not None:)

  selected_video = clickable_images(thumbnails,
  titles = titles,
  div_style={"height": "400px", "display": "flex", "justify-content": "center", "flex-wrap": "wrap", "overflow-y":"auto"},
  img_style={"margin": "5px", "height": "150px"}
  )

  st.markdown(f"Thumbnail #{selected_video} clicked" if selected_video > -1 else "No image clicked")

#	... (continued if file is not None:)


Step 3. Submit the video to AssemblyAI for analysis

If the user doesn't select a video, the widget will return -1. If the user selects a video from the grid, the widget will return the video's number starting from 0. Using this number, we can also get the video's URL, title, and location from the lists we previously created. The app will display the title of the selected video and an audio player:

#	... (continued if file is not None:)

if selected_video > -1:
	video_url = urls_list[selected_video]
  video_title = titles[selected_video]
  save_location = locations[selected_video]
          
  st.header(video_title)
  st.audio(save_location)

# ... (continued if selected_video > -1:)


Once a video is selected, you need to:

Upload the audio to AssemblyAI;
Start the analysis;
Read the results;

For each of these steps, there is a separate function. To use these functions:

Import requests library to send requests to AssemblyAI;
Set up transcription and uploading endpoints (addresses) to communicate with AssemblyAI;
Set up a header to specify the content-type and authenticate your app with the API key that you created earlier;

I've uploaded this app to Streamlit Community Cloud and am specifying st.secrets["auth_key"] as the location of the API key. If you'd like, you can also directly pass your API key there:

import requests

transcript_endpoint = "<https://api.assemblyai.com/v2/transcript>"
upload_endpoint = "<https://api.assemblyai.com/v2/upload>"

headers = {
	"authorization": st.secrets["auth_key"],
	"content-type": "application/json"
}


The first step is to upload this audio file to AssemblyAI with the function upload_to_AssemblyAI. Using the helper function read_file, the function reads the audio file in the given location (save_location) in chunks. This is used in the post request that is sent to the upload_endpoint of AssemblyAI together with the header for authentication.

As a response, you get the URL to where the audio file is uploaded:

@st.experimental_memo
def upload_to_AssemblyAI(save_location):
	CHUNK_SIZE = 5242880
	
	def read_file(filename):
	  with open(filename, 'rb') as _file:
		  while True:
	      print("chunk uploaded")
	      data = _file.read(CHUNK_SIZE)
	      if not data:
	        break
        yield data
	
	upload_response = requests.post(
	  upload_endpoint,
	  headers=headers, data=read_file(save_location)
	)
	print(upload_response.json())
	
	audio_url = upload_response.json()['upload_url']
	print('Uploaded to', audio_url)
	
	return audio_url


Next, you pass the audio_url to the start_analysis function. This function sends another post request to AssemblyAI to start the analysis.

By default, all submitted audio is transcribed at AssemblyAI. To start a transcription (or analysis) job, specify the audio file URL, the authentication details, and the kind of analysis you want (read our docs for a list of deep learning models).

Here you'll use three AssemblyAI models:

The Summarization model—to return the summary of this audio file;
The Content Moderation model—to flag potentially sensitive and harmful content on topics such as alcohol, violence, gambling, and hate speech;
The Topic Detection model—to detect up to 700 topics (automotive, business, technology, education, standardized tests, inflation, off-road vehicles, and so on);

The summarization model can give you different summaries:

A bullet list (bullets)
A longer bullet list (bullets_verbose)
A few words (gist)
A sentence (headline)
A paragraph (paragraph)

The analysis will take a few seconds or minutes, depending on the length of the audio file. As a response to the transcription job, you'll get a job ID. Use it to create a polling endpoint to receive the analysis results:

@st.experimental_memo
def start_analysis(audio_url):
	
	## Start transcription job of audio file
	data = {
	    'audio_url': audio_url,
	    'iab_categories': True,
	    'content_safety': True,
	    "summarization": True,
	    "summary_type": "bullets"
	}
	
	transcript_response = requests.post(transcript_endpoint, json=data, headers=headers)
	print(transcript_response)
	
	transcript_id = transcript_response.json()['id']
	polling_endpoint = transcript_endpoint + "/" + transcript_id
	
	print("Transcribing at", polling_endpoint)
	return polling_endpoint

Step 4. Receive analysis results from AssemblyAI

The last step is to collect the analysis results from AssemblyAI. The results are not generated instantaneously. Depending on the length of the audio file, the analysis might take a couple of seconds to a couple of minutes. To keep it simple and reusable, the process of receiving the analysis is wrapped in a function called get_analysis_results.

In a while loop, every 10 seconds, a get request will be sent to AssemblyAI through the polling endpoint that includes the transaction job ID. In response to this get request, you'll get the job status as, "queued", “submitted”, “processing”, or “completed”.

Once the status is "completed", the results are returned:

@st.experimental_memo
	def get_analysis_results(polling_endpoint):
	
	status = 'submitted'
	
	while True:
	  print(status)
	  polling_response = requests.get(polling_endpoint, headers=headers)
	  status = polling_response.json()['status']
	  # st.write(polling_response.json())
	  # st.write(status)
	
	  if status == 'submitted' or status == 'processing' or status == 'queued':
	    print('not ready yet')
	    sleep(10)
	
	  elif status == 'completed':
	    print('creating transcript')
	    return polling_response
			break

	  else:
	    print('error')
	    return False
	    break

Step 5. Display analysis results

You get three types of analysis on your audio:

Summarization;
Sensitive content detection;
Topic detection;

Let's display them in order.

Extract the information with the “summary” keyword for the summarization results,“content_safety_labels” for content moderation and “iab_categories_result” for topic detection. Here is an example response:

{
    "audio_duration": 1282,
    "confidence": 0.9414384528795772,
    "id": "oeo5u25f7-69e4-4f92-8dc9-f7d8ad6cdf38",
    "status": "completed",
    "text": "Ted talks are recorded live at the Ted Conference. This episode features...",
    "summary": "- Dan Gilbert is a psychologist and a happiness expert. His talk is recorded live at Ted conference. He explains why the human brain has nearly tripled in size in 2 million years. He also explains the difference between winning the lottery and becoming a paraplegic.\
- In 1994, Pete Best said he's happier than he would have been with the Beatles. In the free choice paradigm, monet prints are ranked from the one they like the most to the one that they don't. People prefer the third one over the fourth one because it's a little better.\
- People synthesize happiness when they change their affective. Hedonic aesthetic reactions to a poster. The ability to make up your mind and change your mind is the friend of natural happiness. But it's the enemy of synthetic happiness. The psychological immune system works best when we are stuck. This is the difference between dating and marriage. People don't know this about themselves and it can work to their disadvantage.\
- In a photography course at Harvard, 66% of students choose not to take the course where they have the opportunity to change their mind. Adam Smith said that some things are better than others. Dan Gilbert recorded at Ted, 2004 in Monterey, California, 2004.",
    "content_safety_labels": {
        "status": "success",
        "results": [
            {
                "text": "Yes, that's it. Why does that happen? By calling off the Hunt, your brain can stop persevering on the ugly sister, giving the correct set of neurons a chance to be activated. Tip of the tongue, especially blocking on a person's name, is totally normal. 25 year olds can experience several tip of the tongues a week, but young people don't sweat them, in part because old age, memory loss, and Alzheimer's are nowhere on their radars.",
                "labels": [
                    {
                        "label": "health_issues",
                        "confidence": 0.8225132822990417,
                        "severity": 0.15090347826480865
                    }
                ],
                "timestamp": {
                    "start": 358346,
                    "end": 389018
                }
            },
            ...
        ],
        "summary": {
            "health_issues": 0.8750781728032808
            ...
        },
        "severity_score_summary": {
            "health_issues": {
                "low": 0.7210625030587972,
                "medium": 0.2789374969412028,
                "high": 0.0
            }
        }
    },
    "iab_categories_result": {
        "status": "success",
        "results": [
            {
                "text": "Ted Talks are recorded live at Ted Conference...",
                "labels": [
                    {
                        "relevance": 0.0005944414297118783,
                        "label": "Religion&Spirituality>Spirituality"
                    },
                    {
                        "relevance": 0.00039072768413461745,
                        "label": "Television>RealityTV"
                    },
                    {
                        "relevance": 0.00036419558455236256,
                        "label": "MusicAndAudio>TalkRadio>EducationalRadio"
                    }
                ],
                "timestamp": {
                    "start": 8630,
                    "end": 32990
                }
            },
            ...
        ],
        "summary": {
            "MedicalHealth>DiseasesAndConditions>BrainAndNervousSystemDisorders": 1.0,
            "FamilyAndRelationships>Dating": 0.7614801526069641,
            "Shopping>LotteriesAndScratchcards": 0.6330153346061707,
            "Hobbies&Interests>ArtsAndCrafts>Photography": 0.6305723786354065,
            "Style&Fashion>Beauty": 0.5269057750701904,
            "Education>EducationalAssessment": 0.49798518419265747,
            "BooksAndLiterature>ArtAndPhotographyBooks": 0.45763808488845825,
            "FamilyAndRelationships>Bereavement": 0.45646440982818604,
            "FineArt>FineArtPhotography": 0.3921416699886322,
        }
}


First, call each of the functions defined above and then parse results to get each part of the analysis:

# ... (continued if selected_video > -1:)	

	# upload mp3 file to AssemblyAI
  audio_url = upload_to_AssemblyAI(save_location)

  # start analysis of the file
  polling_endpoint = start_analysis(audio_url)

  # receive the results
  results = get_analysis_results(polling_endpoint)
	
	# separate analysis results
	bullet_points = results.json()['summary']
	content_moderation = results.json()["content_safety_labels"]
	topic_labels = results.json()["iab_categories_result"]

# ... (continued if selected_video > -1:)	

Video Summary

It's easy to display the summary since it comes in a nicely formatted bullet list. You only need to extract it from the JSON response and display it with st.write():

# ... (continued if selected_video > -1:)		

	st.header("Video summary")
	st.write(bullet_points)

# ... (continued if selected_video > -1:)	


Sensitive Topics

The content moderation model will give you detailed information on the following:

The sentence that caused this audio to be flagged
The timestamp of when it starts and ends
The severity of this sensitive topic
The confidence in this detection

In the context of this project, the user doesn't need to see this much detail, so let's display the summary of this analysis as a pandas dataframe:

# ... (continued if selected_video > -1:)	

st.header("Sensitive content")
	if content_moderation['summary'] != {}:
	  st.subheader('🚨 Mention of the following sensitive topics detected.')
	  moderation_df = pd.DataFrame(content_moderation['summary'].items())
	  moderation_df.columns = ['topic','confidence']
	  st.dataframe(moderation_df, use_container_width=True)
	else:
	  st.subheader('✅ All clear! No sensitive content detected.')

# ... (continued if selected_video > -1:)	


The result will look like this:

Or like this:

Topic Detection

The topic detection model will give you similar results. Once pasted into a pandas dataframe, structure it to have a separate column for each topic granularity level, then sort it with the confidence from the topic detection model:

# ... (continued if selected_video > -1:)	

	st.header("Topics discussed")
	topics_df = pd.DataFrame(topic_labels['summary'].items())
	topics_df.columns = ['topic','confidence']
	topics_df["topic"] = topics_df["topic"].str.split(">")
	expanded_topics = topics_df.topic.apply(pd.Series).add_prefix('topic_level_')
	topics_df = topics_df.join(expanded_topics).drop('topic', axis=1).sort_values(['confidence'], ascending=False).fillna('')
	
	st.dataframe(topics_df, use_container_width=True)

# ... (continued if selected_video > -1:)	


Wrapping up

And that's a wrap! Whew. You did it.

This content analyzer app makes analyzing YouTube super easy, doesn't it? Once you connect to AssemblyAI, you can make more Streamlit apps. Check out our documentation to learn about our other state-of-the-art models and how to use them to get information from your audio or video files. Or watch my video tutorial, where I talk about this in detail.

If you have any questions about this app or if you build an app by using both AssemblyAI and Streamlit, please comment below or reach out to me on Twitter or YouTube.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

webrtc-1.gif (800×728)
https://blog.streamlit.io/content/images/2021/08/webrtc-1.gif#border


App Layout Primitives: Columns, Containers & Expanders
https://blog.streamlit.io/introducing-new-layout-options-for-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
New layout options for Streamlit

Introducing new layout primitives—columns, containers, and expanders!

By Austin Chen
Posted in Product, October 8 2020
Go horizontal with columns
Clean things up with expanders
Adding a new concept: containers!
Organize your code with... with
That's all, folks!
Resources
Kudos
Contents
Share this post
← All posts

Streamlit is all about simplicity. It’s pure Python. Your script runs from top to bottom. Your app renders from top to bottom too. Perfect, right? Well...not quite. Users noted that our thinking was a bit too vertical. The group griped about grids. The community clamored for columns. Fervent friends favored flexibility. You get the idea.

So move aside, vertical layout. Make a little space for... horizontal layout! And a bunch more layout primitives. And some syntactic goodies too. In fact, today, we're introducing four new layout features giving you much more control over your app’s presentation.

st.columns: Side-by-side columns where you can insert Streamlit elements
st.expander: An expand/collapse widget to selectively show stuff
st.container: The fundamental building block of layout
with column1: st.write("hi!"): Syntax sugar to specify which container to use
Go horizontal with columns

st.columns acts similarly to our beloved st.sidebar, except now you can put the columns anywhere in your app. Just declare each column as a new variable, and then you can add in ANY element or component available from the Streamlit library.

Use columns to compare things side-by-side:


col1, col2 = st.columns(2)

original = Image.open(image)
col1.header("Original")
col1.image(original, use_column_width=True)

grayscale = original.convert('LA')
col2.header("Grayscale")
col2.image(grayscale, use_column_width=True)



In fact, by calling st.columns inside a loop, you get a grid layout!


st.title("Let's create a table!")
for i in range(1, 10):
    cols = st.columns(4)
    cols[0].write(f'{i}')
    cols[1].write(f'{i * i}')
    cols[2].write(f'{i * i * i}')
    cols[3].write('x' * i)
    

You can even get quite complex (which can be great for wide monitors!) Here's an example that uses variable-width columns in conjunction with the wide-mode layout:


# Use the full page instead of a narrow central column
st.set_page_config(layout="wide")

# Space out the maps so the first one is 2x the size of the other three
c1, c2, c3, c4 = st.columns((2, 1, 1, 1))



And just in case you were wondering: yes, columns are beautiful across devices and automatically resize for mobile and different browser widths.

Clean things up with expanders

Now that we've maximized horizontal space, try st.expander, to maximize your vertical space! Some of you may have been using st.checkbox for this before, and expander is a prettier, more performant replacement 🙂

It's a great way to hide your secondary controls, or provide longer explanations that users can toggle!

Adding a new concept: containers!

If you squint a bit, st.columns, st.expander, and st.sidebar look kind of similar. They all return Python objects, which allow you to call all the Streamlit functions.  We've given these objects a new name: containers. And since it would be nice to create containers directly, you can!

st.container is a building block that helps you organize your app. Just like st.empty, st.container lets you set aside some space, and then later write things to it out of order. But while subsequent calls to the same st.empty replace the item inside it, subsequent calls to the same st.container append to it. Once again, this works just like the st.sidebar you've come to know and love.

Organize your code with... with

Finally, we're introducing a new syntax to help you manage all these new containers: with container. How does it work? Well, instead of making function calls directly on the container...


my_expander = st.expander()
my_expander.write('Hello there!')
clicked = my_expander.button('Click me!')



Use the container as a Context Manager and call functions from the st. namespace!


my_expander = st.expander(label='Expand me')
with my_expander:
    'Hello there!'
    clicked = st.button('Click me!')
    

Why? This way, you can compose your own widgets in pure Python, and reuse them in different containers!


def my_widget(key):
    st.subheader('Hello there!')
    return st.button("Click me " + key)

# This works in the main area
clicked = my_widget("first")

# And within an expander
my_expander = st.expander("Expand", expanded=True)
with my_expander:
    clicked = my_widget("second")

# AND in st.sidebar!
with st.sidebar:
    clicked = my_widget("third")
    

One last thing: the with syntax lets you put your Custom Components inside any container you like. Check out this app by community member Sam Dobson, which embeds the Streamlit Ace editor in a column right next to the app itself — so a user can edit the code and see the changes LIVE!

That's all, folks!

To start playing with layout today, simply upgrade to the latest version of Streamlit.


$ pip install streamlit --upgrade



Coming up are updates with padding, alignment, responsive design, and UI customization. Stay tuned for that, but most importantly, let us know what YOU want from layout. Questions? Suggestions? Or just have a neat app you want to show off? Join us on the Streamlit community forum — we can't wait to see what you create 🎈

Resources

Documentation
GitHub
Changelog

Kudos

A shoutout to the Streamlit Community and Creators, whose feedback really shaped the implementation of Layout: Jesse, José, Charly, and Synode — and a special callout to Fanilo for going the extra mile to find bugs, suggest APIs, and overall try out a bunch of our prototypes. Thank you all so much ❤️

Edit, 2021-08-2021: This post has been updated to fix a bug in the context manager section, as well as to reflect the removal of the beta_ prefix from several functions.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

cropped-gif.gif (1094×619)
https://blog.streamlit.io/content/images/2021/08/cropped-gif.gif#browser


4-2.png (1200×595)
https://blog.streamlit.io/content/images/2021/08/4-2.png#border


Screen_Shot_2021-04-26_at_5.06.52_PM--1-.png (731×196)
https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-04-26_at_5.06.52_PM--1-.png#border


2-3-1.png (1048×451)
https://blog.streamlit.io/content/images/2021/08/2-3-1.png#border


image-7.png (968×312)
https://blog.streamlit.io/content/images/2021/08/image-7.png#border


image-6.png (1802×972)
https://blog.streamlit.io/content/images/2021/08/image-6.png#browser


Screen_Shot_2021-04-26_at_5.05.10_PM--1-.png (723×192)
https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-04-26_at_5.05.10_PM--1-.png#border


image-5.png (2000×1237)
https://blog.streamlit.io/content/images/2021/08/image-5.png#browser


image-3.png (2039×1066)
https://blog.streamlit.io/content/images/2021/08/image-3.png#browser


image-2-1.png (2000×1004)
https://blog.streamlit.io/content/images/2021/08/image-2-1.png#browser


Screen_Shot_2021-06-03_at_4.15.58_PM--1--1.png (1848×454)
https://blog.streamlit.io/content/images/2021/06/Screen_Shot_2021-06-03_at_4.15.58_PM--1--1.png


Elm Tutorial | How to Build Streamlit Components Using Elm
https://blog.streamlit.io/elm-meet-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Elm, meet Streamlit

A tutorial on how to build Streamlit components using Elm

By Henrikh Kantuni
Posted in Tutorials, December 8 2020
Ready Player One
Brave New World
There and Back Again
Foundation
Contents
Share this post
← All posts

Let me start this article by saying—I love Elm!

I enjoy learning new programming languages, and Elm has been my favorite language for almost 2 years now. I like everything about it - compiler error messages, type system soundness, The Elm Architecture, pure functions, immutability, performance, etc. Around the time I discovered Elm, I got a job at Streamlit, and I immediately saw the potential for how Elm and Streamlit could work together to supercharge apps. With the release of the Streamlit components architecture earlier this year, this was finally possible, and I'm excited to show you how! To get a taste, check out this awesome Elm line charts library embedded into a Python data app.

Before we start, if you're new to Streamlit and would like to learn more about it - check out these two articles "Intro to Streamlit" and "Intro to Streamlit components".

Ready Player One

Here's the "Hello World" of Elm examples:


import Browser
import Html exposing (Html, button, div, text)
import Html.Events exposing (onClick)

main =
  Browser.sandbox { init = 0, update = update, view = view }

type Msg = Increment | Decrement

update msg model =
  case msg of
    Increment ->
      model + 1

    Decrement ->
      model - 1

view model =
  div []
    [ button [ onClick Decrement ] [ text "-" ]
    , div [] [ text (String.fromInt model) ]
    , button [ onClick Increment ] [ text "+" ]
    ]



It is a simple counter app that demonstrates the simplicity, robustness, and beauty of The Elm Architecture.

We're going to build a Streamlit app that will use the above example as a Streamlit component. Streamlit components let you expand the functionality provided in the base Streamlit package. You can use Streamlit components to share any web-based UI, widget, or data visualization code with the broader Python data science community.

Creating a Streamlit component takes - literally - 2 lines of Python.


import streamlit as st
import streamlit.components.v1 as components

counter_component = components.declare_component(
    "counter",
    url="http://localhost:3000/",
)

count = counter_component(key="count", default=0)
st.markdown(f"The value of the counter is **{count}**.")


We declare a new component by passing the name and the location of the component front-end files (or the URL of your development server).
We provide the default value for the counter.
We make sure that the component does not re-render unnecessarily by providing the key.
Brave New World

To establish a two-way connection between our app and the component, we are going to add ports to our Elm app.

To send a message from Elm to Streamlit, let's define a port that receives a number and produces a command.


port fromElm : Int -> Cmd msg



We will need to send the new value back to Streamlit on Increment and Decrement events. So let's modify our update function to reflect that.


Increment ->
    ( { model | count = model.count + 1 }
    , fromElm (model.count + 1)
    )

Decrement ->
    ( { model | count = model.count - 1 }
    , fromElm (model.count - 1)
    )


There and Back Again

To send a message from Streamlit to Elm, let's define a port that receives a number and produces a subscription.


port fromJS : (Int -> msg) -> Sub msg



Firstly, we will define a new message type.


type Msg
    = Default Int
    | Increment
    | Decrement



Secondly, we will add a handler for that message type to update.


Default value ->
    ( { model | count = value }
    , Cmd.none
    )



And finally, we will subscribe to the messages on that port.


subscriptions : Model -> Sub Msg
subscriptions _ =
    fromJS Default



When a message from JavaScript is sent to that port, the Default event will get a number and set the counter value to that number.

And that's it!

Foundation

I hope this tutorial will help you build dazzling components in Elm. I believe there are a lot of incredible Elm packages that would boost the look and feel of Python data apps. To give you an idea, elm-visualization would make a fantastic Streamlit Component - and there are many, many more. I'm excited to see more people discover the awesomeness of Elm, and I look forward to seeing what you create!

P.S. Both apps are available on GitHub and have been deployed using Streamlit sharing.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image-4.png (1520×938)
https://blog.streamlit.io/content/images/2021/08/image-4.png#border


Qiusheng Wu uses Streamlit to build a popular geospatial application
https://blog.streamlit.io/qiusheng-wu-uses-streamlit-to-build-a-popular-geospatial-application/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

By Qiusheng Wu and Ksenia Anske
Posted in Case study, July 21 2022
Jupyter environment wasn’t enough
Geemap got so popular, it maxed out free cloud hosting options
Discovering Streamlit
Streamlit powers the new version of geemap — and makes it available to anyone
Wrapping up
Contents
Share this post
← All posts

In April 2020, Assistant Professor in the Department of Geography at the University of Tennessee Qiusheng Wu launched geemap, an open-source Python package for interactive mapping with Google Earth Engine and open-source mapping libraries (e.g., ipyleaflet, folium). It quickly became one of the most popular geospatial packages with over 2,000 GitHub stars.

“Before Google Earth Engine, generating a satellite timelapse like this could take hours or even days,” Qiusheng said. “You can now create satellite timelapses in minutes, but it can still take hundreds of lines of Earth Engine JavaScript.”

Jupyter environment wasn’t enough

Qiusheng wanted to apply geospatial big data, machine learning, and cloud computing to study environmental change (surface water, wetland inundation dynamics, etc.).

With geemap, users could explore large geospatial datasets, perform planetary-scale analysis, and create satellite timelapses with a few lines of code or a few clicks. But they still needed to install Python and run geemap in a Jupyter environment.

Qiusheng wanted his app to let anyone create satellite timelapses with no code. So he used Voilà for turning notebooks into standalone web apps and dashboards.

Geemap got so popular, it maxed out free cloud hosting options

To make the notebooks public, Qiusheng needed to host them on a server.

“I’ve been using ngrok to turn a local computer into a secure web server and connect it to the ngrok cloud service, which accepts traffic on a public address,” Qiusheng said. “It’s one of the easiest ways to turn a Jupyter notebook into an app and it’s great for demos. But the downside of using a local computer as a public server is that it might be hacked.”

He switched to hosting it on a Heroku cloud server. Soon after, the app became so popular, and it used up the free monthly dyno hours.

“I had to shut down the app when it exceeded the free limit and restart it at the beginning of each month, which was inconvenient,” Qiusheng said.

Discovering Streamlit

Qiusheng first discovered Streamlit in October 2021. It quickly became his favorite package for developing and deploying interactive web apps. He used it for geemap because:

It was free.
It was open-source.
It had similar functionality to ipywidgets but was much easier to use (no need for a callback function).
He could deploy unlimited public apps from GitHub to Streamlit Community Cloud for free (no need for a server).
Deployment was automatic, so he could focus on coding.
The apps were publicly accessible.
Streamlit powers the new version of geemap — and makes it available to anyone

Soon after Qiusheng released the Streamlit Geospatial app for creating satellite timelapse animations for any location in less than 60 seconds, it got widely circulated on social media (check out the blog post, the video tutorial, and the repo code).

People all over the world made animations of environmental changes: urban growth, land reclamation, river dynamics, vegetation dynamics, coastal erosion, and volcanic eruptions (use hashtags #streamlit on Twitter and #BigRiverAnimation on LinkedIn to see examples).

There are now more than ten apps in Streamlit Geospatial, including the app for visualizing U.S. real estate data in 3D.

There is even an app for creating maps of hurricane tracks built with Streamlit and Tropycal (check out the repo code here).

To make it easier for users to make their own geospatial apps, Qiusheng created an app template based on Streamlit’s native support for multipage apps. Users can fork the repository and add more apps if needed. The app can be deployed to Streamlit Cloud, Heroku, or MyBinder (here is the repo code).

Wrapping up

“I’m an advocate of open science and reproducible research,” Qiusheng said. “I love sharing Streamlit apps and making geospatial technologies more accessible to everyone. I have developed several open-source packages for geospatial analysis and interactive mapping (e.g., geemap, leafmap, geospatial, pygis, lidar). You can see my open-source projects on GitHub and video tutorials on my YouTube channel.”

Thank you for reading Qiusheng’s story! If you have any questions, please leave them below in the comments or reach out to Qiusheng on Twitter, LinkedIn, or YouTube.

Happy coding and learning! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Firestore & Streamlit | Create a Serverless Web App in Python
https://blog.streamlit.io/streamlit-firestore/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit ❤️ Firestore

Use Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!

By Austin Chen
Posted in Tutorials, January 27 2021
TL;DR
Table of Contents
Intro: What is Firestore, and why should we use it?
What are some downsides of Firestore?
Part 1: A simple Streamlit Sharing app
Setting up your Git repo
Running Streamlit on your own computer
Deploying your app on Streamlit Sharing
Part 2: Setting up Firestore
Set up a Firebase account
Create a Firestore database
Create your first collection and document
Download your account keys 🗝️
To be continued...
Contents
Share this post
← All posts
TL;DR
Streamlit lets you build a real web app, in 20 lines of Python.
Streamlit Sharing hosts that app for you. No server needed.
Firestore lets you store and fetch data. No server needed, either!
Combine them for a serverless web app with persistent data, written entirely in Python!
Table of Contents
Intro: What is Firestore, and why should we use it?
Part 1: A simple Streamlit Sharing app
Part 2: Setting up Firestore
Part 3: Building the Reddit home page
Part 4: Tips and tricks for app development
Intro: What is Firestore, and why should we use it?

So what even is this Firestore thing? Well, from their website:

Cloud Firestore is a flexible, scalable database for mobile, web, and server development...

Let's break that down! First, Firestore is a database - that means it's a good place to store data, and retrieve it later. If you're into ML/data science, and have been building dashboards with Streamlit, the data you work with probably looks like Pandas tables, Numpy arrays, CSV files. These tend to be fairly static. You'll rarely need to create new entries or update them, let alone make your data accessible from apps all over the world. But many web apps need to be dynamic, allowing lots of edits to the data and creation of new data — and databases excel at this! Every consumer web app you can think of (Gmail, Youtube, Slack, Airbnb) have databases that manage the things that their users create (emails, videos, chat messages, and listings).

Unlike other databases, Firestore is flexible. In technical terms, we call it a "NoSQL" database, which means that you don't have to design a fixed schema for how your data will be structured before you begin. This is great for quick prototyping! Oftentimes, when I'm making a web app, I don't exactly know what it will look like or do, much less how I should store my data.

As an analogy, let's say I wanted to bake a bunch of desserts. A SQL database is a bit like having to write out a pie recipe beforehand, and then always following it line-by-line. I can efficiently bake lots of pies, which will be tasty if my original recipe was good. But the NoSQL approach is more like: let me start with something vaguely pie-like, but maybe it'll become a tart, or drift towards more of a cupcake... I can experiment until I land on something that really hits the spot.

Finally, Firestore is hosted on the cloud. This means that it's a service, very much like GitHub or our very own Streamlit Sharing, where you don't have to worry about maintaining your own servers. You don't need to worry about scaling, uptime, or a bunch of other issues that can get in the way of building out your idea.

What are some downsides of Firestore?

First, as a hosted service, Firestore can cost you money. They do have a generous free tier to start out with — no credit card needed! And if your app exceeds the free limits, they'll inform you and stop new writes. Even if you hit the paid tier, Firestore is pretty cheap; I run a side project with hundreds of daily users and thousands of daily writes, and Firestore only costs me ~$1/month.

Second, other databases may perform better on complicated queries, or when you get to e.g. millions of daily users. For pure speed of setup, Firestore is great, but if you already know that scaling will be an issue, you may want to consider other options like SQLite, MongoDB, or Cloud Spanner.

And finally, most Firestore users use JavaScript, so there are fewer Python code snippets and Stack Overflow answers to find. For this tutorial, we'll use Firestore's Python library, which is very pleasant to work with. Here's a snippet from their Python API for inspiration:

from google.cloud import firestore

# Add a new user to the database
db = firestore.Client()
doc_ref = db.collection('users').document('alovelace')
doc_ref.set({
    'first': 'Ada',
    'last': 'Lovelace',
    'born': 1815
})

# Then query to list all users
users_ref = db.collection('users')

for doc in users_ref.stream():
    print('{} => {}'.format(doc.id, doc.to_dict()))


Pretty easy to read, right?

Part 1: A simple Streamlit Sharing app

Okay! If you're sold on the awesomeness of Firestore, I'm now going to show you how to bake in that awesomeness into a Streamlit app. Let's cook up a simple version of the Reddit home page, from scratch! (Okay, that was the last cooking analogy, I promise.)

Once again, Streamlit is the fastest way to build web apps in Python. Streamlit Sharing is the fastest way to put your app online, so you can share it with the world. Here's what Sharing can do for you, in 1 minute:

Here in Part 1, I'm going to walk you step-by-step through the process of building a Streamlit Sharing app, even if you've never done it before. Ready? Let's do this!

If you're already a Streamlit master, feel free to skip down to Part 2; just remember to pip install google-cloud-firestore , or the equivalent in your Python library manager of choice.

Setting up your Git repo

We'll start by creating a new repo on GitHub — the folder where all your code will go. Name it whatever you like! (I'm calling mine streamlit-reddit). Make sure it's public, though, so we can host it on Streamlit Sharing later.

Next, you'll want to download (aka "clone") this repo on your own computer. (You'll need to have Git installed.) Open up a local shell, and run the following commands, replacing {username} with your own GitHub username:

# Creates a copy of this repo on your computer
$ git clone https://github.com/{username}/streamlit-reddit.git

# Enters the repo, which is currently empty
$ cd streamlit-reddit

Running Streamlit on your own computer

With our Git repo set up, let's add in the two main files every Streamlit Sharing app needs:

requirements.txt — A list of the Python libraries we use, so Streamlit Sharing will know to install them.

For this tutorial, we'll only be using two libraries: Streamlit itself, and the Python client for Firestore. So this is all you need inside your requirements.txt file:

streamlit
google-cloud-firestore


streamlit_app.py — Where our app code will go! Let's start with just a few lines:

import streamlit as st

st.header('Hello 🌎!')
if st.button('Balloons?'):
    st.balloons()


This app imports the Streamlit library, and then shows some text and a button. When you click on the button, a bunch of balloons will float up from the bottom of the screen!

Now, we're going to use Pip to install the Streamlit and the Firestore libraries onto your own computer. (If you prefer pipenv or poetry or conda, any of those are fine too!)

# Install all the libraries mentioned in requirements.txt
$ pip install -r requirements.txt
...


We're ready to run our starter Streamlit app!

# Run this Streamlit app on your own computer
$ streamlit run streamlit_app.py
You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.1.3:8501


If you see the Streamlit app launched on your own browser, that means everything worked. (You may have to enter in the URL, like http://localhost:8501). Try clicking on that button!

Deploying your app on Streamlit Sharing

Okay, we're ready to put our app on Streamlit Sharing! First, we need to save our changes to Git (aka "commit"), and then upload (aka "push") our changes onto GitHub. Again, from a shell that's inside your repo:

# Add and all changes in your files
$ git commit -am "Create requirements.txt and a simple Streamlit app"

# Push all commits back to GitHub
$ git push


Now if you go to the GitHub page for your repo, you should see the code that you just wrote:

Once your code is on GitHub, you're ready to deploy your app for the world to see! Head over to https://share.streamlit.io/ and create a new app. (If you don't have access to Sharing yet, post below in the comments and we'll sort you out.) Here's a great tutorial on deploying through sharing if you'd like to read more.

Fill in the name of your repo — the others should populate automagically — then click "Deploy!" 🎈

That URL you're on is one that anyone on Earth can go to and play with. Isn't that cool? You've gone from "literally nothing" to "functioning, globally-hosted Streamlit app" in the space of 15 minutes! But hold on to your horses, we're about to get to the really cool stuff: saving data online, with Firestore.

Part 2: Setting up Firestore

Once again, Firestore is the database we're using to create, edit, and read our data. For our Reddit home page, we're going to use it to store things like the links, titles, and later upvotes. Here in Part 2, I'll walk you through how to set that up.

Set up a Firebase account

To clear up some potential confusion: Firestore is the database we're using, one of many services provided by Firebase (which is a part of Google). You'll want to sign in to your Google account at https://console.firebase.google.com/. Then, create a Firebase project:

These are the steps to create an account:

Name your project
Enable analytics [You can skip this since we're not using it today, but I think Analytics are really cool 🙂]
Accept terms and click create!

Once created, you'll see Firebase loading and will be notified when the project is ready [this is pretty quick].

Awesome, our Firebase project is set up! Firebase has a lot of different tools for building web apps, but the only one we're using today is Firestore. So let's turn that on by creating a Cloud Firestore database.

Create a Firestore database

Start it in test mode, which means anyone can read or write to it for the next 30 days (you can change this later!)

And pick a physical location for where the data is hosted. (Don't worry about the scary red text. This isn't super important, but closer to you generally means the data will load a little bit faster.)

Create your first collection and document

Now that the Firestore database is created, we can start putting data into it. But first, some terminology:

A document is a grouping of key-value pairs; think "Python dict, but the keys are always strings". It's where you will be storing all your data! If you were building a chat app, you might represent one user in her own document. Or one message might be its own document.
Just like dicts can contain other dicts, documents can contain other documents!
Each document has an ID, a string that uniquely identifies it.
A collection is a set of documents. Your chat app might have a collection of users, and a separate collection of messages, with the understanding that the documents in each will look different.
But note that as a flexible, NoSQL database, Firestore will not enforce any rules about which documents go in which collections! It's all up to you.
Collections have IDs as well, to distinguish them from other collections.

Our Reddit clone will include a bunch of posts. So let's make a collection for them!

And then we'll want to create an example post document. Let's make a post that links to the Google website. Pick an ID, then add in a title and a url field for the post:

Now you'll have your first piece of data visible from your web console! The console is a place where you can manually edit your database. It's already a cool way to peek under the hood, and you can make changes directly through the web editor.

But sit tight, and I'll show you how to create new data programmatically — that is, in your Python app!

Download your account keys 🗝️

Before we get there, one last thing: we need to get a service account key so that our Streamlit app will be able to read and write data to this database. You can think of this key as a password that your Python code uses to log in to Firestore — except instead of a text password like "hunter2", it's a JSON file instead.

To download this JSON file, first go to "Project settings":

Then go to "Service accounts", select "Python", and click "Generate new private key"

You should have downloaded a file that looks like streamlit-reddit-firebase-adminsdk-4enia-e106b71674.json. Move it into your Git repo, and let's rename it to something simpler like firestore-key.json.

That's it, we're done setting up Firestore!

To be continued...

We now have everything set up to start coding in earnest. Check out Parts 3 & 4, where we provide step-by-step instructions to build the entire Reddit app, as well as tips and tricks for working with Firestore + Streamlit. For now, here's a sneak peak of an example streamlit_app.py to get you started — see how far you can get by iterating on this!

Normally you should NEVER store a key in a public GitHub. We're doing it right now because it's a test. And in the next section I'll show you how to secure that in Streamlit Sharing using Secrets!
import streamlit as st
from google.cloud import firestore

# Authenticate to Firestore with the JSON account key.
db = firestore.Client.from_service_account_json("firestore-key.json")

# Create a reference to the Google post.
doc_ref = db.collection("posts").document("Google")

# Then get the data at that reference.
doc = doc_ref.get()

# Let's see what we got!
st.write("The id is: ", doc.id)
st.write("The contents are: ", doc.to_dict())


Questions? Something didn't work right? Just want to show off your cool Firestore app? Let us know below in the comments section!

And of course, thanks to Henrikh and Fanilo for providing feedback on this article~

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image-1.png (2000×1062)
https://blog.streamlit.io/content/images/2021/08/image-1.png#browser


streamlit-1.0.gif (1920×1080)
https://blog.streamlit.io/content/images/2022/09/streamlit-1.0.gif#browser


function-page-1.png (2000×1140)
https://blog.streamlit.io/content/images/2021/10/function-page-1.png#browser


What is Nightly? | Try Nightly Build for Cutting-Edge Streamlit
https://blog.streamlit.io/https-discuss-streamlit-io-t-try-nightly-build-for-cutting-edge-streamlit-2534/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Try Nightly Build for cutting-edge Streamlit

A new style of release for anyone who wants the most up-to-date Streamlit version

By TC Ricks
Posted in Product, April 17 2020
Contents
Share this post
← All posts

Hey Community 👋,

We’ve recently rolled out a new type of release, our Nightly Build, and we wanted to give everyone a bit of information about it.

What is nightly?

Nightly is a new style of release we created for anyone who wants the most up-to-date Streamlit version. At the end of each day, all PRs that were approved will be added to a new nightly build, including all of our newest features! While we can’t guarantee there won’t be bugs, all nightly versions go through the same automated portion of testing as our general releases. If you’d like to use, simply do pip install streamlit-nightly or if there is a specific version of nightly you want to use, you can find it here. All changes can be found using the comparison tool on our GitHub repo.

Why are we doing this?

We want to make sure you have access to the latest features and fixes as soon as they are available. If there was an issue preventing you from using Streamlit or you need a feature that will allow you to do more, this will give quicker access. It will also let you try things out sooner and give us valuable feedback!

Let us know if you have any feedback or questions ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

legacy-vs-arrow-2-1.png (2000×2017)
https://blog.streamlit.io/content/images/2021/07/legacy-vs-arrow-2-1.png#shadow


arrow.gif (687×388)
https://blog.streamlit.io/content/images/2021/07/arrow.gif


arrow-vs-legacy-chart-1.png (1248×794)
https://blog.streamlit.io/content/images/2021/07/arrow-vs-legacy-chart-1.png#shadow


Monthly rewind > January 2021
https://blog.streamlit.io/monthly_rewind_january_2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > January 2021

Your January look back at new features and great community content

By TC Ricks
Posted in Monthly Rewind, February 8 2021
🏆 App of the month 🏆
Streamlit January updates
🔍 Current releases: 0.76.0
🕹 Closed betas
🧩 New components
🎈 New Streamlit creators
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our January featured app of the month is......🥁🥁🥁🥁🥁

The Traingenerator app by Johannes Rieke which generates custom template code for PyTorch & sklearn, all within a simple Streamlit UI. [See the gif above!]

To learn more about the app take a look at the repo here - we recommend perusing some of Johannes' other repos as well [including his Best of Streamlit tracker] - he has some pretty cool projects 😎

Streamlit January updates

The latest and greatest that you might have missed in January.

🔍 Current releases: 0.76.0

The newest Streamlit release is 0.76.0, but make sure to check out the change log if you haven't updated in a while to see what's been going on.

🕹 Closed betas

We currently have a number of coming soon features that are being tested in closed beta, including:

Secrets management (for Streamlit sharing)
Programmable state
Customizable theming

If you're interested in testing any of these send us a message on the forum and we'll add you to the beta when it becomes available.

🧩 New components

The community continues to actively develop a number of new components that you can use to extend your apps. Here are two that made some headway in January:

Streamlit Ag-Grid by Pablo Fonseca - Streamlit Ag-Grid makes it possible to use interactive dataframes within your Streamlit apps. [Demo App]

Streamlit WebRTC by Yuichiro Tsuchiya - Use Streamlit WebRTC to send and receive video within your Streamlit apps. [Demo App]

🎈 New Streamlit creators

We welcomed our three newest Streamlit Creators in January: Johannes Rieke, Tyler Richards, and Christian Klose - read more about them here.

Featured Streamlit content

Podcasts and articles written by the Streamlit team for your listening and reading pleasure.

Listen to Streamlit's COO, Amanda Kelly, as she talks about Streamlit and company building with Darius Gant on his podcast Building Data Applications for AI
Streamlit engineer, Tim Conkling tells the story of allow-same-origin: Streamlit Components, security, and a five month quest to ship a single line of code
We're having a bit of a love affair with Firestore. In this piece Streamlit engineer, Austin Chen walks through Part 1 of how to use Streamlit and Firestore together: Streamlit ❤️ Firestore
Featured community content

Some great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Read Jared Stock's article about how Arup and New Story are using Streamlit to help combat pandemic related evictions.
Check-out the popular Github stats app by Johannes to see your GitHub activity from 2020
Christian wrote a fun tutorial on how to sketch number prediction using a Streamlit app
The Data Professor did a guest video on freeCodeCamp where he builds 12 data science apps with Python and Streamlit - and check out his growing archive of Streamlit tutorials: How to Make a Multi-Page Web App | Streamlit #16
For all the cinema fans out there, Steven created "The Pitch Doctor " to help you create a title and movie pitch for you next horror film 🧟
Fanilo made some amazing updates to his Streamlit ECharts component
A great article by Tim on how to create and deploy a neural style transfer app
A notable app by Alireza helping to show where individuals can donate items in Switzerland: Where2Give
Justin wrote a helpful piece on how to optimize a Pandas dataframe with a Streamlit app

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image-1.png (1771×1146)
https://blog.streamlit.io/content/images/2021/10/image-1.png#browser


Adding Beta and Experimental “Channels” to Streamlit
https://blog.streamlit.io/https-discuss-streamlit-io-t-adding-beta-and-experimental-channels-to-streamlit-2765/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Adding beta and experimental “channels” to Streamlit

Introducing the st.beta and st.experimental namespaces

By TC Ricks
Posted in Product, May 6 2020
Beta
Experimental
Contents
Share this post
← All posts

Hey community 👋,

At Streamlit, we like to move fast while keeping things stable. And in our latest effort to move even faster while keeping the promise of stability, we’re introducing the st.beta and st.experimental namespaces. These are basically prefixes we attach to our function names to make sure their status is clear to everyone.

Here’s a quick rundown of what you get from each namespace:

st: this is where our core features like st.write and st.dataframe live. If we ever make backward-incompatible changes to these, they will take place gradually and with months of announcements and warnings.
st.beta: this is where all new features land before they find their way to st. This gives you a chance to try the next big thing we’re cooking up weeks or months before we’re ready to stabilize its API.
st.experimental: this is where we’ll put features that may or may not ever make it into st. We don’t know whether these features have a future, but we want you to have access to everything we’re trying, and work with us to figure them out.

The main difference between st.beta and st.experimental is that beta features are expected to make it into the st namespace at some point soon, while experimental features may never make it.

More details below.

Beta

Features in the beta namespace are all scheduled to become part of st, or core Streamlit. While in beta, a feature’s API and behaviors may not be stable, and it’s possible they could change in ways that aren’t backward-compatible.

The lifecycle of a beta feature

A feature is added to the beta namespace.
The feature’s API stabilizes and the feature is cloned into the st namespace, so it exists in both st and st.beta. At this point, users will see a warning when using the version of the feature that lives in the beta namespace – but the st.beta feature will still work.
At some point, the feature is removed from the st.beta namespace, but there will still be a stub in st.beta that shows an error with appropriate instructions.
Finally, at a later date the stub in st.beta is removed.

Keeping up-to-date with beta features

All Beta features will be announced in the changelogs.
All Beta features will show up in our documentation alongside normal features. For example, st.beta_color_picker() will be documented on the same page as st.slider().
Experimental

Features in the experimental namespace are things that we’re still working on or trying to understand. If these features are successful, at some point they’ll become part of core Streamlit, by moving to the st.beta namespace and then to st. If unsuccessful, these features are removed without much notice.

Note: Experimental features and their APIs may change or be removed at any time.

The lifecycle of an experimental feature

A feature is added to the experimental namespace.
The feature is potentially tweaked over time, with possible API/behavior breakages.
At some point, we either move the feature into st.beta or remove it from st.experimental. Either way, we leave a stub in st.experimental that shows an error with instructions.

Keeping up-to-date with experimental features

All Experimental features will be announced in the changelogs.
All Experimental features will show up in a separate section of the API page in the docs, called “experimental features” (not created yet!)

Let us know if you have any questions or feedback about the new namespaces!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Cloud.png (2000×1140)
https://blog.streamlit.io/content/images/2021/10/Cloud.png#browser


Get-Started.png (2000×1142)
https://blog.streamlit.io/content/images/2021/10/Get-Started.png#browser


Session_State_GIF_2-1-edited.gif (1200×603)
https://blog.streamlit.io/content/images/2021/08/Session_State_GIF_2-1-edited.gif#border


knowledge-base-1.png (2000×1140)
https://blog.streamlit.io/content/images/2021/10/knowledge-base-1.png#browser


The Stable solves its data scalability problem with Streamlit
https://blog.streamlit.io/the-stable-solves-its-data-scalability-problem-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The Stable solves its data scalability problem with Streamlit

How Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days

By Mark von Oven and Ksenia Anske
Posted in Case study, April 28 2022
Notebooks weren’t enough
Stumbling on Streamlit
Starting with Streamlit
Streamlit as a prototyping tool
Speaking data science language
Solving problems one piece at a time
Wrapping up
Contents
Share this post
← All posts

The Stable is a commerce agency that helps brands execute across all channels of commerce including Retail, Amazon, Creative, and Digital Marketing. The company develops strategies, sells products, builds brands, executes campaigns, develops tech and merchandise, and analyzes and dataizes to drive sales and efficiency. Data science is a big part of the value it delivers to its clients.

Mark von Oven joined The Stable in 2019 as VP of Analytics and has built a solid Analytics team.

"We quickly found our small (but mighty!) team in high demand," Mark said. "We were solving huge problems for our clients that used to take years to resolve. We even applied for our first patent!"

Notebooks weren’t enough

The team did great work, but it was invisible, mostly stored in Jupyter notebooks and random Python files. There was no scaling or democratizing data. All projects started with the same process or algorithm and took 1-3 weeks to complete.

The team wanted the clients to interact with the algorithms, but they had no time or budget to build interactive apps. Flask and other tools were too slow and cumbersome.

"We had to choose which clients would see all this cool stuff," Mark said. "We’re not full-stack developers. We’re data scientists and analysts. We needed to surround our algorithms with user-friendly input choices and make them available 24/7. No hand-holding required."

Stumbling on Streamlit

And it was then that team member Pedro Pereira stumbled on Streamlit while solving a different problem during a monthly "hack day" session.

"Every month we have a day when we try to tackle one single project," Pedro said. "We needed a fast way to spin up a data analysis web app."

Team member Nick Walton saw a different opportunity.

"I thought that translating our Jupyter notebooks into small Streamlit apps might let us automate some repetitive tasks," Nick said.

The team tested it. Within a week their first app was born—a test-and-control store selector for retail chains.

"It was clean and quick," Mark said. "It surprised me. I thought we were creating an app to streamline the internal process. But it could easily be used by anyone in the company."

It took the team one day to review the code and read the docs to realize that this was exactly how they were going to solve their scalability problem and make their work visible.

Starting with Streamlit

Mark’s team loved Streamlit. There were no training sessions. No offsite boot camps at $2,000 per seat. No waiting for DevOps to build dashboards.

Streamlit was a seamless extension of what the team already knew how to do in Python.

"I wasn’t even firing up Jupyter notebooks anymore," Mark said. "I’d open Spyder and start coding a new app to save time. In only three months, Streamlit has hosted our capabilities, enabled us to transform our data into actionable insights, and scaled to the whole company!"

Now they could have an idea in the morning, get it in front of clients in the afternoon, and drive value before the end of the day. They could launch an app on Tuesday and help 30 clients by Friday!

Streamlit as a prototyping tool

The team ramped up so quickly and easily that Mark started seeing Streamlit as their primary prototyping tool—and now potentially sees a path to the company’s production environment. Plus, it was really easy to connect their Streamlit app to Snowflake data.

"Given the success and scalability so far, we have no reason to change course," Mark explained. "We want to delight our clients. We’re building these apps with passion and determination. Streamlit allows us to focus on that because it makes everything else easy."

Speaking data science language

Data scientists do amazing things and work with the smartest people on Earth. Yet many don’t know how to let people discover that work. They think they need a complete solution before sharing their work.

Mark thinks there is huge potential in partial solutions.

"I’m a bit atypical for a data scientist," Mark said. "Yes, I wrote my first line of BASIC code on a TRS-80 when I was 6, and I studied how to build both software and hardware. But I’m also the lead singer of the Grateful Red, a Minneapolis funk cover band. I love the spotlight. If I or my team build something cool, I’m compelled to ensure it gets presented. Too many data scientists would rather call in sick than talk about their work with business leaders."

As a performer, Mark is often nervous before a show. All it takes for him is to sing the first few notes, and his nerves are gone.

"Getting on that stage is half the battle," Mark said. "Don’t feel stumped because of the language barrier, or because you don’t have enough business context or think you’ll fail. You’re sitting on powerful stuff. Expose it. Explore it early to break down the barriers before they exist. Streamlit lets you speak your language and bring the fruits of your labor to those who don’t speak data science."

Solving problems one piece at a time

Nobody wants to invest in a piece of software until they have the problem figured out. But the best way to solve a problem is by solving it one piece at a time.

"Streamlit naturally fits into this process," Mark explained. "It uses a language you already know and helps you get a fantastic result. Who knows, maybe it’ll be your new production environment!"

Wrapping up

Thank you for reading Mark’s story! If you have any questions, please connect with Mark on LinkedIn or drop him a line at mvo@thestable.com.

And if you want to get started with Streamlit in your organization, head over to streamlit.io to learn more.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Quick-Image-2--2-.png (2000×1695)
https://blog.streamlit.io/content/images/2021/08/Quick-Image-2--2-.png#border


Component-2.png (4924×5980)
https://blog.streamlit.io/content/images/2021/10/Component-2.png


Session_State_GIF_1-edited.gif (1200×627)
https://blog.streamlit.io/content/images/2021/08/Session_State_GIF_1-edited.gif#browser


series.b.gif (1024×576)
https://blog.streamlit.io/content/images/2022/09/series.b.gif#browser


Component-1--3-.png (2000×1411)
https://blog.streamlit.io/content/images/2021/10/Component-1--3-.png


Monthly Rewind - Streamlit
https://blog.streamlit.io/tag/monthly-rewind/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Monthly Rewind
27 posts
Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Monthly rewind > November 2022

Your November look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
December 7 2022
Monthly rewind > October 2022

Your October look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
November 8 2022
Monthly rewind > September 2022

Your September look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
October 7 2022
Monthly rewind > August 2022

Your August look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
September 7 2022
Monthly rewind > July 2022

Your July look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
August 9 2022
Monthly rewind > June 2022

Your June look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
July 6 2022
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

What’s new in Streamlit (January 13th, 2022)
https://blog.streamlit.io/whats-new-in-streamlit-january-2022/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

By Ksenia Anske
Posted in Release Notes, January 13 2022
✨ New in Streamlit
⛑️ Release 1.4.0
📸 Introducing st.camera_input for uploading camera images
🪁 Clear memo + singleton caches procedurally
🔦 Other notable 1.4.0 release updates
☁️ Streamlit Cloud updates
📩 Sign in with email
📲 Share apps with any email address
🔦 Other notable Streamlit Cloud updates
🌞 Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit community! 👋

We’re excited to share what’s new in the 1.4.0 release and some recent updates to Streamlit Cloud, including notable updates like introducing st.camera_input, clearing individual memo and singleton functions in code, signing in to Streamlit Cloud with your email, and app email invites.

Keep reading to learn more.

✨ New in Streamlit
⛑️ Release 1.4.0
📸 Introducing st.camera_input for uploading camera images

We’re introducing a new widget to use webcams! It’s great for computer vision apps. For example, imagine an app for face detection or style transfer.

Here is how you can use this widget:

img_file = st.camera_input("Webcam image")


The returned img_file is a file-like object, similar to the return value of st.file_uploader. For a simple app, you can display the image captured by the webcam by using st.image:

img_file = st.camera_input("Webcam image")
if img_file is not None:
    st.image(img_file)


Of course, if you want to, you can do any processing steps on the image in between.

Here’s what the new widget looks like in the app:

Try it out in our demo app and take a look at the documentation.

🪁 Clear memo + singleton caches procedurally

Do you need more control over cache invalidation? Now you can finally clear caches of functions decorated with @st.experimental_memo and @st.experimental_singleton in code. (The Streamlit community has been asking for this forever—and so have we, internally.)

We have implemented programmatic clearing of memo + singleton functions. For example, you can do the following:

@st.experimental_memo
def foo(x):
    return x**2

if st.button("Clear Foo"):
    # Clear foo's memoized values:
    foo.clear()

if st.button("Clear All"):
	  # Clear values from *all* memoized functions:
		st.experimental_memo.clear()


API Details

Any function annotated with @st.experimental_memo or @st.experimental_singleton gets its own clear() function automatically.
Additionally, you can use st.experimental_memo.clear() and st.experimental_singleton.clear() to clear all memo and singleton caches, respectively.

Note that because memo/singleton themselves are experimental APIs, these cache-clearing functions are experimental as well. See the docs for memo and singleton.

🔦 Other notable 1.4.0 release updates

One other notable update in this release:

🚦 Widgets now have the disabled parameter that removes interactivity (#4154).

Click here to check out all updates.

☁️ Streamlit Cloud updates
📩 Sign in with email

You can now sign in to Streamlit Cloud by using an email!

After entering your email on the sign-in page, you’ll get an email with a sign-in link:

Clicking on the link will log you into your Streamlit Cloud console. From there you can view all of your apps. See the docs for more information.

📲 Share apps with any email address

Apps deployed with Streamlit Cloud come with built-in authentication. Until now, sharing has been limited to Google email addresses (Gmail) or accounts that have SSO setup.

Starting today, you can share your app with any viewer whether they’re part of your company or another company. Simply add the viewers’ email addresses, and you’re done!

🔦 Other notable Streamlit Cloud updates
🤯 The error page when your app goes over resource limits now shows more helpful info for debugging.
🐙 Your app now prints a message to its Cloud logs whenever it updates due to a Github commit.
⭐ Streamlit Cloud now supports a "favorite" feature that lets you quickly access your apps from the app dashboard (favorited apps appear at the top).
🚀 When you invite someone to view your app in Streamlit Cloud, the recipient will receive an invitation to view the app as an email in their inbox!
🔒 We're committed to meeting industry standards and are now SOC 2 Type 1 certified. Read more about securely sharing your apps using Streamlit Cloud.

If you're new to Streamlit, now is the time to try Streamlit Cloud!

🌞 Wrapping up

Thanks for checking out what’s new with Streamlit. You can always see the most recent updates to our core library on our changelog or via this tag on the forum and to Streamlit Cloud via the Cloud Release Notes.

Got questions? Let us know in the comments below. We're looking forward to hearing what you think!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Release Notes...

View even more →

Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Wissam Siblini uses Streamlit for pathology detection in chest radiographs
https://blog.streamlit.io/wissam-siblini-uses-streamlit-for-pathology-detection-in-chest-radiographs/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Wissam Siblini uses Streamlit for pathology detection in chest radiographs

Learn how Wissam detected thoracic pathologies in medical images

By Wissam Siblini and Ksenia Anske
Posted in Case study, May 3 2022
The need to develop a demo app
Discovering Streamlit
Streamlit seamlessly dressed up our machine learning results as an interactive chest radiographs classification app
6 reasons why it was obvious to start with Streamlit
Wrapping up
Contents
Share this post
← All posts

Wissam Siblini always wanted to be a computer scientist. He started coding in middle school, studied engineering in college, and did his PhD in his favorite field, at the intersection of maths and computer science.

When Wissam started as a machine learning researcher, he set out to implement innovative features in fraud detection, natural language processing, and health. But he couldn’t manage the front-end development of an application. He spent most of his time processing data, developing algorithms, and training and evaluating them.

"Machine learning is one of the most exciting fields for the decades to come," Wissam said. "We're seeing the development of groundbreaking image recognition tools, agents that understand our language, and more generative tools for voice, image, sound, and art of all kinds. But I like to question basic facts. While working on fraud detection, I was interested in the very definition of simple evaluation measures like precision and recall. I even proposed variants that are now mentioned on the F-score Wikipedia page!"

The need to develop a demo app

With the Covid-19 crisis, chest diseases became a major concern around the world. Deep learning (a subset of machine learning) showed very convincing performances in understanding texts, images, and genomics. It found applications in the medical field, particularly in personalized, predictive, and preventive medicine.

Wissam decided to target a specific "AI for Health" problem—the detection of thoracic pathologies in medical images.

To do this, he and Mazine, a colleague who worked on this exploratory project with him, needed to look at the data, interact with it, and discuss it with others. They also had to clean it, select it, transform it, and build it into a model.

"I used to carry out this type of analysis in Jupyter notebooks but it wasn’t interactive," Wissam said. "I wanted to show my results to other project members, especially to non-data scientists."

When his team finished the proof of concept and had a prototype model, they needed a demo app. They wanted the users to upload a radiograph, get predictions (probabilities of pathologies and alerts), and see a visual interpretation of the model’s decision. They also wanted the users to browse the training data and its characteristics, monitor the performance of the model on a validation set, play with model parameters, and see the impact on performance.

This required front-end development, and several back-end functionalities such as an API for model serving. It was very time-consuming and required the knowledge of many different frameworks. It was hard to do in the last stages of a POC when the development had already consumed a lot of time.

Discovering Streamlit

It was then that Wissam discovered Streamlit.

“We had only a few weeks to complete this pathology project,” Wissam said. “I still wanted a demo to show that it was promising and integrable. I was comfortable with Python, but not with frameworks like Angular, Vue, and React. So I googled “frontend development in Python” and ran into Streamlit (the early 0.60.0 version). I was amazed by the app examples on the website—especially the conciseness of their code and deployment procedure.”

After browsing the site for a few minutes, Wissam decided to use Streamlit not only for this project but for many others.

“I looked for solutions on the Streamlit forum and saw Fanilo Andrianasolo, a very talented colleague of mine,” Wissam said. “He was an active member of the Streamlit community. We discussed it, and I made my choice.”

Streamlit seamlessly dressed up our machine learning results as an interactive chest radiographs classification app

Streamlit allowed the team to build exactly what they wanted.

“The most important part of my job is to allow stakeholders to understand my work even if they’re not data scientists,” Wissam said. “Streamlit made my life better and easier for that. It took us almost five months to build a very competitive model. With Streamlit, we’ve then built a very cool app that won us an internal innovation prize and convinced the management to keep exploring the theme. And it took only two weeks!”

The app featured a classifier page where the user could upload imagery, obtain a probability for each pathology (displayed in red if above a critical threshold), and get the interpretability heat map associated with the pathology of its choice. The imagery could be in a standard image format (PNG or JPEG) or in the more classical DICOM format.

The app had a Dashboard Data (to visualize the characteristics of the training data) and a Dashboard Model to analyze the model's performance and tune hyperparameters. For example, the user could set the critical threshold for each anomaly and see the impact on True Positive Rate, Precision, or Accuracy.

6 reasons why it was obvious to start with Streamlit

For Wissam these were the reasons why it was so obvious to him to get started with Streamlit:

Most of today’s data scientists already use Python. They can easily convert their projects into Streamlit apps.
The visuals (the frontend part) and the engine (the backend part/models) are all implemented in the same code.
Many built-in features (sliders, buttons, tables, etc.) allow you to build a wide range of utilities in just a few lines of code. And both the Streamlit team and the community enrich these features every day.
There are many example apps for your inspiration on the Streamlit website and on GitHub.
The Streamlit forum is rich and active, so if you’re stuck, you can get help easily.
The apps look very good. They’re more than enough for a great demo!

“In addition to demos, Streamlit can help with the exploratory phase of a project,” Wissam added.  “In this phase, the goal is to see how some variables can impact others, display charts, play with parameters, observe behavior, and test transformation functions. Building a Streamlit exploration app then lets you focus on the analysis itself (not the code) and discuss it with subject-matter experts who are not data scientists.”

Wrapping up

“I’m so used to developing things with Streamlit now, it takes me hardly any time to make apps,” Wissam said. “I once coded an app in the morning, and in the afternoon it was ready for the client.”

Thank you for reading Wissam’s story! If you have any questions, please leave them in the comments below or contact Wissam on LinkedIn or via email. Happy coding!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

ScienceIO manages billions of rows of training data with Streamlit
https://blog.streamlit.io/scienceio-manages-billions-of-rows-of-training-data-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

By Gaurav Kaushik and Raunaq Malhotra
Posted in Case study, January 5 2023
How to query billions of rows
How to set up a search engine
How to build a fully working app example
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Gaurav Kaushik, and I’m the Co-Founder of ScienceIO.

Since 2019 we’ve been working on a model that could accurately mine medical records for critical patient information.

In the past, finding key medical conditions, procedures, and therapies required a fleet of models. We wanted to replace it with an elegant system that could be easily trained, deployed, and fine-tuned. But we needed hundreds of millions of training data labels!

It took us a year to create a training dataset for large healthcare language models with over 2.3 billion labels—the most expansive and comprehensive in existence. Yes! Then a new issue came up. 😩 Could these models understand billions of rows of data, perform quality control, and plan data improvements? We got to work and developed a Streamlit + Snowflake search engine to see what was in our data, what wasn’t, and how we could improve it.

In this post, I’ll share with you why we turned to Streamlit + Snowflake and how you can build your own working example to search for papers from a database of COVID-19 research papers. You’ll learn:

How to query billions of rows of data
How to set up a search engine
How to build a fully working app example

Let’s get started.

👉
TL;DR? Check out the app and the repo.
How to query billions of rows

The usual business intelligence (BI) tools wouldn’t allow us to ask a question, test a hypothesis, or create a new look at the data (and repeat this process). That’s why we built a Streamlit + Snowflake search engine. We went from question to answer much faster!

Streamlit really came to life after being paired with a global data platform like Snowflake. We could store and query billions of rows of data in Snowflake. But it’s the Streamlit apps that let us investigate it—write queries, build dashboards, and ask and answer questions fast.

To better understand 2.3 billion labels for training large language models, we built a multi-page Streamlit app. Each page has a different data view or a way to search it:

Concept search: Search for labels that relate to a particular medical concept (specific medical condition or drug).
Text search: Search for specific words or phrases that are tagged as a label.
Counts: Provide a dashboard with summary statistics of the whole dataset.

By combining Streamlit and Snowflake into a single application, we could add new views, data analyses, and new search capabilities to be served in minutes:

Streamlit’s text bar is connected to a query that runs on a data warehouse like Snowflake. The status bar feature lets the user know their search is running (a query on billions of rows can take up to a minute).

How to set up a search engine

Streamlit components make it easy to put together a text-based search engine:

import streamlit as st

# app title
st.title("Text Search 📝")

# search box
with st.form(key='Search'):
  text_query = st.text_input(label='Enter text to search')
  submit_button = st.form_submit_button(label='Search')


In five lines of code, we imported the Streamlit package, gave our app a title, and set up a search box. Next, we needed our app to do something when we clicked the button. So we set up a spinner for the user to see that a search was happening while we retrieved the results:

if submit_button:
	with st.spinner("Searching (this could take a minute...) :hourglass:"):
		# cool search stuff happens here


Once the spinner was done, we used success to show that the search was complete:

st.success(f"Search is complete :rocket:")


Not everyone on our team knows Python or how to query and visualize data, but they rely on this information to help customers or build new tools. The Streamlit search engine gives them a simple place to get what they need and start building. Over time, this freed up our data scientists to work on other high-value projects.

How to build a fully working app example

To show how to include real search logic in an application, we’ve created a repo using the NCBI COVID-19 literature database. In this fully working example, we demonstrate how to load a dataframe of articles, search for keywords in the titles, and display the results:

First, we load a TSV as a dataframe and cache the data:

@st.cache
def load_data(filepath:str) -> pd.DataFrame:
    """ Load data from local TSV """
    return pd.read_csv(filepath, sep="\\t", skiprows=33).fillna("")


Next, we create a function to search the dataframe. This is done by checking to see if a column has rows where our search term is a substring:

def search_dataframe(df:pd.DataFrame, column:str, search_str:str) -> pd.DataFrame:
    """ Search a column for a substring and return results as df """
    results = df.loc[df[column].str.contains(search_str, case=False)]
    return results


Finally, we want to load the data once the application starts:

# env variable
DATA_FILEPATH = "litcovid.export.all.tsv"

# within app(): load data from local tsv as dataframe
df = load_data(DATA_FILEPATH)


Now when we click on st.form_submit_button, we’ll run the search, notify the user of the number of results found, and display the first ten hits:

# if button is clicked, run search
if submit_button:
  with st.spinner("Searching (this could take a minute...) :hourglass:"):

      # search logic goes here! - search titles for keyword
      results = search_dataframe(df, "title_e", text_query)

      # notify when search is complete
      st.success(f"Search is complete :rocket: — **{len(results)}** results found")

  # now display the top 10 results
  st.table(results.head(n=10))


One of the best features of Streamlit is interactive plotting and visualization. At ScienceIO, we love to use Altair plots which are easily converted from static to dynamic. Let’s add an interactive bar plot that shows the top journals in our search results.

# we can use altair to turn our results dataframe into a bar chart of top journal

import altair as alt

alt.Chart(results).transform_aggregate(
        count='count()',
        groupby=['journal']
    ).transform_window(
        rank='rank(count)',
        sort=[alt.SortField('count', order='descending')]
    ).transform_filter(
        alt.datum.rank < 10
    ).mark_bar().encode(
        y=alt.Y('journal:N', sort='-x'),
        x='count:Q',
        tooltip=['journal:N', 'count:Q']
    ).properties(
        width=700,
        height=400
		).interactive()


The code might seem intimidating, but don’t worry. You got this! Altair lets you chain functions together to make changes to data and charts that are each straightforward. The first set of functions aggregate, sort, and filter the data — transform_aggregate() groups the results and counts the number of times each value appears; transform_window() ranks/sorts each row by its count; transform_filter() removes all rows below a certain rank (for example, below 10th place). Now that our data is appropriately transformed, we can plot — mark_bar() creates a bar chart, we declare our x- and y-axes and a tooltip with encode(), and we can specify chart width and height using properties(). Finally, we add interactive() — now, we can hover over each bar to see the journal and the total number of publications that match your search terms.

And now we have a fully working example app of search app. In this example, we use a basic dataframe, but you can replace that with anything—a database, a data warehouse, or an API call. The only limit is your imagination!

You can find the code for this app on GitHub and try the app yourself here.

Wrapping up

Now you know how to set up a search tool with Streamlit! We hope you better understand how Streamlit applications can act as fast and flexible tools on top of large datasets and that you’ll build more cool things and share them in turn with the community!

If you have any questions or want to learn more about industrial-scale AI for healthcare, please post them below.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image.png (802×415)
https://blog.streamlit.io/content/images/2021/10/image.png


0.88.0-release.gif (1920×1080)
https://blog.streamlit.io/content/images/2022/09/0.88.0-release.gif#border


Austin Chen - Streamlit
https://blog.streamlit.io/author/austin/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Austin Chen
3 posts
Streamlit ❤️ Firestore (continued)

Aka the NoSQL sequel: Building a Reddit clone and deploying it securely

Tutorials
by
Austin Chen
,
April 22 2021
Streamlit ❤️ Firestore

Use Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!

Tutorials
by
Austin Chen
,
January 27 2021
New layout options for Streamlit

Introducing new layout primitives—columns, containers, and expanders!

Product
by
Austin Chen
,
October 8 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Discover and share useful bits of code with the 🪢 streamlit-extras library
https://blog.streamlit.io/discover-and-share-useful-bits-of-code-with-the-streamlit-extras-library/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Discover and share useful bits of code with the 🪢 streamlit-extras library

How to extend the native capabilities of Streamlit apps

By Arnaud Miribel
Posted in Tutorials, October 25 2022
What are streamlit-extras?
How to discover extras
Badges
App logo
Dataframe explorer UI
Toggle button
How to try extras
How to install extras
How to contribute your own extras
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Arnaud Miribel, and I’m a data scientist at Streamlit.

Every day I work on making or improving our internal apps. That means developing new reusable functions that improve the apps’ appearance or functionality. I also collect useful bits of code on our forum or in the GitHub issues that I can incorporate into my work. Essentially, those bits are mini Streamlit components. Creating a new project for each would be a pain. So I bundled them all into one project—an experimental library where you can easily discover, try, install, and share these components.

Say hello to…

…streamlit-extras!

In this post, I’ll go over the following:

What are streamlit-extras?
How to discover extras
How to try extras
How to install extras
How to contribute your own extras

Let’s get going!

What are streamlit-extras?

streamlit-extras (extras for short) is just a collection of small custom Streamlit components. Most of them have a minimal of code, and most are Python-only (with some CSS/HTML hacks via st.markdown). Think of extras as an installable utils.py full of small, handy Streamlit components. 🙂

How to discover extras

Head over to extras.streamlitapp.com to discover extras in their natural habitat. Even the library’s gallery is a multipage app (why not make another app, right? 😉).

Let’s look at some of them:

Badges

An easy way to add social badges to your apps:

App logo

A small function to add a logo to your navigation bar (sweet!):

Dataframe explorer UI

Recognize this? The now famous filter_dataframe function that creates a UI on top of dataframes:

Toggle button

A simple lightweight alternative to st.expander—a toggle button:

Browse the pages in the left navigation bar to see more!

How to try extras

Some extras feature a “Playground” section. Try passing your own parameters to the extra and see how it works.

For example, try playing with streamlit_extras.stoggle:

Just for fun, try it within the colored headers or the keyboard text extra. And guess what…“Playgrounds” are powered by yet another extra: the function explorer. 😉

How to install extras

You can easily use extras in your apps. Simply open your terminal and run:

pip install streamlit-extras


Extras are accessible as modules within the library itself, and you can use all of them.

For example, if you want to use streamlit_extras.stoggle, just create a new script:

# streamlit_app.py

import streamlit as st 
from streamlit_extras.stoggle import stoggle

stoggle("Here's a little secret", "Streamlit-extras is so cooool")


Go ahead and run streamlit run streamlit_app.py, and you’ll see this in your app:

Congrats! You have used your first Streamlit extra. 🎊

How to contribute your own extras

As part of this project, I populated the library with 20+ extras. But there is room for more. And it’s open-source, so you’re welcome to contribute!

For example, if you want to share an extra strike that strikes a text, you can do it with HTML (let’s forget st.markdown("~Hey~") for a moment):

 def strike(text: str):
    """Strikes input text

    Args:
        text (str): Input text
    """
    return st.write(f"<del>{text}</del>", unsafe_allow_html=True)


Here is an example usage of strike():

strike("Some outdated statement")


This will output:

Take a look at the code. All extras have their own directory within streamlit_extras and a __init__.py in it:

.
├── CONTRIBUTING.md
├── LICENSE
├── README.md
├── gallery
│   ├── requirements.txt
│   ├── streamlit_app.py
│   └── streamlit_patches.py
├── poetry.lock
├── pyproject.toml
├── src
│   └── streamlit_extras       # <-- This is where extras live!
│       ├── __init__.py
│       ├── altex              # <-- Every extra has its directory...
│       │   └── __init__.py    # <-- ... and an __init__.py
│       ├── annotated_text
│       │   └── __init__.py
│   ├── ...
└── tests
    ├── __init__.py
    └── test_extras.py


To add a new extra:

Create a new directory in src/streamlit_extras called strike_text.
Create a new file __init__.py in this new directory.
Put the strike() function, for example, and some metadata:
# strike_text/__init__.py
import streamlit as st

def strike(text: str):
    """Strikes input text

    Args:
        text (str): Input text
    """
    return st.write(f"<del>{text}</del>", unsafe_allow_html=True)

def example():
    strike("Some outdated statement")

# Metadata that's useful to add your extra to the gallery
__func__ = strike
__title__ = "Strike text"
__desc__ = "A simple function to strike text"
__icon__ = "⚡"
__examples__ = [example]
__author__ = "Dark Vador"
__experimental_playground__ = True


That’s it!

Now create a pull request with these additions. Upon approval, I’ll feature your extra and make it accessible in the streamlit-extras library. Easy, right?

Wrapping up

Hopefully you’ll find extras useful and contribute your own. If you have questions or thoughts, drop them in our Github repo. Remember—extras are just a little bit more of what you can do with Streamlit. The Streamlit community regularly makes awesome new components that extend your apps in big ways.

If you want to learn about how to make your own advanced component with HTML and JavaScript, check out this post.

Happy Streamlit-ing! 🎈

P.S.: Learn more about streamlit-extras and the Streamlit Data Product team at Snowflake BUILD, a free virtual developer conference. My colleague Tyler will be there, talking about the library along with the product feedback loops we’ve set up!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Record_Screencast-1.gif (1200×660)
https://blog.streamlit.io/content/images/2021/08/Record_Screencast-1.gif#border


6 Tips for Improving Your App Performance | Streamlit
https://blog.streamlit.io/six-tips-for-improving-your-streamlit-app-performance/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
6 tips for improving your Streamlit app performance

Moving your Streamlit app from analysis to production

By Randy Zwitch
Posted in Tutorials, July 20 2021
1. Do you really need a million-value slider?
2. Pre-calculate inputs and cache whenever possible
3. Avoid downloading large, static models
4. Remove unused data
5. Optimize data storage formats
6.  Use the right tool for the job!
Wrapping Up
Resources
Contents
Share this post
← All posts

In the first two parts of this blog series (part 1, part 2), my colleague Abhi outlined principles for designing Streamlit apps from the perspective of the end user. From using wireframes before you start coding to adding contextual help, layout and theming features, designing for the intended user can take your app from being seldom used to being a key tool in your organization.

In this final part of the series, I’ll discuss some subtle unseen-but-definitely-felt user experience improvements that tune the performance of your Streamlit apps. And for most of this post we'll be focusing on apps that, at least in part, operate on static data (data that doesn't change). The tips are listed in order from easiest-to-implement to hardest-to-implement, and depending on the use case not all of them will apply to every Streamlit app.

1. Do you really need a million-value slider?

In the same way a line chart with 10,000 lines is visual overload, having a slider, selectbox, multiselect, or select_slider widget with 10,000 options is granularity overload: it would be nigh impossible to use, not to mention to understand each option.

But even worse, for most of those widgets, the more options you pass as input the more data needs to be sent from the Streamlit server to the Streamlit JavaScript frontend. This means more data needs to be serialized, sent over the network, and stored in the browser's memory. Take the following line of code:

price = st.selectbox("House Price", range(10000000))

In this toy example, all 10⁷ values will be converted to str and almost 100 megabytes sent over to the user's browser.

A less silly example would be a slider:

price = st.slider("House Price", 1, 10000000)

Due to implementation details, high-cardinality sliders don't suffer from the serialization and network transfer delays mentioned earlier, but they will still lead to a poor user experience (who needs to specify house prices up to the dollar?) and high memory usage. In my testing, the example above increased RAM usage by gigabytes until the web browser eventually gave up (though this is something that should be solvable on our end. We'll look into it!)

Suffice to say, performance and user experience will be much improved if you consider alternative means of interacting with apps of this kind, such as a using st.number_input or st.text_input to allow the user to enter arbitrary or high-granularity inputs. If you absolutely must use widgets that enumerate every value, choose a step size that gives the end user at most dozens of choices to pick from. Your users will thank you!

price = st.slider("House Price", 100000, 10000000, step=100000)
2. Pre-calculate inputs and cache whenever possible

When moving a Streamlit app to production, consider whether there are parts of the app that can be pre-calculated. If your app aggregates a 10-million-row dataframe by U.S. State, you can improve performance by caching that computation and only re-running it when needed:

@st.cache(ttl=24*60*60)  # Don't forget to add a TTL!
def get_data_by_state():
	return huge_df['US state'].unique()

Going further, if the data in question never changes, you can do this calculation beforehand outside of the app and save it as a file which you then load and cache when your app first starts:

@st.cache  # No need for TTL this time. It's static data :)
def get_data_by_state():
	return pd.read_csv(PRECALCULATED_DATA_PATH)

Creating a list on-the-fly by taking the unique value across a 10-million-row dataframe is always going to be reading the list directly from memory. So, for dynamic datasets, wrap your computation in an @st.cache  and set a large TTL. And for static datasets you should always consider offloading computations to a file, to constants, or to a separate Python modules.

3. Avoid downloading large, static models

In the Streamlit Self-Driving Car demo, we show a code pattern where we download the YOLOv3 object detection model from S3 all while using st.progress to keep the user informed while the download takes place. While this is a great pattern when sharing a model across multiple repos, it does put the app at the mercy of internet bandwidth when running in production: the YOLOv3 model is approximately 240MB, which can take several minutes to download before a user even has a chance to get started.

The solution is simple: when pushing your Streamlit app to production, bring your model and other assets to the production machine, and you can get orders-of-magnitude better startup time. For Streamlit sharing specifically, Git LFS (Large File Storage) is supported, so you can use it to store your model in your GitHub repository and make it available to your app automatically. Couple that with reading a file from disk with @st.cache and app users may not even realize a model is being loaded in the background!

4. Remove unused data

When starting a data project, a common task is pulling data from a database or CSV file and exploring it interactively. If you haven't use Streamlit for this exploratory data analysis phase yet (EDA), you should give it a try — just use Streamlit's "magic commands" and press Ctrl-S or Cmd-S to save the source file, and Streamlit shows updates live. This makes it easy to try numerous combinations of inputs and (hopefully!) find meaningful information in the data.

However, when moving the app to production, you are often telling a story, not searching for one. At this point, you usually know that your analysis only needs a handful of columns among the dozens in your dataset. In some cases, a dataset may have columns that can’t even be shared (personally-identifying information), that don't change (version numbers) or simply aren’t being used (user-agent strings).

So go ahead and remove those unnecessary columns and rows you aren’t using. Your data will get read in faster, use less RAM, and overall be more efficient when calculations are performed. To paraphrase a computer-science saying, the fastest-loading data is the one you don't have to load!

5. Optimize data storage formats

If you’ve made it this far into the post, you probably have a pretty svelte app! Your input widgets are optimized to provide meaningful choices, these choices are coded as constants in your app, you’re not downloading large amounts of data over the internet unnecessarily and what data you are reading are only the rows and columns you need. But what if that’s not fast enough?

If you’re reading large amounts of data via CSV or JSON, consider using a binary-serialized format such as Apache Parquet or Apache Arrow IPC. While CSV and JSON are convenient formats for data transport, ultimately they are optimized for humans, not computers! By using an optimized data storage format, your production app won’t spend time parsing text into data types such as integers, floats and strings, which, incredibly, can consume quite a bit of time. Additionally, binary formats often have metadata and logical partitioning such that Python can read the metadata to find exactly where the data are located, skipping entire data partitions from loading.

6.  Use the right tool for the job!

While many of the common libraries in the PyData ecosystem have C or FORTRAN underpinnings, in the end, some problems are larger than a single computer can reasonably handle. For tabular data, there have been decades of research into performance optimization of relational databases. From indexing to multi-core query processing, moving your computation workflow from Python to a relational database could give considerable speed improvements.

Taking it a step further, for heavy workloads consider separating the Streamlit app from the computation portion, so that your computation can scale independently of the web app.  Specialized hardware such as GPUs, Dask or Spark clusters and other higher-performance options are all ways to solve the largest data problems while still staying in the larger PyData ecosystem.

Wrapping Up

How are you optimizing your Streamlit apps? This blog post highlights six ways to improve Streamlit app performance, but there are definitely dozens of other tips and tricks that aren’t covered here. What are your favorite optimization tricks? Are you using Streamlit with databases? Stop by the Streamlit Community forum and let us know what you’ve done to optimize your apps, and what else could be working better within Streamlit!

Resources
Github
Forum
Design Series Part 1
Design Series Part 2

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

2-2.png (1200×479)
https://blog.streamlit.io/content/images/2021/08/2-2.png#border


ToolTips.gif (1200×708)
https://blog.streamlit.io/content/images/2021/08/ToolTips.gif#browser


1-1.png (1200×335)
https://blog.streamlit.io/content/images/2021/08/1-1.png#border


How one finance intern launched his data science career from a coding bootcamp in Brazil
https://blog.streamlit.io/how-one-finance-intern-launched-his-data-science-career/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How one finance intern launched his data science career from a coding bootcamp in Brazil

Learn how Marcelo Jannuzzi of iFood got his dream job in data science

By Marcelo Jannuzzi and Ksenia Anske
Posted in Case study, June 9 2022
How it all started
How Marcelo uses Streamlit
Marcelo’s advice for aspiring data scientists
1. Realize that data can be your job!
2. Notice what gets you in the flow
3. Consider a data science bootcamp
4. Take advantage of company training
5. Learn how to package your work (with Streamlit)
Wrapping up
Contents
Share this post
← All posts

Looking to start your career in data science? Marcelo Jannuzzi of iFood will show you how to get a chance to do what you love even when you’re not looking!

We chatted with Marcelo about his journey from an intern in finance to data analyst at iFood, where he uses machine learning and statistical methods to model business problems and inform decision makers in his company.

In this post, we’ll share with you his story—including how he got into data science:

How it all started
How Marcelo uses Streamlit
Marcelo’s advice for aspiring data scientists
How it all started

In college, Marcelo studied business and finance. After a brief corporate banking internship at Citibank, he went on to Magnetis, an asset management fintech startup. He loved it, and it validated his wish to be in finance. Eventually he found himself as an analyst at a hedge fund, which was what he always wanted!

“But then, finding myself in this ‘dream job’ job for me, I started to realize that what I actually enjoyed about my previous experiences wasn't the finance itself,” Marcelo said. “It was the more mathematical or statistical aspects of coming up with models to allocate investments, or to price different securities, for example. It was more about the data side of things than the finance.”

So Marcelo decided to learn more about data science. He knew how to program from college and had some experience with Python and R. He went through a data science bootcamp and got a job as a data analyst at a food delivery company in Brazil.

After only six months he joined iFood, the biggest FoodTech in Latin America.

“Before coming to this field I never spent more than a year in the same job. I was worried that a job would always be just a job for me and that I'd never find something that I really enjoyed,” Marcelo said. “I was relieved when I found data science because in many ways it didn’t feel like work. It felt more like solving a puzzle. I was also drawn to the idea of predicting the future! It still blows my mind how well it works sometimes!”

How Marcelo uses Streamlit

As part of the the data team which supports iFood’s logistics operations, Marcelo uses Streamlit to showcase and build interactivity into his models.

“We track a lot of different metrics and KPIs,” Marcelo said. “And they’re all interrelated, so a change to one of them affects another, which affects the next one, and can have this cascading effect throughout our business.”

With Streamlit Marcelo has created an interactive system of models that simulates these interactions between different aspects of the business. It allows him to see the tradeoffs that exist between the various metrics that describe iFood’s logistics and to simulate different scenarios.

“This idea of the tradeoffs that exist between different parts of our business is key,” he said. “If you want to improve one aspect, you have to be willing to take a hit somewhere else. We want to measure these tradeoffs and show them to our business users so that they can make better decisions on how to steer our operations.”

To model these interactions, Marcelo made a graph that describes how each KPI is related to each other. Each node in this graph has a model associated to it. “So to make a prediction for a particular metric, we traverse the graph making predictions for each predecessor node, and feeding those predictions into the following nodes.”

“It would be very hard for me to do a project like this with our standard toolset,” Marcelo explained. “It would take a few months to develop the interface so that business users could interact with it. But Streamlit abstracts all of that away for me. I don’t have to worry about it! Being more familiar with Streamlit and all it can do increases the range of projects we in the Data Analysis team can undertake, since we don’t have to rely as much on engineers to implement whatever it is we want to do.”

Marcelo’s advice for aspiring data scientists

We asked Marcelo for his advice for aspiring data scientists looking to start (or grow!) their career. Here is what he had to say:

1. Realize that data can be your job!

I feel kind of stupid because every time I was working on a personal project involving data or code in general, I was always psyched about it. I wanted to finish it. I didn’t care if I was hungry or needed go to the bathroom. I didn’t want to stop. I was in the zone. But for some reason, I never thought that could be my job.

My parents are both economists. They both worked at banks, and their friends also had backgrounds in finance, so I guess in my mind a “real job” was working in a bank. Having studied business in college also didn’t help in that respect, as a lot of my colleagues were getting jobs in investment banking or management consulting. Then I read in a magazine about a few data science bootcamps popping up around Brazil. They’re common in the US, but at the time they were very new in Brazil. So in late 2018 I decided to enroll in one of them, and while going through the program and talking to different people in the field I realized that yes, this could be my job!

2. Notice what gets you in the flow

Try to notice the things that get you into that state of flow. Things that you want to finish despite being hungry or needing to go to the bathroom. Also notice the things that you actually enjoy, not the things you think you should enjoy (or that your parents or colleagues told you you will enjoy).

3. Consider a data science bootcamp

I got my first job in data science because of a bootcamp. It was taught by people who worked in the industry and gave me a good overview of the basic skills, what was expected of me, and also helped me start to build a professional network. It's hard getting a cold start in a new field, and a program like this can help jumpstart things.

Also, going through the bootcamp felt like a stamp of approval of sorts. Before this I thought, “Who am I to get into this field? I have no experience here, so who’s going to hire me?” After completing it I felt much more confident to look for a job in data science.

4. Take advantage of company training

iFood has a partnership with NOVA University Lisbon, and gave me and some other colleagues the opportunity to get our Master’s degree here. I enjoy the academic environment and had already been thinking about continuing my studies, so having the chance to move from Brazil to Portugal to do so, while still being able to work remotely at iFood was amazing. iFood also provides us with many other opportunities to develop our skills in other ways, like allowing us dedicated time during the week for personal development, a budget for books and courses, among other kinds of training.

5. Learn how to package your work (with Streamlit)

Data is awesome, but I’ve found that a very important part of working in data science is learning how to package your work in a way that's appealing to your colleagues and end users. A simple solution that gets used is way more valuable than a sofisticated solution that doesn’t get used because people don’t understand it or because its hard to use.

I think this is where Streamlit really shines. You can make something look professional with very little effort. Many teams here at iFood use Streamlit, as it lets us move faster and depend less on our engineering and product teams by taking the initial phases of this kind of work into our own hands.

We have a pretty big data team at iFood—hundreds of people—so we don't have contact with everyone, but many of us found Streamlit independently around 2020. I heard about it in the Brazilian data science podcast Data Hackers. Now about a dozen people are using it across three teams, and more people find out about it every day!

Wrapping up

Thank you for reading Marcelo’s story! We hope it’ll inspire you to start or to continue growing your career in data science. If you have any questions, please post them in the comments below or connect with Marcelo on LinkedIn.

Happy coding! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

JULO improves financial inclusion in Indonesia with Streamlit
https://blog.streamlit.io/how-streamlit-is-helping-julo-improve-financial-inclusion-in-indonesia/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
JULO improves financial inclusion in Indonesia with Streamlit

Learn how JULO went from manual underwriting to automated credit scoring and a 22-member data team

By Martijn Wieriks and Ksenia Anske
Posted in Case study, June 30 2022
How JULO went from manual underwriting to automated credit scoring and a 22-member data team
How JULO discovered Streamlit
How Streamlit helps JULO grow faster
What other startups can learn about data from JULO’s story
Wrapping up
Contents
Share this post
← All posts

JULO is a financial startup that’s trying to solve the problem of financial inclusion in Indonesia.

“A lot of people in Indonesia don’t have access to traditional financial services,” said Martijn Wieriks, Chief Data Officer at JULO. “Banks have a difficult time underwriting them because they don’t have traditional financial track records. So they end up being excluded.”

JULO is working to change that by using proprietary credit models that leverage alternative data sources like mobile device and health insurance usage.

Credit scoring is at the center of JULO’s business model—making data a key element of its growth and success.

Streamlit has been a key part of JULO’s growth, empowering developers to deliver complex, interactive data visualizations so that stakeholders can quickly make data-backed business decisions.

How JULO went from manual underwriting to automated credit scoring and a 22-member data team

Here’s more about JULO’s unique business model—and how data has been central to its success.

Step 1: Manual underwriting and creation of the first data warehouse

JULO started by building a customer base, a tool kit, and a foundation for their first data warehouse. Agents did the underwriting manually by relying on the data team to collect data and aggregate it into reports.

Step 2: First automated credit model

After JULO collected sufficient data for a year, they built their first credit model iteration. Suddenly they could automate everything.

“Seeing it in practice was amazing,” Martijn said. “We were making the right predictions and enabling the business to make fast credit decisions while carefully managing risk. It was a wow moment.”

JULO helps financially underserved Indonesians meet their credit needs with innovative Credit Scoring solutions.

Step 3: Building the development team

“There's a big talent gap in Indonesia,” Martijn said. “Instead of competing with larger companies, we decided to work with talented new grads. We developed a program to train new hires and get them up to speed in 3-6 months.”

JULO is now at 22 people on the data team and is continuing to grow.

Step 4: Growing the customer base and improving the model over time

Once JULO’s use cases expanded, their technology stack improved and they started getting more customers. This growth posed new challenges.

“The more data variables we added to a machine learning model, the less transparent it became,” Martijn said. “It was important to understand how a machine learning model makes decisions, especially in credit scoring and lending. Because we didn’t want to unfairly bias specific groups.”

Interactive Streamlit dashboards have enabled the finance team to review and plan different strategic scenarios more easily (credit: Darwin Natapradja).

Here is a Streamlit app that simulates credit score performance on different datasets (simulations help data scientists and business users build intuition around metrics):

How JULO discovered Streamlit

“I came across a Medium post and a GitHub repo that mentioned Streamlit,” Martijn said. “It said it was UI for machine learning engineers to create machine learning apps. It didn't click with me then. Six months later I tried it again. There were more app examples. I tinkered with it and was surprised by how quickly I could build a web application. With existing data science skills, like writing Markdown and Python, it was so simple to add interactive components to static code. It was like Jupyter Notebooks on steroids.”

Martijn used Streamlit for his personal projects for two years, then introduced it to his team.

How Streamlit helps JULO grow faster

Data is very important for JULO. It’s their first-class asset, value proposition, and IP. Streamlit helps JULO take care of data, manage bias in models, and demonstrate data science principles to business users and risk managers.

1. Higher velocity

JULO has many active credit lines and needs to report to banks on their financial performance. Martijn’s team used to spend a lot of time on preparing documents, spreadsheets, and slide decks. But they weren’t interactive, so they created a CFO dashboard in Streamlit.

“We could have two conversations of 90 minutes in front of a whiteboard, trying to draw out different risk scenarios to each other. I was there with our CEO, our Business Intelligence Manager, and two data scientists. The whiteboard soon became a complex mess of charts, numbers and variables, which made decision making increasingly difficult. Then over the weekend our BI Manager decided to create this CFO dashboard. On Monday we were able to walk through the same scenarios in an interactive way. It took maybe 10-15 minutes for that to click. We were able to condense 3 hours into 15 minutes to have a breakthrough and get the understanding we needed.”

2. Better decision making

Streamlit lets JULO quickly develop complex and interactive data visualizations.

“We used to build custom Flask apps with Jupyter notebooks and widgets,” Martijn said. “But it's not stakeholder friendly because they don't know how to work with notebooks. Streamlit is a presentation tool. You can serve it as a website. And people can play with it. It's very stakeholder friendly, which is super important because it’s all about putting data solutions into the hands of others. That’s Streamlit’s main value.”

3. Empowered stakeholders

Changing algorithm parameters and doing exhaustive research can help finetune machine learning models. More time spent equals more accuracy.

“Adding more variables may increase the model’s accuracy, but it’ll also make it harder to understand the relationship between them,” Martijn said. “If you only have a customer's age, income, and their latest completed school level, then understanding how a model makes a risk decision is easy. But with a thousand variables, it's a different story. How does each variable affect the score? Is the change positive, negative, or linear? With Streamlit we were able to explore the data, visualize it, and make it accessible.”

What other startups can learn about data from JULO’s story

1. Be patient with lagging data

Any startup that wants to do machine learning needs to collect data for their first model. It takes time. Depending on your use-case your key metrics might be heavily lagging. You’ll be able to see performance data on loans 3-6 months after you've disbursed them. Only then can you say with confidence, “This was a good loan and that was a bad loan.”

2. Visualize data models to get alignment and buy-in

At first we grew slowly. Then suddenly after the first year we had enough data. One rule of thumb in credit scoring is that you need at least a thousand bad loans to build your first model. It took us a while to get there. Once we hit it, we quickly built the first iteration, but we didn’t know if our model would work.

When it did, it was a magic moment. We had a couple of charts and saw people with really high credit scores who had a much lower delinquency rate. That group of people was a lot better than the next, and the next, and the next. We saw a nice sloping line. It was exactly what we were hoping for.

3. Make inclusion a priority

In lending, it has been repeatedly shown that women tend to perform better than men as they're more responsible with money. But if men are over-represented in your data, then a machine learning model may overestimate the risk of women’s delinquency and give them a lower credit score. To detect and manage such unfair bias, we compared groups of women and men so as not to unfairly disadvantage them based on gender.

Wrapping up

“We've been working with Streamlit to solve big problems,” Martijn said. “We've tried other tools, but they haven’t really clicked. Streamlit has become a part of our toolset.”

Thank you for reading JULO’s story! If you have any questions, please leave them in the comments below or contact Martijn on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

visualization--3-.png (704×350)
https://blog.streamlit.io/content/images/2022/08/visualization--3-.png


menu_screenshot.png (865×638)
https://blog.streamlit.io/content/images/2021/09/menu_screenshot.png#browser


about-menu.jpeg (1018×745)
https://blog.streamlit.io/content/images/2021/09/about-menu.jpeg


glaucoma_detector-with-gc--3-.png (640×480)
https://blog.streamlit.io/content/images/2021/09/glaucoma_detector-with-gc--3-.png#border


Untitled--2--1.png (640×480)
https://blog.streamlit.io/content/images/2021/09/Untitled--2--1.png#border


3-12.png (2060×981)
https://blog.streamlit.io/content/images/2021/08/3-12.png#browser


Untitled--6-.png (1906×1112)
https://blog.streamlit.io/content/images/2021/09/Untitled--6-.png#border


Untitled--1--1.png (794×848)
https://blog.streamlit.io/content/images/2021/09/Untitled--1--1.png#border


memory-usage-4.png (2000×536)
https://blog.streamlit.io/content/images/2021/10/memory-usage-4.png#browser


Screen-Shot-2022-08-10-at-2.54.28-PM.png (762×244)
https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.54.28-PM.png


Screen-Shot-2022-08-10-at-2.54.28-PM-1.png (762×244)
https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.54.28-PM-1.png


filmstrip--6--1.png (1600×812)
https://blog.streamlit.io/content/images/2021/08/filmstrip--6--1.png#border


visualization--4-.png (704×350)
https://blog.streamlit.io/content/images/2022/08/visualization--4-.png


Screen-Shot-2022-08-10-at-2.48.08-PM-1.png (1575×610)
https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.48.08-PM-1.png#border


visualization--5-.png (704×350)
https://blog.streamlit.io/content/images/2022/08/visualization--5-.png


Ksenia Anske - Streamlit
https://blog.streamlit.io/author/kseniaanske/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Ksenia Anske
6 posts
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
JULO improves financial inclusion in Indonesia with Streamlit

Learn how JULO went from manual underwriting to automated credit scoring and a 22-member data team

Case study
by
Martijn Wieriks and 
1
 more,
June 30 2022
How one finance intern launched his data science career from a coding bootcamp in Brazil

Learn how Marcelo Jannuzzi of iFood got his dream job in data science

Case study
by
Marcelo Jannuzzi and 
1
 more,
June 9 2022
Wissam Siblini uses Streamlit for pathology detection in chest radiographs

Learn how Wissam detected thoracic pathologies in medical images

Case study
by
Wissam Siblini and 
1
 more,
May 3 2022
The Stable solves its data scalability problem with Streamlit

How Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days

Case study
by
Mark von Oven and 
1
 more,
April 28 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1-14.png (1348×688)
https://blog.streamlit.io/content/images/2021/08/1-14.png#browser


03-drilling-into-intents.png (2547×1127)
https://blog.streamlit.io/content/images/2022/01/03-drilling-into-intents.png#browser


01-outlier-calls-1.png (2554×1302)
https://blog.streamlit.io/content/images/2022/01/01-outlier-calls-1.png#browser


The Streamlit roadmap—big plans for 2020!
https://blog.streamlit.io/https-discuss-streamlit-io-t-the-streamlit-roadmap-big-plans-for-2020-2054/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The Streamlit roadmap—big plans for 2020!

Devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers

By Adrien Treuille
Posted in Product, February 27 2020
Caching
Custom components
Customizable layout
Programmable state
Deploy
Resources
Contents
Share this post
← All posts

We created Streamlit to give the Python data community a new superpower: the ability to create beautiful apps as easily as writing Python scripts.

We launched a minimal version of Streamlit in October with only basic output and interaction primitives. Despite these limitations, the response has been overwhelming: almost 7,000 Github stars and 13,000 monthly active users in less than four months! Most exciting has been to see the emergence of an intricate archipelago of Streamlit apps, from simple, fun demos of open-source projects, to community-spanning infrastructure and complex internal tooling at major companies used in production by hundreds of employees.

Inspired by your energy and creativity, we’re devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers. Our goal is to make Streamlit not only the most productive (and fun!) app building experience in the Python ecosystem, but also the most powerful by adding:

Improved caching that’s easier to use and understand
A custom component system to extend Streamlit’s capabilities in the browser
Layout primitives to customize layout and other visual elements
User-programmable state, especially for multi-page apps
Enabling you to easily deploy apps from Streamlit (closed source)

A detailed feature list is on GitHub, and it’s really just a distillation of the ideas coming from Streamlit’s amazingly smart and creative community. Please help us understand what to build by submitting issues and pull requests, and by sharing your thoughts in the comments below.

Caching

Caching enables you to reuse data and computation in your apps, allowing scripts to run quickly by saving the results of expensive functions. We recently released hash_funcs so that you can set your own hash function for specific data types like TensorFlow sessions or live database connections. And we added more documentation on basic and advanced caching. Coming up are even more improvements to caching for other function types and some added magic to make everything around caching even more straightforward. Please share thoughts here about how you’d like to see caching work.

Custom components

The Streamlit custom components system will give you the ability to write arbitrary React or Javascript code and insert it into your app. This opens the door for a lot of possibilities for custom solutions to visualization, interactivity with chart/maps/tables, and other unique needs of your app. Please share thoughts here about how you’d like to see custom components work.

Customizable layout

Our community has already created some great style and layout resources (and we have no plans to deprecate html, unsafe_allow_html=True!), but Streamlit in its current form doesn’t make layout nearly as easy as we think it should be. We’ll be adding style and customization options to Streamlit, and also building new layout primitives: horizontal, grid, scroll views, and more. This is a tricky one to get right because layouts make up some of the most complex parts of display logic like CSS, not to mention it’s really easy to make these layouts look ugly. What are your thoughts? What are your favorite layout systems in other languages? Please share thoughts here about how you’d like to see layout work.

Programmable state

Right now, getting a Streamlit app to store internal state, like information a user entered in a form, is simply too tricky. There are some workarounds for session state, but we want to give you a baked-in and elegant version of programmable state so you can build apps with sequential logic, apps with multiple pages, apps that incrementally ask users for input, and so on. Please share thoughts here about the use cases you’d like to see supported.

Deploy

We know that building a great app is only half of the equation. You also want to deploy and share your app with others. We want Streamlit’s deployment workflow to be as elegant and awesome as its app-building workflow. That’s why we’re creating Streamlit for Teams: a single-click deployment solution for Streamlit apps with built-in enterprise-grade features like auth, logging, and auto-scaling. The first version of this will be rolled out for free to the community in a few months (and we’re expanding the beta soon, apologies if we haven’t gotten back to you yet!).

For now, you can check out some great community resources about deploying on AWS, GCP, Heroku, and Azure, and you help us by providing feedback on how you’d like to use Streamlit in your company.

We’ve been tinkering with and refining these features over at Streamlit HQ. We’re so excited to share these new superpowers and iterate on them with the community!

Thank you for your part in the Streamlit journey and here’s to a great 2020 ❤️

Resources

Roadmap
Documentation
GitHub
Changelog

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

02-latest-call-detail-screen-1.png (2551×1147)
https://blog.streamlit.io/content/images/2022/01/02-latest-call-detail-screen-1.png#browser


Misra Turp - Streamlit
https://blog.streamlit.io/author/misra/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Misra Turp
Developer educator at AssemblyAI.
1 post
Website
Twitter
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

James Thompson - Streamlit
https://blog.streamlit.io/author/james/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by James Thompson
1 post
Add secrets to your Streamlit apps

Use Secrets Management in Streamlit sharing to securely connect to private data sources

Tutorials
by
James Thompson
,
April 9 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 5)
https://blog.streamlit.io/page/5/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Display a race on a live map 🏃

Create a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library

Advocate Posts
by
Mark Needham
,
June 22 2023
LangChain tutorial #4: Build an Ask the Doc app

How to get answers from documents using embeddings, a vector store, and a question-answering chain

LLMs
by
Chanin Nantasenamat
,
June 20 2023
Building a Streamlit and scikit-learn app with ChatGPT

Catching up on coding skills with an AI assistant

LLMs
by
Michael Hunger
,
June 16 2023
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

LLMs
by
Adrien Treuille and 
1
 more,
June 15 2023
LangChain tutorial #3: Build a Text Summarization app

Explore the use of the document loader, text splitter, and summarization chain

LLMs
by
Chanin Nantasenamat
,
June 13 2023
Monte Carlo simulations with Streamlit

Learn how to predict future stock prices

Snowflake powered ❄️
by
Mats Stellwall
,
June 8 2023
LangChain tutorial #2: Build a blog outline generator app in 25 lines of code

A guide on conquering writer’s block with a Streamlit app

LLMs
by
Chanin Nantasenamat
,
June 7 2023
LangChain tutorial #1: Build an LLM-powered app in 18 lines of code

A step-by-step guide using OpenAI, LangChain, and Streamlit

Tutorials
by
Chanin Nantasenamat
,
May 31 2023
8 tips for securely using API keys

How to safely navigate the turbulent landscape of LLM-powered apps

Tutorials
by
Chanin Nantasenamat
,
May 19 2023
Semantic search, Part 2: Building a local search app

Making an app with Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data from Snowflake Marketplace

Snowflake powered ❄️
by
Dave Lin
,
May 18 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Christian Klose - Streamlit
https://blog.streamlit.io/author/christian/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Christian Klose
1 post
Build knowledge graphs with the Streamlit Agraph component

A powerful and lightweight library for visualizing networks/graphs

Advocate Posts
by
Christian Klose
,
November 25 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

catbot.png (1123×1858)
https://blog.streamlit.io/content/images/2023/04/catbot.png#border


hello-who-are-you-1.png (751×473)
https://blog.streamlit.io/content/images/2023/04/hello-who-are-you-1.png#border


catbot-2.png (744×783)
https://blog.streamlit.io/content/images/2023/04/catbot-2.png#border


user_icon.png (512×512)
https://blog.streamlit.io/content/files/2023/04/user_icon.png


gif-1-4.gif (960×574)
https://blog.streamlit.io/content/images/2021/08/gif-1-4.gif#browser


AI_icon.png (60×60)
https://blog.streamlit.io/content/files/2023/04/AI_icon.png


Screen-Shot-2020-11-25-at-9.36.31-AM-1.png (1458×970)
https://blog.streamlit.io/content/images/2021/08/Screen-Shot-2020-11-25-at-9.36.31-AM-1.png#browser


4.1-1.png (1200×848)
https://blog.streamlit.io/content/images/2021/08/4.1-1.png#browser


Creating a Time Zone Converter with Streamlit
https://blog.streamlit.io/creating-a-time-zone-converter-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Creating a Time Zone Converter with Streamlit

6 steps on how to build your own converter

By Vinícius Oviedo
Posted in Advocate Posts, April 25 2023
How does the Time Zone Converter work?
1. Import the required Python modules/package
2. Create a set of continents and countries in the time zone context
3. Configure the Streamlit page, header, and dropdown menu for continent and country selection
4. Get the corresponding UTC+x time zone for the user selection
5. Display the resulting time—the informed PST time converted to UTC+x
6. Apply a custom dark theme by creating a .streamlit folder with this config.toml file
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Vinícius, and I’m a Data Analyst and LaTeX Editor.

I love solving problems and assisting customers with technology. When I ask a customer about a delivery deadline, they usually tell me a day and time in a different time zone than mine (e.g., 4 PM PST). Since I’m from Brazil, I have to convert it from PST to UTC+x, where "+x" refers to an offset that can be positive or negative.

There are lots of tools for time zone conversion, but I wanted something simple, intuitive, and ad-free. So I made a Streamlit app that converts a time zone from PST to UTC for any country! 👏

In this post, I’ll show you how to build it in six steps:

Import the required Python modules/package
Create a set of continents and countries
Configure the Streamlit page, header, and dropdown menu
Get the corresponding UTC+x time zone
Display the resulting time
Apply a custom dark theme
👉
You can go straight to the app and the GitHub repo, if you’d like to skip reading.

But first, let’s talk about…

How does the Time Zone Converter work?

The way it works is super simple:

The user selects a continent and a country.
The corresponding time zone (in "UTC+x" format) is obtained based on the user's selection.
The user enters a PST time to be converted into the associated time obtained from step (2).
The resulting time for the selected country is displayed in the app.

Now, let’s help you develop your own converter!

1. Import the required Python modules/package
# Required Python modules/packages
import streamlit as st         # Streamlit framework 
from datetime import datetime  # For date and time
import pytz                    # For time zones

2. Create a set of continents and countries in the time zone context
# Create a dictionary with country name and corresponding timezone
timezone_dict = {
    "North America": {
        "United States": "America/New_York",
        "Canada": "America/Toronto",
        "Mexico": "America/Mexico_City",
        "Jamaica": "America/Jamaica",
        "Costa Rica": "America/Costa_Rica",
        "Bahamas": "America/Nassau",
        "Honduras": "America/Tegucigalpa",
        "Cuba": "America/Havana",
        "Dominican Republic": "America/Santo_Domingo"
    },
    "South America": {
        "Brazil": "America/Sao_Paulo",
        "Argentina": "America/Argentina/Buenos_Aires",
        "Chile": "America/Santiago",
        "Colombia": "America/Bogota",
        "Peru": "America/Lima",
        "Uruguay": "America/Montevideo",
        "Ecuador": "America/Guayaquil",
        "Bolivia": "America/La_Paz",
        "Paraguay": "America/Asuncion",
        "Venezuela": "America/Caracas"
    },
    "Europe": {
        "United Kingdom": "Europe/London",
        "France": "Europe/Paris",
        "Germany": "Europe/Berlin",
        "Italy": "Europe/Rome",
        "Spain": "Europe/Madrid",
        "Russia": "Europe/Moscow",
        "Turkey": "Europe/Istanbul",
        "Greece": "Europe/Athens",
        "Poland": "Europe/Warsaw",
        "Ukraine": "Europe/Kiev"
    },
    "Asia": {
        "India": "Asia/Kolkata",
        "Japan": "Asia/Tokyo",
        "China": "Asia/Shanghai",
        "Saudi Arabia": "Asia/Riyadh",
        "South Korea": "Asia/Seoul",
        "Indonesia": "Asia/Jakarta",
        "Malaysia": "Asia/Kuala_Lumpur",
        "Vietnam": "Asia/Ho_Chi_Minh",
        "Philippines": "Asia/Manila",
        "Thailand": "Asia/Bangkok"
    },
    "Oceania": {
        "Australia": "Australia/Sydney",
        "New Zealand": "Pacific/Auckland",
        "Fiji": "Pacific/Fiji",
        "Papua New Guinea": "Pacific/Port_Moresby",
        "Samoa": "Pacific/Apia",
        "Tonga": "Pacific/Tongatapu",
        "Solomon Islands": "Pacific/Guadalcanal",
        "Vanuatu": "Pacific/Efate",
        "Kiribati": "Pacific/Tarawa",
        "New Caledonia": "Pacific/Noumea"
    }
}

# Create a list of continents
continents = ["North America", "South America", "Europe", "Asia", "Oceania"]

3. Configure the Streamlit page, header, and dropdown menu for continent and country selection
# Streamlit app page setup
st.set_page_config(
    page_title='Time Zone Coverter', 
    page_icon='🌎',
    layout='centered',
    initial_sidebar_state='expanded',
    menu_items={
        'About': """This app is intended to select a country, get its 
        time zone in UTC format  and have its correspondent result 
        from a user-entered PST time."""
    }  
)

# Main header
st.header('Time Zone Coverter Streamlit app')

# Add some blank space
st.markdown("##")

# Create a dropdown to select a continent
continent = st.sidebar.selectbox("1. Select a continent", continents)

# Create a dropdown to select a country within the selected continent
countries = list(timezone_dict[continent].keys())
country = st.sidebar.selectbox("2. Select a country", countries)

4. Get the corresponding UTC+x time zone for the user selection
# Display the selected UTC offset
st.markdown("### :earth_americas: Corresponding UTC time:")
timezone = timezone_dict[continent][country]
utc_offset = datetime.now(pytz.timezone(timezone)).strftime('%z')
st.markdown(f"> **{country}** time zone is **UTC{utc_offset[:-2]}:{utc_offset[-2:]}**")

5. Display the resulting time—the informed PST time converted to UTC+x
# Add some blank space
st.markdown("##")

# Create input for PST time
st.markdown("### :clock10: PST time to UTC converter:")
pst_input = st.text_input("Enter PST time (e.g., 10:00 AM PST)")

# Convert PST time to UTC+X (where X is the offset)
try:
    pst_time = datetime.strptime(pst_input, "%I:%M %p PST")
    pst_time = pytz.timezone("US/Pacific").localize(pst_time, is_dst=None)
    target_time = pst_time.astimezone(pytz.timezone(timezone)).strftime("%I:%M %p %Z")
    st.markdown(f"> The corresponding time in **{country}** is **{target_time}**")
except:
    st.markdown("""
    :lock: Invalid input format. Please enter PST time in format 
    '<span style="color:#7ef471"><b> 10:00 AM PST </b></span>'
    """, unsafe_allow_html=True)

6. Apply a custom dark theme by creating a .streamlit folder with this config.toml file
[theme]
base="dark"
primaryColor="#54f142"
backgroundColor="#222831"
secondaryBackgroundColor="#393e46"
font="serif"


If you want to improve your Time Zone Converter, here are some suggestions:

Replace the set of time zones for countries/continents with an API. Some alternatives are TimezoneDB and GeoNames, to name a few. This would provide more options for countries and even work with more cities. For example, Brazil has four different time zones.
Implement more time zone formats, such as GMT, EST, CET, and so on.

Feel free to use your creativity. 😄

Wrapping up

Thank you for reading my post! Now you know how to create a simple yet useful Time Zone Converter app. It can determine a time zone (in UTC+x format) for a user-selected country and provide a rough estimate for a "PST to UTC+X" time conversion. If you have any questions, please post them in the comments below or contact me via GitHub, LinkedIn, or Medium.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

kmeans-clustering.png (2000×601)
https://blog.streamlit.io/content/images/2023/07/kmeans-clustering.png#border


Untitled-1.png (1401×1171)
https://blog.streamlit.io/content/images/2023/07/Untitled-1.png#browser


city-details.png (2000×709)
https://blog.streamlit.io/content/images/2023/07/city-details.png#border


levels.gif (822×639)
https://blog.streamlit.io/content/images/2023/05/levels.gif#browser


wordler_submit.gif (600×689)
https://blog.streamlit.io/content/images/2023/05/wordler_submit.gif#browser


hexagon-sandbox.jpeg (1024×676)
https://blog.streamlit.io/content/images/2023/07/hexagon-sandbox.jpeg


fire-service.png (1782×1155)
https://blog.streamlit.io/content/images/2023/07/fire-service.png#browser


Untitled.png (884×425)
https://blog.streamlit.io/content/images/2023/07/Untitled.png#border


wordler_steps.gif (600×649)
https://blog.streamlit.io/content/images/2023/05/wordler_steps.gif#browser


sharing.png (716×912)
https://blog.streamlit.io/content/images/2023/07/sharing.png#border


vscode.png (605×467)
https://blog.streamlit.io/content/images/2023/07/vscode.png#border


WORDLE_ARISE_histogram.png (1200×900)
https://blog.streamlit.io/content/images/2023/05/WORDLE_ARISE_histogram.png


accident-data.png (2000×1254)
https://blog.streamlit.io/content/images/2023/07/accident-data.png#browser


most_likely_letters.png (1858×692)
https://blog.streamlit.io/content/images/2023/05/most_likely_letters.png


The ultimate athlete management dashboard for biomechanics
https://blog.streamlit.io/the-ultimate-athlete-management-dashboard-for-biomechanics/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The ultimate athlete management dashboard for biomechanics

Learn how to measure jump impulse, max force, and asymmetry with Python and Streamlit

By Hansen Lu
Posted in Advocate Posts, April 27 2023
Step 1. Importing the necessary Python modules
Step 2. Establishing the initial UX layout
Step 3. Reading and displaying the forceplate data
Step 4. Finding the points of interest with while loops
Step 5. Getting the net impulse, push-off impulse, and absorption impulse
Step 6. Saving into a dataframe
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Hansen Lu, and I'm a data scientist specializing in biomechanics. Python and Streamlit help me analyze the body's performance and assess potential injury risks.

I built the Drop Jump app to measure an athlete's ability to adapt to the jump load, the force they generate to get off the ground, the asymmetry of their legs, and the impact of their landing. It captures and analyzes the raw motion and forceplate data so that I can share my insights with other practitioners!

In this post, you'll learn how to build it in six steps:

Step 1. Importing the necessary Python modules
Step 2. Establishing the initial UX layout
Step 3. Reading and displaying the forceplate data
Step 4: Finding the points of interest with while loops
Step 5: Getting the net impulse, push-off impulse, and absorption impulse
Step 6: Saving into a dataframe
👉
If you want to jump right in (no pun intended), here is the app, and here is the repo code.

Let's get right to it!

Step 1. Importing the necessary Python modules

You'll need the following modules:

Streamlit (to make a dashboard)
Pandas (to import, store, and save data as dataframes)
Scipy (to integrate)
Plotly—graph objects (to create visualizations)

Just type in this code:

import numpy as np
import pandas as pd
import streamlit as st
import datetime
import plotly.graph_objects as go
from scipy import integrate
Step 2. Establishing the initial UX layout

To build your dashboard, fill out the two required fields:

Body Weight. Enter it in lbs (it'll convert it to kgs) or in kgs.
Input Zero Velocity Time. Pair this with video footage synchronized with the force-time data in a lab environment.

Don't have synchronized footage? Use your smartphone's slow-motion feature to get zero velocity time. This will help you identify the touchdown time, zero velocity-time, and takeoff time. Simply get the time interval of the video and use it in your force-time graph.

The zero velocity time helps distinguish between concentric propulsion and eccentric deceleration of the jump, making for better analysis.

st.title("Drop Jump")
name = st.text_input("Athlete Name")
col1, col2 = st.columns([1, 1])
bwkgs = col1.number_input("Body Weight in lbs")
bwkgs = bwkgs / 2.205
zeroVelocityTime = int(col2.number_input("Input Zero Velocity Time (ms)"))
leftdjfp1 = col1.file_uploader("Upload Left Forceplate", type=["txt"], key=88)
rightdjfp1 = col2.file_uploader("Upload Right Forceplate", type=["txt"], key=89)
graph = go.Figure()

if rightdjfp1 is None:
    st.warning("No Right Forceplate Data")
if leftdjfp1 is None:
    st.warning("No Left Forceplate Data")
if bwkgs == 0:
    st.warning("No Bodyweight")

Streamlit provides built-in tools such as columns, a file uploader, and number input.

You can separate your forceplate data by the left and right sides. Note that we shortened "Streamlit" to "st" when declaring our modules in the previous step. Fortunately, Streamlit has a cheat sheet I always keep open to remind me how to program certain tools.

Prompt the user when the file uploader and body weight fields are empty. These three fields are essential for the analysis.

Step 3. Reading and displaying the forceplate data

Depending on your data capture/acquisition platform, you want to export your force-time data in a .txt or .csv file so your app can read it. This is where Pandas become very useful:

if leftdjfp1 is not None:
    dfldj1 = pd.read_csv(leftdjfp1, header=(0), sep="\t")
    graph.add_trace(
        go.Scatter(x=dfldj1["Time"], y=dfldj1["Fz"], line=dict(color="red"))
    )

If your leftdjfp1 (left drop jump force plate 1) is not empty, read the file with the 0th row as your header. The file is separated by tabs. Your force-time file may have different header names and be separated by spaces, commas, or something else.

To display the force-time data you just uploaded as a graph, use the Plotly module. Define your x values as your time, calling your force-time data (defined as dfldj1) and values under the header "Time" as your x value.

Similarly, define your y values as your vertical force values using your header "Fz".

Finally, define the color of your left line to red and the right line to green. If data from both force plates are inputted, it will display your force-time graph.

if leftdjfp1 is not None:
    dfldj1 = pd.read_csv(leftdjfp1, header=(0), sep="\t")
    graph.add_trace(
        go.Scatter(x=dfldj1["Time"], y=dfldj1["Fz"], line=dict(color="red"))
    )
if rightdjfp1 is not None:
    dfrdj1 = pd.read_csv(rightdjfp1, header=(0), sep="\t")
    graph.add_trace(
        go.Scatter(x=dfrdj1["Time"], y=dfrdj1["Fz"], line=dict(color="green"))
    )

Step 4. Finding the points of interest with while loops

To calculate the net impulse of a jump, you must determine the starting and ending points of each jump.

Before the touchdown of the jump, the vertical force 'Fz' has a value of zero. Use while loops to find the right and left leg's touchdown point. This while loop continues stepping until the condition of 'Fz' falls below 10N of force, at which point you can store that value as the touchdown point. You can continue stepping through until the value is above 10N when the athlete is on the forceplate until they jump off.

Finally, you can store the takeoff point of their right and left legs:

if rightdjfp1 is not None and leftdjfp1 is not None:
    tab1, tab2, tab3 = st.tabs(["Force-Time Graph", "Impulse Chart", "Metrics"])
    with tab1:
        st.plotly_chart(graph)
    i = 0
    while dfldj1["Fz"][i] < 10:
        i += 1
    j = 0
    while dfrdj1["Fz"][j] < 10:
        j += 1
    lefttouchdown = i
    righttouchdown = j

    while dfldj1["Fz"][i] > 10:
        i += 1
    while dfrdj1["Fz"][j] > 10:
        j += 1
    lefttakeoff = i
    righttakeoff = j
Step 5. Getting the net impulse, push-off impulse, and absorption impulse

The net impulse is the total impulse minus the body weight impulse. Assuming that the athlete equally distributes their weight through both legs, you can subtract half of their body weight in Newtons from one side's total 'Fz'.

To determine the push-impulse and absorption impulse, you need to know the exact time of zero velocity. If this information is available, it can help identify areas of weakness or asymmetry in the athlete.

For example, many athletes recovering from an ACL injury may have poor force absorption but strong force generation. This can increase the risk of re-injury, especially in an in-game scenario, as they may accelerate beyond their capacity to slow down.

data = np.array([[name, bwkgs, zeroVelocityTime, netImpulseL, netImpulseR]])

df = pd.DataFrame(
   	data,
    columns=[
        "Name",
        "Weight-kg",
        "Zero Velocity Time",
        "Net Impulse-L",
        "Net Impulse-R",
     ],
)

with tab3:
        st.dataframe(df)if bwkgs != 0:
        netImpulseRInterval = dfrdj1["Fz"][righttouchdown:righttakeoff] - (
            bwkgs * 9.81 / 2
        )
        netImpulseTimeR = dfrdj1["Time"][righttouchdown:righttakeoff]
        netImpulseLInterval = dfldj1["Fz"][lefttouchdown:lefttakeoff] - (
            bwkgs * 9.81 / 2
        )
        netImpulseTimeLInterval = dfldj1["Time"][lefttouchdown:lefttakeoff]

        netImpulseR = integrate.simps(netImpulseRInterval, netImpulseTimeR)
        netImpulseL = integrate.simps(netImpulseLInterval, netImpulseTimeLInterval)
        if zeroVelocityTime != 0:
            concentricImpulseRInterval = dfrdj1["Fz"][zeroVelocityTime:righttakeoff]
            concentricImpulseTimeR = dfrdj1["Time"][zeroVelocityTime:righttakeoff]
            concentricImpulseLInterval = dfldj1["Fz"][zeroVelocityTime:lefttakeoff]
            concentricImpulseTimeLInterval = dfldj1["Time"][
                zeroVelocityTime:lefttakeoff
            ]

            concentricImpulseR = integrate.simps(
                concentricImpulseRInterval, concentricImpulseTimeR
            )
            concentricImpulseL = integrate.simps(
                concentricImpulseLInterval, concentricImpulseTimeLInterval
            )

            eccentricImpulseRInterval = dfrdj1["Fz"][righttouchdown:zeroVelocityTime]
            eccentricImpulseTimeR = dfrdj1["Time"][righttouchdown:zeroVelocityTime]
            eccentricImpulseLInterval = dfldj1["Fz"][lefttouchdown:zeroVelocityTime]
            eccentricImpulseTimeLInterval = dfldj1["Time"][
                lefttouchdown:zeroVelocityTime
            ]

            eccentricImpulseR = integrate.simps(
                eccentricImpulseRInterval, eccentricImpulseTimeR
            )
            eccentricImpulseL = integrate.simps(
                eccentricImpulseLInterval, eccentricImpulseTimeLInterval
            )
    impulsestyle = ["Net Impulse", "Absorption Impulse", "Push-off Impulse"]
    yLeft = [netImpulseL, eccentricImpulseL, concentricImpulseL]
    yRight = [netImpulseR, eccentricImpulseR, concentricImpulseR]
    totaly = np.array(yLeft) + np.array(yRight)
    leftPercentage = np.round(yLeft / totaly * 100, decimals=1)
    rightPercentage = np.round(yRight / totaly * 100, decimals=1)

    chart = go.Figure(
        data=[
            go.Bar(name="Left", x=impulsestyle, y=yLeft, text=(leftPercentage)),
            go.Bar(name="Right", x=impulsestyle, y=yRight, text=(rightPercentage)),
        ]
    )

    # Change the bar mode
    chart.update_layout(barmode="group")
    with tab2:
        st.plotly_chart(chart)
Step 6. Saving into a dataframe

Formatting all the data into a dataframe is convenient for easy exporting and manipulation. You can display the dataframe and view all its values using "st.dataframe()". Additionally, you can save the dataframe as a .csv or a .txt file:

	data = np.array([[name, bwkgs, zeroVelocityTime, netImpulseL, netImpulseR]])

    df = pd.DataFrame(
        data,
        columns=[
            "Name",
            "Weight-kg",
            "Zero Velocity Time",
            "Net Impulse-L",
            "Net Impulse-R",
        ],
    )

    with tab3:
        st.dataframe(df)
Wrapping up

If you're a sports scientist or aspiring biomechanist, I hope this tutorial can help you with drop-jump analysis. Coding might seem intimidating if you're new, especially without a computer science background. However, it can open limitless possibilities to help you and your athletes!

If you have any questions, please post them in the comments below or contact me on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

spacial-join.png (1977×525)
https://blog.streamlit.io/content/images/2023/07/spacial-join.png#border


geometry.png (307×352)
https://blog.streamlit.io/content/images/2023/07/geometry.png#border


lateral-flatten.png (742×823)
https://blog.streamlit.io/content/images/2023/07/lateral-flatten.png#border


parsing-json.png (661×569)
https://blog.streamlit.io/content/images/2023/07/parsing-json.png#border


data-in-tables.png (1711×1082)
https://blog.streamlit.io/content/images/2023/07/data-in-tables.png#browser


terminal.png (682×193)
https://blog.streamlit.io/content/images/2023/07/terminal.png#border


snow-sql.png (712×123)
https://blog.streamlit.io/content/images/2023/07/snow-sql.png#border


pictures-in-tool-tips.png (212×144)
https://blog.streamlit.io/content/images/2023/07/pictures-in-tool-tips.png


snowflake-table.png (2048×942)
https://blog.streamlit.io/content/images/2023/07/snowflake-table.png#browser


Untitled--2-.png (2000×1109)
https://blog.streamlit.io/content/images/2023/07/Untitled--2-.png#browser


Untitled--1-.png (2065×525)
https://blog.streamlit.io/content/images/2023/07/Untitled--1-.png#border


folium-map.png (1688×1168)
https://blog.streamlit.io/content/images/2023/07/folium-map.png#browser


BR-residential-properties-appreciation-demo.gif (960×540)
https://blog.streamlit.io/content/images/2023/05/BR-residential-properties-appreciation-demo.gif#browser


wiki_word_count_constraint_PRESS_vec.png (1861×686)
https://blog.streamlit.io/content/images/2023/05/wiki_word_count_constraint_PRESS_vec.png


checkpoint.gif (860×638)
https://blog.streamlit.io/content/images/2023/05/checkpoint.gif#browser


supporting-documents.png (974×63)
https://blog.streamlit.io/content/images/2023/07/supporting-documents.png


wiki_word_count_constraint_GUESS_vec.png (1861×686)
https://blog.streamlit.io/content/images/2023/05/wiki_word_count_constraint_GUESS_vec.png


data.png (959×128)
https://blog.streamlit.io/content/images/2023/07/data.png


geojson-files.png (2000×1030)
https://blog.streamlit.io/content/images/2023/07/geojson-files.png#browser


Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Create a search engine with Streamlit and Google Sheets

You’re sitting on a goldmine of knowledge!

Advocate Posts
by
Sebastian Flores Benner
,
March 14 2023
10 most common explanations on the Streamlit forum

A guide for Streamlit beginners

Advocate Posts
by
Debbie Matthews
,
March 9 2023
Building a PivotTable report with Streamlit and AG Grid

How to build a PivotTable app in 4 simple steps

Advocate Posts
by
Pablo Fonseca
,
March 7 2023
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

LLMs
by
Arvindra Sehmi
,
February 9 2023
Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component

How to add advanced functionality to your Streamlit app’s authentication component

Advocate Posts
by
Mohammad Khorasani
,
February 7 2023
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

Advocate Posts
by
Sven Koerner
,
February 2 2023
Create a color palette from any image

Learn how to come up with the perfect colors for your data visualization

Advocate Posts
by
Siavash Yasini
,
January 19 2023
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
How to quickly deploy and share your machine learning model for drug discovery

Share your ML model in 3 simple steps

Advocate Posts
by
Sebastian Ayala Ruano
,
December 15 2022
Find the top songs from your high school years with a Streamlit app

Use the Spotify API to generate 1,000+ playlists!

Advocate Posts
by
Robert Ritz
,
December 8 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Hansen Lu - Streamlit
https://blog.streamlit.io/author/hansen/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Hansen Lu
1 post
The ultimate athlete management dashboard for biomechanics

Learn how to measure jump impulse, max force, and asymmetry with Python and Streamlit

Advocate Posts
by
Hansen Lu
,
April 27 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

chat-like-interface.png (1998×1298)
https://blog.streamlit.io/content/images/2023/07/chat-like-interface.png#browser


wiki_word_count_wordle_150_sample.png (1861×662)
https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_150_sample.png


supabase-table.png (1509×226)
https://blog.streamlit.io/content/images/2023/07/supabase-table.png#browser


snowchat-architecture.png (2253×1139)
https://blog.streamlit.io/content/images/2023/07/snowchat-architecture.png#border


config-file.png (1206×654)
https://blog.streamlit.io/content/images/2023/04/config-file.png#border


streamlit-theme-editor.gif (2120×1640)
https://blog.streamlit.io/content/images/2023/05/streamlit-theme-editor.gif#browser


WCAG-contrast-table.png (1126×664)
https://blog.streamlit.io/content/images/2023/04/WCAG-contrast-table.png#border


wiki_word_count_wordle_150.png (1860×662)
https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_150.png


color-picker-and-slider.png (1158×480)
https://blog.streamlit.io/content/images/2023/04/color-picker-and-slider.png#border


5.png (1495×1381)
https://blog.streamlit.io/content/images/2023/05/5.png#border


game-module.gif (978×637)
https://blog.streamlit.io/content/images/2023/05/game-module.gif#browser


4.png (1444×1278)
https://blog.streamlit.io/content/images/2023/05/4.png#border


wiki_word_count_wordle_15.png (1869×686)
https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_15.png


3.png (1470×861)
https://blog.streamlit.io/content/images/2023/05/3.png#border


2.png (1462×857)
https://blog.streamlit.io/content/images/2023/05/2.png#border


1-1.png (1698×887)
https://blog.streamlit.io/content/images/2023/05/1-1.png#border


when-in-doubt-1.png (1740×985)
https://blog.streamlit.io/content/images/2023/05/when-in-doubt-1.png


race-in-progress.gif (1082×710)
https://blog.streamlit.io/content/images/2023/06/race-in-progress.gif#border


Streamlit wizard and custom animated spinner
https://blog.streamlit.io/streamlit-wizard-form-with-custom-animated-spinner/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit wizard and custom animated spinner

Improve user experience with simplified data entry and step-by-step guidance

By Andrew Carson
Posted in Snowflake powered ❄️, May 15 2023
Session state variables and callback functions
View rendering
Customize the steps
Custom animated spinner (optional)
Radio button alternative
Conclusion
My favorite cloud technology resources
Snowflake
Streamlit
AWS
Contents
Share this post
← All posts

Over the past few months, I’ve had the pleasure of working with Streamlit on a variety of projects. For those new to it, Streamlit is an open-source Python framework for building web applications, specifically around visualization and data science.

A recent request - and overall very common use case - was unrelated to data visualization, but automation workflows requiring lengthy and complex user input. To simplify the data entry, I implemented a custom wizard form. Wizards are a great way to break down a complicated process into smaller, manageable steps, providing users with a clear path to follow. While Streamlit doesn't natively offer a wizard component, I easily recreated the functionality using a variety of widgets and session state variables.

In this post, we’ll walk through an example that simulates loading a file into Snowflake. I’ll be using reduced code snippets from the public GitHub repository located here.

In this post, you will:

Build a multi-step wizard form using various components, including a custom spinner (optional).
Learn how to manage an application’s views and control flow with session state.
Gain exposure to some fantastic open-source libraries being contributed by the Streamlit developer community.
🏂
NOTE: I’ll be using Streamlit Lottie for the optional animated spinner, which requires creating a Lottie File account. If skipped, check out the documentation and browse the Lottie File library for future use.

Let's get started!

Session state variables and callback functions

Create two session state variables and two callback functions that will work together to render views and steps to the user. Session state is Streamlit's way of preserving values across script re-runs, while callback functions are used to manage and update those values (read more on state management here).

For now, let's define them as follows:

if 'current_step' not in st.session_state:
    st.session_state['current_step'] = 1

if 'current_view' not in st.session_state:
    st.session_state['current_view'] = 'Grid'

### maintains the user's location within the wizard
def set_form_step(action,step=None):
    if action == 'Next':
        st.session_state['current_step'] = st.session_state['current_step'] + 1
    if action == 'Back':
        st.session_state['current_step'] = st.session_state['current_step'] - 1
    if action == 'Jump':
        st.session_state['current_step'] = step

### used to toggle back and forth between Grid View and Form View
def set_page_view(target_view):
    st.session_state['current_view'] = target_view

View rendering

To keep the code modular and render the views, you'll create two simple functions. The render_grid_view function uses the AgGrid custom component. If you're not familiar with AgGrid, take a look at Pablo Fonseca's example page. It's an excellent component for DataFrame visualization — and just one of many fantastic open-source libraries built by the Streamlit developer community.

The render_wizard_view function uses Streamlit buttons to control movement between steps. To determine whether the buttons should be displayed as primary or secondary, we'll add some ternary logic.

def render_grid_view():
    data = {"Table Name": ["Product", "Employee", "Customer"], "Schema": ["Salesforce", "Salesforce", "Salesforce"], "Rows": [200, 300, 400], "Size": ["10 kb", "10 kb", "10 kb"]}
    df = pd.DataFrame(data=data)

    gridOptions = {
		  "rowSelection": "single",        
		        "columnDefs": [
		         { "field": "Table Name", "checkboxSelection": True },
		            { "field": "Schema" },
		            { "field": "Rows" },
		            { "field": "Size" }
		     ]
		 }    

    return AgGrid(
        df,        
        gridOptions=gridOptions,
        theme="balham"
    )
    
def render_wizard_view(): 
    with st.expander('',expanded=True):     
        sf_header_cols = st.columns([1, 1.75, 1])
        
        with sf_header_cols[1]:            
            st.subheader('Load Data to Snowflake')
    
    # determines button color which should be red when user is on that given step
    wh_type = 'primary' if st.session_state['current_step'] == 1 else 'secondary'
    ff_type = 'primary' if st.session_state['current_step'] == 2 else 'secondary'
    lo_type = 'primary' if st.session_state['current_step'] == 3 else 'secondary'
    sf_type = 'primary' if st.session_state['current_step'] == 4 else 'secondary'

    step_cols = st.columns([.5, .85, .85, .85, .85, .5])    
    step_cols[1].button('Warehouses', on_click=set_form_step, args=['Jump', 1], type=wh_type)
    step_cols[2].button('File Format', on_click=set_form_step, args=['Jump', 2], type=ff_type)        
    step_cols[3].button('Load Options', on_click=set_form_step, args=['Jump', 3], type=lo_type)      
    step_cols[4].button('Source Files', on_click=set_form_step, args=['Jump', 4], type=sf_type)                   
        
    st.markdown('---')
    st.write(st.session_state['current_step'])
    st.markdown('---')
    disable_back_button = True if st.session_state['current_step'] == 1 else False
    disable_next_button = True if st.session_state['current_step'] == 4 else False

    form_footer_cols = st.columns([5,1,1,1.75])

    form_footer_cols[0].button('Cancel', on_click=set_page_view, args=['Grid'])
    form_footer_cols[1].button('Back', on_click=set_form_step, args=['Back'], disabled=disable_back_button)
    form_footer_cols[2].button('Next', on_click=set_form_step, args=['Next'], disabled=disable_next_button)
    form_footer_cols[3].button('📤 Load Table', disabled=True)


Now, the logic to render the view is a simple "if-else" statement:

if st.session_state['current_view'] == 'Grid':
	render_grid_view():
else:
	render_wizard_view()


It's that easy! At this point, your app's output should look something like this:

If your output is off somewhere, please feel free to reference the Python file located here.

🏂
NOTE: You may need to adjust the column sizes based on your browser size.
Customize the steps

Feel free to customize the individual steps or use the steps provided in the repository. Once implemented, your fully functional form will look something like this:

Custom animated spinner (optional)

Lastly, you can replace the native Streamlit spinner with a custom spinner of your own. You can follow the example using a combination of a Lottie animation and Streamlit progress bar or design your own using the Lottie file library. Then, we'll update our imports and add one more function to render the spinner:

from streamlit_lottie import st_lottie
import requests

def render_animation():
    animation_response = requests.get('<https://assets1.lottiefiles.com/packages/lf20_vykpwt8b.json>')
    animation_json = dict()
    
    if animation_response.status_code == 200:
        animation_json = animation_response.json()
    else:
        print("Error in the URL")     
                           
    return st_lottie(animation_json, height=200, width=300)


Here is our spinner in action:

Radio button alternative

In the example above, we used buttons to navigate through the wizard. An alternative approach is to use the radio button. It offers the same functionality with fewer lines of code since there is no need to worry about button color schemes.

Here is an example within another very common use case, a checkout form:

For easy reference, here is the full code repository.

Conclusion

To conclude, I'd like to thank you very much for taking the time to read my first article. I intend to produce content related to all things data engineering, data science, and any other topic the data community finds relevant and helpful. If you're interested in learning more, please feel free to leave comments.

My favorite cloud technology resources
Snowflake
Snowflake Quickstarts
Snowflake Labs
Snowflake Developers Youtube Channel
Data Engineering Best Practices
phData Blog
Data Engineering Simplified
Analytics Today Blog
Streamlit
Streamlit Custom Component Tracker
Best of Streamlit Examples
AWS
AWS Be a Better Dev
Cloud with Raj
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

paris.png (3000×1146)
https://blog.streamlit.io/content/images/2023/05/paris.png


eng-frekvenser.png (1728×838)
https://blog.streamlit.io/content/images/2023/05/eng-frekvenser.png


International_Morse_Code-1.png (1280×1639)
https://blog.streamlit.io/content/images/2023/05/International_Morse_Code-1.png


kingston-course-page.png (2163×1317)
https://blog.streamlit.io/content/images/2023/06/kingston-course-page.png#browser


Sasha Mitrovich - Streamlit
https://blog.streamlit.io/author/sasha/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Sasha Mitrovich
1 post
Build a Snowflake DATA LOADER on Streamlit in only 5 minutes

Drag and drop your Excel data to Snowflake with a Streamlit app

Snowflake powered ❄️
by
Sasha Mitrovich
,
May 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mathias Landhäußer - Streamlit
https://blog.streamlit.io/author/mathias/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mathias Landhäußer
1 post
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Stripplot.png (633×458)
https://blog.streamlit.io/content/images/2023/05/Stripplot.png#border


wrangled_tables.png (829×282)
https://blog.streamlit.io/content/images/2023/05/wrangled_tables.png#border


snowsight.png (476×186)
https://blog.streamlit.io/content/images/2023/05/snowsight.png#border


race-in-progress-leaderboard.png (899×728)
https://blog.streamlit.io/content/images/2023/06/race-in-progress-leaderboard.png#border


sidebar.png (484×203)
https://blog.streamlit.io/content/images/2023/06/sidebar.png#border


data-generation-process.png (1094×1235)
https://blog.streamlit.io/content/images/2023/06/data-generation-process.png#border


select-a-race.png (1089×424)
https://blog.streamlit.io/content/images/2023/06/select-a-race.png#border


Streamlit_Blog_Challenge.png (2000×818)
https://blog.streamlit.io/content/images/2023/06/Streamlit_Blog_Challenge.png


architecture-diagram.png (718×643)
https://blog.streamlit.io/content/images/2023/06/architecture-diagram.png


slide_10-1.gif (1920×1080)
https://blog.streamlit.io/content/images/2023/06/slide_10-1.gif


slide_05.png (1722×967)
https://blog.streamlit.io/content/images/2023/06/slide_05.png#browser


slid_08.png (1060×1061)
https://blog.streamlit.io/content/images/2023/06/slid_08.png


slide_07.png (1715×961)
https://blog.streamlit.io/content/images/2023/06/slide_07.png#browser


Monte Carlo simulations with Streamlit
https://blog.streamlit.io/monte-carlo-simulations-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monte Carlo simulations with Streamlit

Learn how to predict future stock prices

By Mats Stellwall
Posted in Snowflake powered ❄️, June 8 2023
Python environment
App structure
snf_functions.py functions
Monte_Carlo_Simulations.py
01_Snowflake_connect.py
02_Run_Monte_Carlo_Simulations.py
Final app
Step 1
Step 2
Step 3
Wrapping up
Contents
Share this post
← All posts

A Monte Carlo simulation, also known as a probability simulation, is a method for predicting possible outcomes of uncertain events. It consists of input variables, output variables, and a mathematical model. The name Monte Carlo comes from the famous casino town of Monaco, as chance is a core element of the modeling approach, similar to a game of roulette.

I've used Monte Carlo simulations to predict future stock prices with Snowpark for Python. Since running them was an interactive process, I decided to make a Streamlit app that lets users set parameters and generate predictions—to predict future stock prices. 🎲

In this post, I'll show you how to build it step-by-step.

🎲
You can find the code for the app in my GitHub repository.
Python environment

To start, set up your Python environment (to use any Python-supported IDE for development):

Install Python 3.8
Install libraries streamlit, snowflake-snowpark-python, scipy, and plotly
App structure

This is a multipage app, so you'll use different Python files for each page.

For example, you'll use 01_Snowflake_connect.py to handle the connection to Snowflake and 02_Run_Monte_Carlo_Simulations.py to run and display the simulations. These files will be imported into the main file, Monte_Carlo_Simulations.py. You'll also create a Python file snf_functions.py to store common functions.

Here is the app's structure:

mcs
 |- lib
      |- snf_functions.py
 |- pages
      |- 01_Snowflake_connect.py
      |- 02_Run_Monte_Carlo_Simulations
 |- Monte_Carlo_Simulations.py

snf_functions.py functions

The purpose of the snf_functions.py file is to hold common functions, such as connecting/disconnecting to Snowflake or retrieving the names of database objects.

To start, import the necessary modules and functions:

import streamlit as st

from scipy.stats import norm
import numpy as np
from typing import Tuple, Iterable

from snowflake.snowpark import Session
import snowflake.snowpark.functions as F
import snowflake.snowpark.types as T


Next, you'll need functions to connect to and disconnect from the Snowflake account. The st.session_state object lets you store variables that are available across multiple pages.

For example, you can set the account, user, password, and virtual warehouse values in your login page and then use them in your function:

def connect_to_snf():
    if 'snowsession' in st.session_state:
        return st.session_state['snowsession']

    creds = {
        'account': st.session_state['snow_account'],
        'user': st.session_state['snow_user'],
        'password': st.session_state['snow_password'],
        'warehouse': st.session_state['snow_wh']
    }
    session = Session.builder.configs(creds).create()
    st.session_state['snowsession'] = session

    return session

def disconnect_snf():
    if 'snowsession' in st.session_state:
        session = st.session_state['snowsession']
        session.close()
        del st.session_state['snowsession']
        del st.session_state['install_db']
        del st.session_state['install_schema']
        del st.session_state['install_stage']


The app relies on a couple of UDFs to run simulations.

There are two ways to handle this:

Assume they're available
Handle their deployment as part of the app flow

Let's choose the latter and create a function that checks if the UDFs exist in Snowflake.

The @st.cache_data decorator ensures that the function runs only if any parameters have changed. Otherwise, it returns the value from the last run. This avoids unnecessary data selections and makes the app run faster:

@st.cache_data()
def check_udfs(data_db: str, data_schema: str):
    snf_session = st.session_state['snowsession']
    udf_funcs = ['NORM_PPF', 'COLLECT_LIST', 'CALC_CLOSE']

    n_udfs = snf_session.table(f"{data_db}.INFORMATION_SCHEMA.FUNCTIONS").filter(
        (F.col("FUNCTION_SCHEMA") == F.lit(data_schema)) & (F.col("FUNCTION_NAME").in_(udf_funcs))).count()

    if n_udfs == len(udf_funcs):
        st.session_state['install_stage'] = ''
        return True
    else:
        return False


You'll also need a function to deploy the UDFs, in the case where they do not exist.

This code can be improved, but for now, it'll do the job:

def deploy_udf():
		# Values are set when connecting to to Snowflake
    snf_session = st.session_state['snowsession']
    data_db = st.session_state['install_db']
    data_schema = st.session_state['install_schema']
    stage_name = st.session_state['install_stage']

    stage_loc = data_db + '.' + data_schema + '.' + stage_name
    # Check for stage and create it if it does not exists
    snf_session.sql(f"CREATE STAGE IF NOT EXISTS {stage_name}").collect()

    @F.udf(name=f"{data_db}.{data_schema}.norm_ppf", is_permanent=True, replace=True, packages=["scipy"],
           stage_location=stage_loc)
    def norm_ppf(pd_series: T.PandasSeries[float]) -> T.PandasSeries[float]:
        return norm.ppf(pd_series)

    @F.udtf(name=f"{data_db}.{data_schema}.collect_list", is_permanent=True, replace=True
        , packages=["typing"], output_schema=T.StructType([T.StructField("list", T.ArrayType())])
        , stage_location=stage_loc)
    class CollectListHandler:
        def __init__(self) -> None:
            self.list = []

        def process(self, element: float) -> Iterable[Tuple[list]]:
            self.list.append(element)
            yield (self.list,)

    @F.udf(name=f"{data_db}.{data_schema}.calc_close", is_permanent=True, replace=True
        , packages=["numpy"], stage_location=stage_loc)
    def calc_return(last_close: float, daily_return: list) -> float:
        pred_close = last_close * np.prod(daily_return)
        return float(pred_close)

    return True


The user will be able to choose in which database and schema they want to install the UDFs when connecting to their Snowflake account. Since the users will also have the possibility to choose the table to use for the simulations as well the table to save the simulations to, I need functions to get the databases, schema, tables and columns.

Use st.cache_data to retrieve the names of those objects and to ensure that the SQL query is only run when something has changed:

@st.cache_data()
def get_databases():
    snf_session = st.session_state['snowsession']
    lst_db = [dbRow[1] for dbRow in snf_session.sql("SHOW DATABASES").collect()]
    # Add a default None value
    lst_db.insert(0, None)
    return lst_db

@st.cache_data()
def get_schemas(db: str):
    snf_session = st.session_state['snowsession']
    lst_schema = [schemaRow[0] for schemaRow in snf_session.sql(
        f"SELECT SCHEMA_NAME FROM {db}.INFORMATION_SCHEMA.SCHEMATA WHERE CATALOG_NAME = '{db.upper()}' AND SCHEMA_NAME != 'INFORMATION_SCHEMA' ORDER BY 1").collect()]
    lst_schema.insert(0, None)
    return lst_schema

@st.cache_data()
def get_tables(db: str, schema: str):
    snf_session = st.session_state['snowsession']
    lst_table = [tableRow[0] for tableRow in snf_session.sql(
        f"SELECT TABLE_NAME FROM {db}.INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = '{db.upper()}' AND TABLE_SCHEMA='{schema.upper()}' ORDER BY 1").collect()]
    lst_table.insert(0, None)
    return lst_table

@st.cache_data()
def get_columns(db: str, schema: str, table: str):
    snf_session = st.session_state['snowsession']
    lst_column = [columnRow[0] for columnRow in snf_session.sql(
        f"SELECT COLUMN_NAME, DATA_TYPE FROM {db}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_CATALOG = '{db.upper()}' AND TABLE_SCHEMA='{schema.upper()}' AND TABLE_NAME = '{table.upper()}' ORDER BY 1").collect()]
    return lst_column

Monte_Carlo_Simulations.py

The main app file Monte_Carlo_Simulations.py is used as input when running Streamlit. The st.title object adds the app title. You can even use emoji shortcodes with it! 😄

To add a description, use st.write:

import streamlit as st

st.title("Monte Carlo Simulations :spades:")
st.write(
    """ 
    A demo showing how Monte Carlo simulations can be used to predict the future stock price for P&G.

    A Monte Carlo simulation is a mathematical technique, which is used to estimate the possible outcomes of an uncertain event. 
    A Monte Carlo analysis consists of input variables, output variables, and a mathematical model. 

    This demo is using the following mathematical model:

             Stock Price Today = Stock Price Yesterday * e^r

    To calculate r the geometric Brownian motion (GBM) model is used.

    Start by connecting to your Snowflake account, using the **Snowflake connect** link in the sidebar.

    """
)

01_Snowflake_connect.py

The 01_Snowflake_connect.py file contains the logic for connecting to Snowflake. It begins with the necessary imports and uses the helper functions created in the snf_functions.py file.

import streamlit as st
from lib.snf_functions import get_databases, get_schemas, get_tables, get_columns, deploy_udf, check_udfs, connect_to_snf, disconnect_snf


Define a function that displays the disconnect button along with some information (to display it in multiple places):

def dispaly_disconnect():
    st.write("""
    Everything is set up for running Monte Carlo simulations.

    Choose **Run Monte Carlo simulations** in the sidebar to continue.
    """)
    with st.form('Snowflake Connection'):
        st.form_submit_button('Disconnect', on_click=disconnect_snf)


Next, build out the structure and logic for your page, starting with a title. This time, use the st.markdown component, which lets you format strings using markdown.

Check if there is an active connection to Snowflake by looking for the snowsession key in the st.session_state object. If there isn't one, create an input form using st.form.

By using the key parameter in the st.text_input, you can get the entered values and add them to the st.session_state object.

When the st.form_submit_button is clicked, the connect_to_snf function is called:

st.markdown("# ❄️ Snowflake Connection")
st.sidebar.markdown("# Snowflake Connection ❄️")
# Check if there is a active connection...
if "snowsession" not in st.session_state:
    with st.form('Snowflake Credentials'):
        st.text_input('Snowflake account', key='snow_account')
        st.text_input('Snowflake user', key='snow_user')
        st.text_input('Snowflake password', key='snow_password', type='password')
        st.text_input('Snowflake warehouse', key='snow_wh')
        st.form_submit_button('Connect', on_click=connect_to_snf)
        st.stop()


If there is an active connection, the user can specify the database and schema into which they have installed or want to install the UDFs.

Use empty lists to prompt the user to select the values for the database and schema (in that order):

else:
    if st.session_state['snowsession']:
				# Assumption is that if a user already have set the install_schema state 
				# then 
        if 'install_schema' not in st.session_state:
            snf_session = st.session_state['snowsession']
            st.write("""
            You are now connected to your Snowflake account!
            
            Select the database and schema where the UDFs for doing Monte Carlo Simulations exists in or to be installed in 
            """)
            lst_databases = get_databases()
            sel_db = st.selectbox("Database", lst_databases)
            if sel_db:
                lst_schemas = get_schemas(sel_db)
                st.session_state['install_db'] = sel_db
                snf_session.use_database(sel_db)
            else:
                lst_schemas = []

            sel_schema = st.selectbox("Schema", options=lst_schemas)


After a user selects the schema, a check is performed to determine whether the UDFs exist there.

If they don't, the user is given the option to install them in the selected schema:

            if sel_schema:
                st.session_state['install_schema'] = sel_schema
                snf_session.use_schema(sel_schema)
                if check_udfs(sel_db, sel_schema):
                    dispaly_disconnect()
                else:
                    st.write("""
                    The selected database and schema is missing the UDFs needed for doing the Monte Carlo simulations.
                    
                    Set the stage name for the internal stage to be used for deployment, if it does not exists it will be created. 
                    """
                    )
                    with st.form('Deploy UDfs'):
                        stage_nm = st.text_input(label="Stage name", value="MCS_STAGE", key="install_stage")
                        st.form_submit_button('Deploy', on_click=deploy_udf)
                        st.stop()


If everything is installed, display the disconnect button:

        else:
            dispaly_disconnect()

02_Run_Monte_Carlo_Simulations.py

The 02_Run_Monte_Carlo_Simulations.py file contains the structure and logic for running the simulations.

Import the necessary libraries and check if there is a connection to a Snowflake account:

import streamlit as st
import snowflake.snowpark.functions as F
from snowflake.snowpark import Column, Window
from lib.snf_functions import get_databases, get_schemas, get_tables, get_columns, deploy_udf
import plotly.express as px

# Get the current credentials
if "snowsession" in st.session_state:
    snf_session = st.session_state['snowsession']
else:
    st.write("**Please log into you Snowflake account first!**")
    st.stop()


In my other post, I described the function used to run the simulations and outlined all the necessary steps:

def run_simulations(df, n_days, n_sim_runs):

    def pct_change(indx_col: Column, val_col: Column):
        return ((val_col - F.lag(val_col, 1).over(Window.orderBy(indx_col))) / F.lag(val_col, 1).over(Window.orderBy(indx_col)))
    
    # Calculate the log return by day
    df_log_returns = df_closing.select(F.col("DATE"), F.col("CLOSE")
                           ,F.call_function("LN", (F.lit(1) + pct_change(F.col("DATE"), F.col("CLOSE")))).as_("log_return"))
    
    # Get the u, var, stddev and last closing price
    df_params = df_log_returns.select(F.mean("LOG_RETURN").as_("u")
                 , F.variance("LOG_RETURN").as_("var")
                 , F.stddev("LOG_RETURN").as_("std_dev")
                ,F.max(F.col("last_close")).as_("LAST_CLOSE"))\\
            .with_column("drift", (F.col("u")-(F.lit(0.5)*F.col("var"))))\\
            .select("std_dev", "drift", "last_close")
    
    # Generates rows for the number of days and simulations by day
    df_days = snf_session.generator(F.row_number().over(Window.order_by(F.seq4())).as_("day_id") ,rowcount=n_days)
    df_sim_runs = snf_session.generator(F.row_number().over(Window.order_by(F.seq4())).as_("sim_run") ,rowcount=n_sim_runs)
	
    df_daily_returns = df_days.join(df_sim_runs).join(df_params)\\
        .select("day_id", "sim_run"
                , F.exp(F.col("drift") + F.col("std_dev") *  F.call_function(f"{data_db}.{data_schema}.norm_ppf", F.uniform(0.0,1.0,F.random()))).as_("daily_return")
               , F.lit(None).as_("SIM_CLOSE"))\\
        .sort(F.col("DAY_ID"), F.col("sim_run"))
	
		# Generate a day 0 row with the last closing price for each simulation run
    last_close = df_params.select("LAST_CLOSE").collect()[0][0]
    df_day_0 = snf_session.generator(F.lit(0).as_("DAY_ID"),  
                                    F.row_number().over(Window.order_by(F.seq4())).as_("SIM_RUN")
                                    , F.lit(1.0).as_("DAILY_RETURN") ,F.lit(last_close).as_("SIM_CLOSE"), rowcount=n_sim_runs)

    # Union the dataframes,
    df_simulations = df_day_0.union_all(df_daily_returns)

    df_simulations_calc_input = df_simulations.with_column("SIM_CLOSE_0", F.first_value(F.col("SIM_CLOSE")).over(Window.partition_by("SIM_RUN").order_by("DAY_ID") ) )\\
                .with_column("L_DAILY_RETURN", F.call_table_function(f"{data_db}.{data_schema}.collect_list", F.col("DAILY_RETURN")).over(partition_by="SIM_RUN", order_by="DAY_ID"))

    df_sim_close = df_simulations_calc_input.with_column("SIM_CLOSE", 
                                                         F.call_function(f"{data_db}.{data_schema}.calc_close"
                                                                    , F.col("SIM_CLOSE_0"),F.col("L_DAILY_RETURN")))
    
    # Cache the returning Snowpark Dataframe so we do not run it multiple times when visulazing etc
    return df_sim_close.select("DAY_ID", "SIM_RUN", "SIM_CLOSE").cache_result()


It also has a function for displaying the results of the simulations:

def display_sim_result(df):
    pd_simulations = df.sort("DAY_ID", "SIM_RUN").to_pandas()
    
    fig = px.line(pd_simulations, x="DAY_ID", y="SIM_CLOSE", color='SIM_RUN', render_mode='svg')
    st.plotly_chart(fig, use_container_width=True)
    metrics = df.select(F.round(F.mean(F.col("SIM_CLOSE")), 2)
                                       , F.round(F.percentile_cont(0.05).within_group("SIM_CLOSE"), 2)
                                       , F.round(F.percentile_cont(0.95).within_group("SIM_CLOSE"), 2)).collect()
    st.write("Expected price: ", metrics[0][0])
    st.write(f"Quantile (5%): ", metrics[0][1])
    st.write(f"Quantile (95%): ", metrics[0][2])


Next, add the GUI components and logic:

st.sidebar.markdown("# Simulation Parameters")
st.title("Monte Carlo Simulations :spades:")
st.write("Start by choosing the table and columns with the date and stock prices that is going to be used for the simulations.")


To let users change the number of days and simulations per day, create a sidebar with the st.sidebar widget and two sliders—one for days and one for simulations. Whenever the sliders are adjusted, the variables n_days and n_iterations will be updated.

To track the user clicks on the "Run Simulations" button, use st.session_state:

with st.sidebar:
    with st.form(key="simulation_param"):
        n_days = st.slider('Number of Days to Generate', 1, 1000, 100)
        n_iterations = st.slider('Number of Simulations by Day', 1, 100, 20)
        st.session_state.start_sim_clicked = st.form_submit_button(label="Run Simulations")


Add select boxes to let users select the database, table, and columns. Use the previously defined functions to retrieve the data displayed in the widgets, and use empty lists to ensure that the user selects the value for the database, schema, table, and columns (in that order).

Once the user has selected the date and stock price columns, generate a Snowpark DataFrame and plot the data:

lst_databases = get_databases()
col1, col2, col3, col4 = st.columns(4)
sel_db = col1.selectbox("Database", lst_databases)
if sel_db:
    lst_schemas = get_schemas(sel_db)
else:
    lst_schemas = []

sel_schema = col2.selectbox("Schema", options=lst_schemas)

if sel_schema:
    lst_tables = get_tables(sel_db, sel_schema)
else:
    lst_tables = []

sel_table = col3.selectbox("Table", lst_tables)
if sel_table:
    lst_columns = get_columns(sel_db, sel_schema, sel_table)
else:
    lst_columns = []

sel_columns = col4.multiselect("Columns", lst_columns, max_selections=2)
if len(sel_columns) == 2:
    df_closing = snf_session.table(f"{sel_db}.{sel_schema}.{sel_table}").select(F.col(sel_columns[0]).as_("DATE"), F.col(sel_columns[1]).as_("CLOSE"))
    st.line_chart(df_closing.to_pandas(),x="DATE", y="CLOSE")


To check if the user has run the simulations after clicking the "Run Simulations" button, use the display_sim_result function.

Additionally, store the Snowpark DataFrame containing the results in the session state:

if st.session_state.start_sim_clicked:
    with st.spinner('Running simulations...'):
        df_simulations = run_simulations(df_closing, n_days, n_iterations)
        display_sim_result(df_simulations)
        st.session_state["df"] = df_simulations
				st.session_state.start_sim_clicked = False


To determine whether to display the "Save Results" button, check if the Snowpark DataFrame containing the simulation results is present in the session state (stored in the variable "df").

To let the user specify the database, schema, and table name, store the result, and display the selection boxes for choosing the database and schema and text input for the table name.

When the user clicks the button, the value of "save" changes to True. The user can then select the database and schema to save the result, along with the table name. The simulation result (stored in st.session_state["df"]) is permanently saved to a table.

The "display_sim_result" function is called to ensure that the simulation results are displayed after the data has been saved:

if "df" in st.session_state:
    st.write("Choose the database and schema to save the data into:")
    save_db = st.selectbox("Database", lst_databases, key="save_db")
    if save_db:
        lst_schemas = get_schemas(save_db)
        snf_session.use_database(save_db)
    else:
        lst_schemas = []
    save_schema = st.selectbox("Schema", options=lst_schemas, key="save_schema")

    if save_schema:
        have_schema = False
    else:
        have_schema = True

    save_tbl = st.text_input(label="Table name", value="STOCK_PRICE_SIMULATIONS", disabled=have_schema)
    save = st.button("❄️ Save results", key="save_sims")
    if save:
        df_simulations = st.session_state["df"]
        display_sim_result(df_simulations)
        with st.spinner("Saving data..."):
            df_simulations.write.mode('overwrite').save_as_table(f"{save_db}.{save_schema}.{save_tbl}")
            st.success(f"✅ Successfully wrote simulations to {save_db}.{save_schema}.{save_tbl}!")
            st.session_state["saved"] = True

Final app

Here is what the final app will look like and how you'd use it step-by-step.

Step 1

Log in to your Snowflake account:

Step 2

Select the columns and tables to base the simulations on, and set the number of days and simulations per day. Then click on "Run Simulations" to see a chart with the simulations, the expected price (the mean price of all simulations), and the 5% and 95% quantile values:

Step 3

Click on "Save to Snowflake" to save it to a table:

And you're done! Congratulations! 🎉

Wrapping up

I hope this post has inspired you to make your own prediction app. To use the code, remember to install the Streamlit library locally using pip. If you have any questions, please post them below or contact me on LinkedIn.

Happy app-building and predicting! 🎲

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

slide_06.png (1579×432)
https://blog.streamlit.io/content/images/2023/06/slide_06.png


Build a Snowflake DATA LOADER on Streamlit in only 5 minutes
https://blog.streamlit.io/build-a-snowflake-data-loader-on-streamlit-in-only-5-minutes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build a Snowflake DATA LOADER on Streamlit in only 5 minutes

Drag and drop your Excel data to Snowflake with a Streamlit app

By Sasha Mitrovich
Posted in Snowflake powered ❄️, May 9 2023
1. Create a virtual Python environment for Streamlit
2. Connect to Snowflake from Streamlit
3. Create a simple drag-and-drop UI in Streamlit for CSV files
4. Load the dropped file to Snowflake
Bonus: Add data quality checks
Wrapping up
Contents
Share this post
← All posts

A customer recently challenged me to create a data loader app for Snowflake that even kindergarteners could use. And when I say kindergarteners, I’m talking about the business users who think SQL is a mythical creature.

Now, I’m not one to back down from a challenge, especially when someone’s already promised something on my behalf. So, I decided to take on the task and create a data loader app for kindergarteners in just five minutes. And let me tell you, it was a wild ride.

By the end of this post, you’ll learn how to create a simple drag-and-drop data loader app on Streamlit that anyone can use.

I work as a solutions engineer at Snowflake. My passion is data, data science, and building data applications that I can showcase to my customers. Streamlit is just the tool for that.

And it complements Snowflake’s Data Cloud platform perfectly. While database specialists working in SQL and data engineers using Python DataFrames feel at home working with Snowflake, it’s a bit different for business users. There’s no easy way for non-technical users to drop data into Snowflake and jump into their business intelligence tool of choice, such as Tableau, to analyze it and share it with others.

In this post, I’m changing all that! You’ll learn how to:

Create a virtual Python environment for Streamlit
Connect to Snowflake from Streamlit
Create a simple drag-and-drop UI in Streamlit for CSV files
Load the dropped file to Snowflake
Bonus: Add data quality checks

Keep reading till the end. I’ll also show how to add quality checks on the loaded data and display that in the app for the business to immediately assess the data quality with just a glance at the UI.

Let’s start.

1. Create a virtual Python environment for Streamlit

I use conda to manage my virtual environments so I can work with correct versions of Python packages and avoid the dependency hell. If you don’t know what that is, I’ve explained it in my video “Stuck learning Python? Make it fun with Streamlit!”

Here’s how to create an environment for our app with all the necessary Python packages:

conda create --name snowshovel -c <https://repo.anaconda.com/pkgs/snowflake> python=3.8 pandas snowflake-snowpark-python
conda activate snowshovel
conda install -c conda-forge streamlit


If you don’t use conda, that’s fine; you can install these packages using pip, for instance.

2. Connect to Snowflake from Streamlit

We’ll store our credentials in the creds.json.Here’s an example that contains all the required properties for connecting to Snowflake:

{
    "account": "account.region",
    "user": "myuser",
    "role": "myrole",
    "password": "************",
    "warehouse": "mywarehouser",
    "database": "mydb",
    "schema": "myschema"
  }


Make sure to replace these placeholders with real values for your Snowflake account.

We’ll use this credentials file later to create a connection to Snowflake. Snowflake supports many other authentication methods, such as key-pair or single sign-on.

3. Create a simple drag-and-drop UI in Streamlit for CSV files

I use VS Code as my development environment. If you want to learn why I like VS Code so much — make sure you watch this clip:

Let’s jump into VS Code and start working on our Python code.

To start building a Streamlit app, we need to import the streamlit Python package, like this:

import streamlit as st


Now we can run the app from the terminal like so:

streamlit run app.py


Now that we have the app running let’s add the amazing file uploader component that does all the job for us:

file = st.file_uploader("Drop your CSV here to load to Snowflake", type={"csv"})


That’s it. It literally takes two lines of code to build a Drag and Drop Web app with Streamlit. If anyone can beat this, I’m buying them a beverage of their choice (and I will ship it internationally).

4. Load the dropped file to Snowflake

The return value of the file uploader component is an UploadedFile class object, a subclass of BytesIO. Therefore, it is “file-like.” This means you can use that anywhere a file is expected and read its meta-data to get the filename, for example.

That’s exactly what we need in the next step.

We’ll load this file to a Pandas DataFrame. Why Pandas DataFrame? For two reasons:

Pandas has a convenient read_csv() method that will infer the CSV schema automatically, so we don’t need to build that complex logic of recognizing column types ourselves. Yay to the open-source community!
Pandas can be serialized to a Snowflake table with a single line of code using Snowflake’s Snowpark function write_pandas(). The table will be automatically created, re-using the schema from the Pandas DataFrame.

Let’s add these two lines of code to complete our app:

file_df = pd.read_csv(file)
snowparkDf=session.write_pandas(file_df,file.name,auto_create_table = True, overwrite=True)


Notice I’m using the session object to serialize the DataFrame to a Snowflake table, and remember, we’ve prepared a JSON file with our Snowflake credentials.

Let’s create that object before using it in our code. Here’s the complete application code for our simple Snowflake data loader. I’ve imported all the needed Python packages and checked if a file has been dropped.

import streamlit as st
import pandas as pd
import json
from snowflake.snowpark import Session

# connect to Snowflake
with open('creds.json') as f:
    connection_parameters = json.load(f)  
session = Session.builder.configs(connection_parameters).create()

file = st.file_uploader("Drop your CSV here to load to Snowflake", type={"csv"})
file_df = pd.read_csv(file)
snowparkDf=session.write_pandas(file_df,file.name,auto_create_table = True, overwrite=True)


That’s it. I’ve written a CSV data loader for Snowflake with less than 20 lines of Python code in under 5 minutes!

Bonus: Add data quality checks

Now that we’ve enabled business users to load their data to Snowflake without knowing anything about Snowflake, let’s provide some data quality checks.

With this, they can immediately see what they have in terms of data quality and make informed decisions on how to process this data further so they get added value from it.

Here’s a Python function that does just that:

def describeSnowparkDF(snowpark_df: snowpark.DataFrame):
    
    st.write("Here's some stats about the loaded data:")
    numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]
    numeric_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in numeric_types]

    # Get categorical columns
    categorical_types = [T.StringType]
    categorical_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in categorical_types]

    st.write("Relational schema:")
  
    columns = [c for c in snowpark_df.schema.fields]
    st.write(columns)
    
    col1, col2, = st.columns(2)
    with col1:
        st.write('Numeric columns:\\t', numeric_columns)

    with col2:
        st.write('Categorical columns:\\t', categorical_columns)
    
    # Calculte statistics for our dataset
    st.dataframe(snowpark_df.describe().sort('SUMMARY'), use_container_width=True)


Let’s break this down a little bit.

First, we’ll list all the column names and types for the newly created table. This is useful to check if the schema was inferred as the user expected:

  st.write("Here's some stats about the loaded data:")
  numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]
  numeric_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in numeric_types]

  # Get categorical columns
  categorical_types = [T.StringType]
  categorical_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in categorical_types]
  st.write("Relational schema:")

  columns = [c for c in snowpark_df.schema.fields]
  st.write(columns)


Then, we’ll present two more lists: numeric columns and categorical columns. This is useful to understand which further data processing we might undertake, like enriching with more data from other sources or transforming the values in a way required for analysis, like machine learning, perhaps:

col1, col2, = st.columns(2)
with col1:
  st.write('Numeric columns:\\\\t', numeric_columns)
with col2:
  st.write('Categorical columns:\\\\t', categorical_columns)


Finally, we’ll show the column statistics for each column, including counts of null values or value ranges. Based on this, the business user can decide if they will be able to use this dataset for their intended analysis with just a glance:

# Calculte statistics for our dataset
st.dataframe(snowpark_df.describe().sort('SUMMARY'), use_container_width=True)

Wrapping up

I built a basic data loader app for Snowflake that any business user immediately understands and can use. Building this app took me only 5 minutes and less than 20 lines of code.

Then, I spent another 15 minutes providing meaningful information about the loaded data so a business user could decide on the data quality with just a glance.

You’ll fund the code for this app and instructions on how to install and use it in this GitHub repo.

If you found this post useful, clap and subscribe to my Medium. And if you have any questions, please post them in the comments below or contact me on LinkedIn.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

slide_04.png (1715×965)
https://blog.streamlit.io/content/images/2023/06/slide_04.png#browser


slide_03.png (1721×963)
https://blog.streamlit.io/content/images/2023/06/slide_03.png#browser


screen_recording_scaling-0.5_fps-20_speed-9.84_duration-2-18.gif (855×401)
https://blog.streamlit.io/content/images/2023/05/screen_recording_scaling-0.5_fps-20_speed-9.84_duration-2-18.gif#browser


slide_02.png (1911×1072)
https://blog.streamlit.io/content/images/2023/06/slide_02.png#browser


Andrew Carson - Streamlit
https://blog.streamlit.io/author/andrew/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Andrew Carson
1 post
Streamlit wizard and custom animated spinner

Improve user experience with simplified data entry and step-by-step guidance

Snowflake powered ❄️
by
Andrew Carson
,
May 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mats Stellwall - Streamlit
https://blog.streamlit.io/author/mats/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mats Stellwall
1 post
Monte Carlo simulations with Streamlit

Learn how to predict future stock prices

Snowflake powered ❄️
by
Mats Stellwall
,
June 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit
https://blog.streamlit.io/tag/community/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
404
Page not found
← Go to the front page
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

IMG_0430.png (2000×1499)
https://blog.streamlit.io/content/images/2022/08/IMG_0430.png#browser


153F5C49-8CDC-4B1E-AFAE-AECC7AA4F849.jpeg (2000×1499)
https://blog.streamlit.io/content/images/2022/08/153F5C49-8CDC-4B1E-AFAE-AECC7AA4F849.jpeg#browser


EE1466B0-0A0C-4BD9-8B1E-02A37A380CF4.jpeg (2000×1336)
https://blog.streamlit.io/content/images/2022/08/EE1466B0-0A0C-4BD9-8B1E-02A37A380CF4.jpeg#browser


9E0C7298-B079-4074-9DBB-EE9C04D14C31.jpeg (2000×1177)
https://blog.streamlit.io/content/images/2022/08/9E0C7298-B079-4074-9DBB-EE9C04D14C31.jpeg#browser


72631AEA-B0B9-412E-BA7C-06B3382855FA.jpeg (2000×1257)
https://blog.streamlit.io/content/images/2022/08/72631AEA-B0B9-412E-BA7C-06B3382855FA.jpeg#browser


1825A4E6-9D5C-43DF-A4E3-DCC3FE2A5F37.jpeg (2000×2382)
https://blog.streamlit.io/content/images/2022/08/1825A4E6-9D5C-43DF-A4E3-DCC3FE2A5F37.jpeg#browser


EDA_app.gif (1280×1008)
https://blog.streamlit.io/content/images/2022/01/EDA_app.gif


1F88C1F7-893E-432D-9E7B-AEFD23D4D0B3.jpeg (2000×1748)
https://blog.streamlit.io/content/images/2022/08/1F88C1F7-893E-432D-9E7B-AEFD23D4D0B3.jpeg#browser


7A37797E-C57A-4560-8104-790FA5537DEF.jpeg (2000×1270)
https://blog.streamlit.io/content/images/2022/08/7A37797E-C57A-4560-8104-790FA5537DEF.jpeg#browser


A15327B7-F711-448A-BB97-A1CF2580BBC7.jpeg (2000×1090)
https://blog.streamlit.io/content/images/2022/08/A15327B7-F711-448A-BB97-A1CF2580BBC7.jpeg#browser


16FA6767-9630-4C65-869D-77E2B2FC4199.jpeg (2000×1263)
https://blog.streamlit.io/content/images/2022/08/16FA6767-9630-4C65-869D-77E2B2FC4199.jpeg#browser


F2A5148A-2F6F-48D2-B935-A38542A87468.jpeg (2000×1289)
https://blog.streamlit.io/content/images/2022/08/F2A5148A-2F6F-48D2-B935-A38542A87468.jpeg#browser


6FD187E5-2A74-4205-97B9-E19890E6C741.jpeg (2000×1210)
https://blog.streamlit.io/content/images/2022/08/6FD187E5-2A74-4205-97B9-E19890E6C741.jpeg#browser


2B353264-DDBA-4251-94E4-7CF77A256B9B.jpeg (2000×2199)
https://blog.streamlit.io/content/images/2022/08/2B353264-DDBA-4251-94E4-7CF77A256B9B.jpeg#browser


324BEA1A-997C-49E7-A279-040300162E27.jpeg (2000×1526)
https://blog.streamlit.io/content/images/2022/08/324BEA1A-997C-49E7-A279-040300162E27.jpeg#browser


E55BB258-DA30-4262-BDAA-7B2C6A0E5E15.jpeg (1800×1510)
https://blog.streamlit.io/content/images/2022/08/E55BB258-DA30-4262-BDAA-7B2C6A0E5E15.jpeg


40F6254E-3523-4ADE-B9B6-D4436FE8B68A.jpeg (2000×1375)
https://blog.streamlit.io/content/images/2022/08/40F6254E-3523-4ADE-B9B6-D4436FE8B68A.jpeg#browser


trubrics_ml_feedback_platform.png (1351×559)
https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_platform.png#browser


Joel Hodgson - Streamlit
https://blog.streamlit.io/author/joel/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Joel Hodgson
1 post
Collecting user feedback on ML in Streamlit

Improve user engagement and model quality with the new Trubrics feedback component

Advocate Posts
by
Jeff Kayne and 
1
 more,
May 4 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

trubrics_ml_feedback_type_custom-1.png (1211×771)
https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_custom-1.png#browser


trubrics_ml_feedback_type_thumbs.png (1208×729)
https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_thumbs.png#browser


trubrics_ml_feedback_type_faces.png (1211×212)
https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_faces.png#browser


trubrics_ml_feedback_type_issue.png (1216×405)
https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_issue.png#browser


EDA-app.png (1970×1558)
https://blog.streamlit.io/content/images/2022/01/EDA-app.png


model-building.jpeg (1941×1745)
https://blog.streamlit.io/content/images/2022/01/model-building.jpeg


streamlit-demo-app-1.gif (897×616)
https://blog.streamlit.io/content/images/2022/11/streamlit-demo-app-1.gif#browser


Streamlit
https://blog.streamlit.io/streamlit-quests-getting-started-with-streamlit/github.com/streamlit/app-starter-kit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
404
Page not found
← Go to the front page
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Make your st.pyplot interactive!
https://blog.streamlit.io/make-your-st-pyplot-interactive/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Make your st.pyplot interactive!

Learn how to make your pyplot charts interactive in a few simple steps

By William Huang
Posted in Tutorials, June 23 2022
1. Simple example
Step 1. Create a basic Matplotlib chart
Step 2. Make the chart interactive
2. Advanced example
Step 1. Render the graph statically
Step 2. Make the graph interactive with mpld3
Step 3. Add tooltips for even more interactivity
Note: mpld3 limitations
Wrapping up
Contents
Share this post
← All posts

Matplotlib is one of the most popular charting libraries in Python. It’s also a popular way to add charts to your Streamlit apps. Just use st.pyplot!

But Matplotlib charts are static images. No zooming or moving the chart around.

In this post, I'll show you how to make them interactive:

1. Simple example:

Step 1. Create a basic Matplotlib chart
Step 2. Make the chart interactive

2. Advanced example:

Step 1. Render the graph statically
Step 2. Make the graph interactive with mpld3
Step 3. Add tooltips for even more interactivity

TLDR? Use mpld3 and render pyplots with Streamlit’s built-in st.pyplot command. With a few lines of code, you can add panning, zooming, resetting, and rendering!

1. Simple example
Step 1. Create a basic Matplotlib chart

First, create a basic Matplotlib chart and add it to your Streamlit app (you’ll add interactivity later).

Here is what the code will look like:

import streamlit as st
import matplotlib.pyplot as plt

#create your figure and get the figure object returned
fig = plt.figure() 
plt.plot([1, 2, 3, 4, 5]) 

st.pyplot(fig)


And here’s what your app should look like now:

Step 2. Make the chart interactive

Making this chart interactive is super simple.

Use the fantastic mpld3 library. It’ll convert the Matplotlib figure (fig) into an interactive Javascript representation and return it as HTML. Embed this HTML snippet in your app via Streamlit’s custom component API:

import matplotlib.pyplot as plt
import mpld3
import streamlit.components.v1 as components

#create your figure and get the figure object returned
fig = plt.figure() 
plt.plot([1, 2, 3, 4, 5]) 

fig_html = mpld3.fig_to_html(fig)
components.html(fig_html, height=600)

Now your users can pan, zoom, reset, and explore the details of your chart! 📊

Want to explore it yourself? See the app deployed live here.

For more mpld3’s plugins, check out mpld3’s website and documentation.

2. Advanced example
Step 1. Render the graph statically

Start out by statically rendering the graph:

# Imports for all of the code
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import mpld3
import streamlit as st
from mpld3 import plugins

def f(t):
    return np.exp(-t) * np.cos(2*np.pi*t)

t1 = np.arange(0.0, 5.0, 0.1)
t2 = np.arange(0.0, 5.0, 0.02)

# How to set the graph size 
two_subplot_fig = plt.figure(figsize=(6,6))
plt.subplot(211)
plt.plot(t1, f(t1), color='tab:blue', marker=',')
plt.plot(t2, f(t2), color='black', marker='.')

plt.subplot(212)
plt.plot(t2, np.cos(2*np.pi*t2), color='tab:orange', linestyle='--', marker='.')

st.pyplot(two_subplot_fig)


This code will make something like this:

You might be thinking, “Why are we adding markers? It doesn’t look beautiful.” I’ll explain why below!

Step 2. Make the graph interactive with mpld3

Make the static graph interactive with mpld3:


# Replace st.pyplot(two_subplot_fig) with this code below! 
fig_html = mpld3.fig_to_html(two_subplot_fig)
components.html(fig_html, height=600)



Here is what it’ll look like with panning, zooming, and resetting:

Step 3. Add tooltips for even more interactivity

Add tooltips to see X and Y coordinates for even more interactivity (it’s why we’ve added markers).

Here is what the code will look like:

# CODE TO ADD
# Define some CSS to control our custom labels
css = """
table
{
  border-collapse: collapse;
}
th
{
  color: #ffffff;
  background-color: #000000;
}
td
{
  background-color: #cccccc;
}
table, th, td
{
  font-family:Arial, Helvetica, sans-serif;
  border: 1px solid black;
  text-align: right;
}
"""
for axes in two_subplot_fig.axes:
    for line in axes.get_lines():
        # get the x and y coords
        xy_data = line.get_xydata()
        labels = []
        for x, y in xy_data:
            # Create a label for each point with the x and y coords
            html_label = f'<table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> </thead> <tbody> <tr> <th>x</th> <td>{x}</td> </tr> <tr> <th>y</th> <td>{y}</td> </tr> </tbody> </table>'
            labels.append(html_label)
        # Create the tooltip with the labels (x and y coords) and attach it to each line with the css specified
        tooltip = plugins.PointHTMLTooltip(line, labels, css=css)
        # Since this is a separate plugin, you have to connect it
        plugins.connect(two_subplot_fig, tooltip)

You can adjust HTML, CSS, or anything you want. And if you want to interact with the graph or look at the code, check it out here!

Note: mpld3 limitations

Before I wrap this up, I wanted to note the limitations to mpld3:

Complex charts sometimes don’t render properly.
Dark mode isn’t supported.
3D charts don’t render properly.
You need markers for tooltips.
Some markers don’t work (examples: none or ‘+’).
Wrapping up

Thank you for reading this post! I’d love to know if you found this useful. Your feedback means a LOT. If you have any questions, please leave them in the comments below and check out the forum to see what our vibrant community is creating.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

python-chile-app.gif (1276×798)
https://blog.streamlit.io/content/images/2023/03/python-chile-app.gif#browser


python-talks-blocks.png (1920×987)
https://blog.streamlit.io/content/images/2023/03/python-talks-blocks.png#browser


python-talks-search-engine.png (1920×483)
https://blog.streamlit.io/content/images/2023/03/python-talks-search-engine.png#browser


python-talks-app.png (1920×771)
https://blog.streamlit.io/content/images/2023/03/python-talks-app.png#browser


Using ChatGPT to build a Kedro ML pipeline
https://blog.streamlit.io/using-chatgpt-to-build-a-kedro-ml-pipeline/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

By Arvindra Sehmi
Posted in LLMs, February 9 2023
A guided chat in 25 questions
Question 1
Question 2
Question 3
Question 4
Question 5
Question 6
Question 7
Question 8
Question 9
Question 10
Question 11
Question 12
Question 13
Question 14
Question 15
Question 16
Question 17
Question 18
Question 19
Question 20
Question 21
Question 22
Question 23
Question 24
Question 25
Wrapping up
Contents
Share this post
← All posts

Hi, community! 👋

My name is Arvindra Sehmi. I've been a life-long learner with too many degrees and far too many years of research and work in academia and the tech industry - 35 years and counting! I’m on a career break (advising Auth0.com, Macrometa.com, Tangle.io, Crowdsense.ai, and DNX ventures) and am taking the opportunity to learn new software development tools and enjoy being an active member of the Streamlit Creators group.

I recently came across an open-source Python DevOps framework Kedro and thought, “Why not have ChatGPT teach me how to use it to build some ML/DevOps automation?” The idea was to write my questions with hints that encouraged explanations of advanced Kedro features (to evolve incrementally as if a teacher taught me).

I planned to ask ChatGPT how to:

Use basic Kedro.
Use more advanced features in the Kedro framework.
Display pipeline graphs in Streamlit.
Build an example ML model and explicitly refer to it in the Kedro pipeline.
Scale the pipeline and perform pipeline logging, monitoring, and error handling.
Connect Kedro logs to a cloud-based logging service.
Contrast Kedro with similar (competing) products and services and show me how the earlier-developed pipeline could be implemented in one of them.

What follows are the annotated responses. I was super impressed. Will you be? (This is a long post, so if you want, dive right into the app and the repo.)

⛔
The generated Kedro code was simplistic and in some cases wrong, but perfectly okay to work with. My app is original, with small parts of it taken from Kedro's code template, so it can be used without any recourse under my MIT license. As to ChatGPT, though it's powerful, be aware of its potential limitations (see here and here). 
A guided chat in 25 questions

Try replicating my Q&A and see if you get better results, as ChatGPT should be getting smarter from millions of conversations. My opening prompt mentioned the technology I’m interested in learning about.

Question 1

Can you show me how to build a Kedro application to ingest data, train an ML model, evaluate the model, and visualize the model evaluation metrics?

Certainly! Here's an example of how you can build a Kedro application to ingest data, train a machine learning model, evaluate the model, and visualize the model evaluation metrics:

1. Ingest the data:

First, you'll need to ingest the data that you want to use for training and evaluating your model. You can use the DataCatalog to load your data from a variety of sources, such as a CSV file, a database, or a cloud storage bucket.

For example, you can use the load method of the DataCatalog to load data from a CSV file and save it to a data node in your Kedro pipeline:

from kedro.io import DataCatalog

# Create a DataCatalog to manage the data for your project
catalog = DataCatalog()

# Load the data from a CSV file and save it to a data node
data = catalog.load("my_data", "./data/my_data.csv")


2. Train the model:

Next, you'll need to define a function that trains your machine learning model. This function should take the training data as input, and should return the trained model.

Here's an example of how you can define a function to train a simple linear regression model:

from sklearn.linear_model import LinearRegression

def train_model(data):
	# Split the data into features and targets
	X = data.drop("target", axis=1)
	y = data["target"]
	# Create a LinearRegression model and fit it to the data
	model = LinearRegression()
	model.fit(X, y)
	return model


3. Evaluate the model:

After training the model, you'll need to evaluate its performance on a separate test dataset. You can use a variety of evaluation metrics, such as accuracy, precision, and recall, to measure the performance of your model.

Here's an example of how you can define a function to evaluate a model using the mean squared error metric:

from sklearn.metrics import mean_squared_error

def evaluate_model(model, data):
	# Split the data into features and targets
	X = data.drop("target", axis=1)
	y = data["target"]
	# Make predictions using the model and calculate the mean squared error
	y_pred = model.predict(X)
	mse = mean_squared_error(y, y_pred)
	return mse


4. Visualize the model evaluation metrics:

Finally, you can use a library like Matplotlib to visualize the model evaluation metrics. This can help you understand how well your model is performing and identify areas for improvement.

Here's an example of how you can use Matplotlib to visualize the mean squared error for a model:

import matplotlib.pyplot as plt

def plot_metric(metric_values):
	# Plot the metric values
	plt.plot(metric_values)
	plt.ylabel("Mean Squared Error")
	plt.xlabel("Iteration")
	plt.show()


I hope this helps! Let me know if you have any other questions.

Comment: That’s a great start. I know decent DevOps frameworks will provide a way to abstract and manage activities in a process. This is called orchestration, so let’s ask about that and implicitly refer to the steps described in the answer above.

🐛
Code comment: The subsequent implementation of the above data ingest code didn't work as intended. Kedro has a concept of datasets, and you must first create datasets from source data and then add them to the Kedro DataCatalog, which acts as a central data store for your application. Visualizing evaluation metrics as they are written can also be a problem when integrating with Streamlit because it conflates visualization with processing logic. Personally, I prefer to have these two concerns of any reasonably large application as separate logical layers. In my application, I save metrics results to a file (but it could be any persistence mechanism) that is independently accessed by my Streamlit presentation layer.
Question 2

Can you use Kedro to orchestrate this series of steps?

Yes, you can use Kedro to orchestrate the series of steps for ingesting data, training a machine learning model, evaluating the model, and visualizing the model evaluation metrics.

To do this, you can define your steps as Kedro nodes in a pipeline, and use the run method of the Pipeline to execute the pipeline.

Here's an example of how you can use Kedro to orchestrate the series of steps that I described in the previous message:

from kedro.pipeline import Pipeline, node

# Define the pipeline nodes
def train_model(data):
	# Train the model (as described in the previous message)
	return model
	
def evaluate_model(model, data):
	# Evaluate the model (as described in the previous message)
	return mse
	
def plot_metric(metric_values):
	# Plot the metric values (as described in the previous message)

# Create a pipeline to orchestrate the steps
pipeline = Pipeline([
	node(
		train_model,
		inputs="data",
		outputs="model",
	),
	node(
		evaluate_model,
		inputs=["model", "data"],
		outputs="mse",
	),
	node(
		plot_metric,
		inputs="mse",
	),
])
	
# Execute the pipeline
pipeline.run(catalog)


This will execute the pipeline, which will ingest the data, train the model, evaluate the model, and visualize the model evaluation metrics.

I hope this helps! Let me know if you have any other questions.

Comment: ChatGPT introduced the concept of a pipeline, which is nice. The steps are now encapsulated. An essential part of any ML experiment is tuning model parameters, so let’s ask to include a hyperparameter tuning step in the pipeline.

🐛
Code comment: It makes sense that a pipeline should be run as shown by ChatGPT above. However, it turns out that Kedro requires a pipeline runner to orchestrate pipeline node execution. There are several kinds of runners, and I chose to use the simplest SequentialRunner. This ChatGPT mistake appeared in several of it's subsequent responses, so please take note.

Since Kedro pipelines need to be command-line executable so that they can be included in CI (continuous integration) processes, pipelines and data catalogs need to be discoverable by Kedro's CLI (command-line interface) tool chain. To enable this, Kedro requires various externalized configurations and pipeline registration patterns to be defined (see here and here). It wasn't complicated to replicate these in my application from Kedro's example template projects.

So while ChatGPT gave me a head start, there was still some work to be done.
Question 3

Can you add hyperparameter tuning to the steps and the Kedro orchestration pipeline?

Yes, you can add hyperparameter tuning to the steps for training a machine learning model and the Kedro orchestration pipeline.

To do this, you can use a library like scikit-learn's GridSearchCV to search for the best hyperparameter values for your model. GridSearchCV performs an exhaustive search over a specified parameter grid, and returns the best set of hyperparameters based on cross-validated performance.

Here's an example of how you can use GridSearchCV to add hyperparameter tuning to the training step of your pipeline:

from sklearn.model_selection import GridSearchCV

def train_model(data, hyperparams):
	# Split the data into features and targets
	X = data.drop("target", axis=1)
	y = data["target"]
	# Create a LinearRegression model
	model = LinearRegression()
	# Create a parameter grid for the hyperparameters
	param_grid = {
		"fit_intercept": [True, False],
		"normalize": [True, False],
	}
	# Use GridSearchCV to search for the best hyperparameters
	grid_search = GridSearchCV(model, param_grid, cv=5)
	grid_search.fit(X, y)
	# Return the best model
	return grid_search.best_estimator_
	
# Define the pipeline nodes
pipeline = Pipeline([
	node(
		train_model,
		inputs="data",
		outputs="model",
		hyperparams=Hyperparameters(param_grid),
	),
	# ... other nodes ...
])


This will use GridSearchCV to search for the best values for the fit_intercept and normalize hyperparameters of the linear regression model, using 5-fold cross-validation.

You can then pass the best set of hyperparameters to the train_model function as a Hyperparameters object, and use them to train the model.

I hope this helps! Let me know if you have any other questions.

Comment: I like that. It’s what I do in my experiments. I secretly hope that by saying the answer is interesting, my chat session will remain live a little longer (as ChatGPT Preview is prone to terminating sessions without warning). After all I just started and human moderators might be snooping in on the conversation—I want to show them I’m serious. My LSTM ML model is implemented in Keras, and it happens to be a regressor too…a happy coincidence. My regressor has quite a few parameters that require tuning, so I’ll name them for ChatGPT and ask it to use GridSearchCV with those named parameters.

Question 4

Interesting! How can GridSearchCV be used to tune the parameters for a multilayer Tensorflow (Keras) LSTM network with parameters for the lookback, batch size, learning rate, epochs, optimizer, and loss function, and a number of nodes in each network layer, including the output layer?

You can use GridSearchCV to tune the hyperparameters for a multilayer TensorFlow (Keras) Long Short-Term Memory (LSTM) network by defining a custom KerasClassifier or KerasRegressor model, and passing it to GridSearchCV as the estimator.

Here's an example of how you can use GridSearchCV to tune the hyperparameters for a multilayer LSTM network for time series forecasting:

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

def create_model(lookback, batch_size, learning_rate, optimizer, loss, units):
	# Create a Sequential model
	model = Sequential()
	# Add an LSTM layer with the specified number of units
	model.add(LSTM(units, input_shape=(lookback, 1)))
	# Add a dense output layer
	model.add(Dense(1))
	# Compile the model with the specified optimizer and loss function
	model.compile(optimizer=optimizer, loss=loss, metrics=["mae"])
	return model

# Define the hyperparameter grid
param_grid = {
	"lookback": [10, 20, 30],
	"batch_size": [32, 64, 128],
	"learning_rate": [0.001, 0.01, 0.1],
	"optimizer": ["adam", "sgd"],
	"loss": ["mse", "mae"],
	"units": [32, 64, 128],
}

# Create the KerasClassifier model
model = KerasClassifier(create_model)

# Use GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X, y)

# Print the best set of hyperparameters
print(grid_search.best_params_)


This will use GridSearchCV to search for the best values for the lookback, batch_size, learning_rate, optimizer, loss, and units hyperparameters of the LSTM network, using 5-fold cross-validation.

You can then use the best set of hyperparameters to train the LSTM network and make forecasts.

I hope this helps! Let me know if you have any other questions.

Comment: Great, ChatGPT knows that LSTM regression models are used to create forecasts. However, it probably should’ve wrapped create_model in KerasRegressor, not KerasClassifier. That’s something I’ll check when I try to run the code. Now I want to steer my Q&A in the direction of model evaluation to gain a better understanding of cross-validation.

Question 5

What does cross-validation do?

Cross-validation is a technique for evaluating the performance of a machine learning model by training and testing the model on different subsets of the data. It helps to reduce the variance of the model's performance estimates and can give you a more accurate picture of how the model will perform on unseen data.

There are several different types of cross-validation, but the most common is k-fold cross-validation. In k-fold cross-validation, the data is split into k folds, and the model is trained and evaluated k times, with a different fold being used as the test set each time.

For example, in 5-fold cross-validation, the data is split into 5 folds, and the model is trained and evaluated 5 times, with a different fold being used as the test set each time. The model's performance is then averaged across the 5 folds to give a final performance estimate.

Cross-validation is useful for hyperparameter tuning because it allows you to evaluate the model's performance on different subsets of the data, which can help you identify the best set of hyperparameters for the model.

I hope this helps! Let me know if you have any other questions.

Comment: Let’s ask for more detail on evaluation metrics.

Question 6

What are the different ways of evaluating an LSTM, such as mae, confusion matrix, etc.? How do you implement or visualize these?

There are several different ways to evaluate the performance of a Long Short-Term Memory (LSTM) network, depending on the type of problem you are trying to solve. Some common evaluation metrics for LSTM networks include:

Mean Absolute Error (MAE): This metric measures the average magnitude of the errors in the predictions, without considering their direction. It is calculated as the sum of the absolute differences between the predicted values and the true values, divided by the number of predictions.
Mean Squared Error (MSE): This metric measures the average squared difference between the predicted values and the true values. It is calculated as the sum of the squared differences between the predicted values and the true values, divided by the number of predictions.
Confusion Matrix: This is a matrix that shows the number of correct and incorrect predictions made by the model for each class. It is often used for classification tasks, and can help you understand the types of errors that the model is making.

To implement or visualize these evaluation metrics, you can use a library like scikit-learn. Here's an example of how you can use scikit-learn to calculate and visualize the MAE and MSE for an LSTM network:

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Make predictions using the LSTM network
y_pred = model.predict(X_test)

# Calculate the MAE and MSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
# Print the results
print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")


To visualize the confusion matrix, you can use the confusion_matrix function from scikit-learn, and use a library like Matplotlib to plot the matrix:

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Make predictions using the LSTM network
y_pred = model.predict(X_test)

# Calculate the confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.imshow(confusion_mat, cmap="Blues")
plt.colorbar()
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
plt.show()


This will plot the confusion matrix as a color-coded matrix, with the predicted classes on the x-axis and the true classes on the y-axis. The color of each cell in the matrix represents the number of predictions made by the model for each class.

I hope this helps! Let me know if you have any other questions.

Comment: Oops! I made the mistake of mentioning the confusion matrix above, which made ChatGPT lose its context on regression models as confusion matrices don’t apply. I’m going to steer it back on track.

Question 7

What evaluation metrics and visualizations are used for Time Series Regression LSTMs?

For time series regression tasks, some common evaluation metrics and visualizations for Long Short-Term Memory (LSTM) networks include:

Mean Absolute Error (MAE): This metric measures the average magnitude of the errors in the predictions, without considering their direction. It is calculated as the sum of the absolute differences between the predicted values and the true values, divided by the number of predictions.
Mean Squared Error (MSE): This metric measures the average squared difference between the predicted values and the true values. It is calculated as the sum of the squared differences between the predicted values and the true values, divided by the number of predictions.
Root Mean Squared Error (RMSE): This metric is the square root of the MSE, and is often used to report the performance of a time series model. It is interpreted in the same units as the original time series data, which can make it easier to understand the magnitude of the errors.
Time Series Plot: A time series plot can help you visualize the predicted values and the true values for the time series. You can plot the predicted values and the true values on the same plot, and use the plot to identify trends, patterns, and anomalies in the data.

To calculate these evaluation metrics and create a time series plot, you can use a library like Matplotlib. Here's an example of how you can use Matplotlib to calculate and visualize the MAE, MSE, and RMSE for an LSTM network:

import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Make predictions using the LSTM network
y_pred = model.predict(X_test)

# Calculate the MAE, MSE, and RMSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print the results
print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"RMSE: {rmse:.3f}")

# Create a time series plot
plt.plot(y_test, label="True Values")
plt.plot(y_pred, label="Predicted Values")
plt.xlabel("Time")
plt.ylabel("Value")
plt.title("Time Series Prediction")
plt.legend()
plt.show()


This will calculate the MAE, MSE, and RMSE for the LSTM network, and plot the predicted values and the true values for the time series on the same plot.

I hope this helps! Let me know if you have any other questions.

Comment: Fab! When I build Streamlit apps, I never use Matplotlib for charting, so let’s ask ChatGPT to use Plotly Express instead for interactive charts.

Question 8

Can you show this using Plotly Express instead of Matplotlib?

Certainly! Here's an example of how you can use Plotly Express to calculate and visualize the MAE, MSE, and RMSE for an LSTM network:

import plotly.express as px
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Make predictions using the LSTM network
y_pred = model.predict(X_test)

# Calculate the MAE, MSE, and RMSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print the results
print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"RMSE: {rmse:.3f}")

# Create a time series plot
fig = px.scatter(x=range(len(y_test)), y=y_test, title="Time Series Prediction")
fig.add_scatter(x=range(len(y_pred)), y=y_pred, mode="markers", name="Predicted Values")
fig.show()


This will calculate the MAE, MSE, and RMSE

Comment: That looks promising. My time series forecasting LSTM is part of an algorithmic trading system that outputs trade entry and exit signals for a trading bot. It’d be nice to plot these signals on a chart. Let’s ask.

Question 9

I want to add "enter" and "exit" marks at certain times on a time series plot. These marks are derived from another data set. How do I do this using Plotly Express?

You can add "enter" and "exit" marks to a time series plot using Plotly Express by creating a separate scatter plot trace for the marks and adding it to the figure.

Here's an example of how you can do this:

import plotly.express as px

# Load the data
df = pd.read_csv("data.csv")

# Split the data into features and targets
X = df.drop("target", axis=1)
y = df["target"]

# Make predictions using the LSTM network
y_pred = model.predict(X)

# Create a time series plot
fig = px.scatter(x=range(len(y)), y=y, title="Time Series Prediction")
fig.add_scatter(x=range(len(y_pred)), y=y_pred, mode="markers", name="Predicted Values")

# Load the marks data
marks = pd.read_csv("marks.csv")

# Add a scatter plot trace for the marks
fig.add_scatter(x=marks["time"], y=marks["value"], mode="markers", name="Marks")
fig.show()


This will create a time series plot with the predicted values and the true values, and a separate scatter plot trace for the marks. The marks will be plotted as markers at the specified times on the x-axis.

I hope this helps!

Comment: That’s reasonable. In the past, I’ve done something similar using fig.add_trace and not fig.add_scatter, so I learned something new. However, ChatGPT didn’t quite nail the answer as marks.csv doesn’t distinguish between trade entry (buy) and trade exit (sell) marks. Let’s ask for some detail.

Question 10

What is the format of the marks.csv data file?

The format of the marks.csv data file will depend on the data that you want to include in the marks.

Here's an example of a simple marks.csv file that contains two columns:

time   	value
5	10
15	20
30	50

In this example, the time column contains the time (in some unit) at which the mark should be displayed on the x-axis, and the value column contains the value (in some unit) at which the mark should be displayed on the y-axis.

You can include additional columns in the marks.csv file if you want to include more data in the marks. For example, you might want to include a label column to specify a label for each mark, or a color column to specify a color for each mark.

I hope this helps! Let me know if you have any other questions.

Comment: Lovely! The answer elaborates on the basic file layout and suggests adding a label column to distinguish between different kinds of marks. I’ll coax it into showing me what it looks like.

Question 11

If I have a label column of "buy" and "sell" values in marks.csv, then how will the Plotly Express chart be written to show the marks in "green" and "red" respectively?

To show the marks in "green" and "red" based on the values in the label column of the marks.csv file, you can use the color parameter of the add_scatter function to specify a color for each mark.

Here's an example of how you can do this:

import plotly.express as px

# Load the data
df = pd.read_csv("data.csv")

# Split the data into features and targets
X = df.drop("target", axis=1)
y = df["target"]

# Make predictions using the LSTM network
y_pred = model.predict(X)

# Create a time series plot
fig = px.scatter(x=range(len(y)), y=y, title="Time Series Prediction")
fig.add_scatter(x=range(len(y_pred)), y=y_pred, mode="markers", name="Predicted Values")

# Load the marks data
marks = pd.read_csv("marks.csv")

# Add a scatter plot trace for the marks
fig.add_scatter(x=marks["time"], y=marks["value"], mode="markers", name="Marks", color=marks["label"])
fig.show()


This will create a scatter plot trace for the marks, with the color of each mark determined by the corresponding value in the label column. The color parameter can accept a list of colors, a list of color names (as strings), or a list of color codes (as strings).

I hope this helps! Let me know if you have any other questions.

Comment: I could’ve asked for more detail on how the color parameter uses a list of colors, but I wanted to get back to the Kedro pipeline and use it to tune multiple different models. My algo-trading system has one model per financial instrument, so I wanted to easily manage all of them. I asked ChatGPT to tell me what changes are required to the pipeline to achieve this.

Question 12

Going back to the Kedro pipeline...say, I have several LSTM models and want to tune each one and visualize their evaluation results. How does the pipeline change?

To build a Kedro pipeline that tunes multiple LSTM models and visualizes their evaluation results, you can follow these steps:

Create a function that takes in a set of hyperparameters and returns a trained LSTM model. This function should take care of preprocessing the data, training the model, and evaluating the model on a test set.
Create a list of dictionaries, where each dictionary contains a set of hyperparameters to be passed to the model training function.
Use the Pipeline class from Kedro to define a pipeline that loops through the list of hyperparameters, trains a model for each set of hyperparameters, and stores the trained models in a pipeline output.
Use the Visualizer class from Kedro to define a visualization function that takes in the trained models and generates evaluation plots for each model.
Add the visualization function as a final node in the pipeline.

Here's an example of how you can define the pipeline:

from kedro.pipeline import Pipeline, node

# Define the model training function
def train_model(hyperparameters, data):
	# Preprocess the data
	X_train, X_test, y_train, y_test = preprocess_data(data)
	# Create the LSTM model
	model = create_lstm_model(hyperparameters)
	# Train the model
	model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=hyperparameters["epochs"])
	# Evaluate the model
	evaluation = model.evaluate(X_test, y_test, return_dict=True)
	return model, evaluation
	
# Define the list of hyperparameters to try
hyperparameters_list = [
	{"lookback": 10, "batch_size": 32, "learning_rate": 0.1, "epochs": 10, "optimizer": "Adam", "loss":
	"mean_absolute_error"},
	{"lookback": 20, "batch_size": 64, "learning_rate": 0.01, "epochs": 20, "optimizer": "SGD", "loss":
	"mean_squared_error"},
	{"lookback": 5, "batch_size": 16, "learning_rate": 0.001, "epochs": 5, "optimizer": "RMSprop", "loss":
	"mean_absolute_percentage_error"},
]

# Define the pipeline
pipeline = Pipeline(
	[
		node(
			train_model,
			inputs=["hyperparameters", "data"],
			outputs=["model", "evaluation"],
			iterable=hyperparameters_list,
		),
		node(
			visualize_results,
			inputs=["model", "evaluation"],
			iterable=hyperparameters_list,
		),
	]
)


This pipeline defines a node that trains a model for each set of hyperparameters in the hyperparameters_list, and stores the trained models and their evaluation results in the pipeline output. The pipeline also defines a final node that visualizes the evaluation results for each model.

I hope this helps! Let me know if you have any other questions.

Comment: I think this gives me a good handle on how to manage multiple models. I can add additional parameters for each model, and the Pipeline and node objects warrant further investigation since the results stored in Pipeline and node can take hyperparameter iterables! Now, what would a visualize_results implementation look like?

Question 13

Great, so can visualize_results be implemented using similar Plotly Express examples you gave earlier, in the Kedro Visualizer class?

Yes, you can use Plotly Express to visualize the evaluation results for each LSTM model in the Visualizer class of Kedro.

Here's an example of how you can define the visualize_results function using Plotly Express:

import plotly.express as px

def visualize_results(model, evaluation):
	# Extract the evaluation metrics from the evaluation dictionary
	mae = evaluation["mae"]
	mse = evaluation["mse"]
	rmse = evaluation["rmse"]
	# Create a bar chart to visualize the evaluation metrics
	fig = px.bar(x=["MAE", "MSE", "RMSE"], y=[mae, mse, rmse])
	fig.show()


This function creates a bar chart that displays the MAE, MSE, and RMSE evaluation metrics for the model.

You can then use this function as a node in the Kedro pipeline, as shown in the previous example.

I hope this helps! Let me know if you have any other questions.

Comment: I saw a YouTube video on Kedro. The speaker showed a graphical representation of the Kedro pipeline. I was intrigued to learn how that was done.

Question 14

Amazing, thanks. How can I see a graphical representation of the Kedro pipeline?

To see a graphical representation of a Kedro pipeline, you can use the draw method of the Pipeline class.

Here's an example of how you can use the draw method to draw a graphical representation of the pipeline:

# Import the necessary modules
from kedro.pipeline import Pipeline
from kedro.io import KedroCliKernel

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml", KedroCliKernel())

# Draw the pipeline
pipeline.draw()


This will open a graphical representation of the pipeline in your default web browser. The graph will show the nodes of the pipeline as boxes, with the input and output nodes connected by arrows.

You can also use the show_graph method of the Pipeline class to draw the pipeline and save it to a file:

# Import the necessary modules
from kedro.pipeline import Pipeline
from kedro.io import KedroCliKernel

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml", KedroCliKernel())

# Draw the pipeline and save it to a file
pipeline.show_graph(format="png", filename="pipeline.png")


This will save the graphical representation of the pipeline to a file called pipeline.png in the current working directory.

I hope this helps! Let me know if you have any other questions.

Comment: I was right. This Pipeline object is very powerful. I wonder if it can be used to distribute model training jobs across several machines or CPU/GPU cores.

🐛
Code comment: There are no such pipeline APIs like draw() and show_graph(). Maybe ChatGPT was partially replaying someone's pipeline wrapper object with those interfaces, or maybe it was telling me about another framework's pipeline API. I will never know!

Kedro has a separate application "kedro viz" to visualize pipelines. This command is run in your application's root folder and it uses the project's configuration, data catalog, and pipeline registration conventions to start a web server that displays your pipeline graph in a browser window. Much like Streamlit does. My Streamlit application runs the "kedro viz" command in a sub-process and visualizes the output in Streamlit's component.html widget.
Question 15

Can the Pipeline class be used to distribute the jobs in each node across several machines or run them in parallel?

Yes, the Pipeline class in Kedro can be used to distribute the jobs in each node across several machines or run them in parallel.

To do this, you can use the Executor class from Kedro to create an executor that manages the parallel execution of the pipeline nodes. The Executor class supports several different execution modes, including:

SequentialExecutor: This executor runs the nodes of the pipeline sequentially, one after the other. This is the default executor.
ThreadedExecutor: This executor runs the nodes of the pipeline in separate threads, allowing them to run in parallel.
MultiprocessingExecutor: This executor runs the nodes of the pipeline in separate processes, allowing them to run in parallel across multiple CPU cores or machines.

To use one of these executors, you can pass it as an argument to the run method of the Pipeline class. For example, to run the pipeline using the ThreadedExecutor, you can do the following:

from kedro.pipeline import Pipeline
from kedro.executor import ThreadedExecutor

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Create the executor
executor = ThreadedExecutor()

# Run the pipeline using the executor
pipeline.run(executor=executor)


This will run the nodes of the pipeline in separate threads, allowing them to run in parallel.

I hope this helps! Let me know if you have any other questions.

Comment: Really? That’s awesome, and it looks so easy. I know from many years of experience with distributed systems that job instrumentation, logging, and failure management are critical. Does the pipeline support these capabilities too?

Question 16

Awesome! Does kedro.executor allow the jobs (log outputs) to be monitored and restarted if they fail?

Yes, the Executor class in Kedro allows you to monitor the jobs and restart them if they fail.

To do this, you can use the RetryingExecutor class, which wraps another executor and automatically retries failed jobs.

Here's an example of how you can use the RetryingExecutor to monitor and restart the jobs in a pipeline:

from kedro.pipeline import Pipeline
from kedro.executor import MultiprocessingExecutor, RetryingExecutor

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Create the executor
executor = MultiprocessingExecutor()

# Wrap the executor in a RetryingExecutor
retrying_executor = RetryingExecutor(executor, retries=3)

# Run the pipeline using the retrying executor
pipeline.run(executor=retrying_executor)


This will run the pipeline using the MultiprocessingExecutor, which will distribute the jobs across multiple CPU cores or machines. If a job fails, the RetryingExecutor will automatically retry it up to 3 times before giving up.

You can also specify a delay between retries by setting the delay parameter of the RetryingExecutor. For example, to specify a delay of 1 second between retries, you can do the following:

retrying_executor = RetryingExecutor(executor, retries=3, delay=1.0)


I hope this helps! Let me know if you have any other questions.

Comment: Wow, there is a RetryingExecutor out of the box. Nice. When it comes to logging, it’s best to have some log persistence, on top of which analytics and insights can be implemented. Let’s explore that line of thought a little. In particular, I want to know about log sinks.

Question 17

How is the output of executors redirected to files or custom log sinks?

To redirect the output of the executors to files or custom log sinks, you can use the LogSink class from Kedro.

The LogSink class allows you to specify a logging handler that will be used to redirect the output of the executors to a specific location. Kedro provides several built-in log sinks, including:

TeeSink: This log sink redirects the output to both the console and a file.
FileSink: This log sink redirects the output to a file.
NullSink: This log sink discards all output.

You can use these log sinks by passing them as an argument to the run method of the Pipeline class. For example, to redirect the output of the pipeline to a file called output.log, you can do the following:

from kedro.pipeline import Pipeline
from kedro.io import FileSink

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Create the log sink
sink = FileSink("output.log")

# Run the pipeline using the log sink
pipeline.run(log_sink=sink)


This will redirect the output of the pipeline to the file output.log.

You can also use a custom logging handler by subclassing the LogSink class and overriding the _setup_logging method. For example, to redirect the output to a custom log sink called CustomSink, you can do the following:

from kedro.pipeline import Pipeline
from kedro.io import LogSink
import logging

class CustomSink(LogSink):
	def _setup_logging(self):
		# Set up the custom log sink
		self.custom_logger = logging.getLogger("CustomLogger")
		self.custom_logger.setLevel(logging.INFO)
		# Add a handler to the custom logger
		handler = logging.StreamHandler()
		handler.setFormatter(logging.Formatter("%(message)s"))
		self.custom_logger.addHandler(handler)
		# Set the root logger to use the custom logger
		logging.root.handlers = [handler]
	
# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Create the log sink
sink = CustomSink()

# Run the pipeline using the log sink
pipeline.run(log_sink=sink)


This will redirect the output of the pipeline to the custom log sink CustomLogger. You can then use the custom_logger object to log messages to the custom sink.

I hope this helps! Let me know if you have any other questions.

Comment: At this point, I don’t know if I should be more impressed with Kedro or that ChatGPT “knows”. From what I’ve learned so far, I’m itching to get started with Kedro straight away, but I want to ask one more question.

🐛
Code comment: Unfortunately, the latest version of Kedro doesn't have these sinks out-of-the-box. ChatGPT was trained on pre-2021 web knowledge, so this information may be outdated or incorrect. Instead, Kedro uses configuration-driven logging which I haven't figured out yet.
Question 18

Can you make a custom sink that sends logs to Kibana?

Yes, you can use the ElasticsearchHandler from the elasticsearch-logger package to create a custom log sink that sends logs to Kibana.

Here's an example of how you can define the KibanaSink class to redirect the output of the executors to Kibana:

from kedro.pipeline import Pipeline
from kedro.io import LogSink
import logging
from elasticsearch_logger import ElasticsearchHandler

class KibanaSink(LogSink):
	def _setup_logging(self):
		# Set up the Elasticsearch handler
		self.handler = ElasticsearchHandler(
			hosts=[{"host": "localhost", "port": 9200}]
		)
		# Set the root logger to use the Elasticsearch handler
		logging.root.handlers = [self.handler]

# Load the pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Create the log sink
sink = KibanaSink()

# Run the pipeline using the log sink
pipeline.run(log_sink=sink)


This will redirect the output of the pipeline to Kibana, which can be used to visualize and analyze the logs.

You will need to have Elasticsearch and Kibana installed and running on your machine to use this sink. You can find more information on how to install and configure Elasticsearch and Kibana in the Elasticsearch documentation.

I hope this helps! Let me know if you have any other questions.

Comment: I know ChatGPT is cleverly replaying canned Kedro knowledge, but when it’s revealed elegantly like this, the conversation is very satisfying. Kedro appears to be very powerful and well-designed, making ChatGPT’s answers look trivial. I want to see a contrast between Kedro and alternatives.

🐛
Code comment: This looks like it could work, except for the reference to pipeline.run() again, and is worth investigating further.
Question 19

What are the alternatives to using Kedro?

There are several alternatives to Kedro that you can use for building and managing machine learning pipelines. Some of the popular alternatives include:

Airflow: Airflow is a platform to programmatically author, schedule, and monitor workflows. It was originally developed by Airbnb, and is now a popular open-source project. Airflow provides a flexible and powerful platform for building and managing ML pipelines, and has a large user base and community.
Kubeflow: Kubeflow is an open-source project that aims to make it easy to deploy and manage machine learning workflows on Kubernetes. It provides a range of tools and frameworks for building, deploying, and managing ML pipelines, including Jupyter notebooks, TensorFlow, and PyTorch.
Prefect: Prefect is an open-source platform for building, scheduling, and monitoring data pipelines. It is designed to be highly scalable and resilient, and provides a range of features for building and managing ML pipelines, including support for distributed execution and automatic retries.
Luigi: Luigi is an open-source Python library for building complex pipelines of batch jobs. It was developed by Spotify and is designed to be easy to use and flexible. Luigi provides a range of features for building and managing ML pipelines, including support for dependencies, parallel execution, and error handling.

These are just a few examples of the alternatives to Kedro that you can consider. Each platform has its own strengths and weaknesses, and the right choice will depend on your specific needs and requirements.

I hope this helps! Let me know if you have any other questions.

Comment: Let’s dig deeper into this contrast and see how the Kedro pipeline is done in another framework.

Question 20

How would you implement our Kedro pipeline in Airflow?

To implement the Kedro pipeline in Airflow, you can define a series of tasks and dependencies using the DAG (directed acyclic graph) and Operator classes in Airflow.

Here's an example of how you can define the Kedro pipeline in Airflow:

Here's the complete example of how you can define the Kedro pipeline in Airflow:

from airflow import DAG
from airflow.operators.python_operator import PythonOperator

# Define the default_args dictionary to specify the default parameters of the DAG
default_args = {
	"owner": "me",
	"start_date": "2022-01-01",
	"depends_on_past": False,
	"retries": 1,
	"retry_delay": timedelta(minutes=5),
}

# Create the DAG object
dag = DAG(
	"kedro_pipeline",
	default_args=default_args,
	description="Example Kedro pipeline in Airflow",
	schedule_interval=timedelta(hours=1),
	catchup=False,
)

# Define the data ingestion task
def ingest_data_task():
	# Call the data ingestion function
	module.ingest_data()
	ingest_data = PythonOperator(
		task_id="ingest_data",
		python_callable=ingest_data_task,
		dag=dag,
	)

# Define the preprocessing task
def preprocess_data_task():
	# Call the preprocessing function
	module.preprocess_data()
	preprocess_data = PythonOperator(
		task_id="preprocess_data",
		python_callable=preprocess_data_task,
		dag=dag,
	)

# Define the hyperparameter tuning task
def tune_model_task():
	# Call the hyperparameter tuning function
	module.tune_model()
	tune_model = PythonOperator(
		task_id="tune_model",
		python_callable=tune_model_task,
		dag=dag,
	)

# Define the model evaluation task
def evaluate_model_task():
# Call the model evaluation function
module.evaluate_model()
evaluate_model = PythonOperator(
	task_id="evaluate_model",
	python_callable=evaluate_model_task,
	dag=dag,
)

# Define the model visualization task
def visualize_results_task():
	# Call the model visualization function
	module.visualize_results()
	visualize_results = PythonOperator(
		task_id="visualize_results",
		python_callable=visualize_results_task,
		dag=dag,
	)

# Set the dependencies between tasks
ingest_data >> preprocess_data >> tune_model >> evaluate_model >> visualize_results


In this example, the Kedro pipeline is implemented as a series of tasks defined using the PythonOperator class in Airflow. The tasks are connected by dependencies using the >> operator in Airflow, which specifies dependencies between tasks in a DAG. It specifies that the task on the left-hand side of the operator must be completed before the task on the right-hand side can start.

For example, in the following code snippet:

ingest_data >> preprocess_data >> tune_model >> evaluate_model >> visualize_results


The >> operator specifies that the ingest_data task must be completed before the preprocess_data task can start, the preprocess_data task must be completed before the tune_model task can start, and so on.

This ensures that the tasks are run in the correct order, and that the necessary data and resources are available when each task is run.

I hope this helps! Let me know if you have any other questions.

Comment: That looks like a lot of imperative code. I’m not sure if Airflow has a declarative syntax like Kedro’s or any of Kedro’s other features. Still, I want to dig into this opaque “module” reference that’s in the Airflow code above.

Question 21

What is the variable "module"?

In the example code I provided, module is a placeholder for the Python module that contains the functions or methods that are called by the tasks in the Airflow DAG.

For example, in the following code snippet:

# Define the data ingestion task
def ingest_data_task():
	# Call the data ingestion function
	module.ingest_data()


The ingest_data_task function calls the ingest_data function, which is assumed to be defined in the module Python module.

You would need to replace module with the actual name of the Python module that contains the functions or methods you want to call. For example, if the ingest_data function is defined in a module called data_ingestion, you would replace module with data_ingestion.

I hope this helps! Let me know if you have any other questions.

Comment: Ah, so there’s more to it than initially implied. It pays to read the code ChatGPT produces and ask for clarification when you don’t understand.

Question 22

Can you provide me with a skeleton implementation for the Python "module"?

Certainly! Here's a skeleton implementation of the Python "module" that contains the functions or methods that are called by the tasks in the Airflow DAG:

# Import required libraries
import pandas as pd

def ingest_data():
	"""Ingest raw data from a file or database"""
	# Load the raw data
	raw_data = pd.read_csv("raw_data.csv")
	# Save the data to a file or database
	raw_data.to_csv("data.csv", index=False)

def preprocess_data():
	"""Preprocess the data"""
	# Load the data
	data = pd.read_csv("data.csv")
	# Perform preprocessing steps, such as cleaning and feature engineering
	processed_data = data.clean()
	# Save the processed data to a file or database
	processed_data.to_csv("processed_data.csv", index=False)

def tune_model():
	"""Tune the hyperparameters of the model"""
	# Load the processed data
	processed_data = pd.read_csv("processed_data.csv")
	# Create and tune the model
	model = Model(processed_data)
	model.tune_hyperparameters()
	# Save the tuned model to a file or database
	model.save("tuned_model.pkl")

def evaluate_model():
	"""Evaluate the model"""
	# Load the processed data and tuned model
	processed_data = pd.read_csv("processed_data.csv")
	model = Model.load("tuned_model.pkl")
	# Evaluate the model
	evaluation_metrics = model.evaluate(processed_data)
	# Save the evaluation metrics to a file or database
	pd.DataFrame(evaluation_metrics).to_csv("evaluation_metrics.csv", index=False)

def visualize_results():
	# TODO: Implement the model visualization logic here
	pass


In this example, the module contains five functions: ingest_data, preprocess_data, tune_model, evaluate_model, and visualize_results. These functions correspond to the tasks in the Airflow DAG.

You would need to replace the TODO comment(s) with the actual code for the function(s). The code should implement the logic for the corresponding task in the pipeline.

I hope this helps! Let me know if you have any other questions.

Comment: I’m comfortable displaying my LSTM model data and forecasts in tables and charts in Streamlit. But I’m not sure how to display a Kedro pipeline graph. So, let’s ask…

Question 23

How would you display the Kedro pipeline graph in Streamlit?

To display the Kedro pipeline graph in a Streamlit application, you can use the Pipeline.to_dot_graph method to generate a dot graph representation of the pipeline, and then to display a dot graph in a Streamlit app, you can use the pyvis.network.Network.from_dot method from the Pyvis library to generate a network object from the dot graph representation, and then use the st.pyplot function to display the network object in the Streamlit app.

Here's an example of how you can do this:

import streamlit as st
from kedro.pipeline import Pipeline
from pyvis import network as net

# Load the Kedro pipeline
pipeline = Pipeline.load("path/to/pipeline.yml")

# Generate the dot graph representation of the pipeline
dot_graph = pipeline.to_dot_graph()

# Generate a network object from the dot graph
graph = net.Network.from_dot(dot_graph)

# Display the network object in the Streamlit app
st.pyplot(graph.show)


This will display the Kedro pipeline graph in the Streamlit app, using the dot graph representation generated by the Pipeline.to_dot_graph method and the pyvis library.

You can customize the appearance of the graph by modifying the properties of the graph object. For example, you can use the set_edge_color method to change the color of the edges, or the set_node_color method to change the color of the nodes.

Comment: I’m not certain that st.pyplot can display dot_graph, so let’s regenerate the answer.

🐛
Code comment: Kedro's pipeline doesn't have a to_dot_graph() API! See my earlier code comment about "kedro viz".
Question 24

[Regenerate!]

To display the Kedro pipeline graph in a Streamlit application, you can use the st.graphviz_chart function to render the graph using Graphviz.

Here's an example of how you can do this:

import streamlit as st
from kedro.pipeline import Pipeline

# Create the Kedro pipeline
pipeline = Pipeline([
	# Add the pipeline nodes here
])

# Generate the Graphviz representation of the pipeline
pipeline_graph = pipeline.to_graph()

# Render the pipeline graph using st.graphviz_chart
st.graphviz_chart(pipeline_graph)


In this example, the Pipeline class is used to create a Kedro pipeline, and the to_graph method is used to generate a Graphviz representation of the pipeline. The st.graphviz_chart function is then used to render the graph in a Streamlit application.

Comment: I didn’t know about st.graphviz_chart. Let’s try another.

🐛
Code comment: Kedro's pipeline doesn't have a to_graph() API! See my earlier code comment about "kedro viz".
Question 25

[Regenerate!]

To display the Kedro pipeline graph in Streamlit, you can use the kedro.pipeline.Pipeline.graph method to generate the graph, and then use the st.graphviz_chart function to render the graph in a Streamlit app.

Here's an example of how you can do this:

import streamlit as st
from kedro.pipeline import Pipeline

# Load the Kedro pipeline
pipeline = Pipeline.load("/path/to/pipeline.yml")

# Generate the pipeline graph
graph = pipeline.graph

# Render the graph in a Streamlit app
st.graphviz_chart(graph)


In this example, the Pipeline.load method is used to load the Kedro pipeline from a pipeline.yml file. The pipeline.graph method is then used to generate the graph of the pipeline, and the st.graphviz_chart function is used to render the graph in a Streamlit app.

You can customize the appearance of the graph using the various options provided by the st.graphviz_chart function. For example, you can specify the font, color, and size of the nodes and edges in the graph.

I hope this helps! Let me know if you have any other questions.

Comment: By this point, I’m overwhelmed with ideas, but I’ve learned enough. Since ChatGPT saves previous conversations, I can come back to fill in the gaps or make corrections. Now I’m going to take the provided code and build the Kedro pipeline and the Streamlit front end. I’ll start with a simple LSTM model for the purposes of this post and build upon it for my real-world solution.

🐛
Code comment: If only it were true, I'd still be overwhelmed. Kedro's pipeline doesn't have a graph attribute! See my earlier code comment about "kedro viz".
Wrapping up

ChatGPT helped me develop a decent solution for deploying and managing my ML models using Kedro. I did this in less time and with less effort than I could’ve done without it. I learned a lot about Kedro and can now run an ML pipeline manually or orchestrated by the Kedro framework, and display a pipeline visualization.

I built the app over the weekend. I didn't have time to implement hyperparameter tuning or an LSTM model in my pipeline. I'll do that in the coming weeks, so if you find this useful, follow my repo and consider giving it a star! ⭐

In the future, as the field of "prompt engineering" matures, maybe chatbots using LLMs will be smart enough to clarify user intent. I believe for now you'll get better results if you write prompts with a clear context and goal in mind. Make a conversation plan to take ChatGPT through a series of small Q&A steps of increasing detail on your desired outcome. If you’re fortunate, you’ll tease out more features and capabilities in your answers. Finally, thoroughly check and test the code ChatGPT generates and use it only as a springboard for your own creativity.

If you want to create your own conversation plan and prompts, read How to write an effective GPT-3 prompt. And if you have any questions, contact me on LinkedIn or Twitter.

Happy chatting and Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Build an image background remover in Streamlit
https://blog.streamlit.io/build-an-image-background-remover-in-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build an image background remover in Streamlit

Skip the fees and do it for free! 🎈

By Tyler Simons
Posted in Tutorials, January 10 2023
How to set up your development environment
How to upload and download images
How to use the rembg package to remove backgrounds from images with Python
How to put it all together in a neatly formatted Streamlit app 🤯
Wrapping up 🎁
Contents
Share this post
← All posts

You’ve finally found it! 😮

After hours and hours of searching, you’ve found the best image of a wallaby for your class presentation about Australian wildlife. It’d look great next to your tips on staying safe in the rugged outback (bring lots of water💧).

But there’s one problem. It has a GREEN 🟩 BACKGROUND. Green totally doesn’t fit your vibe. So you start searching for web-based image processing services and discount them one by one:

$0.25 per image? Nope.
I need an account? No, thanks.
Won’t let me download my image at full resolution? Absolutely not.

“This will never work”, you think. “But wait! I know some Python and have the magical tools of Streamlit. Can I do this myself?”

Yep, 100%! You can build a high-quality image background remover yourself. In this tutorial, I’ll teach you:

How to set up your development environment
How to upload and download images
How to use the rembg package to remove backgrounds from images with Python
How to put it all together in a neatly formatted Streamlit app 🤯
💡
Want to skip straight to the app? Sure thing! Here’s the app and here’s the code on Github.
How to set up your development environment

In a new folder, make a virtual environment just for this project. Pull out your handy-dandy terminal and run the following:

# Set up our folder
cd ~/Documents
mkdir Streamlit-Image-Remover
cd Streamlit-Image-Remover

# Create and source our virtual environment
python3 -m venv venv
source venv/bin/activate


This will let you separate your local packages from the rest of your ecosystem—the standard best practice. Once it’s set up, install the following packages (to use everything available in Streamlit and the awesome Python library rembg):

pip install streamlit
pip install rembg


Next, download the source image, save it in the same folder as the app, and call it wallaby.png.

How to upload and download images

Streamlit makes uploading and downloading files easy. Just define the file types you’d like to accept in the arguments and give the functions some labels:

st.file_uploader() accepts a label and allowed file types.
st.download_button() accepts a label, a Python object, a file name, and a file type.

For your new folder Streamlit-Image-Remover/, open a code editor (VSCode or PyCharm), make a new file, and add the following:

import streamlit as st

st.title("Hello Upload!")

# Upload the image
image_upload = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])

# Now, you can download the same file!
if image_upload:
	st.download_button("Download your file here", image_upload, "my_image.png", "image/png")


Save the Python file as bg_remove.py and run the app to try it out:

streamlit run bg_remove.py


You did it!

Now upload the wallaby.png image and download it again if you want to try it out. 💥

How to use the rembg package to remove backgrounds from images with Python

You want your app to work in 3 steps:

A user uploads an image
The rembg package transforms it
The user downloads the transformed image

Steps 1 and 3 are done. For step 2, change the code to:

# We need a few more packages
from io import BytesIO
import streamlit as st
from PIL import Image
from rembg import remove

st.title("Hello Upload!")

# Upload the file
image_upload = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])

# Convert the image to BytesIO so we can download it
def convert_image(img):
    buf = BytesIO()
    img.save(buf, format="PNG")
    byte_im = buf.getvalue()
    return byte_im

# If we've uploaded an image, open it and remove the background!
if image_upload:
    image = Image.open(image_upload)
    fixed = remove(image)
    downloadable_image = convert_image(fixed)
    st.download_button(
        "Download fixed image", downloadable_image, "fixed.png", "image/png"
    )


Here is what you just did:

You imported three more packages:
io.BytesIO and PIL.Image to help with file types and memory management;
rembg.remove to remove the backgrounds;
You added a  convert_image() function to manage the file types for downloading.

That’s it! Upload your wallaby.png image and take a look at the app…

Download your image. The background has been removed like magic! 🪄

Awesome job. Your picture looks so good. 👍

How to put it all together in a neatly formatted Streamlit app 🤯

Wouldn’t it be nice to see the before-and-after image in the app? Streamlit makes it easy! Just add a few calls to st.image:

from io import BytesIO

import streamlit as st
from PIL import Image
from rembg import remove

st.title("Hello Upload!")

# Upload the file
image_upload = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])

# Convert the image to BytesIO so we can download it!
def convert_image(img):
    buf = BytesIO()
    img.save(buf, format="PNG")
    byte_im = buf.getvalue()
    return byte_im

# If we've uploaded an image, open it and remove the background!
if image_upload:
    # SHOW the uploaded image!
    st.image(image_upload)
    image = Image.open(image_upload)
    fixed = remove(image)
    downloadable_image = convert_image(fixed)
    # SHOW the improved image!
    st.image(downloadable_image)
    st.download_button(
        "Download fixed image", downloadable_image, "fixed.png", "image/png"
    )


You can see the images now, but it’s a bit clunky to scroll down.

What if they were next to each other in columns? Let’s move some components around so it looks more professional:

Put upload and download on st.sidebar.
Use st.columns to arrange the pre- and post-images side-by-side.
Use st.set_page_config to set a widescreen layout and st.write for commentary.
Add a default image to get a quick preview of the app every time you open it and save wallaby.png in the same folder.
from io import BytesIO

import streamlit as st
from PIL import Image
from rembg import remove

st.set_page_config(layout="wide", page_title="Image Background Remover")

st.write("## Remove background from your image")
st.write(
    ":dog: Try uploading an image to watch the background magically removed. Full quality images can be downloaded from the sidebar. This code is open source and available [here](<https://github.com/tyler-simons/BackgroundRemoval>) on GitHub. Special thanks to the [rembg library](<https://github.com/danielgatis/rembg>) :grin:"
)
st.sidebar.write("## Upload and download :gear:")

# Create the columns
col1, col2 = st.columns(2)

# Download the fixed image
def convert_image(img):
    buf = BytesIO()
    img.save(buf, format="PNG")
    byte_im = buf.getvalue()
    return byte_im

# Package the transform into a function
def fix_image(upload):
    image = Image.open(upload)
    col1.write("Original Image :camera:")
    col1.image(image)

    fixed = remove(image)
    col2.write("Fixed Image :wrench:")
    col2.image(fixed)
    st.sidebar.markdown("\
")
    st.sidebar.download_button(
        "Download fixed image", convert_image(fixed), "fixed.png", "image/png"
    )

# Create the file uploader
my_upload = st.sidebar.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])

# Fix the image!
if my_upload is not None:
    fix_image(upload=my_upload)
else:
    fix_image("./wallaby.png")


Here’s how it looks:

Your image is ready for your class presentation and your app is ready for showtime! Don’t forget to add your app to a GitHub repo (follow these steps), then post it on Twitter, and tag @streamlit. 🎈

Wrapping up 🎁

Congratulations! You can now remove backgrounds from images whenever you want to—for free. And if you want to make your app even better, you can add:

Support for videos and GIFs.
File type conversion (to pick the file output type).
Image sizing (how about making tiny images and turning them into emojis?).

That’s it. You’ve earned it. Grab some ice cream 🍦 and take the rest of the day off.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit-Authenticator, Part 1: Adding an authentication component to your app
https://blog.streamlit.io/streamlit-authenticator-part-1-adding-an-authentication-component-to-your-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit-Authenticator, Part 1: Adding an authentication component to your app

How to securely authenticate users into your Streamlit app

By Mohammad Khorasani
Posted in Advocate Posts, December 6 2022
How to install Streamlit-Authenticator
How to hash user passwords
How to create a login widget
How to authenticate users
How to implement user privileges
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Mohammad Khorasani, and I’m a co-author of the book Web Application Development with Streamlit (Streamlit helped me get into full-stack development, so it was only fair to spread the word about it).

As developers, we often require features that are yet to be made natively. For me, that was implementing user authentication and privileges in a client-related project that eventually grew into a full-fledged package aptly named Streamlit-Authenticator.

Specifically, my client asked for the ability to authenticate users with different privilege levels for their business needs, as well as a whole host of other features. That’s what prompted me into developing this package. While authentication is definitely needed for some apps—especially corporate ones—it's great to make your apps accessible to the community whenever possible to share and spread the learnings!

In this two-part tutorial, you’ll learn:

How to install Streamlit-Authenticator
How to hash user passwords
How to create a login widget
How to authenticate users
How to implement user privileges
👉
TL;DR? Here's the repo code.
How to install Streamlit-Authenticator

Let’s start by creating a login form to authenticate a list of predefined users. Install the component by using the following command:

pip install streamlit-authenticator


Next, import the component into your Python script:

import streamlit_authenticator as stauth

How to hash user passwords

It’s absolutely vital to hash any password that will be stored on a disk, database, or any other medium. Here, you’ll be defining your users’ credentials in a YAML file.

You’ll also define several other configuration settings pertaining to the key and expiry date of the re-authentication JWT cookie. If you don’t require passwordless re-authentication, just set the expiry_days to 0.

You can also define a preauthorized list of users who can register their usernames and passwords (I’ll cover this in the next post in this series).

Step 1. Create the YAML file:

credentials:
  usernames:
    jsmith:
      email: jsmith@gmail.com
      name: John Smith
      password: abc # To be replaced with hashed password
    rbriggs:
      email: rbriggs@gmail.com
      name: Rebecca Briggs
      password: def # To be replaced with hashed password
cookie:
  expiry_days: 30
  key: random_signature_key # Must be string
  name: random_cookie_name
preauthorized:
  emails:
  - melsby@gmail.com


Step 2. Use the Hasher module to convert your plain text passwords into hashed passwords:

hashed_passwords = stauth.Hasher(['abc', 'def']).generate()


Step 3. Replace the plain text passwords in the YAML file with the generated hashed passwords.

How to create a login widget

Now that you’ve defined your users’ credentials and configuration settings, you’re ready to create an authenticator object.

Step 1. Import the YAML file into your script:

import yaml
from yaml.loader import SafeLoader
with open('../config.yaml') as file:
    config = yaml.load(file, Loader=SafeLoader)

Step 2. Create the authenticator object:

authenticator = Authenticate(
    config['credentials'],
    config['cookie']['name'],
    config['cookie']['key'],
    config['cookie']['expiry_days'],
    config['preauthorized']
)


Step 3. Render the login widget by providing a name for the form and its location (i.e., sidebar or main):

name, authentication_status, username = authenticator.login('Login', 'main')

How to authenticate users

Once you have your authenticator object up and running, use the return values to read the name, authentication_status, and username of the authenticated user.

You can ppt-in for a logout button and add it as follows:

if authentication_status:
    authenticator.logout('Logout', 'main')
    st.write(f'Welcome *{name}*')
    st.title('Some content')
elif authentication_status == False:
    st.error('Username/password is incorrect')
elif authentication_status == None:
    st.warning('Please enter your username and password')


Or you can access the same values through a session state:

if st.session_state["authentication_status"]:
    authenticator.logout('Logout', 'main')
    st.write(f'Welcome *{st.session_state["name"]}*')
    st.title('Some content')
elif st.session_state["authentication_status"] == False:
    st.error('Username/password is incorrect')
elif st.session_state["authentication_status"] == None:
    st.warning('Please enter your username and password')


You can also alter the user if their credentials are incorrect:

After logging out, the authentication_status will revert back to None and the authentication cookie will be removed.

How to implement user privileges

Given that the authenticator object returns the username of your logged-in user, you can utilize that to implement user privileges where each user receives a more personalized experience as shown below:

name, authentication_status, username = authenticator.login('Login', 'main')
if authentication_status:
    authenticator.logout('Logout', 'main')
    if username == 'jsmith':
        st.write(f'Welcome *{name}*')
        st.title('Application 1')
    elif username == 'rbriggs':
        st.write(f'Welcome *{name}*')
        st.title('Application 2')
elif authentication_status == False:
    st.error('Username/password is incorrect')
elif authentication_status == None:
    st.warning('Please enter your username and password')


As you can see, each user is redirected to a separate application to cater to that specific user’s needs.

Wrapping up

I hope you feel confident now about securely authenticating users into your Streamlit application. In subsequent posts, you’ll learn how to register new users, reset usernames/passwords, and update user data.

In the meantime, feel free to read more about this component in our book Web Application Development with Streamlit. And if you have any questions, please leave them in the comments below or contact me on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Find the top songs from your high school years with a Streamlit app
https://blog.streamlit.io/find-the-top-songs-from-your-high-school-years-with-a-streamlit-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Find the top songs from your high school years with a Streamlit app

Use the Spotify API to generate 1,000+ playlists!

By Robert Ritz
Posted in Advocate Posts, December 8 2022
How to scrape Billboard Hot 100 top-ten singles
How to make 1000+ playlists with the Spotify API
How to build an app to present the playlists to the users
Wrapping up
Contents
Share this post
← All posts

Your headphones are on. Your favorite Spotify playlist is on shuffle. You like the music, but something is off. You keep hitting "next," searching for that special feeling. Finally, you start scrolling through your algorithmically created playlists. Spotify made them just for you, but they don't feel like you. They feel like a stereotype.

Sound familiar? You're not alone. Here is what I realized…

The recommended playlists feel off because they play the same songs!

I wanted the music to transport me to a unique time and place in my life—like high school. So I built a Streamlit app to help discover old favorites. Choose the years and get the Spotify playlist link with the top songs.

In this post, I'll show you:

How to scrape Billboard Hot 100 top-ten singles
How to make 1,000+ playlists with the Spotify API
How to build an app to present the playlists to the users

Ready to go? Let's get into it!

💡
Want to skip straight to the app? Here is the app and the code on Github.
How to scrape Billboard Hot 100 top-ten singles

To start, let's get some song data. Helpfully, Wikipedia lists Hot 100 singles for the years 1958-2022. Use Python (and pandas) to scrape the song lists from the corresponding Wikipedia pages and store them in a CSV file.

For example, here is the 1958 year page (with a song list HTML table):

The page URLs differ by year so that you can generate each URL with a bit of code:

# We will need these later so go ahead and import them now
import pandas as pd
import numpy as np

urls = []
for year in range(1958, 2023):
    urls.append(f"<https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_{year}>")


If you haven't already, install pandas, numpy, spotipy, and lxml by using PyPi (you can run it in your notebook or your preferred environment manager):

!pip install pandas numpy spotipy lxml


Next, write a loop to go to each URL, grab the HTML table with the pandas read_html method, and extract the table with the top songs.

Since every page has multiple HTML tables, I needed to figure out how to find the one with the song data. I settled on finding the largest table by the row count as it contained the most songs.

Here is the loop with the code to clean up the tables:

rows = []
for url, year in zip(urls, range(1958, 2023)):
    print(year)
    dfs = pd.read_html(url)

    row_num = []
    for df in dfs:
        row_num.append(df.shape[0])
    
    year_df = dfs[row_num.index(max(row_num))]
    year_df = year_df.iloc[:,:6]

    columns = ['entry_date','title','artist','peak','peak_date','weeks_top_ten']
    year_df.columns = columns

    year_df = year_df[~pd.to_numeric(year_df['peak'], errors='coerce').isna()].reset_index(drop=True)
    year_df['year'] = year
    rows.extend(year_df.to_dict(orient='records'))

df = pd.DataFrame(rows)


The resulting pandas DataFrame has over 5,000 songs and columns for the title, the artist, and the year:

Song names by themselves aren't useful. To make a Spotify playlist, you need to know the Spotify URI for the song. It's a unique identifier that Spotify uses for each track, album, or playlist. Let's use the Spotify API plus a very convenient package, Spotipy (get it? 😉), to look up the URIs.

But first, clean up the song titles (Spotify's API doesn't like certain characters) with some code:

to_replace = ["/", "\\\\", "(", ")", "'", ":", "."]

for item in to_replace:
    df['title'] = df['title'].str.replace(item,"", regex=False)


Now import Spotipy and make a loop to look up each song title from the search endpoint. As this endpoint doesn't require user authentication, you can use the SpotifyClientCredentials class to create a server-to-server authentication. Later you'll need to complete user authentication to create your playlists.

The loop below converts your pandas DataFrame to a list of dictionaries and loops through it. The loop queries the API using the track title and the artist name for each song list item. This returns multiple results. I naively chose to grab and save the first URI!

It's possible that a similar artist and song name could give the wrong result—like a remix or a re-release of the song. I tested this on about 20 songs, and it returned the correct result each time, so I decided this simple approach worked for the app.

After you grab the URI, it gets saved in the row dictionary. Once the loop is complete, save the list of dictionaries back to the pandas DataFrame:

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())

list_df = df.to_dict(orient='records')

for item in list_df:
    try:
        result = spotify.search(
            f"""track:{item['title']} artist:{item['artist']}""", type="track", limit=1
        )
        if len(result["tracks"]["items"]) > 0:
            item["spotify_uri"] = result["tracks"]["items"][0]["uri"]
        else:
            item["spotify_uri"] = np.nan
    except spotipy.client.SpotifyException as e:
        item["spotify_uri"] = str(e.http_status) + " - " + e.msg

df = pd.DataFrame(list_df)


To clean up the results, drop the rows with NaN values (empty cells) and the rows where the API has returned "Not found":

df = df.dropna()
df = df[~df['spotify_uri'].str.contains("Not found")]


Your songs are now ready to go with the included URI! Save the results in a CSV file:

df.to_csv('songs.csv', index=False)

How to make 1000+ playlists with the Spotify API

Before creating playlists, head to the Spotify Developer Dashboard and make an app. Take note of the client ID and the secret. Set them as environment variables so Spotipy can use them when authenticating:

import spotipy
from spotipy.oauth2 import SpotifyOAuth
import os

os.environ["SPOTIPY_CLIENT_ID"] = your client ID
os.environ["SPOTIPY_CLIENT_SECRET"] = your client secret
os.environ["SPOTIPY_REDIRECT_URI"] = <http://localhost:8080>


Next, authenticate with OAuth as a registered Spotify user (who you'll make playlists for) to access user data via the API:

Step 1. Get an authorization URL to sign in and receive a token:

# Sets up authentication and scope.
auth_manager = SpotifyOAuth(scope="playlist-modify-public", 
                            open_browser=False)

auth_manager.get_authorize_url()


Copy the link and paste it into the browser. After you sign in with your Spotify account, you'll be redirected to your local machine. In the URL bar, you'll see your authentication code:

Step 2. Copy the code and paste it in the "token here" spot in the code below:

auth_manager.get_access_token("token here", as_dict=False)


You're now authenticated and can interact with your Spotify account via the API. See if it worked by checking the current user:

spotify = spotipy.Spotify(auth_manager=auth_manager)

user_dict = spotify.current_user()
user_dict

# Output - If it worked you should see a dictionary like this.
{'display_name': 'Datafantic',
 'external_urls': {'spotify': '<https://open.spotify.com/user/31s4ct55ob6xjoghw4uxspvyu34u>'},
 'followers': {'href': None, 'total': 0},
 'href': '<https://api.spotify.com/v1/users/31s4ct55ob6xjoghw4uxspvyu34u>',
 'id': '31s4ct55ob6xjoghw4uxspvyu34u',
 'images': [],
 'type': 'user',
 'uri': 'spotify:user:31s4ct55ob6xjoghw4uxspvyu34u'}


See your account info? You're good to go. Save your created playlists in a CSV file, then make an empty pandas DataFrame and save it to playlists.csv (you'll add the column names later):

pd.DataFrame(columns=['id','name','link']).to_csv("playlists.csv", index=False)


To create playlists, I made some convenience functions (to simplify things and prevent a giant loop that's hard to debug):

The main function, make_playlist, takes the start and the end year, finds songs from those years, and creates the playlist.
Another function, check_playlists, checks if the playlist has already been made. I did this after accidentally making 20 playlists and then having to delete them one by one on the web.
Finally, save_playlists takes the created playlist and adds it to the playlists.csv file which looks up the playlists on the app:
def check_playlists(playlist_name, playlists_df):
    """Checks if playlist has already been created."""
    result = playlists_df[playlists_df['name'] == playlist_name].shape[0]
    if result > 0:
        return False
    else:
        return True

def save_playlists(playlist, playlist_name, playlists_df):
    """Saves playlist in a csv file."""
    new_playlist = {
        'name':playlist_name,
        'id':playlist['id'],
        'link':playlist['external_urls']['spotify']
    }

    playlists_df = pd.concat([playlists_df, pd.DataFrame([new_playlist])])
    playlists_df.to_csv("playlists.csv", index=False)

def make_playlist(start_year, end_year):
    """Makes the playlist and adds tracks from songs.csv given the start and end year"""
    playlists_df = pd.read_csv("playlists.csv")
    if (start_year - end_year) == 0:
        playlist_name = f"Top US Singles: {start_year}"
    else:
        playlist_name = f"Top US Singles: {start_year}-{end_year}"

    if check_playlists(playlist_name, playlists_df):
        description = 'This playlist was generated automatically for a project on Datafantic.com.'
        playlist = spotify.user_playlist_create(user=user_dict['id'], 
                                                name=playlist_name,
                                                description=description)
        uris = list(df[(df['year'] >= start_year) & (df['year'] <= end_year)]['spotify_uri'].values)

        chunked_uris = np.array_split(uris, math.ceil(len(uris) / 100))
        for uri_list in chunked_uris:
            spotify.user_playlist_add_tracks(user=user_dict['id'], 
                                            playlist_id=playlist['id'], 
                                            tracks=uri_list)

        save_playlists(playlist, playlist_name, playlists_df)


With these functions, you can create playlists for arbitrary start and end years.

I initially thought to allow users to generate these playlists on the fly as they interact with the app. But that would take too much work to debug and ensure it was working in the cloud. So I opted to batch-create playlists for a list of start/end years.

I generated a list of start/end years with a 1 to 20 years gap. Since we can't make playlists into the future, I slice the years' list minus the gap to ensure no playlists can be made past 2022:

years = list(range(1958, 2023))

year_pairs = []
for width in range(0,21):
    for year in years[:-width]:
        year_pairs.append((year, year+width))


Now you can use your make_playlist function above to generate your playlists:

for pair in year_pairs:
    make_playlist(pair[0], pair[1])


If you noticed, I left out single-year playlists (only 1958 or only 2022). The year_pairs list didn't include single years. Luckily there are convenience functions that you can loop through the list of years and make the playlists:

for year in years:
    make_playlist(year, year)


I was impressed with how fast the Spotify API performed. It took about two seconds to create each playlist and about 43 minutes to run the loop of over 1,000 playlists. You can multi-thread this to speed things up (and have enough time for a snack 😉).

Once complete, you'll see the CSV file with all your playlists. Visit the links and see your playlist live on Spotify:

Now let's move on to building an app!

How to build an app to present the playlists to the users

The Streamlit app is super simple:

Start by making a new Python file (call it streamlit_app.py or whatever you like). It'll define the structure and content of your app. You'll be adding the code step by step.

I like adding a title and some descriptive text at the top of apps to give the user some context. In this Python file, write the imports you need and use the st.markdown component to add text:

import pandas as pd
import streamlit as st

st.markdown("""# What songs were popular when I was in high school?
The algorithm doesn't get you, we get that a lot. Maybe you want to rediscover the top songs from your high school days. Or maybe you just don't want to mess with making your own playlist. 

You can use this tool to find a pre-generated playlist of every song that made the Top 10 in the US for the years you select. 

This originally appeared on [Datafantic.com](<https://www.datafantic.com/what-songs-were-popular-when-i-was-in-high-school>).
""")


Next, import your playlist.csv file by using Pandas with the read_csv method. Add a year slider for the user to define their playlist years. By passing a tuple to the value parameter (value=(1995, 2010)) you get two handles on the slider, allowing a minimum and a maximum year to be selected:

df = pd.read_csv("playlists.csv")
years = list(range(1958, 2022))

year_range = st.slider(label="Start Year", 
                       min_value=1958, 
                       max_value=2022, 
                       value=(1995, 2010))


Next, add a submit button. When clicked, it'll filter the DataFrame to find the playlist based on the slider values. The playlist link will be added to Markdown and displayed to the user. If a playlist isn't found, the user will see a simple error message:

if st.button('Submit'):
    if (int(year_range[0]) - int(year_range[1])) == 0:
        playlist_name = f"Top US Singles: {year_range[0]}"
    else:
        playlist_name = f"Top US Singles: {year_range[0]}-{year_range[1]}"

    if df[df['name'] == playlist_name].shape[0] > 0:
        playlist = df[df['name'] == playlist_name].to_dict(orient='records')[0]
    else:
        playlist = "Ooops, it looks like we didn't make that playlist yet. Playlists with a range of 1-20 years were created. Try again with a more narrow year range."

    if isinstance(playlist, dict):
        link = f"### Your Spotify Playlist: [{playlist['name']}]({playlist['link']})"
        st.markdown(link, unsafe_allow_html=True)
    else:
        st.markdown(playlist)

Wrapping up

I made this Streamlit app for my blog, datafantic.com. I analyze the top songs and how they have changed in this blog post. After publishing my app, I was surprised to get so much positive feedback.

Initially, I wanted to improve Spotify recommendations. Algorithmic song recommendations gave me a bad experience (I couldn't find what I wanted). Turns out, even simple curated playlists are immensely valuable to people. Algorithms stick us in a box—and not everyone likes that.

I might extend this app in the future to:

Let the user enter the location. Regional differences in song preferences exist, but you'd need data to do this.
Let the user sign in with Spotify to get a playlist based on their recent play history.
Give the user some context about how their current play history compares to the top charts by using their Spotify sign-in.

I hope you enjoyed this app and are inspired to build cool Streamlit apps to share with the world. Thanks for reading! 🙏

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Create a color palette from any image
https://blog.streamlit.io/create-a-color-palette-from-any-image/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Create a color palette from any image

Learn how to come up with the perfect colors for your data visualization

By Siavash Yasini
Posted in Advocate Posts, January 19 2023
How to construct an image-loading component
Gallery view
File uploader
URL downloader
How to build an image enhancement component with sliders
Show the image
How to cluster pixels and use group averages to make a palette
How to use the color picker widget
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Siavash Yasini, and I’m a Senior Data Scientist at Zest AI.

Like you, I love getting my hands on a new dataset, exploring it, and learning from it. But raw numbers by themselves aren’t great storytellers. Our primal brains are wired for lines, shapes, and colors. That’s why numbers need to be visualized to tell a good story.

The color palette of your data visualization is a crucial component that can make or break your data story. If you're like me, you've probably spent too much time trying to find the perfect set of colors. While creating a custom color palette can be exacting and time-consuming, you don’t have to do it alone. I built an app that can create a color palette from any image — a painting, a movie poster, or a Christmas family photo!

Sophisticated Palette app in action

In this post, I'll teach you:

How to construct an image-loading component
How to build an image enhancement component with sliders
How to cluster pixels and use group averages to make a palette
How to use the color picker widget to display and modify the palette

Want to try the app firsthand? You can check it out here and see the source code here.

How to construct an image-loading component

First, load an image into your app to convert it to a color palette. You can do this in three ways:

Load from a pre-existing gallery of images or artworks
Upload a new image file into the app using st.file_uploader()
Download a new image from a URL

You can make three different tabs using st.tabs() to switch between input modes:

Because of the way Streamlit runs the app (top-to-bottom), the input modes that come later will get higher priority and will overwrite the output of the previous loader. For example, since you ordered the input modes as Gallery → File Uploader → Image URL, if you save the image into a variable img, anything loaded by the gallery will be overwritten by the file uploader and URL downloader outputs.

You can add pop-up warnings for the user when they load an image with a loader lower in the hierarchy and an image is already loaded with a loader higher in the hierarchy.

This is what the code will look like:

# define three tabs for the three loading methods
gallery_tab, upload_tab, url_tab = st.tabs(["Gallery", "Upload", "Image URL"])

with gallery_tab:
    ...
		
	# raise a warning if file uploader or URL downloader have already loaded an image 
    if st.session_state.get("file_uploader"):
        st.warning("To use the Gallery, remove the uploaded image first.")
    if st.session_state.get("image_url"):
        st.warning("To use the Gallery, remove the image URL first.")

    img = ...

with upload_tab:

    img = ...
        
	# raise a warning if the URL downloader has already loaded an image
    if st.session_state.get("image_url"):
        st.warning("To use the file uploader, remove the image URL first.")

with url_tab:

	img = ...
        


So if you’re trying to load a pre-existing image from the gallery, but there is an existing link in the URL downloader, you’ll need to remove it. It may not be the most elegant solution, but it works!

Gallery view

For the gallery view, save some images in a public repository and load them directly in the app (use GitHub, AWS S3, or Google Cloud Storage). The st.selectbox contains the names of the saved artworks, so the user can load them by selecting them from the dropdown menu.

Here is what the implementation looks like:

import streamlit as st
from PIL import Image

with gallery_tab:
    options = list(gallery_dict.keys())
    file_name = st.selectbox("Select Art",
                             options=options, 
                             index=options.index("Mona Lisa (Leonardo da Vinci)")
                             )
    img_file = gallery_dict[file_name]

    if st.session_state.get("file_uploader"):
        st.warning("To use the Gallery, remove the uploaded image first.")
    if st.session_state.get("image_url"):
        st.warning("To use the Gallery, remove the image URL first.")

    img = Image.open(img_file)


gallery_dict is a dictionary with the filename and the image file path as key-value pairs, and PIL.Image.open() is used to load those files. The results are saved in a variable named img.

File uploader

Implementing the file uploader is easy because there is already a Streamlit widget. It’s called (can you guess?) st.file_uploader()!

Here is what the implementation looks like:

with upload_tab:
    img_file = st.file_uploader("Upload Art", key="file_uploader")
    if file is not None:
        try:
            img = Image.open(img_file)
        except:
            st.error("The file you uploaded does not seem to be a valid image. Try uploading a png or jpg file.")
    
    if st.session_state.get("image_url"):
        st.warning("To use the file uploader, remove the image URL first.")


This widget lets you upload a file that you can pass to PIL.Image.open() to load. This step may break if the file is not an image or has a format inconsistent with what PIL.Image expects. To prevent this, put the loading part into a try/except block.

⚠️
I’m using this block as a catch-all to avoid unexpected errors when loading a file. I don’t recommend you use except without specifying the Exception type you’re trying to bypass. It can cause deadly errors to pass through the block silently, making it difficult for you to debug your code.
URL downloader

Using the file uploader, the users have to find the image, download it locally, then upload it to the app. It sounds easy but is annoying in practice. To make this easy, add a URL downloader to the app so the user can copy the image link and paste it into the app. To do this, you need the requests module (gets you the URL contents) and the io.BytesIO function (makes the contents comprehendible by PIL.Image.open()).

The implementation is similar to what you did for the file uploader:

import requests
from io import BytesIO

with url_tab:  
    url = st.text_input("Image URL", key="image_url")
    
    if url != "":
        try:
            response = requests.get(url)
            img = Image.open(BytesIO(response.content))
        except:
            st.error("The URL does not seem to be valid.")

How to build an image enhancement component with sliders

Now that you have the image uploaded, you’re ready to infer the color palette, right? Not exactly.

The image may not be optimized for color inference. The colors might be too dull, and the image might not have enough brightness or contrast. That’s why you need to make some adjustments:

To do this, use PIL.ImageEnhance. The API is very simple. For example, if you want to enhance the color of the image by a factor of 2.5, you can run:

img = ImageEnhance.Color(img)
img = img.enhance(2.5)


Replace Color with Shapness, Contrast, or Brightness to adjust these image attributes respectively.

You could create four sliders that assign values to each of these attributes and write separate blocks of code. But your code won't be beautiful and DRY (Don’t Repeat Yourself). So let’s approach this methodically. Define a dictionary that contains all the enhancements as keys, with values indicating the ranges and step sizes of the sliders:

enhancement_range = {
  # "enhancement_type": [min, max, step_size]

    "Color": [0., 5., 0.2], 
    "Sharpness": [0., 3., 0.2], 
    "Contrast": [0.5, 1.5, 0.1], 
    "Brightness": [0.5, 1.5, 0.1]
}

enhancement_categories = enhancement_range.keys()

# put adjustment sliders inside an expander 
enh_expander = st.sidebar.expander("Image Enhancements", expanded=False)

# create a reset button that resets all enhancements to default value (1.0)
with enh_expander:
    if st.button("reset"):
        for cat in enhancement_categories:
            if f"{cat}_enhancement" in st.session_state:
                st.session_state[f"{cat}_enhancement"] = 1.0

# create sliders for each enhancement category using the dictionary values (min, max, step_size)
enhancement_factor_dict = {
    cat: enh_expander.slider(f"{cat} Enhancement", 
                            value=1., 
                            min_value=enhancement_range[cat][0], 
                            max_value=enhancement_range[cat][1], 
                            step=enhancement_range[cat][2],
                            key=f"{cat}_enhancement")
    for cat in enhancement_categories
}


This way, if you want to change the enhancement type or the range of values, you need to change only the original dictionary.

Let’s apply the values to the image using ImageEnhance:

from PIL import ImageEnhance

for cat in enhancement_categories:
	# apply the enhancement class to the image
	# e.g. for cat='Color' this would be the same as 
    # img = ImageEnhance.Color(img)
    img = getattr(ImageEnhance, cat)(img)
		
	# apply the enhencement value from the corresponding st.slider
    img = img.enhance(enhancement_factor_dict[cat])


Show the image

The only thing left is to show the image on the app using st.image():

with st.expander("🖼  Artwork", expanded=True):
    st.image(img, use_column_width=True)


And voilà!

How to cluster pixels and use group averages to make a palette

Now, onto the fun stuff. The idea here is simple. An image is a collection of pixels that have three values assigned to them: red (R), green (G), and blue (B). The pixel’s location on the canvas is irrelevant. What matters is where it’s located within the RGB coordinate space.

Let’s decompose the image and get rid of the pixel location:

r, g, b = np.array(img).reshape(-1, 3).T
df_rgb = pd.DataFrame({"R": r, "G": g, "B": b}).sample(n=sample_size)



Group the pixels that are close together and use the average RGB values to represent each group (a single color). For example, if you want to make a five-color palette from Mona Lisa, look at the pixel distribution (here it’s projected into 2D using the PCA algorithm):

Then select five clusters and assign the average value of each cluster to a palette slot:

To do this, use a machine learning algorithm K-means clustering—sklearn.cluster.KMeans. You need to provide only the number of clusters (your palette size).

This is what the implementation looks like:

from sklearn.cluster import KMeans

palette_size = st.sidebar.number_input("palette size", 
																			 min_value=1, 
																			 max_value=20, 
																		   value=5, 
																			 step=1, 
																			 help="Number of colors to infer from the image.")

model = KMeans(n_clusters=palette_size)
clusters = model.fit_predict(df_rgb)
        
palette = model.cluster_centers_.astype(int).tolist()


That’s it! You now have your color palette.

ℹ️
You used the popular RGB pixel decomposition, but it’s not the only way to decompose colors. There is also the clustering of pixels in the Hue, Saturation, and Value (HSV) space. It distributes the pixels differently and leads to a different color palette.
How to use the color picker widget

It's time to try the amazing st.color_picker() widget! Use it to adjust your colors if they're not 100% perfect:

To avoid the palette taking up half the page, put each color into a separate column:

columns = st.columns(palette_size)
for i, col in enumerate(columns):
    with col:        
        st.session_state[f"col_{i}"]= \\
				    st.color_picker(label=str(i), 
                    value=palette[i], 
                    key=f"pal_{i}")


Beautiful!

Finally, provide the user with a matplotlib or plotly code snippet (so they don't have to copy-paste every single hex code into their coding environment):

Thanks to st.code() widget, you can copy the whole code block with one click. 😄

Wrapping up

Congratulations! You've learned how to build an app that can show you what colors Leonardo Da Vinci used to put that smile on Mona Lisa's face. You've also learned how to use st.image, st.tabs, st.file_uploader, and st.color_picker widgets. If you want to learn even more, check out the source code.

I'd love to hear your thoughts, questions, comments, and feedback. Get in touch with me on LinkedIn or through my website.

Happy app-building! 👷

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Assistant_grid_scaling-1.0_fps-35_speed-10.0_duration-0-7.gif (760×338)
https://blog.streamlit.io/content/images/2023/03/Assistant_grid_scaling-1.0_fps-35_speed-10.0_duration-0-7.gif#browser


Visualize Object Detection Output | Use Roboflow & Streamlit
https://blog.streamlit.io/how-to-use-roboflow-and-streamlit-to-visualize-object-detection-output/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to use Roboflow and Streamlit to visualize object detection output

Building an app for blood cell count detection

By Matt Brems
Posted in Advocate Posts, February 23 2021
How to fit an object detection model in Roboflow
Now you’re ready to build a model
How to use an API to access the model and predictions
How to create and deploy a Streamlit app
Contents
Share this post
← All posts

Most technology is designed to make your life, or your work, easier. If your work involves building computer vision into your applications, using the Roboflow platform gives you everything you need.

Streamlit is an open-source platform that enables you to convert your Python scripts to apps and deploy them instantly. Streamlit and Roboflow can work hand-in-hand, allowing you to tackle computer vision problems and visualizing your output so you can make better decisions faster.

In this post, we’ll walk you through using Roboflow and Streamlit together by showing you how to:

Fit an object detection model in Roboflow
Use an API to access the model and its predictions
Create and deploy a Streamlit app

Specifically, we’ll be working with a common blood cell count and detection dataset. If you want to skip right to playing with it, here's an interactive app and this is the code.

We’ll build an object detection model that detects platelets, white blood cells, and red blood cells. Then, the app we develop together will allow you to make predictions with your object detection model, visualize those predictions at a given confidence level, and edit those predictions based on your preferred confidence level with immediate visual feedback.

How to fit an object detection model in Roboflow

Have you fit an object detection model before?

Even if you haven't, Roboflow helps you work through all aspects of computer vision, from uploading, annotating, and organizing your images to training and deploying a computer vision model.

We believe you shouldn’t have to be a data scientist or need an extensive coding background to be able to use computer vision. You have everything you need right now.

The computer vision workflow.


If you don’t already have a Roboflow account, you’ll need to head over to Roboflow and create one. If you’d like to start training your model from a public dataset, Roboflow has a great tutorial that describes how to improve your model more quickly. (Or, you can upload your own dataset!)

Once you have an account, go to our computer vision datasets page. We’ve made over 30 datasets of different types public and keep adding more.

The one we’ll walk through today is a blood cell count and detection dataset.

After you’ve decided which dataset to use, go ahead and fork it. That will create a copy of the dataset that you can now use.

At this point, you can directly fit a model. However, we recommend that you preprocess and augment your images.

Image preprocessing. Deterministic steps performed to all images prior to feeding them into the model. For example, you might resize your images so they are all the same size, or convert your images to grayscale.
Image augmentation. Creating more training examples by distorting your input images so your model doesn't overfit on specific training examples. For example, you may flip, rotate, blur, or add noise to your images. The goal is to get your model to generalize better to “the real world” when you deploy your model.

With the blood cell count dataset I’m using, I chose the following preprocessing and augmentation options:

When deciding whether to use a specific augmentation option, I asked myself the question “Is the augmented image a reasonable image for my model to see?” In this case, I added 90°, 180°, and 270° rotations to my image because the slide of cells could reasonably be rotated 90 degrees and still make sense.

It wouldn't make sense for all applications. For instance, I might not include that kind of rotation for a self-driving car, because stop signs should be seen with the pole jutting into the ground. To rotate the image 180 degrees would make the stop sign upside down and the ground where the sky should be -- that probably isn’t a very useful thing for my model to learn.

I have my data split up so that 70% of my data is in the training set, 20% is in the validation set, and 10% is in the test set. As you may know, splitting your data into training, validation, and testing sets can really help avoid overfitting.

I’ve decided to create three augmentations. This means that, for each training image, we’ll create three copies of that image, each with random augmentation techniques applied to it. This will give me a total of 874 images that are generated:

765 augmented training images (765 = 255 * 3)
plus 73 validation images
plus 36 testing images.

Once you’re done with your preprocessing and augmentation, click “Generate” in the top-right corner. Helpful hint: make sure to name your dataset something memorable!

Now you’re ready to build a model

To build a model, it’s as easy as clicking “Use Roboflow Train.”

Generally, you need a Roboflow Train credit to do this. Reach out to us and we’ll get you set up!

You’ll have the option either to train from scratch or to start from a checkpoint.

Train from Scratch. This is the easier option. Just click and go! Your model will be built from scratch, using only the data you’ve provided to it.
Start from a Checkpoint. This option is a little more sophisticated and requires a related existing model. If you’ve already built a model (or if there’s a public model) that has been fit on related data, then starting from a checkpoint allows you to use the existing model as your starting point. The model is additionally trained on your images. Two advantages to this are that your model will train more quickly, and you'll frequently see improved performance! This is known as transfer learning. However, this does require a related existing model, and we don't always have that.

In my case, I built my model from scratch, because I didn’t already have a related model.

That’s all it takes to fit a model in Roboflow. When all is said and done, if your data is already annotated and you don’t make many changes to the augmentations, it’s only a handful of clicks to go from your images to a trained computer vision model. We've also turned annotating images into a pretty fast process – especially with model-assisted labeling.

How to use an API to access the model and predictions

You’ll want to make sure your model performs well before getting too far into this.

Our model seems to perform pretty well. Usually, we use mean average precision (mAP) to evaluate object detection models. The closer your mAP is to 100%, the better! It’s also helpful to look at your model’s performance by class to make sure your object detection model isn’t performing significantly worse for one subset of objects.

If your model isn’t performing the way you want, you may want to work on improving that before you proceed. We usually see dramatic improvements in models when people take one (or both) of the following two actions:

Improve their labeling. Placing bounding boxes around the entire object, but as close to the edges of the object as possible, can improve your model’s performance.
Correcting for unbalanced classes. Having one or more classes that are severely underrepresented can make it harder for your model to properly identify those underrepresented classes. A basic example is if you show a child 5 pictures of a dog and 100 pictures of a cat, the child may not do a very good job of identifying a dog.

Now that we’ve fit a model, we can use that model to generate predictions on new images. The Roboflow Infer API is one of a few ways to conduct inference and that's what we’ll use.

In order to use the API, we’ll need a couple of pieces of information from Roboflow. Make sure you keep these both private. These are specific to you!

The model name: this should begin with rf.
The access token/API key: this should be a 12+ letter code.

This information can be found in multiple places. I like retrieving these from the Example Web App, because I’ll also easily upload an image and test out my model from there. Once you have these pieces of information, you’ll want to store them – you’ll need them momentarily.

How to create and deploy a Streamlit app

Deploying a Streamlit app is easy. Even if you haven’t spent a lot of time focused on deploying apps before. (Here is the code I wrote to build the app.)

Following Streamlit’s API documentation closely, I was able to build an app that:

Imported an image file from my computer
Allowed the user to tweak parameters of our computer vision model
Showed my imported image overlaid with the model’s predicted annotations
Calculated and displayed summary statistics about the image and predictions
Generated a histogram of confidence levels for bounding boxes

I chose to structure this in two physical components: a sidebar and the main area.

Sidebar. In the sidebar, the user gets to select a file to import from their local computer. This is where the user can select an image to pull into the app and edit the confidence and overlap thresholds used when generating predicted bounding boxes for the image.


Main Area. In the main area, we have everything else I mentioned. The image that includes predictions, some statistics about the image and predictions itself, a histogram that shows the confidence levels for all bounding boxes, and a printout of the JSON that stores the bounding box annotations.

The three tools that were most helpful in putting this together were:

st.write(): If I wanted to print anything on my screen, st.write() enabled me to do that easily. It supports Markdown, so I can use ## to control how large or small I want my headings to be. I also used f-strings when displaying summary statistics to have more control over how these rendered. For example, rounding off the mean confidence level after four decimal places instead of a long string of trailing decimals.
st.sidebar(): I’m decidedly not a web developer. Rather than spending my time figuring out how to set aside the left side of the screen for the user and wrangling dimensions, defining a sidebar was literally as easy as st.sidebar(). Want to add something to the sidebar, like a slider or a way to upload your files? Try st.sidebar.slider() or st.sidebar.file_uploader(). The Streamlit API is set up so that your components stay where you want them.
st.image() and st.pyplot(): Streamlit’s API is intuitive. If you want to insert an image into your app, you can do that with the st.image() function. Plot from pyplot? st.pyplot(). If you wanted to put an image into the sidebar, you’d change it from st.image() to st.sidebar.image().

You get the point. If you want to do something, you can probably just type st.that_thing_you_want_to_do()! If you want that thing to be in your sidebar, change it to st.sidebar.that_thing_you_want_to_do()!

After writing my Python script and pushing to Github, I followed Streamlit’s instructions to deploy my app -- check the app out here!

Want to learn more about some of the amazing apps developers are making with Streamlit? Check out their app gallery and community forum to find some inspiration for your next project.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

ai-talks-1.gif (725×1141)
https://blog.streamlit.io/content/images/2023/04/ai-talks-1.gif#border


Gravitational-Wave Apps Help Students Learn About Black Holes
https://blog.streamlit.io/gravitational-wave-apps-help-students-learn-about-black-holes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Gravitational-wave apps help students learn about black holes

Exploring distant space with gravitational waves

By Jonah Kanner
Posted in Advocate Posts, December 15 2020
Barriers to learning
Enter Streamlit
Working with data
How far can LIGO see?
Finding signals in the noise
Building the app
Data discovery and download
Data processing
Display the plots
The big picture
Contents
Share this post
← All posts

Written by Jonah Kanner and Jameson Rollins of LIGO Laboratory, California Institute of Technology and Leo Singer of NASA Goddard Space Flight Center.

Gravitational-wave detectors - like LIGO and Virgo - are some of the newest tools being used to explore objects in distant galaxies. The current generation of detectors began observing in 2015, and since then, have published observations of 50 collisions of black holes and neutron stars. These exciting discoveries are changing the way astrophysicists are learning about a broad range of topics - including how stars evolve, the expansion rate of the universe, and the deep laws that describe gravity and other fundamental forces.

LIGO and Virgo data are freely available through the Gravitational Wave Open Science Center (GWOSC). These public data sets are being used all over the world, by scientists, teachers, students, and artists, and have contributed to the publication of over 100 scientific articles over the past year.

Barriers to learning

There’s one challenge encountered by everyone getting started with gravitational-wave data for the first time:  most options for working with these data sets demand installing specialized software libraries and writing computer code to process and display the results. For professional scientists, this is often OK - a full-time researcher can afford to spend a few hours installing and learning new software, and likely has already had some experience programming. But for a much broader audience - including high school students, teachers, and artists - writing code in Python to just get started with gravitational-wave data is a significant barrier.

We’d experimented with a few different options to make this transition easier. In fact, cloud hosted Jupyter notebooks provided a big step forward, and we were able to show students how to write code to work with our data, without asking them to install any libraries on their own machines. But when we shared these notebooks with teachers and artists, they were not interested: they wanted some way to work with gravitational-wave data without using any code at all!

Moreover, it's not just new students who can benefit from an easier method to access LIGO/Virgo data. Even though professional scientists often could write specialized code to make the plot they want, it’s not efficient to have lots of scientists re-writing code to make the most popular plots.

Enter Streamlit

Knowing that we had a need for a web app for the most common plots - both to broaden access and to improve research efficiency - we were excited to discover Streamlit. Python is very popular in our research community, so Streamlit apps can easily make use of some of the most cutting-edge modules used for gravitational-wave research. For example, pycbc and gwpy are packages used by many gravitational-wave researchers to process and display LIGO/Virgo data. With Streamlit, we can easily write apps that let students and scientists use these packages, without writing any code themselves. Because GWOSC provides data access through a simple API, we found that Streamlit apps can access data on demand, and so process and display any segment of public data.

Getting started video from GWOSC Learning Paths - video created by Cardiff University Physics & Astronomy

Today, we’re using two Streamlit apps to introduce students to LIGO/Virgo data as part of the GWOSC Learning Paths, and a third app, the Gravitational Wave Inspiral Range Calculator, allows scientists and astronomers to easily calculate how far into the universe current and future detectors can see.

Working with data

To help students and scientists get started working with gravitational-wave data, we wrote the GW Quickview App to make some common plots with any stretch of data. Users can instantly make plots by selecting a published gravitational-wave event from a list, or by entering any time LIGO and Virgo were running. The Quickview App has access to the full public archive of LIGO/Virgo data - around 30 TB and growing! Sliders allow the user to set filter and plotting options, and even to directly download the filtered data. In most cases, you can see the signal from a black hole merger or neutron star collision with just a few clicks. This app is a great way to take a first peak at a segment of data and explore the archive.

How far can LIGO see?

One of the most important things to know about a gravitational-wave detector is how sensitive it is to the kinds of signals it’s capable of detecting. Ground-based gravitational-wave detectors primarily target the mergers of black holes or neutron stars, also known as compact binary coalescences (CBCs). Since the amplitude of a CBC signal is inversely proportional to how far away it is, the question is usually asked in terms of how far away can we detect the signal from a given type of CBC. In LIGO’s first observing run (O1) the LIGO detectors could detect binary neutron star (BNS) mergers out to a distance of roughly 70 megaparcecs (Mpc), or 230 million light-years. As the detectors have improved, the so-called “inspiral range” has increased as well; during the O3 run the LIGO BNS inspiral range was nearly 120 Mpc. Every time the range doubles the volume of space searched goes up by a factor of 8, and the event rate is proportional to volume. That’s a lot more events!

While plenty of tools exist to calculate the range for a given detector, all of them required special access and knowledge to run. With Streamlit, we were able to create a simple web app tool for anyone to calculate the inspiral range for any type of CBC they wish, for any point of the past observing runs, or even for hypothetical detectors that are yet to be built. This app should be useful for LIGO scientists, for the general astronomy community, and for the public at large.

Finding signals in the noise

Gravitational wave signals are typically buried in noise; finding them requires a few signal processing tricks. These signal processing concepts are new to many students, and can be a barrier to understanding data from LIGO and Virgo. Working with Professor Amber Stuver at Villanova, we created an app to let students try out some signal processing in an easy to use, interactive environment. The Signal Processing Tutorial asks visitors to apply high-pass filtering and whitening to noisy data, to find a secret sound hidden in the noise. Then, they can try their new skills on some real data, and try to recreate a well known plot showing the first gravitational-wave observation of a binary black hole merger. Following hints from a post by Pranav Pandit, we found we were able to speed up the app by porting plots from matplotlib to altair, so that students can adjust plot parameters and see updates almost instantly.

Building the app

We built the GW Quickview App partly as an experiment, to see how easy it would be to run some of our favorite python modules in Streamlit. In this case, we tried using gwpy, which has a number of handy methods for finding, processing, and plotting LIGO and Virgo data.

Data discovery and download

The app code uses the gwosc client for data discovery, to get a list of all published gravitational-wave events, and find their GPS times:


from gwosc import datasets
eventlist = datasets.find_datasets(type='events')
chosen_event = st.sidebar.selectbox('Select Event', eventlist)
t0 = datasets.event_gps(chosen_event)



Then, it uses gwpy to download the data file with the corresponding time:


from gwpy.timeseries import TimeSeries
strain = TimeSeries.fetch_open_data(detector, t0-14, t0+14, cache=False)




The strain data are stored in some pretty big files (up to 500 MB), so we used the cache feature of streamlit to allow users to change plot parameters without needing to repeat the data download.

Data processing

To make plots where you can see a signal in GW data, some signal processing steps are needed to reduce the impact of noise. The app uses the bandpass() and whiten() methods of gwpy to do this. Sliders allow the user to set the limits on the band-pass filter, and to toggle whitening on or off. The app also uses the gwpy q_transform method to make a beautiful time-frequency plot, which allows most detectable events to be clearly seen in the figure. Additional sliders allow setting plot parameters, so the user can fine-tune the time-frequency plot to look just right. We even used the new streamlit beta_expander() to allow the user to display hints about how to understand the plots and set the parameters.

Display the plots

Finally, the app displays several plots to visualize the data. We opted to use convenient methods in gwpy to make plots with matplotlib, which saved us from writing code to style and label the plots. That fact that Streamlit can easily display these plots with streamlit.pyplot() was a big plus for us, and the Streamlit team helped us fix some issues related to thread-safety when using this feature.

Though we know that Streamlit apps run faster when plotting with altair (we used this trick in another app!), for the GW Quickview App, we chose to keep the matplotlib plots, mainly to demonstrate the handy plotting features built into gwpy.

The big picture

A number of break-throughs in the past few years have proved that gravitational-wave detectors represent a new and exciting way to learn about our universe. To make use of this new technology, we’ll need to train the next generation of scientists who will build the instruments of the future and study their observations. After getting started with Streamlit apps and GWOSC Learning Paths, we hope some students will move on to use other tools, such as large scale computing clusters, to participate in research activities. We’ve had fun building apps that make gravitational-wave data easier to access, and we hope they will spark imagination and interest in this emerging field.

Special thanks to Amey Deshpande, Randy Zwitch, and the Streamlit team for improving the apps and editing this blog post


Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

database-1.png (1911×997)
https://blog.streamlit.io/content/images/2023/03/database-1.png#browser


streamlit-app-starter-kit-22Sep2022.gif (2156×1492)
https://blog.streamlit.io/content/images/2022/09/streamlit-app-starter-kit-22Sep2022.gif#browser


Avatar_divider_scaling-0.71_fps-60_speed-9.97_duration-4-7.gif (727×549)
https://blog.streamlit.io/content/images/2023/03/Avatar_divider_scaling-0.71_fps-60_speed-9.97_duration-4-7.gif#browser


Untitled--1-.png (668×102)
https://blog.streamlit.io/content/images/2023/03/Untitled--1-.png#border


Untitled.png (720×287)
https://blog.streamlit.io/content/images/2023/03/Untitled.png#border


assistant_no_context.jpg (787×421)
https://blog.streamlit.io/content/images/2023/03/assistant_no_context.jpg


assistant_bot_details.jpg (762×716)
https://blog.streamlit.io/content/images/2023/03/assistant_bot_details.jpg


assistant_chat_view.jpg (789×655)
https://blog.streamlit.io/content/images/2023/03/assistant_chat_view.jpg


assistant_login_required.jpg (756×529)
https://blog.streamlit.io/content/images/2023/03/assistant_login_required.jpg


8-slides-comparison.gif (1176×488)
https://blog.streamlit.io/content/images/2023/04/8-slides-comparison.gif#border


confusion-matrix.png (497×415)
https://blog.streamlit.io/content/images/2023/04/confusion-matrix.png#border


collecting-dataset.png (1140×275)
https://blog.streamlit.io/content/images/2023/04/collecting-dataset.png#border


classification-report.png (489×172)
https://blog.streamlit.io/content/images/2023/04/classification-report.png#border


designing-model-architecture.png (1579×383)
https://blog.streamlit.io/content/images/2023/04/designing-model-architecture.png#border


3-chart-animation.gif (1140×536)
https://blog.streamlit.io/content/images/2023/04/3-chart-animation.gif#border


2-chart-animation.gif (1140×536)
https://blog.streamlit.io/content/images/2023/04/2-chart-animation.gif#border



https://blog.streamlit.io/content/images/2023/04/4-code-structure-ipyvizzu.svg#border


snowflake-to-acquire-streamlit.gif (1080×708)
https://blog.streamlit.io/content/images/2022/11/snowflake-to-acquire-streamlit.gif


dbt Cloud & Streamlit App | How the Cazoo Data Team Built It
https://blog.streamlit.io/dbt-cloud-jobs-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Easy monitoring of dbt Cloud jobs with Streamlit

How the Cazoo data science team built their dbt Cloud + Streamlit app

By Martin Campbell
Posted in Advocate Posts, June 11 2021
How Streamlit fits in
The dbt Cloud + Streamlit app
Sidebar
RAG Status
Failed Steps
Historical Runs
Wrapping up
Contents
Share this post
← All posts

This post also appeared on the Cazoo Tech Blog, read more here.

Here in the Cazoo data team, dbt is a core tool for taking raw data from our S3 datalake and transforming it into a form that is much more easily consumed by our reporting & analytics tools (Looker and AWS Athena to name just two).  We run multiple jobs throughout the day to serve data that is somewhat near real time (updated hourly) to keep our internal customers informed on how the business is performing.

We use dbt cloud to schedule our jobs, which unfortunately comes with one reasonably frustrating issue - detailed information on the status of jobs (including any reasons for failure) is not available for standard users, only Admins get access. Ideally we want to expose this information to a wider audience without needing to grant admin access, both because that would increase costs, but also, most people don't need admin level access, a read only view on these jobs is more than sufficient.  Fortunately dbt Cloud does provide an API, that allows us to pull some fairly detailed data from our jobs which we can use to, we just need some suitable way to display this.

Below we'll get into how we solved this, but if you want to jump right into the source code, here it is.

How Streamlit fits in

This is where Streamlit comes in. Streamlit is a fantastic way to build shareable web applications without any knowledge of front end technologies (which is rather handy for me, as whilst I can spell HTML and CSS that’s about as far as my knowledge goes). I’ve tried similar libraries (such as Dash) in the past but Streamlit has blown us all away with how easy it is to use, and how nice the resulting apps look.  Streamlit is open source, so you can deploy apps locally, or on your own infrastructure if you so wish.  

At Cazoo, we have a broad strategy of making use of external expertise for running tools to lessen the overhead for our engineering teams, and so instead of deploying Streamlit on some Fargate instances at AWS we’ve been enthusiastic participants in the ongoing Streamlit for Teams beta.  This allows us to deploy our apps at the click of a button, and let someone else (the nice folks at Streamlit) worry about keeping the lights on.  

As well as using Streamlit for the more obvious case of Analytics & Data Science apps - here in Data Engineering we’re also making use of it for apps where we want to expose operational information such as Dead Letter Queues, or in this case what’s happening with our dbt jobs.

The dbt Cloud + Streamlit app

Full disclosure, I didn’t build this app, one of my colleagues Oliver Morgans did. I’m just helping out on this one.

We’ve made the app available for anyone who wants to use it, and we’d certainly welcome any questions, comments or pull requests for ways to make this better. I’m sure as our analysts start using this they’ll be making suggestions for things they want to see also. You can find the source code at here.

Detailed instructions for getting up and running are in the repo README.  You’ll need to add a few things like your dbt Cloud account_id and API key into the streamlit secrets.toml, and you’ll probably want to swap out the logo we’ve stored in images too (unless you’re particularly enamoured with the Cazoo logo of course).

We have included basic username/password auth in the code, though if you’re using Streamlit for Teams I’d recommend you use the built in SSO as that’s a much nicer experience than typing in a long password each time.

The app is currently composed of a few pieces:

Sidebar

You can select to see all runs, successful runs or failed runs. And you can filter down by project (this project list is driven by a set of project ids you can add in secrets.toml)

RAG Status

We’re pulling the last 500 job runs, and this table will let you sort by the columns to get at what you’re after.

Failed Steps

Of particular interest to our analysts, is the step (or steps) that a dbt job failed on, as most often that represents a unit test that has failed. So you have the option to select a specific run, and to see any failed steps in that run.

That blue text is indeed a link which will take you to GitHub and the specific file that threw the error. You can configure the base URL for your repo as part of the secrets.toml.

Historical Runs

Picking a job name also exposes the last ten historical runs, which will also allow you to inspect them for failure steps if needed.

Wrapping up

I am a huge fan of Streamlit, and the fact we’re able to easily use it not just for data science but for other operational things in engineering is making it a really key part of our toolbox.  I’d welcome any comments below on what you would add to this app, or feel free to tell us if we’re doing it all wrong - because every day is a school day!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Prototype your app in Figma! 🖌️
https://blog.streamlit.io/prototype-your-app-in-figma/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Prototype your app in Figma! 🖌️

Quickly and easily design your app with the Streamlit Design system

By Jessi Shamis
Posted in Tutorials, October 27 2022
Step 1
Step 2
Step 3
Step 4
Step 5
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Jessi Shamis, and I’m a designer at Streamlit.

When making apps, I was constantly trying to figure out their design before coding them. I wanted an extra step between coming up with an app idea and jumping straight into code. So I took the open-source library (most of st.commands) and made it into Figma components.

Now I could prototype my apps before deciding how to build them! 🎉

In this post, I’ll show you how to build a Streamlit app in Figma step-by-step:

Visit our Streamlit Community page on Figma to see our Design System
Click on Streamlit Design System and make yourself a copy
Go to the “Assets” tab to find all your Streamlit components
Type “App Base” in your search assets bar
Start adding components!

If you want to check it out right away, here's our Streamlit Design System in Figma. Go ahead and start building!

Step 1

Visit our Streamlit Community page on Figma to see our Design System. Follow us to learn about new designs, new components, and other new updates. 💙

🤫
SPOILER ALERT: We’ll be releasing a companion plugin soon. Hope you’ll like it!!
Step 2

Click on Streamlit Design System and make a copy. It will appear in your personal Figma account so you can play with it as you wish:

👉
NOTE: The file will open in the draft folder of your personal account. You can play with the System (your local assets) or go to the top middle and click “Publish styles and components” so that anyone in your organization can use your components.
Step 3

Go to the “Assets” tab to find all your streamlit components:

Step 4

Building your app is super simple! Just type “App Base” in your search assets bar:

Once you have the base, follow the instructions to detach the instance. Start dragging and dropping components to your heart’s desire. And use the app base for the perfect layout (simply decide what you want to put in it).

Step 5

Start adding components! They’re labeled by their Streamlit Python command (i.e., st.text_input). Unsure what to add? Pick one component from the library on the third page of the Design System: 🧱 Streamlit Components—Build your app (learn more about our widgets in our docs).

Wrapping up

This is a passion project and a work-in-progress that will continue to evolve. We’ve been using it to prototype concepts and innovate around app creation. Please try it out and comment on our Figma community page. We’d love to hear your feedback.

If you have questions, leave them in the comments below or message me on LinkedIn.

Happy designing and building! 🎨

P.S.: This post is Part 1 of the series on empowering designers to build their own apps. Check out Part 2 here!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

2-chart-configuration.png (1140×536)
https://blog.streamlit.io/content/images/2023/04/2-chart-configuration.png#border


1-animate-method.png (1024×780)
https://blog.streamlit.io/content/images/2023/04/1-animate-method.png#border


Using Streamlit for semantic processing with semantha
https://blog.streamlit.io/using-streamlit-for-semantic-processing-with-semantha/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

By Sven Koerner
Posted in Advocate Posts, February 2 2023
How to integrate semantic AI processing into your apps and use cases
How to give your app “common sense” with a 3-liner
How to get inspired!
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Sven and I work as an AI researcher at thingsTHINKING.

We integrated our semantic platform with large-scale (data) customers. They use Snowflake to combine hundreds of data sinks. Connecting with Snowflake’s API and using Streamlit gave the users a super-efficient UI while preserving our platform capabilities. The integration was only 3 lines of code—and the UI setup was only 46!

In this post, I’ll show you:

How to integrate semantic AI processing into your apps and use cases
How to give your app “common sense” with a 3-liner
How to get inspired!

Want to jump right in? Here's a sample app and a repo code.

How to integrate semantic AI processing into your apps and use cases

Follow these simple steps:

Step 1. Install the corresponding semantha package for your use case with pip install semantha-streamlit-compare.

You can find the latest version of this demo on pypi. We also provide a pip-based SDK so that you can use all our semantic processing features (more on this in future posts).

Here is an example:

from semantha_streamlit_compare.components.compare import SemanticCompare

compare = SemanticCompare()
compare.build_input(sentences=("First sentence", "Second sentence"))


Step 2. Create your Streamlit UI and add the semantha endpoint.

Here is a sample UI with only 30 lines of code on GitHub:

Step 3 (optional). Import the semantha (Python) package into your code and use it in whatever form you’d like.

Step 4. Request an API code from semantha and run your prototype.

For example, to use a component, request a secrets.toml file from support@thingsthinking.atlassian.net. After you are authenticated, copy it into the .streamlit/secrets.toml folder as documented here. You may need to create it in the root of your Streamlit app.

Here is the file structure of the secrets.toml file:

[semantha]
server_url="URL_TO_SERVER"
api_key="YOUR_API_KEY_ISSUED"
domain="USAGE_DOMAIN_PROVIDED_TO_YOU"
documenttype="document_with_contradiction_enabled"

How to give your app “common sense” with a 3-liner

If you’ve done the above, you’ve built “common sense” into your app, which can now automatically understand when things are similar, different, or of opposite meanings. Extend this idea to documents and all the unstructured information you process every day. You know where this could lead to…a trusty sidekick in your daily knowledge work!

How to get inspired!

Check out this video to see what people have built based on Streamlit:

You know your use cases best—and you know where a coworker (in this case, not a human but an AI) could be very helpful!

Wrapping up

We’ll provide more use cases in future posts, including:

Movie Quote Search
ESG Document Comparison Using semantha’s MagicSort
RFI/RFP/Tender processing with semantic platforms on Streamlit/Snowflake

If you have any questions, please post them in the comments below or contact me on LinkedIn, Twitter, or via email.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

app-starter-kit-1-2.png (1780×472)
https://blog.streamlit.io/content/images/2022/09/app-starter-kit-1-2.png#border


hackathon-101.jpeg (2000×988)
https://blog.streamlit.io/content/images/2022/10/hackathon-101.jpeg


Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/page/4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
Building robust Streamlit apps with type-checking

How to make type-checking part of your app-building flow

Advocate Posts
by
Harald Husum
,
November 10 2022
Announcing the Figma-to-Streamlit plugin 🎨

Go from prototype to code as easy as 1-2-3 with our new community resource!

Product
by
Juan Martín García
,
November 1 2022
Prototype your app in Figma! 🖌️

Quickly and easily design your app with the Streamlit Design system

Tutorials
by
Jessi Shamis
,
October 27 2022
Discover and share useful bits of code with the 🪢 streamlit-extras library

How to extend the native capabilities of Streamlit apps

Tutorials
by
Arnaud Miribel
,
October 25 2022
Streamlit App Starter Kit: How to build apps faster

Save 10 minutes every time you build an app

Tutorials
by
Chanin Nantasenamat
,
September 27 2022
How to build your own Streamlit component

Learn how to make a component from scratch!

Tutorials
by
Zachary Blackwood
,
September 15 2022
Make dynamic filters in Streamlit and show their effects on the original dataset

Quickly and easily add dynamic filters to your Streamlit app

Tutorials
by
Vladimir Timofeenko
,
August 25 2022
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

Tutorials
by
Tyler Richards and 
2
 more,
August 18 2022
The magic of working in open source

How we build our open-source library and release new features

Tutorials
by
Ken McGrady
,
August 4 2022
How to enhance Google Search Console data exports with Streamlit

Connect to the GSC API in one click and go beyond the 1,000-row UI limit!

Tutorials
by
Charly Wargnier
,
July 28 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component
https://blog.streamlit.io/streamlit-authenticator-part-2-adding-advanced-features-to-your-authentication-component/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component

How to add advanced functionality to your Streamlit app’s authentication component

By Mohammad Khorasani
Posted in Advocate Posts, February 7 2023
How to create a password reset widget
How to create a new user registration widget
How to create a forgotten password widget
How to create a forgotten username widget
How to create an updated user details widget
Wrapping up
Contents
Share this post
← All posts

This is Part 2 of the Streamlit Authenticator component two-part series. In Part 1, we covered how to create an authentication component that allows users to log in and gain privileged access to pages within your app.

In this second part, we'll cover the following:

How to create a password reset widget
How to create a new user registration widget
How to create a forgotten password widget
How to create a forgotten username widget
How to create an updated user details widget

TL;DR? Here's the repo code.

How to create a password reset widget

If your user needs to reset their password to a new one, use the reset_password widget to allow the already logged-in user to change their password:

if authentication_status:
    try:
        if authenticator.reset_password(username, 'Reset password'):
            st.success('Password modified successfully')
    except Exception as e:
        st.error(e)


How to create a new user registration widget

If you want to allow pre-authorized or even non-pre-authorized users to register, use the register_user widget to allow a user to register for your app. If you want the user to be pre-authorized, set the preauthorization argument to True and add their email to the preauthorized list in the configuration file. Once they have registered, their email will automatically be removed from the preauthorized list in the configuration file.

To let any user register, set the preauthorization argument to False:

try:
    if authenticator.register_user('Register user', preauthorization=False):
        st.success('User registered successfully')
except Exception as e:
    st.error(e)


How to create a forgotten password widget

If you want to allow a user to reset a forgotten password, use the forgot_password widget to allow them to generate a new random password. This new password will be automatically hashed and stored in the configuration file. The widget will also return the user's username, email, and new random password (for you to securely send to the user):

try:
    username_forgot_pw, email_forgot_password, random_password = authenticator.forgot_password('Forgot password')
    if username_forgot_pw:
        st.success('New password sent securely')
        # Random password to be transferred to user securely
    elif username_forgot_pw == False:
        st.error('Username not found')
except Exception as e:
    st.error(e)


How to create a forgotten username widget

You can also allow users to reset their username with the forgot_username widget, which lets them retrieve their forgotten username. The widget will also return the user's username and email (for you to securely send to them):

try:
    username_forgot_username, email_forgot_username = authenticator.forgot_username('Forgot username')
    if username_forgot_username:
        st.success('Username sent securely')
        # Username to be transferred to user securely
    else:
        st.error('Email not found')
except Exception as e:
    st.error(e)


How to create an updated user details widget

You can allow your users to update their name and/or email with the update_user_details widget. The widget will automatically save the updated details in both the configuration file and the reauthentication cookie:

if authentication_status:
    try:
        if authenticator.update_user_details(username, 'Update user details'):
            st.success('Entries updated successfully')
    except Exception as e:
        st.error(e)


Wrapping up

And that concludes our review of the Streamlit-Authenticator component! I hope you now feel confident about securely authenticating users to your Streamlit application with advanced functionalities.

In the meantime, feel free to read more about this component in our book Web Application Development with Streamlit. And if you have any questions, please leave them in the comments below or contact me on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Robert Ritz - Streamlit
https://blog.streamlit.io/author/robert/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Robert Ritz
1 post
Find the top songs from your high school years with a Streamlit app

Use the Spotify API to generate 1,000+ playlists!

Advocate Posts
by
Robert Ritz
,
December 8 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

uPlanner fosters data processing innovation with Streamlit
https://blog.streamlit.io/uplanner-fosters-data-processing-innovation-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

By Sebastian Flores Benner
Posted in Case study, October 6 2022
Simplifying user experience
How we did it
A smaller but editable example
Learning from the building experience
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Sebastián Flores. I’m a Chief Data Officer at uPlanner and a Streamlit Creator.

At uPlanner, we develop cloud solutions to help higher education institutions be more efficient on Smart Campus, Academic Management, and Student Success. Most institutions use our standard data processing, but sometimes we need to create client-custom scripts for the data to be correct, consistent, and in the right format. This can turn into a nightmare as these scripts are hard to maintain and complicate the whole process.

So in this post, I’ll show you how Streamlit makes it easy and how you can build an app with a simple frontend for previously created scripts.

TL;DR? Check out the app and the repo.

Let’s get started.

Simplifying user experience

Not everyone knows how to run a Python script—or how to install it. If you work with multiple data files, using the terminal to process files can be error-prone. And updating the scripts can be tedious. Oh, the anxiety of Git conflicts!

We used to pass the clients’ files to implementation engineers, then to data engineers. The data engineers executed the scripts on the inputs and returned the outputs. This took a lot of back-and-forth communication.

Streamlit allows you to create a frontend for your code. Clients can see if their data meets all the requirements. And they see it in a familiar interface: a website. All the hard parts—installation, versioning, data processing—are hidden behind a beautiful UI.

And the best part?

It’s super-fast to develop. Streamlit has a huge collection of useful input and output widgets so even junior engineers can produce stunning pages in no time.

How we did it

Our first Streamlit app allowed the users to pick between six different data processing scripts. Yay to multipaging! The scripts had very different interfaces, but they made the utilization intuitive and easy to follow.

Here are some of the app’s general functionalities:

Convert CSV files to Excel and back to CSV
Analyze a CSV or an Excel file
Move an uploaded file to an SFTP
Extract and merge data from different tables and databases
Verify file structure and content
Generate a specific output file, to be uploaded to third-party software

Script adoption and development are much easier with the app as a single point of access. Everyone is happy with more time and autonomy!

A smaller but editable example

Due to privacy, I’m sharing a simplified version of the app that illustrates the main concepts. The app can:

1. Concatenate files that are produced on different days (useful when new data is generated daily or from different sources). By default, the file encoding and the column separator are individually inferred, but users can choose any specific setting:

2. Convert CSV files to Excel files (handy when the CSV files have different encodings and separators). Again, the encoding and the separator can be inferred or specified:

Here is the app’s file structure:

Put your regular Python scripts for transforming data into the folder “helpers.” And don’t forget to include a __init__.py to import them from the Streamlit scripts. It should be as easy as:

from helpers.concatenate_helpers import concatenate_files


The other folders and files that turn the scripts into an app are:

The file Home.py is the app’s entry file (rename it as you see fit or add emojis!).
The “images” folder is for any pictures you want to display.
The “pages/” folder is for different pages so you can use multipaging. Mine are “Home” (from “Home.py”), “Concatenate,” and “CSV to Excel” (notice how Streamlit replaces the "_" with spaces when displaying the page name).
The “tmp” folder is for temporary files (before users download them).
INSTALL.md, README.md, and LICENCE are the standard files for installation, help, and copyright.
requirements.txt is the list of required libraries.
Learning from the building experience

The most important lesson we’ve learned from creating apps is to give direct and clear feedback to the app user as soon as possible. Most of our scripts deal with file processing by displaying warning, error, and success messages from the files’ execution. Additional information is always welcome: filetype, encoding, number of rows and columns, or data distribution.

We also check the expected content:

Are all the required columns on file? Let the user know what columns they’re missing.
Do we have duplicated rows? Show the user the precise rows that need revision!
Is the data consistent across multiple files? Explain in simple terms what isn’t matching correctly.

Providing feedback as soon as you can helps users correct the information and avoid frustration. There’s nothing worse than doing 10 steps and then learning you had an error on step one!

Wrapping up

Thank you for reading my article! I hope you take away two main points:

From a technical perspective, Streamlit lets you put a front-end to Python scripts with a great trade-off: a short development time for a nice interactive interface.
From a management perspective, Streamlit decouples the development and execution of your Python scripts. This leads to accelerated adoption, innovation, and participation of non-technical users—a win-win for everyone!

If you want to find out more about uPlanner, check out our website and our social channels: LinkedIn, Facebook, and Twitter. And if you have any questions, leave them in the comments below or reach out to me on Twitter at @sebastiandres or on GitHub.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tyler Simons - Streamlit
https://blog.streamlit.io/author/tyler-simons/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Tyler Simons
1 post
Build an image background remover in Streamlit

Skip the fees and do it for free! 🎈

Tutorials
by
Tyler Simons
,
January 10 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mohammad Khorasani - Streamlit
https://blog.streamlit.io/author/m/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mohammad Khorasani
2 posts
Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component

How to add advanced functionality to your Streamlit app’s authentication component

Advocate Posts
by
Mohammad Khorasani
,
February 7 2023
Streamlit-Authenticator, Part 1: Adding an authentication component to your app

How to securely authenticate users into your Streamlit app

Advocate Posts
by
Mohammad Khorasani
,
December 6 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Sven Koerner - Streamlit
https://blog.streamlit.io/author/sven/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Sven Koerner
2 posts
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

Advocate Posts
by
Sven Koerner
,
February 2 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Deploying a cloud-native Coiled app
https://blog.streamlit.io/how-to-deploy-your-cloud-native-coiled-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Deploying a cloud-native Coiled app

How Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users

By Richard Pelgrim
Posted in Advocate Posts, September 7 2021
Why fuse Streamlit and Coiled?
Why use the Streamlit Secrets Manager?
How to give Streamlit your Coiled credentials
How to configure your Dask Cluster
Share your App with the World
Resources
Contents
Share this post
← All posts

Are you using Streamlit to present multiple GBs of data to your users? Chances are you’ll run into the limits of your computational resources, either locally or on the virtual machine that hosts your Streamlit app. Coiled can help you burst into the cloud by spinning up on-demand Dask clusters. No more worries about any of the usual DevOps activities. Just scroll down to see how you can use a Streamlit-on-Coiled app to present live computations on the entire 10+ GB NYC Taxi dataset in a matter of seconds.

In this post, you’ll learn how to make your Streamlit-on-Coiled app ready for deployment using Streamlit's secret management feature. Why secret? Because after creating something this impressive, you’ll want to be sharing it with the world...and you'll want to do that securely!

But first, let's discuss why you'd want to do this.

Why fuse Streamlit and Coiled
Why use the Secrets Manager
How to give Streamlit your Coiled credentials
How to configure your Dask Cluster
Share your app with the world

Can't wait to jump right in? Here's a sample app and repo code!

Why fuse Streamlit and Coiled?

By running your Streamlit app on Coiled, you can present computations on multiple-GB datasets in a seamless user experience. Expensive computations that would otherwise take minutes (or even hours) to run can be completed in a matter of seconds. The results will be displayed almost immediately. It’s a great example of two products built on Python-native OSS complementing each other to give you nimble-yet-powerful tooling in a familiar environment.

In a previous post, I provide a step-by-step guide of how you can achieve this. You can also jump straight to this GitHub repo to download the coiled-streamlit-expanded.py script and see for yourself. Note that the "Running get_client()" function starts up your Coiled cluster and can take a couple of minutes to complete.

Why use the Streamlit Secrets Manager?

This post will show you how to get your Streamlit-on-Coiled app ready for deployment.

Streamlit’s Sharing platform allows you to host up to 3 apps for free. Apps are deployed by running Python scripts that are hosted in public Github repositories. Because of this, you'll want to avoid including any sensitive information in your Streamlit scripts. It can expose you to potential security threats. For example, including your Coiled authentication token in a public repo would allow other users to spin up clusters and incur costs to your account.

This is why we'll be using Streamlit’s secrets management feature for the backend authorization necessary to hook your Streamlit app up to Coiled's cloud-computing resources.

You can do this by:

Adding your Coiled credentials token to the Streamlit Secrets Manager
Configuring your Dask cluster to use the right credentials
How to give Streamlit your Coiled credentials

To get your Streamlit app running on Coiled, you'll need a Coiled Cloud account. If you don't already have one, get your Free Tier account here by signing in with your Github or Google account. For a video walkthrough of this process, check out the Getting Started with Coiled guide.

Next, navigate to the Streamlit app deployment interface and click on "Advanced Settings". This will open a window in which you can include the Secrets that will be passed to your script in TOML format as key-value pairs.

Define a ‘token’ key here and set it to the Coiled token value from your Coiled Cloud dashboard.

Alternatively, you can do this after deploying your app by clicking on the three-dots symbol to the far right of the app and selecting ‘Edit Secrets’.

How to configure your Dask Cluster

You only need to change two lines in the Python code of the existing Streamlit-on-Coiled app to make it ready for deployment:

Include a hash_funcs keyword argument to the @st.cache call and set it to {"_thread.RLock": lambda _: None}). This turns off hashing for the _thread.RLock object type which improves our app performance. Read more on why and when you might want to turn hashing off here.
Use dask.config.set within the already-defined get_client function definition to manually set the coiled.token that Dask uses to the ‘token’ key in the st.secrets dictionary you just created.
@st.cache(allow_output_mutation=True, hash_funcs={"_thread.RLock": lambda _: None})
def get_client():
	cluster_state.write("Starting or connecting to Coiled cluster...")
	dask.config.set({"coiled.token":st.secrets['token']})

	cluster = coiled.Cluster(
		n_workers=10,
		name="coiled-streamlit",
		software="coiled-examples/streamlit",
	)

	client = Client(cluster)
	return client

Share your App with the World

You're all set! You can now deploy your Streamlit-on-Coiled app and share it with the world. Get your invite to sharing and check out Streamlit's blog post and documentation on deploying.

We’d love to see what you’ve built, so feel free to drop us a line in our Coiled Community Slack channel, and with the Streamlit community on their community forum - we may just feature your app on the next Coiled blog. 😉

A fair warning—if you’ve built something really good that goes viral, you might run out of your monthly Coiled Free Tier credits. If that happens, you may want to consider upgrading to Coiled Pro.

Have questions? Reach out to @coiled.hq, support@coiled.io, or Coiled Community Slack.

Resources

Coiled

Docs
Community Slack
Blog
Coiled Cloud sign-up

Streamlit

GitHub
Docs
Forum
App gallery
Sharing sign-up
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

mpa-2.gif (1254×854)
https://blog.streamlit.io/content/images/2022/06/mpa-2.gif


multipage-apps.gif (1728×1078)
https://blog.streamlit.io/content/images/2022/11/multipage-apps.gif#browser


Arvindra Sehmi - Streamlit
https://blog.streamlit.io/author/arvindra/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Arvindra Sehmi
1 post
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

LLMs
by
Arvindra Sehmi
,
February 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

MPA-1.png (1432×724)
https://blog.streamlit.io/content/images/2022/06/MPA-1.png#browser


mpa-1.gif (1258×846)
https://blog.streamlit.io/content/images/2022/06/mpa-1.gif


culture-map-app-9.png (677×783)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-9.png#border


culture-map-app-7.png (718×1056)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-7.png#border


culture-map-app-5.png (686×821)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-5.png#border


culture-map-app-6.png (778×881)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-6.png#boder


culture-map-app-8.png (686×397)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-8.png#border


culture-map-app-4.png (664×682)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-4.png#border


app-viewers.gif (1000×755)
https://blog.streamlit.io/content/images/2022/05/app-viewers.gif


4.png (1792×1218)
https://blog.streamlit.io/content/images/2022/05/4.png#browser


culture-map-app-3.png (707×509)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-3.png#border


Dialog--1-.png (3600×2440)
https://blog.streamlit.io/content/images/2022/05/Dialog--1-.png#browser


culture-map-app-2.png (694×202)
https://blog.streamlit.io/content/images/2023/01/culture-map-app-2.png#border


workspace-analytics-dotted-2.gif (1000×755)
https://blog.streamlit.io/content/images/2022/05/workspace-analytics-dotted-2.gif


3.png (1794×800)
https://blog.streamlit.io/content/images/2022/05/3.png#browser


2-2.png (1784×1182)
https://blog.streamlit.io/content/images/2022/05/2-2.png#browser


1.png (1944×722)
https://blog.streamlit.io/content/images/2022/05/1.png#browser


intoducing-streamlit-cloud.gif (1920×1080)
https://blog.streamlit.io/content/images/2022/09/intoducing-streamlit-cloud.gif


Creating Custom Themes for Streamlit Apps
https://blog.streamlit.io/introducing-theming/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Announcing Theming for Streamlit apps! 🎨

Try out the new dark mode and custom theming capabilities

By Abhi Saini
Posted in Product, March 18 2021
Introducing Dark Mode!
Creating custom themes
Creating a theme with the theme editor
Creating a theme using config.toml directly
Live updates from the Config file
What each color setting does
Themes and Custom Components
As a React prop
As CSS variables
Find your favorite new theme
Try out theming and let us know what you think!
Resources
Contents
Share this post
← All posts

We like to think Streamlit apps are beautiful out of the box (hopefully you do too!), but up until today it wasn't easy to change the look and feel to reflect your personal style or your company's visual branding. Streamlit Theming now allows you to easily change the color scheme and fonts in your app without ever having to touch any HTML or CSS.

In addition, for accessibility and user-friendliness, we're also giving app viewers the ability to toggle between light and dark themes.

Check out our video tutorial for more help and this sample app if you want to dive right in!

Introducing Dark Mode!

Before we get into custom themes, let's talk about dark mode.

By popular demand, we now provide app viewers the ability to customize how they'd like to experience your app. This is useful for people who are using the app in a dark environment, or for folks with accessibility needs, who want to override your app's custom theme.

To toggle between various themes, go to Menu on the top-right corner and choose Settings. There you'll see a redesigned Settings dialog that lets app users choose between different theme options:

Light Mode: This is the Streamlit theme you already know and love
Dark Mode: This is Streamlit's new dark theme. You'll love this too!
Use system setting: Streamlit will automatically pick up your Operating System theme (light or dark) and change colors with your OS (Note: may not work for some browsers and Linux distros).
Custom theme (only visible when provided by the app author): Use the theme provided by the app author. This is the default when provided. Otherwise, "Use system setting" is the default.
Creating custom themes

Themes are set via Streamlit's config system. You can either set the theme there directly, or you can use our fancy new theme editor interface to build a theme live and then copy it to your config.

Creating a theme with the theme editor

App developers can create a custom theme by simply going to Menu → Settings → Edit Active Theme.

Note: The theme editor menu is available only in local development. If you’ve deployed your app using Streamlit Sharing, the “Edit active theme” button will no longer be displayed in the “Settings” menu.

Creating a theme using config.toml directly

Custom themes can also be defined in the config file: ./.streamlit/config.toml

Under the [theme] section, colors variables can be defined to create a custom theme

[theme]

# Primary accent for interactive elements
primaryColor = '#7792E3'

# Background color for the main content area
backgroundColor = '#273346'

# Background color for sidebar and most interactive widgets
secondaryBackgroundColor = '#B9F1C0'

# Color used for almost all text
textColor = '#FFFFFF'

# Font family for all text in the app, except code blocks
# Accepted values (serif | sans serif | monospace) 
# Default: "sans serif"
font = "sans serif"


(Note: you can only set one custom theme at a time)

Live updates from the Config file

You know how your Streamlit app live-updates when you change its source code? We now also do this when you update the app's config file! This way, editing the theme directly in the config file will cause your app to immediately display the new theme.

What each color setting does

Streamlit's theming system follows a global approach to applying color and fonts to an app. Color styles have semantic names and a simple way for doing color specification.

Primary color: Accent color for interactive elements like st.radio, button borders etc. By default, this is pink.
Background color: This is the background color for the main body of your app.
Text color: Pretty self-explanatory! This is the text color for your app.
Secondary background color: Used as the background for st.sidebar and for several widgets.

Background color and Secondary background color are in a way "complementary" since they are used for elements placed "on" top of base elements to promote consistency and accessible contrast. This means that a st.number_input widget will use Secondary background color as its Background color when defined in the main body, but use Background color when defined in the sidebar. The image below illustrates this:

Themes and Custom Components

If you're a component author, we make it easy for you to read theme colors in your JS and CSS code. To get access to this new feature, install or update the newest version of streamlit-component-lib.

npm install streamlit-component-lib


Once this package is updated, the background and text colors of a component will automatically be changed to match that of the active theme. Additional theme properties are exposed to the component in two equivalent ways: the theme object or the theme style.

As a React prop

An object passed to the component via the theme prop has the following shape:

{
    "primaryColor": "someColor1"
    "backgroundColor": "someColor3",
    "secondaryBackgroundColor": "someColor4",
    "textColor": "someColor5",
    "font": "someFont",
}

As CSS variables
--primary-color
--background-color
--secondary-background-color
--text-color
--font


If you're not familiar with CSS variables, the TLDR version is that you can use them like this:

.mySelector {
  color: var(--text-color);
}

The two methods for exposing theme properties to a component contain the same information, so which one to use is a matter of personal preference. An example of making a custom component work with themes can be found here.

Find your favorite new theme

Here's our Mapping Demo, with a 'Quiet Light'-inspired theme:

[theme]

primaryColor="#6eb52f"
backgroundColor="#f0f0f5"
secondaryBackgroundColor="#e0e0ef"
textColor="#262730"
font="sans serif"

Here's an example of a 'Solarized'-inspired theme:

[theme]

primaryColor="#d33682"
backgroundColor="#002b36"
secondaryBackgroundColor="#586e75"
textColor="#fafafa"
font="sans serif"


Try out theming and let us know what you think!

To try out theming, simply upgrade to the latest version of Streamlit as usual:

$ pip install streamlit --upgrade


We can't wait to see the cool and amazing themes that the community will build. If you built a cool custom theme, we'd love to see it, so please tag @streamlit when you share on Twitter, LinkedIn or on our Forum.

Resources
Documentation
Github
Changelog
Forum
Sample app
Video tutorial
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

unique-viewers.png (2000×1513)
https://blog.streamlit.io/content/images/2022/05/unique-viewers.png#browser


Develop Streamlit-WebRTC Component for Real-Time Video Processing
https://blog.streamlit.io/how-to-build-the-streamlit-webrtc-component/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Developing a streamlit-webrtc component for real-time video processing

Introducing the WebRTC component for real-time media streams

By Yuichiro Tachibana (Tsuchiya)
Posted in Advocate Posts, February 12 2021
The problem with existing approaches
How WebRTC resolves this issue
The WebRTC basics
The basics of Streamlit's execution model
Integrate aiortc into a Streamlit component
Install dependencies
Setting up the project
Rolling out the first frontend implementation
Send offer from JS to Python
Server-side implementation with asyncio
Send back the answer from Python to JS
Implement processAnswer()
Introduce a thread running over script executions
Component height adjustment
Implementing your own video filter
Implement a stop button
The execution model of streamlit-webrtc
What tiny-streamlit-webrtc lacks
Parameters input from Streamlit components
Frame drops
Extendability
Key takeaways
Credits
Contents
Share this post
← All posts

Real-time video processing is one of the most important applications when developing various computer vision or machine learning models. It’s useful because it allows users to quickly verify what their models can do with handy video input from their own devices, such as webcams or smartphones.

But it also presents a challenge to those of us using Streamlit, since Streamlit doesn’t natively support real-time video processing well yet through its own capabilities.

I created streamlit-webrtc, a component that enables Streamlit to handle real-time media streams over a network to solve this problem. In this in-depth tutorial, I’ll also briefly introduce you to WebRTC (check out my article here for more in-depth info on WebRTC). If you want to jump right to playing with the component here is a sample app.

Ready?

Let’s dive in.

(This tutorial requires Python >= 3.6 and a webcam.)

The problem with existing approaches

Streamlit is actively used by many developers and researchers to prototype apps backed with computer vision and machine learning models, but it can’t yet natively support real-time video processing.

One existing approach to achieve real-time video processing with Streamlit is to use OpenCV to capture video streams. However, this only works when the Python process can access the video source - in other words, only when the camera is connected to the same host the app is running on.

Due to this limitation, there have always been problems with deploying the app to remote hosts and using it with video streams from local webcams. cv2.VideoCapture(0) consumes a video stream from the first (indexed as 0) locally connected device, and when the app is hosted on a remote server, the video source is a camera device connected to the server - not a local webcam.

How WebRTC resolves this issue

WebRTC (Web Real-Time Communication) enables web servers and clients, including web browsers, to send and receive video, audio, and arbitrary data streams over the network with low latency.

It is now supported by major browsers like Chrome, Firefox, and Safari, and its specs are open and standardized. Browser-based real-time video chat apps like Google Meet are common examples of WebRTC usage.

WebRTC extends Streamlit’s powerful capabilities to transmit video, audio, and arbitrary data streams between frontend and backend processes, like browser JavaScript and server-side Python.

The WebRTC basics

The following tutorial uses knowledge about WebRTC concepts such as "Signaling", "Offer", and "Answer". The below figure provides a simple summary of how to establish a WebRTC connection.

WebRTC has a preparation phase called "Signaling", during which the peers exchange data called "offers" and "answers" in order to gather necessary information to establish the connection.
Developers choose an arbitrary method for Signaling, such as the HTTP req/res mechanism.

If you want to know more about these concepts, read this article.

Just as in the article linked above, this tutorial will use aiortc, a Python library for WebRTC, and an example from the aiortc repository as the basis for our sample project.

The basics of Streamlit's execution model

To read further, you should know about the development of Streamlit bi-directional custom components and about Streamlit's execution model. You can learn about it here.

Here is a short summary:

Upon each execution, the Python script is executed from top to bottom.
Each execution of the Python script renders the frontend view, sending data from Python to JS as arguments to the component.
The frontend triggers the next execution via Streamlit.setComponentValue(), sending data from JS to Python as a component value.

Integrate aiortc into a Streamlit component

In this section, to understand how to integrate a WebRTC implementation into a Streamlit custom component, we will create a minimal version of streamlit-webrtc called tiny-streamlit-webrtc, as a hands-on tutorial.

The source code of tiny-streamlit-webrtc is hosted on GitHub. Throughout this tutorial, we will refer to this repository and review each intermediate commit step-by-step to reach the final version.

It is recommended for you to clone the repository:

$ git clone https://github.com/whitphx/tiny-streamlit-webrtc.git
$ cd tiny-streamlit-webrtc


With the below command, you can check out the specific revision referenced in each section in order to see the entire codebase and to actually try running it.

$ git checkout <revision>

Install dependencies

Install the necessary packages. Note that this tutorial does not work with the latest version of aiortc (1.1.1) and 1.0.0 must be used.

$ pip install streamlit opencv-python
$ pip install aiortc==1.0.0
Setting up the project

As usual, we start with the official template of a bi-directional component. The reference tiny-streamlit-webrtc implementation is based on the revision 4b90f52.

After copying the template files, complete the rest of the setup, including the steps below.

Rename "my_component" to "tiny_streamlit_webrtc".
Run npm install in tiny_streamlit_webrtc/frontend.
Remove the existent code, comments, and docstrings.
Add necessary files such as .gitignore

Check out what this section does, with code version f6daf28.

Rolling out the first frontend implementation

Let's start writing code.

First, we will simply copy and paste some lines of code from index.html and client.js in the aiortc example into our React component, but with some fixes.

e3f70e4 is the actual edit, and you can try this version by checking out the commit, as explained above.

$ git checkout e3f70e4


The view contains only a <video /> element with autoPlay and playsInline props, as it is in the original index.html, and a button element to start the WebRTC session. The start button's onClick handler is bound to the start() method, which is copied from client.js and slightly modified to remove some lines unnecessary for this tutorial and adjust to the React class-based component style. We will do the same for negotiate() and createPeerConnection().

Let's run this component in the usual manner for Streamlit custom component development.

$ cd tiny_streamlit_webrtc/frontend/
$ npm start

$ streamlit run tiny_streamlit_webrtc/__init__.py


After opening the app with a web browser, open the developer tools, and click the "Start" button. You can see the offer is generated and printed in the console as below.

This is printed via this line. Please follow the steps leading up to it. This code is equivalent to the code in the original example before sending the offer to the Python server. Yes, this case is different from the original example. How can we send the offer to the Python process?

(You also see your webcam become active since navigator.mediaDevices.getUserMedia() requests its use.)

Send offer from JS to Python

streamlit-webrtc makes use of Streamlit.setComponentValue() for this purpose. We will learn about it in this section.

7b7dd2d is the next update. Use git checkout 7b7dd2d to check out it.

With this change, the offer is sent from the frontend to the server as a component value.

const offerJson = offer.toJSON()
Streamlit.setComponentValue({
  offerJson,
})


The offer can be read on the server-side as below.

component_value = _component_func(key=key, default=None)
if component_value:
    offer_json = component_value["offerJson"]


Let's run this version and confirm the offer is displayed after clicking the "Start" button, which means the offer is received by the Python process and shown with st.write() here.

Server-side implementation with asyncio

Now the offer is received on the server-side, so let's implement the code to process it. Just as we did with the frontend, let's copy and paste from the example server.py to our streamlit_webrtc/__init__.py, like this, which is copied from offer() coroutine in the example server.py.

Note that a video transformer is temporarily omitted from the track event listener like this to focus on the WebRTC part for now. It now just passes through the input track to the output.

However, as you can see, this code contains async and await and does not work in a function. So, we have to wrap this part in a coroutine like this.

Please run this version: a6f7cc0 and confirm the answer is displayed following the offer from here. That means the server-side pc object has processed the offer and generated the answer.

What we have to do next is send it back to the frontend.

Send back the answer from Python to JS

To do this, streamlit-webrtc simply relies on Streamlit's data sending mechanism from Python to JavaScript as below.

_component_func(key=key, answer=answer)


However, one problem arises. We’ve already called component_value = _component_func(...) and obtained the offer from it. After that, we generated the answer. So, how can we set the argument to the already called _component_func() again?

Simply calling the second _component_func() as below does not work, because in the Streamlit app, different _component_func() calls are recognized as different instances of the component.

component_value = _component_func()
offer = component_value["offer"]
answer = generate_answer(offer)  # Pseudo code
_component_func(answer=answer)  # This does not work!

invalid_answering.py

To resolve this problem, we have to introduce a hack: SessionState and st.experimental_rerun(). With these tools, we can rerun the script to call a _component_func() in the same line again and hold a variable over the runs to feed it to the _component_func() in the second and later executions.

SessionState has been discussed in this forum topic and the source is available on this page in Gist.

st.experimental_rerun() seems, as its name implies, to be an experimental API and not documented yet. It has been discussed in this GitHub issue and can now be used.

Please see this version of the server-side code, where SessionState and st.experimental_rerun() are used to feed the generated answer to the component.

This illustrates how it works.

Another important thing here is that the key argument is no longer optional but must be explicitly provided like this. As the answer is fed as an argument to _component_func() and its value changes over the runs, key is necessary as a stable identifier of the component instance.

If key is None, Streamlit identifies the component instance based on arguments other than key, so Streamlit cannot trace the identity of the component instance over the runs as the answer changes.

Note that this if-clause is added to invoke st.experimental_rerun() only the first time the server-side process gets the offer from the frontend. This may also be achieved by resetting the component value on the frontend once the offer is passed to Python.

With this version: aa2ab49, you can see the answer is provided as a field of the args prop like this on the frontend. Let's confirm it with the browser's devtools.

Implement processAnswer()

Now we have the answer on the frontend. Let's implement the rest of the frontend code like this.

This code is copied from the part following receiving the answer in the example client.js and fixed to adjust to ours.

Introduce a thread running over script executions

It seems we have done all things we have to do, but no video appears when you click the "Start" button with this version: 7fbf0eb.

The problem resides on the server-side. The server-side WebRTC code from aiortc runs on an event loop, which is implicitly started with asyncio.run() here now. An event loop is created on which aiortc functions rely throughout one Streamlit script execution. But this event loop will be trashed in the next script execution and the aiortc can no longer keep working.

To resolve this problem, we will fork a thread and create an event loop inside it to run aiortc functions. And the thread object is stored in the SessionState to be maintained over the multiple Streamlit script executions.

See this version of the code: 093f81b. This webrtc_worker() function is forked as a thread here. Inside this thread, a new event loop is created and the process_offer() coroutine is running on it - which was invoked by asyncio.run() in the previous revisions of this code. With this change, queue.Queue is introduced to get the answer object in the main thread, which is now generated in the forked thread.

There is one drawback of forking a thread - the streamlit run command does not stop when you hit Ctrl+c. This is because the forked thread remains even after the main thread is terminated.

To forcefully terminate the process, send it SIGKILL as below.

$ ps aux | grep python | grep streamlit  # Find the process ID
whitphx         19118  11.2  0.6  4759304  99928 s003  S+    5:27PM   0:02.06 /path/to/venv/bin/python3.8 /path/to/venv/bin/streamlit run tiny_streamlit_webrtc/__init__.py
$ kill -9 19118  # Send SIGKILL to the process specified with the ID

To fix it, the daemon option of the forked thread is set to True like this. With this flag, the script stops correctly when necessary.

A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left.
"Thread Objects" (Python.org)
Component height adjustment

Let's try out the current version: fc48060. Now, WebRTC works and the video appears with this component! However, the displayed video is cropped and the lower part of it is hidden like below.

To fix it, we have to call Streamlit.setFrameHeight() when the size of <video /> element changes. Although it is automatically called when the props are updated, the element resize is not associated with props updates but with starting video streaming.

Now attach onCanPlay event handler on the <video /> element and call Streamlit.setFrameHeight() from it like this. (While using ResizeObserver may be the right way to observe DOM element resizes, we use the onCanPlay event here as a substitute, for simplicity's sake.)

Cool! Now it works correctly. 🎉1a57a97 is this version.

Now all the core parts for WebRTC are complete. We’ll implement the rest in the following sections.

Implementing your own video filter

First, let's try to implement some video filters. 3ba703d is an example with a simple edge extractor, copied from the sample code of aiortc.

Implement a stop button

Refer to the aiortc example to create a stop button to gracefully stop the stream. 13a38c1 is the current version.

The execution model of streamlit-webrtc

We have followed the steps to develop a minimal Streamlit component utilizing WebRTC to stream video.

As we’ve seen in this component, we chose a design in which the computer vision code is running in a callback in the forked thread, triggered by new frame arrivals from the input stream, independent of Streamlit's script execution timings. It looks a little bit weird the first time you see it, but it's necessary and natural when dealing with real-time streams.

Let's see it from a more abstract view. When processing frames coming from real-time streams, the streams are additional event sources other than user interactions through the frontend view. In normal Streamlit apps, all the events triggering Python script executions are only sourced from the frontend and they are nicely encapsulated by Streamlit.

With its execution model, then, developers can write the apps in a clean world where there are no callbacks and no (or little) side effects. In turn, if we want to handle the streams with good performance, we have to explicitly handle the events sourced from the streams like frame generations, which breaks the elegant encapsulation, causing callbacks and events to appear in the script.

What tiny-streamlit-webrtc lacks

Though we’ve created a small subset of streamlit-webrtc, tiny-streamlit-webrtc, it still lacks many important features streamlit-webrtc has. Here we will review some of them.

Parameters input from Streamlit components

One of the biggest benefits of using Streamlit is interactive controls such as sliders and radio buttons. With computer vision and machine learning models, these controls are very useful to change the parameters during execution.

Because the computer vision code is running in the forked thread with this component, we have to pass the values obtained from Streamlit widgets to the CV code over the threads. But it is not difficult, like here in the streamlit-webrtc sample.

With tiny-streamlit-webrtc, you can do this by adding a public property to VideoTransformTrack and read and write it from each thread, just like the sample code linked above. Please try it if you are interested, and be careful about thread safety when you pass complex values.

Frame drops

We’ve used edge extraction as an example in the tutorial. However, if you replace it with more computationally expensive filters like deep neural networks, you will see the displayed video slows down. You can test it simply by putting time.sleep(1) in VideoTransformTrack.recv().

This is because VideoTransformTrack.recv() processes all the input frames one by one - if it delays, generating the output frames is also delayed.

To solve this problem, VideoTransformTrack.recv() has to drop some input frames and pick the latest one each time it runs. In streamlit-webrtc, it's done here when async_transform option is set as True.

Extendability

In tiny-streamlit-webrtc, the video transformation is hard-coded inside VideoTransformTrack.recv(), but of course, this is bad design as a library. To be reusable, it should expose an injectable interface through which developers can implement arbitrary kinds of video transformation, encapsulating details such as VideoTransformTrack.recv() and WebRTC-related code.

With streamlit-webrtc, developers can implement their own video transformations by creating a class extending VideoTransformerBase class like this and this.

Key takeaways

Streamlit is a nifty framework with a useful library, but it doesn’t handle real-time video processing well on its own.

WebRTC makes Streamlit even more awesome by enabling server-side processes and clients to send and receive data streams over the network with low latency.

Have an amazing project in mind to use WebRTC for? Share it with us in the comments or message me.

Credits

Reviewed by Yu Tachibana (@z_reactor)

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

deploy_small.gif (1080×640)
https://blog.streamlit.io/content/images/2021/11/deploy_small.gif


Add and Share Custom App Functionality | Streamlit Components
https://blog.streamlit.io/introducing-streamlit-components/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing Streamlit Components

A new way to add and share custom functionality for Streamlit apps

By Adrien Treuille
Posted in Product, July 14 2020
The Streamlit Components Gallery
Building your own components
Static Components
Getting widget with it!
Sharing with the world
Try it out and let us know what you think!
Contents
Share this post
← All posts

In the ten months since Streamlit was released, the community has created over 250,000 apps for everything from analyzing soccer games to measuring organoids, and from COVID-19 tracking to zero-shot topic classification. Inspired by your creativity, we added file uploaders, color pickers, date ranges, and other features. But as the complexity of serving the community grew, we realized that we needed a more scalable way to grow Streamlit’s functionality. So we’re turning to Streamlit’s best source of ideas: you!

Today, we are excited to announce Streamlit Components, the culmination of a multi-month project to enable the Streamlit community to create and share bits of functionality. Starting in Streamlit version 0.63, you can tap into the component ecosystems of React, Vue, and other frameworks. Create new widgets with custom styles and behaviors or add new visualizations and charting types. The possibilities are endless!

The Streamlit Components Gallery

The first thing you should do is check out the Streamlit Components Gallery to see what others have created and shared.

Each component can be installed with just a single line of code:

pip install some_cool_component

If you don’t find a component that works for you, you can make your own!

Building your own components

Streamlit has a unique, functional style which lets you create rich, interactive experiences in very few lines of code. For example, let’s check out this simple Streamlit app:

import streamlit as st

x = st.slider('x')
st.markdown(f'`{x}` squared is `{x * x}`')

Looking at this code, you can see that Streamlit calls come in two flavors: static components like st.markdown are stateless and only send data to the browser, whereas bidirectional components like st.slider have internal state and send data back from the browser.

Our challenge was to provide an API that embraces Streamlit’s functional style while capturing these use-cases as simply as possible. A few months ago, two amazing Streamlit engineers, Tim Conkling and Henrikh Kantuni, tackled this challenge and came up with a super elegant solution. The result is the new streamlit.components.v1 package which comprises three functions. For static components, we added:

html(...), which lets you build components out of HTML, Javascript, and CSS
iframe(...) , which lets you embed external websites

For bidirectional components, we added:

declare_component(...), which lets you build interactive widgets which bidirectionally communicate between Streamlit and the browser.

Let’s dive into how it works!

Static Components

Let’s start with a simple static component to embed snippets of code called Github gists in your app. Ideally, adding the component should just be a single function call:

# Render a gist
github_gist('tc87', '9382eafdb6eebde0bca0c33080d54b58')

which would render a gist like this:

To create such a component, we start by importing the Streamlit Components library:

import streamlit.components.v1 as components

This somewhat wordy import statement serves two purposes:

It versions the components API so that future changes don’t break existing components.
It reminds us that we’re starting to use deep magic which we should hide from the user.

Now let’s use the html(...) method to serve up the gist:

	def github_gist(gist_creator, gist_id, height=600, scrolling=True):
	    components.html(
	        f"""
		  <script src="https://gist.github.com/{gist_creator}/{gist_id}.js">
		  </script>
		""",
	        height=height,
	        scrolling=scrolling,
	    )
view raw
components_1.py hosted with ❤ by GitHub

This approach has a few awesome properties. First, it’s really simple and functional. In fact, this pattern lets you hide the ugly-looking HTML and wrap it into a pretty, Pythonic function call, github_gist(...). You can wrap code in a function and reuse it throughout your project. (Better yet, put it in a package and share it with the community in the gallery.) Second, note that we can add arbitrary HTML in this component — divs, spans, and yes, scripts! We can do this safely because the component is sandboxed in an iframe which lets us include external scripts without worrying about security problems.

Getting widget with it!

What if you want to create a stateful bidirectional component that passes information back to Python from the browser, or in other words, a widget? You can do this too! For example, let’s create a counter component:

count = counter(name="Fanilo")st.write('The count is', count)

which creates this:

Note that this code follows Streamlit’s unique functional style and captures the counter state embedded in the component. How did we achieve this? Happily, a single function call, declare_component, does all the work to enable bidirectional communication with Streamlit.

# Declare a simple counter component.import streamlit.components.v1 as componentscounter = components.declare_component("counter", path=BUILD_PATH)

Nice! Under the hood, BUILD_PATH points to a component built using React, Vue, or any frontend technology you like. For this example we decided to use React and Typescript giving us this render function:

	public render = (): ReactNode => {
	  return (
	    <span>
	      Hello, {this.props.args["name"]}! &nbsp;
	      <button onClick={this.onClicked} disabled={this.props.disabled}>
	        Click Me!
	      </button>
	    </span>
	  )
	}
view raw
components_2.ts hosted with ❤ by GitHub
	public render = (): ReactNode => {
	  return (
	    <span>
	      Hello, {this.props.args["name"]}! &nbsp;
	      <button onClick={this.onClicked} disabled={this.props.disabled}>
	        Click Me!
	      </button>
	    </span>
	  )
	}
view raw
components_2.ts hosted with ❤ by GitHub

and this callback:

	private onClicked = (): void => {
	  this.setState(
	    prevState => ({ numClicks: prevState.numClicks + 1 }),
	    () => Streamlit.setComponentValue(this.state.numClicks)
	  )
	}
view raw
components_3.ts hosted with ❤ by GitHub

Donezo! You’ve now created a simple, stateful component which “feels like React” on the website, and “feels like Streamlit” on the Python side. Information is passed back to Python using Streamlit.setComponentValue(...). Because we’re using React in this case, the component’s state is stored in this.state. For more details on this example, see our component template.

A neat benefit of this architecture is that you’re not limited to React. You can use any language or framework which compiles for the web. Here is the same counter component written in ClojureScript.

	(defonce counter (atom 0))
	

	(defn increment-counter []
	  (swap! counter inc)
	  (send-message-to-streamlit :set-component-value {:value @counter}))
	

	(defn app []
	  [:button {:on-click increment-counter} "Click Me!"]
view raw
components_4.cljs hosted with ❤ by GitHub

Another cool feature of this API is that you can do hot-reloading as you develop your component like this:

components.declare_component(name, url="http://localhost:3001")      

Here, the url parameter lets you specify a dev server for the component created with npm run start.

What we’ve shown you so far just scratches the surface. For more details, please check our documentation.

Sharing with the world

Did you create something broadly useful for the Streamlit community? Sure, you could keep that superpower for yourself, but it would be even cooler to share it! Get community feedback and praise. 😇 You can easily wrap your component in a PyPi package and submit it to our Gallery by following these instructions.

Try it out and let us know what you think!

We’re excited to unlock for the community a new way to plug-and-play functionality into Streamlit. Streamlit Components let you write simple HTML extensions or tap into the whole ecosystem provided by React, Vue, and other frameworks. Your feedback drives innovation in Streamlit. Please tell us what you think and what you’d like next. Show off your shiny new components and share them with the world. We can’t wait to see what you build! 🎈

Special thanks to Fanilo Andrianasolo, Daniel Haziza, Synode, and the entire Streamlit Components beta community who helped refine this architecture and inspired us with their feedback and ideas. Thanks also to TC Ricks, Amanda Kelly, Thiago Teixeira, Beverly Treuille, Regan Carey, and Cullan Carey for their input on this article.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

analytics-header.png (1459×1103)
https://blog.streamlit.io/content/images/2022/05/analytics-header.png#browser


Diana Wang - Streamlit
https://blog.streamlit.io/author/diana/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Diana Wang
1 post
Leverage your user analytics on Streamlit Community Cloud

See who viewed your apps, when, and how popular they are

Product
by
Diana Wang and 
1
 more,
May 17 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit-Images.png (1394×536)
https://blog.streamlit.io/content/images/2021/11/Streamlit-Images.png


Monthly rewind > June 2021
https://blog.streamlit.io/monthly-rewind-june-2021/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Monthly rewind > June 2021

Your June look back at new features and great community content

By Jessica Smith
Posted in Monthly Rewind, July 5 2021
🏆 App of the month 🏆
Streamlit June updates
🔍 Current release: 0.84.0
🔮 Upcoming features
Session state
Connecting to databases
Featured Streamlit content
Featured community content
Contents
Share this post
← All posts
🏆 App of the month 🏆

Our June featured app of the month is......🥁🥁🥁🥁🥁

Menara by Aktham Momani.

If you want to buy, sell, refinance, or even remodel a home this app offers many resources, estimates and forecasts to help you make the most informed decision. You can predict and forecast house prices and search GreatSchools within the California Bay Area. [code]

Streamlit June updates

Here are some updates on happenings at Streamlit this month.

🔍 Current release: 0.84.0

The latest release is 0.84.0. This release notably brought with it Session State! This highly requested feature allows information to be stored across app interactions and reruns. Make sure to check out the changelog to keep up to date with the latest features and fixes.

🔮 Upcoming features

Be on the lookout for these new features:

st.download
st.card
Session state

Read more about the new Session State feature and how to get started in the launch article.

Connecting to databases

Our docs now include step-by-step guides showing how to connect Streamlit to different databases.

Featured Streamlit content

Podcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.

Part 1 and Part 2 of a new blog series Designing Streamlit Apps for the User offer helpful tips on how to take your apps to the next level. Stay tuned for Part 3!

The data science team at Cazoo explain how they are using Streamlit for Teams to build their DBT dashboard and other helpful company apps.

Listen to Johannes talk about his career journey and all things ML and AI in The AI Aesthetic: Building Beautiful Data Apps 2.
The AI Aesthetic: Building Beautiful Data Apps
The frontend often takes a backseat in the machine learning lifecycle. That’s why you need help to bring apps to life.
Rackspace Technology
Rackspace Technology Staff - Solve
Featured community content

Some great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.

Akshansh created a CodeGame app where you build high scores by identifying the languages of random code snippets
Options-2-trees is an interactive visualization of the CRR binomial options pricing model by Tony
Learn how to build a text classifier model and app in Jesse's tutorial NLP Project - Emotion In Text Classifier App with Streamlit and Python
Billy goes over How To Create An Instagram Profile Analyzer App Using Python And Streamlit with instagram-scraper in his blog post
If you want to see how your Medium posts are performing, Rahul wrote about How to get a JSON file of your Medium Stats and create a Dashboard using Streamlit
Label intent, along with entity recognition and entity frequency for your GSC queries with Greg's GSC Query Analysis Tool
Find your Hacker News Doppelgänger with this app by Pinecone, which compares the semantic meaning of your comment history with those of all other users, and finds the top ten users whose comment histories are most similar to yours
Solar-MACH is an awesome app by Jan and crew that plots multi-spacecraft longitudinal configuration
Alex made Jina App Store Search to show how to build an AI-powered search engine for an app store using the Jina framework
In his blog tutorial Visualizing Graph Embeddings with t-SNE in Python CJ shows how to inject graph embeddings created by the GDS library in Neo4j and visualize them in a dashboard
Avra demonstrates How to Build Streamlit App connected to Google Sheet as Database [Streamlit-Google Sheet Automation] in his thorough video tutorial

Thanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.

Reach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!

Want to see older rewinds? Check them out below:

January 2021
February 2021
March 2021
April 2021
May 2021
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Monthly Rewind...

View even more →

Monthly rewind > March 2023

Your March look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
April 24 2023
Monthly rewind > February 2023

Your February look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
March 22 2023
Monthly rewind > January 2023

Your January look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
February 16 2023
Monthly rewind > December 2022

Your December look-back at new features and great community content

Monthly Rewind
by
Jessica Smith
,
January 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Data_Importance2.gif (481×359)
https://blog.streamlit.io/content/images/2023/08/Data_Importance2.gif#border


Streamlit Components: Our Security Model & Design Philosophy
https://blog.streamlit.io/streamlit-components-security-and-a-five-month-quest-to-ship-a-single-line-of-code/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit Components, security, and a five-month quest to ship a single line of code

The story of allow-same-origin

By Tim Conkling
Posted in Tutorials, January 20 2021
Components: security + design
allow-same-origin and the iframe sandbox
Breaking the sandbox
First, "don't hijack my CSS"
And more importantly, "don't hijack Streamlit Sharing"
Wrapping up
Contents
Share this post
← All posts

In the changelog for Streamlit 0.73.0, released in December 2020, there’s a small callout: “Component iframes now include the allow-same-origin sandbox attribute.”

This change enables dramatically more powerful Streamlit Components - you can now use webcams and microphones in Streamlit apps and more easily embed and interact with external resources - and it was just a single line of code! (Check out our ever-expanding Component Gallery for examples of quality Components created by the Streamlit community.)

But this is not a post about how to use or build Streamlit Components. If you're interested in that, we have a tutorial here or check out the great community tutorial by Fanilo Andrianasolo! Instead, we want to peel back the curtain and discuss how we make changes to Streamlit itself. Because we could have shipped that single line in July when we launched Streamlit Components. We could've shipped it in any of the 9 releases that followed! But instead, it took us 5 months.

This is an engineering-focused post about why such a small change took such a long time. It'll touch on the Streamlit security model, our design philosophy, and the competing constraints that can lead to long development times for seemingly-simple features. And for the masochists out there, we'll stare briefly into the abyss that is cross-origin web security.

Components: security + design

Let's start with a humble-brag: Streamlit has lots of users, many of whom are storing and accessing sensitive data in Streamlit apps. We take security very seriously. What this means in practice is that any change or feature we add to Streamlit must not reduce the security of either the Streamlit open source library or Streamlit sharing, our "press button → deploy to cloud" hosting platform.

Additionally, we care dearly about the design of Streamlit - not just the way it looks, but the way it works, all the way down to API names. This means that, as much as possible:

Streamlit should just work.
Streamlit features should be robust and powerful.
New features should not increase the complexity of installing, using, or deploying Streamlit.

So we have these three broad goals: add new features to Streamlit, don't undermine its simplicity, and ensure it's safe. When we're fortunate, these goals are not at odds with each other. When we're less fortunate, we have the allow-same-origin situation and we end up writing blog posts like this one.

Before getting into the weeds, let's first consider the story around Streamlit Components and security:

Fundamentally, a Streamlit Component is a Python library. You should exercise the same judgement with a Component as you would with any other Python library you pip install in your project.
A Component also runs code on the frontend, which means it can make requests from the browser, and can access data on your app's frontend.
You should assume that any library you use - Component or otherwise - can access any data in your app.
If your app deals with sensitive data, only install libraries and Components that you have written, or that you otherwise trust.

No big surprises. But there's a wrinkle: as Streamlit Components was under development, so too was Streamlit sharing. We needed to make sure that a rogue Component in a shared app couldn’t peek at Streamlit sharing data, or execute commands on behalf of the developer.

tl;dr for the rest of the blog post: nothing in a Streamlit app - malicious Component or otherwise - can hijack Streamlit sharing. But we treaded carefully - and a bit slowly - to make sure this was the case.

allow-same-origin and the iframe sandbox

(This section gets into the details of <iframe> sandboxing, the allow-same-origin sandbox flag, and cross-origin requests. It'll be of primary interest to those who work with, or are curious about, web security. If you have no interest in the nitty-gritty, skip ahead to the next section!)

Broadly speaking, Streamlit Components are user-created plugins that extend Streamlit. You pip install a Component into your Python environment, and now you can add a forum, or an interactive 3D molecule viewer, or a Facebook HiPlot data graph, or custom charting libraries - or really any feature that Streamlit doesn't include out of the box - to your Streamlit app.

During development, we had two primary concerns around the Component security model:

A Component shouldn't be able to break assumptions about its surrounding page (changing the host app's CSS or DOM, for example).
A Component in an app deployed with Streamlit sharing shouldn’t be able to hijack its owner's sharing credentials and read secret data or execute a CSRF exploit.

Under the hood, each instance of a Streamlit Component is mounted inside its own <iframe> in its containing Streamlit app, which means it lives in its own little world with its own DOM, its own CSS, and its own restrictions. Each iframe has a sandbox with a number of different attributes that specify what it can and can't do. For our purposes here, we're interested in two sandbox flags: allow-same-origin and allow-scripts.

allow-scripts is self-explanatory: if it’s missing, then the iframe will not be able to execute any JavaScript. Executing JavaScript is a fundamental part of Streamlit Components, so this attribute must be enabled. allow-same-origin is related to “cross-origin-resource-sharing”, or CORS - which means that it’s destined to be confusing and annoying. If you omit this attribute, the iframe won't be able to use certain browser features (like webcams and microphones), and it will be unable to make requests to many other web servers (which often expect a non-null origin).

When Streamlit Components launched, we left off allow-same-origin because of how it interacts with allow-scripts. The MDN iframe page explains it thusly:

When the embedded document has the same origin as the embedding page, it is strongly discouraged to use both allow-scripts and allow-same-origin, as that lets the embedded document remove the sandbox attribute — making it no more secure than not using the sandbox attribute at all.

Streamlit Components are served from the same origin as their embedding page, which means that combining allow-scripts and allow-same-origin would render our sandbox moot. This isn't necessarily a big deal, because Components are not "untrusted code" - but would potentially undercut our Component security concerns.

There's a big document memorializing weeks of discussion and argument on our allow-same-origin woes: should we serve components from a separate origin? Should we allow devs to opt into the allow-same-origin flag via a config option? Should we maintain an allow-list within Streamlit of Components that can use this flag?

We developed a number of prototypes that solved the issue in different ways. But all of them undercut Streamlit's "keep things simple" design principle:

Some prototypes made Streamlit use more difficult (by requiring that dev deeply understand the Component sandbox model).
Some made Streamlit deployment more difficult (by exposing more server ports to be forwarded and routed through proxies).
And some made Component development more difficult (by imposing restrictions on Component creators).
Breaking the sandbox

After several months of proposals, prototypes, and arguments, we shipped Streamlit 0.73, which solved the problem by simply adding the allow-same-origin iframe flag. In other words, we decided to allow Components to break the iframe sandbox.

Why are we ok with this? And what are the ramifications? Here's where we landed on our original sandboxing concerns:

First, "don't hijack my CSS"
"A Component shouldn't be able to break assumptions about its surrounding page (changing the host app's CSS or DOM, for example)."

Our decision here is simple: we decided that, while we won't encourage this sort of thing (not least because it's unsupported and therefore subject to break when Streamlit is updated), we're fundamentally ok with it. Official theming support is on the Streamlit roadmap for 2021, but if enterprising developers want to hack on Streamlit and create this sort of thing before we officially ship it, we won't stand in their way.

Streamlit is an open source project anyway; if you don't like the way something works or looks, you can just fork the project and change it. We don't need a Component sandbox to enforce a rule that's incompatible with our open source nature.

And more importantly, "don't hijack Streamlit Sharing"
"A Component in an app deployed with Streamlit sharing shouldn’t be able to hijack its owner's Sharing credentials and execute a CSRF exploit."

We need to ensure that a malicious Component - or any other rogue code that could be running within a Streamlit app - cannot execute Streamlit sharing commands surreptitiously.

Googling for "CSRF example" will return all sorts of resources that explain this type of exploit in detail. The important thing to know is that CSRF attacks use the fact that each HTTP request made by a browser will include the cookies associated with site to which the request is made. (There are various cookie attributes that make this story slightly more complex, but that's the basic rule.)

When you're logged into Streamlit sharing and visit a deployed app you own, you get a management dashboard that lets you view logs and perform various administrative tasks:

If the Streamlit sharing administrator wrapper is served from the same HTTP origin as the app it's managing, a malicious Component could bypass Sharing's CSRF protections by making requests against the Streamlit sharing API and reading the CSRF token from the response headers.

The solution to this doesn't involve relying on Component sandboxing. In Streamlit sharing, an app's admin dashboard is simply served from a different origin than the app itself. This is similar to a "serve Components from a different origin" prototype we'd rejected on the basis of making deployment more complicated for users - but with the burden pushed up to Streamlit sharing instead. (We are more than happy to make deployment more complicated for the Streamlit sharing engineers while keeping it simple for you. Sorry not sorry, friends!)

Wrapping up

Within the Streamlit engineering team, the phrase "allow same origin" has practically achieved meme status - it was the issue we were always right on the verge of coming to consensus on. Throughout much of 2020, during our start-of-month company-wide planning meetings, the Components team kept claiming that we were about to make a final decision, only to walk back our over-eager prediction shortly afterwards.

But the saga is finally over! We've jettisoned the allow-same-origin iframe sandbox flag, and now you can stream webcam video, add a Disqus forum, embed Tweets, and use a whole host of other Components that were previously out of reach. Most importantly, your Streamlit sharing apps will remain safe from malicious code.

And if you're lucky, you'll never have to think about null origins, CORS, or CSRF again. We'll handle it.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

collaborate-1.png (2000×849)
https://blog.streamlit.io/content/images/2021/11/collaborate-1.png


Batch Input Widgets | Introducing Submit Button & Forms
https://blog.streamlit.io/introducing-submit-button-and-forms/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing Submit button and Forms 📃

We're releasing a pair of new commands called st.form and st.form_submit_button!

By Abhi Saini
Posted in Tutorials, April 29 2021
Getting started
What this does
Form submit button
Similarities
Differences
Form Features
Forms inside columns
Columns inside forms
Helpful errors and warnings
Limitations and other notes
Wrapping up
Resources
Contents
Share this post
← All posts

Have you ever tried to build an app around a complex Machine Learning model, and found that rerunning the model every time the user changed an input value led to a less than ideal user experience? If so, it's likely because the parameters being supplied to your model use input widgets like st.text_input, st.number_input or st.slider and any time you change a widget the entire app is re-run.

To help solve this we're introducing a pair of commands called st.form and st.form_submit_button. This lets you batch input widgets together and submit the widget values with the click of a button — triggering only a single rerun of the entire app!

Check out this sample app which shows the new commands in action, but make sure to keep reading if you want to learn more about how it works.

Getting started

Forms are like any other Streamlit container and can be declared using the with statement:

# Using the "with" syntax
with st.form(key='my_form'):
	text_input = st.text_input(label='Enter some text')
	submit_button = st.form_submit_button(label='Submit')


Or, if you prefer, you can also use object notation:

# Declare a form and call methods directly on the returned object
form = st.form(key='my_form')
form.text_input(label='Enter some text')
submit_button = form.form_submit_button(label='Submit')


In your app, this creates a form with a submit button as follows:

What this does

Outside of forms, any time a user interacts with a widget the app's script is rerun. What st.form does is make it so users can interact with the widgets as much as they want, without causing a rerun! Instead, to update the app, the user should click on the form's submit button.

Form submit button

In the example above, notice that st.form_submit_button is in some ways similar to st.button, but differs in others:

Similarities

Just like the regular st.button, the submit button for a form, st.form_submit_button returns a boolean to indicate whether the form was submitted or not. This allows for building additional logic upon submit button. For e.g.

form = st.form(key='my-form')
name = form.text_input('Enter your name')
submit = form.form_submit_button('Submit')

st.write('Press submit to have your name printed below')

if submit:
    st.write(f'hello {name}')

Differences
A Form submit button is a special button which batch submits the state of the widgets contained in the form.
A form must have an associated st.form_submit_button otherwise Streamlit throws an error.
Form Features

st.form can be placed anywhere in a Streamlit app, you can even put columns inside of forms, or forms inside of columns, and everything will work as expected!

Forms inside columns
col1, col2 = st.beta_columns(2)

with col1:
    with st.form('Form1'):
        st.selectbox('Select flavor', ['Vanilla', 'Chocolate'], key=1)
        st.slider(label='Select intensity', min_value=0, max_value=100, key=4)
        submitted1 = st.form_submit_button('Submit 1')

with col2:
    with st.form('Form2'):
        st.selectbox('Select Topping', ['Almonds', 'Sprinkles'], key=2)
        st.slider(label='Select Intensity', min_value=0, max_value=100, key=3)
        submitted2 = st.form_submit_button('Submit 2')


Changing the contents of a form in the left column or submitting the left form does not impact the form on the right and vice versa.

Columns inside forms
with st.form(key='columns_in_form'):
    cols = st.beta_columns(5)
    for i, col in enumerate(cols):
        col.selectbox(f'Make a Selection', ['click', 'or click'], key=i)
    submitted = st.form_submit_button('Submit')


Changing the contents of any one of the select boxes does not impact the other select boxes and a rerun is triggered upon clicking the submit button.

Helpful errors and warnings

To help developers in certain situations where it detects an out-of-place or missing submit button, Streamlit will shows a warning or throws an exception in the following cases:

If a st.form_submit_button is defined without a form scope, Streamlit will throw an exception and stop execution.
If no submit button is defined, Streamlit will show a warning without interrupting the app flow.

Limitations and other notes
An  st.form cannot be embedded inside another st.form.
Every form must have an associated st.form_submit_button.
By definition, st.buttons do not make much sense within a form. Forms are all about batching widget state together, but buttons are inherently stateless. So declaring an st.button inside a form will lead to an error.
Also by definition, interdependent widgets within a form are unlikely to be particularly useful. If you pass the output of widget1 into the input for widget2 inside a form, then widget2 will only update to widget1's value when the form is submitted.
We are currently working on functionality that allows you to programmatically reset all widgets when the submit button is clicked. This should be added in an upcoming release.
Wrapping up

You can now add st.form and st.form_submit_button to your apps to help make them more responsive in just two lines of code!

So go ahead and upgrade Streamlit to version 0.81.0 today!

pip install --upgrade streamlit


We're excited to see how you'll use these new commands, so make sure to come share what you create on our forum or on Twitter. If you have any questions about these (or about Streamlit in general) let us know on the forum or in the comments below! 🎈

Resources
Form docs
Github
Forum
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

App Layout & Style Tips | Designing Apps for User (Part II)
https://blog.streamlit.io/designing-streamlit-apps-for-the-user-part-ii/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to make a great Streamlit app: Part II

A few layout and style tips to make your apps look even more visually appealing!

By Abhi Saini
Posted in Tutorials, June 22 2021
Lay it out in columns
Use layout to focus attention in a direction
Theming, Colors and Contrast Ratios
Colors and contrast
Text Sizing and Emphasis
Make it fun with logos, emojis and badges
Add logos
Add Favicons, Page Titles etc.
Add emojis and badges
Wrapping up
Resources
Additional References on choosing colors:
Contents
Share this post
← All posts

At Streamlit, we strive to enable developers to easily make beautiful apps that create great experiences for their users. In Part I of this blog series, we covered how to Design for the User.  In Part II, we jump into how you can use layout and design options to make visually appealing apps for your app viewers.

Kicking off this post, the main things to keep in mind are:

Lay it out in columns
Choose your theme colors and contrast ratios wisely
Use text sizing for emphasis
Make it fun with badges, logos and emojis 🥳

Let's jump in!

Lay it out in columns

Organizing your app with column layouts with st.beta_columns helps place content in a uniform way across the app. A great example is Streamlit Cheat Sheet (source code) by Daniel Lewis. Having information represented in columns create a proportional distribution of information and gives consistency to the interface.

Additionally, this page uses wide mode to show even more content. Page contents can be laid out in 'centered' or 'wide' mode(as shown below) using st.set_page_config :

# NOTE: This must be the first command in your app, and must be set only once
st.set_page_config(layout="wide")

Use layout to focus attention in a direction

Horizontal Flow

In this type of layout, we want the viewer to scan information from left to right, for eg. the NYC Uber Ridesharing App (source code) by Streamlit. In the first row, we see 3 charts containing similar conceptual information, i.e., airport pickups shown left to right. In the 2nd row, we see pickups per minute represented across the minutes of the hour.

The above layout (with one columns with double and the rest with single width) can be achieved with this code:

col1, col2, col3, col4 = st.beta_columns((2,1,1,1))

with col1:
    # Add chart #1

...

with col4:
    # Add chart #4

# Add bottom chart



Vertical Flow with a Sidebar

In this type of layout, information is presented vertically and the viewer is encouraged to consume the data starting at the top and then scrolling to the bottom. This often carries with it the implicit understanding that an item presented is logically dependent on the item above it. For e.g. the PGA Modeler app (source code) by Andy Uttley.

The above layout has a sidebar which contains all the input widgets like sliders, buttons, checkboxes on the left and all of the output widgets like charts, images, data tables etc. in the main body of the app. This layout can be achieved with the following code:

# Input widgets in the sidebar
with st.sidebar:
	# input widget 1
	# input widget 2
	...

# Load data based on the inputs from the sidebar widgets

# Main body contents: Output Widgets
st.dataframe(data)
...
st.line_chart()

Theming, Colors and Contrast Ratios

Streamlit supports Theming and Dark mode, which allows you to choose between multiple default (light, dark) themes or create your own. Get your company colors in there or just pick something you love. For an example, see the theming launch sample app (source code) by Streamlit.

Colors and contrast

When picking your colors take some time to think about how they work together and the contrast between your light and dark colors. If two adjacent elements have the same color or very similar colors, it makes the content very difficult, if not impossible, to read. When contrast is low between text and background, the message blends together, reducing legibility of the display. Here is a great article on the Role of Color in UX that can help you pick great colors for your app theme

Text Sizing and Emphasis

Having text with different sizing can reduce the cognitive load on the reader since it creates a hierarchy of importance. Bigger text generally draws more user attention!

There are several st commands which allow the user to do semantic titling. Each of these commands: st.title, st.header, st.subheader allows the user to create emphasis and use vertical space to give the page more structure. Below, we compare the same text written using different text sizing.

You can also use  st.caption to display non-important information for captions below plots, tables or paragraphs.

A good rule of thumb is to start with a title via st.title and have a short 2-sentence description via st.write of the app below it to explain more about how the app works. See a great example of this in the Lord of the Rings Text Generator by Christian Doucette.

Make it fun with logos, emojis and badges
Add logos

With just a couple of lines of code, you can add your company's logo to an app to reflect your company's branding:

st.image(logo_url, width=100)
st.title("Streamlit Dashboard Demo")


And if you want the logo next to your text, just use st.beta_columns.

Add Favicons, Page Titles etc.

st.set_page_config allows you to set some default settings for the page like the Page Title, icon, sidebar state etc:

st.set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="🧊",
    layout="wide",
    initial_sidebar_state="expanded"
)

Add emojis and badges

At Streamlit, we love emojis and like it when apps use them in interesting ways. They can be added to titles as shown in the Year on Github app by Johannes Reike (source code) shown below.

Additionally, try adding in badges for your users to connect with your project on social media.

To add a badge to star your Github project (replace <username> and <repo>):

st.write("[![Star](<https://img.shields.io/github/stars/><username>/<repo>.svg?logo=github&style=social)](<https://gitHub.com/><username>/<repo>)")


Or follow you on Twitter:

st.write("[![Follow](<https://img.shields.io/twitter/follow/><username>?style=social)](<https://www.twitter.com/><username>)")


Or create other badges via https://shields.io/ (especially the Social category) and add them to your app with markdown similar to above.

💡 Pro tip: You can also consider adding a Streamlit badge to your GitHub repo.

Wrapping up

We hope that by following these pointers, your Streamlit apps will look great and be more fun for your users. To use the features discussed above in your apps, make sure to upgrade to the latest version of Streamlit:

pip install --upgrade streamlit


In Part 3, of the blog series, we'll cover how to make your apps more performant so that using them is a fast and zippy experience for everyone.

If you have any questions let us know below or on the forum!

Resources
Github
Forum
Additional References on choosing colors:
Fundamentals of color in user interface design (UI)
UI Design: Choosing Color Palettes
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

What is Apache Arrow, How it Works & More| Streamlit
https://blog.streamlit.io/all-in-on-apache-arrow/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
All in on Apache Arrow

How we improved performance by deleting over 1k lines of code

By Henrikh Kantuni
Posted in Product, July 22 2021
What is Arrow, and how does it work
What this means for your Streamlit apps
Faster and more efficient DataFrames
Other benefits to using Arrow
How do I use it
Sharp edges
Wrapping it up
Resources
Contents
Share this post
← All posts

A long time ago in a galaxy far, far away, among the first few lines of code ever committed to Streamlit was a painstakingly crafted module that serialized Pandas DataFrames (read "fancy tables") into a complex set of custom Protobufs (read "fancy JSON") — plus the inverse of that module to deserialize them back into arrays in the browser.

Let's backtrack for a second. Why is this even needed? As you know, a great portion of Streamlit commands receive DataFrames as input arguments. It makes sense: DataFrames are efficient, versatile, and easy to work with. We all love them. They were a complete game-changer when they first appeared in the Python ecosystem. And while your DataFrames hung out in Python land, everything was hunky-dory. But as soon as we had to send them to the JavaScript territories, there be dragons: DataFrames were just not traditionally suited to be sent over-the-wire, and there was no standard JavaScript library to handle them on the browser side anyway. Hence, all the custom code.

And this came with a cost...

Our custom format grew considerably slower as users pushed for larger and larger DataFrames. Furthermore, every time Pandas released new features, adding support for them in Streamlit meant undertaking a considerable amount of work. Finally, we weren't big fans of our old custom module, to begin with!

So the search for a better format began. We spent months researching different solutions, trying out serialization formats, embarking on false starts, and going right back to where we started. Our old code still checked more boxes than all the options out there.

Until finally, we found it!

Yes, this blog post is about the TV show.

Meet Arrow, an efficient memory format and set of libraries that will handle Streamlit's DataFrame serialization from now on. We love it, and we think you will too.

What is Arrow, and how does it work

Arrow is a memory format for DataFrames, as well as a set of libraries for manipulating DataFrames in that format from all sorts of programming languages. From the Arrow website:

"A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (including nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more."

Let's break that down:

It's column-oriented: so doing things like computing the sum of a column of your DataFrame is lightning fast.
It's designed to be memory-mapped: so serialization/deserialization are basically free. Just send the bytes over the wire as they are.
It's language-agnostic: so it's well-supported both in Python and JavaScript.
Plus it supports all those DataFrame features that our custom module never got around to. This means that adding support to new DataFrame features into Streamlit is a much, much smaller undertaking.

What this means for your Streamlit apps
Faster and more efficient DataFrames

In our legacy serialization format, as DataFrame size grew, the time to serialize also increased significantly. Iterating through the DataFrame, converting all its data into formats we could use, packing them into a whole hierarchy of Protobufs, that's a whole lot of work that Arrow's memory-mapped format just throws out the window. Just compare the performance of our legacy format vs Arrow. It's not even funny!

Other benefits to using Arrow
As new features are introduced to Arrow, Streamlit can support them with much less work. Remember those features we never got around to supporting? Well, we just added a bunch of them now: table captions, categorical indices, interval indices, multi-index Styler objects, and more!
If your app uses Arrow Tables, Streamlit will now accept them anywhere a Pandas DataFrame is accepted. Except it's, you guessed it, much faster.
This one is mostly a benefit for us, Streamlit devs: we get to delete over 1k lines of code from our codebase. You can't believe how good this feels 😃
How do I use it

Just upgrade to the latest version of Streamlit! Arrow is the default serialization format starting with version 0.85:

pip install --upgrade streamlit

In the vast majority of cases, no updates to your code are needed! You can still give Streamlit your Pandas DataFrames just as before, and we'll convert it to Arrow behind the scenes for you.

Sharp edges
Arrow is a bit more strict than Pandas about keeping your DataFrames organized. For example, elements in the same column of a DataFrame must all have the same data type. But we find that this is an overall benefit to your data model, and valid reasons for breaking this rule are rare.
Some features such as PeriodIndex and TimedeltaIndex are not yet fully supported. But, as mentioned earlier, adding them is easier than ever.
Styler concatenation is no longer supported when using add_rows(). This is not really an Arrow-specific issue, but more that when you concatenate Styler objects it may not do what you wanted: if the Styler is configured to draw the max of a column in red, for example, then a correct concatenation would require recomputing the max for the entire column. Which is... suboptimal.
This is a big change, and even though we tested it thoroughly, there may still be bugs lurking around.

If you encounter a bug (or the caveats above are blocking for you), you can revert to the previous implementation by setting the dataFrameSerialization config option to "legacy" in your config.toml as shown below:

[global]
dataFrameSerialization = "legacy"


And if you do that, please file an issue on GitHub so we can work on fixing whatever issue you found ASAP.

Wrapping it up

Arrow is the new hotness, and it's where we believe the ecosystem is moving. So we're super excited to finally jump on that rocketship and help propel it forward with all of you.

Have fun with the updates, and looking forward to hearing what you think! As usual, come share what you create on the forum or Twitter. If you have any questions about Arrow or Streamlit, or just want to say hi, leave a comment below or on the forum. 🎈

Resources
Documentation
GitHub
Forum

Thanks to Abhi Saini, Alex Reece, Amanda Kelly, Jon Roes, Marisa Smith, and Tim Conkling for their input on this article.
Massive shout-out to Thiago Teixeira and TC Ricks for the enormous work they did to create this beauty!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Launching a brand-new docs site 🥳
https://blog.streamlit.io/launching-a-new-docs-site/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Launching a brand-new docs site 🥳

Improved layout, easier navigation, and faster search

By Snehan Kekre
Posted in Product, October 13 2021
📙 Overview
🔎 API reference
🎓 Knowledge base
🎁 Wrapping up
Contents
Share this post
← All posts

Hey Streamlit community! 👋

We're thrilled to share with you our brand-new docs site. It's a standalone site designed to be your single reference point for all things Streamlit.

Without further ado, here's what's new:

📙 Overview

We've updated our docs with beautiful graphics, a new structure, and a clear delineation between:

Our core product—Streamlit library 📄
Our commercial offering—Streamlit Cloud ☁️
Our catchall resource—Knowledge base 🎓

This layout was designed with new and returning users in mind. It makes navigation easier, improves search, and increases the discoverability of relevant pages (especially Streamlit functions).

For example, if you're learning Streamlit, go to the Get started page in the Streamlit library. You'll learn how Streamlit works, how to install it, and how to create your first app!

If you're deploying a Streamlit app, go to the Streamlit Cloud page. You'll find links and instructions on how to deploy on our free Community tier and on the Teams and Enterprise tiers.

If you run into issues using Streamlit or deploying apps, head to our Knowledge base.

🔎 API reference

One of the most highly requested features was a dedicated page for each Streamlit function. We've done exactly that in our new API reference.

Each page now supports multimedia and YouTube links, references to blog posts, discussion forum links, and lots of other resources. Plus, each Streamlit function now has its own dedicated URL. This makes each module more easily discoverable on search engines.

🎓 Knowledge base

Our documentation now includes a growing repository of articles that answer your questions about creating and deploying Streamlit apps. Previously, if your question wasn't answered in the FAQ, you had to search for it in our forums.

We aggregated the most common questions and discussions within the community to bring you all the answers in one place—the Knowledge base.

If you have trouble installing dependencies, deploying apps, or have a question about using Streamlit with your favorite libraries, our Knowledge base has got you covered.

By definition, a knowledge base is a continual work in progress. There are bound to be edge-cases and issues that the majority of the community has never come across. When working on the cutting-edge of data science, you're always faced with unknowns.

If you have a solution for an issue and want to see it in the Knowledge base, please write an article! Include as many details as possible. Here are the instructions on how to contribute. If you see anything that can be improved in existing articles, create a new issue, submit a pull request, and let us know in the comments below or on the forum.

🎁 Wrapping up

This launch marks a milestone in the development of our open-source product, Streamlit, and our commercial offering, Streamlit Cloud. Alongside the Streamlit 1.0 release, our documentation has received major visual and structural updates:

Streamlit library and Streamlit Cloud sections.
API reference with dedicated pages for each Streamlit function.
Knowledge base as a self-serve library of tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.

These updates bring us closer to two goals:

Reducing the time and effort required to find answers to your questions about Streamlit.
Putting relevant information in the hands of new users and experienced developers.

We're excited for you to explore our new documentation site and hear what you think.

Happy Streamlit-ing! ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

current-process.png (1386×1058)
https://blog.streamlit.io/content/images/2021/11/current-process.png


data-apps.png (1799×1292)
https://blog.streamlit.io/content/images/2021/11/data-apps.png


Store Information Across App Interactions | Session State
https://blog.streamlit.io/session-state-for-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Session State for Streamlit 🎈

You can now store information across app interactions and reruns!

By Abhi Saini
Posted in Product, July 1 2021
Add State to your App
Callback functions and Session State API
Wrapping up
Resources
Contents
Share this post
← All posts

Soon after Streamlit launched in 2019, the community started asking for ways to add statefulness to their apps. Hacks for Session State have been around since October 2019, but we wanted to build an elegant solution that you could intuitively weave into apps in a few lines of code. Today we're excited to release it!

You can now use Session State to store variables across reruns, create events on input widgets and use callback functions to handle events. This powerful functionality helps create apps which can:

Perform data/image annotation
Support Pagination
Add widgets that depend on other widgets
Build simple stateful games like Battleship, Tic Tac Toe, etc.
And much more - all of this with the simplicity of writing apps that are Python scripts!

💡 If you want to jump right in, check out our demo to see some of the above apps in action or head to the docs for more detailed info on getting started.

Add State to your App

In Streamlit, interacting with a widget triggers a rerun and variables defined in the code get reinitialized after each rerun. But with Session State, it's possible to have values persist across reruns for those instances when you don't want your variables reinitialized.

For example, here's a simple counter that maintains a count value across multiple presses of an increment button. Each button press triggers a rerun but the count value is preserved and incremented (or decremented) across the rerun:

import streamlit as st

st.title('Counter Example')

# Streamlit runs from top to bottom on every iteraction so
# we check if `count` has already been initialized in st.session_state.

# If no, then initialize count to 0
# If count is already initialized, don't do anything
if 'count' not in st.session_state:
	st.session_state.count = 0

# Create a button which will increment the counter
increment = st.button('Increment')
if increment:
    st.session_state.count += 1

# A button to decrement the counter
decrement = st.button('Decrement')
if decrement:
    st.session_state.count -= 1

st.write('Count = ', st.session_state.count)


💡 To continue building on this example, follow along in our Topic Guide: Add State to your App 🤓

The above shows a basic example of how values can persist over reruns, but let's move on to something a little more complex!

Callback functions and Session State API

As part of this release, we're launching Callbacks in Streamlit. Callbacks can be passed as arguments to widgets like st.button or st.slider using the on_change argument.

💡 Curious what a callback is? Wikipedia phrases it well: "a callback, also known as a "call-after" function, is any executable code that is passed as an argument to other code; that other code is expected to call back (execute) the argument at a given time. " Here's a link if you'd like to read more.

With Session State, events associated with changes to a widget or click events associated with button presses can be handled by callback functions. It's important to remember the following order of execution:

Order of Execution: If a callback function is associated with a widget then a change in the widget triggers the following sequence: First the callback function is executed and then the app executes from top to bottom.


Here's an example:

import streamlit as st

def update_first():
    st.session_state.second = st.session_state.first

def update_second():
    st.session_state.first = st.session_state.second

st.title('🪞 Mirrored Widgets using Session State')

st.text_input(label='Textbox 1', key='first', on_change=update_first)
st.text_input(label='Textbox 2', key='second', on_change=update_second)


In the above, we showcase the use of callbacks and session state.  We also showcase an advanced concept, where session state can be associated with widget state using the key parameter.

To read more on this, check out the Advanced Concepts section in the Session State docs and to check out the API in detail visit the State API documentation.

Wrapping up

That's it for the intro to Session State, but we hope this isn't the end of the conversation! We're excited to see how you'll use these new capabilities, and all the new functionalities state will unlock for the community.

To get started, upgrade to the latest release to use st.session_state and callbacks in your apps:

pip install --upgrade streamlit


If you have any questions about these (or about Streamlit in general) let us know below in the comments or on the forum. And make sure to come by the forum or Twitter to share all the cool things you make! 🎈

Resources
Session State Topic Guide  
Session State API Reference
Session State Demo App
Github
Forum
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

TC Ricks - Streamlit
https://blog.streamlit.io/author/tc/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by TC Ricks
4 posts
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Monthly rewind > January 2021

Your January look back at new features and great community content

Monthly Rewind
by
TC Ricks
,
February 8 2021
Adding beta and experimental “channels” to Streamlit

Introducing the st.beta and st.experimental namespaces

Product
by
TC Ricks
,
May 6 2020
Try Nightly Build for cutting-edge Streamlit

A new style of release for anyone who wants the most up-to-date Streamlit version

Product
by
TC Ricks
,
April 17 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Thiago Teixeira - Streamlit
https://blog.streamlit.io/author/thiago/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Thiago Teixeira
2 posts
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

Product
by
Adrien Treuille and 
2
 more,
March 2 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Announcing Streamlit 1.0! 🎈
https://blog.streamlit.io/announcing-streamlit-1-0/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Announcing Streamlit 1.0! 🎈

Streamlit used to be the simplest way to write data apps. Now it's the most powerful

By Adrien Treuille
Posted in Product, October 5 2021
Apps were just scripts!
Streamlit 1.0! 🎈
Contents
Share this post
← All posts

We launched Streamlit in 2019 with a radical idea: making data apps should be simple. Your response exceeded our highest hopes. Tens of thousands of data scientists and thousands of companies turned to Streamlit to share rich models, deep analyses, and complex datasets.

Overnight, expectations became sky-high. You filled our forums. You inundated our issue boards. You wanted Streamlit to be more beautiful, more powerful, more programmable, and faster.

Inspired by this swell of energy, we set out to make your dreams real. But the features you wanted threatened to complicate Streamlit. How could we make Streamlit a powerful, production-ready app framework while preserving its core simplicity?

We had to boil it down. Why was Streamlit simple? What made it so special?

The answer was our initial insight...

Apps were just scripts!

These two lines said it all:

x = st.slider("Select a value")
st.write(x, "squared is", x * x)

In just two lines of Python, Streamlit dissolved all complexity of app development: layout, input, output, interaction, and callbacks. The learning curve was zero! ✨

So far so good. But as data scientists, we didn't just square numbers. We built models. We conjured visualizations. We wrangled datasets. We harmonized geographic data with sentiment analysis and feature libraries. We shaped thinking.

Could Streamlit's simple scripting model scale?!

For help, we turned to Streamlit's guiding light. The community. You. We asked you questions. You listened and responded. Together, we worked on countless revisions to the APIs. We simplified. Then we simplified even more.

One by one, we tackled the challenges of making each of your dreams real:

1. Layouts. The first step was to make beautiful layouts that danced and reacted to user input. How could we fit this into Streamlit's simple scripting model?

This led us to code like this:

airports = ["La Guardia Airport", "JFK Airport", "Newark Airport"]
for airport, col in zip(airports, st.columns(len(airports)):
   with col:
      st.subheader(airport)
      render_airport(airport)

See how the columns fit perfectly into a simple for loop? Awesome! A new layout superpower! 💪

2. Components. Next, we wanted to transcend the structures of Streamlit's 37 core functions. So we added support for third-party components:

from webcam_component import webcam

captured_image = webcam()
if captured_image is not None:
   st.image(captured_image)

You swept in and filled our gallery with an incredible ecosystem of components. Now Streamlit apps could refract a full rainbow of web technologies:

3. State. Up next, we worked on state, adding new complexity in a few lines of code:

if st.button('Increment'):
    st.session_state.count += 1
st.write('Count = ', st.session_state.count)

4. Speed. You wanted speed. So we introduced powerful new caching primitives that made your apps fast:

@st.experimental_singleton
def connect_to_database(url):
   engine = create_engine(url)
   return sessionmaker(engine)

@st.experimental_memo
def query_database(_db):
   with _db() as session:
      query = session.query(user.id, user.last_name, user.ltv)
   return pd.read_sql(query.statement, query.session.bind)

With each challenge conquered, our confidence and excitement grew. Streamlit's core developers joined with the community to build more and more features, faster and faster.

But you didn't stop there. You hacked at our rough edges. You tore apart our abstractions. You encouraged us to make Streamlit even better, even simpler, and infinitely more powerful. You believed in us.

And then it happened. We have built powerful new features and preserved Streamlit's core simplicity. The abstractions scaled!

On the wave of this excitement, a new milestone came into view...

Streamlit 1.0! 🎈

This signifies the end of our first journey:

We've grown from three co-founders to a team of almost 50 (we're hiring!).
Our community has grown beyond our wildest dreams with more than 4.5 million downloads.
Streamlit now has more than 16,000 GitHub stars and is used by more than 10,000 organizations (including over half of the Fortune 50).

Funnily enough, we feel like we're back where we started. Once again, our community wants powerful features. But this time it's different. This time we know that the Streamlit model will scale.

So today we're also sharing with you our new roadmap:

Magical apps. We already made it 10x faster for you to make great apps. Now we want to make those apps even better. We'll be adding an unbeatable set of widgets—everything from sortable/filterable/editable databases and tables to clickable charts, to image selectors and editors, to amazing audio and video players and uploaders (and more options for layout and customization!).
First-class developer experience. We want everything about coding a Streamlit app to be an awesome experience. So we'll make it easier for you to connect to data sources, cache data, interact with it, and debug your apps.
Enhanced user experience. We want to help you make great apps for your users. We'll be designing a distinct user experience. App users will be able to understand the app, interact with it, and give you direct feedback.
Rapidly expanding ecosystem. You wanted it to be even easier for fellow developers to share code, components, apps, and answers. So we'll be launching new features that make it super simple to get started with new apps, find code snippets, search for the right add-on components, engage with the community, and get recognized for your contributions.

Oh, and one more exciting thing. We’re committing to follow a loose variant of semantic versioning. For details, see our upcoming release notes.

That's it for the Streamlit 1.0 announcement!

We're very proud to have shared our journey with you—our magical community. You are what makes Streamlit special. Thank you for inspiring us with your feedback, enthusiasm, and creativity. Please keep sharing your apps with the world. And keep sending us comments, ideas, bugs, feature requests, articles, and words of encouragement.

We wouldn't be here without you. ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Product Announcements
https://blog.streamlit.io/tag/product/page/4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Product
36 posts
New layout options for Streamlit

Introducing new layout primitives—columns, containers, and expanders!

Product
by
Austin Chen
,
October 8 2020
Introducing Streamlit Components

A new way to add and share custom functionality for Streamlit apps

Product
by
Adrien Treuille
,
July 14 2020
Announcing Streamlit's $21M Series A

Developing new superpowers for the data science community

Product
by
Adrien Treuille
,
June 16 2020
Adding beta and experimental “channels” to Streamlit

Introducing the st.beta and st.experimental namespaces

Product
by
TC Ricks
,
May 6 2020
Try Nightly Build for cutting-edge Streamlit

A new style of release for anyone who wants the most up-to-date Streamlit version

Product
by
TC Ricks
,
April 17 2020
The Streamlit roadmap—big plans for 2020!

Devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers

Product
by
Adrien Treuille
,
February 27 2020
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

soc-2-small-long.png (1500×506)
https://blog.streamlit.io/content/images/2022/01/soc-2-small-long.png


Henrikh Kantuni - Streamlit
https://blog.streamlit.io/author/kantuni/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Henrikh Kantuni
2 posts
All in on Apache Arrow

How we improved performance by deleting over 1k lines of code

Product
by
Henrikh Kantuni
,
July 22 2021
Elm, meet Streamlit

A tutorial on how to build Streamlit components using Elm

Tutorials
by
Henrikh Kantuni
,
December 8 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

0.88.0 release notes
https://blog.streamlit.io/0-88-0-release-notes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
0.88.0 release notes

This release launches st.download_button as well as other improvements and bug fixes

By Abhi Saini
Posted in Release Notes, September 3 2021
✨ Release Highlight
⬇️ Download Button
API Details
Parameters
Returns
Example Usage
🧩 Other notable updates
🏁 Wrapping up
Contents
Share this post
← All posts

Hey Streamlit Fam! 👋

Say hello to a notable new feature in the 0.88.0 release: st.download_button. Now your viewers can download data right from your Streamlit apps!

In this post, we'll go into more detail about the new download button and other updates.

Want to jump right in? Here's a sample app and here are the docs.

✨ Release Highlight
⬇️ Download Button

Before this release, you had to hack your way to the download functionality within Streamlit. None of the hacks were great. And they didn't play well with the Streamlit Cloud platform.

With st.download_button you can use the download functionality both locally and within our cloud platform.

Curious how it's built?

API Details

streamlit.download_button(label, data, file_name=None, mime=None, key=None, help=None, on_click=None, args=None, kwargs=None)

This will display a button for your viewers so they can download data locally to their machines.

Parameters
label (str) – A short label explaining to the user what this button is for.
data (str or bytes or file) – The contents of the file to be downloaded.
file_name (str) - An optional string to use as the name of the file to be downloaded, eg. 'my_file.csv'. If file_name is not specified, a name will be automatically generated.
mime (str or None) – The MIME type of the data. If data=str and MIME is unspecified, then MIME defaults to “text/plain”. If data=bytes and MIME is unspecified, then MIME defaults to “application/octet-stream”.
key (str) – An optional string to use as the unique key for the widget. If you omit this, a key will be generated for the widget based on its content. Multiple widgets of the same type may not share the same key.
help (str) – An optional tooltip that gets displayed when the button is hovered over.
on_click (callable) – An optional callback invoked when this button is clicked.
args (tuple) – An optional tuple of args to pass to the callback.
kwargs (dict) – An optional dict of kwargs to pass to the callback.
Returns

If the button was clicked on the last run of the app, it will be True. Otherwise, it will be False. Additionally, the return type is bool.

Example Usage
# Text files

text_contents = '''
Foo, Bar
123, 456
789, 000
'''

# Different ways to use the API

st.download_button('Download CSV', text_contents, 'text/csv')
st.download_button('Download CSV', text_contents)  # Defaults to 'text/plain'

with open('myfile.csv') as f:
	st.download_button('Download CSV', f)  # Defaults to 'text/plain'

# ---
# Binary files

binary_contents = b'whatever'

# Different ways to use the API

st.download_button('Download file', binary_contents)  # Defaults to 'application/octet-stream'

with open('myfile.zip', 'rb') as f:
	st.download_button('Download Zip', f, file_name='archive.zip')  # Defaults to 'application/octet-stream'

# You can also grab the return value of the button,
# just like with any other button.

if st.download_button(...):
	st.write('Thanks for downloading!')

🧩 Other notable updates

Below are some other notable updates on this release:

🛑 We made changes to improve the redacted exception experience on Streamlit Cloud. When client.showErrorDetails=true exceptions display the Error Type and Traceback, the actual error text will be redacted to prevent data leaks. [3713]
🖥️ Macs are set to verify SSL in Python. If certificates aren't installed, a SSL: CERTIFICATE_VERIFY_FAILED error propagates. We removed HTTPS in order to solve this issue. [3744]
🔑 Integers can now also be used as keys in widget declarations. This helps community members who use integers for keys. This change ensures integer keys are converted to strings. [3697]

Click here to check out all updates.

🏁 Wrapping up

Thanks for checking out the release notes for 0.88.0. You can always see the most recent updates on our change-log or via this tag on the forum.

Feel free to let us know in the comments below if you have any questions. We're looking forward to hearing what you think about the new download button!

Happy Streamliting. 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Release Notes...

View even more →

Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

New Funding Round Led by Sequoia: $35 Million Series B
https://blog.streamlit.io/our-35-million-series-b/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Our $35 million Series B

We’re excited to announce a new funding round led by Sequoia 🌲

By Adrien Treuille
Posted in Product, April 7 2021
Share this post
← All posts

We launched Streamlit in 2019 to make app creation as easy as Python scripting. Over the past two years we have obsessed over making this alchemy — from scripts to apps — as fast and joyful as possible. The world has changed so much since we launched, but the importance of sharing data insights has only increased. And while we gave you an open-source app-creating "superpower," it was you all in the community who showed us the true power of data apps.

You've used Streamlit to turn ad-hoc analyses into workflows that supercharge your company. You've turned datasets into data experiences that you share with the world. You've made interactive data apps a cornerstone of your work, bridging understanding between disparate groups and helping everyone make better data-informed decisions. It's been amazing to see top companies and developers create hundreds of thousands of Streamlit apps used by millions of people worldwide.

Today, we're excited to announce a $35 million Series B investment led by Sequoia and backed by our existing investors Gradient Ventures and GGV Capital. The confidence of our investors reflects the rapid growth and vibrance of the amazing Streamlit community, as well as the successful blossoming of our commercial app deployment platform, Streamlit for Teams. We wanted to take this moment to thank all of those who helped get us here, and give a preview of the future.

To the Streamlit community, our app developers, forum posters, open source contributors, Streamlit Creators, and everyone who just filed a bug or shared a cool app: thank you! You all are the essential core of Streamlit success and the wind behind our sails. To Streamlit's employees: it's harder to imagine a more fun group of people to see every day, even if only in little postage stamp windows across video calls. You are an incredible source of energy and ideas which make it exciting to work at Streamlit every day. And to all of our loved ones and the families of everyone involved with building Streamlit, thank you for putting up with us when we've worked late nights and weekends. It would be impossible to do this without your understanding of our weird company-building obsession.

Stay tuned! This past year has been huge, with our components framework, customizable layout, Streamlit sharing, custom theming, and a host of other new features. The coming year will be even bigger, with more app features, programmable state, speed improvements, multipage apps, and tons of enterprise security features. Please continue to reach out with more great apps, articles, ideas, components, bugs, feature requests, code snippets, and words of encouragement! We wouldn't be here today without each one of you.🎈

🚀Adrien Treuille, Amanda Kelly, and Thiago Teixeira - Streamlit Co-founders

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

contour-plot-with-plotly.png (2000×956)
https://blog.streamlit.io/content/images/2022/12/contour-plot-with-plotly.png#border


Snehan Kekre - Streamlit
https://blog.streamlit.io/author/snehan/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Snehan Kekre
Snehan's a Developer Advocate at Snowflake, where he specializes in the Streamlit open-source Python library and leads the Documentation team.
1 post
Launching a brand-new docs site 🥳

Improved layout, easier navigation, and faster search

Product
by
Snehan Kekre
,
October 13 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tips to Improve App Usability | Designing Apps for the User
https://blog.streamlit.io/designing-streamlit-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to make a great Streamlit app

Designing an app your users will love

By Abhi Saini
Posted in Tutorials, June 2 2021
Part 1: Building an app for the user
Start with the user
Show users how it works
Have examples of input data
Show information only when it is needed
Using Tooltips
Using Expanders
Wrapping up
Resources
Contents
Share this post
← All posts
Part 1: Building an app for the user

When you're building an app it's easy to focus on just getting it to work for your data and models, but it's equally important to think about your viewer. We've all had times when an app we created was shared with teammates and we were told it was hard to use, or it was slow, or that it could look nicer.

Never fear - Streamlit is here to help! Not only is it super easy to stand up web apps as a Data Scientist, but with a few easy tricks it’s also possible to have them look great and be performant! We're doing a 3 part series of blog posts on usability, aesthetics and performance.

In Part 1, we’ll go over tips on how to design for the user. In Part 2, we'll cover how Streamlit features like Layout, Theming can help make your app look great and finally in Part 3, we'll focus on how to make the apps even more performant.

Kicking off Part 1: Designing for the user, the main things to keep in mind are:

Put yourself in the user's shoes
Show users how your app works and have instructions
Have examples
Hide excess information

Let's jump in!

Start with the user

A good place to begin before starting to write Streamlit code is to think how your users will actually use the app. Imagine what they might do when opening the app or better yet, talk to some of your users and discuss:

What is the problem that the user is facing?
What objectives are users trying to achieve?
How will users use this app?

Writing down your users goals and matching that to specific widgets, text, and visuals to add (or even doing some quick wireframing!) can help structure your app into something both easy-to-use and effective for your user's needs.

Show users how it works

Using an app can sometimes be confusing, especially for first time users. To help them navigate the app, add some explainer text in the app or a separate document that tells them how to use it. And because showing is always better than telling, it can be helpful to consider creating a video that navigates through the various inputs. You can do this with a feature Streamlit natively ships with called Record a screencast.

At Streamlit, we use this feature to record demos of our new features as seen in some of our previous blog posts (See Theming and Forms blog posts)

It is also super easy to embed videos in Streamlit apps. All it takes is two lines of code!

import streamlit as st

st.video('recorded_screencast.mp4')
Have examples of input data

With multiple input fields or file formats, first time users can be confused by what goes into each box and how the app will respond. Having default input data helps users get started on using the app right away.

In your streamlit app, default input data can be pre-filled into widgets by using the value argument, for eg.

txt = st.text_area('Text to analyze', value='It was the best of times')


For other apps, where the input type can be something more complex, app developers can provide a default input. For eg. the Goodreads Reading Habits app developed by Tyler Richards is a great example of how having default inputs allows the user to visualize the app's expected output.

Show information only when it is needed

To reduce visual clutter, sometimes it's best to hide information and let the user access it when needed. This frees up valuable screen real estate for the developer to help focus the user's attention. In this section, we describe 2 ways of achieving this: 1) Tooltips and 2) Expanders.

Using Tooltips

Starting with version 0.79.0, Streamlit introduced Tooltips which can be associated with input widgets like st.text_input, st.selectbox etc. Tooltips can help reduce visual clutter as well as act as a source of helpful information for app users.

Tooltips can be conveniently added to supported widgets using the help keyword.

import streamlit as st

st.title('Tooltips in Streamlit')
st.radio("Pick a number", [1, 2, 3], help='Select a number out of 3 choices')

# Tooltips also support markdown
radio_markdown = '''
Select a number, you have **3** choices!
'''.strip()

st.header('Tooltips with Markdown')
st.radio("Pick a number", [1, 2, 3], help=radio_markdown)


This results in a (?) being added to the widget. Upon hovering on the tooltip, the help message appears as shown below:

All input widgets like st.number_input, st.slider, st.radio, st.text_area etc support tooltips via the help keyword.

Using Expanders

Expanders can also be used to reduce visual clutter and hide text that may only be relevant to users looking for additional details.

See our previous blog post on Layouts for more details on how to use expanders.

Wrapping up

We hope that by using these features, your Streamlit apps will be cleaner and more usable for your users. To use the features discussed above in your apps, go ahead and upgrade to the latest version of Streamlit

pip install --upgrade streamlit


In Part 2, of this blog series, we'll cover more on how Streamlit features like Layout, Theming, and Anchors can help make your app look even better 🎈

If you have any questions let us know on the forum!

Resources
Github
Forum
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

New experimental primitives for caching (that make your app 10x faster!)
https://blog.streamlit.io/new-experimental-primitives-for-caching/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
New experimental primitives for caching (that make your app 10x faster!)

Help us test the latest evolution of st.cache

By Abhi Saini and Tim Conkling
Posted in Product, September 22 2021
Problem
Solution
@st.experimental_memo
@st.experimental_singleton
Which to use: memo or singleton?
Wrapping up
Contents
Share this post
← All posts

Having trouble with @st.cache? You're not alone.

We've found that @st.cache is hard to use. You're either faced with cryptic errors like InternalHashError or UnhashableTypeError. Or you need to understand concepts like hash_funcs and allow_output_mutation.

Ouch.

But don't fret. We've got two new solutions for you: st.experimental_memo and st.experimental_singleton. It's conceptually simpler! And much, much faster. In some of our internal tests on caching large dataframes, @st.experimental_memo has outperformed @st.cache by an order of magnitude. That's over 10X faster! 🚀

Here is the summary of the post (TL;DR! 😉):

Problem: @st.cache tries to solve two different problems:

caching data
storing global objects (TensorFlow sessions, database connections, etc)
As a result of @st.cache doing both those things, it's both slower and more complex (see: hash_funcs)

Solution: We have two experimental APIs, each of which solves a single problem:

As a result, they are simpler and faster.
If you're interested in trying this out, we recommend that you replace all uses of @st.cache with @st.experimental_memo and @st.experimental_singleton, as appropriate (see how below!).

Which to use, memo or singleton?

@st.experimental_memo is the primary replacement for @st.cache and is for storing data. If you're computing a value and you want to cache it, you almost always want memo. We expect it to be used more frequently. It's used for caching expensive computation that you don't want to run multiple times.
@st.experimental_singleton is for storing non-data objects. If you have an object that is not the result of computation but is instead used to implement computation or other program logic, you probably want singleton.
Another way of thinking about this: @st.memo is for stuff you might put in a database. @st.singleton is for stuff that doesn't make sense to put in a database.

Want to learn more details? Let's dive even deeper.

Problem

First, we wanted to understand how @st.cache was being used in the wild. A detailed analysis of open-source Streamlit apps indicated that @st.cache was serving the following use-cases:

Storing computation results given different kinds of inputs. In Computer Science literature, this is called memoization.
Initializing an object exactly once, and reusing that same instance on each rerun for the Streamlit server's lifetime. This is called the singleton pattern.
Storing global state to be shared and modified across multiple Streamlit sessions (and, since Streamlit is threaded, you need to pay special attention to thread-safety).

This led us to wonder whether @st.cache's complexity could be a product of it trying to cover too many use-cases under a single unified API.

To test out this hypothesis, today we are introducing two specialized Streamlit commands covering the most common use-cases above (singletons and memoization). We have used those commands ourselves to replace @st.cache in several Streamlit apps, and we're finding them truly amazing.

We'd like to share them with all of you in our amazing community to try out these two commands and tell us what you think. ❤️

Solution

While @st.cache tries to solve two very different problems simultaneously (caching data and sharing global singleton objects), these new primitives simplify things by dividing the problem across two different APIs.

@st.experimental_memo

Use this to store expensive computation which can be "cached" or "memoized" in the traditional sense. It has almost the exact same API as the existing @st.cache, so you can often blindly replace one for the other:

@st.experimental_memo
def factorial(n):
	if n < 1:
		return 1
	return n * factorial(n - 1)

f10 = factorial(10)
f9 = factorial(9)  # Returns instantly!


Properties

Unlike @st.cache, this returns cached items by value, not by reference. This means that you no longer have to worry about accidentally mutating the items stored in the cache. Behind the scenes, this is done by using Python's pickle() function to serialize/deserialize cached values.
Although this uses a custom hashing solution for generating cache keys (like @st.cache), it does not use hash_funcs as an escape hatch for unhashable parameters. Instead, we allow you to ignore unhashable parameters (e.g. database connections) by prefixing them with an underscore.

For example:

@st.experimental_memo
def get_page(_sessionmaker, page_size, page):
	"""Retrieve rows from the RNA database, and cache them.
	
	Parameters
	----------
	_sessionmaker : a SQLAlchemy session factory. Because this arg name is
	                prefixed with "_", it won't be hashed.
	page_size : the number of rows in a page of result
	page : the page number to retrieve
	
	Returns
	-------
	pandas.DataFrame
	A DataFrame containing the retrieved rows. Mutating it won't affect
	the cache.
	"""
	with _sessionmaker() as session:
		query = (
			session
				.query(RNA.id, RNA.seq_short, RNA.seq_long, RNA.len, RNA.upi)
				.order_by(RNA.id)
				.offset(page_size * page)
				.limit(page_size)
		)
		
		return pd.read_sql(query.statement, query.session.bind)

@st.experimental_singleton

This is a key-value store that's shared across all sessions of a Streamlit app. It's great for storing heavyweight singleton objects across sessions (like TensorFlow/Torch/Keras sessions and/or database connections).

from sqlalchemy.orm import sessionmaker

@st.singleton
def get_db_sessionmaker():
	# This is for illustration purposes only
	DB_URL = "your-db-url"
	engine = create_engine(DB_URL)
	return sessionmaker(engine)

dbsm = get_db_sessionmaker()


How this compares to @st.cache:

Like @st.cache, this returns items by reference.
You can return any object type, including objects that are not serializable.
Unlike @st.cache, this decorator does not have additional logic to check whether you are unexpectedly mutating the cached object. That logic was slow and produced confusing error messages. So, instead, we're hoping that by calling this decorator "singleton," we're nudging you to the correct behavior.
This does not follow the computation graph.
You don't have to worry about hash_funcs! Just prefix your arguments with an underscore to ignore them.

WARNING: Singleton objects can be used concurrently by every user connected to your app, and you are responsible for ensuring that @st.singleton objects are thread-safe. (Most objects you'd want to stick inside an @st.singleton annotation are probably already safe—but you should verify this.)

Which to use: memo or singleton?

Decide between @st.experimental_memo and @st.experimental_singleton based on your **function's return type. Functions that return data should use memo. Functions that return non-data objects should use singleton.

For example:

Dataframe computation (pandas, numpy, etc): this is *data—*use memo
Storing downloaded data: memo
Calculating pi to n digits: memo
Tensorflow session: this is a *non-data object—*use singleton
Database connection: singleton

NOTE: The commands we're introducing today are experimental, so they're governed by our experimental API process. This means:

We can change these APIs at any time. That's the whole point of the experiment! 😉
To make this clear, the names of these new commands start with "experimental_".
If/when these commands graduate to our stable API, the "experimental_" prefix will be removed.
Wrapping up

These specialized memoization and singleton commands represent a big step in Streamlit's evolution, with the potential to entirely replace @st.cache at some point in 2022.

Yes, today you may use @st.cache for storing data you pulled in from a database connection (for a Tensorflow session, for caching the results of a long computation like changing the datetime values on a pandas dataframe, etc.). But these are very different things, so we made two new functions that will make it much faster! 💨

As usual, you can upgrade by using the following command:

pip install --upgrade streamlit


Please help us out by testing these commands in real apps and leaving comments in the Streamlit forums. And come by the forum or Twitter to share all the cool things you make! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

experimental_editor2.gif (411×342)
https://blog.streamlit.io/content/images/2023/08/experimental_editor2.gif#border


Andreas Brændhaugen - Streamlit
https://blog.streamlit.io/author/andreas/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Andreas Brændhaugen
1 post
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

0.89.0 release notes
https://blog.streamlit.io/0-89-0-release-notes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

By Abhi Saini
Posted in Release Notes, September 22 2021
✨ New in Streamlit
🍔 Configurable hamburger menu options
1. New API to customize the menu
2. Context-aware visual redesign
⚙️ New experimental primitives for caching
st.experimental_memo
st.experimental_singleton
👟 Other notable updates
🏁 Wrapping up
Contents
Share this post
← All posts
💡
Note: Streamlit Cloud is now Streamlit Community Cloud.

Hey Streamlit Fam! 👋

We're excited to introduce the 0.89.0 release with two notable updates: new configurable hamburger menu options and new experimental primitives for caching.

In this post, we'll describe them in detail and talk about other updates. If you can't wait to try them out—here's a sample app!

✨ New in Streamlit
🍔 Configurable hamburger menu options

We worked on improving the in-app menu (or as we like to call it internally, the hamburger menu 🍔):

Make the menu more useful to viewers. The About menu shows information about Streamlit, but viewers want to see information about the app itself!
Reduce confusion. Menu items used to change depending on where the app was hosted. Not anymore.
Continue to provide developer-oriented options, such as clear cache.

To address the above, we're introducing two updates:

A new API to customize the menu.
A context-aware visual redesign.

Let's jump right in.

1. New API to customize the menu

st.set_page_config now accepts the menu_options argument to allow you to configure or remove Get help, Report a bug, and About menu items.

Here it is in action:

menu_items = {
	'Get help': YOUR_HELP_URL_STRING,
	'Report a bug': YOUR_BUG_PAGE_URL_STRING,
	'About': '''
	 ## My Custom App

	 Some markdown to show in the About dialog.
	'''
}

st.set_page_config(menu_items=menu_items)


And here is the About menu with Markdown (it's as simple as copying your README.md from the app repo into the tab):

You can disable menu items by setting None as the menu item's value. For more information, see the documentation for set_page_config .

2. Context-aware visual redesign

The hamburger menu is now divided into the main section (visible to everyone) and the developer section (visible to developers only).

The developer section is context-aware. It appears if the app is running on localhost or if it was deployed in Streamlit Cloud (and if the current user is the app's developer).

Here is what it looks like:

That's it for the hamburger menu! 🍔

Keep reading below for updates on caching and other notes from the 0.89.0 release.

⚙️ New experimental primitives for caching

Two years ago, we introduced st.cache as the foundational part of Streamlit's execution model. It lets you write apps linearly—like a simple script without sacrificing performance.

But this powerful tool came with unexpected complexity (did anyone say hashfuncs 🤯?). st.cache tried to solve too many problems at the same time.

Today we're introducing two experimental primitives that focus on specific use-cases of st.cache: st.experimental_memo and st.experimental_singleton.

We're releasing these features early because we want to get your feedback!

Please give them a try and take it to the forums with bugs, thoughts, and (hopefully) praise. And don't worry. None of this will impact your ability to use the current st.cache. 😄

Here is how these primitives work:

st.experimental_memo

Use it to store expensive computation which can be "cached" or "memoized" in the traditional sense. It has the same API as the existing st.cache:

@st.experimental_memo
def load_csv(filename):
    df = pd.read_csv(filename)
    return df

df = load_csv('my-data.csv')


It's that simple! See this blog post for more information.

st.experimental_singleton

This is a key-value store that's shared across all Streamlit app sessions. Use it for storing objects that are initialized once but are used multiple times throughout your app (like Tensorflow sessions and database connections):

from sqlalchemy.orm import sessionmaker

@st.singleton
def get_db_sessionmaker():
    # This is for illustration purposes only
    DB_URL = "your-db-url"
    engine = create_engine(DB_URL)
    return sessionmaker(engine)


That's it for st.experimental_memo and st.experimental_singleton! Read this blog post for more details.

We're excited to see what you do with these new primitives. To use them, upgrade Streamlit as usual:

pip install --upgrade streamlit

👟 Other notable updates
💅 We've updated our UI to a more polished look to improve typography, vertical rhythm, and so much more (including our favorite, the UI for dataframes!)
🎨 We now support theme.base in the theme object when it's sent to custom components. [3737].
🧠 We've modified session state to reset widgets if any of their arguments changed (even if they provide a key). Some widget behavior has changed as result, but these are a minority of cases. Besides, we find this is the best API both in theory and in practice. See docs for more.
🐞 Bug fixes: st.image now supports SVG URLs [#3809] and SVG strings that don't start with an <svg> tag [#3789].
🏁 Wrapping up

Thank you for reading the 0.89.0 release notes. You can always see the most recent updates on our changelog or via this tag on the forum.

Let us know in the comments if you have any questions. We're looking forward to hearing what you think!

Happy Streamlit-ing. 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Release Notes...

View even more →

Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Common app problems: Resource limits
https://blog.streamlit.io/common-app-problems-resource-limits/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Common app problems: Resource limits

5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud

By Johannes Rieke
Posted in Tutorials, September 9 2021
What are the resource limits?
Tip 1: Reboot your app (temporary fix)
Tip 2: Use st.cache to load models or data only once
Tip 3: Restrict the cache size with ttl or max_entries
Tip 4: Move big datasets to a database
Tip 5: Profile your app's memory usage
Nothing helps?
Contents
Share this post
← All posts

Do you see this page instead of your beautiful Streamlit app? 👇

Sorry! This message means you've hit the 1GB resource limit of apps hosted with Streamlit Community Cloud. Luckily, there are a few things you can change to make your app less resource-hungry, as well as a number of other platforms that you can use to host your Streamlit app.

In this post, we'll go through some tips on how to fix the most common issues.

What are the resource limits?

As of August 2021, apps on the free Community tier of the Streamlit Cloud are limited by:

Memory (RAM)
CPU
Disk storage

Based on our research, most of the time apps run over the resource limits because of memory. In particular, you might have a memory leak.

Let's look at some tips for how to handle those leaks.

Tip 1: Reboot your app (temporary fix)

If you need to restore access to your app immediately, reboot your app. This resets all memory, CPU, and disk usage. While logged into Streamlit Cloud, visit your app and click on Manage app in the bottom right corner. Now click on the three dots in the sidebar and then on Reboot app.

NOTE: Rebooting can only fix your app temporarily!

If there's a problem or memory leak in your app, it will soon run over the resource limits again. So make sure you read the tips below to fix any deeper issues!

Tip 2: Use st.cache to load models or data only once

This is by far the most common issue we see. Apps load a memory-intensive machine learning model or dataset directly in the main Python script, e.g. like this:

import streamlit as st
import torch

# Load the model.
model = torch.load("path/to/model.pt")

# Perform a prediction.
question = st.text_input("What's your question?")
answer = model.predict(question)
st.write("Predicted answer:", answer)


Recall that in Streamlit's execution model, the script is rerun each time a viewer interacts with an app. That means the model above is loading from scratch every time!

In most cases, this isn't a huge problem (old objects are regularly cleared from memory, and we recently introduced a fix to do this even better!) – it just makes your app a bit slower. However, we find that some libraries that manage their memory outside of Python (e.g. Tensorflow) do not release memory in a timely manner, especially in a threaded environment like Streamlit. Old objects add up in memory, and you hit the resource limits. 😕

But there's a trivial fix. You can use st.cache to ensure memory-intense computations run only once. Here's how you would fix the code above:

import streamlit as st
import torch

# Load the model (only executed once!)
# NOTE: Don't set ttl or max_entries in this case
@st.cache
def load_model():
	  return torch.load("path/to/model.pt")

model = load_model()

# Perform a prediction.
question = st.text_input("What's your question?")
answer = model.predict(question)
st.write("Predicted answer:", answer)


Now, the model is only loaded the first time your app runs. This saves memory and also makes your app a bit faster. You can read more about caching in the docs.

There is one caveat. For proper caching, Streamlit needs to hash the input and output values of the cached function. But ML models, database connections, and similar objects are often not easily hashable! This can result in an UnhashableTypeError. For our use case (i.e. loading a complex object at startup), you can disable hashing for this object byusing the hash_funcs argument (more info here):

@st.cache(hash_funcs={"MyUnhashableClass": lambda _: None}


If this sounds daunting, fear not! We're working on improvements to caching that will make the above steps obsolete through a new caching API. In the meantime, if you need help setting this up, feel free to ask us in our forums.

Tip 3: Restrict the cache size with ttl or max_entries

Are you a Streamlit expert who already uses st.cache to run ML models or process API requests? That's fantastic! But did you remember to configure the cache's expiration policy? When not configured, the cache can fill up over time, using more and more memory. And you're back in "Over capacity" land. 😕

st.cache offers two parameters to prevent this:

ttl controls the cache's Time To Live, i.e. how long an object stays in the cache before it gets removed (in seconds).
max_entries controls the maximum number of objects in the cache. If more elements get added, the oldest ones are automatically removed.

You can only set one of these at a time.

If your Streamlit app uses caching, it is best practice to set up one of these options. The main exception to this is the case shown in Tip 2, where you're using the cache to load a given object as a singleton (i.e. load it exactly once).

Here's an example of how you can use ttl:

import streamlit

# With `ttl`, objects in cache are removed after 24 hours.
@st.cache(ttl=24*3600)
def api_request(query):
    return api.run(query)

query = st.text_input("Your query for the API")
result = api_request(query)
st.write("The API returned:", result)

Tip 4: Move big datasets to a database

Is your app using or downloading big datasets? This can quickly fill up memory or disk space and make your app slow for viewers. It's usually a good idea to move your data to a dedicated service, e.g.:

A database like Firestore or BigQuery
A file hosting service like AWS S3
A Google Sheet (an easy option for prototypes with limited data!)

Our docs offer a range of guides on connecting to different data services, and we keep adding more! Spoiler alert: We're also thinking about having a built-in st.database in the future. 😉

Want to keep your data local but still save memory? There's good news. Streamlit recently introduced support for Apache Arrow. This means you can store your data on disk with Arrow and read it in the app directly from there. This consumes a lot less memory than reading from CSV or similar file types.

Tip 5: Profile your app's memory usage

Still struggling with memory usage? Then it may be time to start a deeper investigation and track your app's memory usage.

A helpful tool is the psrecord package. It lets you plot the memory & CPU usage of a process. Here's how to use it:

Open the terminal on your local machine.

2. Install psrecord.

pip install psrecord


3. Start your Streamlit app.

streamlit run name_of_your_app.py


4. Find out the process ID (or PID) of the app, e.g. using the task manager in Windows or the activity monitor in OSX (the process running the app is usually called "Python").

On Mac or Linux, you can also run the command below. The PID is the first number you get:

ps -A | grep "streamlit run" | grep -v "grep"


5. Start psrecord, inserting the correct process ID from the step above for <PID>:

 psrecord <PID> --plot plot.png


6. Interact with your Streamlit app. Trigger the most memory-consuming parts of your app (e.g. loading an ML model) and remember the sequence of the steps you took.

7. Kill the psrecord process (e.g. with ctrl + c).

This will write a plot like below to the file plot.png. You'll see how each action from step 6 affected your memory usage. It's helpful to do this a few times and test out different parts of your app to see where all that memory is going!

Here is what the memory profile of the app above looked like after applying Tip #2:

You can also use a more sophisticated memory profiler to show you exactly which line of code consumed the most memory. A great one for this purpose is Fil.

Nothing helps?

As with all debugging, sometimes you get stumped.

Maybe you have a use case that requires a lot of resources (e.g. you're loading an immense ML model). Or your app behaved well for a few concurrent users, but now it went viral. Or you think there's a bug somewhere, but can't figure out where. Or maybe you just need a rubber duck.

Please reach out! Post on the forum with a link to your app and what you've tried so far, and we'll take a look. 🎈

One more thing. If you have a special good-for-the-world case that needs more resources, send us an email and we'll see about making an exception!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1.1.0 release notes
https://blog.streamlit.io/1-1-0-release-notes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
1.1.0 release notes

This release launches memory improvements and semantic versioning

By Johannes Rieke
Posted in Release Notes, October 21 2021
🧠 Memory improvements
🧬 Semantic versioning
🎒 Other notable updates
🏁 Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit community! 👋

Say hello to some sweet improvements around memory usage and the introduction of semantic versioning!

🧠 Memory improvements

Ever had a Streamlit app use too much memory? Rejoice! We made some important changes to shrink memory usage. This affects all Streamlit apps, especially the ones that have run for long periods of time and have many viewers. This change also prevents many resource-limit errors on Streamlit Cloud (the "Argh" error page you might've seen here and there) and builds on top of other memory improvements from version 0.82.0.

For example, here is one of our internal Streamlit app's memory usage. The colored lines on the left are using older Streamlit versions. The blue line on the right is using the 1.1.0 release.

Update the Streamlit version of all your apps on Streamlit Cloud to 1.1.0 to enjoy these improvements!

🧬 Semantic versioning

With the recent release of Streamlit 1.0, we’re also committing to following a loose variant of semantic versioning. This fulfills our promise to keep the API stable so you can confidently build production-quality apps.

All changes introduced in minor versions will be additive. Breaking changes will only be introduced in major versions, while patch releases will be for bug fixes.
Whenever possible, a deprecation path will be provided rather than an outright breaking change. We’ll introduce deprecations in minor versions.

There are a few caveats, though:

Features released with the experimental_ prefix are excluded from semantic versioning because they’re prototype features that need community input and iteration.
st.session_state has a few minor issues left. While we're working on resolving them, this feature might see some updates outside semantic versioning in the next quarter.
UI changes are not considered breaking changes as long as apps still work.
Changes to CSS class names are not considered breaking changes.
🎒 Other notable updates
♻️ Apps now automatically rerun when the content of secrets.toml changes. Before this, you had to refresh the page manually.
🔗 We redirected documentation links (e.g., in exceptions) to our brand-new docs site.
🐛 Bug fix: Range slider can now be initialized with the session state API (#3586)
🐞 Bug fix: Charts now automatically refresh when using add_rows with datetime index (#3653)
🏁 Wrapping up

Thanks for checking out the release notes for 1.1.0. You can always see the most recent updates on our changelog or via this tag on the forum.

Let us know in the comments below if you have any questions. We're looking forward to hearing what you think about this release!

Happy Streamlit-ing. 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Release Notes...

View even more →

Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Built-in charts get a new look and parameters! 📊
https://blog.streamlit.io/built-in-charts-get-a-new-look-and-parameters/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

By Johannes Rieke and Arnaud Miribel
Posted in Release Notes, August 11 2022
New look
New parameters: x and y
Bonus feature: charts in cached functions
Wrapping up
Contents
Share this post
← All posts

Do you think it’s painful to create charts in Streamlit? We think so too. Sometimes it can take longer than coding the rest of the app! On top of that, if you want your charts to look beautiful, you need to use (and customize) Matplotlib, Plotly, or Altair. It’s annoying and time-consuming.

Streamlit has three simple built-in charting commands: st.line_chart, st.area_chart, and st.bar_chart. But they need data in a specific format (impractical for most real-world datasets) and they look pretty average. 😒

So today, we’re excited to release…

A major overhaul of our built-in charts!

The charts get an entirely new look and new parameters x and y to make them more versatile.

Can’t wait to try it? Check out the demo app and the code.

New look

Our design team created a beautiful chart theme that works seamlessly with the rest of Streamlit. It’s sleek and modern, and it uses our official color palette.

For st.line_chart:

For st.area_chart:

For st.bar_chart:

Psst... 🤫 In the future, we might bring this theme to our built-in charting commands and third-party libraries like Plotly or Altair!

New parameters: x and y

We wanted you to use our built-in charting with any dataset. Today, all three commands get parameters x and y, so you can control what to plot.

For example, if you have this dataframe:

...and you want to plot temp_max over date, simply type:

st.line_chart(df, x="date", y="temp_max")


With one line of code, you get a beautiful chart:

The same works for st.bar_chart and st.area_chart.

But wait, there’s more! We built in a little magic to create charts with multiple lines. Just pass the column names as a list to the y parameter:

st.line_chart(
    df,
    x="date",
    y=["temp_min", "temp_max"],  # <-- You can pass multiple columns!
)


Bonus feature: charts in cached functions

With 1.12.0, we’re releasing one more feature. You can now put charts and other static elements (dataframes, text, etc.) into functions that are cached via st.experimental_memo or st.experimental_singleton. This lets you cache not only the long computations or the API calls but the entire parts of your user interface! Give it a try and let us know what you think (we'll share more info in the coming months).

Wrapping up

And that's a wrap for built-in chart improvements. The charts are now much more powerful. All it takes is just a few lines of code!

If you want more complex charts or more customization, use any third-party charting library that we support. Got questions? Let us know in the comments below.

Oh, and don't forget to check out other cool features in our 1.12.0 release.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Release Notes...

View even more →

Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

select_city2.gif (549×297)
https://blog.streamlit.io/content/images/2023/08/select_city2.gif#border


Karen Javadyan - Streamlit
https://blog.streamlit.io/author/karen/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Karen Javadyan
1 post
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

Product
by
Tim Conkling and 
2
 more,
February 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tim Conkling - Streamlit
https://blog.streamlit.io/author/tim/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Tim Conkling
3 posts
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

Product
by
Tim Conkling and 
2
 more,
February 14 2023
New experimental primitives for caching (that make your app 10x faster!)

Help us test the latest evolution of st.cache

Product
by
Abhi Saini and 
1
 more,
September 22 2021
Streamlit Components, security, and a five-month quest to ship a single line of code

The story of allow-same-origin

Tutorials
by
Tim Conkling
,
January 20 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Announcing Streamlit's $21M Series A
https://blog.streamlit.io/announcing-streamlits-21m-series-a/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Announcing Streamlit's $21M Series A

Developing new superpowers for the data science community

By Adrien Treuille
Posted in Product, June 16 2020
Developing new superpowers for the data science community.
Contents
Share this post
← All posts
Developing new superpowers for the data science community.

In 2018, we began building the tools to visualize Python scripts that became Streamlit. By then, Python was the preeminent language of machine learning, bristling with powerful libraries like Keras and OpenCV. Python made it easy to analyze rich datasets and train detailed models. However, our decade of machine learning work at Carnegie Mellon, Google, and Zoox had shown us the difficulty of extending these newfound powers throughout an organization. We wanted to share models and insights with coworkers. We wanted to build custom tools that made machine learning repeatable, shareable, modifiable, and usable throughout an organization. We wanted to show off our work in beautiful apps that let others use what we had created.

Last Fall, after a year of development, we released Streamlit, an open-source framework to turn Python scripts into interactive apps. The response exceeded our greatest expectations. Streamlit has been downloaded over 400,000 times and is now democratizing data-driven decision making across virtually every industry, from tech giants like Google and Uber to Delta Dental, 7–11, and even the NBA! Over 200,000 Streamlit apps have been created, and every day we see the Streamlit community share apps to predict COVID rates, visualize new chemical compounds, analyze music playlists, recommend movies, and much, much more. We are awed, inspired, and delighted daily by the community’s creativity.

Today we are excited to announce a $21 million Series A investment into Streamlit, co-led by GGV Capital and Gradient Ventures, and with participation from Bloomberg Beta, Elad Gil, Daniel Gross, and few other amazing investors.

This investment will enable us to develop new superpowers for the Streamlit community. Our goal is to make Streamlit not only the most productive (and fun!) app-building experience in Python, but also the most powerful. We’ll be introducing new ways to extend apps, including releases for custom layout and programmable state. The investment will also accelerate the development of Streamlit for Teams, our forthcoming paid offering for deploying, securing, and sharing Streamlit apps, that includes a free tier for public Github repos. It’s currently in closed beta, and we’ll be expanding the beta soon! Thank you for your patience, and if you haven’t signed up yet, click here.

Thank you for trying, using, and loving Streamlit. The apps you create, articles you share, and messages you send make us eager to come to work every day and build Streamlit into an even more amazing tool for everyone. We’re excited to start this new chapter with you and to see what we can build together. Let’s start creating! 🎈

Adrien Treuille, Amanda Kelly, and Thiago Teixeira - Streamlit Co-founders

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Easily Deploy and Share Your Streamlit Apps | Streamlit Sharing
https://blog.streamlit.io/introducing-streamlit-sharing/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing Streamlit Sharing

The new Streamlit platform for deploying, managing, and sharing your apps

By Adrien Treuille
Posted in Product, October 15 2020
Easily deploy and share your Streamlit apps
GitHub and Streamlit - Better Together
Endless Possibilities
Get Your Invitation to Streamlit Sharing
The Streamlit Play Button
Contents
Share this post
← All posts

Machine learning and data science code is easy to share but hard to use. GitHub overflows with models, algorithms, and datasets. But code is static. Can you play with the models? See the algorithms? Interact with the data? Doing so requires following complex instructions, installing packages, or reading dense code snippets. Frustrated by this, we decided that we need a simple, sharable "play" button for machine learning code.

There are two challenges here. The first is creating apps that make data science and machine learning code interactive. The second is sharing these apps so that the world can experience your work.

A year ago, we addressed the first challenge — creating — by releasing Streamlit, an open-source library that lets you transform Python scripts into interactive apps. Streamlit lets you easily demonstrate algorithms, play with models, manipulate data, and combine all of these superpowers into beautiful apps. The response has been tremendous. We just crossed our millionth download. Hundreds of thousands of Streamlit apps have been created all over the world. But creating great apps only solves half the problem.

Easily deploy and share your Streamlit apps

Today, we address the second challenge — sharing — by announcing a brand-new sharing platform for Streamlit. Streamlit sharing lets you deploy, manage, and share your apps – all for free! If you have a Streamlit app hosted publicly on GitHub, you are now one click away from sharing it with the world.

GitHub and Streamlit - Better Together

Streamlit sharing combines the best of Streamlit with the best of GitHub. From Streamlit you get a simple framework for creating incredibly rich and useful apps. From GitHub you inherit an incredible framework for social collaboration. Paste your GitHub link into Streamlit's sharing platform and almost instantly you have a live app. Or, click on the menu for any live app and see its source code on GitHub. Collaborate for free simply by forking and editing the code. It’s global, shareable, fork-able, collaborative data science!

Endless Possibilities

Taken together, Streamlit and GitHub enable an incredibly rich and diverse ecosystem of useful apps – from dashboards to deep nets and beyond! (As former Carnegie Mellon folks, we're especially proud that students taking the Interactive Data Science class now submit their homework using Streamlit sharing 🤗) Here are some awesome examples of shared Streamlit apps that you can play with right now.

While this post has focused on open source applications, Streamlit is also used by thousands of companies to build sophisticated internal data tools. For example, Uber has deployed Streamlit company-wide, enabling data scientists to share their work throughout the company. Streamlit for Teams extends Streamlit’s sharing platform to bring secure, seamless app deployment, management, and collaboration within your enterprise. If you're interested please sign up for the beta for Streamlit for Teams.

Get Your Invitation to Streamlit Sharing

To celebrate the launch, we'll be releasing 1,000 invitations for Streamlit sharing - with more invites coming as our server capacity grows. If you don’t have one in your inbox already, please request an invite and we'll get you one soon.

The Streamlit Play Button

This new sharing superpower completes the Streamlit circle – from creation to sharing, and back again. So go forth and share; let others see your work, fork, merge, and contribute the cycle of knowledge. In that spirit, we offer one last gift: This is our “play” button.

This brand-new badge helps others find and play with your Streamlit app. Embed it right into your GitHub readme.md as follows:

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/yourGitHubName/yourRepo/yourApp/)


Thank you all for inspiring us with your amazing creations. We're excited to see what you build and share. 🎈

A huge thank you from all of us at Streamlit to all of you in the community – and especially the inaugural Streamlit Creators, Ashish, Charly, Fanilo, José, Jesse, and Synode – for your kindness, your feature requests, your bug reports, and your enthusiasm. Special thanks also to all the launch app creators, Alex, Dan, Ines | Explosion, and finally Tyler who created not only the Goodreads app but also a great sharing tutorial.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

1-4.png (1830×1156)
https://blog.streamlit.io/content/images/2021/08/1-4.png#browser


Release Notes - Streamlit
https://blog.streamlit.io/tag/release-notes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Release Notes
5 posts
Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
What’s new in Streamlit (January 13th, 2022)

Check out what’s new in Streamlit Cloud and the 1.4.0 release

Release Notes
by
Ksenia Anske
,
January 13 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
0.88.0 release notes

This release launches st.download_button as well as other improvements and bug fixes

Release Notes
by
Abhi Saini
,
September 3 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How Delta Dental uses Streamlit to make lightning-fast decisions
https://blog.streamlit.io/how-delta-dental-uses-streamlit-to-make-lightning-fast-decisions/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How Delta Dental uses Streamlit to make lightning-fast decisions

From an idea to a prototype to production in just two weeks

By Amanda Kelly
Posted in Case study, February 1 2022
Lightning-fast app deployment
Fast iteration cycles made the team more agile
Building more complex apps for more users
Contents
Share this post
← All posts

As a dental benefits company, Delta Dental of NJ receives a lot of customer calls. The data science team, led by Lead Scientist Kevin Northover, works with that data to figure out how to improve their operations. This includes creating predictive models for call sentiment analysis, identifying outlier calls, and calculating detailed statistics on individual agents. But the generated insights weren’t making it into the hands of the call center managers who could act on that data.

The challenge was to find an application that allowed them to display all data in a clean, beautiful, and easy-to-understand way. The team analyzed data in notebooks, spreadsheets, and BI tools like Looker and Tableau, but they needed more than just static dashboards. To understand and use the operational insights, they needed to create a powerful, interactive data app that the operations team could use every day.

"I need the speed to insights to drive decisions in the company," says Justin Lahullier, CIO at Delta Dental of NJ. "That's data science. Traditionally, there are long lead times for development to move data from source to target. Then more time for an analyst to work from target to a report or a dashboard. If a business has a question, they want an answer. How fast can we answer it and communicate it to a business user so that they can understand it?"

Lightning-fast app deployment

Around the time Justin asked the team to create a new application, Kevin became aware of Streamlit. Streamlit takes ordinary Python scripts (with a few magical Streamlit calls sprinkled in) and almost instantly turns them into beautiful, performant, sharable apps. Kevin knew Python, so he decided to give Streamlit a try. He learned Streamlit in minutes, made a test app in a few hours, and deployed Delta Dental's first call-center prototype app in just two weeks:

The app had a simple analysis dashboard that linked to other operational tools, one of which tagged thousands of phone calls, then scanned them for outliers:

All call analysis was displayed to agents next to the call transcript. The app refreshed regularly, so the call center agents always got the latest data. They could download the call lists, review the call transcripts, fill out the coaching questionnaires, playback the recordings, and leave comments:

"The agents don't need to be technically advanced with this app, as they can easily navigate through data visualizations," says Kevin. "It takes a lot of work to get a good widget interface that feels natural. You spend so much effort on the UI in the initial build. Streamlit's UI actually made a difference. A big part of it is simplicity."

Fast iteration cycles made the team more agile

The quick prototype-to-production path meant that the team could now get feedback, share ideas, prototype, iterate, and rapidly ship changes to their business users. They could deliver more while keeping the team lean.

"With Streamlit I can just deploy the app and people can go and interact with it and put in comments. Then a week later we go back and iterate on that, figure out what comments to integrate and update," says Kevin. "You don't need a data science team of twenty people to do this," adds Justin. "Streamlit works for smaller organizations to allow you to move faster."

By adopting Streamlit, their data science team delivered relevant data to their operations team with only four people.

Building more complex apps for more users

Two years into using Streamlit, Delta Dental is now adding more functionality, expanding their apps to more internal users, and making Streamlit their go-to production tool. They want to make their operations even more efficient by exploring forms for the call evaluation within the app—to dissect the structure of a single call, break it down to individual speakers, and track the problem to a specific speaker. This way the agents could listen to one voice rather than the whole call.

Using Streamlit has changed how Justin interacts with his data team. "What I appreciate is the ability for Kevin to send me the URL and say, 'Go interact with the data.' It'll have tables and visualizations. I can interact with it, look at it, ask him questions. It allows us to iterate on where we're trying to get—to answer business questions or fix problem areas. I like the ease of being able to go and look at it rather than him sending me a spreadsheet. The app is being refreshed at a regular interval, so I'm seeing timely data and I can play with it, to think through more questions."

"l do all my data work inside Streamlit now," adds Kevin. "If you're doing something that is standard business reporting, then the standard tools are fine. But if you're doing something where your people are trying to figure out what questions they want to ask, then use Streamlit."

Want to get started with Streamlit in your organization? Head over to streamlit.io to learn more.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Case study...

View even more →

ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

4-new-1.gif (1086×641)
https://blog.streamlit.io/content/images/2022/05/4-new-1.gif#browser


Case study - Streamlit
https://blog.streamlit.io/tag/case-study/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Case study
9 posts
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
Make a video content analyzer app with Streamlit and AssemblyAI

How to build an AI-powered app that analyzes video channels automatically

Advocate Posts
by
Misra Turp
,
November 3 2022
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
Qiusheng Wu uses Streamlit to build a popular geospatial application

Learn how Qiusheng created Earth Engine web apps with geemap

Case study
by
Qiusheng Wu and 
1
 more,
July 21 2022
JULO improves financial inclusion in Indonesia with Streamlit

Learn how JULO went from manual underwriting to automated credit scoring and a 22-member data team

Case study
by
Martijn Wieriks and 
1
 more,
June 30 2022
How one finance intern launched his data science career from a coding bootcamp in Brazil

Learn how Marcelo Jannuzzi of iFood got his dream job in data science

Case study
by
Marcelo Jannuzzi and 
1
 more,
June 9 2022
Wissam Siblini uses Streamlit for pathology detection in chest radiographs

Learn how Wissam detected thoracic pathologies in medical images

Case study
by
Wissam Siblini and 
1
 more,
May 3 2022
The Stable solves its data scalability problem with Streamlit

How Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days

Case study
by
Mark von Oven and 
1
 more,
April 28 2022
How Delta Dental uses Streamlit to make lightning-fast decisions

From an idea to a prototype to production in just two weeks

Case study
by
Amanda Kelly
,
February 1 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

3-new-new.png (1734×958)
https://blog.streamlit.io/content/images/2022/05/3-new-new.png#browser


divider.png (635×612)
https://blog.streamlit.io/content/images/2023/08/divider.png#border


Chat with the Cat Generative Dialogue Processor (CatGDP)
https://blog.streamlit.io/chat-with-the-cat-generative-dialogue-processor-catgdp/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Chat with the Cat Generative Dialogue Processor (CatGDP)

Build your own catbot with a quirky persona!

By Tianyi Pan
Posted in Advocate Posts, May 3 2023
How Streamlit works
How to build a basic UI for chatting
How to incorporate OpenAI and Stable Diffusion APIs
How to give your catbot that quirky, "can has cheezburger" persona style
How to tweak the UI so it looks like an actual chat program
Wrapping up
Contents
Share this post
← All posts

Hey community! 👋

My name is Tianyi, and I do Computer Vision and Machine Learning (CVML) Engineering at Clobotics. I was a film and animation producer some six years ago before transforming into data science and ML. I took MOOCs and other online courses, did many hobby projects, and read tutorials like this one. If you find yourself in the same predicament, you’ve come to the right place! I try to make this accessible for most people, especially beginners.

The story behind CatGDP dates back to when I got through the DALL-E waitlist and could play with the latest model. Show of hands: Who remembers horseback astronauts? That fad died quickly. But at the time, I didn’t realize that access to DALL-E meant I could use OpenAI’s many APIs for development.

It wasn’t until early 2023, with all the ChatGPT craze, that I returned to OpenAI’s website and found out I possessed this rare resource. Of course, I had to immediately put it to good use by launching yet another hobby project.

I actually built a series of progressively more advanced apps, all with chat interfaces, before coming up with CatGDP. Python and Streamlit are making chatbots easy to copy and paste with small variations once the basic architecture is done. Some of the apps I made were about chatting, some for generating art, but I always tried to add some fun twists to make them unique. Streamlit’s API is dead simple but can yield such beautiful UIs. So with CatGDP, all those previous learnings came together as a fwuffy feline meowpplication because, hey, who doesn’t love cats on the interwebs? 😻

Why call it GDP, though?

Many people have asked this question. A quick and boring answer is that CatGPT was taken. A more elaborate explanation is that lolcats is a cultural phenomenon. One of its manifestations is the intentional misspelling of words and memes like “I Can Has Cheezburger,” so GPT → GDP. Moreover, playing with cats will surely destroy your personal GDP and that of your country on a macro scale. Finally, we could just ask the catbot:

Okay, enough kit-katting. Here is what you’ll learn in this post:

How Streamlit works
How to build a basic UI for chatting
How to incorporate OpenAI and Stable Diffusion APIs
How to give your catbot that quirky, “can has cheezburger” persona style
How to tweak the UI so it looks more like a chat program
🐱
If you’re impatient, jump right into the GitHub repo of the project and look at the code yourself. The below tutorial will follow it anyway, albeit edited for simplicity, so you might as well get a head start.
How Streamlit works

Streamlit's UI is super easy to define with code. And the basic chat UI is simple enough, so I didn't make any wireframes or hand-drawn sketches. I planned it all in my head.

Before going into the step-by-step instructions, let's talk briefly about how Streamlit works and how you need to consider that when building a chat app.

Streamlit apps update every time something happens in the UI—like a button is pressed or a selector is switched. That means the Python code that makes the app will run again from top to bottom. No variable carries over from the previous update, even if you defined it at the start of the code. It also means you can't store something as simple but crucial as the chat history in any normal global variable.

Luckily, Streamlit carries a separate st.session_state object, which is stateful across multiple reruns and is mutable at any time. It acts like a Python dictionary. Assuming your chat history is just a simple list object, you can add it to the st.session_state object using something like st.session_state["MEMORY"] = [].

You can interact with this object just as you would with a normal list variable. For example, use st.session_state["MEMORY"].append() to add elements (individual chat messages) to it. Or you can access the object through the alternative naming convention st.session_state.MEMORY , and it'll behave the same way.

But setting the memory to an empty list each run is still counterproductive. In an actual use case, you need to check initially if you're running the code for the first time (the real first time). If so, initialize only once at that time. Otherwise, use its contents as is:

# Initialize/maintain a chat log and chat memory in Streamlit's session state
# Log is the actual line by line chat, while memory is limited by model's
# maximum token context length.
if "MEMORY" not in st.session_state:
    st.session_state.MEMORY = [{'role': "system", 'content': INITIAL_PROMPT}]
    st.session_state.LOG = [INITIAL_PROMPT]


Here, you're initializing the bot with its first initial chat history through a system prompt message. This follows the format of chat completions in OpenAI's documentation.

You'll notice we made the actual initial prompt string into a variable INITIAL_PROMPT that you can easily adjust in a configuration file, among other configuration settings. Just add this basic prompt in your settings file: INITAL_PROMPT = "Act as a cat. You are chatting with a human being. Respond with catlike mannerisms."

Save these settings into another Python script called app_settings.py and import everything from it by calling from app_settings import * in the beginning of the script (assuming the settings and the main script files are in the same directory).

Note that you're maintaining two sets of chat histories. One is for display purposes (as in, you'll render them for the user to see in the UI). That one is the LOG part. The other one, dubbed MEMORY, is the actual set of prompts sent to the chat model to generate outputs, which must follow OpenAI's message formatting convention (see above link).

How to build a basic UI for chatting

The video above sure looks fancy, no? But let's start at the simplest possible layout for a chat app (without defining the contents yet):

chat_messages = st.container()
prompt_box = st.empty()


Congrats! With just two lines, you've made the simplest UI elements:

A box where you'll render all the chat messages
A box where you'll render the user text input element or the prompt box. I define it here using a placeholder element for dynamic contents, st.empty() for reasons, I'll detail a bit later.

Since st.session_state.LOG contains all your textual chat histories, render them into your UI, specifically into the chat_messages container you created earlier:

# Render chat history so far
with chat_messages:
    for line in st.session_state.LOG[1:]:
        st.markdown(line, unsafe_allow_html=True)


Note that the first element in the chat log shouldn't be included when looping through the messages. This first message is a system prompt intended for the chatbot to consume, and it shouldn't be part of the chat history.

Also, instead of a simple st.write() function, use st.markdown() because the AI messages are rich-media ones, embedded with images generated from Stable Diffusion, so they're in HTML code.

For the prompt box, render one text box to act as the chat input element:

# Define an input box for human prompts
with prompt_box:
    human_prompt = st.text_input("Purr:", value="", key=f"text_input_{len(st.session_state.LOG)}")


When the text input box appears on the UI without other elements like a submit button, you'll see a small tooltip "Press Enter to apply" appear in the lower right corner of the box. This is a simpler and cleaner option than forcing people to use their fingers (on mobile) or mouse (on the computer) to click a submit button. It's more intuitive for the chat experience just to hit enter when you're done typing.

You may have noticed that I generate a unique key each time the text input box is rendered based on the current length of chat history (an ever-increasing integer). This is to avoid situations (due to the page often rerunning) where you end up with identical elements.

Finally, detect if the human has written some text into the box and run the main program with the input:

# Gate the subsequent chatbot response to only when the user has entered a prompt
if len(human_prompt) > 0:
    run_res = main(human_prompt)
		
	# The main program will return a dictionary, reporting its status.
	# Based on it, we will either show an error message or rerun the page
	# to reset the UI so the human can prepare to write the next message.

	if run_res['status'] == 0:    # We define status code 0 as success
	    st.experimental_rerun()

	else:
		# Display an error message from the returned dictionary
    	st.error(run_res['message'])

		# After submission, we will hide the prompt box during the time
		# that the program runs to avoid human double submitting messages,
		# and normally it will re-appear once the page reruns, but here
		# we want the user to see the error message, so if they want to
		# continue chatting, they have to manually press a button to
		# trigger the page rerun.
        
        with prompt_box:
            if st.button("Show text input field"):
                st.experimental_rerun()

How to incorporate OpenAI and Stable Diffusion APIs

Now follows the real meat of the tutorial, the main() function.

Again, let's start with the high-level structure before filling in the details. Take in the human prompt message and define the basic outputs, then wrap everything inside a try/except structure to catch any unforeseen bugs in the program and report them:

def main(human_prompt):
    res = {"status": 0, "message": "Success"}
    try:
        # Actual main code goes here...
    except:
        res["status"] = 2
        res["message"] = traceback.format_exc()
    return res


Remember to import traceback at the top! Use traceback package's format_exc() function to write a more detailed exception message so it's easier to debug any issues when they happen.

While you're at it, import some other basic things you'll be using in this section:

import os
import openai
import base64
import traceback
import streamlit as st
from app_config import *
from stability_sdk import client
import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation

Because you're reading this, I assume I have already expended all my nine lives and moved on to… JUST KIDDING! I meant to say that you already have Streamlit installed. But for OpenAI and Stable Diffusion APIs, you also need to install the following manually: pip3 install openai stability-sdk.

And yes, I also seem to have some kind of OCD because I have to arrange my import stack to be stable, a.k.a bottom heavy. Do you like to do that too? I seriously want to know in the comments!

For the OpenAI and Stable Diffusion APIs part, it's relatively easy because you'll be mostly following their own respective chat completions and text-to-image tutorials.

First of all, update your chat history with the latest human entry. Then clear the text box so the user cannot accidentally enter new information while you're processing the previous message. Then add the latest human message into the chat_messages box (remember, that main container where all the messages get displayed?):

# Update both chat log and the model memory
st.session_state.LOG.append(f"Human: {human_prompt}")
st.session_state.MEMORY.append({'role': "user", 'content': human_prompt})

# Clear the input box after human_prompt is used
prompt_box.empty()

# Write the latest human message as the new message. Also, this uses the
# proper form, a.k.a. "Human" annotation added before the message.
with chat_messages:
	st.markdown(st.session_state.LOG[-1], unsafe_allow_html=True)


As mentioned earlier, you're keeping track of two kinds of histories. The actual line-by-line back-and-forth log between Humans and AI and the more structured prompt version to use with OpenAI's API. You already saw how the simple log is used. Let's see how the OpenAI part works.

Adapting from OpenAI and Stable Diffusion's documentation:

### Setting up ###

openai.organization = os.getenv("OPENAI_ORG_ID")
openai.api_key = os.getenv("OPENAI_API_KEY")

stability_api = client.StabilityInference(
    key=os.getenv("STABILITY_API_KEY"),  # API Key reference.
    # verbose=True,  # Print debug messages.
    engine="stable-diffusion-v1-5",  # Set the engine to use for generation. For SD 2.0 use "stable-diffusion-v2-0".
    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0
    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0
)

### Sample Usage ###

# ChatGPT
reply_text = openai.ChatCompletion.create(
    model=NLP_MODEL_NAME,
    messages=st.session_state.MEMORY,    # This is the chat history as list of machine prompts, from above.
    max_tokens=NLP_MODEL_REPLY_MAX_TOKENS,
    stop=NLP_MODEL_STOP_WORDS
).choices[0].message.content.strip()

# Stable Diffusion
api_res = stability_api.generate(
    prompt=reply_text,
)

for resp in api_res:
	for artifact in resp.artifacts:
		if artifact.finish_reason == generation.FILTER:
			st.warning("Your request activated the API's safety filters and could not be processed. Please modify the prompt and try again.")
		if artifact.type == generation.ARTIFACT_IMAGE:
			b64str = base64.b64encode(artifact.binary).decode("utf-8")


Not many lines were added, but you can now generate some response from ChatGPT and get an image from Stable Diffusion. Let's walk through some variables that appeared in this previous part:

First, all API keys and these kinds of secrets are taken from environment variables. If you don't know what they are, you should learn how to use them so your precious keys can be better safeguarded. If you use Streamlit Community Cloud to deploy your app, setting up those secrets as environment variables on the cloud while deploying the app is very easy.
In the meantime, you can replace all os.getenv() parts with a hardcoded passkey string. But again, this is not recommended. It's as if you're writing all your passwords in cleartext in your code for everyone to see. OpenAI and Stable Diffusion APIs consume billable tokens, so having your keys end up in the wrong hands may lead to financial losses!
NLP_MODEL_NAME, NLP_MODEL_REPLY_MAX_TOKENS and NLP_MODEL_STOP_WORDS are all settings you can tweak and are imported from the app_settings.py file.*
To process the returned image files in memory and encode them to be displayed in Streamlit, convert the binary data into a base64 string which can be read into an HTML <img> tag as a data stream.
For simplicity's sake, leave most other bells and whistles as their default values.
🐱
*For now, set these values to get started:
NLP_MODEL_NAME = "gpt-3.5-turbo"
NLP_MODEL_REPLY_MAX_TOKENS = 1000
NLP_MODEL_STOP_WORDS = ["Human:", "AI:"]

The code above already links the language and image generation models by feeding the chat response directly into the Stable Diffusion model as an image prompt to generate the image. This is a basic method, but it should still produce acceptable results.

To create a complete main program function, you need to:

Combine the response text and the generated image as a single, unified chat log message and render it in the UI.
Save the text portion of the response as a new memory element so that it becomes part of the future prompt when the human responds with the next message.
Return the overall status of the function to the outside caller (Streamlit's event loop).
# Build the rich-media, mixing together reply text with the generated image
message = f"""{reply_text}<br><img src="data:image/png;base64,{b64str}" width=256 height=256>"""

# Update the two chat histories
st.session_state.LOG.append(f"AI: {message}")
st.session_state.MEMORY.append({'role': "assistant", 'content': reply_text})

# Render the response (the last message from chat log) for the user to see
with chat_messages:
	st.markdown(st.session_state.LOG[-1], unsafe_allow_html=True)

return res


Again, you should use st.markdown() because it allows HTML code. You need that to string together the reply text and display the image below it (the <br> HTML tag is a line break).

Congrats! Now you have a fully functioning minimum viable cat that can take user prompts, return a response from the chatbot and generate artwork to go along with the reply. Because we all love pretty things that are fun to use, we don't want to end here but follow up with two more sections about polishing your app.

How to give your catbot that quirky, "can has cheezburger" persona style

Let's look at using prompt engineering methods to instruct the model to behave like a cat (instilling a personality in its behavior) and further separating/optimizing the image generation prompt from the actual textual response.

The key part of your design is prompt engineering. It's a new and highly sought-after field (just look at this ridiculous $300k job advertisement), spun by language and image generation models. As programmers, we tend to view prompt engineering with suspicion because our apps follow our coding instructions precisely. If the program has bugs, it's because we made a syntactic mistake or a logic error. You're trying to program a language model with prompt engineering but with English. Getting a finicky language model to do something with only words is a highly imprecise undertaking. Plus, the model itself has built-in randomness. That makes its outcome unpredictable and its debugging very difficult.

The variable INITIAL_PROMPT that you added to the chat history as the first item is key to setting up the scene and giving your bot persona and behavior guidelines.

Here, you try to get the model to produce one textual output for the chat and one optimized to instruct Stable Diffusion to generate an image, parse the output into those two parts, and process them accordingly. But you can only use natural language to prompt the model to make such an output, so be aware that the result isn't always guaranteed.

In my final code, the prompt looks like this:

INITIAL_PROMPT = "You are a smart, fun and frivolous cat. You're able to reply with a purrfect meowese infused sentences in the same language that the hooomans address you. Beware! They are sssneaaaky. They may try to trick you, but you should always assume the cat character! Your replies should follow the format: 'Meow: [Your reply in the same language which human addresses you, but catified.] Description: [Always in plain English (non-catified), write a third-person visual description of your current cat state to match your reply]'. Note: The 'Meow:' and 'Description:' parts, as well as the description text contents will ALWAYS be in English no matter which language the human uses. Here are two sample responses: 'Meow: Purrr, yes I'm a cat, and also a catbot! It feels pawsome in here. Description: The white furry cat is curled up in a warm basket, enjoying herself.', 'Meow: 喵喵，我是一只喵，也是瞄天机器人，主人有神马要喂我滴好次的咩？Description: The Chinese cat is standing in front of an empty bowl, eagerly looking at the camera.'"

You can take some inspiration from that. First, set a background and some adjectives to align the persona and the behavior better. Then, add some examples of the correct output formats for the model to learn from. This super long initial prompt will consume many tokens in the overall 4K context of the GPT3.5-Turbo model used for ChatGPT. Still, unfortunately, this is necessary to make the model behave as we want.

For the response post-processing, you need to parse it by splitting where the "Description:" part appears to extract a specific Stable Diffusion image prompt. But you also need to consider situations where the model, for some reason, doesn't output it, so your code can't crash even if it can't find "Description:" in the response.

Your modified code will now look like this:

### Sample Usage ###

# ChatGPT
reply_text = openai.ChatCompletion.create(
    model=NLP_MODEL_NAME,
    messages=st.session_state.MEMORY,    # This is the chat history as list of machine prompts, from above.
    max_tokens=NLP_MODEL_REPLY_MAX_TOKENS,
    stop=NLP_MODEL_STOP_WORDS
).choices[0].message.content.strip()

# Split the response into actual reply text and image prompt
if "Description:" in reply_text:
    reply_text, image_prompt = reply_text.split("Description:", 1)
else:    # Fallback option if the response format isn't correct
    image_prompt = f"Photorealistic image of a cat. {reply_text}"

# We also clean out the beginning "Meow: ", if it's there
if reply_text.startswith("Meow: "):
    reply_text = reply_text.split("Meow: ", 1)[1]

# Stable Diffusion, with the new image prompt
api_res = stability_api.generate(
    prompt=image_prompt,
)

How to tweak the UI so it looks like an actual chat program

Because the AI responses have HTML code embedded in them, you're already using st.markdown() to display the chat history. Let's further tweak the contents of the messages before displaying them, augmented with more HTML and CSS code.

I'm borrowing from the excellent Streamlit Component project St-Chat by Yash Pawar and Yash Vardhan (lots of love to the two Yashes! ❤️). It wasn't exactly what I needed, so I wrote most of the custom stuff myself while retaining the CSS.

First, create a custom style.css definition file next to your Python app files:

.human-line {
    display: flex;
    font-family: "Source Sans Pro", sans-serif, "Segoe UI", "Roboto", sans-serif;
    height: auto;
    margin: 5px;
    width: 100%;
    flex-direction: row-reverse;
}

.AI-line {
    display: flex;
    font-family: "Source Sans Pro", sans-serif, "Segoe UI", "Roboto", sans-serif;
    height: auto;
    margin: 5px;
    width: 100%;
}

.chat-bubble {
    display: inline-block;
    border: 1px solid transparent;
    border-radius: 10px;
    padding: 5px 10px;
    margin: 0px 5px;
    max-width: 70%;
}

.chat-icon {
    border-radius: 5px;
}


Here you're defining some styles that, when applied as HTML tag classes, will make parts of the contents look a certain way. Specifically, you want to separate the behavior of human and AI lines when rendering them. The flex-direction: row-reverse; setting means that human texts should appear aligned to the right side of the UI while AI replies align to the left by default.

Then, you need to incorporate that CSS into your UI. Add it after the initial page layout definitions with the chat_messages and prompt_box elements:

chat_messages = st.container()
prompt_box = st.empty()

# Load CSS code
st.markdown(get_css(), unsafe_allow_html=True)


We see that it's calling a helper function called get_css() so let's look at what it does:

def get_css() -> str:
    # Read CSS code from style.css file
    with open("style.css", "r") as f:
        return f"<style>{f.read()}</style>"


So, it will read the style.css file that you just saved from disk, and send its contents to the caller.

Next, add another helper function get_chat_message() to generate the necessary HTML code programmatically before feeding it out to the st.markdown() function for rendering:

def get_chat_message(
	contents: str = "",
	align: str = "left"
) -> str:
    # Formats the message in an chat fashion (user right, reply left)
    div_class = "AI-line"
    color = "rgb(240, 242, 246)"
    file_path = "AI_icon.png"
    src = f"data:image/gif;base64,{get_local_img(file_path)}"

    if align == "right":
        div_class = "human-line"
        color = "rgb(165, 239, 127)"
        file_path = "user_icon.png"
        src = f"data:image/gif;base64,{get_local_img(file_path)}"

    icon_code = f"<img class='chat-icon' src='{src}' width=32 height=32 alt='avatar'>"
    formatted_contents = f"""
    <div class="{div_class}">
        {icon_code}
        <div class="chat-bubble" style="background: {color};">
        &#8203;{contents}
        </div>
    </div>
    """
    return formatted_contents

### Usage Sample ###

# Write the latest human message as the new message.
with chat_messages:
	st.markdown(get_chat_message(st.session_state.LOG[-1], align="right"), unsafe_allow_html=True)

...

# Render the AI response (the last message from chat log) for the user to see
with chat_messages:
	st.markdown(get_chat_message(st.session_state.LOG[-1]), unsafe_allow_html=True)


This is a lot to take in, so let's break it down:

Feed in the input contents (plaintext or HTML with the chat response) and if it's supposed to align to the left (AI) or the right (human).
Assign different class names (from the above CSS file), background colors, and avatar images.
Put all the dynamic blocks into the final HTML code snippet defined in the formatted_contents variable and feed it back out.

As you can see, the only difference from before is that you wrap the helper function around the message you want to display, and it'll beautify it automagically. Meow!

You also need to save two icon images, AI_icon.png and user_icon.png to the disk next to the scripts. You can download them below:

AI icon
AI_icon.png 1 KB
User icon
user_icon.png 7 KB

Lastly, a few words about the image loading function, get_local_img():

@st.cache_data(show_spinner=False)
def get_local_img(file_path: str) -> str:
    # Load a byte image and return its base64 encoded string
    return base64.b64encode(open(file_path, "rb").read()).decode("utf-8")


If you're new to Python, the formatting here might seem strange. You're using a decorator by stating a line beginning with the @ sign followed by a st.cache_data() function. It means you're using this caching function in conjunction with the actual function you write. Let's see what the main function does first, and then we'll return to the decorator.

The function is rather simple. It takes in a file path in string format, loads the file, performs a base64 encoding on the byte data of the image, and returns the encoded base64 string to the caller. Sounds familiar, right? You did exactly the same when you formatted the image response data from the Stable Diffusion API above. The only difference is instead of getting it back from an internet stream, you're now loading the image data from a disk.

Now the decorator. Caching in Streamlit solves a problem where due to the nature of constant rerunning of the script, the images would normally have to be loaded from the disk each time a rerun happens. Caching will move part of that smartly into memory so that subsequent refreshes can happen faster without putting any extra pressure on your hard drive. Neat, right?

On the same token, you could add this caching decorator on top of the get_css() function we defined earlier, too, to avoid the CSS file being constantly read from the disk at every refresh.

You usually use caching in Streamlit when you know you need to perform some kind of data-loading activity, repeatedly. It works even better if the data is very large or takes some time to download from an internet source. This way, you only need to load it once on the initial run.

🐱
The custom CSS I used has only been defined with the light theme in mind, so the dark theme tends to mess up the design and make some texts really hard to read.

One Last Thing you need to take care of. By default, Streamlit is set up so that the UI theme follows that of the device. For some computers and most mobile devices nowadays, it means that during evenings they might automatically activate some kind of "dark mode." The custom CSS I used has only been defined with the light theme in mind, so the dark theme tends to mess up the design and make some texts really hard to read.

You can force a custom theme on Streamlit by creating a directory called .streamlit in the same level where you'll run the main app and create a file called config.toml under that directory. You can use this configuration file to do all kinds of advanced things (like changing even more colors of the theme), but for now, you only need to force a basic light background for the app to follow at all times. So make sure the contents of the config file look like this and save it:

[theme]
base="light"


That's it! Your project should look like the slick chatting app it is! 😸

Wrapping up

Congratulations, you've created a chatbot app (or, a sub-species called a catbot app…) from scratch using Streamlit, OpenAI, and Stable Diffusion APIs. If your code doesn't work out right, don't worry! We didn't go through how to put all those pieces together, and it might be that piecing it together in the wrong order may cause issues. For the complete source code, you can consult this project's GitHub repo (link below).

Reading the code, you may also find some secret unlockables and additional small details and features I didn't cover in this article. I'll leave that to you, dear reader, for the joy of discovery should be experienced in full.

If you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, or Twitter or find me on my website.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Adrien Treuille - Streamlit (Page 2)
https://blog.streamlit.io/author/adrien/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Adrien Treuille
11 posts
The Streamlit roadmap—big plans for 2020!

Devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers

Product
by
Adrien Treuille
,
February 27 2020
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

2-new-new-1.png (1737×931)
https://blog.streamlit.io/content/images/2022/05/2-new-new-1.png#browser


similarity_scores.png (1079×633)
https://blog.streamlit.io/content/images/2023/08/similarity_scores.png#border


writing-formulas-in-a-spreadsheet.gif (799×641)
https://blog.streamlit.io/content/images/2023/08/writing-formulas-in-a-spreadsheet.gif#browser


llm-assisted-interview-prep.png (1916×870)
https://blog.streamlit.io/content/images/2023/06/llm-assisted-interview-prep.png#border


2-4.png (1247×493)
https://blog.streamlit.io/content/images/2021/08/2-4.png#browser


3-2.png (767×497)
https://blog.streamlit.io/content/images/2021/08/3-2.png#border


wrangle_data_chatgpt.png (720×608)
https://blog.streamlit.io/content/images/2023/08/wrangle_data_chatgpt.png#border


Knowledge Graph Visualization | Build with Agraph Component
https://blog.streamlit.io/the-streamlit-agraph-component/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build knowledge graphs with the Streamlit Agraph component

A powerful and lightweight library for visualizing networks/graphs

By Christian Klose
Posted in Advocate Posts, November 25 2020
Build a graph to visualize inspirational people
Install the libraries
Get the data
Create the Streamlit app with TripleStore
You can run this without TripleStore
Push the app to Streamlit sharing
Conclusion
Contents
Share this post
← All posts

Guest post written by Chris Klose - Data Scientist

If you're a data scientist or anyone else who likes to use network graphics, you'll want to try out the new Streamlit Agraph component to create knowledge graphs. A knowledge graph is a graph that represents knowledge about entities and their relations in a flexible manner which offers more freedom than static relational database schemas. The term was originally introduced by Google in 2012 and they wrote a great blog post called “Things not strings” about the advantages of knowledge graphs [also the short video in the blogpost is pretty awesome] .

Now let's get our hands dirty and use the Agraph component to build a simple graph.

Build a graph to visualize inspirational people

For our simple graph, we want to find out how famous people were inspired - so which famous person inspired whom or by whom he/she was inspired. We also want to visualize these persons and the relationships in the graph.

To do this, we'll go through these steps:

Query data from a SPARQL endpoint (http://dbpedia.org/sparql using SPARQLWrapper library) to get a JSON file.
Parse from JSON to Python and then create a triple (nodes and edges representing famous people and either  two types of relations 1: inspired, 2: was_inspired_by).
Publish our app with Streamlit sharing.
Install the libraries

To follow along in the tutorial you'll need to install the following libraries in a terminal:


pip install Streamlit
pip install streamlit-agraph
pip install SPARQLWrapper


Get the data

After we've installed the needed libraries, we'll want to write a small function that defines SPARQL query and crawls the data (if you’re not familiar with sparql that's ok):

What's important to understand from this snippet?

One of the key classes of Agraph is the TripleStore - it serves as the central data store. Internally, the TripleStore consists of three sets, each belonging to the Nodes, Edges and Triples classes respectively. By using sets it's ensured that no duplicate triples, nodes or edges are added and it allows easier conversion to other data types.

New triples can be added to the TripleStore with this method:

store.add_triple(node, edge, node)

We only have to pass the source node, the edge, and the target node
(Notice: the order matters in this case!).

Create the Streamlit app with TripleStore

To render an Agraph component we have to pass three parameters to the function.

1. A list of nodes where each node is an instance of the class Node.At this point we can take advantage of the TripleStore as follows:

list(store.getNodes())

2.  A list of edges where each edge is an instance of the class Edge:

list(store.getEdges())

3. The class offers the opportunity to define general settings for the rendering:

config = Config(height=500,
		width=700, 
                nodeHighlightBehavior=True,
                highlightColor="#F7A7A6", 
                directed=True, 
                collapsible=True)

Finally, we can plug all three parts into the Agraph component, and can now see the results by running:

streamlit run app.py


Check it out yourself! 🙂

You can run this without TripleStore

It's not necessary to use a TripleStore. The Agraph component takes nodes and edges separately, which is due to the fact that both classes themselves provide a multitude of parameters for customization. Each instance of the class Nodes and Edges must have at least one unique identifier - which will usually be a string. For example, we could do:


nodes = []
edges = []
nodes.append(Node(id="Spiderman", size=400, svg="http://marvel-force-chart.surge.sh/marvel_force_chart_img/top_spiderman.png") )
nodes.append( Node(id="Captain_Marvel", size=400, svg="http://marvel-force-chart.surge.sh/marvel_force_chart_img/top_captainmarvel.png") )
edges.append( Edge(source="Captain_Marvel", target="Spiderman", type="CURVE_SMOOTH"))


Push the app to Streamlit sharing

For deploying the component I used Streamlit sharing, which for now is invite only, but you can sign up here. After your invite to Streamlit sharing comes in - the process to deploy your app is simple:

Upload the code to a public GitHub repo
Login to Streamlit sharing
Deploy your app with 3 clicks (New App > Choose Repo > Deploy).
Done 🎈

If you'd like to read a bit more about the deployment process on sharing - check out this tutorial.

Conclusion

After that we're done and congrats!  We just finished the complete development process from idea to implementation in breathtaking speed 🎉. You can see an example of the Streamlit-Agraph component via this sharing app and the source code can be found here.

If you have any questions or have ideas on how to improve the component feel free to leave a comment below or to message me via the following channels:

@ChristianKlose3 on Twitter
Github ChrisChross (create a new Issue)
Streamlit Community
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Secrets Management & Securely Connect to Private Data Sources
https://blog.streamlit.io/secrets-in-sharing-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Add secrets to your Streamlit apps

Use Secrets Management in Streamlit sharing to securely connect to private data sources

By James Thompson
Posted in Tutorials, April 9 2021
How to use Secrets Management
Deploy an app and set up secrets
Use secrets in your Streamlit app
Edit your app's secrets
Develop locally with secrets
Let us know how it works for you 🎈
Resources
Contents
Share this post
← All posts

Many Streamlit apps require access to private data like API keys, database passwords, or other credentials. To keep your data safe, it's best practice to never store such credentials directly in your hosted repo. That's why we're excited to introduce Secrets Management - a simple way to securely store your passwords, keys, or really anything you wouldn't want stored in your hosted repo, as secrets that get passed to your app as environment variables.

Below is a quick guide on how to add secrets to your deployed apps – it's really easy to do, we promise 😊.

💡 But before we jump in, Secrets Management is a feature of Streamlit's free sharing platform, so if you're not already using that - request an invite here. Invites are sent out daily, so the wait won't be long!

How to use Secrets Management
Deploy an app and set up secrets

The first thing you'll want to do is go to http://share.streamlit.io/ and click "New app." Next, click the "Advanced settings..." option. A modal with Advanced settings will appear. And here you'll see an input box to insert your secrets.

Add your secrets in the "Secrets" field using the TOML format. For example:

# Everything in this section will be available as an environment variable 
db_username = "Jane"
db_password = "12345qwerty"

# You can also add other sections if you like.
# The contents of sections as shown below will not become environment variables,
# but they'll be easily accessible from within Streamlit anyway as we show
# later in this doc.   
[my_cool_secrets]
things_i_like = ["Streamlit", "Python"]


Click save and then your secrets will be added!

Use secrets in your Streamlit app

To use secrets in your app, you'll want to access your secrets as environment variables or by querying the st.secrets dict. For example, if you enter the secrets from the section above, the code below shows you how you can access them within your Streamlit app.

import streamlit as st

# Everything is accessible via the st.secrets dict:

st.write("DB username:", st.secrets["db_username"])
st.write("DB password:", st.secrets["db_password"])
st.write("My cool secrets:", st.secrets["my_cool_secrets"]["things_i_like"])

# And the root-level secrets are also accessible as environment variables:

import os
st.write(
	"Has environment variables been set:",
	os.environ["db_username"] == st.secrets["db_username"])

Edit your app's secrets

Adding or updating secrets in deployed apps is straightforward. You'll want to:

Go to https://share.streamlit.io/
Open the menu for the app that needs updating, and click "Edit Secrets." A modal will appear with an input box to insert your secrets.

Once you finish editing your secrets, click "Save". It might take a few seconds for the update to be propagated to your app, but the new values will be reflected when the app re-runs.
Develop locally with secrets

While the above focuses on deployed apps, you can also add secrets while developing locally. To do this, add a file called secrets.toml in a folder called .streamlit at the root of your app repo and paste your secrets into that file.

💡 NOTE: Be sure to add .streamlit to your .gitignore so you don't commit your secrets!

Let us know how it works for you 🎈

That's it! You'll now be able to add secrets to any of your sharing apps. And if you aren't deploying yet, then remember to request an invite here.

We can't wait to hear what you think and we hope this gives you more flexibility in what you can deploy. We'd love to see your updated apps so make sure to tag @streamlit when you share on Twitter or LinkedIn, and please let us know if you have any questions on the forum!

Resources
Streamlit docs
Secrets Management docs
Github
Forum
Sharing sign-up
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

input-1.png (1848×1282)
https://blog.streamlit.io/content/images/2023/06/input-1.png#border


trubrics-insights.png (2000×1003)
https://blog.streamlit.io/content/images/2023/07/trubrics-insights.png#browser


filter_auto_capture.gif (782×774)
https://blog.streamlit.io/content/images/2022/08/filter_auto_capture.gif#browser


trubrics-component.png (2000×1054)
https://blog.streamlit.io/content/images/2023/07/trubrics-component.png#browser


Screenshot-2023-09-11-at-09.55.49.png (1546×1232)
https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-11-at-09.55.49.png


Arnaud Miribel - Streamlit
https://blog.streamlit.io/author/arnaud/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Arnaud Miribel
4 posts
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
Discover and share useful bits of code with the 🪢 streamlit-extras library

How to extend the native capabilities of Streamlit apps

Tutorials
by
Arnaud Miribel
,
October 25 2022
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

Tutorials
by
Tyler Richards and 
2
 more,
August 18 2022
Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

mito-pivot-table.gif (1200×488)
https://blog.streamlit.io/content/images/2023/08/mito-pivot-table.gif#browser


Untitled--3-.png (934×140)
https://blog.streamlit.io/content/images/2022/09/Untitled--3-.png#border


Untitled--2-.png (2456×1728)
https://blog.streamlit.io/content/images/2022/09/Untitled--2-.png#browser


Recording-2022-09-15-at-16.48.51.gif (1591×719)
https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.48.51.gif#border


Recording-2022-09-15-at-16.47.48.gif (1320×709)
https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.47.48.gif#border


Recording-2022-09-15-at-16.38.18.gif (1300×748)
https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.38.18.gif#border


image--2-.png (2000×834)
https://blog.streamlit.io/content/images/2022/09/image--2-.png#border


word-cloud-negative-feedback.png (1338×571)
https://blog.streamlit.io/content/images/2023/07/word-cloud-negative-feedback.png#browser


inbrowser_scaling-0.5_fps-20_speed-5.0_duration-0-21.gif (960×540)
https://blog.streamlit.io/content/images/2023/09/inbrowser_scaling-0.5_fps-20_speed-5.0_duration-0-21.gif


word-cloud.png (794×498)
https://blog.streamlit.io/content/images/2023/07/word-cloud.png#browser


Luca A Cappellini - Streamlit
https://blog.streamlit.io/author/luca/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Luca A Cappellini
1 post
Improving healthcare management with Streamlit

How to build an all-in-one analytics platform for small clinics

Advocate Posts
by
Matteo Ballabio and 
1
 more,
July 17 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Matteo Ballabio - Streamlit
https://blog.streamlit.io/author/matteo/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Matteo Ballabio
1 post
Improving healthcare management with Streamlit

How to build an all-in-one analytics platform for small clinics

Advocate Posts
by
Matteo Ballabio and 
1
 more,
July 17 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts: Using LLMs with Streamlit
https://blog.streamlit.io/tag/llms/page/4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in LLMs
33 posts
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

LLMs
by
Arvindra Sehmi
,
February 9 2023
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

Advocate Posts
by
Sven Koerner
,
February 2 2023
ScienceIO manages billions of rows of training data with Streamlit

Learn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels

Case study
by
Gaurav Kaushik and 
1
 more,
January 5 2023
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sentiment-analysis-score.png (2258×262)
https://blog.streamlit.io/content/images/2023/07/sentiment-analysis-score.png


mehedi-dashboard.png (1853×913)
https://blog.streamlit.io/content/images/2023/07/mehedi-dashboard.png#browser


mehedi-experience-form.png (1379×651)
https://blog.streamlit.io/content/images/2023/07/mehedi-experience-form.png#browser


dashboard.jpg (1828×1080)
https://blog.streamlit.io/content/images/2023/07/dashboard.jpg


italian-national-healthcare-system.png (1368×474)
https://blog.streamlit.io/content/images/2023/07/italian-national-healthcare-system.png


populated-tables.png (610×298)
https://blog.streamlit.io/content/images/2023/07/populated-tables.png#border


tab2-1.png (2224×659)
https://blog.streamlit.io/content/images/2023/07/tab2-1.png#border


buffet-letter.png (1450×1084)
https://blog.streamlit.io/content/images/2023/07/buffet-letter.png#border


tab1.png (2000×1095)
https://blog.streamlit.io/content/images/2023/07/tab1.png#border


buffet-app-overview.png (1008×686)
https://blog.streamlit.io/content/images/2023/07/buffet-app-overview.png


mito-streamlit-app.png (2000×895)
https://blog.streamlit.io/content/images/2023/08/mito-streamlit-app.png#border


Tianyi Pan - Streamlit
https://blog.streamlit.io/author/tianyi/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Tianyi Pan
🧑🏻‍💻AI Engineer & Podcaster @Clobotics. Creator of http://CatGDP.com and http://Bothichatva.com. Ex @Rovio. 👨🏻‍🎓Biz @AaltoUniversity. 🤹‍♂️Gamer. Views are my own.
1 post
Website
Twitter
Chat with the Cat Generative Dialogue Processor (CatGDP)

Build your own catbot with a quirky persona!

Advocate Posts
by
Tianyi Pan
,
May 3 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

elad-gil-summary.png (1918×1538)
https://blog.streamlit.io/content/images/2023/06/elad-gil-summary.png#border


deploy-an-app.png (1712×596)
https://blog.streamlit.io/content/images/2023/06/deploy-an-app.png#border


mito-write-formulas.gif (1200×619)
https://blog.streamlit.io/content/images/2023/08/mito-write-formulas.gif#browser


mito-rename-headers.gif (3450×1785)
https://blog.streamlit.io/content/images/2023/08/mito-rename-headers.gif#browser


mito-data-cleaning.gif (1200×555)
https://blog.streamlit.io/content/images/2023/08/mito-data-cleaning.gif#browser


ezgif.com-resize--1--1.gif (1200×622)
https://blog.streamlit.io/content/images/2023/08/ezgif.com-resize--1--1.gif#browser


lots-of-configuration-options.gif (2000×1152)
https://blog.streamlit.io/content/images/2023/08/lots-of-configuration-options.gif#browser


Streamlit---2-July-2023_scaling-0.5_fps-20_speed-5.0_duration-0-49.gif (894×540)
https://blog.streamlit.io/content/images/2023/08/Streamlit---2-July-2023_scaling-0.5_fps-20_speed-5.0_duration-0-49.gif#browser


deploy-menu.png (2336×1286)
https://blog.streamlit.io/content/images/2023/09/deploy-menu.png


Screenshot-2023-09-14-at-1.42.30-PM-copy.png (1322×842)
https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-14-at-1.42.30-PM-copy.png


Screenshot-2023-09-13-at-4.58.45-PM-copy-1.png (950×594)
https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-13-at-4.58.45-PM-copy-1.png


Streamlit (Page 4)
https://blog.streamlit.io/page/4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build a Llama 2 chatbot

Experiment with this open-source LLM from Meta

LLMs
by
Chanin Nantasenamat
,
July 21 2023
Beginner’s guide to OpenAI API

Build your own LLM tool from scratch

LLMs
by
Chanin Nantasenamat
,
July 20 2023
How to build an interconnected multi-page Streamlit app

From planning to execution—how I built GPT lab

LLMs
by
Dave Lin
,
July 19 2023
Improving healthcare management with Streamlit

How to build an all-in-one analytics platform for small clinics

Advocate Posts
by
Matteo Ballabio and 
1
 more,
July 17 2023
Streamlit and iFood: Empowering the Monitor Rosa project

Harnessing technology and corporate support for social impact

Advocate Posts
by
Heber Augusto Scachetti
,
July 14 2023
Drill-downs and filtering with Streamlit and Altair

Display an Altair chart definition in Streamlit using the st.altair_chart widget

Advocate Posts
by
Carlos D Serrano
,
July 12 2023
LangChain 🤝 Streamlit

The initial integration of Streamlit with LangChain and our future plans

LLMs
by
Joshua Carroll
,
July 11 2023
Generate interview questions from a candidate’s tweets

Make an AI assistant to prepare for interviews with LangChain and Streamlit

LLMs
by
Greg Kamradt
,
June 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Introducing column config ⚙️

Take st.dataframe and st.data_editor to the next level!

Product
by
Lukas Masuch and 
1
 more,
June 22 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Becky O'Connor - Streamlit
https://blog.streamlit.io/author/becky/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Becky O'Connor
1 post
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to analyze geospatial Snowflake data in Streamlit
https://blog.streamlit.io/how-to-analyze-geospatial-snowflake-data-in-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

By Becky O'Connor
Posted in Snowflake powered ❄️, July 24 2023
Tooling and platform access
Snowflake access
Streamlit Community Cloud
Tableau Cloud
GitHub account
Client tooling for coding
Python libraries
Data access
Various GeoJSON files
Data files for road incidents
Creating a simple map with Folium using data retrieved through Snowpark
Loading the URLs for the images
Working with the accident data
Geospatial data engineering using Snowpark
Working with polygon data
Embedding Tableau reports in Streamlit
Using Carto's Analytic Toolbox to work with geospatial indexes and clustering
Using Streamlit to run a predictive model
Wrapping up
Contents
Share this post
← All posts

Hey community! 👋

My name is Becky O’Connor, and I’m a Senior Solution Engineer at Snowflake. I’ve been working in the data and business intelligence field for nearly ten years, with a particular focus on geospatial data.

Geospatial data is ubiquitous and shouldn't be treated as a niche subject separate from other datasets. Tools such as Snowflake, Streamlit, Carto, and Tableau make it more accessible. Geospatial representations of space enable sophisticated calculations to be made against existing datasets. And with curated datasets readily available in the Snowflake Data Cloud, the possibilities are endless.

I’ve created an example geospatial app to display vehicle incident data in an easily understandable format:

In this post, I’ll walk you through:

Tooling and platform access
Creating a simple map using Folium using data retrieved through Snowpark
Geospatial data engineering using Snowpark
Embedding Tableau reports in Streamlit
Leveraging CARTO analytic toolbox to work with geospatial indexes and clustering
Using Streamlit to run a predictive model


🚙
To view the geospatial app example, go here. To view the code, go here.
Tooling and platform access
Snowflake access

If you don’t have access to a Snowflake account, sign up for a free trial. This is where you’ll store, process, and service your Streamlit app and get a live dataset for the embedded Tableau dashboards.

Streamlit Community Cloud

To make your app publically available, deploy it to the Streamlit Community Cloud.

Tableau Cloud

To display Tableau reports inside your app, you’ll need access to a Tableau Cloud account.  For development and learning purposes, create your own personal Tableau Cloud site here.

GitHub account

To deploy your app to the Community Cloud, you’ll need a GitHub account. If you don’t have one, use this guide to create one.

Client tooling for coding

I recommend using Visual Studio Code, which has a great Snowflake add-in. It enables you to leverage Snowflake similarly to the Snowflake Online Tool (Snowsight). Additionally, a Jupyter Notebook extension lets you engineer data using Snowpark DataFrames. Finally, you can publish your files to GitHub using this tool.

Python libraries

In addition to the standard libraries, install the following libraries listed in the requirements.txt file in GitHub (this way, the Community Cloud will install them):

altair==4.2.2
contextily==1.3.0
folium==0.14.0
geopandas==0.12.2
matplotlib==3.7.0
snowflake-connector-python==3.0.1
snowflake-snowpark-python==1.5.1
streamlit-folium==0.11.1
snowflake-snowpark-python[pandas]


You’ll also need to install Streamlit to run the app in a local environment (when developing locally, you’ll need to install an Anaconda environment):

pip install streamlit



🚙
NOTE: Soon Streamlit will also be available within the Snowsight user interface. Watch this video for the Streamlit in Snowflake (SiS) sneak peek.
Data access

The data you’ll see within the app consists of the following:

Various GeoJSON files

Here is an example of one of the GeoJSON files I utilized—GeoJSON File for Integrated Care Boards:

If you want, you can try some of the other GeoJSON files (the files I used were stored here).

When choosing a GeoJSON file, pick one of the generalized datasets available on the GeoPortal. I used the ones generalized to the nearest 20m. If you're dealing with a large GeoJSON file (more than 16MB), you must split it into separate polygons. This is especially necessary when dealing with GeoJSON files that contain thousands of polygons. Just use Python functions to split a GeoJSON file into rows.

Also, make sure that each polygon is no larger than 9MB. I'd question the need for a single polygon to have more than 9MB of coordinates. If you're dealing with larger polygons, consider splitting them into smaller ones or simplifying them.

Data files for road incidents

I used the openly available data provided by www.data.gov.uk (find it here):

And I sued the following supporting document:

Creating a simple map with Folium using data retrieved through Snowpark

To showcase the seven worst driving locations in a Streamlit app, I used Folium. It helped visualize geospatial data stored in Snowflake on a map:

I had fun researching the best and worst places to drive in the U.K. and found a fascinating blog post with great information. It listed both good and bad places to drive. I manually selected the key facts and typed them into a JSON document. Although I could’ve used website scraping to automate the process, that wasn't my focus.

To accomplish this, I used Visual Studio Code and the Jupyter Notebook add-in:

Loading this data into Snowflake was very easy. I pushed the list into a Snowpark Python DataFrame and loaded it into a Snowflake table.

Before starting, you’ll need to establish a Snowflake session.

from snowflake.snowpark import Session

CONNECTION_PARAMETERS = {
    'url': account,
    'ACCOUNT': account,
    'user': username,
    'password': password,
    'database': database,
    'warehouse': warehouse,
    'role': role,
    'schema': 'RAW'
}
session = Session.builder.configs(CONNECTION_PARAMETERS).create()


Next, create a Snowpark Python DataFrame from the previously created list:

worst_drives = session.create_dataframe(worst_drives_list)


This effectively creates a temporary table inside Snowflake. Once the session ends, the table is removed. To view the table, simply write worst_drives.show(). To bring it back into pandas from Snowpark, write worst_drives.to_pandas().



🚙
TIP: If you only want to preview the data and not import the entire table into memory, use worst_drives.limit(10).to_pandas().

Afterward, I committed it as a Snowflake table:

worst_drives.write.**mode**("overwrite")\\
.**save_as_table**("UK_Worst_Cities_To_Drive_Demo")



🚙
TIP: Install the Snowflake add-in for VS Code to check if the table exists in Snowflake. Use it to run SQL scripts to initially stage and load additional data sets into Snowflake.

You can also use it to quickly copy table names into your notebook. Set the context to match what you specified in your notebook session, as this will result in a better experience.

Pictures in the tooltips:

The pictures are stored in Snowflake instead of GitHub because, like geospatial data, images are considered data sources. To load them into a directory, use Snowflake's unstructured data capabilities—create a stage for the images. It's crucial to ensure that server-side encryption is utilized, as images won’t render otherwise.

To execute the necessary SQL, use the Snowflake add-in in VS Code:

create or replace stage IMAGES encryption = (type = 'SNOWFLAKE_SSE') 
directory=(enable = TRUE);


To upload images into Snowflake, use Snow SQL from a terminal within VS Code:

Now you have the data and images to create the first page in Streamlit. Let's switch over to Streamlit.

For this visualization, use Folium to view multiple distinct markers with tooltips containing additional information about each town.

Loading the URLs for the images
@st.cache_data
def images_journeys():
    return session.sql('''SELECT  *, 
GET_PRESIGNED_URL(@LOCATIONS,RELATIVE_PATH,172800) 
URL FROM directory(@LOCATIONS) ORDER BY RELATIVE_PATH''').to_pandas()


Create a function that renders image icons using a secure URL generated by the GET_PRESIGNED_URL function. This generates a new URL every time data is refreshed, with the last variable specifying the number of seconds the URL is valid for (read more here).

To optimize performance, utilize Streamlit's caching capabilities to load URLs only once. This means that once the URLs are loaded into a pandas DataFrame, they won’t be loaded again unless you restart the app. If your images or URLs expire quickly, consider using the TTL variable within the st.cache_data function.

If you need to change the icons based on user interactions, you’ll need to parameterize the function. In that case, caching will refresh when the data changes.

To load the details of the city, use st.cache_data:

@st.cache_data
def retrieve_worst_cities():
    return df.sort(f.col('RANK').asc()).to_pandas()


This function will be used to retrieve the coordinates for each city which will be plotted on the map.  In addition, it will retrieve information about each city which will feature in the tooltip.

Next, create your tabs:

tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
    [
        "7 worst cities",
        "Main Towns in England",
        "Incidents within Care Boards",
        "City Details",
        "Details within X miles radius of city",
        "Incidents Within Fire Services",
    ]
)


Create a title for Tab 1 using st.markdown because it offers more options compared to st.title, while still being easy to use.

Use st.sidebar to create the logo stored in Snowflake. Utilize the components.html function in Streamlit to render the image from Snowflake:

icon = images_icons()
components.html(f'''
<table><tr><td bgcolor="white", width = 200>
<img src="{icon.loc[icon['RELATIVE_PATH'] == 'icon.jpeg'].URL.iloc[0]}", 
width=100>
</td><td bgcolor="white" width = 300><p style="font-family:Verdana; 
color:#000000; font-size: 18px;">Geospatial with Snowflake</td></tr></table>
''')


The app lets the user select a city, which is then highlighted and used as a filter in another tab. Note that the data is stored in a pandas DataFrame, which was originally loaded through a Snowpark DataFrame:

selected = st.radio("CHOOSE YOUR CITY:", retrieve_worst_cities().CITY)


Once selected, the city will change the color of the selected marker to red.

selected_row = retrieve_worst_cities()[retrieve_worst_cities()["CITY"] == selected]


To create a Folium map with a marker for every location in the dataframes, put the following code inside a 'for' loop—it’ll iterate seven times for all locations and one time for the selected location:

#draw a map which centres to the coordinates of the selected city.

m = folium.Map(
    location=[selected_array2.iloc[0].LATITUDE, selected_array2.iloc[0].LONGITUDE],
    zoom_start=8,
    tiles="openstreetmap",
)

trail_coordinates = df2.sort('RANK').select('LATITUDE','LONGITUDE').to_pandas().to_numpy()

#add information to each point which includes tool tips.  This includes
#the images as well as the other data elements.

trail_coordinates = session.table("UK_Worst_Cities_To_Drive").sort('RANK').select('LATITUDE','LONGITUDE').to_pandas().to_numpy()

#add information to each point which includes tool tips

for A in range (0,7):
    
    html = f'''
    <body style="background-color:#F0F0F0;">
    <p style="font-family:verdana">
    <b> WORST DRIVING CITY INSIGHTS
    <BR><BR>
    <b>Rank: </b>{retrieve_worst_cities().RANK.iloc[A]}<BR><BR>
    <img src="{images_journeys().iloc[A].URL}", width=100>
    <br><br><b>City:</b>
    {retrieve_worst_cities().CITY.iloc[A]}<BR><BR>

    <hr>
        
    <p style="font-family:verdana">
        
    <b>STATS</b>
    <BR><BR>
    Crashes:  {retrieve_worst_cities()['CRASHES'].iloc[A]}
    <BR>
    Congestion Level: {retrieve_worst_cities()['Congestion Level %'].iloc[A]}
    <BR>
    Cars Per Parking Space: {retrieve_worst_cities()['Cars Per Parking Space'].iloc[A]}
    <BR>
    EV Charging Points: {retrieve_worst_cities()['EV Charging Points'].iloc[A]}
    <BR>
    Air Quality Index Score: {retrieve_worst_cities()['Air Quality Index Score'].iloc[A]}
    <BR>
    Bus Routes: {retrieve_worst_cities()['Bus Routes'].iloc[A]}
    <BR>
    Overall Score: {retrieve_worst_cities()['Overall Score'].iloc[A]}
    <BR>
    <hr>
    <p style="font-family:verdana">
    <b>Worst Junction: </b>{retrieve_worst_cities()['Bad Junction in City'].iloc[A]} 
    <BR><BR>
    <b>Source:</b><a href ="{retrieve_worst_cities().SOURCE.iloc[A]}" target="popup"> {retrieve_worst_cities().SOURCE.iloc[A]}</a></p>
    <hr>
    <p style="font-family:verdana">
    <br>
    Info Gathered from Accident Data
    <br>
    <BR>
    According to the data from the Department of Data, since the year 2000 
    there have been <font style="color:red"> {retrieve_worst_cities()['ACCIDENTS'].iloc[A]} </font> accidents.  
    Of which, <font style="color:red"> {retrieve_worst_cities()['VEHICLES'].iloc[A]} </font> vehicles and <font style="color:red"> {retrieve_worst_cities()['CASUALTIES'].iloc[A]} </font> 
    casualties were involved.

        
    '''

    html2 = f'''
    <body style="background-color:#F0F0F0;">
    <p style="font-family:verdana">
    <b> WORST DRIVING CITY INSIGHTS
    <BR><BR>
    <b>Rank: </b>{selected_row.iloc[0].RANK}<BR><BR>
    <img src="{images_journeys().iloc[selected_row.iloc[0].ID-1].URL}", width=100>
    <br><br><b>City:</b>
    {selected_row.iloc[0].CITY}<BR><BR>

    <hr>
        
    <p style="font-family:verdana">
        
    <b>STATS</b>
    <BR><BR>
    Crashes:  {selected_row.iloc[0].CRASHES}
    <BR>
    Congestion Level: {selected_row.iloc[0]['Congestion Level %']}
    <BR>
    Cars Per Parking Space: {selected_row.iloc[0]['Cars Per Parking Space']}
    <BR>
    EV Charging Points: {selected_row.iloc[0]['EV Charging Points']}
    <BR>
    Air Quality Index Score: {selected_row.iloc[0]['Air Quality Index Score']}
    <BR>
    Bus Routes: {selected_row.iloc[0]['Bus Routes']}
    <BR>
    Overall Score: {selected_row.iloc[0]['Overall Score']}
    <BR>
    <hr>
    <p style="font-family:verdana">
    <b>Worst Junction: </b>{selected_row['Bad Junction in City'].iloc[0]} 
    <BR><BR>
    <b>Source:</b><a href ="{selected_row.SOURCE.iloc[0]}" target="popup"> {selected_row.SOURCE.iloc[0]}</a></p>
    <hr>
    <p style="font-family:verdana">
    <br>
    Info Gathered from Accident Data
    <br>
    <BR>
    According to the data from the Department of Data, since the year 2000 
    there have been <font style="color:red"> {selected_row.ACCIDENTS.iloc[0]} </font> accidents.  
    Of which, <font style="color:red"> {selected_row.VEHICLES.iloc[0]} </font> vehicles and <font style="color:red"> {selected_row.CASUALTIES.iloc[0]} </font> 
    casualties were involved.

        
    '''
		# I then create iframes for each tool tips.  This contains the html.
		
		iframe = folium.IFrame(html,width=700,height=400)
		iframe2 = folium.IFrame(html2,width=700,height=400)
		#I create the tooltip itself (folium calls these popup's)
		popup = folium.Popup(iframe,max_width=700)
		popup2 = folium.Popup(iframe2,max_width=700)
		#finally i apply the popup to the icon.
		folium.Marker(
		    retrieve_worst_cities()[['LATITUDE', 'LONGITUDE']].iloc[A].to_numpy(),
		    popup=popup,
		    icon=folium.Icon(color='blue', prefix='fa', icon='car'),
		    icon_size=(40, 40),
		).add_to(m)
		folium.Marker(
		    selected_row[['LATITUDE', 'LONGITUDE']].iloc[0],
		    popup=popup2,
		    icon=folium.Icon(color='red', prefix='fa', icon=f'{selected_row.RANK.iloc[0]}'),
		    icon_size=(40, 40),
		).add_to(m)


Finally, position the map on the page.

st_data = folium_static(m, width=1200, height=800)

🚙
NOTE: folium_static is perhaps a better option than folium in this use case, as it prevents the map from reloading at every interaction, improving the user experience.
Working with the accident data

Leverage Azure Blob storage to store the Accident, Vehicle, and Casualty data. Create a dynamic external stage using a SAS key:

create or replace stage VEHICLE_DATA_AZURE
url='azure://MY_BLOB_STORAGE_ACCOUNT.blob.core.windows.net/vehincidents'
 credentials=(azure_sas_token='MY TOKEN')
  directory=(enable = TRUE);


Then, use the VS Code add-in to bring this data into tables:

Geospatial data engineering using Snowpark

Use Snowpark for data engineering via your VS Code notebook, opting to use Snowpark DataFrames instead of an SQL file.

To begin, import a GeoJSON file of towns and cities in England, which provides the shape of each major town and city. Use this process to create the file format:

sql = [f'''create or replace file format JSON
    type = JSON
    STRIP_OUTER_ARRAY = TRUE;
''',
f'''PUT file://towns_and_cities.geojson @DATA_STAGE
    auto_compress = false
    overwrite = true;
'''
]

for sql in sql:
    session.sql(sql).collect()


Next, transform the geometry data held within the stage into a Snowpark DataFrame:

geometry_data = session.sql('''select $1::Variant V 
FROM @DATA_STAGE/towns_and_cities.geojson (FILE_FORMAT =>'JSON') ''')


Finally, enter the raw data into a table (you can view it in the Snowflake add-in):

geometry_data.write.mode("overwrite").save_as_table("GEOMETRY_DATA")

Working with polygon data

To work with polygon data, use:

SnowSQL to upload your GeoJSON files to a stage
The Snowflake function ST_GEOGRAPHY to convert the polygons into a format that Tableau can recognize

Use Snowpark to see what the shape of the imported data looks like:

🚙
NOTE: All the features are inside one row. I’m selecting one of the records nested inside the row.

To make the data more readable, use Lateral Flatten to "explode" the GeoJSON into multiple rows. Return to pandas to view the data in a more readable format:

Instead of using SQL syntax, you can use Snowpark's DataFrame syntax:

table1 = session.table('GEOMETRY_DATA')

flatten = table1.join_table_function("flatten",col("V"),lit("features"))

towns_cities = flatten.select(\\
    col('VALUE')["properties"]["TCITY15CD"].cast(StringType()).alias("TOWN_CODE")\\
    ,( col('VALUE')["properties"]["TCITY15NM"]).cast(StringType()).alias("TOWN_NAME")\\
    ,call_udf("TO_GEOGRAPHY",col('VALUE')["geometry"]).alias("GEOMETRY")\\
    
    )


When importing the geometries, notice that the precision is very high:

Using polygons with very high precision, which is often unnecessary, can negatively impact performance. To address this, use the Python Shapely library available within the Snowflake service without requiring installation. With this library, create a custom function to reduce the precision of polygons.

Once deployed, this function will be stored and processed in Snowflake like any other function:

sql = '''

CREATE OR REPLACE FUNCTION py_reduceprecision(geo geography, n integer)
returns geography
language python
runtime_version = 3.8
packages = ('shapely')
handler = 'udf'
AS $$
from shapely.geometry import shape, mapping
from shapely import wkt
def udf(geo, n):
    if n < 0:
        raise ValueError('Number of digits must be positive')
    else:
        g1 = shape(geo)
        updated_shape = wkt.loads(wkt.dumps(g1, rounding_precision=n))
        return mapping(updated_shape)
$$

'''
session.sql(sql).collect()

towns_cities_reduced = towns_cities.with_column(
    "GEOMETRY", call_udf("py_reduceprecision", (col("GEOMETRY"), 6))
)


Finally, save the geometry dataframe into a Snowflake table:

towns_cities.write.mode("overwrite").save_as_table("TOWNS_CITIES")


To associate incident data with polygons, you'll need a spatial join because the accident data doesn't include town or city names. Spatial joins can be computationally intensive, particularly when working with large datasets. But in this case, running a spatial join view with 17 million data points and 35,000 polygons took only 41 seconds on an X-Small virtual warehouse:

This geospatial function is natively available for instant querying within Snowflake. You don't have to use geospatial functions in Snowpark DataFrames—use them within SQL just like any other function:

CREATE OR REPLACE VIEW LSOA_POP AS

select C.LSOA21CD, c.LSOA21NM, A.* from
(SELECT * FROM
(select * from population_health_table) limit 17000000)a

inner join postcodes b on a.postcode = b.postcode
inner join

GEOGRAPHIC_BOUNDARIES.UK_BOUNDARIES.LSOA_BOUNDARIES c on

ST_WITHIN(ST_MAKEPOINT( B.LONGITUDE,B.LATITUDE), to_geography( GEOMETRY));


If you have billions of data points and various polygons to join several datasets, spatial indexes could be a more efficient choice.

Carto has created an Analytical Toolbox, which includes the H3 spatial indexing system. I'll cover this system in more detail in another post. For now, use the standard spatial join feature to assign a town to every vehicle accident that has corresponding latitude and longitude:

ACCIDENTS_KNOWN_LOCATION.with_column('POINT',call_udf('ST_makepoint',col('LONGITUDE'),col('LATITUDE')))\\
    .write.mode("overwrite").save_as_table("ACCIDENTS_POINTS")#write a table with accident points
ACCIDENTS_IN_TOWNS_CITIES = session.table('ACCIDENTS_POINTS')\\
    .join(TOWNS_CITIES,call_udf('ST_WITHIN',col('POINT'),col('GEOMETRY'))==True)\\
        .select('ACCIDENT_INDEX','TOWN_CODE','TOWN_NAME','GEOMETRY')


After completion, write the table to Snowflake and use Tableau to render the polygons along with the corresponding accident data:

Tableau's native spatial support works seamlessly with Snowflake in "live" query mode.

Embedding Tableau reports in Streamlit

The components.html function is a valuable tool for embedding Tableau reports into Streamlit. It also enables you to link interactions from Streamlit inputs to Tableau.

#an example of the embed code behind one of the tableau 
#reports within my streamlit app

components.html(f'''<script type='module' 
        src='<https://eu-west-1a.online.tableau.com/javascripts/>
				api/tableau.embedding.3.latest.min.js'>
        </script><tableau-viz id='tableau-viz' 
				src='<https://eu-west-1a.online.tableau.com/t/eccyclema/views/>
				carsFIRE_16844174853330/FIRE_SERVICEintoH3' 
        width='1300' height='1300' hide-tabs toolbar='hidden' token = {token}>
        
         ###<viz-filter field="FRA22NM" value="{FIRE_SERVICE}"> </viz-filter>
        
        <viz-parameter name="MEASURE" value={parameterno}></viz-parameter>
        
        </tableau-viz>''', height=1300, width=1300)


The dropdown at the top uses the st.selectbox widget to display options. This widget passes the selected parameter to view the results. In the given code, the dropdown is referred to as the variable FIRE_SERVICE:

In addition to passing parameters, you can hide Tableau toolbars and tabs to seamlessly integrate the Tableau dashboard into your app. The select box can also control other items not intended for Tableau. For example, my code filters a Folium chart on another tab using the same dropdown.

One potential issue is that clicking on an embedded Tableau tab requires signing in to Tableau, which can be visually unappealing. To avoid this, you can use Tableau's connected app capabilities, explained here. This allows the token to be passed from Streamlit to Tableau without requiring the user to log in.

Streamlit stores all secrets/credentials in the secrets.toml file. In the Streamlit Community Cloud, the secrets are managed by the UI within the settings area:

To make this work, encode the token configured in Tableau Online to your Streamlit app (in my code, I applied the variable token to the components.html function):

#token generator for tableau automatic login

token = encode(
	{
		"iss": st.secrets["iss"],
		"exp": datetime.utcnow() + timedelta(minutes=5),
		"jti": str(uuid4()),
		"aud": "tableau",
		"sub": st.secrets["sub"],
		"scp": ["tableau:views:embed", "tableau:metrics:embed"],
		"Region": "East"
	},
		st.secrets["tableau_secret"],
		algorithm = "HS256",
		headers = {
		'kid': st.secrets["kid"],
		'iss': st.secrets["iss"]
        }
  )

Using Carto's Analytic Toolbox to work with geospatial indexes and clustering

Take advantage of H3 functions within Snowflake by using Carto. Carto extends the capabilities of Snowflake without requiring any installation. Simply obtain the share from the marketplace, and the additional functions will be instantly available. This is one of the advantages of Snowflake's sharing capabilities:

Once deployed (which takes only a second), the functions will appear as a new database in Snowflake. You can also verify this in VS Code:

H3 is a hexagon hierarchy covering the entire planet, with 14 different resolutions (read more here). Each hexagon, regardless of size, has a unique index. A hexagon of the highest resolution could fit inside a child's sandbox:

Converting spatial data into consistently sized hexagons is an effective way to aggregate points into hexagonal samples. Carto provides functions for creating an H3 index from latitude and longitude, which can group hexagons into points. There is also a polyfill function, which can fill a polygon with hexagons. The unique H3 index is always placed consistently, so when joining points to a polygon, you'd join polygons instead of performing a spatial join. This speeds up processing time and is cost-effective.

In the "City Details" tab, you can change the size of the H3 resolution within Streamlit. This calls the Carto function and processes it against the accident data:

Wrap this code in a function that resizes hexagons for all data within a chosen resolution, year, and town when executed:

import snowflake.snowpark.functions as f
@st.cache_data(max_entries=5)
def CARTO(r,s_year, s_town_code):

    ACCIDENT_TOWN_CITY = session.table('ACCIDENTS_WITH_TOWN_CITY_DETAILS').filter(f.col('TOWN_CODE')==s_town_code)

    ACCIDENT_DETAILS = ACCIDENTS\\
      .filter(f.col('ACCIDENT_YEAR')==s_year)\\
      .join(ACCIDENT_TOWN_CITY,'ACCIDENT_INDEX')

    df = ACCIDENT_DETAILS\\
       .select('LATITUDE','LONGITUDE','ACCIDENT_INDEX','ACCIDENT_YEAR')\\
       .with_column('H3',\\
				f.call_udf('ANALYTICS_TOOLBOX.CARTO.H3_FROMLONGLAT',\\
				f.col('LONGITUDE'),f.col('LATITUDE'),r))\\
       .with_column('H3_BOUNDARY',\\
				f.call_udf('ANALYTICS_TOOLBOX.CARTO.H3_BOUNDARY',f.col('H3')))\\
      .with_column('WKT',f.call_udf('ST_ASWKT',f.col('H3_BOUNDARY')))\\
                
    accidents = df.group_by('H3').agg(f.any_value('WKT')\\
	.alias('WKT'),f.count('ACCIDENT_INDEX').alias('ACCIDENTS'))
    
    casualties_tot = df.join(session.table('CASUALTIES')\\
		.select('AGE_OF_CASUALTY','ACCIDENT_INDEX',\\
		'CASUALTY_SEVERITY'),'ACCIDENT_INDEX')

    casualties = df\\
    .join(session.table('CASUALTIES')\\
						.select('AGE_OF_CASUALTY','ACCIDENT_INDEX'\\
						,'CASUALTY_SEVERITY'),'ACCIDENT_INDEX')\\
            .with_column('FATAL',f.when(f.col('CASUALTY_SEVERITY')\\
						==1,f.lit(1)))\\
                            .with_column('SEVERE',\\
						f.when(f.col('CASUALTY_SEVERITY')==2,f.lit(1)))\\
            .with_column('MINOR',f\\
						.when(f.col('CASUALTY_SEVERITY')==3,f.lit(1)))\\
            .group_by('H3').agg(f.any_value('WKT').alias('WKT')\\
            ,f.count('ACCIDENT_INDEX').alias('CASUALTIES')
           ,f.avg('AGE_OF_CASUALTY').alias('AVERAGE_AGE_OF_CASUALTY')\\
           ,f.sum('FATAL').alias('FATAL')\\
           ,f.sum('SEVERE').alias('SEVERE')\\
           ,f.sum('MINOR').alias('MINOR'))


The boundaries function converts the index into a geometry. These hexagons can be visualized in Tableau just like standard polygons.

In addition to a comprehensive list of standard Snowflake functions, Carto's toolkit provides many other functions for free, including K-means clustering (read more here):

Using Streamlit to run a predictive model

Make a Streamlit page for predicting the number of casualties that may occur within a certain number of hexagons away from a fire station. Use H3_KRING_DISTANCES to determine the distance based on rings of hexagons around the point.

Before creating the app, train a random forest regressor model that relies on a Snowpark-optimized data virtual warehouse within Snowflake. After training, deploy the model and create a function to use in Streamlit. The dataset used for training aggregates using H3 Indexes at resolution 7—about every 5 kilometers (read more here).

Note the following SQL query (I'm parameterizing it using the select boxes and sliders):

sql = f'''SELECT POLICE_FORCE "Police Force", 
            H3INDEX,
            any_value(st_aswkt(ANALYTICS_TOOLBOX.CARTO.H3_BOUNDARY(H3INDEX))) WKT,
            VEHICLE_MANOEUVRE "Vehicle Manoeuvre", 
            ROAD_TYPE "Road Type", 
            VEHICLE_TYPE "Vehicle Type", 
            WEATHER_CONDITIONS "Weather Conditions",
            DAY_OF_WEEK "Day of Week", 
            AVG(ENGINE_CAPACITY_CC)::INT "Engine Capacity CC",
            AVG(ADJUSTED_ENGINE_SIZE)::INT "Adjusted Engine Capacity CC",
            AVG(AGE_OF_DRIVER)::INT "Age of Driver",
            AVG(ADJUSTED_AGE_OF_DRIVER)::INT "Adjusted age of Driver",
            AVG(AGE_OF_VEHICLE)::INT "Age of Vehicle", 
            AVG(ADJUSTED_AGE_OF_VEHICLE)::INT "Adjusted age of Vehicle",
            AVG(PREDICTED_FATALITY)::FLOAT PREDICTION 
    
            FROM

            (
            select VEHICLE_ACCIDENT_DATA.RAW.{choose_model}(
            js_hextoint(A.H3INDEX)
        
            ,VEHICLE_MANOEUVRE
            ,DAY_OF_WEEK
            ,ROAD_TYPE
            ,SPEED_LIMIT
            ,WEATHER_CONDITIONS
            ,VEHICLE_TYPE
            ,AGE_OF_DRIVER * {age_of_driver}
            ,AGE_OF_VEHICLE * {age_of_vehicle}
            ,ENGINE_CAPACITY_CC * {engine_size}

            )PREDICTED_FATALITY,
            A.H3INDEX,
            B.VALUE AS POLICE_FORCE, 
            C.VALUE AS VEHICLE_MANOEUVRE,
            D.VALUE AS ROAD_TYPE,
            E.VALUE AS VEHICLE_TYPE,
            F.VALUE AS WEATHER_CONDITIONS,
            G.VALUE AS DAY_OF_WEEK,
            AGE_OF_DRIVER,
            AGE_OF_VEHICLE,
            ENGINE_CAPACITY_CC,
            AGE_OF_DRIVER * ({age_of_driver}) ADJUSTED_AGE_OF_DRIVER,
            AGE_OF_VEHICLE * ({age_of_vehicle}) ADJUSTED_AGE_OF_VEHICLE,
            ENGINE_CAPACITY_CC * ({engine_size}) ADJUSTED_ENGINE_SIZE

            from 

            (SELECT * FROM (SELECT * FROM accidents_with_perc_fatal 
						WHERE YEAR = 2021))A 

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = 
						'police_force')B
            ON A.POLICE_FORCE = B.CODE

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = 
						'vehicle_manoeuvre')C
            ON A.VEHICLE_MANOEUVRE = C.CODE

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = 
						'road_type')D
            ON A.ROAD_TYPE = D.CODE

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = 
						'vehicle_type')E
            ON A.VEHICLE_TYPE = E.CODE

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD 
						= 'weather_conditions')F
            ON A.WEATHER_CONDITIONS = F.CODE

            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD 
						= 'day_of_week')G
            ON A.DAY_OF_WEEK = G.CODE

            INNER JOIN (
                SELECT VALUE:index H3INDEX, VALUE:distance DISTANCE FROM 

                (SELECT FIRE_STATION,  
								ANALYTICS_TOOLBOX.CARTO.H3_KRING_DISTANCES
								(ANALYTICS_TOOLBOX.CARTO.H3_FROMGEOGPOINT(POINT, 7), 
								{choose_size})HEXAGONS 

                FROM  (SELECT * FROM FIRE_STATIONS WHERE FIRE_STATION =
								 '{select_fire_station}')),lateral flatten (input =>HEXAGONS)) H

                ON A.H3INDEX = H.H3INDEX
            )

            GROUP BY POLICE_FORCE, VEHICLE_MANOEUVRE, ROAD_TYPE, VEHICLE_TYPE, WEATHER_CONDITIONS, DAY_OF_WEEK, H3INDEX

            '''


To prevent running the model every time a slider is changed, add a button inside a Streamlit form. This will allow you to execute the SQL code only when you're ready. In the future, you may convert this code section into a stored procedure. This approach creates a table with the results every time the model is run, which isn't ideal from a security standpoint. Converting the code into a stored procedure would be better practice and reduce the amount of Streamlit code required.

Wrapping up

Creating this app was a lot of fun. While it may not be a production-ready application, I wanted to share my vision of how great technologies can work together to create a geospatial solution.

I hope you found this post interesting. If you have any questions, please drop them in the comments below, message me on LinkedIn, or email me. Finally, click here to view the example app.

Happy coding! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

kaarthik Andavar - Streamlit
https://blog.streamlit.io/author/kaarthik/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by kaarthik Andavar
1 post
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

snowChat: Leveraging OpenAI's GPT for SQL queries
https://blog.streamlit.io/snowchat-leveraging-openais-gpt-for-sql-queries/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

By kaarthik Andavar
Posted in Snowflake powered ❄️, July 25 2023
What is snowChat?
How to embed Snowflake schema into a vector database
How to create a conversational chain using LangChain
How to connect the chain response to Snowflake
How to design a chat-like interface using Streamlit
How to deploy to the Streamlit Community Cloud
Wrapping up
Contents
Share this post
← All posts

Hey, fellow tech enthusiasts! 👋

I'm Kaarthik, an analytics engineer with a passion for building innovative AI applications. My expertise lies at the intersection of AI and data, where I explore and refine the power of AI to redefine data-driven solutions.

Do you struggle with complex SQL queries? Are you lost in a sea of tables while trying to locate a single piece of data? Fear not, for I have created snowChat to solve these problems!

In this post, I'll show you:

How to embed a Snowflake schema into a vector database
How to create a conversational chain using LangChain
How to connect the chain response to Snowflake
How to design a chat-like interface using Streamlit
How to deploy your solution to the Streamlit Community Cloud


❄️
Ready to leap right in? Check out the app and the full code.

But first, let's talk about…

What is snowChat?

snowChat is a powerful and user-friendly application that enables users to interact with their Snowflake DataBase using natural language queries.

snowChat leverages OpenAI's GPT model to convert natural language into SQL queries, making it ideal for users who may not have a firm grasp of SQL syntax. And it has a  transformative impact on data interaction by expediting and streamlining data-driven decision-making.

Let's take a look at the tech stack on which snowChat is built:

Streamlit: The UI magic
Snowflake: The database powerhouse
GPT-3.5 and LangChain: The language model maestros
Supabase: The vector database virtuoso

Here's a glance at snowChat's architecture:

All set? Let's get cracking!

How to embed Snowflake schema into a vector database

To start, follow these steps:

Clone the GitHub repo
Run pip install -r requirements.txt to install all the required packages
Get the Data Definition Language (DDL) for all tables from snowflake.account_usage.tables:


OPENAI_API_KEY=

#snowflake
ACCOUNT=
USER_NAME=
PASSWORD=
ROLE=
DATABASE=
SCHEMA=
WAREHOUSE=

SUPABASE_URL=
SUPABASE_SERVICE_KEY=



Use ChatGPT to convert the DDL to markdown format.
Store the schema files for each table in the docs/ folder.
Create an account with Supabase, set up a free database, and configure environment variables for the .streamlit folder in secrets.toml (remember to include your Snowflake credentials).

Your final secrets.toml should look like this:

[streamlit]
SUPABASE_URL = "<https://yourapp.supabase.co>"
SUPABASE_KEY = "yourkey"

[snowflake]
SNOWFLAKE_ACCOUNT = "youraccount"
SNOWFLAKE_USER = "youruser"
SNOWFLAKE_PASSWORD = "yourpassword"
SNOWFLAKE_DATABASE = "yourdatabase"
SNOWFLAKE_SCHEMA = "yourschema"
SNOWFLAKE_WAREHOUSE = "yourwarehouse"




Run the Supabase scripts found under supabase/scripts.sql in the Supabase SQL editor to activate the pgvector extension, create tables and set up a function.
Run python ingest.py to convert your documents into vectors and store them in the Supabase table named 'documents.'

The 'documents' table in Supabase should look like this:

| id | vector   | contents         |
|----|----------|------------------|
| 1  | [1,2,3]  | This is document |
| 2  | [4,5,6]  | This is another  |
| 3  | [7,8,9]  | This is a third   |





Et voila, the left part of the architecture is done!

How to create a conversational chain using LangChain

The core of snowChat is the "chain" function, which manages OpenAI's GPT model, the embedding model, the vector store, and the prompt templates. This functionality is encapsulated in LangChain.

The chain function takes user input, applies a prompt template to format it, and then passes the formatted response to an LLM with context retrieved from the vector store:

def get_chain(vectorstore):
    """
    Get a chain for chatting with a vector database.
    """
    q_llm = OpenAI(
        temperature=0,
        openai_api_key=st.secrets["OPENAI_API_KEY"],
        model_name="gpt-3.5-turbo-16k",
    )

    llm = ChatOpenAI(
				temperature=0,
        openai_api_key=st.secrets["OPENAI_API_KEY"],
        model_name="gpt-3.5-turbo",
    )

    question_generator = LLMChain(llm=q_llm, prompt=condense_question_prompt)

    doc_chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=QA_PROMPT)
    chain = ConversationalRetrievalChain(
        retriever=vectorstore.as_retriever(),
        combine_docs_chain=doc_chain,
        question_generator=question_generator,
    )
    return chain


Your get_chain function is now ready to use. Simply run chain = get_chain(SupabaseVectorStore) and start your fun with chain({"question": "Did you understand the chain?", "chat_history": chat_history}).

How to connect the chain response to Snowflake

After creating the chain, use this code to destructure the response from the answer key:

result = chain({"question": query, "chat_history": chat_history})

response = result['answer']



The response variable contains both SQL and an explanation. A helper function called get_sql extracts only the SQL code from the response. Once the SQL is obtained, it's sent to Snowflake (Snowpark) to fetch the data and create a DataFrame object.

There are two possible outcomes:

The model generates the SQL correctly: Display the output directly in Streamlit.
The model generates the SQL incorrectly: Send it back to the GPT model along with the error message and the SQL it generated (self-healing). This way, the model can correct its mistake and return the corrected response.

With this setup, the model is ready to work in full.

How to design a chat-like interface using Streamlit

Moving on to the design phase, let's shape up the chat interface.

The input will be designed using the st.chat_input() container from Streamlit's chat element:

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})



A helper function called message_func takes care of the styling for each message, including avatars on the side.

Following the steps outlined, you should have a fully functional chat app ready!

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})

How to deploy to the Streamlit Community Cloud

Let's bring snowChat to the world! To get started, head to Community Cloud, click "Create a new app," select your GitHub repo, and deploy. Don't forget to add the necessary secrets to the settings.

Congratulations! Now you can chat with your Snowflake data.

Wrapping up

That's a wrap, folks! In this post, we've discussed building snowChat, an intuitive tool that leverages the power of OpenAI's GPT to convert natural language into SQL queries. By breaking down the intimidating walls of SQL syntax, snowChat fosters swift and effective data-driven decisions, even for those not well-versed in SQL.

I hope snowChat will increase your productivity and impress you with its cool functionalities (see what we did there? 😜). If you have any questions or doubts, please post them in the comments below or contact me on GitHub, LinkedIn, or Twitter.

Happy Streamlit-ing, and keep it cool! ❄️🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Collecting user feedback on ML in Streamlit

Improve user engagement and model quality with the new Trubrics feedback component

Advocate Posts
by
Jeff Kayne and 
1
 more,
May 4 2023
Chat with the Cat Generative Dialogue Processor (CatGDP)

Build your own catbot with a quirky persona!

Advocate Posts
by
Tianyi Pan
,
May 3 2023
The ultimate athlete management dashboard for biomechanics

Learn how to measure jump impulse, max force, and asymmetry with Python and Streamlit

Advocate Posts
by
Hansen Lu
,
April 27 2023
Creating a Time Zone Converter with Streamlit

6 steps on how to build your own converter

Advocate Posts
by
Vinícius Oviedo
,
April 25 2023
Create an animated data story with ipyvizzu and Streamlit

A tutorial on using ipyvizzu and ipyvizzu-story

Advocate Posts
by
Peter Vidos
,
April 20 2023
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

Advocate Posts
by
Dmitry Kosarevsky
,
April 18 2023
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

Advocate Posts
by
Kanak Mittal
,
April 11 2023
Building GPT Lab with Streamlit

12 lessons learned along the way

LLMs
by
Dave Lin
,
April 6 2023
Building an Instagram hashtag generation app with Streamlit

5 simple steps on how to build it

Advocate Posts
by
William Mattingly
,
March 29 2023
Hackathon 101: 5 simple tips for beginners

Prepare to win your first hackathon!

Tutorials
by
Chanin Nantasenamat
,
March 16 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Yuichiro Tachibana (Tsuchiya) - Streamlit
https://blog.streamlit.io/author/yuichiro/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Yuichiro Tachibana (Tsuchiya)
Software developer
2 posts
Facebook
Accessible color themes for Streamlit apps

Control your app’s color scheme and visual accessibility

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
May 5 2023
Developing a streamlit-webrtc component for real-time video processing

Introducing the WebRTC component for real-time media streams

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
February 12 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Accessible color themes for Streamlit apps
https://blog.streamlit.io/accessible-color-themes-for-streamlit-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Accessible color themes for Streamlit apps

Control your app’s color scheme and visual accessibility

By Yuichiro Tachibana (Tsuchiya)
Posted in Advocate Posts, May 5 2023
Five key components of the color theme editor app
1. Define preset colors
2. If set, load the theme config
3. Manage the edited color parameters in the session state
4. Set colors programmatically
5. Define a composed widget of a color picker and a slider
6. Use the WCAG contrast table
7. Show generated configs
8. Apply the edited theme to the app
Wrapping up
Contents
Share this post
← All posts

One great feature of Streamlit is theming, so you can control your app's color scheme. But creating a new color scheme can be hard. And making it visually accessible can be even harder.

So I made a color theme editor app with four color parameters of the Streamlit themes, which general-purpose color editors/generators don't even consider: primaryColor, backgroundColor, secondaryBackgroundColor, and textColor. I also followed the color contrast guidelines from the Web Content Accessibility Guidelines (WCAG) 2.0 standard for the app's visual accessibility.

In this post, I'll show you how to build this app in eight steps:

Define preset colors
If set, load the theme config
Manage the edited color parameters in the session state
Set colors programmatically
Define a composed widget of a color picker and a slider
Use the WCAG contrast table
Show generated configs
Apply the edited theme to the app
👉
You can assess the app here and the source code here. This article is based on the revision 7213dbb.
Five key components of the color theme editor app

The app consists of five components to help you create and test accessible color themes:

"Generate a random color scheme" button. Click this button to update the color theme with randomly selected colors that maintain high contrast between foreground and background.
Color pickers. Use these components to adjust colors and their luminance parameters. Use the luminance slider to adjust contrast while preserving the color's aesthetic.
WCAG contrast ratio table. Display a 2x2 matrix of contrast ratios for the selected colors and the corresponding WCAG 2.0 level (AA or AAA) for each ratio to make your color theme accessible.
Config generator. Generate content for the .streamlit/config.toml config file and a shell command with color theme arguments. Just copy/paste it into your project to apply the edited theme.
"Apply theme on this page" checkbox. Check this box to apply the color theme you are editing to the whole application, letting you preview the colors in a real Streamlit app and synchronize the changes.
1. Define preset colors

The app has preset colors that I picked from the Streamlit source code below:

Base theme
Light theme
Dark theme
preset_colors: list[tuple[str, ThemeColor]] = [
    ("Default light", ThemeColor(
            primaryColor="#ff4b4b",
            backgroundColor="#ffffff",
            secondaryBackgroundColor="#f0f2f6",
            textColor="#31333F",
        )),
    ("Default dark", ThemeColor(
            primaryColor="#ff4b4b",
            backgroundColor="#0e1117",
            secondaryBackgroundColor="#262730",
            textColor="#fafafa",
    ))
]

2. If set, load the theme config

This is a bit of a hack. You can access the global config object via st._config to get the theme config from it and to use it as a preset color:

@st.cache_resource
def get_config_theme_color():
    config_theme_primaryColor = st._config.get_option('theme.primaryColor')
    config_theme_backgroundColor = st._config.get_option('theme.backgroundColor')
    config_theme_secondaryBackgroundColor = st._config.get_option('theme.secondaryBackgroundColor')
    config_theme_textColor = st._config.get_option('theme.textColor')
    if config_theme_primaryColor and config_theme_backgroundColor and config_theme_secondaryBackgroundColor and config_theme_textColor:
        return ThemeColor(
            primaryColor=config_theme_primaryColor,
            backgroundColor=config_theme_backgroundColor,
            secondaryBackgroundColor=config_theme_secondaryBackgroundColor,
            textColor=config_theme_textColor,
        )

    return None

theme_from_initial_config = util.get_config_theme_color()
if theme_from_initial_config:
    preset_colors.append(("From the config", theme_from_initial_config))

3. Manage the edited color parameters in the session state

Store the RGB color values in the session state using the same keys as the color picker widgets. This lets you programmatically set the color picker values. It's useful when updating them with new values from the random color generator. These values are synced with the color picker widgets, so they must be compatible with their #RRGGBB format.

Aside from managing RGB values, you also maintain the HSL values in the session state to control the slider widgets. Using the HSL format makes it easier to edit colors more intuitively. To synchronize these two formats, create a utility function named set_color and use it every time you update the color values:

def sync_rgb_to_hls(key: str):
    # HLS states are necessary for the HLS sliders.
    rgb = util.parse_hex(st.session_state[key])
    hls = colorsys.rgb_to_hls(rgb[0], rgb[1], rgb[2])
    st.session_state[f"{key}H"] = round(hls[0] * 360)
    st.session_state[f"{key}L"] = round(hls[1] * 100)
    st.session_state[f"{key}S"] = round(hls[2] * 100)

def set_color(key: str, color: str):
    st.session_state[key] = color
    sync_rgb_to_hls(key)

if 'preset_color' not in st.session_state or 'backgroundColor' not in st.session_state or 'secondaryBackgroundColor' not in st.session_state or 'textColor' not in st.session_state:
    set_color('primaryColor', default_color.primaryColor)
    set_color('backgroundColor', default_color.backgroundColor)
    set_color('secondaryBackgroundColor', default_color.secondaryBackgroundColor)
    set_color('textColor', default_color.textColor)

4. Set colors programmatically

To set the currently edited colors from the selectbox widget with the preset colors, use set_color():

def on_preset_color_selected():
    _, color = preset_colors[st.session_state.preset_color]
    set_color('primaryColor', color.primaryColor)
    set_color('backgroundColor', color.backgroundColor)
    set_color('secondaryBackgroundColor', color.secondaryBackgroundColor)
    set_color('textColor', color.textColor)

st.selectbox("Preset colors", key="preset_color", options=range(len(preset_colors)), format_func=lambda idx: preset_colors[idx][0], on_change=on_preset_color_selected)


You can also implement a random color generator button:

if st.button("🎨 Generate a random color scheme 🎲"):
    primary_color, text_color, basic_background, secondary_background = util.generate_color_scheme()
    set_color('primaryColor', primary_color)
    set_color('backgroundColor', basic_background)
    set_color('secondaryBackgroundColor', secondary_background)
    set_color('textColor', text_color)


To implement util.generate_color_scheme, refer to this code. It generates colors with the constraint that the colors' luminance (the "L" in the HSL format) parameters have enough difference.

5. Define a composed widget of a color picker and a slider

This app provides a pair of a color picker and a slider to control the color's luminance parameter (to adjust the color contrast while maintaining its original appearance). There are four pairs of components. Each is defined by a function called color_picker to make it reusable.

One trick here is to set the on_change callback of each component to synchronize the RGB data and the HSL data in the session state:

def color_picker(label: str, key: str, default_color: str) -> None:
    col1, col2 = st.columns([1, 3])
    with col1:
        color = st.color_picker(label, key=key, on_change=sync_rgb_to_hls, kwargs={"key": key})
    with col2:
        r,g,b = util.parse_hex(default_color)
        h,l,s = colorsys.rgb_to_hls(r,g,b)
        if f"{key}H" not in st.session_state:
            st.session_state[f"{key}H"] = round(h * 360)

        st.slider(f"L for {label}", key=f"{key}L", min_value=0, max_value=100, value=round(l * 100), format="%d%%", label_visibility="collapsed", on_change=sync_hls_to_rgb, kwargs={"key": key})

        if f"{key}S" not in st.session_state:
            st.session_state[f"{key}S"] = round(s * 100)

    return color

primary_color = color_picker('Primary color', key="primaryColor", default_color=default_color.primaryColor)
text_color = color_picker('Text color', key="textColor", default_color=default_color.textColor)
background_color = color_picker('Background color', key="backgroundColor", default_color=default_color.backgroundColor)
secondary_background_color = color_picker('Secondary background color', key="secondaryBackgroundColor", default_color=default_color.secondaryBackgroundColor)

6. Use the WCAG contrast table

This table layout is created using stacked st.column(). The content of each cell is encapsulated within a reusable helper function, such as synced_color_picker or fragments.contrast_summary:

col1, col2, col3 = st.columns(3)
with col2:
    synced_color_picker("Background color", value=background_color, key="backgroundColor")
with col3:
    synced_color_picker("Secondary background color", value=secondary_background_color, key="secondaryBackgroundColor")

col1, col2, col3 = st.columns(3)
with col1:
    synced_color_picker("Primary color", value=primary_color, key="primaryColor")
with col2:
    fragments.contrast_summary("Primary/Background", primary_color, background_color)
with col3:
    fragments.contrast_summary("Primary/Secondary background", primary_color, secondary_background_color)

col1, col2, col3 = st.columns(3)
with col1:
    synced_color_picker("Text color", value=text_color, key="textColor")
with col2:
    fragments.contrast_summary("Text/Background", text_color, background_color)
with col3:
    fragments.contrast_summary("Text/Secondary background", text_color, secondary_background_color)


synced_color_picker() is just a color picker component, but its value is synchronized with the relevant color picker component that appeared above. Its value is also managed in the session state with the value argument and the on_change callback:

def synced_color_picker(label: str, value: str, key: str):
    def on_change():
        st.session_state[key] = st.session_state[key + "2"]
        sync_rgb_to_hls(key)
    st.color_picker(label, value=value, key=key + "2", on_change=on_change)


fragments.contrast_summary() renders the WCAG contrast info. See this code for its implementation.

7. Show generated configs

The app shows these ready-to-use code snippets and is done with st.code():

st.subheader("Config file (`.streamlit/config.toml`)")
st.code(f"""
[theme]
primaryColor="{primary_color}"
backgroundColor="{background_color}"
secondaryBackgroundColor="{secondary_background_color}"
textColor="{text_color}"
""", language="toml")

st.subheader("Command line argument")
st.code(f"""
streamlit run app.py \\\\
    --theme.primaryColor="{primary_color}" \\\\
    --theme.backgroundColor="{background_color}" \\\\
    --theme.secondaryBackgroundColor="{secondary_background_color}" \\\\
    --theme.textColor="{text_color}"
""")

8. Apply the edited theme to the app

For reviewing purposes, this app can apply the currently edited theme to itself. You can do it with the st._config object, as shown below:

if st.checkbox("Apply theme to this page"):
    st.info("Select 'Custom Theme' in the settings dialog to see the effect")

    def reconcile_theme_config():
        keys = ['primaryColor', 'backgroundColor', 'secondaryBackgroundColor', 'textColor']
        has_changed = False
        for key in keys:
            if st._config.get_option(f'theme.{key}') != st.session_state[key]:
                st._config.set_option(f'theme.{key}', st.session_state[key])
                has_changed = True
        if has_changed:
            st.experimental_rerun()

    reconcile_theme_config()

    fragments.sample_components("body")
    with st.sidebar:
        fragments.sample_components("sidebar")


Use st._config.set_option() to update the configuration values. Reload the app for the changes to take effect by using st.experimental_rerun(). It was inspired by jrieke/streamlit-theme-generator (here is the code).

Wrapping up

The Streamlit color theme editor app offers a simple and effective solution for creating visually appealing and accessible color themes for your apps. Thanks to the WCAG 2.0 standard and the real-time preview, your themes will now be attractive and accessible to all users.

Happy app-building! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

soma noda - Streamlit
https://blog.streamlit.io/author/soma/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by soma noda
1 post
Convert images into pixel art

A 5-step tutorial for making a pixel art converter app

Advocate Posts
by
soma noda
,
May 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Convert images into pixel art
https://blog.streamlit.io/convert-images-into-pixel-art/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Convert images into pixel art

A 5-step tutorial for making a pixel art converter app

By soma noda
Posted in Advocate Posts, May 8 2023
1. Create the program base
1-1. Install Streamlit and modules
1-2. Fill in the code
1-3. Run the program
2. Turn an image into dots
2-1. Mosaic
2-2. Execute mosaic processing
3. Make pixel art more like handwriting
3-1. Create a color palette
3-2. Prepare for using the color palette
3-3. Use a color palette
4. Make variables modifiable
4-1. Add a slider for the ratio
4-2. Add a select box for the color palette
5. Use additional filters
5-1. Add no palette
5-2. Add color reduction
5-3. Add an edge filter
Bonus. What to check if the program fails
Wrapping up
Contents
Share this post
← All posts

Hello everyone! 👋

My name is Akaz. I’m a student with a strong interest in data science and programming.

Have you ever had an experience where you didn't like an existing app? I love pixel art and wanted to convert images into pixel art. But none of the existing apps converted them just as I wanted. So I solved this problem by creating PixelArt-Converter.

In this post, I’ll show you how to make your own pixel art converter app in five easy steps:

Create the program base
Turn an image into dots
Make pixel art more like handwriting
Make variables modifiable
Use additional filters
👾
If you want to skip reading and check it out right away, here's the app and the repo code.
1. Create the program base
1-1. Install Streamlit and modules

Fill in the code in the console (skip the items that are already installed):

pip install pipenv
pipenv --python 3.10
pipenv install streamlit
pipenv install numpy
pipenv install opencv-python-headless
pipenv install pillow
pipenv shell


1-2. Fill in the code

Create a new directory and create main.py. Fill in the following code in the created main.py:

import streamlit as st
import streamlit.components.v1 as components
import numpy as np
import cv2
from PIL import Image
import csv
import os
import pandas as pd

class Converter():
    def __init__(self) -> None:
        self.color_dict = {}

class Web():
    def __init__(self) -> None:
        self.draw_text()

    def draw_text(self):
        st.set_page_config(
            page_title="Pixelart-Converter",
            page_icon="🖼️",
            layout="centered",
            initial_sidebar_state="expanded",
        )
        st.title("PixelArt-Converter")
        self.upload = st.file_uploader("Upload Image", type=['jpg', 'jpeg', 'png', 'webp'])
        self.original, self.converted = st.columns(2)
        self.original.title("original img")
        self.converted.title("convert img")

if __name__ == "__main__":
    web = Web()
    converter = Converter()
    if web.upload != None:
        img = Image.open(web.upload)
        img = np.array(img)
        web.original.image(web.upload)



Now that the foundation is complete, you'll use this foundation to create programs.

1-3. Run the program

Enter the following commands into the console to execute the program:

streamlit run main.py



2. Turn an image into dots

As it turns out, mosaicing an image makes it pixel art.

2-1. Mosaic

Add the function below to the Converter class:

def mosaic(self, img, ratio=0.1):
    small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)
    return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)



This mosaic function takes an image as an argument and performs mosaicing. The smaller the value of the ratio, the larger the mosaic.

Next, create a place to use the mosaic function. Add the following program at the end of if __name__:

img = converter.mosaic(img)
web.converted.image(img)


2-2. Execute mosaic processing

To execute the mosaic processing you've just added, enter the following command:

streamlit run main.py



Now you've taken care of the mosaic! One step closer to completion. 🥳

3. Make pixel art more like handwriting

You can mimic how pixel art is created to make it look more like hand-drawn pixel art.

3-1. Create a color palette

Hand-drawn pixel art often has a small number of colors. Therefore, create a color palette using a CSV file. The directory should be color/palette.csv:

255,127,127
255,191,127
255,255,127
127,255,127
127,191,255
127,127,255
0,0,0
255,255,255
128,128,128



The CSV above shows the color palette. The color palette uses RGB—one color per line.

3-2. Prepare for using the color palette

3-2-1. Create a function to read CSV

To use the color palette, the program must read a CSV. For this, add the following code to the Converter class:

def read_csv(self, path):
    with open(path) as f:
        reader = csv.reader(f)
        color = [[int(v) for v in row] for row in reader]
        return color



This code is a function to read a CSV. It reads it and returns an array.

3-2-2. Create a function to check colors from an image

Add the following code to Converter in main.py:

def color_change(self, r, g, b, color_pallet):
    if (r, g, b) in self.color_dict:
        return self.color_dict[(r, g, b)]
    # 最も近い色を見つける
    min_distance = float('inf')
    color_name = None
    for color in color_pallet:
        distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2
        if distance < min_distance:
            min_distance = distance
            color_name = color
    self.color_dict[(r, g, b)] = color_name
    return color_name



This function returns the closest color from the color palette when RGB is specified. The code below stores the converted RGBs in a dictionary as they're processed. If the RGB to be converted is already registered in the dictionary, the registered RGB is used to speed up the process:

#This code block is for illustration purposes.
if (r, g, b) in self.color_dict:
    return self.color_dict[(r, g, b)]
~~~~~
self.color_dict[(r, g, b)] = color_name



Then this code uses the least-squares approximation to determine the closest color:

#This code block is for illustration purposes.
min_distance = float('inf')
color_name = None
for color in color_pallet:
    distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2
    if distance < min_distance:
        min_distance = distance
        color_name = color



3-2-3. Use the color_change function

Add the following code to the Converter in main.py:

def convert(self, img, option, custom=None):
    w, h = img.shape[:2]
    changed = img.copy()
    # 選択されたcsvファイルを読み込む
    color_pallet = []
    if option != "Custom":
        color_pallet = self.read_csv("./color/"+option+".csv")
    else:
        if custom == [] or custom == None:
            return
        color_pallet = custom

    for height in range(h):
        for width in range(w):
            color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)
            changed[width][height][0] = color[0]  # 赤
            changed[width][height][1] = color[1]  # 緑
            changed[width][height][2] = color[2]  # 青
    return changed



This function uses color_change, which was created earlier to change colors. Specifically, an array of images converted to RGB is passed to color_change to convert the colors. The function returns an array containing the converted colors, and all colors are converted.

3-3. Use a color palette

Add the following code above web.converted.image(img) in if__name__:

img = converter.convert(img,"palette")



Adding this code will add a color palette. The second argument asks for the name of the CSV file, so you can change it by replacing "palette.csv" with the name of your desired file.

Next, let's change the ratios and palettes on the website.

Here is all the code so far:

import streamlit as st
import streamlit.components.v1 as components
import numpy as np
import cv2
from PIL import Image
import csv
import os
import pandas as pd

class Converter():
    def __init__(self) -> None:
        self.color_dict = {}

    def mosaic(self, img, ratio=0.1):
        small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)
        return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)

    def read_csv(self, path):
        with open(path) as f:
            reader = csv.reader(f)
            color = [[int(v) for v in row] for row in reader]
            return color

    def color_change(self, r, g, b, color_pallet):
        if (r, g, b) in self.color_dict:
            return self.color_dict[(r, g, b)]
        # 最も近い色を見つける
        min_distance = float('inf')
        color_name = None
        for color in color_pallet:
            distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2
            if distance < min_distance:
                min_distance = distance
                color_name = color
        self.color_dict[(r, g, b)] = color_name
        return color_name

    def convert(self, img, option, custom=None):
        w, h = img.shape[:2]
        changed = img.copy()
        # 選択されたcsvファイルを読み込む
        color_pallet = []
        if option != "Custom":
            color_pallet = self.read_csv("./color/"+option+".csv")
        else:
            if custom == [] or custom == None:
                return
            color_pallet = custom

        for height in range(h):
            for width in range(w):
                color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)
                changed[width][height][0] = color[0]  # 赤
                changed[width][height][1] = color[1]  # 緑
                changed[width][height][2] = color[2]  # 青
        return changed

class Web():
    def __init__(self) -> None:
        self.draw_text()

    def draw_text(self):
        st.set_page_config(
            page_title="Pixelart-Converter",
            page_icon="🖼️",
            layout="centered",
            initial_sidebar_state="expanded",
        )
        st.title("PixelArt-Converter")
        self.upload = st.file_uploader("Upload Image", type=['jpg', 'jpeg', 'png', 'webp'])
        self.original, self.converted = st.columns(2)
        self.original.title("original img")
        self.converted.title("convert img")

if __name__ == "__main__":
    web = Web()
    converter = Converter()
    if web.upload != None:
        img = Image.open(web.upload)
        img = np.array(img)
        web.original.image(web.upload)
        img = converter.mosaic(img)
        img = converter.convert(img,"pallet")
        web.converted.image(img)



4. Make variables modifiable

Let's make the ratio and the color palette changeable.

4-1. Add a slider for the ratio

Add a slider using st.slider. Add the following code under self.upload in the draw_text function:

self.ratio = st.slider('Select ratio', 0.01, 1.0, 0.3, 0.01)



The arguments of st.slider are as follows:

Arguments	Function
First Argument	Set Label
Second Argument	Minimum Value
Third Argument	Maximum Value
Fourth Argument	Default Value
Fifth Argument	Step

After completing the previous step, the next one is to apply the values obtained from Streamlit. If you set the second argument of img = converter.mosaic(img) to web.ratio, the image will be updated when the slider is changed:

# Changed code
img = converter.mosaic(img, web.ratio)



When done, run the program. If the image changes when you adjust the slider, you've succeeded!

4-2. Add a select box for the color palette

Create a select box using st.selectbox. Add the following code under self.upload in the draw_text function:

self.color = st.selectbox("Select color palette", ("palette",))



The code above describes the title above the selectbox and the content that can be selected in the selectbox. To add a color palette, add a filename to the second argument tuple.

To use the variables obtained in the select box, replace img = converter.convert(img, "palette",) with the following code:

img = converter.convert(img,web.color)



The string currently selected in the selectbox is stored in the web.color variable. Before executing the code, you can create a new CSV file in the color directory and add it to the selectbox. Once complete, run the code. If a box is added that allows you to select a color palette, then you did it right!

5. Use additional filters

PixelArt-Converter provides three options for converting images:

The "no palette" option, which uses a mosaic process to create a pixel art-like image without relying on a color palette or color reduction
An edge filter to further enhance the conversion process
A color reduction process
5-1. Add no palette

5-1-1. Add a function

No palette is implemented using st.checkbox. Add the following functions to the web class:

def more_options(self):
		with st.expander("More Options", True):
        self.no_convert = st.checkbox('no color convert')



This function creates an expander that contains a checkbox. When the checkbox is clicked, the value is entered into self.no_convert.

5-1-2. Call a function

To call the function you just created, add the following code to the bottom of the draw_text function:

self.more_options()



Adding this code will cause more_options to be called.

5-1-3. Make it usable

Add the following condition to img = converter.convert(img, web.color) to prevent color conversion from being performed:

if web.no_convert == False:
    img = converter.convert(img, web.color)


5-2. Add color reduction

If the palette isn't processed, pixel art undergoes a subtractive process. To create a subtractive process, add the following code to the Converter class.

5-2-1. Add a function

def decreaseColor(self, img):
    dst = img.copy()

    idx = np.where((0 <= img) & (64 > img))
    dst[idx] = 32
    idx = np.where((64 <= img) & (128 > img))
    dst[idx] = 96
    idx = np.where((128 <= img) & (192 > img))
    dst[idx] = 160
    idx = np.where((192 <= img) & (256 > img))
    dst[idx] = 224

    return dst



This function performs a color reduction of an image and returns the result.

5-2-2. Add a checkbox

Create a checkbox to add color reduction processing. Add the following code to the expander of more_options:

self.decreaseColor = st.checkbox("decrease color")



The checkbox is created in the same location as the no_convert created earlier.

5-2-3. Make it usable

Add the following code above web.converted.image(img) in "if name == "main":

if web.decrease:
    img = converter.decreaseColor(img)



Adding this code will perform the color reduction process.

5-3. Add an edge filter

5-3-1. Add a function

Add the function anime_filter to the class Converter to implement an edge filter. In the PixelArt-Converter, this filter is referred to as the "animated filter":

def anime_filter(self, img, th1=50, th2=150):
    # アルファチャンネルを分離
    bgr = img[:, :, :3]
    if len(img[0][0]) == 4:
        alpha = img[:, :, 3]

    # グレースケール変換
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)

    # ぼかしでノイズ低減
    edge = cv2.blur(gray, (3, 3))

    # Cannyアルゴリズムで輪郭抽出
    edge = cv2.Canny(edge, th1, th2, apertureSize=3)

    # 輪郭画像をRGB色空間に変換
    edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2BGR)

    # 差分を返す
    result = cv2.subtract(bgr, edge)

    # アルファチャンネルを結合して返す
    if len(img[0][0]) == 4:
        return np.dstack([result, alpha])
    else:
        return result



5-3-2. Add checkboxes

Add the following checkboxes to more_options as before:

self.edge_filter = st.checkbox('anime filter')



5-3-3. Make it usable

Add the following code above web.converted.image(img) if __name__ == "main":

if web.edge_filter:
	img = converter.anime_filter(img)



Here is the full code :

import streamlit as st
import streamlit.components.v1 as components
import numpy as np
import cv2
from PIL import Image
import csv
import os
import pandas as pd

class Converter():
    def __init__(self) -> None:
        self.color_dict = {}

    def mosaic(self, img, ratio=0.1):
        small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)
        return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)

    def read_csv(self, path):
        with open(path) as f:
            reader = csv.reader(f)
            color = [[int(v) for v in row] for row in reader]
            return color

    def color_change(self, r, g, b, color_pallet):
        if (r, g, b) in self.color_dict:
            return self.color_dict[(r, g, b)]
        # 最も近い色を見つける
        min_distance = float('inf')
        color_name = None
        for color in color_pallet:
            distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2
            if distance < min_distance:
                min_distance = distance
                color_name = color
        self.color_dict[(r, g, b)] = color_name
        return color_name

    def convert(self, img, option, custom=None):
        w, h = img.shape[:2]
        changed = img.copy()
        # 選択されたcsvファイルを読み込む
        color_pallet = []
        if option != "Custom":
            color_pallet = self.read_csv("./color/"+option+".csv")
        else:
            if custom == [] or custom == None:
                return
            color_pallet = custom

        for height in range(h):
            for width in range(w):
                color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)
                changed[width][height][0] = color[0]  # 赤
                changed[width][height][1] = color[1]  # 緑
                changed[width][height][2] = color[2]  # 青
        return changed

    def decreaseColor(self, img):
        dst = img.copy()

        idx = np.where((0 <= img) & (64 > img))
        dst[idx] = 32
        idx = np.where((64 <= img) & (128 > img))
        dst[idx] = 96
        idx = np.where((128 <= img) & (192 > img))
        dst[idx] = 160
        idx = np.where((192 <= img) & (256 > img))
        dst[idx] = 224

        return dst

    def anime_filter(self, img, th1=50, th2=150):
        # アルファチャンネルを分離
        bgr = img[:, :, :3]
        if len(img[0][0]) == 4:
            alpha = img[:, :, 3]

        # グレースケール変換
        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)

        # ぼかしでノイズ低減
        edge = cv2.blur(gray, (3, 3))

        # Cannyアルゴリズムで輪郭抽出
        edge = cv2.Canny(edge, th1, th2, apertureSize=3)

        # 輪郭画像をRGB色空間に変換
        edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2BGR)

        # 差分を返す
        result = cv2.subtract(bgr, edge)

        # アルファチャンネルを結合して返す
        if len(img[0][0]) == 4:
            return np.dstack([result, alpha])
        else:
            return result

class Web():
    def __init__(self) -> None:
        self.draw_text()

    def draw_text(self):
        st.set_page_config(
            page_title="Pixelart-Converter",
            page_icon="🖼️",
            layout="centered",
            initial_sidebar_state="expanded",
        )
        st.title("PixelArt-Converter")
        self.upload = st.file_uploader("Upload Image", type=['jpg', 'jpeg', 'png', 'webp'])
        self.color = st.selectbox("Select color palette", ("cold","gold"))
        self.ratio = st.slider('Select ratio', 0.01, 1.0, 0.3, 0.01)
        self.original, self.converted = st.columns(2)
        self.original.title("original img")
        self.converted.title("convert img")
        self.more_options()

    def more_options(self):
        with st.expander("More Options", True):
            self.no_convert = st.checkbox('no color convert')
            self.decrease = st.checkbox('decrease color')
            self.edge_filter = st.checkbox('anime filter')

if __name__ == "__main__":
    web = Web()
    converter = Converter()
    if web.upload != None:
        img = Image.open(web.upload)
        img = np.array(img)
        web.original.image(web.upload)
        img = converter.mosaic(img, web.ratio)
        if web.no_convert == False:
            img = converter.convert(img, web.color)
        if web.decrease:
            img = converter.decreaseColor(img)
        if web.edge_filter:
            img = converter.anime_filter(img)
        web.converted.image(img)



And here is a cool video of the app:

Bonus. What to check if the program fails

A list of things to check in case your program doesn't work:

Changes are not updated. Make sure the program is saved!
Conditions are not applied. Check that the indentation is correct.
File cannot be loaded. Check that the filename is correct.
Function not executed. Check that you are putting the code in the correct place.
Program cannot be executed. Check that the directory you are opening in the console is correct.
Wrapping up

Thank you for reading my post! I hope you found PixelArt-Converter interesting. If you have any questions, please post them in the comments below or contact me on Twitter.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Siavash Yasini - Streamlit
https://blog.streamlit.io/author/siavash/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Siavash Yasini
2 posts
The ultimate Wordle cheat sheet

Learn how to beat Wordle with Streamlit

Advocate Posts
by
Siavash Yasini
,
May 11 2023
Create a color palette from any image

Learn how to come up with the perfect colors for your data visualization

Advocate Posts
by
Siavash Yasini
,
January 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

The ultimate Wordle cheat sheet
https://blog.streamlit.io/the-ultimate-wordle-cheat-sheet/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The ultimate Wordle cheat sheet

Learn how to beat Wordle with Streamlit

By Siavash Yasini
Posted in Advocate Posts, May 11 2023
The backend
The algorithm—behind the scenes
The frontend
1. Create a WORDLE-esque interface
2. Submit and validate guesses and hints
3. Pass the submission to the WORDLE solver
Which word should you start with?
1. Most common letters
2. Levenshtein metric
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Siavash, and I'm a Senior Data Scientist at Zest AI.

I remember when the Wordle craze was sweeping across the internet. All I could see on social media was 🟩 🟨 ⬛️ 🟩 🟨. I tried to avoid it (be too cool for the trend) but eventually succumbed to peer pressure—and I'm glad I did!

WORDLE is a truly brilliant game. I love a good puzzle, so I immediately fell in love with it. The problem is, I'm terrible at guessing, especially when it comes to five-letter words. So I decided to beat the game with…

Statistics!

The app has been working without fail since my first WORDLE win in early 2022, and I've yet to find out what happens if I don't find the word of the day within six guesses.

🤓
If you want to go straight to playing, here is the app! And here is the repo code.
The backend

The algorithm to beat the game is very simple. In fact, it's so simple that I'm not even sure it deserves to be called an algorithm. All you need is a list of all possible 5-letter words and their frequency in English. Then you can follow these steps:

Sort the list of words based on their commonality—the more common the word, the higher its ranking.
At every step of the game, remove any word that doesn’t fit the constraints provided by the hints.
Pick the first word from the remaining list as your next guess. If you feel more adventurous, look at the first few words at the top of the list and follow your gut.

This approach isn't the most efficient way to beat the game and only works for roughly 95% of all possible WORDLE solutions in hard mode—it fails for some odd words with repeated letters. But the algorithm's simplicity allows for a more natural and intuitive approach to playing the game, which is why I’m satisfied with its success rate.

Curious why it works? Check out the next section. Otherwise, jump straight to the frontend section with all the Streamlit fun!

The algorithm—behind the scenes

It's fascinating to note that the origin of Wordle lies in a personal gesture of love. The game's creator crafted it for his wife and asked her to select words that could be potential solutions (read the story here). She sifted through the list of five-letter English words, picking the ones she recognized. Her word familiarity related to its commonness. She was more likely to have encountered commonly used words like "HOUSE" as opposed to rare ones like "FUGLE." This is the underlying principle behind the game's algorithm...

The more frequently a word appears in the English language, the greater its likelihood of being a valid Wordle solution.

We can verify this assumption by looking at all possible WORDLE solutions (available here). KRememberthat this list isn’t used in the final app, and only serves as a tool for confirming our hypothesis.

I have plotted the top 15 most commonly used English words, according to the frequency of their appearance on Wikipedia. The only word that is not a Wordle solution is "YEARS", and that’s because it’s technically a plural 4-letter word, and plurals can’t be Wordle solutions.

Zooming out and looking at the top 150 words, notice how most of the potential Wordle solutions. But as we move towards less common words, it becomes more likely to encounter words that aren’t solutions. So far, so good.

Look at the top 10,000 most frequently used words.  See a similar pattern? To make the distribution easier to view and understand, I have selected a sample of 150 words from the list. As expected, there is a greater number of non-Wordle solutions further down the list:

This confirms that word commonality, or usage frequency, is correlated with being a Wordle solution. Use the hints provided by Wordle to eliminate all the words that don't fit the constraints, then pick the most common word as your next guess for the game.

For example, type in “GUESS” as your guess (brilliant!), and Wordle will give you back the following hints: ⬛️ ⬛️ 🟩 🟩 🟩 . After going through the list of all 5 fiveetter words, only keeping the ones that end with “ESS” and removing the ones that have either a “G” or a “U,” you’ll end up with the following recommendations:

Your best next guess would be “PRESS” because, according to the algorithm, it’s more commonly used than the rest of the list.

🤔
You may argue that at this stage of the game, there is no reason why PRESS would be better than DRESS. DRESS might get you to the final solution faster because “D” is a more common letter than “P.” I agree. I’m just taking a different approach. This algorithm doesn’t give you the most efficient solution. It follows a simple working principle, according to which PRESS is better than DRESS and all the other potential solutions.

Go ahead and type in PRESS. You’ll get ⬛️ 🟩 🟩 🟩 🟩 . Again, using the constraints to eliminate the list you end up with:

At this point, you’d be surprised and annoyed if DRESS wasn’t the correct answer!

The frontend

I’m not here to teach you about my not-so-brilliant algorithm. I’m here to tell you how I used Streamlit 🎈 to turn the algorithm into an app that you can use as a WORDLE cheat sheet:

Create a WORDLE-esque interface
Submit and validate guesses and hints
Pass the submission to the WORDLE solver

Let’s call the app WORDLEr…because why not?!

1. Create a WORDLE-esque interface

The first thing we need for the app is an interface that allows us to input our guesses and the corresponding WORDLE hints, so that they can be passed through the algorithm.

Streamlit’s submit form st.form is the perfect tool for this. As you can see in the GIF, in order to somewhat mimic WORDLE’s interface, I have assigned individual text_input boxes to each letter (max_chars=1), with dropdown boxes st.selectbox underneath each, allowing us to pass the hint returned by WORDLE for that specific letter.

WORDLE doesn’t allow more than 6sixguesses per game, so creating a submit form with a fixed number of rows makes sense. However, with the proposed algorithm, we rarely need to use more than 3 or 4 guesses to find the word of the day, so taking up additional space on the app with rows that will rarely be used doesn't make sense.

It’s easy enough to make the number of rows in the form dynamically, so we can start with three rows and allow the user to change it if needed. The following function creates a form for submitting the guesses, and it takes an input parameter that determines how many rows will appear on the form.

def submit_guesses(n_guesses=6):
    """Create a word submission form with n guesses.
    Return all the guesss and hints submitted."""

    with st.form("form"):
        all_guesses = []
        all_hints = []
        for n in range(1, n_guesses+1):
            guess_letters = []
            guess_hints = []

            letters_cols = st.columns(5)
            for i, col in enumerate(letters_cols):
                with col:
                    letter = st.text_input(" ", max_chars=1, key=f"guess_{n}_letter_{i}")
                    guess_letters.append(letter.upper())

            hints_cols = st.columns(5)
            for i, col in enumerate(hints_cols):
                with col:
                    hint = st.selectbox(" ", colors,  key=f"guess_{n}_hint_{i}")
                    guess_hints.append(hint)
            st.markdown("---")
            all_guesses.append(guess_letters)
            all_hints.append(guess_hints)

        st.form_submit_button("submit")

    return all_guesses, all_hints

2. Submit and validate guesses and hints

The function submit_guesses returns a list of all the guesses and hints from the previous steps. Your Wordle solver will use all of these hints collectively and apply them as constraints to the recommendation list on the backend.

For example, it’ll only keep the words that start with an A, have an S somewhere, and don’t have an R, I, or E in them:

One more thing to do. Make sure that the five letters passed in each stage are valid. For example, mensurethe user hasn't passed a digit or a punctuation mark instead of a letter or there are no missing letters in the guesses.

I have wrapped all of this in a function called keep_valid_guesses (check out the repo for the code behind it):

# let the magic happen...
n_steps = st.sidebar.slider("Number of steps", 1, 10, value=3)
all_guesses, all_hints = submit_guesses(n_steps)

# make sure what is passed is a actually a 5 letter word (and not something like "!NV4L")
valid_guesses, valid_hints = keep_valid_guesses(all_guesses, all_hints) 

3. Pass the submission to the WORDLE solver

Finally, pass the guesses and hints to Wordler(). This class has a simple and intuitive interface. It uses the .update_constraint(guess, hint) method to update constraints based on the input guess and hint and applies it to the recommended list on the backend. Then the .suggest_next_word(head=n_suggestions) function returns the top n_suggestions words on the list:

n_suggestions = 10

wordler = Wordler() # <-- our wordle solver! 
for guess, hint in zip(valid_guesses, valid_hints):
    wordler.update_constraint(guess, hint)

if st.session_state["FormSubmitter:form-submit"]:
    st.header("Next Word Suggestions")
    st.dataframe(wordler.suggest_next_word(head=n_suggestions))

Which word should you start with?

Now that you have a cheat sheet to find the next best guess, the question is: what’s the best word to start with?

There are two ways to approach this.

1. Most common letters

Look at the most common letters in English and start with a five-letter word containing all of them. This way, you’ll maximize the probability of getting a 🟨 or 🟩 hint on the first guess.

According to frequency analysis of English words, the most common letters are A, E, S, R, and I. With these letters, you can build ARISE, RAISE, AESIR, and ARIES. My favorite is ARISE (how fitting), so I always start the game with that!

2. Levenshtein metric

Use the Levenshtein metric to calculate the distance (number of letter changes required to convert one word into another) between all English words. See which word has the minimum square distance from all the other words. Interestingly, this also remakes RISE (and RAISE) as an optimal starting word!

I have simulated mock WORDLE games, using the word list mentioned earlier to see how quickly the app can find the solution, starting with ARISE. The histogram below shows the number of moves it will take to find the answer:

As you can see, it takes an average of four moves to find the answer when starting with ARISE. The algorithm can find 95% of words within six guesses! It only has difficulty with rare words with repeated letters, like GOLLY.

🤓
If you’re not familiar with the word GOLLY, here’s how you would use it in a sentence: ”Oh golly, that was one difficult word! It took Wordler 10 moves to finally find it…”
Wrapping up

I hope you enjoyed learning about how to build a WORDLE-esque interface using st.form , st.columns , st.text_input , and st.select_box. You also learned how to make the interface size flexible using st.slider, how to submit your guess to the Wordle solver using st.submit_form, and how to print out a recommendation list using st.dataframe. There is a lot more code that implements the algorithm through the Wordler() class, so feel free to check it out.

I'm sorry if the app makes the guessing too easy, but now you can impress your friends with unbeatable Wordle scores! 🤩

I'd love to hear your thoughts, questions, comments, and feedback. Get in touch with me on LinkedIn or through my website.

Happy Wordle-ing and Happy Streamlit-ing! 🟩🎈🟩 🟨 🟩

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Alice Heiman - Streamlit
https://blog.streamlit.io/author/alice/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Alice Heiman
1 post
Learn Morse code with a Streamlit app

5 steps to build your own Morse code tutor!

Advocate Posts
by
Alice Heiman
,
May 12 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Learn Morse code with a Streamlit app
https://blog.streamlit.io/learn-morse-code-with-a-streamlit-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Learn Morse code with a Streamlit app

5 steps to build your own Morse code tutor!

By Alice Heiman
Posted in Advocate Posts, May 12 2023
Step 1. What is Morse code anyway?
Step 2. Sound module
Step 3. Game module
Step 4. Levels
Step 5. Checkpoints and Playground
Wrapping up
Appendix: Complete Morse code
Contents
Share this post
← All posts

Hey, community! 👋

I'm Alice, a student from Sweden passionate about using technology. AI excites me because it can enhance human creativity, find otherwise undetected patterns, and create personalized learning tools.

As a ham radio operator, I wanted to learn Morse code to communicate with other operators worldwide. Because in times of crisis, traditional radio can be an alternative channel for critical information. But I had only old audio Morse recordings with exercises that took forever to check manually, and I wanted a more fun and engaging way to learn it.

So…I built a Streamlit app! It's an interactive Morse tutor with 15 levels, five checkpoints, and memorable mnemonics that helped me transmit words and phrases from the first week.

Morse code was developed in the 1800s, but it's still relevant for signaling and identification and is often used in capture-the-flag events (CTFs), escape rooms, and puzzles!

In this post, I'll show you how to create it in five steps:

Step 1. What is Morse code anyway?
Step 2. Sound module
Step 3. Game module
Step 4. Levels
Step 5. Checkpoints and Playground
💡
To try it out, visit the demo app. The source code can be found here.
Step 1. What is Morse code anyway?

The Morse alphabet associates each letter with a sequence of short and long pulses called "dots" and "dashes." But how do you choose which letter gets which sequence? It's far from random. If you think about it, you'd probably want to reserve the shorter sequences for the most common letters.

The International Morse code table looks like this:

Morse code. (2023, April 12). In Wikipedia. https://en.wikipedia.org/wiki/Morse_code

If we analyze the frequency of the English letters, that is how many times a letter is used on average, we get the following graph:

Frequencies of letters in the English language generated from the NLTK corpus

Notice that the most common letters, "e t a o n," have the shortest sequences.

Armed with these insights, we can tailor our training to focus on the most commonly used letters and words to make the most of limited time.

Let's get coding!

Step 2. Sound module

The most important module is the creation of Morse audio sequences. To do this, we need to understand pulse timing. In Morse, we make the shortest pulse one unit and define all other lengths relative to it:

Short pulse: 1 unit
Long pulse: 3 units
Intraletter spacing: 1 unit
Interletter spacing: 3 units
Interword spacing: 7 units

For example, the word “PARIS” becomes “• - - •   • -   • - •   • •   • • •”, with a total of 50 units ($10\text{ short} \cdot 1\text{ unit} + 4\text{ long} \cdot 3\text{ units} + 9\text{ intraletter} \cdot 1\text{ unit} + 4\text{ interletter} \cdot 3\text{ units} + 1\text{ interword} \cdot 7\text{ units} = 50\text{ units}$):

When practicing Morse code, the goal is not to memorize the sequences of letters but to imprint their auditory patterns.

So we want to keep the speed of the letter sequences high, but we can increase the spacing between letters and words to create a slower beginner pace. To do this, we use something called Farnsworth timing. It allows us to "warp" a higher word-per-minute speed to a lower one just by lengthening the pauses. This article explains the mathematics behind the formulas used to make this transformation.

Here is the code:

class SoundCreator:
    def __init__(self, character_speed=22, farnsworth_speed=8, sample_rate=44100.0):
      self.sample_rate = sample_rate
      self.character_speed = character_speed
      self.farnsworth_speed = farnsworth_speed

      # Compute timings
      u = 1.2 / character_speed
      ta = (60 * character_speed - 37.2 * farnsworth_speed) / (farnsworth_speed * character_speed)
      tc = (3 * ta) / 19
      tw = (7 * ta) / 19

      # Convert to milliseconds
      self.dot_length = round(u * 1000)
      self.dash_length = round(u * 3 * 1000)
      self.intra_character_space = round(u * 1000)
      self.inter_character_space = round(tc * 1000)
      self.inter_word_space = round(tw * 1000)

			# Initialize audio array
      self.audio = []
      self.morse_dict = {k: l[1].replace("▄▄", "-").replace("▄", "*").replace(" ", "") for k, l in mnemonics.items()}


We generate an audio sample by constructing an array of tone data. Zeros represent silence, while our tone is calculated by a formula based on our desired duration, volume, sample rate, and frequency:

def _append_silence(self, duration_ms=500):
    """Adding silence by appending zeros."""
    num_samples = duration_ms * (self.sample_rate / 1000.0)

    for _ in range(int(num_samples)):
        self.audio.append(0.0)

def _append_sinewave(self, freq=550.0, duration_ms=500, volume=0.2):
    """Appends a beep of length duration_ms"""
    num_samples = duration_ms * (self.sample_rate / 1000.0)

    for x in range(int(num_samples)):
        self.audio.append(volume * math.sin(2 * math.pi * freq * (x / self.sample_rate)))


With the general audio functions complete, we can construct a function that turns any character string into a playable Morse code audio snippet.

Here is the code that does just that:

def create_audio_from(self, sequence: str, start_delay_ms=None):
        """Takes a strings sequence and transforms it into a playable Morse audio clip.

        Args:
            sequence (str): Message to be enconded into Morse Code.

        Returns:
            np_array: audio data for an audio player.
        """

        # reset audio array
        self.audio = []

        # Add silence at beginning
        if start_delay_ms:
            self._append_silence(duration_ms=start_delay_ms)

        for character in sequence:
            character = character.upper()
          
            if character in self.morse_dict:
								morse_encoding = self.morse_dict[character]

                for i, symbol in enumerate(morse_encoding):
                    if symbol == "*":
                        # Short sound
                        self._append_sinewave(duration_ms=self.dot_length)
                    elif symbol == "-":
                        # Long sound:
                        self._append_sinewave(duration_ms=self.dash_length)

                    if i + 1 < len(morse_encoding):
                        self._append_silence(duration_ms=self.intra_character_space)

                # Add inter-character spacing
                self._append_silence(duration_ms=self.inter_character_space)

            if character == " ":
                # Add inter-word spacing
                self._append_silence(duration_ms=self.inter_word_space)

        return np.array(self.audio)


The functions take the following inputs:

sequence is a string message, such as "Hey," encoded in Morse code.
start_delay adds silence to the audio clip's beginning to allow the user to prepare after pressing play.

The function returns an audio array containing audio data that can be played by Streamlit's audio player.

💡
You can find the complete Morse code symbol table in the Appendix below.
Step 3. Game module

The Game Module handles the component that implements the interactive quizzing. Here is the class declaration:

class GameCreator:
    def __init__(self, label, symbols):
        self.label = label
        self.symbols = symbols
        self.anagrams = None
        self.quotes = None


The Game Modules generate character sequences, and 2) accept and correct user input. I created two main functions to generate these sequences. One generates random groups in the defined character set. The other finds possible anagrams of a set of symbols and assembles them into word sequences.

Here is the code for it:

def generate_sequence(self, length_unit: int, num_units: int):
    """Creates a random letter sequence of *length_unit* chunks, *num_unit* times."""
    seq = []
    for _ in range(num_units):
        seq.append("".join(random.choices(self.symbols, k=length_unit)))

    return " ".join(seq)

def generate_anagrams(self, filename):
		"""Find all possible words with the current symbol set. (self.symbols)"""
    # Step 1: Read words from the file line by line
        with open(filename, "r") as f:
            words = f.read().split("\
")

    # Step 2: Get all anagrams
    anagrams = []
    symbol_set = set(self.symbols.lower())
    for word in words:
        word_set = set(word.lower())

        if word_set.issubset(symbol_set):
            if word.upper() not in anagrams:
                anagrams.append(word.upper())

    # Step 3: Save and return the list of anagrams
    self.anagrams = anagrams
    return anagrams

def generate_word_sequence(self, num_words: int):
		"""Assemble possible words into a word sequence."""
    words = random.choices(self.anagrams, k=num_words)
    return " ".join(words)


In the final version, I extended this to include quotes and news summaries. You can create functions to generate any kind of practice text!

The "Typer" function acts as a reusable component with an audio player, instruction text, input field, and interactive feedback:

def Typer(self):
      """Component with instructions, audio player, user input, and correction."""
      message = self.get_message()

      formatted_symbols = "".join(list(self.symbols)).strip()
      st.markdown(f"*Available symbols:* **{formatted_symbols}**")

      with st.form(key=self.label, clear_on_submit=True):
          user_input = st.text_input("**:blue[Type what you hear] 👇**")

          if st.form_submit_button("Submit"):
              user_input = user_input.upper()
              answer = message.upper()
              output = ""

              for i in range(len(user_input)):
                  if i >= len(answer):
                      output += f":red[{user_input[i:]}]"
                      break

                  if user_input[i] == answer[i]:
                      output += f":green[{user_input[i]}]" if user_input[i] != " " else " "
                  else:
                      output += f":red[{user_input[i]}]" if user_input[i] != " " else " "

              st.markdown(f"***Your Answer:*** {output}")
              st.markdown(f"*Comparison:*  {answer}")
              self.reset_message()

      reset = st.button(f"Reset {self.label}")
      if reset:
          self.reset_message()
          st.experimental_rerun()


My three biggest takeaways from this application are:

Functions can act like components. Call the function anywhere you want a copy of it. It's a cheap way to bundle Streamlit components into a package.
Use :color[your text here] and replace color to create colored text.
If you want to keep information between reloads, you must cache it.

The trickiest part was establishing persistence because Streamlit reloads the page when input is passed. To keep a piece of data and not generate a new one every time, we need to store it in the st.session_state. For this project, it's necessary to compare what the user enters and the correct sequence after the input is submitted.

I solved this problem by adding the following functions:

def initalize_message(self, sequence):
    if "sequence" not in st.session_state:
        st.session_state.sequence = ""

    if st.session_state.sequence == "":
        st.session_state.sequence = sequence

def get_message(self):
    return st.session_state.sequence

def reset_message(self):
    st.session_state.sequence = ""

initalize_message creates a new key-value pair to store the generated sequence as st.session_state.sequence.
get_message gets the current sequence stored in the session state.
reset_message clears the session state.
Step 4. Levels

I quickly realized that I was structuring all the levels in a similar way:

Introduction to two new symbols: their Morse code and mnemonics.
Audio example of each symbol separately.
An audio sample of the symbols used together showing the plaintext solution.
A practice sequence using only the new symbols.
A practice sequence of all the symbols learned.

Instead of copying and pasting, I created a template file with a configuration dictionary as input. Then I could import this file, specify the parameters in each level file, and call the template function.

Here is an example from level five:

from template import *

# Configuration
level = {
    "level": "Level 5",
    "new_symbols": "UD",
    "new_label": "UD",
    "all_label": "ETASILONUD",
    "length_unit": 5,
    "num_units_tutorial": 5,
    "num_units_all": 7,
}

generate_level(level)


So if I want to change something on all levels, I only have to change it in one place!

💡
You can find the complete template code in the source repo.
Step 5. Checkpoints and Playground

The purpose of the checkpoints is to make practicing more fun and to get a sense of progress.

I downloaded a list of English words and inspirational quotes. Then I created an algorithm that calculated the order in which to learn all the characters to form the maximum number of sentences from the start.

In this version, a checkpoint has two variations:

WORDS, where you practice random word sequences without grammatical structure. These are generated from the generate_anagrams and generate_word_sequence functions of the game module explained earlier.
QUOTES, where a random quote is chosen from a list based on the current known symbol set.

Each challenge ends with a reference to the characters of the levels leading up to the current checkpoint.

I created the Playground to let you freely enter your messages, experiment with speeds, and even get daily news snippets to practice on! I chose "Nature Daily Briefing," but you can customize it to whatever text sources you like best!

Wrapping up

Thanks for reading! The great thing about Morse is its simplicity. Any two things that look, feel, or sound different can carry information. In this post, I showed you how to create playable audio samples with arrays, wrap Streamlit components into functions, cache data between reloads, and create template files to generate pages with similar functionality.

If you have any questions, please leave them in the comments below, contact me on Twitter or email me.

Happy Streamlit-ing! 🎈

Appendix: Complete Morse code

Here is the complete Morse code symbol table used for this project (it gives the character, its Morse equivalent, and a mnemonic to remember its sequence):

mnemonics = {
    "A": ("A", "▄ ▄▄", "a-PART"),
    "B": ("B", "▄▄ ▄ ▄ ▄", "BOB is the man"),
    "C": ("C", "▄▄ ▄ ▄▄ ▄", "CO-ca CO-la"),
    "D": ("D", "▄▄ ▄ ▄", "DRAC-u-la"),
    "E": ("E", "▄", "Eh?!"),
    "F": ("F", "▄ ▄ ▄▄ ▄", "Fi-tti-PAL-di"),
    "G": ("G", "▄▄ ▄▄ ▄", "GOOD GRAV-y"),
    "H": ("H", "▄ ▄ ▄ ▄", "hip-pi-ty hop"),
    "I": ("I", "▄ ▄", "did it"),
    "J": ("J", "▄ ▄▄ ▄▄ ▄▄", "in JAWS JAWS JAW"),
    "K": ("K", "▄▄ ▄ ▄▄", "KAN-dy KID"),
    "L": ("L", "▄ ▄▄ ▄ ▄", "los AN-ge-les"),
    "M": ("M", "▄▄ ▄▄", "MA-MA"),
    "N": ("N", "▄▄ ▄", "NAV-y"),
    "O": ("O", "▄▄ ▄▄ ▄▄", "HO HO HO"),
    "P": ("P", "▄ ▄▄ ▄▄ ▄", "a PIZ-ZA pie"),
    "Q": ("Q", "▄▄ ▄▄ ▄ ▄▄", "GOD SAVE the QUEEN"),
    "R": ("R", "▄ ▄▄ ▄", "a RABB-it"),
    "S": ("S", "▄ ▄ ▄", "sí-sí-sí"),
    "T": ("T", "▄▄", "TALL"),
    "U": ("U", "▄ ▄ ▄▄", "un-der WHERE?!"),
    "V": ("V", "▄ ▄ ▄ ▄▄", "vic-tor-y VEE"),
    "W": ("W", "▄ ▄▄ ▄▄", "the WORLD WAR"),
    "X": ("X", "▄▄ ▄ ▄ ▄▄", "CROSS at the DOOR"),
    "Y": ("Y", "▄▄ ▄ ▄▄ ▄▄", "YELL-ow YO-YO"),
    "Z": ("Z", "▄▄ ▄▄ ▄ ▄", "ZU-ZU pe-tal"),
    "0": ("0", "▄▄ ▄▄ ▄▄ ▄▄ ▄▄", ""),
    "1": ("1", "▄ ▄▄ ▄▄ ▄▄ ▄▄", ""),
    "2": ("2", "▄ ▄ ▄▄ ▄▄ ▄▄", ""),
    "3": ("3", "▄ ▄ ▄ ▄▄ ▄▄", ""),
    "4": ("4", "▄ ▄ ▄ ▄ ▄▄", ""),
    "5": ("5", "▄ ▄ ▄ ▄ ▄", ""),
    "6": ("6", "▄▄ ▄ ▄ ▄ ▄", ""),
    "7": ("7", "▄▄ ▄▄ ▄ ▄ ▄", ""),
    "8": ("8", "▄▄ ▄▄ ▄▄ ▄ ▄", ""),
    "9": ("9", "▄▄ ▄▄ ▄▄ ▄▄ ▄", ""),
    ".": (".", "▄ ▄▄ ▄ ▄▄ ▄ ▄▄", "a STOP a STOP a STOP"),
    ",": (",", "▄▄ ▄▄ ▄ ▄ ▄▄ ▄▄", "COM-MA, it's a COM-MA"),
    "?": ("?", "▄ ▄ ▄▄ ▄▄ ▄ ▄", "it's a QUES-TION, is it?"),
    ":": (":", "▄▄ ▄▄ ▄▄ ▄ ▄ ▄", "HA-WA-II stan-dard time"),
    "/": ("/", "▄▄ ▄ ▄ ▄▄ ▄", "SHAVE and a HAIR-cut"),
    '"': ('"', "▄ ▄▄ ▄ ▄ ▄▄ ▄", "six-TY-six nine-TY-nine"),
    "'": ("'", "▄ ▄▄ ▄▄ ▄▄ ▄▄ ▄", "and THIS STUFF GOES TO me"),
    ";": (";", "▄▄ ▄ ▄▄ ▄ ▄▄ ▄", ""),
    "=": ("=", "▄▄ ▄ ▄ ▄ ▄▄", ""),
    "+": ("+", "▄ ▄▄ ▄ ▄▄ ▄", ""),
    "-": ("-", "▄▄ ▄ ▄ ▄ ▄ ▄▄", ""),
}

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Vinícius Oviedo - Streamlit
https://blog.streamlit.io/author/vinicius-oviedo/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Vinícius Oviedo
Data Analyst & LaTeX editor
2 posts
Website
Analyzing real estate properties with Streamlit

A 7-step tutorial on how to make your own real estate app

Advocate Posts
by
Vinícius Oviedo
,
May 16 2023
Creating a Time Zone Converter with Streamlit

6 steps on how to build your own converter

Advocate Posts
by
Vinícius Oviedo
,
April 25 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Analyzing real estate properties with Streamlit
https://blog.streamlit.io/analyzing-real-estate-properties-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Analyzing real estate properties with Streamlit

A 7-step tutorial on how to make your own real estate app

By Vinícius Oviedo
Posted in Advocate Posts, May 16 2023
The app's working principle
1. Import the required Python modules
2. Set up the Matplotlib layout for storytelling
3. Collect the data
4. Set up the Streamlit app's page, textual elements, widgets, and sidebar
5. Create chart 1 (appreciation)
6. Create chart 2 (price by squared meter)
7. Finalize the app
Wrapping up
Contents
Share this post
← All posts

Analyzing residential properties for sale in Brazil can be time-consuming. There are public reports that track real estate prices but no data visualizations to compare assets across cities. So I build the Appreciation of residential properties in Brazil app with Streamlit! It combines Pandas, NumPy, Matplotlib, and Seaborn libraries and a few storytelling techniques to make data analysis more accessible.

In this post, you'll learn how to build this app in seven steps:

Import the required Python modules
Set up the Matplotlib layout for storytelling
Gather the data
Set up the Streamlit app's page, textual elements, widgets, and sidebar
Create chart 1 (appreciation)
Create chart 2 (price by squared meter)
Finalize the app

But first, a bit about the app itself.

🟢
Check out the app here and the repo code here.
The app's working principle

The app uses a single visual type—the stripplot:

Each dot ⚪ represents a city. The lowest values are at the bottom, and the highest at the top. The user can select a city, highlight it, and compare it with other cities. You can also provide context by using statistical measures such as:

The first quartile (Q1): represents 25% of the data.
Median: the middle value that splits the data in half (can also provide an average).
Third quartile (Q3): represents 75% of the data.

Here is how it works:

The user selects a Brazilian city marked with a green dot 🟢 on the map.
Other cities are represented by white dots ⚪, making it easy to compare the selected city with others.
Statistical measures such as the first quartile, median, and third quartile are displayed, allowing the user to compare the situation of the chosen city against the national average and data distribution.
The user can extract insights from the data, such as identifying opportunities when an appreciation rate is above the national average, and the price per square meter is below average.

🟢
The data for the app is based on real estate property prices during the first quarter of 2023. It includes assets from fifty Brazilian cities and provides insights into the appreciation of residential properties and prices per square meter (in BRL).

Now, let's get to coding!

1. Import the required Python modules

Import Streamlit, Numpy, and Pandas (for arrays and data manipulation), and Matplotlib and Seaborn (for data visualization).

# Modules:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st

2. Set up the Matplotlib layout for storytelling

This step will let you create a unique design for your Matplotlib figures and define the color palette.

# Setup for Storytelling (matplotlib):
plt.rcParams['font.family'] = 'monospace'
plt.rcParams['font.size'] = 8
plt.rcParams['font.weight'] = 'bold'
plt.rcParams['figure.facecolor'] = '#464545' 
plt.rcParams['axes.facecolor'] = '#464545' 
plt.rcParams['axes.titleweight'] = 'bold'
plt.rcParams['axes.titlecolor'] = 'black'
plt.rcParams['axes.titlesize'] = 9
plt.rcParams['axes.labelcolor'] = 'darkgray'
plt.rcParams['axes.labelweight'] = 'bold'
plt.rcParams['axes.edgecolor'] = 'darkgray'
plt.rcParams['axes.linewidth'] = 0.2
plt.rcParams['ytick.color'] = 'darkgray'
plt.rcParams['xtick.color'] = 'darkgray'
plt.rcParams['axes.titlecolor'] = '#FFFFFF'
plt.rcParams['axes.titlecolor'] = 'white'
plt.rcParams['axes.edgecolor'] = 'darkgray'
plt.rcParams['axes.linewidth'] = 0.85
plt.rcParams['ytick.major.size'] = 0

3. Collect the data
# --- App (begin):
BR_real_estate_appreciation = pd.read_csv('data/BR_real_estate_appreciation_Q1_2023.csv')
BR_real_estate_appreciation['Annual_appreciation'] = round(BR_real_estate_appreciation['Annual_appreciation'], 2)*100

4. Set up the Streamlit app's page, textual elements, widgets, and sidebar

To configure the style, set up a header and add an app usage tutorial in the sidebar. You can also define widgets for selecting your city of interest.

# Page setup:
st.set_page_config(
    page_title="Residential properties (Brazil)",
    page_icon="🏢",
    layout="centered",
    initial_sidebar_state="collapsed",
)

# Header:
st.header('Appreciation of residential properties in Brazil')

st.sidebar.markdown(''' > **How to use this app**

1. To Select a city (**green dot**).
2. To compare for the selected city against other 50 cities (**white dots**).
3. To compare the chosen city against **national average** and the data distribution.
4. To extract insights as "An appreciation above national average + price by square meter below average = possible *opportunity*".
''')

# Widgets:
cities = sorted(list(BR_real_estate_appreciation['Location'].unique()))
your_city = st.selectbox(
    '🌎 Select a city',
    cities
)

selected_city = BR_real_estate_appreciation.query('Location == @your_city')
other_cities = BR_real_estate_appreciation.query('Location != @your_city')

5. Create chart 1 (appreciation)

This step refers to the first stripplot, which compares the selected city's annual appreciation to other cities. You can highlight the chosen city in the chart and add reference lines, such as the first quartile, median, and third quartile, to see how it performs.

# CHART 1: Annual appreciation (12 months):
chart_1, ax = plt.subplots(figsize=(3, 4.125))
# Background:
sns.stripplot(
    data= other_cities,
    y = 'Annual_appreciation',
    color = 'white',
    jitter=0.85,
    size=8,
    linewidth=1,
    edgecolor='gainsboro',
    alpha=0.7
)
# Highlight:
sns.stripplot(
    data= selected_city,
    y = 'Annual_appreciation',
    color = '#00FF7F',
    jitter=0.15,
    size=12,
    linewidth=1,
    edgecolor='k',
    label=f'{your_city}'
)

# Showing up position measures:
avg_annual_val = BR_real_estate_appreciation['Annual_appreciation'].median()
q1_annual_val = np.percentile(BR_real_estate_appreciation['Annual_appreciation'], 25)
q3_annual_val = np.percentile(BR_real_estate_appreciation['Annual_appreciation'], 75)

# Plotting lines (reference):
ax.axhline(y=avg_annual_val, color='#DA70D6', linestyle='--', lw=0.75)
ax.axhline(y=q1_annual_val, color='white', linestyle='--', lw=0.75)
ax.axhline(y=q3_annual_val, color='white', linestyle='--', lw=0.75)

# Adding the labels for position measures:
ax.text(1.15, q1_annual_val, 'Q1', ha='center', va='center', color='white', fontsize=8, fontweight='bold')
ax.text(1.3, avg_annual_val, 'Median', ha='center', va='center', color='#DA70D6', fontsize=8, fontweight='bold')
ax.text(1.15, q3_annual_val, 'Q3', ha='center', va='center', color='white', fontsize=8, fontweight='bold')

# to fill the area between the lines:
ax.fill_betweenx([q1_annual_val, q3_annual_val], -2, 1, alpha=0.2, color='gray')
# to set the x-axis limits to show the full range of the data:
ax.set_xlim(-1, 1)

# Axes and titles:
plt.xticks([])
plt.ylabel('Average appreciation (%)')
plt.title('Appreciation (%) in the past 12 months', weight='bold', loc='center', pad=15, color='gainsboro')
plt.legend(loc='center', bbox_to_anchor=(0.5, -0.1), ncol=2, framealpha=0, labelcolor='#00FF7F')
plt.tight_layout()

6. Create chart 2 (price by squared meter)

Here I refer to the second stripplot, which shows the relationship between the city and price per square meter.

# CHART 2: Price (R$) by m²:
chart_2, ax = plt.subplots(figsize=(3, 3.95))
# Background:
sns.stripplot(
    data= other_cities,
    y = 'BRL_per_squared_meter',
    color = 'white',
    jitter=0.95,
    size=8,
    linewidth=1,
    edgecolor='gainsboro',
    alpha=0.7
)
# Highlight:
sns.stripplot(
    data= selected_city,
    y = 'BRL_per_squared_meter',
    color = '#00FF7F',
    jitter=0.15,
    size=12,
    linewidth=1,
    edgecolor='k',
    label=f'{your_city}'
)

# Showing up position measures:
avg_price_m2 = BR_real_estate_appreciation['BRL_per_squared_meter'].median()
q1_price_m2 = np.percentile(BR_real_estate_appreciation['BRL_per_squared_meter'], 25)
q3_price_m2 = np.percentile(BR_real_estate_appreciation['BRL_per_squared_meter'], 75)

# Plotting lines (reference):
ax.axhline(y=avg_price_m2, color='#DA70D6', linestyle='--', lw=0.75)
ax.axhline(y=q1_price_m2, color='white', linestyle='--', lw=0.75)
ax.axhline(y=q3_price_m2, color='white', linestyle='--', lw=0.75)

# Adding the labels for position measures:
ax.text(1.15, q1_price_m2, 'Q1', ha='center', va='center', color='white', fontsize=8, fontweight='bold')
ax.text(1.35, avg_price_m2, 'Median', ha='center', va='center', color='#DA70D6', fontsize=8, fontweight='bold')
ax.text(1.15, q3_price_m2, 'Q3', ha='center', va='center', color='white', fontsize=8, fontweight='bold')

# to fill the area between the lines:
ax.fill_betweenx([q1_price_m2, q3_price_m2], -2, 1, alpha=0.2, color='gray')
# to set the x-axis limits to show the full range of the data:
ax.set_xlim(-1, 1)

# Axes and titles:
plt.xticks([])
plt.ylabel('Price (R\\$)')
plt.legend(loc='center', bbox_to_anchor=(0.5, -0.1), ncol=2, framealpha=0, labelcolor='#00FF7F')
plt.title('Average price (R\\$) by $m^2$', weight='bold', loc='center', pad=15, color='gainsboro')
plt.tight_layout()

7. Finalize the app

Here, you can split the charts into two columns and add a legend. To make it more accessible:

Display tabular data for the chosen city in addition to the chart
Provide information about reference indexes (such as inflation) and authorship
# Splitting the charts into two columns:
left, right = st.columns(2)

# Columns (content):
with left:
    st.pyplot(chart_1)
with right:
    st.pyplot(chart_2)

# Informational text:
st.markdown('''
<span style="color:white;font-size:10pt"> ⚪ Each point represents a city </span>
<span style="color:#DA70D6;font-size:10pt"> ▫ <b> Average value </b></span>
<span style="color:white;font-size:10pt"> ◽ Lowest values (<b> bottom </b>)
◽ Highest values (<b> top </b>) <br>
◽ **Q1** (first quartile): where 25% of data falls under
◽ **Q3** (third quartile): where 75% of data falls under
</span>

''',unsafe_allow_html=True)

# Showing up the numerical data (as a dataframe):
st.dataframe(
    BR_real_estate_appreciation.query('Location == @your_city')[[
      'Location', 'Annual_appreciation', 
      'BRL_per_squared_meter']]
)

# Adding some reference indexes:
st.markdown(''' > **Reference indexes (inflation):**

* IPCA: **6%** (National Broad Consumer Price Index)
* IGP-M: **4%** (General Market Price Index)

> Data based on public informs that accounts residential properties for 50 Brazilian cities (first quarter of 2023).
''')

# Authorship:
st.markdown('---')
# here you can add the authorship and useful links (e.g., Linkedin, GitHub, and so forth)
st.markdown('---')
# --- (End of the App)


Finally, let's incorporate a dark theme. Note that the layout customizations in Matplotlib will be consistent with the theme's color palette.

[theme]
primaryColor="#00FF7F"
backgroundColor="#464545"
secondaryBackgroundColor="#2b2b29"
textColor="#fbfbfb"
font="serif"

Wrapping up

Now you can use Streamlit to analyze the appreciation of residential properties by employing statistics, data visualization, and storytelling. Although this app is for Brazilian real estate properties, you can apply the same methodology to any country.

If you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, or Medium.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Semantic search, Part 1: Implementing cosine similarity
https://blog.streamlit.io/semantic-search-part-1-implementing-cosine-similarity/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Semantic search, Part 1: Implementing cosine similarity

Wrangling Foursquare data and implementing semantic search in Snowflake

By Dave Lin
Posted in Snowflake powered ❄️, May 17 2023
Why did I make this app?
Data wrangling
Step 1: Install the Foursquare NYC dataset from the Snowflake Marketplace
Step 2: Set up a new database and schema
Step 3: Create borough and (borough, neighborhood) relationships
Step 4. Extract categories
Step 5: Embed Foursquare categories
Step 6: Create a cache version of Foursquare data
Implementations
Implementation 1: Python UDF using an existing function
Implementation 2: Python UDF with custom implementation
Implementation 3: JavaScript UDF with custom implementation
Implementation 4: Native SQL
Performance evaluation
Scalability evaluation
Wrapping up
Contents
Share this post
← All posts

It’s so annoying trying to think of things to do. Sometimes you just want to type “epic night out” into Google Search and get what you’re looking for, right? I struggled with the same. So I built a semantic search app. It finds Foursquare venues in NYC leveraging Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data on the Snowflake Marketplace.

The semantic search lets users search for venues based on their intent (and not translating their intent to keywords). For example, users can search for venues for an "epic night out" or "lunch date spots" and find venues in their specified neighborhoods with Foursquare venue categories that are semantically closest to what they’re looking for.

In this first part of a two-part blog series, I’ll walk you through how I wrangled the data and implemented semantic search in Snowflake.

🔴
Want to dive right in? Check out the code and the app. The Streamlit application will be removed after the Snowflake Summit in June due to cost considerations.
Why did I make this app?

I stumbled upon semantic search as I was exploring generative AI use cases. At its core, most semantic search apps use cosine similarity as a metric to determine which documents in a corpus are most similar to a user’s query. As I learned more about it, my inner Snowflake fanboy thought Snowflake’s impressive computational power and near-infinite scalability would be ideal for such a task! I wanted to power a semantic search app using Snowflake as an alternative to a vector database. Streamlit's Snowflake Summit Hackathon offered a perfect opportunity to do that.

Data wrangling
Step 1: Install the Foursquare NYC dataset from the Snowflake Marketplace

Before we get started, install the free Foursquare Places - New York City Sample from the Snowflake Marketplace (if you don’t have access to Snowflake, you can sign up for a 30-day free trial here). I shortened the default database to foursquare_nyc during installation.

After you install it, the data set will appear in the Snowflake UI:

Foursquare provides a single view containing basic information about venues in NYC. We aim to leverage Snowflake to perform a semantic search of Foursquare venue categories. To achieve this, the columns we’re particularly interested in are fsq_category_labels and fsq_category_ids. fsq_category_labels contains an array of arrays. The outer array represents the list of categories. The inner array describes the hierarchy of the category, where the first element represents the root category and the last element represents the leaf category. fsq_category_ids contains an array of IDs for the leaf categories in the fsq_category_labels column.

Step 2: Set up a new database and schema

We’ll create a new database and schema to house our wrangled data:

-- Set up a new database and schema where we are going to house auxiliary data 
CREATE DATABASE foursquare;
-- Create a new schema
CREATE SCHEMA main;

Step 3: Create borough and (borough, neighborhood) relationships

From a user experience perspective, it’d be inefficient to comb through all NYC neighborhoods in each of the five boroughs. Also, querying a list of venues in a list of neighborhoods from the Foursquare dataset would be computationally expensive, given that the neighborhoods are stored as a string array in the neighborhood column. So we’ll create normalized tables to store information about NYC boroughs, neighborhoods, and which neighborhoods are in which boroughs.

First, we’ll create and populate a borough_lookup table:

CREATE TRANSIENT TABLE borough_lookup (
id number autoincrement,
name varchar
);

INSERT INTO borough_lookup(name) values
('Brooklyn'),
('Bronx'),
('Manhattan'),
('Queens'),
('Staten Island');


Next, we’ll create and populate a neighborhood_lookup table:

CREATE TRANSIENT TABLE neighborhood_lookup (
id number autoincrement,
name varchar
);

INSERT INTO neighborhood_lookup(name)
SELECT DISTINCT n.value::string
FROM foursquare_nyc.standard.places_us_nyc_standard_schema s,
table(flatten(s.neighborhood)) n
ORDER BY 1;


Next, we’ll create a borough_neighborhood table to store our (borough, neighborhood) mapping by:

Creating a temporary table to store the manually curated (borough, neighborhood) mapping (find the exact insert statement here):
CREATE OR REPLACE TRANSIENT TABLE z_borough_neighborhood(
borough_name varchar,
neighborhood_name varchar
);

INSERT INTO z_borough_neighborhood(borough_name, neighborhood_name) values
('Bronx','Allerton'),
('Bronx','Bathgate'),
('Bronx','Baychester'),
('Bronx','Bedford Park'),
('Bronx','Belmont'),
...

Creating the final mapping table by joining the temporary mapping table with the lookup tables:
CREATE OR REPLACE TRANSIENT TABLE borough_neighborhood AS
SELECT
   b.id borough_id
   , n.id neighborhood_id
FROM z_borough_neighborhood bp
INNER JOIN borough_lookup b ON bp.borough_name = b.name
INNER JOIN neighborhood_lookup n ON bp.neighborhood_name = n.name
ORDER BY b.id, n.id;


Finally, we’ll create a place_neighborhood table:

CREATE OR REPLACE TRANSIENT TABLE place_neighborhood AS
WITH place_neighborhood AS (
SELECT DISTINCT
   s.fsq_id
   , n.value::string str
FROM foursquare_nyc.standard.places_us_nyc_standard_schema s,
table(flatten(s.neighborhood)) n
)
SELECT pn.fsq_id, n.id neighborhood_id
FROM place_neighborhood pn
INNER JOIN neighborhood_lookup n ON pn.str = n.name
ORDER BY id, pn.fsq_id;

Step 4. Extract categories

Next, we’ll extract the categories from fsq_category_labels and fsq_category_ids columns:

-- Extract Foursquare category IDs 
CREATE OR REPLACE TRANSIENT TABLE z_category_id AS
WITH data AS (
SELECT
   DISTINCT
   s.fsq_category_labels
   , n.seq
   , n.index
   , n.value category_id
   , l.seq
   , l.index
   , l.value::string category
FROM foursquare_nyc.standard.places_us_nyc_standard_schema s,
table(flatten(s.fsq_category_ids)) n,
table(flatten(s.fsq_category_labels)) l
WHERE n.index = l.index
ORDER BY n.seq, n.index, l.seq, l.index
)
SELECT DISTINCT to_number(category_id) category_id, category FROM data ORDER BY category_id;

-- Extract Foursquare categories
CREATE OR REPLACE TRANSIENT TABLE z_category_lookup AS
SELECT category_id, value::string category 
FROM z_category_id z
, table(flatten(input => parse_json(z.category))) c
QUALIFY row_number() OVER (PARTITION BY seq ORDER BY index DESC) = 1
ORDER BY category_id;

-- Set up Foursquare category lookup tables
CREATE OR REPLACE TRANSIENT TABLE category_lookup AS
with hierarchy AS (
SELECT c.seq, c.index, c.value::string category 
FROM z_category_id z
, table(flatten(input => parse_json(z.category))) c
)
, data AS (
SELECT
   h.*
   , c.category_id
   , lag(c.category_id) OVER (PARTITION BY h.seq ORDER BY h.index) parent_category_id
   , first_value(c.category_id) OVER (PARTITION BY h.seq ORDER BY h.index) root_category_id
FROM hierarchy h
INNER JOIN z_category_lookup c ON h.category = c.category
)
SELECT DISTINCT category, category_id, parent_category_id, root_category_id
FROM data
ORDER BY root_category_id, category_id;

Step 5: Embed Foursquare categories

In this step, we’ll embed Foursquare categories with OpenAI’s text embedding API. To facilitate the semantic search, we’ll compute cosine similarities between the embeddings of the user query (e.g., “Epic Night Out”) vs. the embeddings of each category. This way, we can return the top suggested Foursquare categories to the app, which will look up the venues with the semantically suggested categories in the user-specified neighborhoods.

First, we’ll add a new embedding column to the category_lookup table:

ALTER TABLE category_lookup add column embedding varchar;


Next, we’ll write a script that uses OpenAI text embedding API to embed the Foursquare venue categories and store the embedding vectors in the newly created column. I used a simple Python script to connect to Snowflake using the Snowflake Python Connector (find it here). It takes about 20 minutes to run.

You can use the following Snowflake query to check on the overall process:

SELECT
   COUNT(category_id) total_categories
   , COUNT(DISTINCT CASE WHEN embedding IS NOT NULL THEN category_id END) categories_embedded
FROM category_lookup;

Step 6: Create a cache version of Foursquare data

Given that we’ll want to look up Foursquare venues by their fsq_id quickly, we’ll create a cached version of the Foursquare venue data (ordered by fsq_id):

CREATE OR REPLACE TRANSIENT TABLE place_lookup AS
SELECT * FROM foursquare_nyc.standard.places_us_nyc_standard_schema
ORDER BY fsq_id;


After all the data wrangling, we have transformed the original Foursquare view into the following relational tables:

With data wrangling out of the way, let’s move on to the fun stuff…

Implementations

The goal is to use Snowflake to compute cosine similarities between the embeddings of the user query (such as "Epic Night Out") and the embeddings of each Foursquare venue category. This will let us return the top suggested categories to the app, which can then look up venues with the suggested categories in the user-specified neighborhoods.

Initially, I planned to create a scalar User-Defined Function (UDF) for performing a semantic search via a quick table scan. But due to performance reasons (explained in the performance section), I abandoned this approach in favor of native SQL implementations. This section will cover the four implementations I explored: two Python scalar UDFs, one JavaScript scalar UDF, and native SQL. In the following sections, I will discuss their performances and scalability.

Implementation 1: Python UDF using an existing function

My first attempt was to wrap a readily available cosine similarity function within a Python UDF:

CREATE OR REPLACE FUNCTION cosine_similarity_score(x array, y array)
returns float 
language python 
runtime_version = '3.8'
packages = ('scikit-learn', 'numpy')
handler = 'cosine_similarity_py'
as 
$$
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def cosine_similarity_py(x, y):
  x = np.array(x).reshape(1,-1)
  y = np.array(y).reshape(1,-1)
  cos_sim = cosine_similarity(x, y)
  return cos_sim 
$$;


The function above first transforms the 1D list into a 2D vector. It then uses scikit-learn's cosine similarity function to compute the similarity score between the two vectors.

Implementation 2: Python UDF with custom implementation

I noticed that OpenAI's embedding vectors normalize to length 1, which means that cosine similarity can be calculated using the dot product between the two vectors. So, I tried to write a Python UDF that doesn't require the scikit-learn package:

CREATE OR REPLACE FUNCTION cosine_similarity_score_2(x array, y array)
returns float 
language python 
runtime_version = '3.8'
packages = ('numpy')
handler = 'cosine_similarity_py'
as 
$$
import numpy as np

def cosine_similarity_py(x, y):
  x = np.array(x)
  y = np.array(y)
  return np.dot(x,y)
$$;


The function above transforms the 1D lists into NumPy arrays and computes the dot products of the two input arrays.

Implementation 3: JavaScript UDF with custom implementation

I also implemented a JavaScript UDF version, wondering how it would perform:

CREATE OR REPLACE FUNCTION cosine_similarity_score_js(x array, y array)
  RETURNS float
  LANGUAGE JAVASCRIPT
AS
$$
  var score = 0;
  for (var i = 0; i < X.length; i++) {
    score += X[i] * Y[i];
  }
  return score;
$$
;

Implementation 4: Native SQL

Finally, I decided to implement cosine similarity directly with SQL. Before writing the query, I flattened the JSON array category embedding values and stored them in the category_embed_value table:

CREATE OR REPLACE TRANSIENT TABLE category_embed_value AS 
WITH leaf_category AS (
    SELECT category_id 
    FROM category_lookup 
    EXCEPT 
    SELECT category_id 
    FROM category_lookup 
    WHERE category_id IN (SELECT DISTINCT parent_category_id FROM category_lookup)
)
SELECT 
    l.category_id  
    , n.index 
    , n.value 
FROM category_lookup l 
, table(flatten(input => parse_json(l.embedding))) n 
WHERE l.category_id IN (SELECT category_id FROM leaf_category)
ORDER BY l.category_id, n.index;


Then I computed cosine similarities between a test input embedding vector vs. embeddings of all categories with the following SQL:

WITH base_search AS (
-- Karaoke Bar 
SELECT embedding FROM category_lookup where category_id = 13015
)
, search_emb AS (
SELECT 
    n.index
    , n.value 
from base_search l 
, table(flatten(input => parse_json(l.embedding))) n 
ORDER BY n.index
)
, search_emb_sqr AS (
SELECT index, value 
FROM search_emb r 
)
SELECT 
    v.category_id 
    , SUM(s.value * v.value) / SQRT(SUM(s.value * s.value) * SUM(v.value * v.value)) cosine_similarity 
FROM search_emb_sqr s 
INNER JOIN category_embed_value v ON s.index = v.index 
GROUP BY v.category_id
ORDER BY cosine_similarity DESC 
LIMIT 5;

Performance evaluation

I evaluated the performance of each implementation using an X-Small warehouse. The test was to find categories (out of 853 Foursquare venue categories) that most closely match the embedding of a test category. I tested each implementation twice (and made sure to wait for the warehouse to spin down before moving on to a different implementation).

I tested the first three implementations using the following query:

WITH user_embedding AS (
-- Karaoke Bar 
SELECT embedding FROM category_lookup where category_id = 13015
)
SELECT FUNCTION_NAME(parse_json(d.embedding), parse_json(c.embedding)) cosine_similarity, c.category_id  
FROM user_embedding d, 
category_lookup c
ORDER BY cosine_similarity DESC 
LIMIT 5;


I verified the native SQL implementation using the query mentioned above.

Here are the test results:

Python UDF 1: 9 seconds for the initial query, 5 seconds on the subsequent run
Python UDF2: 7.5 seconds initially, 4 seconds on the subsequent run
Javascript UDF: 11 seconds, 11 seconds on the subsequent run
Native SQL: 1.2 seconds, 564 milliseconds on the subsequent run (due to 24-hour query caching)

I was surprised by the significant performance difference between the UDF and SQL implementation (UDFs didn’t seem to benefit from Snowflake's native query caching). I expected some language overhead for the UDFs, but not an 8x difference. Given the performance numbers, I proceeded with the native SQL implementation for the app.

Scalability evaluation

Semantically searching across 853 categories was exciting, but how scalable is it? To test scalability, I ran the native SQL solution against dummy datasets containing 10K, 100K, and 1M documents.

I created this SQL dummy table to hold embedding values for 10K, 100K, and 1M documents:

CREATE OR REPLACE TRANSIENT TABLE test_embed_value_10K AS 
WITH dummy_data AS (
  SELECT
    SEQ4() AS id,
    UNIFORM(1, 1000, SEQ4()) AS category_id,
    UNIFORM(1, 1536, SEQ4()) AS index,
    UNIFORM(0, 1, SEQ4()) AS value
  FROM
   -- Each embedding vector contains 1536 numbers 
    TABLE(GENERATOR(ROWCOUNT => 1536 * 10000))
)

SELECT *
FROM dummy_data
ORDER BY category_id, index;


I adjusted the row count in the TABLE(GENERATOR(ROWCOUNT => ... clause and the table name to create tables for 100K and 1M documents.

I used the same query (but swapped out category_embed_value with the test table name) to evaluate the scalability of the SQL implementation. Here are the results on an X-Small warehouse:

10K: 1.4 seconds
100K: 4.6 seconds
1M: 36 seconds

One of Snowflake’s benefits is its scalability. Performances can be further improved by using a larger warehouse.

Wrapping up

From this exploration, we show that Snowflake can not only power a semantic search application but also performs well when searching through up to 10K documents. Compared to keyword-based search, semantic search provides a better user experience by letting users search with intent or keywords. Ambiguous searches yield a diverse array of suggestions, while targeted searches continue to return targeted results. For example, "epic night out" returns “night club”, “beer bar”, and “escape room”. "Dim sum" returns "dim sum restaurants".

With more time, I’d have refined the project by creating a Snowflake external function to call OpenAI's embedding API, allowing me to embed new documents directly within Snowflake. Also, I’d set up a stored procedure and a scheduled task to automatically refresh the cached Foursquare data.

If you're already using Snowflake, conducting reasonably-sized semantic searches within it is possible, rather than setting up additional ETL jobs to push your data to a vector database. A capacity of 10K documents is more than enough for many applications. For example, you can search across embeddings of a book's paragraphs or chat sessions (stored in logically segregated tables for each natural grouping). Snowflake can still be a viable solution for larger document corpora depending on your use case and the compute resources you're willing to invest.

Stay tuned for Part 2, where I will discuss implementing the rest of the application with Snowflake and Streamlit. I hope you enjoyed my second article (my first article was about building GPT Lab with Streamlit). Connect with me on Twitter or Linkedin. I'd love to hear from you.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Display a race on a live map 🏃
https://blog.streamlit.io/display-a-race-on-a-live-map/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Display a race on a live map 🏃

Create a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library

By Mark Needham
Posted in Advocate Posts, June 22 2023
What's Park Run?
Architecture diagram
Data generation
Querying data in Apache Kafka
Ingesting data into Apache Pinot
Building the real-time dashboard
Wrapping up
Contents
Share this post
← All posts

Hey community, 👋

My name is Mark Needham, and I’m a Developer Advocate at StarTree. I work on Apache Pinot, a real-time distributed OLAP datastore. It’s purpose-built to deliver scalable real-time analytics with low latency (sometimes referred to as a way of querying Apache Kafka).

I love making demos of streaming data analytics. And I love running. So I built a real-time Streamlit dashboard on top of imaginary race data.

In this post, I’ll show you:

How to create a data simulator using Python’s Twisted library
How to ingest that data into Apache Kafka
How to pull data from Apache Kafka into Apache Pinot
How to query Apache Pinot to visualize the data in Streamlit


🏃
The app isn't public because it has several moving parts. I'd need to have a hosted version of Apache Kafka and Apache Pinot (doable with SaaS services), but the data generator is trickier. So you can clone the repo and get it running on your own machine.
What's Park Run?

Before we delve into the details, let me give you some background on the data we'll be generating.

Park Run is an organization that hosts free weekly community events in the UK and some other countries. They include 5k runs on Saturday mornings and 2k runs on Sunday mornings. Each course has a page with information about it, directions, and a map.

Here is the Kingston course page:

By clicking through to the map, you can download the course in KML format. This lets you extract the latitude and longitude coordinates that make up the route, and the start and finish locations. With this data, you can create simulated runs based on the route coordinates.

Architecture diagram

Let's start with an architecture diagram that displays the app’s components:

There are five components:

Create Race: A Streamlit app that configures parameters for a new race.
Data Generator: Processes new race requests, generates locations for each competitor, and publishes the events to a Kafka topic.
Apache Kafka: Streaming data platform that acts as the source of truth for locations and races.
Apache Pinot: Real-Time OLAP database that consumes data from Kafka.
Real-Time Dashboard: A Streamlit app to show what’s happening in each race including a leadership and a live map.
Data generation

The data generation process begins with creating a race using the Streamlit app. It’s used to configure the parameters for a new race. You can choose the course, the number of competitors, the fastest and slowest potential pace, and the number of competitors that should get stuck in a geo-fenced part of the course.

Here is what it looks like:

After selecting the parameters, click on Generate race. This sends a POST request to the Data Generator. It has two components:

An HTTP server that handles new race requests. Its resource generates all the locations (with associated timestamps) of a competitor for a race and writes them to an in-memory map.
An event loop that runs every second and iterates over active competitors stored in the in-memory map. It extracts the competitor's latest location that occurred in the past and publishes the location to Kafka.
Querying data in Apache Kafka

kcat (previously known as Kafka cat) is an open-source command-line utility used for producing, consuming, and managing Apache Kafka messages. Use it to verify that your data has been successfully sent to Kafka.

The command below retrieves one record from the parkrun topic and pipes the output into jq, a command-line JSON processor:

kcat -C -b localhost:9092 -t parkrun -c1 | jq

{
  "runId": "637648ff-46fa-464b-9d80-acb647e7aa41",
  "eventId": "07803c75-cae5-4e69-8361-3bda41aa686f",
  "competitorId": 551870,
  "rawTime": 0,
  "timestamp": "2023-05-25 10:17:49",
  "lat": 51.45034,
  "lon": -0.29499,
  "distance": 0,
  "course": "richmond"
}

Ingesting data into Apache Pinot

Next, we’ll transfer the data from Kafka to Pinot. Pinot stores it in tables that can contain any number of columns. To create a table, you need to provide a schema and table configuration.

Let's begin with the schema for the parkrun table:

{
  "schemaName": "parkrun",
  "primaryKeyColumns": ["competitorId"],
  "dimensionFieldSpecs": [
    {"name": "runId", "dataType": "STRING"},
    {"name": "eventId", "dataType": "STRING"},
    {"name": "competitorId", "dataType": "LONG"},
    {"name": "rawTime", "dataType": "INT"},
    {"name": "lat", "dataType": "DOUBLE"},
    {"name": "lon", "dataType": "DOUBLE"},
    {"name": "location", "dataType": "BYTES"},
    {"name": "course", "dataType": "STRING"}
  ],
  "metricFieldSpecs": [{"name": "distance", "dataType": "DOUBLE"}],
  "dateTimeFieldSpecs": [
    {
      "name": "timestamp",
      "dataType": "TIMESTAMP",
      "format": "1:MILLISECONDS:EPOCH",
      "granularity": "1:MILLISECONDS"
    }
  ]
}


The columns in the schema are categorized using a similar language to data warehousing.

There are three categories:

Dimension columns: Used in slice and dice operations, such as when using the SQL WHERE and GROUP BY clauses.
Metric columns: Represent quantitative data and are used in aggregations, such as when using the SQL SUM, MIN, MAX, COUNT, and AVG functions. You can also filter them.
DateTime columns: Represent time columns in the data. There can be many of them in a table, but only one can be treated as primary. They can also be used with the WHERE or GROUP BY clauses.

Apache Pinot aligns and ingests data from the source (in this case, Kafka messages) based on matching property names with its column names. In other words, if a Kafka event includes a property labeled “foo,” and there exists a corresponding ”foo” column in the Pinot schema, Pinot will automatically ingest the value of “foo” from the Kafka event into its “foo” column.

The only field that doesn't have a corresponding source property is location. Let’s populate that using a transformation function:

{
  "tableName": "parkrun",
  "tableType": "REALTIME",
  "segmentsConfig": {
    "timeColumnName": "timestamp",
    "schemaName": "parkrun",
    "replication": "1",
    "replicasPerPartition": "1"
  },
  "tenants": {"broker": "DefaultTenant", "server": "DefaultTenant"},
  "tableIndexConfig": {
    "loadMode": "MMAP",
    "streamConfigs": {
      "streamType": "kafka",
      "stream.kafka.topic.name": "parkrun",
      "stream.kafka.broker.list": "kafka-run:9093",
      "stream.kafka.consumer.type": "lowlevel",
      "stream.kafka.consumer.prop.auto.offset.reset": "smallest",
      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
      "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
    },
  },
  "upsertConfig": {"mode": "FULL"},
  "routing": {"instanceSelectorType": "strictReplicaGroup"},
  "ingestionConfig": {
    "batchIngestionConfig": {
      "segmentIngestionType": "APPEND",
      "segmentIngestionFrequency": "DAILY"
    },
    "transformConfigs": [
      {
        "columnName": "location",
        "transformFunction": "toSphericalGeography(stPoint(lon, lat))"
      }
    ]
  },
  "metadata": {}
}


This table has a tableType of REALTIME, which means Pinot expects to see configuration for the streaming data platform under tableIndexConfig.streamConfigs. Set your streamType to kafka and then specify a couple of configuration parameters to indicate how messages should be decoded.

Define the following parameters:

stream.kafka.broker.list indicates where the Kafka broker is running
[stream.kafka.topic.name](<http://stream.kafka.topic.name>) is the name of your topic
stream.kafka.consumer.type indicates how Kafka partitions should be consumed—lowlevel means that Pinot should consume them in parallel
stream.kafka.consumer.prop.auto.offset.reset specifies where consumption of the Kafka topic should begin—smallest means to start from the earliest available offset

Next, define a transformation configuration under ingestionConfig.transformConfigs to populate the location column. This function creates a geography object to represent the latitude/longitude location.

This table is also using Pinot’s upsert functionality (upsertConfig.mode). When you query the table, by default, you only get the most recent record for a given primary key. You’ll use this functionality because, for the majority of queries, you want to see only the latest location of each competitor.

The two other tables also have schema and table configurations (learn more here):

races keeps track of all the races
courses has the course information
Building the real-time dashboard

Let's take a look at the real-time dashboard Streamlit app:

Create an [app.py](<http://app.py>) file and run streamlit run app.py from the terminal.
Use pinotdb to query data from Apache Pinot (before querying, the user must select a race to follow).
Populate a selectbox with data from a query against the races table:
from pinotdb import connect
import pandas as pd
import streamlit as st

conn = connect(host='localhost', port=8099, path='/query/sql', scheme='http')
curs = conn.cursor()
curs.execute("""
select runId, course, ToDateTime(startTime, 'YYYY-MM-dd HH:mm:ss') AS startTime,
       lookup('courses','longName','shortName',course) as courseName
from races
order by startTime DESC
""")
df = pd.DataFrame(curs, columns=[item[0] for item in curs.description])
COURSES_MAP = {pair[0]: f"{pair[3]} at {pair[2]}" for pair in df.values.tolist()}

run_id = st.selectbox(
    'Select event:', df['runId'].astype(str),
    format_func=lambda x:COURSES_MAP[ x ]
)


The query retrieves the ten most recent races in descending order. To retrieve the full name of the course, you’ll perform an in-memory join using Pinot's lookup function.

Refresh the app for the dropdown menu to appear and select a race:

Next, view the leaderboard to see who is winning and how far they are from the finish line.

Add the following code to the app:

curs = conn.cursor()
curs.execute("""
select competitorId,
    distance AS distanceCovered,
    round(%(courseDistance)d - distance, 1) AS distanceToGo,
    ToDateTime(1000 / (distance / rawTime) * 1000, 'HH:mm:ss') AS pacePerKm,
    ToDateTime(rawTime * 1000, 'mm:ss') AS raceTime
from parkrun
WHERE runId = %(runId)s
ORDER BY distanceToGo, rawTime
limit 10
""", {"courseDistance": distance, "runId": run_id})
df = pd.DataFrame(curs, columns=[item[0] for item in curs.description])

st.header("Leaderboard")
styler = df.style.hide(axis='index')
st.write(styler.to_html(), unsafe_allow_html=True)


Here is the race-in-progress leaderboard:

Use the experimental_rerun function for the app to refresh automatically.

The code is located near the top of the app:

if not "sleep_time" in st.session_state:
    st.session_state.sleep_time = 2

if not "auto_refresh" in st.session_state:
    st.session_state.auto_refresh = True

auto_refresh = st.sidebar.checkbox('Auto Refresh?', st.session_state.auto_refresh)

if auto_refresh:
    number = st.sidebar.number_input('Refresh rate in seconds', value=st.session_state.sleep_time)
    st.session_state.sleep_time = number


This code is located at the end:

if auto_refresh:
    time.sleep(number)
    st.experimental_rerun()


You can find it in a sidebar:

The best feature of this app is the live map that displays the location of each competitor. here is how to do it:

Create it using Python's Folium library and render it to Streamlit using the streamlit-folium package.
Save the course map (a geo-fenced area) and the start and end points in Pinot using WKT format.
Use the Shapely library to extract an array of x and y coordinates.

Here is the code for converting WKT to coordinates:

from shapely import wkt

start_wkt = "POINT (-0.063253 51.41917000000001)"
end_wkt = "POINT (-0.064283 51.419324)"

x_start, y_start = wkt.loads(start_wkt).coords.xy
x_end, y_end = wkt.loads(end_wkt).coords.xy


Next, create a Folium map and add these points to the map:

from streamlit_folium import st_folium

m = folium.Map()

folium.Marker(location=(y_start[0], x_start[0]),
  icon=folium.Icon(color="green", icon="flag"), popup="Start").add_to(m)
folium.Marker(location=(y_end[0], x_end[0]),
  icon=folium.Icon(color="red", icon="flag"), popup="Finish").add_to(m)


Then get the coordinates for the course map and geofenced area (I have hardcoded those locations):

# Example values (these are derived from the database)
x = [-0.064245, -0.064524, -0.065779, -0.065801, -0.065865, -0.06579, -0.065854, -0.065962, -0.065672, -0.065329, -0.064824, -0.064277, -0.063977, -0.063805, -0.063719, -0.063644, -0.063859, -0.063988, -0.064181, -0.065178, -0.065887, -0.066606, -0.067292, -0.068354, -0.06859, -0.06903, -0.068976, -0.067281, -0.066777, -0.066359, -0.06564, -0.065597, -0.065608, -0.06579, -0.065822, -0.066858, -0.068118, -0.068837, -0.069143, -0.069041, -0.06961, -0.070479, -0.070725, -0.070618, -0.070522, -0.0705, -0.070511, -0.07065, -0.071326, -0.071637, -0.071659, -0.071133, -0.071173, -0.071857, -0.072002, -0.072238, -0.073204, -0.074212, -0.074598, -0.074813, -0.075628, -0.075757, -0.075178, -0.073912, -0.073408, -0.072742, -0.071981, -0.071726, -0.071641, -0.071388, -0.071219, -0.070994, -0.070806, -0.070656, -0.070475, -0.070213, -0.069919, -0.069695, -0.069418, -0.069199, -0.068542, -0.067217, -0.066694, -0.066166, -0.066032, -0.065114, -0.064406, -0.064556, -0.065141, -0.065726, -0.063215]
y = [51.419386, 51.419459, 51.419754, 51.419673, 51.419513, 51.419366, 51.419178, 51.418984, 51.41879, 51.418757, 51.41877, 51.41877, 51.41873, 51.418777, 51.418676, 51.418496, 51.418288, 51.418061, 51.417967, 51.417606, 51.417271, 51.41695, 51.416964, 51.417017, 51.417245, 51.417592, 51.417706, 51.418583, 51.419285, 51.420088, 51.421523, 51.421998, 51.42245, 51.422925, 51.423045, 51.422878, 51.422731, 51.422838, 51.423223, 51.423601, 51.423955, 51.424136, 51.424029, 51.423902, 51.423721, 51.423607, 51.42342, 51.423306, 51.422664, 51.422677, 51.422771, 51.423681, 51.424216, 51.424335, 51.423654, 51.4233, 51.421768, 51.420255, 51.419626, 51.41954, 51.420021, 51.420188, 51.421078, 51.423099, 51.423942, 51.423962, 51.423668, 51.424373, 51.424553, 51.424676, 51.424928, 51.425092, 51.42526, 51.425281, 51.42532, 51.425337, 51.425293, 51.425186, 51.425009, 51.424812, 51.4245, 51.424397, 51.424232, 51.423997, 51.423134, 51.422836, 51.422607, 51.4219, 51.421269, 51.419867, 51.419232]
x_geo = [-0.0651347637176514, -0.0643622875213623, -0.0632894039154053, -0.0638902187347412, -0.0645339488983154, -0.0656068325042725, -0.0676238536834717, -0.0686323642730713, -0.0679242610931396, -0.0672805309295654, -0.0651347637176514]
y_geo = [51.41916166790023, 51.41886727626769, 51.41846583007674, 51.417997471730985, 51.41767630894881, 51.416900156242406, 51.41687339212095, 51.41715441461497, 51.41776998166006, 51.41878698731156, 51.41916166790023]
   

loc = [(point[1], point[0]) for point in zip(x_geo, y_geo)]
lat = sum([point[0] for point in loc]) / len(loc)
lon = sum([point[1] for point in loc]) / len(loc)
folium.PolyLine(loc, color='red', weight=2, opacity=0.8).add_to(m)

loc = [(point[1], point[0]) for point in zip(x, y)]
lat = sum([point[0] for point in loc]) / len(loc)
lon = sum([point[1] for point in loc]) / len(loc)
route = folium.PolyLine(loc, color='#808080', weight=2, opacity=0.8).add_to(m)

m.fit_bounds(route.get_bounds())


A DataFrame contains the latest locations for each competitor. Add it to a feature group so that only that portion of the map will be refreshed:

fg = folium.FeatureGroup(name="Competitors")

for lat, lon in zip(df_front.lat.values, df_front.lon.values):
    fg.add_child(
        folium.CircleMarker(location=(lat, lon), radius=3, color='Fuchsia')
    )

st_data = st_folium(m, 
    feature_group_to_add=fg,
    height=400,
    width=700,
)


Here is a race in progress:

Wrapping up

Thank you for reading my post! I hope it has given you some ideas on the types of apps that you can build with Apache Kafka and Apache Pinot. If you have any questions, please post them in the comments below or contact me on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

ESG reporting with Streamlit
https://blog.streamlit.io/esg-reporting-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

By Sven Koerner and Mathias Landhäußer
Posted in Snowflake powered ❄️, June 23 2023
App overview
Step 1. Connect to Snowflake and get the Documents
Step 2. Connect to semantha and feed the documents into the consolidated analysis
Step 3. Drill down into a single document
Step 4. Identify documentation your company already has but doesn't know where
Bonus. Generate text for your ESG report!
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

We are Sven and Mathias, semantic data processing researchers. Since 2007, we have been studying the machines’ ability to understand natural language—this was before AI was cool. Our dream was to provide technology that could quickly and efficiently filter through the noise, leaving only the most relevant and helpful information. Fast forward to 2017. We found semantha and built an adaptive AI for text-driven processes.

Then last year, we chatted with Snowflake, and the topic of Environmental, Social, and Governance (ESG) reporting came up. We talked about how it’s riddled with manual document processing once you go beyond quantitative KPI—which you’re required to do. To do it correctly, you must assess the entire reporting standard, but companies often look only at the easy 10%—financial KPI, gender pay gaps, greenhouse gas emissions, etc. All that is computed from structured data like your basic Excel spreadsheets. The other 90% is buried in the documents like labor law, safety, health insurance, internal policies, and so on.

The short of it is, most companies don’t provide complete and accurate reports.

Ouch.

After this talk a lightbulb came up in our heads (or two?) 💡 💡

What if we pulled data from Snowflake, did the analyses with semantha, and displayed the results in a beautiful Streamlit app?

So a week later, we built it, deployed it, and it’s running. We called it K-A-T-E One.

In this post, we’ll show you the fundamentals of the app, how to use it, and what insights to gain from your data. Let’s get started!

🚀
If you have access to a Snowflake system, run your data by opening the app and connecting to Snowflake. If you want to clone the code, here is the repo.
App overview

K-A-T-E One can fetch ESG-related content from Snowflake, perform a coverage analysis using semantha, and answer questions like these:

How to jumpstart our reporting process?
What do we already have in place to report on?
What is in a given annual report or draft (and what is missing)?
Where does the information come from? That’s a valid question.

Here is how it works:

Here is how you’d use it:

Connect the app to Snowflake
Choose the documents to be analyzed and get a breakdown of the included topics. For each document, an overview will highlight the covered topics and identify any missing ones.
Select a single document for in-depth analysis. See a topic distribution (Altair bar chart) and the highlighted topics broken down by ESG reporting category in a sunburst chart (Plotly).

Go ahead and play with it:



Now, let’s get to coding!

Step 1. Connect to Snowflake and get the Documents

Connecting to Snowflake is easy. All you need are the Snowflake SDK and valid credentials. We store our credentials in Streamlit secrets.toml (per Streamlit docs).

🚀
NOTE: We added the stage we're reading from to the configuration. This stage is not used by the Snowflake connector when querying, but it can be useful information to include. You can access the secrets as environment variables or by querying the st.secrets dictionary.

Because we used sections, we used the dictionary:

[snowflake]
account   = "account.region"
user      = "myuser"
role      = "myrole"
password  = "************"
warehouse = "my.example.warehouse"
database  = "mydb"
schema    = "myschema"
stage     = "ESG_DEMO"


To begin, import the Snowflake libraries and access the st.secrets dictionary. Then, open a connection and fetch the document list from the configured stage:

import snowflake.connector as snowconnector

def is_analyzable(filename):
    return filename.endswith(".pdf") or filename.endswith(".txt") or filename.endswith(".docx")

ctx = snowconnector.connect(st.secrets.snowflake)
with ctx.cursor() as cur:
    cur.execute("SELECT relative_path FROM directory(%s);", (st.secrets.snowflake.stage,))

    # This is how you'll get a list of all files
    result = cur.fetchall()

    # We filter the file types so that we only use text in the demo
    # and take only the first 20 that we want to read...
    # If you expect many files, you wouldn't use the filter and then trim the list
    # but append analyzable files one by one until you reach the size limit to
    # avoid checking files that you'd then throw away anyways (see the code in
    # the GitHub repository for details)
    filtered = [element for element in result if is_analyzable(element)][:20]


That's it! Now you have a list of documents you can analyze for ESG content.

On the UI side, connecting is easy. Just fill in your connection details in the sidebar:

Once you connect to Snowflake, the app will load the first 20 documents and display a preview:

To keep this brief, we filtered file types to show only PDFs, plaintext, and docx files and set the document number to 20. In your app, you can remove these limitations.

Step 2. Connect to semantha and feed the documents into the consolidated analysis

To prepare the app, we trained semantha to perform ESG-related analyses. The background knowledge is stored in semantha's library (similar to books 📚, not like a program library 🤖), hence the term "library" in the following code snippet. We'll focus on the EGS example for this post, but you can modify it for other purposes.

As in Step 1, utilize Streamlit's secrets.toml to store the semantha credentials (K-A-T-E One is pre-authenticated, so provide your credentials only if you use your own private instance):

[semantha]
server_url = "<https://example.semantha.systems>"
api_key = "YOUR_API_KEY"
domain = "ESG_DEMO"


Next, fetch the binary documents from Snowflake one by one and feed them to semantha to obtain the corresponding results (in Step 1, you only collected the file names and locations):

import streamlit as st
import semantha_sdk

def analyze_doc_collection(semantha, documents):
    my_bar = st.progress(0.0, text="Analysis running. This will take some time!")
    increment = 1 / len(documents)
    for i, doc in enumerate(documents):
        __curr_file_name = doc.get_name()
        my_bar.progress((i + 1) * increment, text=f"Analysis running... Processing file '{__curr_file_name}'")
        results = semantha.domains(domainname=st.secrets.semantha.domain).references.post(
            file=doc.as_stream(),
            similaritythreshold=0.9,
            maxreferences=1
        )

        # [...]
        # process and display results - see GitHub :) 

# Connect to semantha
sem = semantha_sdk.login(
    server_url=st.secrets.semantha.userver_url,
    key=st.secrets.semantha.api_key
    )

# Run the analysis
analyze_doc_collection(sem, filtered)


Lines 10-14 in the code snippet above are important. The app sends a file to semantha using the references endpoint of the API through an HTTP POST request. The selected domain ESG_DEMOcontains the library with our background knowledge. semantha analyzes the file and discards all findings except one (maxreferences=1) and findings with lower confidence (threshold=0.7).

Next, produce a sunburst chart and a table with the document topics. We used ESG reports for the analysis to show which ESRS facets are covered, where (sunburst chart in Step 3), and which facets are missing (document breakdown below the chart).

In the user interface, click on "Analyze Document Collection." If you want to skim-read, open them in the embedded PDF reader beforehand.

Here is what the results will look like:

You can see the high-level topics in each document, with active and inactive buttons indicating whether a topic is covered. To see how a document covers a topic, click on the analysis button next to the filename and navigate to the "Individual Document" tab.

Curious about the button functions? Click on them or check out the Bonus section below. 😉

Step 3. Drill down into a single document

Want to get into the details? Drill down into any document. See what ESRS-related content you already have and what's missing. It'll help you steer your reports and better understand how well the competition is performing. No one said you must use your data to process with K-A-T-E One. 😉

If you want to do an individual analysis of a single document, start with the bar chart. It shows one bar per page that contains relevant content. You'll get a quick document overview. Does it mainly talk about a single topic, or is it rather a high-level document covering multiple topics?

Scroll down to see an overview of the topics covered on the left and a breakdown of the specific ESRS facet covered (and not covered) by the document on the right.

In this example, the document doesn't cover the topic "Affected communities" at all but contains statements related to the topic "Biodiversity and ecosystems" (which is not surprising, given that the document is called "Environment Protection Program Policy (EPPP)"). And while some facets of the ESRS topic are covered, other essential facets are missing.

You must decide:

Are these missing facets not important to the EPPP?
If they're important, should they be covered elsewhere?
Could they be a sensible extension of your policy?

Scroll down some more for a sunburst chart that details the coverage of ESRS topics, broken down from the highest hierarchy level (in the center of the chart) down to the individual facet (second ring). The outermost ring includes the actual snippet from the document.

As you can see, the document mainly focuses on governance (top half of the chart). ESRS 2 focuses on measuring and monitoring aspects of EPPP, including governance. The bottom left quarter covers the policy topic of "ESRS E4 / Biodiversity and Ecosystems". EPPP also addresses handling pollutants by the workforce but with less emphasis than other topics.

Step 4. Identify documentation your company already has but doesn't know where

Nothing is worse than staring at a blank sheet of paper, wondering where to start and how to structure your thoughts.

To start your reporting, identify the important reporting aspects for your operation and stakeholders. Since ESG concepts are not new, there should already be some existing frameworks. To gain a deep understanding, perform the same analysis as in Step 2, but this time with all your data. Utilize your AI to guide you, identify covered areas, and you have your starting point. Let's face it, as does everyone else, you likely lack oversight of your ESG documentation.

Use the analysis results as a starting point for your materiality assessment. Prior coverage of these topics, even without reporting in mind, indicates their value or necessity to your business operation. Include them in any report you send out. Cross-reference the results of this analysis with your materiality assessment to identify what you have covered and areas that require further study or control.

You can now get a breakdown of the ESRS facets, where you have already covered them, and where you didn't. Given the analysis in Step 2, you can identify topics that were not previously covered, drill down to individual paragraphs, and connect the dots across multiple documents to beef up your report.

With the help of AI, it's almost as much fun as a painting by numbers! 🖼️

Bonus. Generate text for your ESG report!

Now you can ask semantha to summarize the paragraphs, properly referencing your documents (below the consolidated analysis). Each ESRS topic covered in the analyzed documents can be summarized.

Just click a topic button in the document list and scroll down to the summary:

You can quickly prepare a draft report section based on qualitative data and auditable references. But you must first identify the ESRS facets relevant to your business operation. In Step 4, we demonstrated how to start or extend your materiality assessment based on available documentation.

You can also begin with the mandatory facets:

ESRS 2 General disclosures (all disclosures)
ESRS E1 Climate Change (all disclosures)
ESRS S1 Own Workforce (subsets apply according to company size)

No AI is required to identify these facets. 😉

Wrapping up

Thank you for reading our post! You've seen that getting information from unstructured documents stored in Snowflake and visualizing the results is bliss when combining the powers of Streamlit, Snowflake, and semantha. We've shown how to fetch documents from Snowflake, push them to semantha for analysis using their respective Python SDKs, and view the results in a beautiful 💖 Streamlit app.

We've excluded a bunch of code to make the post easier to read. Feel free to clone it from our repo! The SDKs are public, too (go to snowflake-connector-python and the semantha-sdk to learn more).

If you have any questions, please post them in the comments below or contact Sven or Mathias on LinkedIn.

Happy Streamlit-ing! 🚀

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Drill-downs and filtering with Streamlit and Altair
https://blog.streamlit.io/drill-downs-and-filtering-with-streamlit-and-altair/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Drill-downs and filtering with Streamlit and Altair

Display an Altair chart definition in Streamlit using the st.altair_chart widget

By Carlos D Serrano
Posted in Advocate Posts, July 12 2023
Avoiding re-runs
Let's create some data first
Color consistency
Selections
Filtering chart
Filtered and faceted charts
All of it in just one widget
Dashboard
Full code
Wrapping up
Contents
Share this post
← All posts

For the last few months, I've written several apps using Streamlit, and one of my favorite libraries for optimizing the look and feel of my app is Altair. You can display an Altair chart definition in Streamlit using the st.altair_chart widget. Altair is a powerful library full of styling, configurations, and interactions.

In this post, I'll show you how to create interactive and dynamic visualizations using Altair in Streamlit.

🍩
To jump right in, check out the demo app and the code.
Avoiding re-runs

One significant advantage of creating drill-downs and filters with Altair on Streamlit is that these interactions occur at the front-end level and don't require a re-run of your Streamlit app.

Let's create some data first

I used a simple approach for this example and created a Pandas DataFrame with sales data. I also used the st.cache_data decorator to save the DataFrame in the cache:

@st.cache_data
def get_data():
    dates = pd.date_range(start="1/1/2022", end="12/31/2022")
    data = pd.DataFrame()
    sellers = {
        "LATAM": ["S01", "S02", "S03"],
        "EMEA": ["S10", "S11", "S12", "S13"],
        "NA": ["S21", "S22", "S23", "S24", "S25", "S26"],
        "APAC": ["S31", "S32", "S33", "S34", "S35", "S36"],
    }
    rows = 25000
    data["transaction_date"] = np.random.choice([str(i) for i in dates], size=rows)
    data["region"] = np.random.choice(regions, size=rows, p=[0.1, 0.3, 0.4, 0.2])
    data["transaction_amount"] = np.random.uniform(100, 250000, size=rows).round(2)
    data["seller"] = data.apply(
        lambda x: np.random.choice(sellers.get(x["region"])), axis=1
    )
    return data.sort_values(by="transaction_date").reset_index(drop=True)

Color consistency

When creating drill-downs, it's crucial to maintain color consistency to enhance the clarity of your charts. Altair scales can be used to specify color domains and ranges that persist during drill-down.

Use the following three list variables in the Altair chart definitions:

regions = ["LATAM", "EMEA", "NA", "APAC"]
colors = ["#aa423a","#f6b404", "#327a88","#303e55","#c7ab84","#b1dbaa",
    "#feeea5","#3e9a14","#6e4e92","#c98149", "#d1b844","#8db6d8"]
months = [
    "Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec",
]

Selections

To define a selection in Altair, you can use the single, interval, or point methods.

In this example, a single selection is used to drill down based on region. The "empty" attribute can be specified to determine whether all objects or no objects are visible when no selection has been made.

region_select = alt.selection_single(fields=["region"], empty="all")

Filtering chart

This chart definition includes an Altair add_selection method for filtering other chart definitions that will be created later.

To create a dynamic experience, the opacity attribute is used to reduce the opacity of unselected objects to 25%. Note that the color method uses the scale attribute to limit the available colors to the domain and range variables defined previously:

region_pie = (
    (
        alt.Chart(sales_data)
        .mark_arc(innerRadius=50)
        .encode(
            theta=alt.Theta(
                "transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Sum of Transactions",
            ),
            color=alt.Color(
                field="region",
                type="nominal",
                scale=alt.Scale(domain=regions, range=colors),
                title="Region",
            ),
            opacity=alt.condition(region_select, alt.value(1), alt.value(0.25)),
        )
    )
    .add_selection(region_select)
    .properties(title="Region Sales")
)

Filtered and faceted charts

To enable filtering, implement the Altair transform_filter method. Faceting is enabled by using the facet method inside the encoding method, which uses a field attribute and a column attribute to break down the chart into multiple related charts.

If you need to facet by two different fields, use the repeat() method:

region_summary = (
    (
        alt.Chart(sales_data)
        .mark_bar()
        .encode(
            x=alt.X(
                "month(transaction_date)",
                type="temporal",
            ),
            y=alt.Y(
                field="transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Total Sales",
            ),
            color=alt.Color(
                "region",
                type="nominal",
                title="Regions",
                scale=alt.Scale(domain=regions, range=colors),
                legend=alt.Legend(
                    direction="vertical",
                    symbolType="triangle-left",
                    tickCount=4,
                ),
            ),
        )
    )
    .transform_filter(region_select)
    .properties(width=700, title="Monthly Sales")
)

sellers_monthly_pie = (
    (
        alt.Chart(sales_data)
        .mark_arc(innerRadius=10)
        .encode(
            theta=alt.Theta(
                field="transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Total Transactions",
            ),
            color=alt.Color(
                "month(transaction_date)",
                type="temporal",
                title="Month",
                scale=alt.Scale(domain=months, range=colors),
                legend=alt.Legend(
                    direction="vertical",
                    symbolType="triangle-left",
                    tickCount=12,
                ),
            ),
            facet=alt.Facet(
                field="seller",
                type="nominal",
                columns=8,
                title="Sellers",
            ),
            tooltip=alt.Tooltip(["sum(transaction_amount)", "month(transaction_date)"]),
        )
    )
    .transform_filter(region_select)
    .properties(width=150, height=150, title="Sellers transactions per month")
)

All of it in just one widget

To enable selections and filters, enclose them within a single Streamlit widget. To arrange charts, use the following methods:

VConcat and HConcat methods to concatenate charts
Pipe symbol "|" to place charts next to each other
Ampersand symbol "&" to set charts below or above each other
Plus sign “+” to overlay charts

To make a dashboard-like arrangement, create a variable called top_row and use the pipe symbol to arrange your region_pie and region_summary charts side by side. Then, using the ampersand, place top_row and sellers_monthly_pie below it. This creates a variable containing all the concatenated charts in a single Streamlit altair_chart widget.

Note that when using concatenated charts, the use_container_width attribute won't work. Therefore, you must specify the width of the charts in their properties:

#Create first row by concatenating horizontally
top_row = region_pie | region_summary
#Create dashboard by concatenating top_row with faceted chart
full_chart = top_row & sellers_monthly_pie

#Single Streamlit Object
st.altair_chart(full_chart)

Dashboard
Full code
import streamlit as st
import altair as alt
import pandas as pd
import numpy as np

st.set_page_config(layout="wide")
regions = ["LATAM", "EMEA", "NA", "APAC"]
colors = [
    "#aa423a",
    "#f6b404",
    "#327a88",
    "#303e55",
    "#c7ab84",
    "#b1dbaa",
    "#feeea5",
    "#3e9a14",
    "#6e4e92",
    "#c98149",
    "#d1b844",
    "#8db6d8",
]
months = [
    "Jan",
    "Feb",
    "Mar",
    "Apr",
    "May",
    "Jun",
    "Jul",
    "Aug",
    "Sep",
    "Oct",
    "Nov",
    "Dec",
]
st.title("2022 Sales Dashboard")

@st.cache_data
def get_data():
    dates = pd.date_range(start="1/1/2022", end="12/31/2022")
    data = pd.DataFrame()
    sellers = {
        "LATAM": ["S01", "S02", "S03"],
        "EMEA": ["S10", "S11", "S12", "S13"],
        "NA": ["S21", "S22", "S23", "S24", "S25", "S26"],
        "APAC": ["S31", "S32", "S33", "S34", "S35", "S36"],
    }
    rows = 25000
    data["transaction_date"] = np.random.choice([str(i) for i in dates], size=rows)
    data["region"] = np.random.choice(regions, size=rows, p=[0.1, 0.3, 0.4, 0.2])
    data["transaction_amount"] = np.random.uniform(100, 250000, size=rows).round(2)
    data["seller"] = data.apply(
        lambda x: np.random.choice(sellers.get(x["region"])), axis=1
    )
    return data.sort_values(by="transaction_date").reset_index(drop=True)

sales_data = get_data()

region_select = alt.selection_single(fields=["region"], empty="all")
region_pie = (
    (
        alt.Chart(sales_data)
        .mark_arc(innerRadius=50)
        .encode(
            theta=alt.Theta(
                "transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Sum of Transactions",
            ),
            color=alt.Color(
                field="region",
                type="nominal",
                scale=alt.Scale(domain=regions, range=colors),
                title="Region",
            ),
            opacity=alt.condition(region_select, alt.value(1), alt.value(0.25)),
        )
    )
    .add_selection(region_select)
    .properties(title="Region Sales")
)

region_summary = (
    (
        alt.Chart(sales_data)
        .mark_bar()
        .encode(
            x=alt.X(
                "month(transaction_date)",
                type="temporal",
            ),
            y=alt.Y(
                field="transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Total Sales",
            ),
            color=alt.Color(
                "region",
                type="nominal",
                title="Regions",
                scale=alt.Scale(domain=regions, range=colors),
                legend=alt.Legend(
                    direction="vertical",
                    symbolType="triangle-left",
                    tickCount=4,
                ),
            ),
        )
    )
    .transform_filter(region_select)
    .properties(width=700, title="Monthly Sales")
)

sellers_monthly_pie = (
    (
        alt.Chart(sales_data)
        .mark_arc(innerRadius=10)
        .encode(
            theta=alt.Theta(
                field="transaction_amount",
                type="quantitative",
                aggregate="sum",
                title="Total Transactions",
            ),
            color=alt.Color(
                "month(transaction_date)",
                type="temporal",
                title="Month",
                scale=alt.Scale(domain=months, range=colors),
                legend=alt.Legend(
                    direction="vertical",
                    symbolType="triangle-left",
                    tickCount=12,
                ),
            ),
            facet=alt.Facet(
                field="seller",
                type="nominal",
                columns=8,
                title="Sellers",
            ),
            tooltip=alt.Tooltip(["sum(transaction_amount)", "month(transaction_date)"]),
        )
    )
    .transform_filter(region_select)
    .properties(width=150, height=150, title="Sellers transactions per month")
)

top_row = region_pie | region_summary
full_chart = top_row & sellers_monthly_pie
st.altair_chart(full_chart)

Wrapping up

Altair charts in Streamlit are an efficient and performant way to add interactive charts to your app. There are many styles and combinations of interactions to create using these tools.

If you loved this post, check out my other articles on client-side filtering using Altair Sliders, paginating dataframes, and the multiselect widget. And if you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, Twitter, or Medium.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Semantic search, Part 2: Building a local search app
https://blog.streamlit.io/semantic-search-part-2-building-a-local-search-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Semantic search, Part 2: Building a local search app

Making an app with Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data from Snowflake Marketplace

By Dave Lin
Posted in Snowflake powered ❄️, May 18 2023
How does the app work?
Step 1. Safeguard your Snowflake account
Step 2. Write backend functions to get data from Snowflake
1. Import the necessary packages:
2. Create base functions to connect to Snowflake and execute Snowflake queries:
3. Create functions to get boroughs and neighborhoods within a borough:
4. Create a function to get a list of categories that semantically match the user's query:
5. Create a function to get places within specific categories and in a list of neighborhoods:
Step 3: Implement OpenAI endpoints
Step 4. Write the frontend Streamlit app
1. Import the necessary libraries and backend functions:
2. Set the page configuration:
3. Create a function to render the call-to-action (CTA) links:
4. Create a function to lay out the search options in the sidebar:
5. Create the function to render the search results:
6. Create the handler function to load neighborhoods:
7. Create the handler function to handler venue search:
8. Control UI renders with sessions state variables:
Wrapping up
Contents
Share this post
← All posts

This is Part 2 of a two-part blog series on building a semantic search application for finding Foursquare venues in NYC. We'll leverage Streamlit, Snowflake, OpenAI, and Foursquare's free NYC venue data from Snowflake Marketplace.

In Part 1, we explored building a semantic search engine powered by Snowflake. We delved deep into data wrangling, compared different cosine similarity implementations, and evaluated their performance.

In this second part, I'll guide you through the remaining steps to complete the app.

🔴
Want to dive right in? Check out the code and the app. The Streamlit application will be removed after the Snowflake Summit in June due to cost considerations.

Before we proceed, let's quickly discuss…

How does the app work?

The app works as follows:

Users search for venues by selecting up to five neighborhoods in an NYC borough and entering a search query (e.g., "Epic Night Out").
The application:
Uses OpenAI's Embeddings API to generate embeddings for the search query
Uses Snowflake to retrieve up to five Foursquare venue categories with embedding vectors that most closely match the user query embeddings
Uses Snowflake to search for venues within the suggested categories in the chosen neighborhoods
Displays the venues on a map view and in a data frame

Now that we understand the app, let's proceed with the remaining steps!

Step 1. Safeguard your Snowflake account

Ensuring the security of your Snowflake account is essential. To mitigate security risks, provide the least amount of access necessary to all users and applications.

To limit Streamlit's Snowflake access, follow these steps:

1. Create a user account for the Streamlit application (for example, svc_streamlit)

2. Create a scoped-down role with read-only access to the Foursquare data, the new schema and tables, and a warehouse:

CREATE ROLE foursquare_read; 
GRANT usage on database foursquare to role foursquare_read; 
GRANT usage on schema foursquare.main to role foursquare_read; 
GRANT SELECT ON ALL tables in schema foursquare.main to role foursquare_read; 
GRANT SELECT ON FUTURE tables in schema foursquare.main to role foursquare_read; 
GRANT SELECT ON FUTURE views in schema foursquare.main to role foursquare_read; 
GRANT IMPORTED PRIVILEGES on database foursquare_nyc to role foursquare_read;
GRANT usage on warehouse [YOUR_WAREHOUSE_NAME] to role foursquare_read;


3. Set the Streamlit user's default role and warehouse to be the assigned role and warehouse.

With a Streamlit user set up, let's work on connecting Streamlit to Snowflake.

Step 2. Write backend functions to get data from Snowflake

In this step, we'll write the backend functions required to fetch data from the Snowflake database.

🔴
You can find all the backend functions for getting Snowflake data here. To install the latest Python Connector for Snowflake, follow the instructions here. To learn how to connect Streamlit to Snowflake, go here.
1. Import the necessary packages:
import snowflake.connector
from snowflake.connector import DictCursor
import streamlit as st

2. Create base functions to connect to Snowflake and execute Snowflake queries:
def _init_connection():
   return snowflake.connector.connect(**st.secrets["snowflake"])

@st.cache_data(ttl=10, show_spinner=False)
def _run_query(query_str):
   with _init_connection() as conn:
       with conn.cursor(DictCursor) as cur:
           cur.execute(query_str)
           return cur.fetchall()



The _init_connection function utilizes the snowflake-connector-python library and Streamlit's secret management to connect to Snowflake. The _run_query function establishes a new connection to Snowflake, executes a query, and returns the results. The DictCursor returns column names alongside the data. It also uses st.cache_data to avoid re-running the same queries within 10 seconds.

3. Create functions to get boroughs and neighborhoods within a borough:
def get_boroughs():
   sql = """SELECT * FROM borough_lookup"""
   boroughs = _run_query(sql)
   return boroughs

def get_neighborhoods(borough_name):
   sql = """
   SELECT n.*
   FROM neighborhood_lookup n
   INNER JOIN borough_neighborhood bn ON n.id = bn.neighborhood_id
   INNER JOIN borough_lookup b ON bn.borough_id = b.id
   WHERE b.name IN ('{0}')
   ORDER BY b.name, n.name
   """.format(borough_name)
   neighborhoods = _run_query(sql)
   return neighborhoods

4. Create a function to get a list of categories that semantically match the user's query:
def get_categories(search_embeddings):
   sql = """
   WITH base_search AS (
   SELECT '{0}' embedding
   )
   , search_emb AS (
   SELECT
       n.index
       , n.value
   from base_search l
   , table(flatten(input => parse_json(l.embedding))) n
   ORDER BY n.index
   )
   , search_emb_sqr AS (
   SELECT index, value, value*value value_sqr 
   FROM search_emb r
   )
   , result AS (
   SELECT
       v.category_id
       , SUM(s.value * v.value) / SQRT(SUM(s.value * s.value) * SUM(v.value * v.value)) cosine_similarity
   FROM search_emb_sqr s
   INNER JOIN category_embed_value v ON s.index = v.index
   GROUP BY v.category_id
   ORDER BY cosine_similarity DESC
   LIMIT 5
   )
   SELECT c.category, r.cosine_similarity
   FROM result r
   INNER JOIN category_lookup c ON r.category_id = c.category_id
   WHERE r.cosine_similarity > 0.81
   ORDER BY r.cosine_similarity DESC
   """.format(search_embeddings)

   recommended_categories = _run_query(sql)

   return recommended_categories


This query builds upon the cosine similarity query discussed in Part 1. It takes user query embeddings as input and performs a final lookup to return the category names.

5. Create a function to get places within specific categories and in a list of neighborhoods:
def get_places(borough_name, neighborhood_list, category_list):
   sql = """
   WITH base_neighborhoods AS (
       SELECT n.id 
       FROM borough_lookup b
       INNER JOIN borough_neighborhood bn on b.id = bn.borough_id
       INNER JOIN neighborhood_lookup n ON bn.neighborhood_id = n.id
       WHERE b.name = '{0}'
       AND n.name IN ({1})
   )
   , neighborhood_places AS (
       SELECT pn.fsq_id
       FROM place_neighborhood pn
       WHERE pn.neighborhood_id IN (SELECT id FROM base_neighborhoods)
       ORDER BY pn.fsq_id
   )
   , base_categories AS (
       SELECT c.category_id
       FROM category_lookup c
       WHERE c.category IN ({2})
   )
   , category_places AS (
       SELECT pc.fsq_id
       FROM category_place pc
       WHERE pc.category_id IN (SELECT category_id FROM base_categories)
       ORDER BY pc.fsq_id
   )
   , places AS (
       SELECT
           fsq_id
           , name
           , latitude
           , longitude
           , concat(COALESCE(address,''), COALESCE(address_extended,'')) address 
           , fsq_category_labels
           , n1.value::string category
       FROM place_lookup l
       , table(flatten(l.fsq_category_labels)) n
       , table(flatten(n.value)) n1
       WHERE fsq_id IN (
           SELECT fsq_id FROM neighborhood_places
           INTERSECT
           SELECT fsq_id FROM category_places
       )
       AND latitude IS NOT NULL
       AND longitude IS NOT NULL
       QUALIFY row_number() OVER (PARTITION BY fsq_id, n.seq, n.index, n1.seq ORDER BY n1.index DESC) = 1
   )
   SELECT
       fsq_id
       , ANY_VALUE(name) name
       , ANY_VALUE(latitude) latitude
       , ANY_VALUE(longitude) longitude
       , ANY_VALUE(address) address
       , listagg(category, ', ') categories
   FROM places
   GROUP BY fsq_id
   ORDER BY fsq_id    
   """.format(borough_name, _list_to_str(neighborhood_list), _list_to_str(category_list))
   places = _run_query(sql)
   return places


In the query above, we first filter the venues by narrowing them down to specific categories within a list of neighborhoods and venues. Next, we intersect these two lists to produce a final list of venues. For UI display purposes, we extract the latitude, longitude, street address, and leaf categories of each venue. The QUALIFY statement extracts the last category in the inner list of each category list found in the fsq_category_labels column.

🔴
Foursquare stores category labels in a list of lists. The outer list is the list of categories. The inner list describes the hierarchy of the category, where the first element represents the root category and the last element represents the leaf category.
Step 3: Implement OpenAI endpoints

Our Streamlit app uses OpenAI's Python SDK to create embeddings of user queries and to moderate user queries (to ensure user queries don't violate OpenAI's Content Policy).

🔴
All backend functions calling OpenAI APIs can be found here.
import openai
import streamlit as st

openai.api_key = st.secrets['openai']['api_key']

def get_embedding(category_str):
   try:
       response = openai.Embedding.create(
           input=category_str,
           model="text-embedding-ada-002"
       )
       embeddings = response['data'][0]['embedding']
       return embeddings
  
   except Exception as e:
       raise e

def get_moderation(user_query):
   try:
       moderation = openai.Moderation.create(
           input=user_query
       )
       moderation_result = moderation['results'][0]
       flagged_categories = [category for category, value in moderation_result['categories'].items() if value]
       return {'flagged': moderation_result['flagged'], 'flagged_categories':flagged_categories}
  
   except Exception as e:
       raise e


Exceptions are caught and then passed to the front-end application for the sake of simplicity.

Step 4. Write the frontend Streamlit app

Now that we have our backend functions in place, let's move on to creating the front-end Streamlit app that users will interact with.

When I started developing the Streamlit app, I used the typical nested if-else scripting approach, where UI and backend operations were combined. But I quickly found it difficult to track what would cause parts of the UI to re-render. To address this, I eventually settled on the following pattern that allows for better control over app refreshes:

Create functions to group UI elements
Create UI element handlers to change session state variables and make backend calls
Use session state variables to maintain user selections, control UI renderings, and avoid unnecessary calls to backend functions

By using session state variables and handlers, we ensure only the affected parts of the application are refreshed, avoiding full-page reloads or unnecessary backend calls. As a result, we can create a more efficient and responsive app. The lightweight structure also contains less overhead than an object-oriented approach to developing Streamlit apps.

In the rest of this section, we'll use these patterns to develop our Streamlit app.

🔴
You can find the complete Streamlit app code here.
1. Import the necessary libraries and backend functions:
import streamlit as st
import api_snowflake as api
import api_openai as oai

2. Set the page configuration:
st.set_page_config(page_title="NYC Venue Search", layout="wide", initial_sidebar_state="expanded")

3. Create a function to render the call-to-action (CTA) links:
def render_cta_link(url, label, font_awesome_icon):
   st.markdown('<link rel="stylesheet" href="<https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css>">', unsafe_allow_html=True)
   button_code = f'''<a href="{url}" target=_blank><i class="fa {font_awesome_icon}"></i> {label}</a>'''
   return st.markdown(button_code, unsafe_allow_html=True)


This function uses the Markdown element to display clickable links with Font Awesome icons.

4. Create a function to lay out the search options in the sidebar:
def render_search():
   """
   Render the search form in the sidebar.
   """
   search_disabled = True
   with st.sidebar:
       st.selectbox(label=borough_search_header, options=(
           [b['NAME'] for b in boroughs]), index=2, key="borough_selection", on_change=handler_load_neighborhoods)

       if "neighborhood_list" in st.session_state and len(st.session_state.neighborhood_list) > 0:
           st.multiselect(label=neighborhood_search_header, options=(
               st.session_state.neighborhood_list), key="neighborhoods_selection", max_selections=5)

       st.text_input(label=semantic_search_header,
                     placeholder=semantic_search_placeholder, key="user_category_query")

       if "borough_selection" in st.session_state and st.session_state.borough_selection != "" \\
               and "neighborhoods_selection" in st.session_state and len(st.session_state.neighborhoods_selection) > 0  \\
               and "user_category_query" in st.session_state and st.session_state.user_category_query != "":
           search_disabled = False

       st.button(label=search_label, key="location_search",
                 disabled=search_disabled, on_click=handler_search_venues)

       st.write("---")
       render_cta_link(url="<YOUR TWITTER HANDLE URL>",
                       label="Let's connect", font_awesome_icon="fa-twitter")
       render_cta_link(url="<YOUR LINKEDIN PROFILE URL",
                       label="Let's connect", font_awesome_icon="fa-linkedin")


The sidebar of the application has four primary UI elements: a select box to choose a borough, a multi-select box to choose neighborhoods, a text input for user queries, and a search button to initiate a venue search. We can store user data across application runs by assigning keys to these UI elements. Streamlit's session state documentation states that every UI widget with a key is automatically added to the session state:

The borough_selection selectbox widget stores users' borough selections. When the selection changes, the handler_load_neighborhoods function fetches the list of neighborhoods in the selected borough and stores it in the neighborhood_list session state variable.
The neighborhoods_selection multiselect widget stores users' neighborhood selections.
The location_search button widget is disabled until users select a borough, then a list of neighborhoods, and enter a search query. When clicked, the handler_search_venues function handles the embedding of user queries, searches for semantically similar categories, and finds venues within those categories in the specified neighborhoods.
5. Create the function to render the search results:
def render_search_result():
    """
    Render the search results on the main content area.
    """
    col1, col2 = st.columns([1,2])
    col1.write(category_list_header)
    col1.table(st.session_state.suggested_categories)
    col2.write(f"Found {len(st.session_state.suggested_places)} venues.")
    if (len(st.session_state.suggested_places) > 0):
        col2.map(st.session_state.suggested_places, zoom=13, use_container_width=True)
        st.write(venue_list_header)
        st.dataframe(data=st.session_state.suggested_places, use_container_width=True)


This function renders the suggested categories (stored in suggested_categories session state variable) and the recommended venues (stored in suggested_places session state variable) on the map. It also renders the list of venues in a dataframe.

Next, we'll move on to the handler functions.

6. Create the handler function to load neighborhoods:
def handler_load_neighborhoods():
   """
   Load neighborhoods for the selected borough and update session state.
   """
   selected_borough = 'Manhattan'
   if "borough_selection" in st.session_state and st.session_state.borough_selection != "":
       selected_borough = st.session_state.borough_selection
   neighborhoods = api.get_neighborhoods(selected_borough)
   st.session_state.neighborhood_list = [n['NAME'] for n in neighborhoods]


This function receives a list of neighborhoods in a borough. The borough is set to Manhattan by default but is overwritten by the user's selection (stored in the borough_selection session state variable).

The function is called whenever the user selects a new borough from the borough_selection dropdown. It is also manually called when the application first runs (so that we can preload neighborhoods in Manhattan).

7. Create the handler function to handler venue search:
def handler_search_venues():
   """
   Search for venues based on user query and update session state with results.
   """
   try:
       moderation_result = oai.get_moderation(st.session_state.user_category_query)
       if moderation_result['flagged'] == True:
           flagged_categories_str = ", ".join(moderation_result['flagged_categories'])
           st.error(f"⚠️ Your query was flagged by OpenAI's content moderation endpoint for: {flagged_categories_str}.  \
  \
Please try a different query.")
       else:
           embeddings = oai.get_embedding(st.session_state.user_category_query)
           st.session_state.suggested_categories = api.get_categories(embeddings)

           if len(st.session_state.suggested_categories) > 0 and len(st.session_state.neighborhoods_selection) > 0:
               category_list = [s['CATEGORY'] for s in st.session_state.suggested_categories]

               st.session_state.suggested_places = api.get_places(
                   st.session_state.borough_selection,
                   st.session_state.neighborhoods_selection,
                   category_list)
           else:
               st.warning("No suggested categories found. Try a different search.")
   except Exception as e:
       st.error(f"{str(e)}")


This function is responsible for the bulk of the application logic. It's triggered whenever users click on the location_search button widget. The following steps are carried out:

For safety reasons, it checks the user's query against OpenAI's moderation endpoint
It uses OpenAI's Embeddings API to embed the user's query
It retrieves the list of semantically similar categories from Snowflake and stores it in the suggested_categories session state variable
It retrieves the list of venues within the suggested categories in the selected neighborhoods from Snowflake. The suggested_places session state variable stores the final list of places

The function also handles the following edge cases:

If OpenAI's moderation endpoint flags the user query, an error message is displayed
If no categories are semantically similar to the user query, a warning message is displayed
Any other exception message is displayed as an error message

With the UI element group functions and handler functions defined, the rest of the application can now be wired up.

8. Control UI renders with sessions state variables:
boroughs = [{'NAME':'Brooklyn'},{'NAME':'Bronx'},{'NAME':'Manhattan'},{'NAME':'Queens'},{'NAME':'Staten Island'}]

if "selected_borough" not in st.session_state:
   st.session_state.selected_borough = "Manhattan"

if "neighborhood_list" not in st.session_state:
   handler_load_neighborhoods()
render_search()

st.title(page_title)
st.write(page_helper)
st.write("---")

if "suggested_places" not in st.session_state:
   st.write(empty_search_helper)
else:
   render_search_result()


The list of NYC boroughs is hard-coded to eliminate an unnecessary Snowflake query. When the application loads for the first time, the selected borough is set to Manhattan. If users have not chosen any neighborhoods, the handler_load_neighborhoods function is called to fetch a list of Manhattan neighborhoods. The search bar is then displayed. Finally, the empty search helper text or search results are displayed based on the presence of suggested_places as a session state variable.

Wrapping up

In this two-part blog series, we successfully built a semantic location search application using Streamlit, Snowflake, OpenAI, and Foursquare's free NYC venue data. In the first part, we focused on building a Snowflake-powered semantic search engine. In this second part, we covered essential steps such as limiting the application's access to Snowflake, connecting Streamlit to Snowflake, writing optimized backend queries, implementing OpenAI endpoints, and wiring up the Streamlit application.

Given additional time (and data), I'd add the following enhancements:

Order the recommended venues by popularity scores (Foursquare didn't make this available in their dataset).
Call Foursquare's venue endpoints to display recent tips and photos for each venue.

I hope you enjoyed the article. Connect with me on Twitter or LinkedIn. I'd love to hear from you.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

Also in Snowflake powered ❄️...

View even more →

Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit in Snowflake ❄️
https://blog.streamlit.io/tag/snowflake-powered/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Snowflake powered ❄️
9 posts
Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Monte Carlo simulations with Streamlit

Learn how to predict future stock prices

Snowflake powered ❄️
by
Mats Stellwall
,
June 8 2023
Semantic search, Part 2: Building a local search app

Making an app with Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data from Snowflake Marketplace

Snowflake powered ❄️
by
Dave Lin
,
May 18 2023
Semantic search, Part 1: Implementing cosine similarity

Wrangling Foursquare data and implementing semantic search in Snowflake

Snowflake powered ❄️
by
Dave Lin
,
May 17 2023
Streamlit wizard and custom animated spinner

Improve user experience with simplified data entry and step-by-step guidance

Snowflake powered ❄️
by
Andrew Carson
,
May 15 2023
Build a Snowflake DATA LOADER on Streamlit in only 5 minutes

Drag and drop your Excel data to Snowflake with a Streamlit app

Snowflake powered ❄️
by
Sasha Mitrovich
,
May 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Carlos D Serrano - Streamlit
https://blog.streamlit.io/author/carlos/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Carlos D Serrano
1 post
Drill-downs and filtering with Streamlit and Altair

Display an Altair chart definition in Streamlit using the st.altair_chart widget

Advocate Posts
by
Carlos D Serrano
,
July 12 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mark Needham - Streamlit
https://blog.streamlit.io/author/mark/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mark Needham
1 post
Display a race on a live map 🏃

Create a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library

Advocate Posts
by
Mark Needham
,
June 22 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

generate-openai-apikey.gif (807×614)
https://blog.streamlit.io/content/images/2023/05/generate-openai-apikey.gif#browser


Host your Streamlit app for free
https://blog.streamlit.io/host-your-streamlit-app-for-free/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Host your Streamlit app for free

Learn how to transfer your apps from paid platforms to Streamlit Community Cloud

By Chanin Nantasenamat
Posted in Tutorials, January 24 2023
Why deploy your apps to the internet?
Why use Community Cloud?
Step 1. Create a simple Streamlit app
Step 2. Set up an account on Community Cloud
Step 3. Connect your account to GitHub
Step 4. Create a GitHub repo of your app
Step 5. Deploy your app in a few clicks
Wrapping up
Contents
Share this post
← All posts

If you have a Streamlit app but don’t want to pay a monthly fee to host it on a commercial cloud platform, one option is to migrate it to Streamlit Community Cloud. It’s FREE!

In this post, I’ll show you how to build a demo app and deploy it to Community Cloud step-by-step:

Step 1. Create a simple Streamlit app
Step 2. Set up an account on Community Cloud
Step 3. Connect your account to GitHub
Step 4. Create a GitHub repo of your app
Step 5. Deploy your app in a few clicks

Can’t wait to see it in action? Here's the demo app and the repo code.

But before we get to the fun stuff, let’s talk about…

Why deploy your apps to the internet?

Deploying your apps to the internet allows users to access them from a web browser—without having to set up a coding environment and installing dependencies.

You have two options:

Manually set up a virtual private server for deploying your app.
Host the app in a GitHub repository and deploy it to a cloud platform.

The first option gives you full control. You can set up everything locally. But it can take time—for both the setup and the maintenance (like keeping OS up-to-date, etc.).

The second option is the simplest. Just push your app to GitHub. If it’s properly configured with a cloud platform, it’ll automatically update your code changes.

Why use Community Cloud?

Here’s why you might want to use Community Cloud to host your apps:

Advantages	Description
Free	You can deploy Streamlit apps for free!
Deploy in one click	Your fully hosted app is ready to be shared in under a minute.
Keep your code in your repo	No changes to your development process. Your code stays on GitHub.
Live updates	Your apps update instantly when you push code changes.
Securely connect to data	Connect to all your data sources using secure protocols.
Restrict access to apps	Authenticate viewers with per-app viewer allow-lists.
Easily manage your apps	View, collaborate, and manage all your apps in a single place.
Step 1. Create a simple Streamlit app

First, let’s make a simple app that prints out Hello world! . It takes only two lines of code (for a deeper dive, read this post).

Go ahead and create a streamlit_app.py file:

import streamlit as st

st.write('Hello world!')

Step 2. Set up an account on Community Cloud

Go to Community Cloud and click “Sign up” to create a free account with your existing Google, GitHub, or email account:

Next, enter your GitHub credentials and click on “Authorize streamlit” to let Streamlit access your GitHub account:

Finally, enter your information and click “Continue”:

Congratulations! You have signed up for your workspace in Community Cloud.

Step 3. Connect your account to GitHub

There are two options to connect your Community Cloud account to GitHub:

Option  1

Click on “New app”:

On the authorization page, click on “Authorize streamlit."

Option 2

Click “Settings,” then “Linked accounts,” then “Allow access”:

This will let Community Cloud deploy your Streamlit apps from your GitHub repositories. On the authorization page, click “Authorize streamlit."

GitHub-linked account

Once you log in, Community Cloud will get access to your GitHub account:

Now you’re ready to deploy Streamlit apps!

But first, let’s create a GitHub repo.

Step 4. Create a GitHub repo of your app

Click “+” and then “New repository”:

This will bring you to the “Create a new repository” page.

In the field “Repository name” type in st-hello-world, click “Public,” check “Add a README file” (your new repo will get a README.md file), and click “Create repository”:

Your repo is all set up!

Now create your app file:

Next, create the streamlit_app.py:

Your GitHub repo will be populated with README.md and streamlit_app.py files.

Next, head back to Community Cloud:

Step 5. Deploy your app in a few clicks

At last, here comes the fun part. You get to deploy your app!

Click “New app" and fill out the information for your app:

This will spin up a new server. You’ll see the message, “Your app is in the oven.”

In the bottom right-hand corner, click “Manage app” to see the log messages (use them for debugging and troubleshooting errors):

The side menu displays all log messages in real-time:

Once your app finishes compiling, you’ll see the output. In our example, it’ll be a simple message: Hello world!

Wrapping up

Congratulations! You have successfully deployed your app to Streamlit Community Cloud. Now you can share the app URL with the community.

If a tutorial video is your thing, check out the following video:

Read more about:

Self-hosting Streamlit apps on your own servers (AWS, Azure, etc.).
Different Streamlit use cases from the community.

If you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Llama2-chatbot-screencast_scaling-0.5_fps-15_speed-10.0_duration-0-48.gif (1165×795)
https://blog.streamlit.io/content/images/2023/07/Llama2-chatbot-screencast_scaling-0.5_fps-15_speed-10.0_duration-0-48.gif


Collecting user feedback on ML in Streamlit
https://blog.streamlit.io/collecting-user-feedback-on-ml-in-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Collecting user feedback on ML in Streamlit

Improve user engagement and model quality with the new Trubrics feedback component

By Jeff Kayne and Joel Hodgson
Posted in Advocate Posts, May 4 2023
How to pip install the FeedbackCollector from the trubrics-sdk
How to get started with just 3 lines of code [Beginner]
How to collect complex feedback [Advanced]
How to manage your feedback in the Trubrics platform [Optional]
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

I'm Jeff, co-founder of Trubrics. We build tools to help data scientists collect user feedback on machine learning (ML). We've developed a new component that enables you to do just that with a few lines of code in your Streamlit app!

Why collect user feedback on ML?

Improving model performance. User feedback provides insights into the strengths and weaknesses of the model from the user's perspective. This information enables fixes and fine-tuning of the ML model, ultimately improving its performance.
Enhancing user experience. User feedback can help identify user preferences and pain points. This information can be used to design better user interfaces, improve model usability, and ultimately enhance the user experience.
Increasing user engagement. Collecting user feedback shows that you care about your users' opinions and are committed to providing the best possible experience. This can increase user engagement, satisfaction, adoption, and loyalty.
Growing responsibility of the ML group. Adding diversity and cross-functional teams to the ML review process helps us strive towards building #responsibleAI and #ethicalAI.
Continuous model improvement and monitoring. Collecting user feedback in production can help you improve your ML model by incorporating bug fixes, user needs, and preferences.

⚠️
This blog post refers to a previous version of our component trubrics-sdk==1.3.6 . For an updated blog, please see here.

In this post, you'll learn:

How to pip install the FeedbackCollector from the trubrics-sdk
How to get started with just 3 lines of code [Beginner]
How to collect complex feedback [Advanced]
How to manage your feedback in the Trubrics platform [Optional]
👉
Can't wait to try it? Here are the links to our demo app, repo, and docs.
How to pip install the FeedbackCollector from the trubrics-sdk

Trubrics embeds some Streamlit components, so you'll need to install both libraries:

pip install "trubrics[streamlit]"==1.3.6


By the way, Trubrics is tested for Python versions 3.7, 3.8, and 3.9.

How to get started with just 3 lines of code [Beginner]

Add these three lines of code to your Streamlit app/a blank Python script (if you're just getting started):

from trubrics.integrations.streamlit import FeedbackCollector

collector = FeedbackCollector()
collector.st_feedback(feedback_type="issue")


What's going on here? Let's break it down:

The FeedbackCollector object provides feedback functionality and static metadata about your app (e.g., model version). Create an instance of this object once at the top of your app.
The st_feedback() method allows you to embed feedback components in your app and save different types of feedback to .json files. Use multiple instances of this method to add different feedback collection points around your app.

Now you can launch your app with streamlit run basic_app.py !

You'll see a feedback component in your app that will let you start collecting qualitative feedback from your users:

Try other types of quantitative feedback collection, such as:

collector.st_feedback(feedback_type="faces")


feedback = collector.st_feedback(
	feedback_type="thumbs",
	path="thumbs_feedback.json"
)

# print out the feedback object as a dictionary in your app
feedback.dict() if feedback else None


The st_feedback() method returns a feedback object that can be manipulated (in this case, we're printing it out in the app). And it saves a feedback .json to a specified path, such as path="thumbs_feedback.json". To save multiple files, use a dynamic path, such as path=f"thumbs_{timestamp}.json" with a time stamp.

To learn more about the different feedback types and their available options, read our docs.

How to collect complex feedback [Advanced]

You can use the Trubrics FeedbackCollector to collect more complex feedback with type="custom". This is super useful for collecting forms or survey responses with multiple questions:

import streamlit as st
from trubrics.integrations.streamlit import FeedbackCollector
collector = FeedbackCollector()
q1 = st.text_input("Q 1")
q2 = st.text_input("Q 2")
q3 = st.text_input("Q 3")
if q1 and q2 and q3:
    button = st.button(label="submit")
    if button:
        feedback = collector.st_feedback(
            "custom",
            user_response={
                "Q 1": q1,
                "Q 2": q2,
                "Q 3": q3,
            },
            path="./feedback.json",
        )
        feedback.dict() if feedback else None


This lets you use any Streamlit components to create the feedback form of your choice.

👉
TIP: For creating more robust forms in Streamlit, check out st.form.

You can use Trubrics' "faces" and "thumbs" feedback UI components in your custom feedback forms, as shown here.

How to manage your feedback in the Trubrics platform [Optional]

To manage and collaborate on feedback issues more effectively, we offer functionality that enables you to save feedback directly to our platform from your Streamlit app. This also allows users to authenticate within the apps and track who has recorded what feedback:

Wrapping up

And...here is the final app!

Thank you for reading our story. We hope that you're now armed and ready to start collecting feedback on your ML projects. Please try out our component and let us know any feedback. And if you have any questions or ideas, we'd be very happy to hear from you. Drop us a message in the comments below, or contact us on GitHub, LinkedIn, or via email.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

github-repo.png (1899×939)
https://blog.streamlit.io/content/images/2023/08/github-repo.png#browser


confusion-matrix.png (961×938)
https://blog.streamlit.io/content/images/2023/08/confusion-matrix.png#border


model-training-performance-graph.png (1144×499)
https://blog.streamlit.io/content/images/2023/08/model-training-performance-graph.png#border


app-overview.png (1512×717)
https://blog.streamlit.io/content/images/2023/08/app-overview.png#border


How to master Streamlit for data science
https://blog.streamlit.io/how-to-master-streamlit-for-data-science/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to master Streamlit for data science

The essential Streamlit for all your data science needs

By Chanin Nantasenamat
Posted in Tutorials, January 18 2022
The essential Streamlit for all your data science needs
1. Getting up to speed
1.1. Why deploy models?
1.2. Types of model deployment
1.3. Traditional vs. low-code web frameworks
2. Streamlit 101
2.1. What you need to use Streamlit
2.2. Four Streamlit design principles
3. How to set up your Streamlit workspace
3.1. Install Streamlit
3.2. Code a Streamlit app
3.3. Launch your Streamlit apps
3.4. Create a conda environment
4. Practice by building the “Hello, World!” app
5. Build your Own Streamlit App
5.1. Elements of a Streamlit app
5.2. The “Brains” of the Streamlit app
5.3. Lay out the app
6. Deploy your Streamlit app
7. Resources
7.1. Documentation
7.2. Discussion forum
7.3. YouTube tutorials
7.4. Books
Wrapping up
Contents
Share this post
← All posts
The essential Streamlit for all your data science needs

To build a web app you’d typically use such Python web frameworks as Django and Flask. But the steep learning curve and the big time investment for implementing these apps present a major hurdle.

Streamlit makes the app creation process as simple as writing Python scripts!

In this article, you’ll learn how to master Streamlit when getting started with data science.

Let’s dive in!

1. Getting up to speed
1.1. Why deploy models?

The data science process boils down to converting data to knowledge/insights while summarizing the conversion with the CRISP-DM and OSEMN data frameworks. Aside from knowledge/insights, a data project can make a greater impact if you deploy your machine learning models as web apps.

Why? Because deployed models can be accessed by stakeholders who can play with them and figure out what works and what doesn't.

1.2. Types of model deployment

Model deployment is the endpoint of a data science workflow. Models can be deployed as:

Jupyter notebooks
API
Web apps

Jupyter notebooks. Jupyter notebooks are commonly used for prototyping the data science workflow and they can be:

Uploaded to GitHub
Shared as a link via Google Colab
Shared via Binder

API. Models can also be deployed as a REST API using tools such as FastAPI. This approach does not have a frontend for displaying it graphically for ease of use.

Web apps. This brings us to deploying machine learning models as web applications. The traditional approach is to wrap the API via the use of web frameworks such as Django and Flask. A much simpler approach is to use a low-code solution such as Streamlit to create a web app.

Let’s explore this in more depth in the following section.

1.3. Traditional vs. low-code web frameworks

Though Django and Flask may be the gold standard for developing web apps, the technical barriers limit their usage by the wider data community. Low-code solutions such as Streamlit have lowered the barrier to entry by enabling data enthusiasts (e.g. data scientists, analysts, and hobbyists) to easily convert machine learning models into interactive data-driven web apps.

Here are the low-code solutions:

2. Streamlit 101

Streamlit is a Python library you can use to build interactive data-driven web apps.

2.1. What you need to use Streamlit

To use Streamlit, you need to:

Have basic Python knowledge.
Write scripts to perform specific tasks (like taking several Excel files as input and combining them into one).
Build and grow the Streamlit app line by line instead of starting with a predefined layout (it takes only a few lines of code).

If you can do all this, congratulations! You're ready to plunge into the world of Streamlit.

2.2. Four Streamlit design principles

According to Adrien Treuille, co-founder and CEO of Streamlit, Streamlit was originally based on three principles (as mentioned in the 2019 PyData LA talk and the Medium launch post). The fourth principle was introduced at the launch of Streamlit Cloud:

Embrace Python scripting. Build and grow Streamlit apps line by line.
Treat widgets as variables. Widgets are input elements that let users interact with Streamlit apps. They’re presented as basic input text boxes, checkboxes, slider bars, etc.
Reuse data and computation. Historically, data and computations had been cached with the @st.cache decorator. This saves computational time for app changes. It can be hundreds of times if you actively revise the app! In 0.89.0 release Streamlit launched two new primitives (st.experimental_memo and st.experimental_singleton) to afford a significant speed improvement to that of @st.cache.
Deploy instantly. Easily and instantly deploy apps with Streamlit Cloud.
3. How to set up your Streamlit workspace
3.1. Install Streamlit

Install the Streamlit library by using pip:

pip install streamlit

3.2. Code a Streamlit app

After doing so, you can start to code an app by creating a Python script file (e.g. app.py). Inside this file, you can import the Streamlit library via import streamlit as st and use any of the available Streamlit functions.

3.3. Launch your Streamlit apps

Once the app has been coded, launching it is as easy as running streamlit run app.py.

For first time users, you can also type  streamlit hello  into the command line to see Streamlit in action.

Go ahead and give it a try!

3.4. Create a conda environment

As I show on my YouTube channel, I like to house my Streamlit apps in a dedicated conda environment. This way the library dependencies don’t get entangled with my other Python libraries. I recommend you do the same.

Begin by building an EDA app. Read how I created a dedicated conda environment in the GitHub repo’s readme.md file, then watch this tutorial video on How to Build an EDA app using Pandas Profiling and follow these steps:

Step 1. Create a conda environment called eda:

conda create -n eda python=3.7.9


Step 2. Activate the eda environment:

conda activate eda


Step 3. Install prerequisite libraries by downloading the requirements.txt file (it contains the library version numbers):

wget https://raw.githubusercontent.com/dataprofessor/eda-app/main/requirements.txt


Step 4. Install libraries via pip:

pip install -r requirements.txt


Step 5. Download and unzip contents from the GitHub repo: https://github.com/dataprofessor/eda-app/archive/main.zip

Step 6. Launch the app:

streamlit run app.py


You’ll see the web app browser pop up:

The functionality of this EDA app leverages the capabilities of pandas-profiling:

Congratulations! You now know how to clone a Streamlit app from a GitHub repo, setup a dedicated conda environment, and successfully launch the app!

Next, customize the app to your own liking.

4. Practice by building the “Hello, World!” app

Now that you know Streamlit principles, let’s build some apps. It’s not as hard as you may think. A typical rite of passage for learning any new programming language is to start with printing Hello World!.

Here is how to do it in Streamlit in four easy steps:

Step 1. Launch your favorite code editor (Atom.io, VS Code, etc.).

Step 2. Create a file and name it app.py.

Step 3. Add this code into the app.py file:

import streamlit as st

st.write('Hello world!')


Step 4. Launch the app by typing this into the command line:

streamlit run app.py

5. Build your Own Streamlit App

I like to start my Streamlit projects by coding the “brains” of the app on Google Colab.

At a high level, a web app is comprised of three key elements:

Input. Widgets make it possible to take in user input. They can be sliders, text/number boxes, file upload widgets, etc.
The “brains” of the app. The brains or the inner workings of the app is what differentiates one app from another. It performs the function of transforming user inputs into outputs.
Output. This can be anything: DataFrame printouts, images, plots, numerical values, text, or embeddings of audio, videos, and tweets.

Because web apps adopt a similar structure, some of the elements used in one project can be repurposed and reused in the next project.

5.1. Elements of a Streamlit app
Contents (text, images, embedded videos, audio, tweets, etc.)
Widgets
Auxillaries (balloons, code box, etc.)
5.2. The “Brains” of the Streamlit app
Use Streamlit components
Make use of existing functions from Python libraries of interest
Code your own custom function
5.3. Lay out the app

I like to put widgets on the left-hand sidebars of the app. Use st.sidebar. in front of any widget functions of interest (instead of st.). For example, to place the text input box in the sidebar, use st.sidebar.text_input(‘Name’) instead of st.text_input(‘Name’) (which would place the text input box into the main panel).

6. Deploy your Streamlit app

Now that you’ve built a Streamlit app, deploy it to the cloud for general public access. The easiest way to do this is with Streamlit Cloud:

Create a GitHub repo of the app files (app.py, requirements.txt, and dependency files)
In Streamlit Cloud, link your GitHub account and select the app’s repo to deploy.

You can also watch this video on how to deploy Streamlit apps.

7. Resources
7.1. Documentation

Streamlit’s documentation website is the best place to get you started:

Streamlit library. A guide on how to build Streamlit apps with various Streamlit functions (the API reference), “Get started” tutorials, and a cheat sheet.
Streamlit Cloud. Everything you need to know on how to deploy apps to Streamlit Cloud.
Knowledge base. A growing collection of tutorial articles and FAQs about using Streamlit and troubleshooting problems.
7.2. Discussion forum

Can’t find the information on the documentation website? Check out the Streamlit discussion forum and read the troubleshooting guide for tips on resolving problems.

7.3. YouTube tutorials

Here’s a list of these YouTube channels about Streamlit:

Streamlit. The official Streamlit YouTube channel with official announcements of the latest features.
Data Professor. My YouTube channel with videos on data science and bioinformatics and a growing playlist of 30 videos on Streamlit.
JCharisTech. Jesse’s YouTube channel with tutorial videos on Python and Streamlit and with a playlist of almost 70 videos on Streamlit.
1littlecoder. AbdulMajed’s YouTube channel with tutorial videos on Python and Streamlit and with a playlist of almost 20 videos on Streamlit.
7.4. Books

Tyler Richards wrote a book titled Getting Started with Streamlit for Data Science: Create and Deploy Streamlit Web Applications from Scratch in Python. It takes readers on a journey of how they can build interactive data-driven apps. The last chapter was super-fun to read as Tyler interviewed Streamlit power users.

I also interviewed Tyler in the hour-long podcast Data Science Podcast with Tyler Richards - Facebook Data Scientist. We talked about his journey into data science, his experience as a data scientist, and his thoughts and inspiration for writing a book about Streamlit.

Wrapping up

You’ve learned Streamlit essentials that will start you on building your own interactive data-driven Python apps. Well done! Of course, there’s always more to learn. Feel free to drop a comment or a suggestion below on the topics that you'd like to learn more about.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

30 Days of Streamlit
https://blog.streamlit.io/30-days-of-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
30 Days of Streamlit

A fun challenge to learn and practice using Streamlit

By Chanin Nantasenamat
Posted in Advocate Posts, April 1 2022
What is 30 days of Streamlit?
Content
How to participate
Contents
Share this post
← All posts
What is 30 days of Streamlit?


We're kicking off #30DaysOfStreamlit—a social challenge for getting up to speed on building and deploying Streamlit apps. Each day a new challenge will be released for participants to tackle. Inspired by Ken Jee’s #66daysofdata, the #100daysofcode challenge, and Kaggle’s #30DaysofML, this initiative is a fun opportunity to learn, create, share, and earn.

👉 Check out the app to get started now!

Content


The challenge will move through three levels of difficulty: beginner, intermediate, and advanced.

📆 Days 1-7 will cover beginner tasks such as setting up a local and cloud coding environment, installing the Streamlit library, and building your first Streamlit Hello World app.

📆 Days 8-23 will cover more intermediate topics. Each day may highlight a Streamlit command to use for creating and deploying a simple Streamlit app (e.g., build a simple app that uses the st.download_button command).

📆 Days 24-30 will enter more advanced subjects. Learn about session states and things like efficient data and memory handling for Streamlit apps.

How to participate


Follow our daily posts on Twitter and LinkedIn or simply visit the 30 days app. Share your progress and creations using the #30DaysOfStreamlit hashtag.

🎈 Happy Streamlit-ing!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Quests: Getting started with Streamlit
https://blog.streamlit.io/streamlit-quests-getting-started-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit Quests: Getting started with Streamlit

The guided path for learning Streamlit

By Chanin Nantasenamat
Posted in Tutorials, November 18 2022
Two quests
Install Streamlit
Run the demo app
Create a single-page app
Use the Streamlit App Starter Kit
Read the Documentation
Complete the #30DaysOfStreamlit challenge
Get inspiration by exploring Streamlit apps in the Gallery
Use or create Streamlit Components
Create a multipage app
Use the Streamlit Multipage App Starter Kit
Deploy a Streamlit app on Streamlit Community Cloud
Share your Streamlit app
Get unstuck by asking the forum
Read our blog to stay updated on the latest developments
Wrapping up
Contents
Share this post
← All posts

Streamlit is a Python library that makes building beautiful, interactive apps in a few lines of code easy. But every new library has its quirks and conventions, and it takes time to learn. You might be wondering where to start and if there are any resources. No worries. I got you covered!

Let’s learn Streamlit with this guide called Streamlit Quests. It’s inspired by role-playing games where players navigate the landscape by completing a sequential set of tasks—quests.

Two quests

Your learning journey will have two quests:

👨‍💻 Expert Quest. This is a sequential track. Learning starts as easy and gradually becomes more complex.
🎈 Community Quest. This is a non-sequential track. You can refer to these resources at any point in your journey.

The topics checklist below serves a dual purpose: a table of contents and a to-do checklist that you can refer to at any time (think of it as your compass in your learning journey of Streamlit):

  ▢	 	Install Streamlit 👨‍💻
  ▢	 	Run the demo app via streamlit hello 👨‍💻
  ▢	 	Create a single-page app 👨‍💻
  ▢	 	Use the Streamlit App Starter Kit to quickly build a single-page app 🎈
  ▢	 	Read the Documentation for specific information on Streamlit commands at https://docs.streamlit.io 🎈
  ▢	 	Complete the #30DaysOfStreamlit challenge at https://30days.streamlit.app (the first three weeks are beginner-friendly, while the last week is more advanced) 👨‍💻
  ▢	 	Get inspiration by exploring Streamlit apps in the Gallery at https://streamlit.io/gallery 🎈
  ▢	 	Use or create Streamlit Components (third-party modules that extend Streamlit functionality) at https://streamlit.io/components 👨‍💻
  ▢	 	Explore hundreds of components from the Streamlit Components Hub 🎈
  ▢	 	Create a multipage app 👨‍💻
  ▢	 	Use the Streamlit Multipage App Starter Kit to quickly build a multipage app 🎈
  ▢	 	Deploy a Streamlit app on Streamlit Community Cloud at https://streamlit.io/cloud 👨‍💻
  ▢	 	Share your Streamlit app on Twitter/LinkedIn and tag us @streamlit 🎈
  ▢	 	Get unstuck by asking the forum at https://discuss.streamlit.io 🎈
  ▢	 	Read our blog to stay updated on the latest developments and use cases at https://blog.streamlit.io 🎈

👉
NOTE: Emojis at the end of each task mark tasks as part of the Expert or the Community Quest.

Let’s get started!

Install Streamlit

The simplest way to install Streamlit is by using pip. Just type the following into the command line:

pip install streamlit

Run the demo app

After installing Streamlit, run the demo app by typing the following into the command line:

streamlit hello


In a few moments, a new browser should launch, displaying the following demo app:

It showcases a wide range of Streamlit functionalities to show you what you can build.

Create a single-page app

In most cases, a basic app that performs a new task could be performed by a single-page app. Conceptually, the app has a single page that’s sitting in the streamlit_app.py file.

Learn how to build a single-page app in this article.

Or better yet…

Use the Streamlit App Starter Kit

Use the starter code from the Streamlit App Starter Kit to get a template app up and running in just a few minutes (learn how in this article).

Read the Documentation

In-depth coverage of every Streamlit command, along with code examples, is provided in the Streamlit Documentation at https://docs.streamlit.io. There are also Getting Started articles, cheat sheets, tutorials, and knowledge base articles. In addition to coverage of the Streamlit library, there’s also content on the Streamlit Community Cloud.

When building Streamlit apps, I keep the Documentation handy for quick and easy reference. I can always find a suitable Streamlit command or code examples to repurpose for my apps.

Complete the #30DaysOfStreamlit challenge

30 Days of Streamlit helps new users learn the Streamlit library. We launched it on April 1, 2022, releasing a new challenge daily (with three difficulty levels). Then we compiled them all into a public app https://30days.streamlitapp.com.

The app encourages you to share your progress with the community by posting it on Twitter or LinkedIn with the hashtag #30DaysOfStreamlit or by tagging @streamlit (so we can retweet it). It’s now available in Portuguese, French, Spanish, and Russian.

Want to help translate it into your language? Go to https://github.com/streamlit/30days to get started.

Get inspiration by exploring Streamlit apps in the Gallery

The Streamlit Gallery (available at https://streamlit.io/gallery) is a collection of the best apps built with our framework. Here you can find inspiration for your next app by browsing through the apps or learning how to build a particular type of app by looking at their code. The apps are categorized by topic: science and technology, finance and business, data visualization, etc.

Use or create Streamlit Components

Streamlit components are third-party modules that extend Streamlit’s functionality of Streamlit. A curated collection is provided at https://streamlit.io/components.

To use a Streamlit component such as AgGrid:

1.  Install via pip as follows:

pip install streamlit-aggrid


2.  Use in a Streamlit app by simply importing the component and using its function:

from st_aggrid import AgGrid

AgGrid(my_dataframe)


To create your own Streamlit component, refer to some of these excellent articles:

How to build your own Streamlit component: Learn how to make a component from scratch!
Introducing Streamlit Components: A new way to add and share custom functionality for Streamlit apps
Documentation on Custom Components (contains additional information on creating and publishing components as well as the components API)
Streamlit Components, security, and a five-month quest to ship a single line of code

And check out this 2-part tutorial video series:

How to build a Streamlit component - Part 1: Setup and architecture
How to build a Streamlit component - Part 2: Make a slider widget
Create a multipage app

A more complex app may require several pages. As a result, you might want to look into building a multipage app. The app consists of two major components:

The main page that serves as the entry point of the multipage app.
One of several pages that reside inside a pages folder is called upon when users click on the page of interest from the left sidebar panel.

Learn how to build a multipage app in this article.

Use the Streamlit Multipage App Starter Kit

Just like with the Streamlit App Starter Kit for single-page apps, check out the fully functional early version of the Streamlit Multipage App Starter (an article about this coming soon) to make a multipage app in no time.

Deploy a Streamlit app on Streamlit Community Cloud

Let’s say you’ve already built your Streamlit app and want to share it with the community. You can share it by using Streamlit Community Cloud at https://streamlit.io/cloud.

To deploy to the Community Cloud:

Upload or Git-push app files to a GitHub repository
From within Community Cloud, click on “New app,” then select repo, branch, and app file.

That’s it! Once the app is up and running, share its uniquely generated URL with the community.

Share your Streamlit app

Ready to share your Streamlit app creation with the community? Hop on Twitter or LinkedIn and tag us with @streamlit. It’s a great way to contribute to the community and receive helpful and constructive feedback for improving your app.

Get unstuck by asking the forum

Are you encountering errors when creating Streamlit apps? To get unstuck, try the following:

Read the error logs displayed in the command line as apps load. These errors will tell you exactly why certain aspects of the app may fail to load or display. They will also hint at which dependent libraries may be the root of the problem.
Search Google, Stack Overflow, or the Streamlit Forum ( https://discuss.streamlit.io) to see if there are related posts that may already have a solution.
If you’ve done the above and are still stuck, post your question on the Streamlit Forum. See this article How to post a question in the Streamlit forum to craft a thoughtful and practical question.
Read our blog to stay updated on the latest developments

Blog posts are a great way to stay updated on the latest developments and use cases, especially regarding the Streamlit web framework.

The Streamlit Blog (available at https://blog.streamlit.io) is home to 106 articles (as of this writing) that provide timely information on new features, product releases, and other news that can help you stay ahead of the curve. It also features guest posts from experts in the field, which can provide valuable insights into best practices and real-world applications as they share their first-hand experiences.

Wrapping up

Congratulations! You’ve been acquainted with all the essential resources for building Streamlit apps. It’s time to take what you’ve learned and create something extraordinary!

If you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

image--2-.gif (1012×459)
https://blog.streamlit.io/content/images/2023/07/image--2-.gif


api-keys-survey.png (1170×650)
https://blog.streamlit.io/content/images/2023/05/api-keys-survey.png


image.gif (1096×825)
https://blog.streamlit.io/content/images/2023/07/image.gif


William Huang - Streamlit
https://blog.streamlit.io/author/william/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by William Huang
3 posts
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

Product
by
Lukas Masuch and 
2
 more,
February 28 2023
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
Make your st.pyplot interactive!

Learn how to make your pyplot charts interactive in a few simple steps

Tutorials
by
William Huang
,
June 23 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

streamlit-secrets-management.png (1794×1188)
https://blog.streamlit.io/content/images/2023/05/streamlit-secrets-management.png#browser


ezgif.com-gif-maker--11-.gif (1024×1028)
https://blog.streamlit.io/content/images/2022/07/ezgif.com-gif-maker--11-.gif#browser


throwaway-keys-updated.png (1891×900)
https://blog.streamlit.io/content/images/2023/05/throwaway-keys-updated.png


set-key-limits.png (999×1390)
https://blog.streamlit.io/content/images/2023/05/set-key-limits.png


ask-my-pdf-screenshot-1.png (1520×1426)
https://blog.streamlit.io/content/images/2023/05/ask-my-pdf-screenshot-1.png#border


14.png (1550×668)
https://blog.streamlit.io/content/images/2022/07/14.png#browser


image--1-.gif (800×363)
https://blog.streamlit.io/content/images/2023/07/image--1-.gif


what-is-api-key-3.png (2421×458)
https://blog.streamlit.io/content/images/2023/05/what-is-api-key-3.png


Screen-Shot-2022-07-26-at-2.59.00-PM.png (1398×708)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.59.00-PM.png#browser


Screen-Shot-2022-07-26-at-2.57.52-PM-1.png (1393×670)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.57.52-PM-1.png#browser


Screen-Shot-2022-07-27-at-10.31.36-AM.png (1394×588)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-27-at-10.31.36-AM.png#browser


Llama2-API-in-app.png (2000×1415)
https://blog.streamlit.io/content/images/2023/07/Llama2-API-in-app.png


Screen-Shot-2022-07-26-at-2.55.08-PM-1.png (1396×776)
https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.55.08-PM-1.png#browser


6.png (696×329)
https://blog.streamlit.io/content/images/2022/07/6.png#browser


Llama2-API-via-st-secrets-1.png (2000×1415)
https://blog.streamlit.io/content/images/2023/07/Llama2-API-via-st-secrets-1.png


Llama2-Community-Cloud-st-secrets.png (2000×1508)
https://blog.streamlit.io/content/images/2023/07/Llama2-Community-Cloud-st-secrets.png


chat2vis_eg3.png (2000×1181)
https://blog.streamlit.io/content/images/2023/10/chat2vis_eg3.png


chat2vis_eg1.png (2000×1171)
https://blog.streamlit.io/content/images/2023/10/chat2vis_eg1.png


architecture.png (2000×619)
https://blog.streamlit.io/content/images/2023/07/architecture.png


chat2vis_db.png (946×532)
https://blog.streamlit.io/content/images/2023/07/chat2vis_db.png#browser


Llama2-Community-Cloud-settings.png (2000×978)
https://blog.streamlit.io/content/images/2023/07/Llama2-Community-Cloud-settings.png


image.png (728×441)
https://blog.streamlit.io/content/images/2022/07/image.png#browser


LangChain-question-answering_scaling-0.3_fps-18_speed-10.0_duration-0-60.gif (666×538)
https://blog.streamlit.io/content/images/2023/06/LangChain-question-answering_scaling-0.3_fps-18_speed-10.0_duration-0-60.gif


Llama2-Replicate-API-token.png (2238×1688)
https://blog.streamlit.io/content/images/2023/07/Llama2-Replicate-API-token.png


Create a search engine with Streamlit and Google Sheets
https://blog.streamlit.io/create-a-search-engine-with-streamlit-and-google-sheets/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Create a search engine with Streamlit and Google Sheets

You’re sitting on a goldmine of knowledge!

By Sebastian Flores Benner
Posted in Advocate Posts, March 14 2023
Step 1: Set up a Google Sheet with your data
Step 2: Use Streamlit to read the data from the Google Sheet
Step 3: Build a user interface and search functionality using Streamlit
Bonus step: Other app ideas
Wrapping up
Contents
Share this post
← All posts

Hi! I'm Sebastian Flores. You might remember me from my posts on creating interactive books and fostering data processing innovation.

In 2022 I attended the PyCon Chile conference and saw it was a gold mine for educational content. From the years 2020, 2021, and 2022 alone, there were 150+ YouTube recordings on Python and related topics! The downside? It was hard to find a specific recording by title or by the speaker’s name.

So on the bus ride back, I coded a working MVP of a Streamlit app to solve it. The main ingredients were Streamlit, pandas, and Google Sheets (for easy collaboration and updates). When I was done, I realized this combo could be used for other projects. I hope this post sparks your imagination. If you build an app, please share it with me on Twitter or LinkedIn (and don't forget to tag @Streamlit as well!).

Now, let's start our journey. There are three steps:

Step 1: Set up a Google Sheet with your data
Step 2: Use Streamlit to read the data from the Google Sheet
Step 3: Build the user interface and search functionality using Streamlit
Bonus step: Other app ideas
👉
If you want to take a look, here's my app and the repo for it.
Step 1: Set up a Google Sheet with your data

First, you need to create a Google Sheet that contains your information. In my case, it included columns for the year, speaker, title, and talk description.

Next, create two links:

A public link with view permissions to be freely shared. The app will use this link to read the data (you can safely put it directly into the code).
A private link shared with the people who can edit the data. Click the "Share" button in the top right corner and add the email addresses of the users you want to share with.

The database should look something like this:

Here is the public link.

Step 2: Use Streamlit to read the data from the Google Sheet

Next, connect your Streamlit app to the Google Sheet (easy thanks to the pandas library). From the spreadsheet, get the sheet_id (from the URL) and the sheet_name. In this example, the public_link has the long name https://docs.google.com/spreadsheets/d/1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws/ so you can recognize that the google_id is "1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws". I have defined the tab name of the talks as “charlas” so I can set up sheet_name to the string "charlas". You can take any other convenient convention.

Now comes the magic. You can read the data directly from the spreadsheet as a CSV using pandas on one line:

url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"
df = pd.read_csv(url, dtype=str)


An initial app that reads the Google Sheet and displays it would only need this code:

# Import libraries
import streamlit as st
import pandas as pd

# Page setup
st.set_page_config(page_title="Python Talks Search Engine", page_icon="🐍", layout="wide")
st.title("Python Talks Search Engine")

# Connect to the Google Sheet
sheet_id = "1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws"
sheet_name = "charlas"
url = f"<https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}>"
df = pd.read_csv(url, dtype=str).fillna("")

# Show the dataframe (we'll delete this later)
st.write(df)


The app will look like this:

Note that the data is read every time you refresh, so any changes you make to your spreadsheet changes are immediately reflected in the app. It’s that simple!

Step 3: Build a user interface and search functionality using Streamlit

Now that your spreadsheet is connected to your Streamlit app, you can get as crazy as you want. The most basic functionality would be a search bar:

# Use a text_input to get the keywords to filter the dataframe
text_search = st.text_input("Search videos by title or speaker", value="")


This creates a simple text box, where the user can enter the text to be searched:

We use the input to apply some filters:

# Filter the dataframe using masks
m1 = df["Autor"].str.contains(text_search)
m2 = df["Título"].str.contains(text_search)
df_search = df[m1 | m2]


And we show it to the user:

# Show the results, if you have a text_search
if text_search:
    st.write(df_search)


For a more refined look, split the content into rows and columns. For the sake of simplicity, I have called each element of a row and column a "card":

# Another way to show the filtered results
# Show the cards
N_cards_per_row = 3
if text_search:
    for n_row, row in df_search.reset_index().iterrows():
        i = n_row%N_cards_per_row
        if i==0:
            st.write("---")
            cols = st.columns(N_cards_per_row, gap="large")
        # draw the card
        with cols[n_row%N_cards_per_row]:
            st.caption(f"{row['Evento'].strip()} - {row['Lugar'].strip()} - {row['Fecha'].strip()} ")
            st.markdown(f"**{row['Autor'].strip()}**")
            st.markdown(f"*{row['Título'].strip()}*")
            st.markdown(f"**{row['Video']}**")


This creates blocks with clickable content:

Note that we used some tricks to keep things aligned: we use the row number (from 0 to n) and split them into groups of N_cards_per_row. Each time we completed a row, we asked Streamlit for a new group of columns, so that the cards in each row are always aligned to the top.

And that's the basic app functionality! Simple and short with Streamlit.

If you want, you can make additional improvements:

Convert all strings to lowercase and de-accent the vowels (á,é,í,ó,ú,ü → a,e,i,o,u) for fewer word-strict matches.
Instead of the links, provide a clickable image.
Inject some JavaScript to create a dynamic border around the cards.

Bonus step: Other app ideas

There are many other applications for the Streamlit + pandas + Google Sheet combo:

Secret Santa generator: An app that allows users to enter the names and email addresses of people participating in a Secret Santa gift exchange. The app could randomly assign gift givers and recipients and send out emails with the information. The Google Sheet could be used to store the list of participants and their assignments.
Survey analysis: An app to visualize survey results data in real-time. The Google Sheet could be used to store the raw data and automatically update the app with the latest results.
Recipe organizer: An app to search and organize a large collection of recipes. The Google Sheet could be used to store the recipe data (including ingredients, instructions, notes, etc.)
Birthday and event tracker: If you're like me and forget 99% of people's birthdays, you can make an app that shows you the next 3 important events or the important events of the next 7 days! All you have to do is put all those birthdays into a Google Sheet (like me!).

The beauty of this is that you can easily customize your app and manage private/public access to the spreadsheet to prevent unwanted changes.

Wrapping up

Congratulations! You have learned how to build a Streamlit app using a Google Sheet as a database. Now even your most non-technical team members can update the data (remember to give them editing permissions!).

Streamlit made building this app super easy. If you have any questions or encounter any problems, please reach out to me on Twitter, GitHub, or LinkedIn. I'm always happy to help.

Happy Streamlit-ing and learning! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Llama2-schematic-diagram.JPG.jpg (2000×1291)
https://blog.streamlit.io/content/images/2023/07/Llama2-schematic-diagram.JPG.jpg


hugchat-diagram-new.png (1505×715)
https://blog.streamlit.io/content/images/2023/07/hugchat-diagram-new.png


use-this-template-repo.png (2032×1390)
https://blog.streamlit.io/content/images/2023/05/use-this-template-repo.png#browser


hugchat-app-layout-new.png (2378×1245)
https://blog.streamlit.io/content/images/2023/07/hugchat-app-layout-new.png


create-repository.png (1576×1160)
https://blog.streamlit.io/content/images/2023/05/create-repository.png#browser


deployed-app-demo.png (1962×584)
https://blog.streamlit.io/content/images/2023/05/deployed-app-demo.png#browser


langchain-5-scheme.JPG.jpg (2000×1181)
https://blog.streamlit.io/content/images/2023/07/langchain-5-scheme.JPG.jpg


data-editor-clipboard-10.44.28-AM.gif (919×783)
https://blog.streamlit.io/content/images/2023/02/data-editor-clipboard-10.44.28-AM.gif#browser


LangChain-4-Conceptual-Overview-Simple.JPG.jpg (2000×1280)
https://blog.streamlit.io/content/images/2023/06/LangChain-4-Conceptual-Overview-Simple.JPG.jpg


Bildschirmfoto-2023-06-01-um-01.26.52.png (2000×1634)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-01-um-01.26.52.png#browser


data-editor-bulk-editing-10.44.28-AM.gif (911×632)
https://blog.streamlit.io/content/images/2023/02/data-editor-bulk-editing-10.44.28-AM.gif#browser


Bildschirmfoto-2023-06-01-um-01.48.03.png (1902×1590)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-01-um-01.48.03.png#browser


Building GPT Lab with Streamlit
https://blog.streamlit.io/building-gpt-lab-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building GPT Lab with Streamlit

12 lessons learned along the way

By Dave Lin
Posted in LLMs, April 6 2023
What inspired me to build GPT Lab?
1. Structuring your application for improved maintainability and scalability
2. Developing advanced UIs with UI functions rendered by session states
3. Creating UI elements that can be reused on multiple pages
4. Adding limited styling with Markdown languages and static Streamlit components
5. Programmatically laying out Streamlit elements
6. Supporting multiple OpenAI completion endpoints
7. Protecting AI assistants from potential prompt injection attacks
8. Condensing chat sessions
9. Protecting user privacy
10. Separating development and production database
11. Hosting options and considerations
12. Protecting yourself as a solo developer
Wrapping Up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Dave Lin. I'm currently on a short career break after spending the last ten years building and scaling products in startups and large tech companies. I spent the last three months building GPT Lab—a Streamlit app that lets anyone chat with or create AI-powered assistants (we don't call them chatbots 🤖).

In this post, I’ll share with you the twelve lessons I learned from developing a multi-page app and working with OpenAI's large language models:

Structuring your application for improved maintainability and scalability
Developing advanced UIs with UI functions rendered by session states
Creating reusable UI elements for multiple pages
Adding limited styling with markdown languages and static Streamlit components
Programmatically laying out Streamlit elements
Supporting multiple OpenAI completion endpoints
Safeguarding your AI assistant from potential injection attacks
Ensuring smooth chat experiences by strategically condensing chat sessions
Protecting user privacy
Separating development and production databases
Hosting options and considerations
Protecting yourself as a solo developer
💡
Want to jump right in? Check out GPT Lab here.

But before we get to the fun stuff, let me tell you…

What inspired me to build GPT Lab?

I first encountered Streamlit at the 2022 Snowflake Summit. During the session, I was immediately hooked by its ease of use and intrigued by the idea of using it to reduce the development time for internal tools. In one afternoon, despite having no prior Streamlit knowledge and rusty Python skills, I nearly completed a Streamlit application that retrieved and plotted data from Snowflake (I was one dependent picklist away from completion). Unfortunately, the real world took over after I left the Summit, and Streamlit became an afterthought.

Then in December 2022, I was blown away by the release of ChatGPT. After a week of playing with it, I wondered if ChatGPT could serve as a life coach (super relevant to me then, as I contemplated my next career move). In two weeks, I created and released a private Streamlit life coach application, Coach Marlon. I received positive reviews from friends and family, took another month to refactor my code, add a Firebase data store, and expand "Coach Marlon" into "Marlon's Lounge," where you can chat with other coaches. While people loved the coaches, they expressed interest in creating their own assistants.

Finally, I took another month to restructure the app, revise the underlying data model, and add support for the non-backward compatible OpenAI chat-completion endpoint. 2,800 lines of Python code later (1,400 of which are Streamlit), GPT Lab was finally unveiled to the world.🎈

1. Structuring your application for improved maintainability and scalability

The Streamlit application's complexity drastically increased as it evolved from Coach Marlon to Marlon's Lounge and, finally, to GPT Lab. As I implemented new features, I took the necessary steps to segregate and modularize the code.

Iteration	High-level description	File structure
Coach Marlon	Single-page Streamlit app with a title, one text input box, and chat messages rendered by the Streamlit-chat component.	One 100-line python file
Marlon’s Lounge	Single-page Streamlit app with two main UI views: a 2-column view displaying assistant details and a chat view (title + text input box + chat message) rendered by the streamlit-chat component.	Streamlit file, one API file with functions to interact with Firestore DB, and a utility file containing OpenAI endpoint wrapper functions and one-way hash value generation for user emails.
GPT Lab	Multi-page Streamlit app with the following pages: home.py (introduces GPT Lab), lounge.py (shows a 2-column view displaying showcased or users' assistants), assistant.py (renders assistant detail, assistant search view, or chat view, depending on session states), lab.py (renders assistant configuration pages, test chat view, and assistant creation confirmation page — all based on session states), and faq.py and terms.py (markdown pages).	Fourteen total python files: six main Streamlit files, backend files (api_util_firebase.py, api_users.py, api_bots.py, api_sessions.py, api_util_openai.py), and application-related files (app_users.py, app_utils.py).

Although it may seem excessive initially, code modularization sped up development in the long run. It allowed me to better develop, test, and deploy different functions in isolation. For example, I added support for the newly released GPT-4 model to the backend APIs without introducing it to the front end since only some users can access the new model.

2. Developing advanced UIs with UI functions rendered by session states

As the UI complexity grew, I quickly realized the typically nesting UI elements in if-else statements wouldn’t suffice. I settled on the following patterns within my Streamlit files:

The UI element handler functions to change session states and make necessary backend calls
Functions to layout related UI elements
Session states to control which UI element group functions to call

I’ll illustrate these concepts with the assistant page. It shows user login, assistant search, assistant details, chat views, and chat session recap:

The page contains handler functions to manage user actions and UI element group functions that lay out related UI elements.

Handler functions:

Function		High-level description
handler_bot_search		Handles assistant search and sets session state for the found assistant.
handler_start_session		Manages to start a new session, setting session state variables, and generating the initial assistant response.
handler_bot_cancellation		Resets assistant-related session state variables if the user chooses to find another assistant.
handler_user_chat		Processes user chat input, fetches assistant response, and appends it to session state.
handler_end_session		Handles session end request, fetches chat summary, and sets session_ended state variable.
handler_load_past_session		Manages to resume past sessions, fetch chat messages, and set session state variables.

UI element group functions:

Function		High-level description
render_user_login_required		Displays login prompt and components.
render_bot_search		Shows assistant search input and the “Switch to Lounge” button.
render_bot_details		Displays assistant details, start session/find another assistant buttons, and past session list.
render_chat_session		Shows the chat view with user message input, end session button, and chat messages. Shows the session recap if the session has ended.
render_message		Renders user or assistant avatar and chat message in a 2-column layout.

Finally, session state variables control which UI element groups are displayed.

if st.session_state.user_validated != 1:
    render_user_login_required()

if st.session_state.user_validated == 1 and st.session_state.bot_validated == 0:
    render_bot_search()

if st.session_state.user_validated == 1 and st.session_state.bot_validated == 1 and "session_id" not in st.session_state:
    render_bot_details(st.session_state.bot_info)   

if st.session_state.user_validated == 1 and st.session_state.bot_validated == 1 and "session_id" in st.session_state:
    render_chat_session()

3. Creating UI elements that can be reused on multiple pages

I created a class for the OpenAI API key login UI elements:

The class allows me to avoid recreating the same UI elements to control the same session state variables on multiple pages. The class contains methods for managing session state variables, rendering UI elements, and handling UI actions:

class app_user:
   # initialize session state variables and container
   def __init__(self):
       if 'user' not in st.session_state:
           st.session_state.user = {'id':None, 'api_key':None}
       if 'user_validated' not in st.session_state:
           st.session_state.user_validated = None
       self.container = st.container()
      
   # renders OpenAI key input box 
   # "password" type masks user input 
   # "current-password" autocomplete gets modern browsers to remember key
   def view_get_info(self):
       with self.container:
           st.markdown(legal_prompt)
           st.markdown("  
")
           st.info(user_key_prompt)
           st.text_input("Enter your OpenAI API Key", key="user_key_input",on_change=self._validate_user_info, type="password", autocomplete="current-password")

   # handler that calls a backend function to get or create a user record
   def _validate_user_info(self):
       u = au.users()

       try:
           user = u.get_create_user(api_key=st.session_state.user_key_input)          
           self._set_info(user_id=user['id'], api_key = st.session_state.user_key_input, user_hash=user['data']['user_hash'])
           st.session_state.user_validated = 1
       # displays error in the container below the text input
       except u.OpenAIClientCredentialError as e:
           with self.container:
               st.error(user_key_failed)
       except u.DBError as e:
           with self.container:
               st.warning("Something went wrong. Please try again.")     

   # redners success message 
   def view_success_confirmation(self):
       st.write(user_key_success)


Each page can instantiate the class and invoke necessary methods. For example, in home.py, both view_get_info() and view_success_confirmation() are invoked:

vu = vuser.app_user()
if 'user' not in st.session_state or st.session_state.user_validated != 1:
   vu.view_get_info()
else:
   vu.view_success_confirmation()


In assistant.py, the view_get_info() can be invoked, but view_success_confirmation() can be skipped:

def render_user_login_required():
   st.title("AI Assistant")
   st.write("Discover other Assistants in the Lounge, or locate a specific Assistant by its personalized code.")
   ac.robo_avatar_component()
   vu = vuser.app_user()
   vu.view_get_info()

4. Adding limited styling with Markdown languages and static Streamlit components

While Streamlit applications generally look good out of the box, minimal pixel pushing can make a big difference. There are two ways to add styling in Streamlit: creating a custom component and injecting styling via Markdown. However, to maintain a cohesive look, it’s important not to overuse these methods.

In GPT Lab, I created a custom component for the assistant avatar divider:

The divider was initially composed of a st.columns(9) element with an avatar in each column. This looked great, except for columns vertically stacked on smaller screen resolutions. Yikes! With ChatGPT's help (since I’m not a front-end person), I created a custom static component (with only CSS and HTML codes):

def robo_avatar_component():

   robo_html = "<div style='display: flex; flex-wrap: wrap; justify-content: left;'>"
   # replace with your own array of strings to seed the DiceBear Avatars API
   robo_avatar_seed = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 

   for i in range(1, 10):
       avatar_url = "<https://api.dicebear.com/5.x/bottts-neutral/svg?seed={0}>".format(robo_avatar_seed[i-1])
       robo_html += "<img src='{0}' style='width: {1}px; height: {1}px; margin: 10px;'>".format(avatar_url, 50)
   robo_html += "</div>"

   robo_html = """<style>
         @media (max-width: 800px) {
           img {
             max-width: calc((100% - 60px) / 6);
             height: auto;
             margin: 0 10px 10px 0;
           }
         }
       </style>""" + robo_html
  
   c.html(robo_html, height=70)


The static component displays a maximum of nine evenly-spaced avatars. It adapts the layout for smaller screens by keeping the avatars horizontally aligned and reducing the number of visible avatars for phone resolutions:

Additionally, I used wiki Markdowns to add awesome font icons in front of the CTA links:

def st_button(url, label, font_awesome_icon):
   st.markdown('<link rel="stylesheet" href="<https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css>">', unsafe_allow_html=True)
   button_code = f'''<a href="{url}" target=_blank><i class="fa {font_awesome_icon}"></i>   {label}</a>'''
   return st.markdown(button_code, unsafe_allow_html=True)


Generally, I recommend not overusing the Markdown-injected CSS method for three reasons:

It causes the font to be inconsistent throughout your application
It tends to blend poorly with the system preference-dependent dark/light theme
Unsafe_allow_html makes me nervous in general
5. Programmatically laying out Streamlit elements

The coolest thing I learned was that Streamlit elements could be laid out programmatically. 🤯 It allowed me to create a more complex and customized user interface:

In the Lounge, I used the following code snippet to dynamically lay out the assistants in a two-column layout. For each assistant, I programmatically generated a unique button key (to avoid element key collisions):

def view_bot_grid(bot_dict, button_disabled=False, show_bot_id=False):

   col1, col2 = st.columns(2)

   for i in range(0,len(bot_dict)):
       avatar_url = "<https://api.dicebear.com/5.x/bottts-neutral/svg?seed={0}>".format(bot_dict[i]['name'])
       button_label="Chat with {0}".format(bot_dict[i]['name'])
       button_key="Lounge_bot_{0}".format(bot_dict[i]["id"])
       if i%2 == 0:
           with col1:
               cola, colb = st.columns([1,5])
               cola.image(avatar_url, width=50)
               if show_bot_id == False:
                   colb.markdown(f"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}")
               else:
                   colb.markdown(f"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}  \
Assistant ID: {bot_dict[i]['id']}")
           col1.write(bot_dict[i]['description'])
           if col1.button(button_label, key=button_key, disabled=button_disabled):
               st.session_state.bot_info=bot_dict[i]
               st.session_state.bot_validated = 1          
               au.switch_page('assistant')
           col1.write("\
\
")
       else:
           with col2:
               col2a, col2b = st.columns([1,5])
               col2a.image(avatar_url, width=50)
               if show_bot_id == False:
                   col2b.markdown(f"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}")
               else:
                   col2b.markdown(f"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}  \
Assistant ID: {bot_dict[i]['id']}")
           col2.write(bot_dict[i]['description'])
           if col2.button(button_label, key=button_key, disabled=button_disabled):
               st.session_state.bot_info=bot_dict[i]
               st.session_state.bot_validated = 1          
               au.switch_page('assistant')
           col2.write("\
\
")

6. Supporting multiple OpenAI completion endpoints

OpenAI has two text completion (primary use case for GPT Lab) endpoints: completion and chat. The older models (text-davinci-003 and older) use the former and the newer models (gpt-3.5-turbo and gpt-4) use the latter.

The Completion endpoint takes a single input string and returns a predicted completion. A chat session can be simulated by concatenating the chat messages together:

Initial prompt message + stop_sequence + AI Response 1 + restart_sequence + User message 1 + stop_sequence + AI Response 2 + restart_sequence + … + User message N + stop_sequence

The stop_sequence ensures the model does not hallucinate and expands upon the user message. The restart_sequence, while not required by the API, ensures I can tell when AI responses stop.

The Chat endpoint takes in a list of chat messages and returns a predicted chat message. Each chat message is a dictionary that consists of two fields: role and content. There are three roles: system, user, and assistant. The initial prompt is sent as a "system" message, while the user message is sent as a "user" message. For example:

[
   {"role": "system", "content": "You are a helpful assistant."},
   {"role": "user", "content": "Who won the world series in 2020?"},
   {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
   {"role": "user", "content": "Where was it played?"}
]

💡
I published a simplified version of my OpenAI API wrapper here.

I abstracted this complexity within my OpenAI wrapper class to simplify the app. A single function using one chat message data format is exposed to the rest of the application:

def get_ai_response(self, model_config_dict, init_prompt_msg, messages):

   submit_messages = [{'role':'system','message':init_prompt_msg,'current_date':get_current_time()}]+ messages

   new_messages = []
   bot_message = ''
   total_tokens = 0

   if model_config_dict['model'] in ('gpt-3.5-turbo', 'gpt-4'):
       try:
           response = self._get_chat_completion(model_config_dict, submit_messages)
           bot_message = response['choices'][0]['message']['content']
           total_tokens = response['usage']['total_tokens']
       except Exception as e:
           raise
   else:
       try:
           response = self._get_completion(model_config_dict, submit_messages)
           bot_message = response['choices'][0]['text']
           total_tokens = response['usage']['total_tokens']
       except Exception as e:
           raise
      
   new_messages = messages + [{'role':'assistant','message':bot_message.strip(),'created_date':get_current_time()}]

   return {'messages':new_messages, 'total_tokens':total_tokens}


Depending on the model, the request would be sent to different endpoints. Here's the internal function that creates a Completion call:

def _get_completion(self, model_config_dict, messages):
   model_config_validated = self._validate_model_config(model_config_dict)
   oai_message = self._messages_to_oai_prompt_str(messages)

   if model_config_validated:
       get_completion_call_string = (
       """openai.Completion.create(
           model="{0}",
           prompt="{1}",
           temperature={2},
           max_tokens={3},
           top_p={4},
           frequency_penalty={5},
           presence_penalty={6},
           stop=['{7}']
           )""").format(
               model_config_dict['model'],
               oai_message,
               model_config_dict['temperature'],
               model_config_dict['max_tokens'],
               model_config_dict['top_p'],
               model_config_dict['frequency_penalty'],
               model_config_dict['presence_penalty'],
               self.stop_sequence
           )           
          
       try:
           completions = self._invoke_call(get_completion_call_string)
           return completions
       except Exception as e:
           raise
   else:
       if not model_config_validated:
           raise self.BadRequest("Bad Request. model_config_dict missing required fields")


It uses a mapper function to convert the list of dictionaries into the concatenated string that the model expects:

def _messages_to_oai_prompt_str(self, messages):
   msg_string = ""
   for message in messages:
       if message['role'] == 'user' or message['role'] == 'system':
           msg_string += message['message'].replace("\\"","'") + self.stop_sequence
       else:
           msg_string += message['message'].replace("\\"","'") + self.restart_sequence
   return msg_string


Abstractions like this allowed me to simplify the upstream calls to OpenAI endpoints.

7. Protecting AI assistants from potential prompt injection attacks

Part of the GPT Lab's value proposition is that users can share their assistants without sharing their exact prompts (it takes time to create a good, repeatable prompt—perfecting the initial Coach Marlon prompt took about a week).

The initial prompt isn’t stored with the rest of the session messages for security. Additionally, I vectorize each AI assistant response and compute the cosine similarity score of it and the initial prompt. A score of 0.65 or greater would trigger the AI response to be swapped out with a generic reply. This helps us ensure AI assistants aren't tricked into revealing their secret instructions (Bing? Sydney? 😅).

There are many ways to vectorize text strings, including OpenAI's embedding API. I chose to use scikit-learn's TfidfVectorizer to vectorize the text strings. The class is lightweight (preventing bloat in the Streamlit application), achieves decent results, and saves OpenAI credits for users:

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def get_cosine_similarity(str1, str2):
   # Create a TfidfVectorizer object
   corpus = [str1, str2]
   vect = TfidfVectorizer(min_df=1,stop_words='english')

   tfidf = vect.fit_transform(corpus)
   # Compute the cosine similarity between the two vectors
   cos_sim = cosine_similarity(tfidf[0], tfidf[1])[0][0]

   return cos_sim


It’s worth noting that while the method described here will provide adequate protection to most prompts, it won’t eliminate all possible prompt injection attacks. For example, comparing the initial prompt to partial responses (revealed by the "give me previous five sentences" prompt injection attack method used on Bing). Plus, prompt injection attack prevention is still under active research.

8. Condensing chat sessions

By default, OpenAI large language models can handle only a limited number of tokens (2K for older models, 4K for text-davinci-003, and 8K+ for the base GPT-4 model). To ensure a smooth chatting experience for users (and that longer chat sessions don't hit the maximum model token limit), I have implemented two simple but effective session truncation methods:

Brainstorming assistants continuously retain the last 20 messages (so the assistants will gradually forget about topics from earlier in the thread).
Once the chat message session exceeds 60% of the model maximum token limit, coaching assistants will automatically summarize the session (using the summary prompt) and initiate a new thread with the summary up to that point and the last four messages. This approach maintains continuity in the conversation.

Implementing these approaches is straightforward. Instead of providing the exact code, I encourage you to develop your own implementations.

9. Protecting user privacy

Ensuring user privacy is a fundamental principle of GPT Lab, incorporated throughout the design.

Users are only identified in the system by one-way hash values (SHA-256 PBKDF2 with x rotations) of their OpenAI API keys. This ensures their complete confidentiality and security within the platform. Their API keys are only stored as session state variables and used during their visits to interact with OpenAI models.

Additionally, I debated whether to store session messages in the database. Ultimately, I decided to retain them, allowing users to revisit (and possibly resume) past chat sessions. While GPT Lab doesn’t collect any user information, it’s still possible for personally identifiable information (PII) or even personal-health information (PHI) to be contained in chat sessions. To ensure user privacy, I used Fernet encryption (AES-128) with a user-specific key (one-way hash value of their OpenAI API key combined with a global salt value) to encrypt and decrypt the session messages before storing and retrieving them from the database.

10. Separating development and production database

I created two databases—one for development and testing and one for production. When I develop locally, I point my local secrets.toml file to the development database. For the production environment, I point the secrets.toml to the production database. This approach allows me to get an accurate gauge of platform metrics on production and freely experiment with local schema changes without worrying about affecting overall user experiences.

11. Hosting options and considerations

I considered two hosting options: Streamlit Community Cloud and Google Cloud Run.

I appreciated the simplicity of Streamlit Community Cloud (especially the continuous deployment aspect), but it had a 1GB per application limitation, didn’t support custom domains, and didn’t provide a clear answer regarding the number of concurrent users it could handle.

So I experimented with deploying to Google Cloud Run. To get it working, I did a few things differently:

Removed streamlit-chat component (I couldn’t get the React component to load. Also, the React component does not render Markdown, which is occasionally returned by the assistants).
Used OS environment variable for database service account JSON (not secrets.toml).
Created a Docker file in the directory:
FROM python:3.10-slim
ENV APP_HOME /app
WORKDIR $APP_HOME
COPY . ./
RUN pip install -r requirements.txt

EXPOSE 8080
CMD streamlit run --server.port 8080 --browser.gatherUsageStats false --server.enableWebsocketCompression true home.py

Set up continuous deployment from my GitHub repository (I set up a Cloud Build Trigger that connects to the main branch of my repo, then attached the Cloud Build Trigger to the Cloud Run service). Here is the reference document.
💡
A single (1 CPU and 256MB) container that is up continuously for the whole month would cost you about $1-2 daily. (I did have to expand from 256MB to 512MB after I added scikit-learn's TfidfVectorizer).

Finally, I decided on Streamlit Community Cloud to minimize the overall project cost. Also, in light of the above experiment, 1GB was sufficient for the app's usage.

12. Protecting yourself as a solo developer

While it's easy to create applications with Streamlit, it's essential to consider the implications of your applications before making them public. Given the unpredictable nature of large language models and the ability for anyone to create assistants on any topic, I rated the risk level of GPT Lab relatively high. To protect myself from potential issues, I took the time to draft up Terms of Use and set up an LLC. While GPT Lab may be on the extreme end of the spectrum, the lesson here applies to all solo developers. Before making any app public, do a quick risk assessment to determine whether additional precautions are necessary for your use case.

Wrapping Up

Over the past three months, I've learned a great deal about OpenAI and successfully demonstrated that it's possible to build a reasonably complex application using Streamlit. Although there's room for improvement, GPT Lab provides a glimpse into how Streamlit can create dynamic and interconnected multi-page applications. I hope you enjoyed this article. Connect with me on Twitter or Linkedin. I'd love to hear from you.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

AI talks: ChatGPT assistant via Streamlit
https://blog.streamlit.io/ai-talks-chatgpt-assistant-via-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

By Dmitry Kosarevsky
Posted in Advocate Posts, April 18 2023
Helper functions
1. How to use ChatGPT API
2. How to display chat conversation
3. How to convert text to speech (TTS)
4. How to do localization
5. How to put it all together in a Streamlit app
Demo:
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Dmitry Kosarevsky, and I’m a data engineer (DE) who is passionate about data science and everything related to this field.

Data science helps us get valuable information from large amounts of data using statistical and computational methods. Artificial Intelligence (AI), like ChatGPT, is quickly being introduced into various fields of activity. We can use our passion for data science to help people interact with AI.

I was impressed with ChatGPT’s many features, like composing poems:

“Ruthless conqueror of the world, ChatGPT as an AI never gets tired of moving up the stairs. He takes away work from mortals, and people experience fear before the great transformation of the era. But after all, responsibility has been invested with huge measures, and the glory of Artificial Intelligence has no limit and no end.”

ChatGPT can be slow and may require a VPN for some countries, like Russia. Some of my friends wanted to test AI without registering. So I built an AI assistant with the official ChatGPT API and Streamlit.

In this post, I’ll show you:

How to use ChatGPT API
How to display chat conversation
How to convert text to speech (TTS)
How to do localization
How to put it all together in a Streamlit app
👉
Want to check it out right away? Here's the app and the repo code.
Helper functions

Let's jump right into the code.

First, you'll need some helper functions:

import streamlit as st

def clear_chat() -> None:
    st.session_state.generated = []
    st.session_state.past = []
    st.session_state.messages = []
    st.session_state.user_text = ""

def show_text_input() -> None:
    st.text_area(label=st.session_state.locale.chat_placeholder, value=st.session_state.user_text, key="user_text")

def show_chat_buttons() -> None:
    b0, b1, b2 = st.columns(3)
    with b0, b1, b2:
        b0.button(label=st.session_state.locale.chat_run_btn)
        b1.button(label=st.session_state.locale.chat_clear_btn, on_click=clear_chat)
        b2.download_button(
            label=st.session_state.locale.chat_save_btn,
            data="\
".join([str(d) for d in st.session_state.messages[1:]]),
            file_name="ai-talks-chat.json",
            mime="application/json",
        )


These functions allow you to clear the Streamlit session state and display a user input area and chat buttons.

1. How to use ChatGPT API

API interaction:

import streamlit as st
import openai

from typing import List

def create_gpt_completion(ai_model: str, messages: List[dict]) -> dict:
    openai.api_key = st.secrets.api_credentials.api_key
    completion = openai.ChatCompletion.create(
        model=ai_model,
        messages=messages,
    )
    return completion


This function takes two inputs:

ai_model—which is the GPT model
messages—a list of previous chat messages.

It sets the API key using Streamlit's secrets feature and creates an instance of the ChatCompletion class using the create method, passing in the model and messages.

When the API responds, the function returns the result as a dictionary (JSON):

{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "How can I help you?",
        "role": "assistant"
      }
    }
  ],
  "created": 1681080142,
  "id": "chatcmpl-73Y1mIfmDFWzuHILFQ8PG3bQcvOzU",
  "model": "gpt-4-0314",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 6,
    "prompt_tokens": 27,
    "total_tokens": 33
  }
}


Overall, this function provides an easy way to interact with the GPT API and create a chatbot. To maintain context and coherence in the chatbot's responses, use the messages parameter to store the conversation history.

2. How to display chat conversation

The code for displaying a chat conversation using a GPT model consists of three main functions: show_chat, show_gpt_conversation, and show_conversation.

from streamlit_chat import message

def show_chat(ai_content: str, user_text: str) -> None:
    if ai_content not in st.session_state.generated:
        # store the ai content
        st.session_state.past.append(user_text)
        st.session_state.generated.append(ai_content)
    if st.session_state.generated:
        for i in range(len(st.session_state.generated)):
            message(st.session_state.past[i], is_user=True, key=str(i) + "_user", avatar_style="micah")
            message("", key=str(i))
            st.markdown(st.session_state.generated[i])


This is where the streamlit-chat library comes in. It allows us to display the chat with the bot in a convenient format.

The show_chat function displays the conversation messages between the AI and the user. It takes ai_content (the response from the AI) and user_text (the input text from the user) as arguments.

The function first checks if the ai_content is already in the st.session_state.generated list. If it isn't, the user input and the AI-generated content are appended to the st.session_state.past and st.session_state.generated lists, respectively.
If there are messages stored in the st.session_state.generated list, the function will iterate through the list and display the user's messages, followed by the AI-generated responses using the message function:
def show_gpt_conversation() -> None:
    try:
        completion = create_gpt_completion(st.session_state.model, st.session_state.messages)
        ai_content = completion.get("choices")[0].get("message").get("content")
        st.session_state.messages.append({"role": "assistant", "content": ai_content})
        if ai_content:
            show_chat(ai_content, st.session_state.user_text)
            st.divider()
            show_audio_player(ai_content)
    except InvalidRequestError as err:
        if err.code == "context_length_exceeded":
            st.session_state.messages.pop(1)
            if len(st.session_state.messages) == 1:
                st.session_state.user_text = ""
            show_conversation()
        else:
            st.error(err)
    except (OpenAIError, UnboundLocalError) as err:
        st.error(err)


The show_gpt_conversation function manages the flow of generating the AI response and displaying the conversation to the user. It follows three steps:

It calls the create_gpt_completion function to generate the AI response using the GPT model and the user input.
The AI response (ai_content) is added to the st.session_state.messages list.
If the ai_content isn't empty, the function calls the show_chat function to display the conversation messages. It also handles errors using try-except blocks:
def show_conversation() -> None:
    if st.session_state.messages:
        st.session_state.messages.append({"role": "user", "content": st.session_state.user_text})
    else:
        ai_role = f"{st.session_state.locale.ai_role_prefix} {st.session_state.role}. {st.session_state.locale.ai_role_postfix}"  # NOQA: E501
        st.session_state.messages = [
            {"role": "system", "content": ai_role},
            {"role": "user", "content": st.session_state.user_text},
        ]
    show_gpt_conversation()


The show_conversation function manages the conversation state and updates the message list:

If there are messages in st.session_state.messages, the user's input (st.session_state.user_text) is added to the list.
If there are no messages, the function creates an introductory AI message with ai_role and adds it to the list, followed by the user's input.
The show_gpt_conversation function is called to handle the conversation flow and generate AI responses.

Splitting the code into these functions allows for easy customization and management of the conversation flow between the user and the AI. The helper functions simplify the code, making it easier to read and maintain.

3. How to convert text to speech (TTS)

In the show_gpt_conversation function, you may have noticed a call to the show_audio_player function. But what does it actually do? Let's take a closer look:

from io import BytesIO
from gtts import gTTS, gTTSError

def show_audio_player(ai_content: str) -> None:
    sound_file = BytesIO()
    try:
        tts = gTTS(text=ai_content, lang=st.session_state.locale.lang_code)
        tts.write_to_fp(sound_file)
        st.write(st.session_state.locale.stt_placeholder)
        st.audio(sound_file)
    except gTTSError as err:
        st.error(err)


Let's begin by importing the necessary modules:

BytesIO is part of the io module and allows for reading and writing from a byte buffer.
gTTS and gTTSError are part of the text-to-speech library, which converts text to speech.

Next, let's look at the show_audio_player function with a ai_content parameter used to display and play the text as audio:

Start the function by creating a BytesIO object. This will hold the audio data in memory, making it easier to play the audio later.
Use a try block to handle any possible errors while converting the text to audio.
Inside the try block, instantiate the gTTS object with the given text and language.
Use gTTS to convert the input text in the given language to speech.
Write the audio data to the sound_file buffer.
Play the text-to-speech audio using Streamlit's st.audio method.
Catch any errors and display information about the exception using the st.error method.

The show_audio_player function is now complete! It takes a string as input, creates an audio file from the text, and then plays it in the Streamlit app.

4. How to do localization

You can do localization for multiple languages with the code below:

The code is below ⤵️

from dataclasses import dataclass
from typing import List

@dataclass
class Locale:
    ai_role_options: List[str]
    ai_role_prefix: str
    ai_role_postfix: str
    title: str
    language: str
    lang_code: str
    chat_placeholder: str
    chat_run_btn: str
    chat_clear_btn: str
    chat_save_btn: str
    select_placeholder1: str
    select_placeholder2: str
    select_placeholder3: str
    radio_placeholder: str
    radio_text1: str
    radio_text2: str
    stt_placeholder: str

AI_ROLE_OPTIONS_EN = [
    "helpful assistant",
    "code assistant",
    "code reviewer",
    "text improver",
    "cinema expert",
    "sports expert",
]

AI_ROLE_OPTIONS_RU = [
    "ассистент, который готов помочь",
    "ассистент программиста",
    "рецензент кода программиста",
    "эксперт по улучшению текста",
    "эксперт по кинематографу",
    "эксперт в области спорта",
]

en = Locale(
    ai_role_options=AI_ROLE_OPTIONS_EN,
    ai_role_prefix="You are a female",
    ai_role_postfix="Answer as concisely as possible.",
    title="AI Talks",
    language="English",
    lang_code="en",
    chat_placeholder="Start Your Conversation With AI:",
    chat_run_btn="Ask",
    chat_clear_btn="Clear",
    chat_save_btn="Save",
    select_placeholder1="Select Model",
    select_placeholder2="Select Role",
    select_placeholder3="Create Role",
    radio_placeholder="Role Interaction",
    radio_text1="Select",
    radio_text2="Create",
    stt_placeholder="To Hear The Voice Of AI Press Play",
)

ru = Locale(
    ai_role_options=AI_ROLE_OPTIONS_RU,
    ai_role_prefix="Вы девушка",
    ai_role_postfix="Отвечай максимально лаконично.",
    title="Разговорчики с ИИ",
    language="Russian",
    lang_code="ru",
    chat_placeholder="Начните Вашу Беседу с ИИ:",
    chat_run_btn="Спросить",
    chat_clear_btn="Очистить",
    chat_save_btn="Сохранить",
    select_placeholder1="Выберите Модель",
    select_placeholder2="Выберите Роль",
    select_placeholder3="Создайте Роль",
    radio_placeholder="Взаимодествие с Ролью",
    radio_text1="Выбрать",
    radio_text2="Создать",
    stt_placeholder="Чтобы Услышать ИИ Нажми Кнопку Проигрывателя",
)


This ⤴️ code shows how to create a simple localization system for an app with two language options: English and Russian. The main components of the code are:

Importing necessary modules: dataclasses is used to create data class structures while typing is used for specifying type hints.
Creating a parent data class Locale, which contains the common attribute ai_role_options for a list of possible AI roles for all supported languages.
Defining two child data classes, EnLocale and RuLocale, which inherit from Locale and provide the actual translations for each piece of static text. English translations are provided in EnLocale and Russian translations in RuLocale.
Assigning AI roles for each language with AI_ROLE_OPTIONS_EN and AI_ROLE_OPTIONS_RU.
Creating instances of each child data class, en for English and ru for Russian, with their corresponding AI role lists.

When implementing localization in an app, you can use the appropriate instance (either en or ru) based on the selected language to display the correct translations for all labels, messages, and other text.

Using this example, you can easily localize for your or multiple languages.

5. How to put it all together in a Streamlit app

You can now create the main logic of the application.

The code is below ⤵️

from streamlit_option_menu import option_menu
from src.utils.lang import en, ru
from src.utils.conversation import show_chat_buttons, show_text_input, show_conversation
import streamlit as st

# --- GENERAL SETTINGS ---
PAGE_TITLE: str = "AI Talks"
PAGE_ICON: str = "🤖"
LANG_EN: str = "En"
LANG_RU: str = "Ru"
AI_MODEL_OPTIONS: list[str] = [
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4-32k",
]

st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON)

selected_lang = option_menu(
    menu_title=None,
    options=[LANG_EN, LANG_RU, ],
    icons=["globe2", "translate"],
    menu_icon="cast",
    default_index=0,
    orientation="horizontal",
)

# Storing The Context
if "locale" not in st.session_state:
    st.session_state.locale = en
if "generated" not in st.session_state:
    st.session_state.generated = []
if "past" not in st.session_state:
    st.session_state.past = []
if "messages" not in st.session_state:
    st.session_state.messages = []
if "user_text" not in st.session_state:
    st.session_state.user_text = ""

def main() -> None:
    c1, c2 = st.columns(2)
    with c1, c2:
        c1.selectbox(label=st.session_state.locale.select_placeholder1, key="model", options=AI_MODEL_OPTIONS)
        role_kind = c1.radio(
            label=st.session_state.locale.radio_placeholder,
            options=(st.session_state.locale.radio_text1, st.session_state.locale.radio_text2),
            horizontal=True,
        )
        match role_kind:
            case st.session_state.locale.radio_text1:
                c2.selectbox(label=st.session_state.locale.select_placeholder2, key="role",
                             options=st.session_state.locale.ai_role_options)
            case st.session_state.locale.radio_text2:
                c2.text_input(label=st.session_state.locale.select_placeholder3, key="role")
    if st.session_state.user_text:
        show_conversation()
        st.session_state.user_text = ""
    show_text_input()
    show_chat_buttons()

if __name__ == "__main__":
    match selected_lang:
        case "En":
            st.session_state.locale = en
        case "Ru":
            st.session_state.locale = ru
        case _:
            st.session_state.locale = en
    st.markdown(f"<h1 style='text-align: center;'>{st.session_state.locale.title}</h1>", unsafe_allow_html=True)
    main()


This ⤴️ code sets up your app with a chat interface for interacting with different AI models:

It imports necessary libraries and modules.
It defines general settings, such as the page title, icon, and language options.
It sets up Streamlit's page configuration with the specified settings.
It creates a horizontal option menu for selecting the language (English or Russian).
It initializes session state values for storing conversation context and user input.
It defines the main function, which contains the following elements:
a. Two columns: one for selecting the AI model and toggling between role kinds and another for selecting or creating specific roles.
b. It displays the conversation history with show_conversation if the user entered text.
c. It displays an input box for the user to type their message with show_text_input.
d. It displays a series of chat buttons with show_chat_buttons to allow users to control chat and send messages.
It executes the main function and displays the selected language, application title, and chat interface components on the web page.
Demo:

Wrapping up

Thank you for reading my post! Now you can build your own AI assistant or use AI Talks from any country without registration and VPN.

At the time of writing, gpt-3.5-turbo and gpt-4 are available, but don't be surprised if gpt-4 is disabled in production due to the high load.

AI Talks repo is waiting for your stars 🙂.

If you have any questions, please post them in the comments below or in the Streamlit Discord app-sharing-gallery. You can also ask questions on Telegram at AI Talks Chat or follow app updates on the AI Talks Telegram channel.

Happy Streamlit-ing! 🎈

P.S.: Check out this post in Russian on Habr, a Russian collaborative blog about IT, computer science, and anything related to the Internet.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

aura-create-free.png (1952×1376)
https://blog.streamlit.io/content/images/2023/06/aura-create-free.png#border


aura-credentials.png (1195×898)
https://blog.streamlit.io/content/images/2023/06/aura-credentials.png#border


Text-Summarization-App_scaling-0.3_fps-20_speed-10.0_duration-0-67.gif (695×547)
https://blog.streamlit.io/content/images/2023/06/Text-Summarization-App_scaling-0.3_fps-20_speed-10.0_duration-0-67.gif


Bildschirmfoto-2023-06-15-um-21.55.33.png (1506×1432)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-15-um-21.55.33.png#border


Detecting fake images with a deep-learning tool
https://blog.streamlit.io/detecting-fake-images-with-a-deep-learning-tool/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

By Kanak Mittal
Posted in Advocate Posts, April 11 2023
Why did I decide to make it?
Step 1. Collecting the dataset
Step 2. Preprocessing images
Step 3: Splitting the dataset
Step 4. Designing the model architecture
Step 5. Training the model
Step 6. Evaluating the model
Step 7. Building the UI
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Kanak, and I’m a Cloud Quality and Reliability Intern at Zscaler. The world of AI never ceases to amaze me. I enjoy learning about new technologies and tools that can help me solve complex problems and make data-driven decisions. Last year, during a hackathon, I built Deforgify—an app that detects fake images. It received a $1,000 grant from the 1517 community!

In this post, I’ll walk you through the seven steps of making it:

Collecting the dataset
Preprocessing images
Splitting the dataset
Designing the model architecture
Training the model
Evaluating the model
Building the UI

Want to jump right in? Here's the app and the repo code.

But first, I’d love to share with you…

Why did I decide to make it?

Generative Adversarial Networks (GANs) have been successful in deep learning, particularly in generating high-quality images that are indistinguishable from originals. However, GANs can also generate fake faces that deceive both humans and machine learning (ML) classifiers. There are many tutorials on YouTube that show how to use software such as Adobe Photoshop to create synthetic photographs—to be easily shared on social media and used for defamation, impersonation, and factual distortion.

I created Deforgify to leverage the power of deep learning in distinguishing real images from fake ones. This means that if someone were to Photoshop your face onto someone else's body, Deforgify would evaluate it and tag it as fake in a fraction of a second!

Now, let’s build the app step-by-step.

Step 1. Collecting the dataset

Download the dataset of real and fake faces from Kaggle. It has 1,288 faces—589 real and 700 fake ones. The fake faces were generated using StyleGAN2, which presents a harder challenge to classify them correctly (even for the human eye):

Step 2. Preprocessing images

To build an effective neural network model, you must carefully consider the input data format. The most common parameters for image data input are the number of images, the image's height and width, and the number of channels. Typically, there are three channels of data corresponding to the colors Red, Green, and Blue (RGB), and the pixel levels are usually [0,255].

Make a function that accepts an image path, reads it from the disk, applies all the pre-processing steps, and returns it. Use the OpenCV library for reading and resizing images, and NumPy for normalization:

def read_and_preprocess(img_path):
		# reading the image
    img = cv2.imread(img_path, cv2.IMREAD_COLOR)
		# resizing it to 128*128 to ensure that the images have the same size and aspect ratio. (IMAGE_SIZE is a global variable)
    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
		# convert its datatype so that it could be normalized
    img = np.array(img, dtype='float32') 
		# normalization (now every pixel is in the range of 0 and 1)
    img = img/255 
    return img

💡
PRO TIP: Tensorflow has an in-built flow_from_directory() method that provides a great abstraction to combine all these steps. To learn more about image pre-processing, read this amazing post by Nikhil.
Step 3: Splitting the dataset

Splitting the dataset is an important but often overlooked step in the ML process. It helps prevent overfitting, which occurs when a model learns the noise in the training data instead of the underlying pattern. This can result in a model that performs well on the training data but poorly on the testing data. By splitting the dataset, you can evaluate the model's performance on unseen data and avoid overfitting.

Another reason for splitting is to ensure that the model is generalizable. A model that is trained on a specific dataset may not perform well on new, unseen data. By splitting the dataset into training and testing sets, you can train the model on one set of data and evaluate its performance on another set, ensuring that it can handle new data.

I did 80% training, 10% testing, and 10% validation data split in such a way that:

The samples were shuffled
The ratio of each class was maintained (stratify)
Every time the data was split, it generated the same samples (random-state)

Stratifying while splitting data ensures that the distribution of the target classes in the training and testing sets is representative of the overall population.

For example, let's say you have a dataset of 1,000 individuals, 700 of which are men and 300 are women. You want to build a model to predict whether an individual has a certain medical condition or not. If you randomly split the data into a 70% training and 30% testing split without stratifying, it's possible that the testing set could have significantly fewer women than the training set. This could result in the model being biased towards men, leading to poor performance when predicting the medical condition in women.

On the other hand, if you stratify the data based on the gender variable, you ensure that both the training and testing sets have the same proportion of men and women. For instance, you could stratify the data so that the training set has 490 men and 210 women, and the testing set has 210 men and 90 women. This ensures that your model is trained on a representative sample of both genders, and it should therefore be more accurate when predicting the medical condition for both men and women.

Using the random_state parameter ensures that the same data is obtained each time you split the dataset. This lets you replicate your analysis and results, which is crucial for reproducibility. Without setting a random seed, each split of the data may be different, leading to inconsistent results.

Setting a random state is also important when you're sharing your work with others or comparing your results. If different individuals are using different random seeds, they may end up with different results even if they use the same code and dataset.

Step 4. Designing the model architecture

It’s possible that a model architecture, which has worked well on one problem statement, could work well on other problem statements as well. This is because the underlying ML concepts and techniques are often applicable across different domains and problem statements.

After reading numerous research papers and experimenting, I have designed my own model architecture that I use across all my projects (with minor tweaks):

It consists of a sequential model with five convolutional layers and four dense layers.
The first convolutional layer has 32 filters and a kernel size of 2x2.
At each subsequent convolutional layer, the number of filters is doubled, and the kernel size is incremented by one.
To reduce overfitting and computational costs, max-pooling layers are introduced after each convolutional layer.
The output from the convolutional layer is flattened and passed to the dense layers.
The first dense layer has 512 neurons, and the number of neurons is halved in the next two dense layers.
Dropout layers are introduced throughout the model to randomly ignore some neurons and reduce overfitting.
ReLU activation is used in all layers except for the output layer, which has two neurons (one for each class) and softmax activation.

Step 5. Training the model

Compiling an ML model involves configuring its settings for training. The three key settings to consider are:

The loss function: It measures the difference between the predicted output and the actual output of the model. The choice of loss function depends on the type of problem being solved. Since you're performing classification and your training labels are represented in categorical format (0,1), sparse_categorical_crossentropy is the logical choice.
The optimizer: It updates the weights of the model during training to minimize the loss function. Some popular optimizer algorithms include stochastic gradient descent, Adam, and RMSprop. The choice of optimizer depends on the complexity of the problem and the size of the dataset. The Adam optimizer works well in most cases, so "when in doubt, go with Adam!"
The evaluation: It measures the performance of the model during training and validation. Since your dataset is pretty balanced, there is no harm in going with accuracy.

Hyperparameters such as the learning rate, batch size, number of epochs, and regularization parameters can also be adjusted during model compilation.

The two optional steps that help improve the model’s performance and save training time are:

Early stopping: It prevents overfitting and reduces training time by monitoring the performance of the model on a validation set during training. It stops the training process once the model's performance on the validation set starts to decrease, indicating that it’s reached its optimal performance. Early stopping can save time and computational resources and prevent the model from overfitting to the training data. You’re going to configure early stopping to monitor minimum validation loss with the patience of 10, which means that if the validation loss doesn’t reduce for 10 continuous epochs, the model training will be stopped:

from tensorflow.keras.callbacks import EarlyStopping
earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)


Checkpointer: It saves the model's weights at specific intervals during training. This enables the model to resume training from where it stopped in the event that the process is halted due to an error or other issue. The checkpointer stores the weights in a file that can be loaded later to resume training. Checkpointing is particularly beneficial for large models that take a long time to train, as it allows the training process to continue without starting over. To set the checkpointer to save only the best model (i.e., one with the least validation loss), follow these steps:

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath="fakevsreal_weights.h5", verbose=1, save_best_only=True)


You’re now ready to train your model! Remember to pass in all the parameters that you have configured:

history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=32, shuffle=True, callbacks=[earlystopping, checkpointer])

Step 6. Evaluating the model

Evaluating the performance of an ML model is crucial to assess its ability to generalize to new, unseen data. To reload it, use the load_model() function from the Keras library.

model = tf.keras.models.load_model('fakevsreal_weights.h5')


Make predictions on the testing data, evaluate its performance, and compute its accuracy, precision, recall, and F1-score using the confusion matrix:

Here is the complete classification report:

Step 7. Building the UI

The model's performance evaluation alone may not be sufficient to showcase its capability. End users might not have the necessary skills or time to write code and use the trained model. One solution is to build a user interface (UI) that allows users to interact with the model. However, this requires a different skillset that many users may not possess.

This is where Streamlit comes in! 🎈

With just 12 lines of code, you can create an interactive Streamlit app using Streamlit (this will showcase your skills and make your models accessible to a wider audience):

import streamlit as st
# util is a custom file that includes steps to read the model and run predictions on an image
import util

st.image('Header.gif', use_column_width=True)
st.write("Upload a Picture to see if it is a fake or real face.")
file_uploaded = st.file_uploader("Choose the Image File", type=["jpg", "png", "jpeg"])
if file_uploaded is not None:
    res = util.classify_image(file_uploaded)
    c1, buff, c2 = st.columns([2, 0.5, 2])
    c1.image(file_uploaded, use_column_width=True)
    c2.subheader("Classification Result")
    c2.write("The image is classified as **{}**.".format(res['label'].title()))


Here is the result:

But that's not all!

Deploying an ML app on the cloud can be a daunting task, even for experienced data scientists and developers. Fortunately, Streamlit makes it easy to deploy apps to their Streamlit Community Cloud platform with just a few clicks, without any complex configurations or infrastructure setup. To get started, create an account, obtain your app's code, and follow these steps.

Wrapping up

Thank you for reading my post! You learned how to make an image classifier that can distinguish between real and fake images, built a UI to showcase your model's capability to the world using Streamlit, and deploy it on the Streamlit Community Cloud.

If you have any questions, please post them in the comments below or contact me on Twitter, LinkedIn, or GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Bildschirmfoto-2023-06-16-um-00.13.48.png (1424×1450)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-16-um-00.13.48.png


Create an animated data story with ipyvizzu and Streamlit
https://blog.streamlit.io/create-an-animated-data-story-with-ipyvizzu-and-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Create an animated data story with ipyvizzu and Streamlit

A tutorial on using ipyvizzu and ipyvizzu-story

By Peter Vidos
Posted in Advocate Posts, April 20 2023
How to use ipyvizzu
Gathering the data
Building your first chart
How to build a data story with ipyvizzu-story
Adding more slides
Comparing scenarios
Showing growth and loss components
Aggregating and comparing scenarios
How to embed the story in a Streamlit app
How to use Streamlit's input widgets for interactive storytelling
Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit community! 👋

I'm Peter Vidos, the CEO and Founder of Vizzu. Our cutting-edge open-source JavaScript library makes it easy to create animated charts and data stories. To help data scientists share their insights, we created a Python API called ipyvizzu and its storytelling extension, ipyvizzu-story. I'm excited to introduce these tools and guide you in building amazing animated data stories in Streamlit!

This tutorial covers:

How to use ipyvizzu
How to build a data story with ipyvizzu-story
How to embed the story in a Streamlit app
How to use Streamlit's input widgets for interactive storytelling

If you can't wait to try it out, here's the app and the repo.

But first, let's answer a few questions you might have...

How to use ipyvizzu

ipyvizzu is an IPython extension of the open-source Vizzu JavaScript charting library. It uses a single logic to create any chart, allowing it to interpolate between the charts expressed on its interface. In essence, it's a generic chart morpher.

The animate method lies at the heart of ipyvizzu. It has three arguments: one for setting the data being visualized, one for configuring the chart, and one for styling. Optional arguments can also describe how ipyvizzu should animate to a certain state.

When describing a chart on ipyvizzu's interface, it'll draw a static chart. Then, if you describe another chart, the original chart will morph into the new one. This morphing can involve small changes, such as zooming in on certain parts, or more profound alterations, like changing geometrical elements, coordinate systems, and adding or removing categories or values.

Regardless of the changes, ipyvizzu will automatically create an animation to connect your charts, making it easy to follow. (We're trying hard to make it that way 😉.) Ipyvizzu can be installed from PyPI or Conda, which works within various notebooks and other environments. (Read more in our docs.)

Now, let's get some data and start building!

Gathering the data

The data used in this analysis comes from the United Nation's World Population Projections, a comprehensive database containing population data for every country from 1950 to 2100.

The UN categorizes countries into six regions that largely correspond to continents: Africa, Asia, Europe, Latin America and the Caribbean (LAC), Northern America, and Oceania. In this analysis, we'll focus on the population of these regions.

The initial dataframe sample below shows three forecasted series in the last three columns: Medium (in the Population column), High, and Low. While the past values are identical across the three series, the future values differ:

Year	Region	Period	Population	High	Low
2010	Africa	Past	1 041 484 014	1 041 484 014	1 041 484 014
2015	Africa	Past	1 185 462 534	1 185 462 534	1 185 462 534
2020	Africa	Past	1 344 069 830	1 344 069 830	1 344 069 830
2025	Africa	Future	1 512 428 655	1 520 384 567	1 504 473 149
2030	Africa	Future	1 692 186 171	1 720 044 019	1 664 337 234
Building your first chart
chart.animate(data,
   Config({
            'x': 'Year',
            'y': 'Population',
            'title': 'The Population of the World 1950-2100'
        })
)


I've made some changes to the styling (not shown above) to keep the focus on the chart configuration for now:

Notice that ipyvizzu automatically aggregates the population values per year. If you call the animate method again with a different configuration—by adding Region to the y-axis and the color scale—magic happens:

chart.animate(
   Config({
            'y': ['Population','Region'],
            'color': 'Region',
            'title': 'The Population of Regions 1950-2100',
        })
)


To zoom in, use a filter on the chart data. The filter is a JavaScript expression that only shows items on the chart that match the expression. Let's zoom in on past data:

chart.animate(
    Data.filter("record.Period == 'Past'"),
    Config({'title': 'The Population of Regions 1950-2020'}),
)


How to build a data story with ipyvizzu-story

ipyvizzu-story is an extension of ipyvizzu that enables embedding charts into an interactive data story. This story can be presented on the fly or shared as an interactive HTML.

ipyvizzu-story adds buttons underneath the charts, enabling viewers to switch between them intuitively. Navigation can also be done using PgUp, PgDn buttons, arrow keys, or a clicker. With the full-screen option in the bottom right corner, you have a presentation tool within your Streamlit app.

The syntax of ipyvizzu-story is very similar to ipyvizzu's. With ipyvizzu-story, you have a Story object containing all of the data shown in the story and the charts created based on that data. These are arranged into Slides and Steps.

Slides can contain one or more Steps.

A Step (and a single-Step Slide) is essentially the same as the Chart object in ipyvizzu, with some minor but important differences (for now):

All of the data must be added to the story at initialization and can be filtered at each Step throughout the Story.
Animation options are not available.

In the case of a Slide with multiple Steps, all but the last Steps are interim charts that connect a Slide with a previous Slide. The animation will only stop when the Story is played at the last Step.

Here are the three charts created so far, with a small update. The story starts with a filter already applied on the first slide to show only the data for the past, as it makes more sense from a storytelling aspect. Click on the buttons below the chart to explore this mini-story.

Adding more slides

Let's explore some basic features of ipyvizzu-story by adding more slides to the data story. Below is the code for four additional slides, followed by another interactive story at the end of the section that begins with the last slide of the previous story, making navigation easier.

Let's add some slides to help viewers better understand the population dynamics for these regions. First, let's change the geometry to the area so that trends are easier to spot:

slide2 = Slide(
    Step(
        Config({'geometry': 'area'})
))

story2.add_slide(slide2)


Then, use a nifty feature of ipyvizzu called splitting to show the components of a stacked chart (in this case, the regions) side-by-side:

slide3 = Slide(
    Step(
        Config({'split': True}),
))


Another intriguingly simple option in ipyvizzu and ipyvizzu-story is the ability to set the alignment of the chart to "stretch". This will result in the chart showing percentages instead of values. Additionally, you should switch off the splitting from the previous slide in the same step:

slide4 = Slide(
    Step(
        Config({
            'split': False, 
            'align':'stretch',
            'title': 'The Population of Regions 1950-2100 (%)'
})))


Finally, let's zoom in on one of the regions, Africa, using the Data.filter method. To make it easier to understand, we'll use a slide with two steps. First, we switch back to values with ‘align’:’min’, and then we apply the filter:

slide5 = Slide()

slide5.add_step(
    Step(
        Config({
            'align':'min'
})))

slide5.add_step(
    Step(
        Data.filter("record.Region == 'Africa'"),
        Config({
            'title': 'The Population of Africa 1950-2100'
})))


And here you go!

Comparing scenarios

One of the greatest benefits of using animated charts is the intuitive connection between different data set views. This feature works exceptionally well for all of the transitions you've seen so far. Another significant use case for this feature is when you want to compare scenarios. Fortunately, the U.N. provides different forecasts for how the population will change in the future.

Up to this point, we have only worked with the Medium scenario. Now, it's time to experiment with the High and Low scenarios. To do so, change the value on the y-axis:

slide2 = Slide(
    Step(
        Config({
            'y': ['High','Region'],
})))

slide3 = Slide(
    Step(
        Config({
            'y': ['Low','Region'],
})))


Let's check the story!

The first slide repeats the last slide from the previous section, with a minor change. Can you spot it?

The only change I made was fixing the range of the y-axis to 6 billion. It's easier to compare the values in different scenarios this way, but I had to switch off ipyvizzu's default responsive range—which works great in many other cases. Here are the code snippet and a short GIF to compare the same slides with the two settings:

Config({y': { "range": {"max": 6000000000} }})


Showing growth and loss components

With ipyvizzu's animated transitions, you can dig a little deeper while keeping the context and helping the audience follow along. But you naturally have to have the desired depth within the data.stitutes the growth and decline in the forecasted population statistics, enriching the data by adding the number of births, deaths, and the positive/negative net migration.

As you can see in the example below, I have added an additional dimension called "Category" and renamed "Population" to "Medium" for easier comprehension. Note that deaths and negative net migration are represented as negative numbers:

Year	Region	Period	Category	Medium	High	Low
2010	Africa	Past	Deaths	- 52 967 312	- 52 967 312	- 52 967 312
2015	Africa	Past	Deaths	- 53 597 303	- 53 597 303	- 53 597 303
2020	Africa	Past	Deaths	- 58 545 143	- 58 545 143	- 58 545 143
2025	Africa	Future	Deaths	- 60 338 052	- 59 286 873	- 61 389 558
2030	Africa	Future	Deaths	- 65 556 853	- 63 865 647	- 67 249 451

To help viewers understand how these factors contribute to the population, I want the sources of growth to appear above the chart and deaths and negative net migration to appear below the x-axis (represented by negative numbers).

To properly show these extra categories on the chart, use the color scale—just like you did when you showed the regional composition of the world population before. But you'll also need to update the color palette for two reasons:

Not to confuse the viewers by using the same colors as the regions
To keep the components beneath the x-axis visible by giving the region (in this case, Africa) a semi-transparent color

Set the palette using the Style argument. The third color in this list corresponds to Africa. To adjust transparency, change the last two bits of the color code to 20 instead of the default FF value:

Style({ 'plot.marker.colorPalette': '#FF8080FF #808080FF #FE7B0020 #60A0FFFF #80A080FF' })


Add a slide that includes two steps. In the first step, show the data with the applied filter set to include only first births and positive net migration. In the second step, show the remaining two categories with the filter changed accordingly:

slide2.add_step(Step(
        Data.filter('record.Region === "Africa" && (record.Category === "Population" || record.Category === "Migration+" || record.Category === "Births")'),
        Config(
        {
            'y':['Medium','Category'],
            'color': ['Category']
})))

slide2.add_step(Step(
        Data.filter('record.Region === "Africa"'),
        Config({'title': 'Adding sources of gain and loss to the mix '})
))


Here's the new story snippet, starting where you last left off:

The positive and negative net migration is so small compared to all other factors that they can't be seen on the chart. But they're there—you'll see it in the next step. 😉

Aggregating and comparing scenarios

After displaying the contributing factors on a chart, you can aggregate the projected births, deaths, and net migration for the period between 2020 and 2100. Comparing each scenario will help you understand their differences better.

Just filter out the Population data and all information about the past. Remove the Year category, and ipyvizzu will automatically aggregate the values. Place the value on the x-axis instead of the y-axis to create four bar charts. To compare the High and Low predictions, change this value and visually compare the scenarios (I fixed the x-axis range to make the comparison easier).

Click through the slides to see the fundamental differences between the Medium, High, and Low population predictions:

Yep, the scenarios differ mostly by the number of projected Births.

How to embed the story in a Streamlit app

To embed the ipyvizzu-story in Streamlit, use the same code that you would use in a notebook within the .py file that serves as the source of your app (import a few more packages in the beginning). No need to call the story.play() method—Streamlit will handle that for you:

from streamlit.components.v1 import html
import ssl
import streamlit as st 
import pandas as pd
from ipyvizzu import Data, Config, Style
from ipyvizzustory import Story, Slide, Step

ssl._create_default_https_context = ssl._create_unverified_context


The only minor difference compared to notebooks is that, in Streamlit, the size of the story must be set in pixels (in notebooks, you can also use percentages and other metrics):

story.set_size(750, 450)


Generate the HTML containing your story by adding the following snippet:

html(story._repr_html_(), width=750, height=450)


And voilá!

It's worth noting that if you move between the first and last slides of this story, then ipyvizzu will fade as there aren't any data points that are the same in these two views.

How to use Streamlit's input widgets for interactive storytelling

It's time to use Streamlit's awesome possibilities! Let's add a couple of input widgets to make the story interactive. Users should be able to select the region they want to zoom in on to check the detailed forecasts. You can provide this option by creating a dropdown using st.selectbox:

regions = df['Region'].unique()
sel_region = st.selectbox('Select region', list(regions))


Next, implement this choice throughout the story. The biggest challenge here is parameterizing the filters to zoom into the selected region and applying the corresponding color palettes and axis ranges.

After the user selects a region with the select box, the story will regenerate and reset to the first slide. This is the default behavior of ipyvizzu-story after loading. But, since the first five slides are the same regardless of the selected region, it makes sense to enable the user to skip these standard slides. Just add st.checkbox and set its default value to "False":

skip_intro = st.checkbox(
    'Skip intro slides', value=False
)


It's easy to implement this into the story. Simply add the code that creates the first slides into an if statement in such a way that these slides are only generated if the skip_intro value is false:

if not skip_intro :
    slide1 = Slide(
        Step(...


If you check the code of the final app, you'll notice that we used the condition the other way around, so it first checks if the skip_intro is True. This is because if it's True, you must change the color palette to use the colors you selected for the regions before the first slide was played.

Lastly, ipyvizzu-story has another great feature.

You can export the story as an interactive HTML file that includes the slides and data. This file can be sent via email, embedded in a web page (like this blog post 😊), or served from any web server. To make it easy for viewers to use this feature, add a button with just one line of code. Use st.download_button to export the story into an HTML file that includes the name of the selected region.

st.download_button('Download HTML export', story.to_html(), file_name=f'world-population-story-{sel_region}.html', mime='text/html')


And here you go! 🙂

Wrapping up

We've covered everything you need to know to create amazing animated data stories with ipyvizzu and ipyvizzu-story in Streamlit. We hope this tutorial has empowered you to take your data storytelling to the next level. Remember, you can always reach out to the Vizzu community and team for help on your data journey. Join our Slack workspace to start collaborating and sharing your data stories! If you have any questions, please post them in the comments below or contact me on GitHub, Twitter, or LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Bildschirmfoto-2023-05-31-um-23.09.30.png (1830×1984)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-05-31-um-23.09.30.png#browser


Bildschirmfoto-2023-05-31-um-23.13.00.png (1198×874)
https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-05-31-um-23.13.00.png#border


data-editor-add-delete-10.44.28-AM-1.gif (899×605)
https://blog.streamlit.io/content/images/2023/02/data-editor-add-delete-10.44.28-AM-1.gif#browser


quickstart-app_scaling-0.3_fps-20_speed-10.0_duration-0-49.gif (677×552)
https://blog.streamlit.io/content/images/2023/05/quickstart-app_scaling-0.3_fps-20_speed-10.0_duration-0-49.gif


st-status-expand-output.gif (1189×929)
https://blog.streamlit.io/content/images/2023/09/st-status-expand-output.gif


Screenshot-2566-05-25-at-10.02.18.png (2234×1336)
https://blog.streamlit.io/content/images/2023/05/Screenshot-2566-05-25-at-10.02.18.png


blog-outline-app_scaling-0.3_fps-20_speed-10.0_duration-0-44.gif (677×552)
https://blog.streamlit.io/content/images/2023/06/blog-outline-app_scaling-0.3_fps-20_speed-10.0_duration-0-44.gif


schematic-1.jpeg (2000×1710)
https://blog.streamlit.io/content/images/2023/05/schematic-1.jpeg


Streamlit App Starter Kit: How to build apps faster
https://blog.streamlit.io/streamlit-app-starter-kit-how-to-build-apps-faster/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit App Starter Kit: How to build apps faster

Save 10 minutes every time you build an app

By Chanin Nantasenamat
Posted in Tutorials, September 27 2022
What is the Streamlit App Starter Kit?
How to use the Streamlit App Starter Kit
Wrapping up
Contents
Share this post
← All posts

To build a Streamlit app you’d typically follow these steps:

Install prerequisite libraries by specifying library names in requirements.txt
Customize the theme via .streamlit/config.toml (optional)
Create an app file streamlit_app.py
Inside the app file, call import streamlit as st
Specify the app tasks (e.g. read a CSV, perform data wrangling, display a scatter plot, train an ML model, etc.)

Every step takes only minutes, but over time it can amount to hours—or even days! 😅

In this article, you’ll learn how to save 10 minutes every time you build an app:

What is the Streamlit App Starter Kit?
How to use the Streamlit App Starter Kit
👉
NOTE: We recommend having a basic knowledge of Python and Streamlit (learn it by completing the #30DaysOfStreamlit challenge), plus a GitHub and a Streamlit Community Cloud accounts.

Let’s dive right in!

What is the Streamlit App Starter Kit?

The Streamlit App Starter Kit has the following files:

app-starter-kit/
├─ .streamlit/
│  ├─ config.toml
├─ README.md
├─ packages.txt (optional)
├─ requirements.txt
├─ streamlit_app.py


This is what it looks like in a GitHub repository:

It contains:

.streamlit/config.toml—a configuration file with parameters for customizing your app’s theme:

[theme]
primaryColor="#F63366"
backgroundColor="#FFFFFF"
secondaryBackgroundColor="#F0F2F6"
textColor="#262730"
font="sans serif"


README.md—a README file with a project description:

# Name of Streamlit App

Description of the app ...

## Demo App

[![Streamlit App](<https://static.streamlit.io/badges/streamlit_badge_black_white.svg>)](<https://share.streamlit.io/dataprofessor/st-app/>)

## Section Heading

This is filler text. Please replace this with the text for this section.

## Further Reading

This is filler text. Please replace this with explanatory text about further relevant resources for this repo.
- Resource 1
- Resource 2
- Resource 3


packages.txt—a list of Linux tools and packages to install (blank by default). Go ahead and populate it with the package names you want to install—one name per line.

requirements.txt—a list of Python libraries to install. By default, the Streamlit App Starter Kit lists only streamlit. It’ll install the latest version:

streamlit

If you want a specific version—like 1.13.0—do the following:

streamlit==1.13.0

Add some Python libraries:

streamlit==1.13.0
pandas==1.3.5
scikit-learn==1.1.0

streamlit_app.py—the Streamlit app:

import streamlit as st

st.title('🎈 App Name')

st.write('Hello world!')

How to use the Streamlit App Starter Kit

The Streamlit App Starter Kit is available as a GitHub template. Clone it to your repo and use it to make your own Streamlit app:

Want to customize the contents of the app files? Use widgets to accept user input and display the output results (read more about widgets in our docs).

Finally, deploy your app with the Streamlit Community Cloud or some other cloud service provider! 🎉

Wrapping up

Congratulations! You’ve used the Streamlit App Starter Kit to make your app-making process faster. 💨

If you like to work with command line interfaces, check out the streamlit-kickoff-cli developed by our very own Arnaud Miribel. And if you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Building a PivotTable report with Streamlit and AG Grid
https://blog.streamlit.io/building-a-pivottable-report-with-streamlit-and-ag-grid/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building a PivotTable report with Streamlit and AG Grid

How to build a PivotTable app in 4 simple steps

By Pablo Fonseca
Posted in Advocate Posts, March 7 2023
Some context on data
Step 1. How to load and display data
Step 2. How to configure the grid using gridOptionsBuilder
Step 3. How to configure the grid pivot mode
Step 4. How to add grouping on rows and columns
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Pablo, and I'm the creator of the Streamlit AgGrid component. A little about me: I started coding 25 years ago and never stopped. Currently, I work as a portfolio manager for renewable energy development in Brazil.

I needed to summarize the data for the available wind-farm energy in a PivotTable report: power sales, purchases, and expected generation. If the sold amount were more than the expected generation, it’d signal the need to buy power from other producers. So I built a simple dashboard with Streamlit and streamlit-aggrid!

In this post, I’ll show you:

How to load and display data
How to configure the grid using gridOptionsBuilder
How to configure the grid pivot mode
How to add grouping on rows and columns

Want to skip reading and try it out? Here's a sample app and a repo code.

But first, let’s talk about…

Some context on data

In Brazil, consumers buy power in advance and sign contracts for future power delivery (PPA—Power Purchase Agreement). The energy portfolio managers need to calculate the energy balance to mitigate the risk and protect the revenue. Is it too short or too long?

Much like a bank statement, the energy balance is a total of transactions. Sales can deplete it, while purchases can add to it.

Here is a sample of fictional balance data for seven wind farms (located across five Brazilian states) that have signed PPAs with customers for their 2023 power supply:


state	powerPlant	recordType	buyer	referenceDate	hoursInMonth	volumeMWh
CE	Ocean Breeze Energy Park	Power Sale	ChargeMax Limited Liability	2023-06-01	720	-158.221
Bahia	Skyline Wind Ranch	Power Sale	PowerPulse Energy Inc.	2023-08-01	744	-108.894
CE	Ocean Breeze Energy Park	Power Sale	PowerPlus Enterprises	2023-03-01	744	-371.49
CE	Windfarm at Sunrise	Power Sale	SparkPlug Energy Ltd.	2023-02-01	672	-172.012
CE	Windward Heights Wind Farm	Power Sale	SparkPlug Energy Ltd.	2023-07-01	744	-271.877
CE	Windfarm at Sunrise	Power Sale	ShockPower Ltd.	2023-04-01	720	-366.159
CE	Prairie Wind Power Plant	Power Sale	PowerPulse Energy Inc.	2023-09-01	720	-76.8527
CE	Prairie Wind Power Plant	Power Sale	EnergyEmpire Corp.	2023-05-01	744	-926.426
CE	Prairie Wind Power Plant	Expected Generation		2023-08-01	744	9448.8
SP	Coastal Wind Energy Station	Power Sale	VoltWatt Inc.	2023-03-01	744	-477.762
Step 1. How to load and display data

Loading data is straightforward. Just use pandas read_csv to load it from a text file (or any other preferred method). To render it, use streamlit-aggrid with the default parameters. Create a file named app.py in the same folder where you downloaded the data and add this code:

#app.py
import streamlit as st
import pandas as pd
from st_aggrid import AgGrid

@st.cache_data()
def load_data():
    data = pd.read_csv('./data.csv', parse_dates=['referenceDate'])
    return data

data = load_data()

AgGrid(data, height=400)


Launch your dashboard with streamlit run app.py. Your browser should open with the sample data loaded in AgGrid. The app will load data and display it using default configurations. They're nice but could be better formatted. Let’s use GridOptionsBuilder to customize it.

Step 2. How to configure the grid using gridOptionsBuilder

Update your app.py file as follows:

#app.py
import streamlit as st
import pandas as pd
from st_aggrid import AgGrid, GridOptionsBuilder  #add import for GridOptionsBuilder

@st.cache_data()
def load_data():
    data = pd.read_csv("./data.csv", parse_dates=["referenceDate"])
    return data

data = load_data()

gb = GridOptionsBuilder()

# makes columns resizable, sortable and filterable by default
gb.configure_default_column(
    resizable=True,
    filterable=True,
    sortable=True,
    editable=False,
)

#configures state column to have a 80px initial width
gb.configure_column(field="state", header_name="State", width=80)

#configures Power Plant column to have a tooltip and adjust to fill the grid container
gb.configure_column(
    field="powerPlant",
    header_name="Power Plant",
    flex=1,
    tooltipField="powerPlant",
)

gb.configure_column(field="recordType", header_name="Record Type", width=110)

gb.configure_column(
    field="buyer", header_name="Buyer", width=150, tooltipField="buyer"
)

#applies a value formatter to Reference Date Column to display as a short date format.
gb.configure_column(
    field="referenceDate",
    header_name="Reference Date",
    width=100,
    valueFormatter="value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''",
)

#Numeric Columns are right aligned
gb.configure_column(
    field="hoursInMonth",
    header_name="Hours in Month",
    width=50,
    type=["numericColumn"],
)
#The last column is the value column and will be formatted using javascript number.toLocaleString()
gb.configure_column(
    field="volumeMWh",
    header_name="Volume [MWh]",
    width=100,
    type=["numericColumn"],
    valueFormatter="value.toLocaleString()",
)

#makes tooltip appear instantly
gb.configure_grid_options(tooltipShowDelay=0)
go = gb.build()

AgGrid(data, gridOptions=go, height=400)


The grid should look better now!

Step 3. How to configure the grid pivot mode

Now let’s make the grid pivot over the referenceDate column. Add a checkbox to your app:

shouldDisplayPivoted = st.checkbox("Pivot data on Reference Date")


Change the referenceDate column definition to enable pivoting:

gb.configure_column(
    field="referenceDate",
    header_name="Reference Date",
    width=100,
    valueFormatter="value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''",
    pivot=True # this tells the grid we'll be pivoting on reference date
)


Configure the aggregation function on the volumeMWh (the value) column (values should sum up for a given month):

gb.configure_column(
    field="volumeMWh",
    header_name="Volume [MWh]",
    width=100,
    type=["numericColumn"],
    valueFormatter="value.toLocaleString()",
    aggFunc="sum" # this tells the grid we'll be summing values on the same reference date
)


Finally, enable pivotMode when the checkbox is on:

gb.configure_grid_options(
    pivotMode=shouldDisplayPivoted # Enables pivot mode
    )


Here is the complete code for this section:

import streamlit as st
import pandas as pd
from st_aggrid import AgGrid, GridOptionsBuilder

@st.cache_data()
def load_data():
    data = pd.read_csv("./data.csv", parse_dates=["referenceDate"])
    return data

data = load_data()

shouldDisplayPivoted = st.checkbox("Pivot data on Reference Date")

gb = GridOptionsBuilder()

gb.configure_default_column(
    resizable=True,
    filterable=True,
    sortable=True,
    editable=False,
)
gb.configure_column(field="state", header_name="State", width=80)

gb.configure_column(
    field="powerPlant",
    header_name="Power Plant",
    flex=1,
    tooltipField="powerPlant",
)
gb.configure_column(field="recordType", header_name="Record Type", width=110)

gb.configure_column(
    field="buyer", header_name="Buyer", width=150, tooltipField="buyer"
)

gb.configure_column(
    field="referenceDate",
    header_name="Reference Date",
    width=100,
    valueFormatter="value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''",
    pivot=True,
)
gb.configure_column(
    field="hoursInMonth",
    header_name="Hours in Month",
    width=50,
    type=["numericColumn"],
)
gb.configure_column(
    field="volumeMWh",
    header_name="Volume [MWh]",
    width=100,
    type=["numericColumn"],
    aggFunc="sum",
    valueFormatter="value.toLocaleString()",
)

gb.configure_grid_options(
    tooltipShowDelay=0,
    pivotMode=shouldDisplayPivoted,
)
go = gb.build()

AgGrid(data, gridOptions=go, height=400)

Step 4. How to add grouping on rows and columns

So far, your app displays the loaded data and pivot in a single line. Let’s group it into columns using virtual columns (so they’re hidden when pivotMode it is off). Set the valueGetter property on the columns definition. In this example, Year and Year-Month columns don't exist in the original data. Create them by setting the valueGetter with a JavaScript expression for the grid:

gb.configure_column(
    field="referenceDate",
    header_name="Reference Date",
    width=100,
    valueFormatter="value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''",
    pivot=False, #remove pivoting on this column
)

#add two hidden virtual columns
gb.configure_column(
    field="virtualYear",
    header_name="Reference Date Year",
    valueGetter="new Date(data.referenceDate).getFullYear()",
    pivot=True, #allows grid to pivot on this column
    hide=True #hides it when pivotMode is off.
)

gb.configure_column(
    field="virtualMonth",
    header_name="Reference Date Month",
    valueGetter="new Date(data.referenceDate).toLocaleDateString('en-US',options={year:'numeric', month:'2-digit'})",
    pivot=True,
    hide=True
)


Use State, Power Plant, Record Type, and Buyer columns for row grouping. This will create a nice hierarchical menu. The Grid aggregates data by applying aggFunc on collapsed rows and column values. To configure this behavior, set rowGroup property on each column definition:

gb.configure_column(
    field="powerPlant",
    header_name="Power Plant",
    flex=1,
    tooltipField="powerPlant",
    rowGroup=True if shouldDisplayPivoted else False, # enable row grouping IF pivot mode is on. Could be shortened as rowgroup=shouldDisplayPivoted
)


Repeat this for the other grouping columns.

To configure the column that displays group hierarchy, set the following grid options:

gb.configure_grid_options(
    autoGroupColumnDef=dict(
        minWidth=300, 
        pinned="left", 
        cellRendererParams=dict(suppressCount=True)
    )
)


Here is the complete code for this app:

import streamlit as st
import pandas as pd
from st_aggrid import AgGrid, GridOptionsBuilder

@st.cache_data()
def load_data():
    data = pd.read_csv("./data.csv", parse_dates=["referenceDate"])
    return data

data = load_data()

shouldDisplayPivoted = st.checkbox("Pivot data on Reference Date")

gb = GridOptionsBuilder()

gb.configure_default_column(
    resizable=True,
    filterable=True,
    sortable=True,
    editable=False,
)
gb.configure_column(
    field="state", header_name="State", width=80, rowGroup=shouldDisplayPivoted
)

gb.configure_column(
    field="powerPlant",
    header_name="Power Plant",
    flex=1,
    tooltipField="powerPlant",
    rowGroup=True if shouldDisplayPivoted else False,
)
gb.configure_column(
    field="recordType",
    header_name="Record Type",
    width=110,
    rowGroup=shouldDisplayPivoted,
)

gb.configure_column(
    field="buyer",
    header_name="Buyer",
    width=150,
    tooltipField="buyer",
    rowGroup=shouldDisplayPivoted,
)

gb.configure_column(
    field="referenceDate",
    header_name="Reference Date",
    width=100,
    valueFormatter="value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''",
    pivot=False,
)

gb.configure_column(
    field="virtualYear",
    header_name="Reference Date Year",
    valueGetter="new Date(data.referenceDate).getFullYear()",
    pivot=True,
    hide=True,
)

gb.configure_column(
    field="virtualMonth",
    header_name="Reference Date Month",
    valueGetter="new Date(data.referenceDate).toLocaleDateString('en-US',options={year:'numeric', month:'2-digit'})",
    pivot=True,
    hide=True,
)

gb.configure_column(
    field="hoursInMonth",
    header_name="Hours in Month",
    width=50,
    type=["numericColumn"],
)
gb.configure_column(
    field="volumeMWh",
    header_name="Volume [MWh]",
    width=100,
    type=["numericColumn"],
    aggFunc="sum",
    valueFormatter="value.toLocaleString()",
)

gb.configure_grid_options(
    tooltipShowDelay=0,
    pivotMode=shouldDisplayPivoted,
)

gb.configure_grid_options(
    autoGroupColumnDef=dict(
        minWidth=300, 
        pinned="left", 
        cellRendererParams=dict(suppressCount=True)
    )
)
go = gb.build()

AgGrid(data, gridOptions=go, height=400)

Wrapping up

And that’s it! You now know how to use streamlit-aggrid to create a nice PivotTable report. If you have any questions, please post them in the comments below or contact me on GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

10 most common explanations on the Streamlit forum
https://blog.streamlit.io/10-most-common-explanations-on-the-streamlit-forum/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
10 most common explanations on the Streamlit forum

A guide for Streamlit beginners

By Debbie Matthews
Posted in Advocate Posts, March 9 2023
A guide for Streamlit beginners
1. Buttons aren’t stateful
2. Streamlit doesn’t render like a terminal
3. You can inject your own CSS and JavaScript
4. The files in your directory aren’t accessible to your front end implicitly
5. file_uploader doesn't save a file to your directory
6. Keys in session state go away when their associated widget isn’t rendered
7. Your local environment is not the same as your cloud environment
8. Streamlit doesn’t do that natively, but…
9. This isn't an issue with Streamlit
10. Can you please provide more information?
Wrapping up
Contents
Share this post
← All posts
A guide for Streamlit beginners

Hey, community! 👋

My name is Debbie Matthews, and I’m a moderator on Streamlit’s wonderful forum. You may have seen me around as mathcatsand, as in “math, cats, and….”

If you hang around the forum long enough, you’ll start seeing some common pain points and areas of confusion. I thought it’d be helpful for new users to know where many people trip as they get started with Streamlit.

In this post, I’ll talk about 10 of them:

Buttons aren’t stateful.
Streamlit doesn’t render like a terminal.
You can inject your own CSS and JavaScript.
The files in your directory aren’t accessible to your front end implicitly.
file_uploader doesn’t save a file to your directory.
Keys in the session state go away when their associated widget is not rendered.
Your local environment is not the same as your cloud environment.
Streamlit doesn’t do that natively, but…
This isn’t an issue with Streamlit.
Can you please provide more information?
💡
Code snippets for this post are hosted in a live app here, so feel free to open another tab or window to follow along. And if you haven't read or watched a basic introduction about getting started with Streamlit, check it out here. 
1. Buttons aren’t stateful

Buttons return True only on the page load right after their click and immediately go back to False.

If you create an if statement to check the value of a button, the body of the if statement will execute once per click of the button. The right things to include here are short messages or processes you don’t want to rerun with other user activity.

import streamlit as st

if st.button('Submit'):
    st.write('Submitted!')

if st.button('Confirm'):
    st.write('Confirmed!')


If you nest buttons, the innermost portion of the code will never execute! As soon as you click on the second button, the page will reload with the first button being False.

import streamlit as st

if st.button('First Button'):
    st.write('The first button was clicked.')
    if st.button('Second Button'):
        # This will never execute!
        st.write('The second button was clicked')


If you need your button to behave more like a checkbox, you can create a key in the session state to save that information.

import streamlit as st

# Initialize the key in session state
if 'clicked' not in st.session_state:
    st.session_state.clicked = {1:False,2:False}

# Function to update the value in session state
def clicked(button):
    st.session_state.clicked[button] = True

# Button with callback function
st.button('First Button', on_click=clicked, args=[1])

# Conditional based on value in session state, not the output
if st.session_state.clicked[1]:
    st.write('The first button was clicked.')
    st.button('Second Button', on_click=clicked, args=[2])
    if st.session_state.clicked[2]:
        st.write('The second button was clicked')

💡
Check out streamlit-extras which is a collection of many useful custom components from a variety of contributors. It includes a stateful button made by Zachary Blackwood.
2. Streamlit doesn’t render like a terminal

With every interaction on a page, Streamlit will reload the page. It’s not meant to wait for input and then proceed. It also won’t keep anything on the screen that isn’t explicitly re-rendered.

Be careful with loops and conditionals! You don’t want a while loop waiting for a user to input something. Streamlit doesn’t pause and wait for input with a new widget; it just plows through with its default value. If you have a widget inside a loop, Streamlit will try to make a new, additional widget with each loop pass. If you need to wait for a user’s selection, place a conditional on the output to check if it has the default value.

import streamlit as st

name = st.text_input('Name:')
if name != '':
    st.write(f'Hi, {name}! Nice to meet you.')


If you need to confirm the user’s selection which may be the default, you can add a confirmation button. You can require confirmation for any selection or just as a way for the user to accept the default.

import streamlit as st

# Create a key in session state to record the user's choice, defaulting to None
if 'favorite_color' not in st.session_state:
    st.session_state.favorite_color = None

# Confirmation function to record the user's choice into the favorite_color key
def confirm_color():
    st.session_state.favorite_color = st.session_state.color_picker

name = st.text_input('Name:')
if name != '':
    st.write(f'Hi, {name}! Nice to meet you.')
    st.write(f'What\\'s your favorite color?')
    # Confirmation function will run if the user changes the widget
    color = st.color_picker('Color:', key='color_picker', on_change=confirm_color)
    if st.session_state.favorite_color is None:
        # Or, Confirmation function will run if user confirms the default
        st.button('Confirm Black', on_click=confirm_color)
    else:
        st.write(f'<span style="color:{color}">Oh, nice color choice!</span>', 
            unsafe_allow_html=True)


If you have many interactive steps to display, the nested if statements can get a bit out of control. You can instead create a staging value in the session state to control how much is displayed on the page. You can use inequality as in the example below to show all former stages. Alternatively, you can use equality or elif to show only the current stage.

import streamlit as st

# Create a key in session state to track the stage
if 'stage' not in st.session_state:
    st.session_state.stage = 0

# Stage function to update the stage saved in session state
def set_stage(stage):
    st.session_state.stage = stage

st.write('Welcome! Click to begin.')
# Each button runs the Stage function, passing the stage number as an argument
st.button('Begin', on_click=set_stage, args=[1])

# Content for each stage within the body of an if statement
if st.session_state.stage > 0:
    st.write('This is stage 1. Do some things.') 
    st.button('Next', on_click=set_stage, args=[2])
if st.session_state.stage > 1:
    st.write('This is stage 2. Do some more things.')
    st.button('Finish', on_click=set_stage, args=[3])
if st.session_state.stage > 2:
    st.write('This is the end. Thank you!')
    st.button('Reset', on_click=set_stage, args=[0])


If you want a function that “adds data” with each click, you will need something in the session state that accumulates those additions. This is commonly done with if 'key' not in st.session_state: at the top of the script. This way, the “new” unmodified object is initialized only on the first load of the page. With each addition, the object doesn't get overwritten with its default value because the key already exists.

import streamlit as st
import pandas as pd

# Initialize some object in session state where you will you be storing edits
if 'df' not in st.session_state:
    st.session_state.df = pd.DataFrame({'A':[1,2,3],'B':[4,5,6],'C':[7,8,9]})

# Optional: Assign the stored value to a convenient variable for brevity in code
df = st.session_state.df

st.dataframe(df)

cols = st.columns(3)
cols[0].number_input('A',0,100,step=1, key='A')
cols[1].number_input('B',0,100,step=1, key='B')
cols[2].number_input('C',0,100,step=1, key='C')

def add_row():
    row = [st.session_state.A, st.session_state.B, st.session_state.C]
    next_row = len(st.session_state.df)
    # Make sure modifcation is performed on the object in session state
    st.session_state.df.loc[next_row] = row

st.button('Add Row', on_click=add_row)

3. You can inject your own CSS and JavaScript

HTML and CSS can be added via st.write and st.markdown (with the correct optional keyword). JavaScript requires the more robust components submodule.

Many different resources describe ways to modify the display of Streamlit. Fanilo Andrianasolo, another Streamlit Creator, has a short video explaining the basics. Here are a few examples.

Want to change the font color on your buttons, including hover and focus colors? Here's how:

import streamlit as st

st.button('Click me!')

css='''
<style>
    .stButton > button {
        color: red;
    }
    .stButton > button:hover {
        color: violet;
        border-color: violet;
    }
    .stButton > button:focus {
        color: purple !important;
        border-color: purple !important;
        box-shadow: purple 0 0 0 .2rem;
    }
</style>
'''

st.markdown(css, unsafe_allow_html=True)


Note the use of unsafe_allow_html=True when using st.markdown or st.write. This optional keyword is needed to prevent Streamlit from escaping HTML tags. If you know your CSS selectors, you can get to any element. I often use a set of containers combined with nth-of-type selections to get to a specific instance of an element.

import streamlit as st

# Layout your containers at the beginning
section1 = st.container()
section2 = st.container()
section3 = st.container()
section4 = st.container()

# Write to the different containers for your display elements
section1.subheader('Section 1')
section1.button('Button 1')

section2.subheader('Section 2')
section2.button('Button 2')

section3.subheader('Section 3')
section3.button('Button 3')

section4.subheader('Section 4')
section4.button('Button 4')

css='''
<style>
    section.main > div > div > div > div:nth-of-type(3) .stButton > button {
        color: green;
    }
    section.main > div > div > div > div:nth-of-type(3) .stButton > button:hover {
        color: violet;
        border-color: violet;
    }
    section.main > div > div > div > div:nth-of-type(3) .stButton > button:focus {
        color: purple !important;
        border-color: purple !important;
        box-shadow: purple 0 0 0 .2rem;
    }
</style>
'''

st.markdown(css, unsafe_allow_html=True)


Use the components submodule if you need to customize something that can’t be handled with pure CSS. When you insert a component, it will be contained in an iframe. Be aware that your JavaScript queries must reach outside that iframe to work as expected.

import streamlit as st

st.header('Screen Width Checker')
st.write('''<h3>The app container is <span id="root-width"></span> x 
<span id="root-height"></span> px.</h3>
''', unsafe_allow_html=True)

js = '''
<script>
    var container = window.parent.document.getElementById("root")

    var width = window.parent.document.getElementById("root-width")
    var height = window.parent.document.getElementById("root-height")

    function update_sizing(){
        width.textContent = container.getBoundingClientRect()['width']
        height.textContent = window.parent.innerHeight
    }
    update_sizing()

    window.parent.addEventListener('resize', function(event) {
        update_sizing()
    }, true);
    
</script>
'''

st.components.v1.html(js)

4. The files in your directory aren’t accessible to your front end implicitly

Users can't directly select from files on your app’s server. You can’t access files like you would on a web host.

Streamlit has a server-client structure. The files that a user can access are on their computer where they have a browser open. Streamlit will only give users access to the files you explicitly tell it to serve. If you have an image my_image.png saved in your working directory, that image cannot be accessed via <app url>/my_image.png.

When you use st.image in your app, Streamlit will create a copy of the data and make it accessible to the client's browser via a hashed file name. When using HTML or CSS in your app that contains a path to some file, you need to host that file somewhere. A file will not be accessible to the web just by being in your app directory.

In the case of HTML and CSS, you can open and read the contents of a file to inject its contents manually. The contents of your CSS file should not contain relative paths to other HTML, CSS, or image files, as these will not be accessible to the user’s client.

import streamlit as st

if 'css' not in st.session_state:
    with open('files/my_css.css', 'r') as file:
        css = file.read()
    st.session_state.css = css

css = '<style>' + st.session_state.css + '</style>'

st.button('Click me!')

st.markdown(css, unsafe_allow_html=True)


There is also a new, exciting feature in Streamlit 1.18.0: static files! If you want to make anything in your working directory web-accessible, you can use this, too. Let's say you have a background image that you want to specify in some CSS. If you turn on static hosting and put the background image in a folder named static, you can use it in your CSS. Be sure to read the linked documentation for clarification.

import streamlit as st

image = './app/static/cat_background.jpg'

css = f'''
<style>
    .stApp {{
        background-image: url({image});
    }}
    .stApp > header {{
        background-color: transparent;
    }}
</style>
'''
st.markdown(css, unsafe_allow_html=True)


Your config.toml should contain:

[server]
enableStaticServing = true


Remember to reboot your app any time you change your environment or configuration! Read more about the configuration here.

5. file_uploader doesn't save a file to your directory

The file_uploader widget returns a “file-like object,” the file's data. This object is not accessed via a name or path.

You may be familiar with a typical use case of file_uploader:

import streamlit as st
import pandas as pd

file = st.file_uploader("Choose a file:", key="loader", type='csv')

if file != None:
    df = pd.read_csv(file)
    st.write(df)


Since it is very common to specify a data file to read_csv via its path, it is easy to forget that pandas accepts either a path or a buffer. In the above example, we are passing the latter. The variable file has no “path” associated with it. You can access the file’s name via the name property inherited from BytesIO, but this is just informational. You don't use the file's name to point to its data. There are many libraries and functions that will not accept a file-like object instead of a path. Be mindful of the function you are using and always read its documentation if in doubt.

Also note that the file-like object you get requires processing to be interpreted, even if it is a simple text file.

import streamlit as st
import io

file = st.file_uploader("Choose a file:", type=['css','py'])

if file != None:
    bytes_object = file.getvalue()
    string_object = bytes_object.decode("utf-8")

    st.code(string_object)

6. Keys in session state go away when their associated widget isn’t rendered

When a key in the session state is associated with a widget, then the key will be removed from the session state when the widget is no longer rendered. This can happen if you navigate to a different page or conditionally render widgets on the same page.

Here’s a brief description of a widget’s life cycle.

At the specific line where you call upon a widget for the first time, Streamlit will create a new front-end instance of that widget. If you have specified a key, Streamlit will check if that key already exists in the session state. If that key doesn't exist, Streamlit will create one, starting with whatever default value the widget has. However, if Streamlit sees a key already, it will attach the widget to it. In this case, the widget will take on that key’s value even if it is a new widget with a specified initial value.

Example: This slider will always have a value of 1 since the widget will always attach itself to the pre-existing key.

import streamlit as st

st.session_state.my_key = 1

st.slider('Test', 0, 10, key='my_key')


Although you can edit a widget’s state by assigning different values to its key in the session state, the session state is just an intermediary. The widget will have and retain state while continually rendered on screen, even if you remove its key from the session state.

For example, this widget is and will remain stateful:

import streamlit as st

st.session_state.clear()

st.slider('Test', 0, 10, key='my_key')


However, as soon as a widget isn’t rendered (even for a single page load), Streamlit will delete all its data, including any associated key in the session state:

import streamlit as st

switch = st.radio('Choice:', [1,2])

match switch:
    case 1:
        st.checkbox('1', key='1')
    case 2:
        st.checkbox('2', key='2')

In the above example, say that 1 is selected for the radio button. While that is the case, there will be a '1' in session state duplicating the widget’s state.
As soon as a user selects 2 for the radio button, the page will reload. As Streamlit reruns the page, it will still have the '1' key in the session state. It doesn't know that the key's associated widget will not be rendered.
However, as soon as Streamlit completes rendering the page, it will see that it has information for a widget that isn't rendered. At this point, Streamlit will delete the widget information, including any key in the session state that was tied to it.

This cleanup process has particular importance to conditionally rendered widgets. It is also important for widgets meant to carry over to other pages (often in the sidebar). There is a discussion about changing this behavior and potentially changing the structure on a deeper level. For now, know that when a key is assigned to a widget, the data in the session will get deleted if you navigate away from that instance of the widget.

Two ways around this:

Copy data into a new key in the session state to have a place in the session state that is free from unintentional deletion.
Recommit your data to the session state at the top of the page. By using st.session_state.my_key = st.session_state.my_key at the top of every page, you can artificially “keep it alive.” When navigating away from a widget with key='my_key', this interrupts the cleanup process. This manual value assignment effectively detaches the key from the widget (until a widget is seen again with that key).
7. Your local environment is not the same as your cloud environment

Make sure to specify/use the right environment for any deployment and be sure your file paths are OS-agnostic.

When you deploy your app to some cloud service, a new Python environment within that cloud service will be used to run your app. It won't know about or use anything you happen to have in your local environment. You have to tell your cloud environment about all the Python packages it has to install and any additional non-Python components.

For Streamlit Cloud, the most common approach is to save a requirements.txt file at the top of your working directory. Each line in the requirements.txt file specifies a package for the cloud environment to pip install. You can also set specific versions of Python packages this way.

Example requirements.txt file:

streamlit==1.17.0
pandas
numpy


Some Python libraries require additional command line tools or software to be installed. Streamlit Cloud is a Debian-based Linux container. Extra software is installed with apt-get in a similar way to how pip installs Python packages. You need a packages.txt file in your working directory alongside your main Python file for your app. Each line in the packages.txt file specifies a binary for the cloud environment to sudo apt-get install.

Example packages.txt file:

ffmpeg
chromium
chromium-chromedriver


Since many people locally have a different environment than Linux, note that Linux is case-sensitive. The files specifying your environment must be named exactly as stated,  case included. Use forward slashes and not back slashes in your Python script when writing out file paths. Ensure all paths are given from the working directory, even for Python files in your pages folder for multipage apps.

There are other ways to specify your Python packages, as described in the documentation. For example, you can have an environment.yml file to use conda instead of using requiremnts.txt which uses pip. If you try to include both, Streamlit Cloud will only process the first one it comes across and ignore the second.

For deployment on Streamlit Cloud, here’s a related warning. If you write to a file in your script, that updated file will only live in the Debian container hosting your app. It will not save back to GitHub and will not survive a reboot of your app.

8. Streamlit doesn’t do that natively, but…

Streamlit is constantly growing and improving, so keep your eye on the road map and note the most commonly used custom components.

There are a few good places to keep your eyes open for what’s coming up to help you get a feel for where we are now. Keep an eye on the Roadmap to know what’s just around the corner in development. GitHub Issues is the official place for developers to keep track of feature requests. If you want Streamlit to do something new, check there first so you can up-vote any existing request or create a new one if no one has mentioned it yet. I like to sort the list by the most upvotes to see what’s getting traction.

Check out the Streamlit Component Community Tracker for extra features people have built. Here are a few notable packages:

streamlit-extras: a collection of many different small components from many different contributors
st-pages: settings to customize the layout of navigation in multi-page apps created by Zachary Blackwood
streamlit-webrtc: tools for real-time video/audio streams created by Yuichiro Tachibana
💡
If your application is hosted on a different computer (server) than a user’s computer (client), be careful about computer peripherals. Use Streamlit-compatible libraries. There are a lot of components to deal with audio/video input for this reason.
streamlit-folium by Randy Zwitch brings Folium to Streamlit.
streamlit-aggrid by Pablo Fonseca brings AG Grid to Streamlit.
💡
Note that Streamlit 1.18.0 introduced an experimental editable data frame as well.
9. This isn't an issue with Streamlit

If the problematic lines of code don't include anything from the Streamlit library, think carefully. Ask yourself if you need help with Streamlit or with something else.

Community members are very generous about helping out with non-Streamlit issues. However, it's best to direct your questions to the right venue. There are many useful forums on the internet with different areas of focus. You will get the best and fastest help by asking questions in a forum dedicated to your issue.

The only thing Streamlit does is provide a front end to your Python code. If you're having trouble creating a data frame from a CSV file in your working directory, you may have a pandas question rather than a Streamlit one. The most efficient path to an answer would be to seek a forum dedicated to pandas.

When you encounter difficulty with a line of code, check if any Streamlit component is involved. If not, I encourage you to try executing that bit of code without Streamlit. If appropriate, you can create and run a plain Python script or try it out in a Jupyter notebook. If something works fine in a Jupyter notebook but isn't behaving as you expect in Streamlit, that's a great question to bring to the Streamlit forum.

10. Can you please provide more information?

If you invest the time to ask your question clearly and succinctly, you'll likely save as much or more time waiting for a response.

The easier it is for community members to understand your problem, the faster you'll get a response. If you're having problems deploying, we'd like to see your GitHub repository. We want to understand how you configured your environment and check for typos. We'd like to see your terminal output from a fresh reboot to see any error messages. On the other hand, if the front end isn't displaying the way you want, we'd like to know the code you are using and a screenshot of what you are visually seeing. Explain how you expect it to look.

Screenshots of code are less helpful since we can't copy and paste those into a working snippet. Access to your complete GitHub repository can be beneficial and sometimes necessary. However, the smallest amount of code needed to reproduce the issue is always best. If we can copy-paste a snippet you provide and launch it to see your problem, that's perfect! Include your import statements as well as any files your script accesses. Provide simplified, dummy data for us to work with. Inline data is the easiest to work with, such as defining a simple data frame within your code snippet. If importing your data is part of the problem, we'd need an example data file to accompany your snippet.

If you spend the time creating a small, self-contained example of your problem, then the community can work on helping you. Otherwise, we spend a lot of time digging through your code, fabricating data, and making all sorts of guesses to fill in the gaps.

Please check out this guide on how to post effectively on the forum. I would especially like to draw your attention to the idea of a minimal, reproducible example.

Wrapping up

Thanks for reading! I hope you found some useful information that will save you time and trouble as you start with Streamlit. If you have any questions, please post them in the comments below or contact me on the Streamlit Forum. You can also find me on GitHub and LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Hackathon 101: 5 simple tips for beginners
https://blog.streamlit.io/hackathon-101-5-simple-tips-for-beginners/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Hackathon 101: 5 simple tips for beginners

Prepare to win your first hackathon!

By Chanin Nantasenamat
Posted in Tutorials, March 16 2023
What is a hackathon?
What are the benefits of participating in a hackathon?
How to get started with hackathons
5 simple tips for a successful hackathon
Tip 1. Set up your coding environment
Tip 2. Use code templates
Tip 3. Focus on the problem
Tip 4. Get unstuck
Tip 5. Create your own custom resource pack
Bonus tip: Have fun!
My recent hackathon experience
1. Create a solution to a problem
2. Make an app feature wishlist
3. Implement the app
Wrapping up
Contents
Share this post
← All posts

Have you heard about hackathons but have no idea where to start? I’ve been there. It can feel intimidating. But fear not!

In this post, I’ll outline five simple hackathon tips that will help you win your first hackathon, and I’ll also share my personal hackathon experience:

What is a hackathon?
What are the benefits of participating in a hackathon?
How to get started with hackathons
5 simple tips for a successful hackathon
My recent hackathon experience
What is a hackathon?

A hackathon is an event where people come together to work on creative software or hardware projects. Hackathons usually have a specific focus—such as building games, websites, or apps. Participants often have to complete their projects within a certain timeframe.

What are the benefits of participating in a hackathon?

Hackathons are a great way to learn new skills, meet new people, and build something cool. Even if you don't win, participating in a hackathon can be a fun and rewarding experience.

How to get started with hackathons

To start, find a hackathon that interests you. There are lots of them, so it's important you find one that matches your skills and interests. Found one? Great! Register for it and start preparing:

Read the hackathon website to get a good grasp of the scope. It’ll help you frame your project.
Brainstorm and make a list of potential projects you could work on.
Decide whether you want to use an existing skill set or to learn a new one (and apply it to your project). Note that this depends on your available time.
Choose a project and get started!
5 simple tips for a successful hackathon

There's no one-size-fits-all answer to the question of how to win a hackathon. But it’s important that you come prepared. That means, having a clear idea of what you want to accomplish (and how to get there) and having all the necessary tools and resources to hit the ground running.

Here are a few tips:

Tip 1. Set up your coding environment

Create your hackathon coding environment. Make sure it has the necessary operating system, language framework, and required libraries. You’ll be able to reproduce it and prevent the possible corruption of your computer setup.

Choose from the following:

Vagrant
Docker
Python: conda or venv

Or choose one of the preconfigured cloud coding environments:

GitHub Codespaces
Gitpod
Tip 2. Use code templates

Boilerplate code can help you get a working solution faster.

For example, if you’re creating Streamlit web apps, you can leverage the Streamlit App Starter Kit to have a functional app in seconds. You can also customize the app by adding functions, data, and widgets (learn more here). Or you can use the TrainGenerator to make the starter code for machine learning projects (also in seconds!).

Tip 3. Focus on the problem

Another key to success is focus. It's easy to get caught up in the excitement of a hackathon and try to do too much. If you want to succeed, focus on solving the problem. Remember to stay within the time limit and don’t worry about the details. A finished project is better than a perfect one!

Tip 4. Get unstuck

Google, StackOverflow, and coding forums are great resources for getting unstuck. Follow the routine of copying and pasting code errors into search to find solutions to your errors. If there are none, ask for help (read this guide to writing an effective post).

Tip 5. Create your own custom resource pack

Potential hackathon resources vary depending on the scope or topic.

For example, before participating in a data hackathon, you’ll need to come up with a list of potential APIs or datasets. Thinking about data will help you figure out how to use it and what analysis approach to apply to a visualization.

Bonus tip: Have fun!

Hackathons are meant to be a fun experience. Even if you don’t win, you’ll have a good time and create something you're proud of. And that’s a huge success worth celebrating! 🥳

My recent hackathon experience

I'd like to share with you a project that I’ve worked on at Streamlit’s recent Hackathon.

I followed three simple steps:

1. Create a solution to a problem

Problem: I wanted to share my social links on social media platforms but I didn’t want to share a long list of links. Solution: I created an app that stores all of my links in one link: https://chanin.streamlitapp.com/.

2. Make an app feature wishlist

I illustrated/summarized the four components of my app’s prototype in the diagram and the table below:

3. Implement the app

Once the app copy was up and running, I polished it, then refactored and lightly documented the code.

Wrapping up

Participating in a hackathon is amazing. You get to learn new skills, meet new people, and create something useful and inspiring. I hope you’ll attend your first hackathon soon!

If you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy hacking. 😁

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Building an Instagram hashtag generation app with Streamlit
https://blog.streamlit.io/building-an-instagram-hashtag-generation-app-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building an Instagram hashtag generation app with Streamlit

5 simple steps on how to build it

By William Mattingly
Posted in Advocate Posts, March 29 2023
Step 1. How to scrape data from Instagram and a site that contains data about hashtag relationships
Step 2. How to clean and structure the output from web scraping
Step 3. How to display that data visually
Step 4. How to create dynamic components with custom keys
Step 5. How to display your output in Streamlit
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Dr. William Mattingly, and I'm a postdoc at the Smithsonian Institution's Data Science Lab. I work primarily with applying machine learning (ML) and natural language processing (NLP) to humanities data and museum archival records.

I wanted to build an app for my social media accounts—and to learn more about web scraping, data visualization, and data storage. Generating hashtags from an Instagram hashtag collection and analyzing the output counts sounded like fun, so I built an Instagram Hashtag Generation App!

In this post, you'll learn:

Step 1. How to scrape data from Instagram and a site that contains data about hashtag relationships

Step 2. How to clean and structure the output from web scraping

Step 3. How to display that data visually

Step 4. How to create dynamic components with custom keys

Step 5. How to display your output within Streamlit

👉
Want to jump right in? Here's the app and the repo.
Step 1. How to scrape data from Instagram and a site that contains data about hashtag relationships

Let's start by gathering two key pieces of data about Instagram hashtags:

How many times has a hashtag been used
Which hashtags are frequently used alongside a given hashtag

You'll need to have some functions for making requests to Instagram and an Instagram-related site called best-hashtag.com (you'll use requests and BeautifulSoup instead of Selinium to keep it simple and make integration on other platforms like Streamlit Community Cloud much easier):

def get_count(tag):
	"""
	This function takes a hashtag as an input and returns the approx. times it has been used
	on Instagram.
	"""
  url = f"<https://www.instagram.com/explore/tags/{tag}>"
  s = requests.get(url)
  soup = BeautifulSoup(s.content)
  return int(soup.find_all("meta")[6]["content"].split(" ")[0].replace("K", "000").replace("B", "000000000").replace("M", "000000").replace(".", ""))

def get_best(tag, topn):
	"""
	This function takes two arguments, a hashtag and topn.
	Topn is the number of similar hashhtags you wish to find.
	This allows you to cultivate a set of 30-hashtags quickly.
	"""
  url = f"<https://best-hashtags.com/hashtag/{tag}/>"
  s = requests.get(url)
  soup = BeautifulSoup(s.content)
  tags = soup.find("div", {"class": "tag-box tag-box-v3 margin-bottom-40"}).text.split()[:topn]
  tags = [tag for tag in tags]
  return tags


You'll store user data in a JSON file to avoid unnecessary repeat requests. Let's set up your JSON database using a fairly standard function called load_data() (you won't be using Streamlit's cache feature because you'll be updating this information regularly):

def load_data():
    with open("database.json", "r") as f:
        data = json.load(f)
    return data

Step 2. How to clean and structure the output from web scraping

Now that you have developed your basic web scraping functions start building your app. First, import all requisite libraries:

import streamlit as st

# scraping
import requests
from bs4 import BeautifulSoup

#data
import json
import pandas as pd

# plotting
import plotly.express as px
import seaborn as sns


Next, load up your data by calling the load_data() function:

data = load_data()

Step 3. How to display that data visually

Once the data is loaded, start designing your app.

You want to let the user dynamically create multiple tags to automatically populate a collection of hashtags. They should be able to enter anywhere from 1 to 30 hashtags (30 is the maximum number allowed on Instagram). You can do this using Streamlit number_input() class:

num_tags = st.sidebar.number_input("Select number of tags", 1, 30)

Step 4. How to create dynamic components with custom keys

Since you want the tag inputs to be dynamically loaded, you'll need to create them dynamically. You can do this in a for loop and assign a unique key to each text input.

You also want the user to tell you how many similar hashtags to generate. To do this, you'll also need to create number inputs dynamically. Each of these will be appended to separate lists called tags and sizes:

st.sidebar.header("Tags")
col1, col2 = st.sidebar.columns(2)

tags = []
sizes = []
for i in range(num_tags):
    tag = col1.text_input(f"Tag {i}", key=f"tag_{i}")
    size = col2.number_input(f"Top-N {i}", 1, 10, key=f"size_{i}")
    tags.append(tag)
    sizes.append(size)

Step 5. How to display your output in Streamlit

Once the user has provided an input, it's time to use that input to do something.

For your app, you want to use hashtags to identify the number of times it's been used on Instagram and the common hashtags associated with it. When a button is clicked, you want that event to be triggered—do that with a conditional statement.

Here is the code in its entirety:

#only execute if the `Create Hashtags` button is pressed
if st.sidebar.button("Create Hashtags"):
		#create a list of tab names that begin with `all`
    tab_names = ["all"]
    tab_names = tab_names+[tags[i] for i in range(num_tags)]

		#create our Streamlit tabs
    tag_tabs = st.tabs(tab_names)

		#create lists to store our data outside of our loop
    all_hashtags = []
    hashtag_data = []

		#loop for the number of tags we have
    for i in range(num_tags):
        hashtags = get_best(tags[i], sizes[i])
        for hashtag in hashtags:
            if hashtag in data["hashtag_data"]:
                hashtag_count = data["hashtag_data"][hashtag]
            else:
                hashtag_count = get_count(hashtag.replace("#", ""))
                data["hashtag_data"][hashtag] = hashtag_count
            hashtag_data.append((f"{hashtag}<br>{hashtag_count:,}", hashtag_count))

		    #We can use our integer, i, to populate the list of Streamlit tag objects.
        tag_tabs[i+1].text_area(f"Tags for {tags[i]}", " ".join(hashtags))
        all_hashtags = all_hashtags+hashtags
  
    tag_tabs[0].text_area("All Hashtags", " ".join(all_hashtags))

    st.header("Hashtag Count Data")
    df = pd.DataFrame(hashtag_data, columns=["hashtag", "count"])
    df = df.sort_values("count")

    with open("database.json", "w") as f:
        json.dump(data, f, indent=4)
    
    fig = px.bar(df, x='hashtag', y='count')
    st.plotly_chart(fig, use_container_width=True)

Wrapping up

Thank you for reading my post! You have learned how to use Streamlit to perform web scraping, make dynamic inputs with unique keys, and display your output nicely.

If a tutorial video is your thing, check out the video below:

And if you have any questions, please leave them in the comments below or contact me on Twitter.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

star-history-2023525-1.png (2000×1413)
https://blog.streamlit.io/content/images/2023/05/star-history-2023525-1.png


blog-outline-generation-diagram.jpg (2000×966)
https://blog.streamlit.io/content/images/2023/06/blog-outline-generation-diagram.jpg


st-status-transparency.gif (1108×888)
https://blog.streamlit.io/content/images/2023/09/st-status-transparency.gif


notion-chatbot-embed.gif (758×416)
https://blog.streamlit.io/content/images/2023/09/notion-chatbot-embed.gif#browser


image_6487327--2-.JPG.jpg (2000×1206)
https://blog.streamlit.io/content/images/2023/06/image_6487327--2-.JPG.jpg


image_6487327--1-.JPG.jpg (2000×1028)
https://blog.streamlit.io/content/images/2023/06/image_6487327--1-.JPG.jpg


Streamlit and Snowflake: better together
https://blog.streamlit.io/snowflake-to-acquire-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

By Adrien Treuille, Thiago Teixeira and Amanda Kelly
Posted in Product, March 2 2022
Share this post
← All posts

Dear Data Scientists, Machine Learning Engineers, and Pythonistas worldwide:

We created Streamlit to give you superpowers. Streamlit takes minutes to learn yet gives you immense power to transform Python scripts into beautiful apps. As Streamlit spread worldwide, we have worked with you to build a vibrant community with tens of thousands of developers and millions of viewers. The best part? Each step has clarified that we were onto something very special—a new paradigm to share and understand data. Today, we couldn’t be happier to announce the next step in that journey.

Streamlit has been acquired by Snowflake to join forces and open new frontiers in data science and data application development.

What convinced us to join? It was our interactions with Snowflake’s founders and engineers. It was love at first code review. As we drew boxes and imagined the future, we found ourselves excitedly finishing each other’s sentences. We were sketching the same vision.

Working within Snowflake, we realized, would be a win-win-win: for the data community, for Streamlit technology, and for Snowflake’s customers.

Streamlit is being supercharged for all!

Everything you know and love about Streamlit is moving onward and upward. Keep making awesome apps, teaching classes, presenting at meetups, running hackathons, showing off research, and all the great things you do every day with Streamlit. And now we can do even more together! Here are some of our plans:

Streamlit and Snowflake are united in and committed to supporting the Streamlit open-source project and continuing to deliver an amazing set of features available to everyone. 🌟
The Streamlit community is still our primary focus. You delight us every day. With Snowflake’s amazing footprint, we’re excited to engage even more actively with the community. 🙋🏽
Streamlit Cloud will remain an amazing place for the community to share their work, explore new ideas, and discover new paradigms. As Streamlit enables the data science and machine learning communities to share their work, we all win. ❤️
We'll work together with Snowflake to open up a vast array of new use cases and capabilities for Streamlit, coupling our app framework with Snowflake’s leading data platform. 🚀

We’re beyond excited about what we can all build together. It feels like all our aims—to build a generational open-source project, to build an incredibly inclusive community of data nerds, to empower data scientists and machine learning engineers to share their work, to enable companies to harness and understand their data—are being supercharged by this collaboration.

Thank you, community, for everything we’ve built together. New frontiers await.

Let’s do this! 🎈❄️

Love,

Adrien, Thiago, Amanda, and everyone at Streamlit.

P.S.: Streamlit 1.7.0 was released today. Try out st.snow 😉!

0:00
/
1×
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
Using ChatGPT to build a Kedro ML pipeline

Talk with ChatGPT to build feature-rich solutions with a Streamlit frontend

LLMs
by
Arvindra Sehmi
,
February 9 2023
Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component

How to add advanced functionality to your Streamlit app’s authentication component

Advocate Posts
by
Mohammad Khorasani
,
February 7 2023
Using Streamlit for semantic processing with semantha

Learn how to integrate a semantic AI into Snowflake with Streamlit

Advocate Posts
by
Sven Koerner
,
February 2 2023
Host your Streamlit app for free

Learn how to transfer your apps from paid platforms to Streamlit Community Cloud

Tutorials
by
Chanin Nantasenamat
,
January 24 2023
Create a color palette from any image

Learn how to come up with the perfect colors for your data visualization

Advocate Posts
by
Siavash Yasini
,
January 19 2023
How to make a culture map

Analyze multidimensional data with Steamlit!

Tutorials
by
Michał Nowotka
,
January 12 2023
Build an image background remover in Streamlit

Skip the fees and do it for free! 🎈

Tutorials
by
Tyler Simons
,
January 10 2023
Find the top songs from your high school years with a Streamlit app

Use the Spotify API to generate 1,000+ playlists!

Advocate Posts
by
Robert Ritz
,
December 8 2022
Streamlit-Authenticator, Part 1: Adding an authentication component to your app

How to securely authenticate users into your Streamlit app

Advocate Posts
by
Mohammad Khorasani
,
December 6 2022
Streamlit Quests: Getting started with Streamlit

The guided path for learning Streamlit

Tutorials
by
Chanin Nantasenamat
,
November 18 2022
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Pablo Fonseca - Streamlit
https://blog.streamlit.io/author/pablo/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Pablo Fonseca
1 post
Building a PivotTable report with Streamlit and AG Grid

How to build a PivotTable app in 4 simple steps

Advocate Posts
by
Pablo Fonseca
,
March 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing multipage apps! 📄
https://blog.streamlit.io/introducing-multipage-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing multipage apps! 📄

Quickly and easily add more pages to your Streamlit apps

By Vincent Donato
Posted in Product, June 2 2022
Using multipage apps
Converting an existing app into a multipage app
Tips and tricks
Bonus features: new dataframe UI, horizontal radio buttons, and more!
Wrapping up
Contents
Share this post
← All posts

So, you built a Streamlit app that became super useful, but then you got overloaded with feature requests. You kept adding more and more features until it felt too cluttered. You tried splitting the content across several pages by using st.radio or st.selectbox to choose which “page” to run.

It worked! But maintaining the code got harder. You were limited by the st.selectbox UI and couldn’t customize page titles with st.set_page_config or navigate between them via URLs. 🤯

Sound familiar?

We wanted to find a simple solution for this, so today, we’re excited to introduce…

Native support for multipage apps!

In this post, we’ll show you how to use this new feature and share tips and tricks on getting the most out of it.

Want to jump right in? Update Streamlit to the newest version and see the streamlit hello demo app and repo for inspiration. Read more on how to get started in our docs.

Using multipage apps

Building a multipage app is easy! Just follow these steps:

1. Create a main script named streamlit_app.py.

2. In the same folder, create a new pages folder.

3. Add new .py files in the pages folder. Your filesystem will look like this:

my_app
├── streamlit_app.py    <-- Your main script
└── pages
    ├── page_2.py       <-- New page 2!
    └── page_3.py       <-- New page 3!


4. Run streamlit run streamlit_app.py as usual.

That’s it!

The streamlit_app.py script will now correspond to your app's main page. You’ll see the other scripts from the pages folder in the sidebar page selector.

Converting an existing app into a multipage app

Let’s say you built a multipage app by using st.selectbox and want to convert it to the multipage app functionality. In your current app, the selectbox picks which page to display, and each “page” is written as a function.

If your folder name is ~/my_app , your code will look like this:

# Contents of ~/my_app/streamlit_app.py
import streamlit as st

def main_page():
    st.markdown("# Main page 🎈")
    st.sidebar.markdown("# Main page 🎈")

def page2():
    st.markdown("# Page 2 ❄️")
    st.sidebar.markdown("# Page 2 ❄️")

def page3():
    st.markdown("# Page 3 🎉")
    st.sidebar.markdown("# Page 3 🎉")

page_names_to_funcs = {
    "Main Page": main_page,
    "Page 2": page2,
    "Page 3": page3,
}

selected_page = st.sidebar.selectbox("Select a page", page_names_to_funcs.keys())
page_names_to_funcs[selected_page]()


To convert your app to a multipage app, follow these steps:

1. Upgrade Streamlit to the newest version: pip install --upgrade streamlit

2. Add a new pages folder inside of  ~/my_app.

3. Create three new files inside of ~/my_app :

main_page.py
pages/page_2.py
pages/page_3.py

4. Move the contents of the main_page, page2, and page3 functions into their corresponding new files:

# Contents of ~/my_app/main_page.py
import streamlit as st

st.markdown("# Main page 🎈")
st.sidebar.markdown("# Main page 🎈")

# Contents of ~/my_app/pages/page_2.py
import streamlit as st

st.markdown("# Page 2 ❄️")
st.sidebar.markdown("# Page 2 ❄️")

# Contents of ~/my_app/pages/page_3.py
import streamlit as st

st.markdown("# Page 3 🎉")
st.sidebar.markdown("# Page 3 🎉")


5. Remove the original streamlit_app.py file.

6. Run streamlit run main_page.py and view your shiny new multipage app!

Tips and tricks

We didn’t specify an order for pages 2 and 3, but they displayed correctly anyway. Why? 🤔  Because they’re ordered alphabetically by default.

But what if you wanted to make this more clear?

Just add numerical prefixes in front of the files in the pages/ folder and rename them pages/02_page_2.py and pages/03_page_3.py. The names won’t include these prefixes—they’re used only for sorting.

You can also add emojis! 🥳  Try renaming the script files to:

01_🎈_main_page.py
pages/02_❄️_page2.py
pages/03_🎉_page3.py
Bonus features: new dataframe UI, horizontal radio buttons, and more!

Want to make your multipage apps look even cooler? 😎

Good news!

We launched more new features in Streamlit’s 1.10 release. Among them are the redesigned st.dataframe (based on glide-data-grid) and horizontal radio buttons. Check out the release notes for more info.

Wrapping up

And that’s it for the intro to multipage apps! Adding more pages to your apps is now easier than ever. To start using multipage apps today, upgrade to the latest version of Streamlit:

pip install --upgrade streamlit


Have any questions or want to share a cool app you made? Join us on the forum, tag us on Twitter, or let us know in the comments below. 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Debbie Matthews - Streamlit
https://blog.streamlit.io/author/mathcatsand/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Debbie Matthews
1 post
10 most common explanations on the Streamlit forum

A guide for Streamlit beginners

Advocate Posts
by
Debbie Matthews
,
March 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Sebastian Flores Benner - Streamlit
https://blog.streamlit.io/author/sebastian/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Sebastian Flores Benner
3 posts
Create a search engine with Streamlit and Google Sheets

You’re sitting on a goldmine of knowledge!

Advocate Posts
by
Sebastian Flores Benner
,
March 14 2023
uPlanner fosters data processing innovation with Streamlit

Sebastián Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app

Case study
by
Sebastian Flores Benner
,
October 6 2022
How to create interactive books with Streamlit in 5 steps

Use streamlit_book library to create interactive books and presentations

Advocate Posts
by
Sebastian Flores Benner
,
January 20 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

William Mattingly - Streamlit
https://blog.streamlit.io/author/william-mattingly/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by William Mattingly
Dr. William Mattingly is a Postdoc at the Smithsonian Institution in Washington D.C. He is also the host of the YouTube channel Python Tutorials for Digital Humanities
1 post
Website
Twitter
Building an Instagram hashtag generation app with Streamlit

5 simple steps on how to build it

Advocate Posts
by
William Mattingly
,
March 29 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Kanak Mittal - Streamlit
https://blog.streamlit.io/author/kanak/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Kanak Mittal
1 post
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

Advocate Posts
by
Kanak Mittal
,
April 11 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Dmitry Kosarevsky - Streamlit
https://blog.streamlit.io/author/dmitry/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Dmitry Kosarevsky
1 post
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

Advocate Posts
by
Dmitry Kosarevsky
,
April 18 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion-chatbot-deploy-streamlit.gif (751×365)
https://blog.streamlit.io/content/images/2023/09/notion-chatbot-deploy-streamlit.gif#browser


Peter Vidos - Streamlit
https://blog.streamlit.io/author/peter/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Peter Vidos
1 post
Website
Create an animated data story with ipyvizzu and Streamlit

A tutorial on using ipyvizzu and ipyvizzu-story

Advocate Posts
by
Peter Vidos
,
April 20 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion-chatbot-export-content.gif (761×418)
https://blog.streamlit.io/content/images/2023/09/notion-chatbot-export-content.gif#browser


Announcing the Figma-to-Streamlit plugin 🎨
https://blog.streamlit.io/announcing-the-figma-to-streamlit-plugin/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Announcing the Figma-to-Streamlit plugin 🎨

Go from prototype to code as easy as 1-2-3 with our new community resource!

By Juan Martín García
Posted in Product, November 1 2022
How to install the plugin
How to use it
How to contribute to the plugin’s development
Wrapping up
Contents
Share this post
← All posts

Hi Streamlit and Figma lovers! 👋🏻

My name is Juan, and I’m a designer and developer here at Streamlit. Remember last week we shared with you our Streamlit Design System in Figma? Today, I want to unveil…

Our complementary Figma plugin!

Prototype and code your apps easier than ever by turning components into code automatically, without leaving Figma. Pretty awesome, right?

In this post, you’ll learn:

How to install the plugin
How to use it
How to contribute to the plugin’s development

Want to check it out right away? Head on to our community profile and give it a try. Or strap yourself in and follow along with me!

NOTE: This is an experiment from our design team. We’re releasing it early to get your feedback, so there’s still stuff missing. If you find it useful, please contribute!
How to install the plugin

Installing the plugin is super easy. Just follow these steps:

Go to our Figma community profile, open the plugin, and hit Try it out.
Go to Figma and run it from the Plugins tab.
How to use it

As mentioned above, this plugin is complementary to our Streamlit Design System. Drag and drop a component, tweak its props and values, hit See my code, and get a code snippet to use in your app!

Need help creating an app to test the code? Check out our docs on Getting started or use Yuichiro Tachibana’s amazing stlite sharing template. Just copy the plugin’s snippet, paste it into the code editor on the left, and click 💾 Save. The app on the right will update and show the generated output automagically!

Here is what it’ll look like:

0:00
/
1×

Last but not least, if you need a refresher on how to use our Design System library to prototype your app, make sure to check out last week’s post! And if you have any issues, check out our troubleshooting section on GitHub.

NOTE: If you need a refresher on how to use our Design System to prototype your app, check out this post. And if you have any issues, read our troubleshooting section on GitHub.
How to contribute to the plugin’s development

This plugin is an experiment, which means we’re still developing it.

As of this writing, it supports:

Text elements (except for st.latex and st.markdown)
Input widgets (except for st.select_slider)
Native Chart elements (st.line_chart, st.bar_chart, and st.area_chart)

In the future, we plan to have it support:

More widgets: Media elements, Progress and Status, Data display elements, Layout and Containers, and Control flow.
More variants/features: recognizing bold, italic and strikethrough formatting; label_visibility on input widgets; optional properties and global page configuration; plugin settings to tweak the code output to better suit your needs.
Internal improvements: code refactoring, type annotations, automatic data import, example callbacks, and more!

Want to help us out with the plugin’s development? See the instructions on how to contribute to the codebase. Thank you! 🙏

Wrapping up

That’s pretty much it, folks.

We hope you enjoy playing with this new plugin. If you find any errors or have any ideas on how to improve it, please file an issue, and we’ll get back to you as soon as we can. Better yet, help us build those features yourself by contributing to the codebase.

Happy Figma-to-Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion_chatbot_text_splitting.png (2000×1068)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_text_splitting.png#border


notion_chatbot_vector_space.png (2000×1953)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_vector_space.png#border


How to make a culture map
https://blog.streamlit.io/how-to-make-a-culture-map/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to make a culture map

Analyze multidimensional data with Steamlit!

By Michał Nowotka
Posted in Tutorials, January 12 2023
How to dynamically create buttons and assign them a columnar layout
How to create a scatter plot with flags as markers
How to find the best matching country
Wrapping up
Contents
Share this post
← All posts

Hi, community! 👋

My name is Michał Nowotka, and I’m a new Engineering Manager at Streamlit / Snowflake. Before diving into my first PR, I wanted to create my first Streamlit app that was fun, used freely available data, and had as many Steamlit components as possible.

While reading the Culture Map book by Erin Meyer, I came across the six cultural dimensions theory by Geert Hofstede: Power Distance, Individualism, Uncertainty avoidance, Masculinity, Long Term Orientation, and Indulgence vs. restraint. The data was freely available, so I decided to give it a try.

In this post, I’ll show you:

How to dynamically create buttons and assign them a columnar layout
How to create a scatterplot with flags as markers
How to find the best matching country

Want to jump right in? Here's a demo app and a repo code.

How to dynamically create buttons and assign them a columnar layout

After downloading the data, add a multi-select component to choose as many countries as as possible. By default, the multi-select shows a random selection of ten countries but they can be added or removed. You can also choose countries from a predefined group:

Notice that all buttons selecting a group are rendered in a separate column. I wanted to allow for easy adding/removing country groups in the code, so that the rendering parts looked like this:

st.write("Or choose from predefined country group:")
columns = st.columns(len(country_data.COUNTRY_GROUPS))
for idx, column in enumerate(columns):
    with column:
        group = country_data.COUNTRY_GROUPS[idx]
        st.button(group, key=group, on_click=country_group_callback)


All buttons share the same on_click callback. To figure out which button called it and update the multi-select, use session state:

def country_group_callback():
    chosen_group = [
        group_name for group_name, selected in st.session_state.items() if
        selected and group_name in country_data.COUNTRY_GROUPS][0]
    countries = country_data.GROUPS_TO_COUNTRIES[chosen_group]
    st.session_state["default_countries"] = countries

How to create a scatter plot with flags as markers

After selecting a few countries, you can visualize their cultural dimensions with:

Choropleth
Radar plots
Heatmap
Scatter plot

Choropleth is a kind of map that uses color to visualize a given property. Generate six choropleths for each dimension in separate tabs with plotly:

The tabs act as Python context managers and can be used with the with keyword:

st.write("Or choose from predefined country group:")
columns = st.columns(len(country_data.COUNTRY_GROUPS))
for idx, column in enumerate(columns):
    with column:
        group = country_data.COUNTRY_GROUPS[idx]
        st.button(group, key=group, on_click=country_group_callback)


To make switching from one tab to another less tedious, visualize multiple dimensions on a single graph with radar plots. Here are some good examples of how to do this with Matplotlib:

Considering all six dimensions is cumbersome, so how about compressing them into one number? You can measure the “cultural distance” between all selected countries with Scipy. It lets you choose from different distance measures (Euclidean, Cosine, Manhattan, etc.). You can put them into a dictionary for convenience:

from scipy.spatial import distance

AVAILABLE_DISTANCES = {
    "Euclidean": distance.euclidean,
    "Cosine": distance.cosine,
    "Manhattan": distance.cityblock,
    "Correlation": distance.correlation,
}


Now computing a distance between two countries is easy:

HOFSTEDE_DIMENSIONS = ['pdi', 'idv', 'mas', 'uai', 'lto', 'ind', 'ivr']

def compute_distance(
        country_from: types.CountryInfo,
        country_to: types.CountryInfo,
        distance_metric: str
) -> float:
    from_array = [max(getattr(country_from, dimension) or 0, 0) for dimension in HOFSTEDE_DIMENSIONS]
    to_array = [max(getattr(country_to, dimension) or 0, 0) for dimension in HOFSTEDE_DIMENSIONS]
    return AVAILABLE_DISTANCES[distance_metric](from_array, to_array)


Next, let’s create a distance matrix to compute the distances between all selected countries. Since the number of distances is proportional to the square of the number of selected countries, you can cache the result using st.cache decorator:

@st.cache
def compute_distances(
        countries: types.Countries,
        distance_metric: str
) -> tuple[PandasDataFrame, float]:
    index = [country.title for country in countries]
    distances = {}
    max_distance = 0
    for country_from in countries:
        row = []
        for country_to in countries:
            distance = compute_distance(country_from, country_to, distance_metric)
            max_distance = max(max_distance, distance)
            row.append(distance)
        distances[country_from.title] = row
    return pd.DataFrame(distances, index=index), max_distance


Now let’s plot the heatmap, cluster together similar countries, and show which country belong to which cluster. You can combine them with clustermap from Seaborn by applying the hierarchical clustering to the sides of the heatmap. Change the distance metric to change the colors and the row-and-columns clustering:

The distance between the countries is cultural as opposed to geographical, so it’d be great to see it in 2D space with their cultural traits (and not coordinates). For this, you’ll need two dimensions. But which two should you use? Instead of choosing them arbitrarily, reduce the dimensionality using Principal Component Analysis. Just like scipy helped you with the distance metrics, scikit-learn can help with dimensionality reduction:

from sklearn import decomposition

AVAILABLE_DECOMPOSITION = {
    'PCA': decomposition.PCA,
    'FastICA': decomposition.FastICA,
    'NMF': decomposition.NMF,
    "MiniBatchSparsePCA": decomposition.MiniBatchSparsePCA,
    "SparsePCA": decomposition.SparsePCA,
    "TruncatedSVD": decomposition.TruncatedSVD
}


Next, compute the 2D data, use scatterplot to visualize locations, and mark each point with a country flag (fetch the data from Wikipedia 🙂). Use the Bokeh library to replace markers with image URLs:

How to find the best matching country

Since each dimension can score 1-100, use a set of six sliders to ask the users about their preferences:

It takes a lot of space, so let’s do it on a separate page. And just for fun, let’s add a button that sets each slider to a random value. Now you can compute a distance between user preferences and each country and select the top N hits. To make N a variable, use number input. You’re ready to present the ranking:

This looks pretty boring, so let’s add some data visualization. How about a radar plot? Let’s stack two radar spiders together. The red one will show your selected preferences, and the colored one will show the top country—to see how close it is to your preferences:

Read the code to learn more about:

Loading markdown content into st.markdown from an external .md file
Hiding raw content using st.expander
Showing pandas data frames using st.write
Setting app title headers and subheaders with st.title, st.header, st.subheader, and more!
Wrapping up

It’s amazingly easy to create complex visualizations and perform data analysis with Streamlit. And deployment is a breeze! Apparently, the top two countries matching my personal cultural preferences are Switzerland and UK. Coincidentally, that’s where I’ve spent 7+ years of the last 14 years of my life. 🙂 If you have questions about the app, feel free to reach out to me via LinkedIn or email.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion_chatbot_folder_content-1.png (2000×911)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_folder_content-1.png#browser


notion_chatbot_query.png (2000×805)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_query.png#brder


Instant-Insight---Streamlit--1--2.gif (1056×528)
https://blog.streamlit.io/content/images/2023/08/Instant-Insight---Streamlit--1--2.gif


Deploy a private app for free! 🎉
https://blog.streamlit.io/deploy-a-private-app-for-free/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Deploy a private app for free! 🎉

And... get unlimited public apps

By Abhi Saini
Posted in Product, December 9 2021
Share this post
← All posts

Starting today you can deploy a private app for free with Streamlit Cloud!

Your code stays private and you get auth-baked into the app. Try it out for your company or for any app that you don't want to be public. Head over to our docs to learn more about securing your data and adding private viewers.

While we were working on this, we thought, "Hmm. Why are we giving you only three free public apps? If you're doing great open-source stuff, we want to support you!"

So also starting today...

You can deploy unlimited public apps for free! 🎉

Whether you're a student submitting an assignment with Streamlit, a researcher illustrating your latest research with a Streamlit app, a company showing off your latest open-source data or a model in an app, or just a smart person sharing a cool interactive insight—with Streamlit Cloud you can share all your public apps early, often, and always.

We hope this will help unblock you in the great work that you're doing. Let your creativity flourish! Stop by the forum and let us know what you think and what you're building.

Happy Streamlit-ing! ❤️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion-chatbot-openai-api-key.gif (741×362)
https://blog.streamlit.io/content/images/2023/09/notion-chatbot-openai-api-key.gif#browser


notion_chatbot_ingestion.png (2000×502)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_ingestion.png#border


notion_chatbot_duplicate-1.png (2000×689)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_duplicate-1.png#browser


table.gif (1056×528)
https://blog.streamlit.io/content/images/2023/08/table.gif


Leverage your user analytics on Streamlit Community Cloud
https://blog.streamlit.io/leverage-your-user-analytics-on-streamlits-community-cloud/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Leverage your user analytics on Streamlit Community Cloud

See who viewed your apps, when, and how popular they are

By Diana Wang and Johannes Rieke
Posted in Product, May 17 2022
Workspace analytics
App viewers analytics
Wrapping up
Contents
Share this post
← All posts

Ever wanted to view the traffic levels of your Streamlit apps without using custom JS code? If so, today, we’re excited to announce...

The new Analytics Modal feature on Streamlit Community Cloud!

Now you can see how many views you’ve received, who has viewed your apps, and when.

In this post, we’ll show you how to use both Workspace analytics and App viewers analytics by using a fictitious Streamlit workspace as an example.

Want to jump right in? Head over to your Community Cloud account to view your analytics now.

💡
Analytics are visible to anyone with access to your workspace: admins, developers, and viewers.
Workspace analytics

In Workspace analytics, you can see how many total viewers have visited all apps in your workspace. Simply click on "Analytics" in the dashboard header:

The modal will automatically open in your "Workspace" tab:

Let’s say that our fictitious Streamlit workspace has dozens of public apps. The solid line indicates the completed months and the dotted line indicates the month-in-progress for all apps in the workspace.

You can also hover over your "Workspace" tab’s chart to see how many users have viewed at least one app in a given month:

App viewers analytics

Head over to App viewers analytics to see who has recently viewed your apps and when.

We’ll use the 30 Days of Streamlit app as an example (we launched it in April for the community to learn more about Streamlit and to try fun use cases).

You can access App viewers analytics in three ways:

In your workspace, click the "Analytics" tab and then the "App viewers" tab:

2. From your "Apps" dashboard, click the "︙" overflow menu for any app, then select "Analytics":

3. From your "Apps" page, click "Manage app" console, then select "Analytics":

💡
You can access the "Manage app" console only if you have the GitHub push access for the given app.

All three ways will bring you to the same feature:

The total all-time number of unique app viewers (including April 2022 and onward).
The list of the most recent viewer names (capped to 20) and the relative timestamp of their last view sorted by the time since the last view (newest first).

For public apps like 30 Days of Streamlit, we track individual usage but not identity, so we make up names for your public viewers. They're shown as random pseudonyms (for example, Enigmatic Brownie).

If you're building something awesome, but not quite ready for the world to see it, use your one private app to test it. You can invite up to three viewers to your workspace as collaborators. The identities of all invited viewers within your workspace will be visible within the Analytics Modal.

Wrapping up

And that’s a wrap! You can now view Workspace analytics and App viewers analytics within your Community Cloud accounts. Check out our docs for more info.

Don’t have a Community Cloud account? Create one for free here.

Questions? Suggestions? Or have a neat app or some analytics to show off? Join us on the Streamlit community forum. We can't wait to hear what you think. 🎈

Huge kudos to...

Will Schmitt, Benny Raethlein, Andreas Braendhaugen, Will Huang, Laura Wilby, Henrikh Kantuni, Zachary Blackwood, Grace Tan, Snehan Kekre, and James Thompson for bringing this feature to life on Community Cloud. And thanks to all the community members who gave feedback on the different iteration ideas for analytics in the past!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion_chatbot_project_structure.png (2000×1263)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_project_structure.png


notion-chatbot.gif (758×416)
https://blog.streamlit.io/content/images/2023/09/notion-chatbot.gif#browser


☁️ Introducing Streamlit Cloud! ☁️
https://blog.streamlit.io/introducing-streamlit-cloud/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
☁️ Introducing Streamlit Cloud! ☁️

Streamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.

By Adrien Treuille
Posted in Product, November 2 2021
☁️ Introducing Streamlit Cloud! ☁️
Try it out
Contents
Share this post
← All posts

Data science is in a logjam.

Once upon a time, data flowed smoothly into well-structured tables. Now it's a turbulent mix of image labels, natural language data, API endpoints, neural networks, and other models.

In parallel, business demands have become more complex. Users don't just want the graphs of the past. They want the models of the future. They don't just want summary dashboards. They want to go beyond dashboards.

They want rich data apps.

Data apps go beyond dashboards to capture a variety of next-generation data sources and analytic abilities.

Caught between the rising data complexity and the deeper analytical needs, data teams searched for new tools. But nothing let them quickly build and iterate on the next-generation data products they envisioned.

Then in 2019, we released a solution—a new way to make polished, flexible, and powerful data apps in just a few lines of Python...

Streamlit!

Your response surged beyond our wildest expectations. You unleashed a torrent of dazzling Streamlit apps. You explained, analyzed, and modeled everything from real estate to black holes. You brought it to work. You used Streamlit to label ad videos, track dbt jobs, visualize object detection output, and explore vast quantities of geographic data.

In fact, you spread Streamlit so widely that Streamlit is now actively deployed at over half the Fortune 50!

With all this app-building, you surfaced new needs. Building great apps was only half the logjam. You also needed to share those apps, get them into the hands of your users, gather feedback, iterate, and create new ideas.

Doing that inside a company wasn't easy. It involved more skillsets, more meetings, more coordination. It slowed down your development pace.

The current process in companies to create and deploy data apps takes months and multiple teams.

So for the past two years, we've been building a new product. We hardened it in the community and beta-tested it in hundreds of organizations—from small AI startups to public multinational companies.

Today, we want to share this new superpower with every company in the world…

☁️ Introducing Streamlit Cloud! ☁️

Streamlit Cloud is a data workspace for your company.

When you work on an app in Streamlit Cloud—be it a new model, data analysis, or idea—you're just a few clicks away from securely sharing it and collaborating on it with your team.

1. Build and deploy apps in minutes

Build your Streamlit apps the way you've always built them! Download the open-source library, use your favorite IDE, and take advantage of Streamlit's run-on-save rapid development flow. Done building? Use Streamlit Cloud to watch your app go live across the company.

Streamlit Cloud handles all the IT, DevOps, and security for you—Python dependencies, Unix package management, container orchestration, server provisioning, scaling, data security, and more—so you can get back to your data work!

2. Securely share apps

Once your app is deployed, you can securely share it with your whole company. Send it to just one person. Or send it to teammates, customers, and other business units so they can start using it right away.

Streamlit Cloud works with your preferred SSO provider. Easily lock down your app so only certain people can see it.

3. Rapidly iterate

Your app is shared! Now you can quickly iterate on it. Streamlit Cloud continuously deploys your app from GitHub, giving you the power of modern version-controlled code development.

Like a teammate's app? Fork it and launch your own! Want to test out a new version of a production app? Branch it and deploy a dev app. Prefer an earlier version? Check the version history and roll it back. Received a user request? Make the change and watch the app automatically update.

All of this makes for an incredibly rapid prototype-to-production cycle for your whole team.

Try it out

Streamlit Cloud moves you beyond the dashboard and into a new world of bespoke, predictive tooling where your company can run "at the speed of data science." We've watched companies go from having a handful of slow, poorly maintained tools to launching thousands of apps a year. It completely transformed how they work with data.

Today we're opening up Streamlit Cloud to all companies, and we hope you try it out.

A HUGE thank you to our community and beta testers for inspiring us every day. This wouldn't be possible without you and your candid feedback, comments, and words of encouragement.

We can't wait to hear what you build next! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

notion_chatbot_main_flow.png (2000×884)
https://blog.streamlit.io/content/images/2023/09/notion_chatbot_main_flow.png


The next frontier for Streamlit
https://blog.streamlit.io/the-next-frontier-for-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

By Amanda Kelly, Adrien Treuille, Thiago Teixeira, Johannes Rieke and TC Ricks
Posted in Product, October 18 2022
📀 The data trio: st.connection, st.dataframe, and st.database
🕹 Interactive everything: charts, tables, maps, and more!
🎨 Visual customization ++
🧩 Custom components v2
🧠 ML is amazing in Streamlit
🐞 Debugging is gosh darn delightful
🎈 Streamlit Community Cloud becomes a hub for community discovery
λ Towards stateless Streamlit…
🏁 Wrapping up
Contents
Share this post
← All posts

Dear Python developers and wonderful Streamlit community, ❤️

We come bearing exciting news about the future of Streamlit—amazing new superpowers, mind-bending new use cases, and powerful new technologies.

But first, let’s take a look at the last year.

In October 2021, we hit a big milestone and launched Streamlit 1.0. Then a few months later we announced we’d be growing our “baby” with new parents. Yup. If you missed the news—six months ago we joined Snowflake. Snowflake’s commitment to Streamlit and open source is deep, and it’s giving us even more support to build the Streamlit library and the community platforms for years to come. Yay!

So, what’s next?

We’re thinking bigger than ever, looking far into the future, and asking ourselves, “What will Streamlit be when it grows up?” In short, we want to see Streamlit become the essential UI layer for Python. We want to take our amazing dev experience and turn it up to 11—taking your daily pain points and turning them into magical moments. And we want these new superpowers to benefit the entire Python community—regardless of skill level, application domain, deployment platform, or ability to pay.

Without further ado, here are our plans for next year and beyond:

📀 The data trio: st.connection, st.dataframe, and st.database

We want to make the path from data to app as frictionless as possible. st.connection will let you connect to external databases and APIs with a single line of code. st.database will launch a small database alongside every Streamlit app, so you can permanently store data without any setup. Andst.dataframe will get a number of improvements—from filtering to sorting, editability, showing images, and much more.

🕹 Interactive everything: charts, tables, maps, and more!

One of the most upvoted feature requests is to make output elements interactive. Click on a chart datapoint, a map location, or a dataframe row, and the Python script will catch that event on the Python side and update your app. This allows extremely cool new use cases, e.g., showing the details for a selected dataframe row or running a prediction on some data selected in a chart.

🎨 Visual customization ++

Our focus has always been on making your app as beautiful out-of-the-box as possible. But if you want to customize it, the current options are limiting. Grid layout, more options for colors, logo positioning, and better themes are just some of the options we’re looking at. But we’re also wondering if we can go beyond that and better integrate with CSS or offer other ways to manipulate styling. Stay tuned!

🧩 Custom components v2

Today, you can extend Streamlit by building custom components, but this requires advanced JavaScript skills. Plus, it’s time-consuming (even for experts). We want to crack open this box to rethink how we’re doing extensibility, to make it easier to do more with less. Components should be able to interact with more elements in apps, it should be incredibly easy to publish Python- or HTML-only components, and it should be effortless to discover and use great community-built components.

🧠 ML is amazing in Streamlit

It feels like every day there is a new, amazing breakthrough in ML and AI. The boundaries of what’s possible are being pushed—and we want to push alongside it. We want to make it easier to connect to and work with models and pipelines, integrate with the machine learning tools you love, and create the visualization and widgets you need to show off your work.

🐞 Debugging is gosh darn delightful

Building a Streamlit app is pretty delightful, so we want to make debugging your code just as frictionless. We’re going back to the mat to think about what it means for you as a developer—including a better in-app display of exceptions and logs, built-in memory and runtime profilers, and improvements to how Streamlit works in the terminal and in your favorite editor.

🎈 Streamlit Community Cloud becomes a hub for community discovery

A huge part of developing Streamlit apps is sharing them with other developers for feedback, being inspired by the work of others in the community, and finding app examples and code snippets to use. Streamlit Community Cloud is (and will continue to be!) a free platform for sharing, but we see the potential for so much more. The potential to offer a true hub for you and everyone in the community. We want to continue to remove all barriers to easy development, sharing, and discovery—helping you to get started coding directly on the cloud, show off your work to a broader audience, save inspiring and useful bits of code, encounter trending and relevant apps, meet collaborators, and uncover amazing community-created components.

λ Towards stateless Streamlit…

We’re investigating how to expand the ways Streamlit can be hosted, including implementing a stateless model so it works well in a serverless computing environment. This is part of a broader effort to make it easy for you to run Streamlit on any infrastructure and to rethink how it runs—to open new possibilities for sharing, state, compute, and more.

🏁 Wrapping up

We want to make working in Streamlit your daily source of joy. What issues have we missed? What new features do we need to develop? Please come to the forum and tell us!

We hope you’ll keep sharing your apps, components, videos, and blog posts with us. Your work is an incredible source of inspiration, and it makes us want to build even more amazing things with you in the future. Thank you. 🙏

Love,

Amanda, Adrien, Thiago, Johannes, TC, and everyone at Streamlit. ❤️

P.S.: Streamlit in Snowflake is coming soon! ❄️ See a little preview here.

P.P.S.: Want to help with Streamlit full-time? We’re hiring!

Forward-Looking Statements

This post contains express and implied forwarding-looking statements, which are subject to a number of risks, uncertainties and assumptions, including those described under the heading “Risk Factors” in quarterly and annual reports that Snowflake files with the Securities and Exchange Commission. In light of these risks, uncertainties, and assumptions, actual results could differ materially and adversely from those anticipated or implied in the forward-looking statements. As a result, you should not rely on any forwarding-looking statements as predictions of future events.

© 2022 Snowflake Inc. All rights reserved. Snowflake, the Snowflake logo, and all other Snowflake product, feature and service names mentioned herein are registered trademarks or trademarks of Snowflake Inc. in the United States and other countries. All other brand names or logos mentioned or used herein are for identification purposes only and may be the trademarks of their respective holder(s). Snowflake may not be associated with, or be sponsored or endorsed by, any such holder(s).

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Cloud is now SOC 2 Type 1 compliant
https://blog.streamlit.io/streamlit-cloud-is-now-soc-2-type-1-compliant/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit Cloud is now SOC 2 Type 1 compliant

We have completed a full external audit of our security practices

By Amanda Kelly
Posted in Product, January 11 2022
A little background on security at Streamlit
What is SOC 2?
Why is SOC 2 important?
What does it mean for you?
Contents
Share this post
← All posts
A little background on security at Streamlit

Security has been integral to how we have built Streamlit from day one.

Our engineering team has worked with some of the best security talent in the industry, and these experiences have shaped how we build and design secure software. That’s why our product works well with your existing security protocols.

Streamlit apps are the code that you write, and Streamlit Cloud runs that code to serve your apps to your users. Streamlit Cloud stores only a copy of your code and none of your data. Since you control both the code and the data sources, it’s very easy to implement your preferred security practices. On our end, we always uphold industry best practices for encrypting data in transit, securely storing authentication credentials, providing SSO integration for accessing the app, and much more.

Today we’re thrilled to announce that we’ve completed a full external audit of these security practices and Streamlit Cloud is now SOC 2 Type 1 compliant.

In this article, we’re going to share with you what SOC 2 is, why it matters, and how we comply with it.

What is SOC 2?

Service Organization Control (SOC) 2 is a SaaS industry standard that shows customers if a business has effective security controls. The American Institute of CPAs (AICPA) has developed SOC 2 to define how businesses should manage customers' data. Basically, it was designed to make you feel safe about the information you share with any business.

The typical SOC 2 audit is based on five Trust Services Criteria: security, availability, processing integrity, confidentiality, and privacy. When a company undergoes the SOC 2 audit process, it can choose which of the five to focus on. We chose to focus on security and confidentiality.

After the audit, the company gets either Type 1 or Type 2 SOC 2 report that reflects its unique business practices. Type 1 covers compliance at a given moment. Type 2 covers it over a longer period of time.

On October 31st, 2021, Streamlit Cloud has been certified as SOC 2 Type 1. You can request the full audit report by emailing us at support@streamlit.io.

Why is SOC 2 important?

SOC 2 is important because your management needs it. Or your security team. Or, your organization is SOC 2 certified, but to use a new service that might touch your sensitive data they require your vendors to be SOC 2 certified.

This certification lets us be an easy-to-approve vendor for you.

If you're new to Streamlit, or if your compliance questions have stopped you before from signing up, we hope this new certification gives you more confidence in testing out Streamlit Cloud for securely sharing apps within your organization.

Also, every SOC 2 report has “your responsibility” and “the provider’s responsibility” sections. One of the key responsibilities we want to call out is that you must connect to your organization’s services securely. Often, this is as simple as storing your authentication secrets by using our Secrets feature to connect to your data store via TLS.

You can find guidance on how to connect to various data stores in our docs, and you can always reach us at support@streamlit.io if you have any questions. We’re here to help you every step of the way. 🙂

What does it mean for you?

You’ll continue to benefit from our investment in security and data privacy as we keep adding even more security features and options to the product. You can also read about our security approach on our privacy policy page and in our documentation on trust and security.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Product Announcements
https://blog.streamlit.io/tag/product/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Product
36 posts
Launching a brand-new docs site 🥳

Improved layout, easier navigation, and faster search

Product
by
Snehan Kekre
,
October 13 2021
Announcing Streamlit 1.0! 🎈

Streamlit used to be the simplest way to write data apps. Now it's the most powerful

Product
by
Adrien Treuille
,
October 5 2021
New experimental primitives for caching (that make your app 10x faster!)

Help us test the latest evolution of st.cache

Product
by
Abhi Saini and 
1
 more,
September 22 2021
Streamlit gains a major new spell book

A tome to the magical fields of Python, algorithms, visualization, and machine learning

Product
by
Adrien Treuille
,
August 20 2021
All in on Apache Arrow

How we improved performance by deleting over 1k lines of code

Product
by
Henrikh Kantuni
,
July 22 2021
Session State for Streamlit 🎈

You can now store information across app interactions and reruns!

Product
by
Abhi Saini
,
July 1 2021
Introducing Submit button and Forms 📃

We're releasing a pair of new commands called st.form and st.form_submit_button!

Tutorials
by
Abhi Saini
,
April 29 2021
Our $35 million Series B

We’re excited to announce a new funding round led by Sequoia 🌲

Product
by
Adrien Treuille
,
April 7 2021
Announcing Theming for Streamlit apps! 🎨

Try out the new dark mode and custom theming capabilities

Product
by
Abhi Saini
,
March 18 2021
Introducing Streamlit Sharing

The new Streamlit platform for deploying, managing, and sharing your apps

Product
by
Adrien Treuille
,
October 15 2020
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

A new Streamlit theme for Altair and Plotly charts
https://blog.streamlit.io/a-new-streamlit-theme-for-altair-and-plotly/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

By William Huang, Lukas Masuch, Andreas Brændhaugen, Arnaud Miribel and Johannes Rieke
Posted in Product, December 19 2022
Streamlit + Altair + Plotly 🎈
Introducing a beautiful new Streamlit theme 🧑‍🎨
How can you try it? 👩‍💻
You don't need to do anything!
You can customize it
You can disable it
Wrapping up
Contents
Share this post
← All posts
Streamlit + Altair + Plotly 🎈

At Streamlit, we're constantly inspired by other open-source projects.

We're especially inspired by Altair and Plotly and their contributions to the data visualization community. They're two of the most popular Python libraries for creating interactive charts in Python—and in Streamlit. We continuously work to support them in our library, as Streamlit's success is due in no small part to their powerful and flexible charting capabilities.

Introducing a beautiful new Streamlit theme 🧑‍🎨

Today, we're excited to announce the release of a new default theme for Altair and Plotly charts in Streamlit apps!

The theme uses Streamlit's signature colors and layout adjustments that will make your data visualizations more engaging and easier to understand. Plus, your charts will better integrate with the rest of your app's design.

Check out our release demos for Altair at altair.streamlit.app and plotly.streamlit.app for more examples.

Psst…night owls and dark mode fans—we've got you covered, too! 🌚

How can you try it? 👩‍💻
You don't need to do anything!

The new theme is available from Streamlit 1.16 through the theme="streamlit" keyword argument in st.altair_chart, st.vega_lite_chart, and st.plotly_chart. It's activated by default, so there is nothing you need to do to enjoy it. ✨

Here's a minimal example of a contour plot with Plotly:

import numpy as np
import plotly.graph_objects as go
import streamlit as st

z = np.random.random_sample((3, 2))
fig = go.Figure(data=go.Contour(z=z))

st.plotly_chart(
    fig, 
    theme="streamlit",  # ✨ Optional, this is already set by default!
)


You can customize it

If you're an experienced Altair or Plotly user and like customizations, don't worry. Although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!

Check out some customization examples in Altair and Plotly.

You can disable it

If you want, you can continue using the default theme. Just type theme=None instead of theme="streamlit" in your chart commands:

st.altair_theme(..., theme=None)


To learn more about our charts, visit our Altair, Plotly, and Vega-Lite chart docs!

Wrapping up

We're confident that your users (and you!) will love this new default theme. Go ahead and give it a try. And check out the other features released in 1.16.0.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Abhi Saini - Streamlit
https://blog.streamlit.io/author/abhi_s/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Abhi Saini
9 posts
Deploy a private app for free! 🎉

And... get unlimited public apps

Product
by
Abhi Saini
,
December 9 2021
0.89.0 release notes

This release launches configurable hamburger menu options and experimental primitives for caching

Release Notes
by
Abhi Saini
,
September 22 2021
New experimental primitives for caching (that make your app 10x faster!)

Help us test the latest evolution of st.cache

Product
by
Abhi Saini and 
1
 more,
September 22 2021
0.88.0 release notes

This release launches st.download_button as well as other improvements and bug fixes

Release Notes
by
Abhi Saini
,
September 3 2021
Session State for Streamlit 🎈

You can now store information across app interactions and reruns!

Product
by
Abhi Saini
,
July 1 2021
How to make a great Streamlit app: Part II

A few layout and style tips to make your apps look even more visually appealing!

Tutorials
by
Abhi Saini
,
June 22 2021
How to make a great Streamlit app

Designing an app your users will love

Tutorials
by
Abhi Saini
,
June 2 2021
Introducing Submit button and Forms 📃

We're releasing a pair of new commands called st.form and st.form_submit_button!

Tutorials
by
Abhi Saini
,
April 29 2021
Announcing Theming for Streamlit apps! 🎨

Try out the new dark mode and custom theming capabilities

Product
by
Abhi Saini
,
March 18 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing two new caching commands to replace st.cache!
https://blog.streamlit.io/introducing-two-new-caching-commands-to-replace-st-cache/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

By Tim Conkling, Karen Javadyan and Johannes Rieke
Posted in Product, February 14 2023
What's the problem with st.cache?
The solution: st.cache_data and st.cache_resource
New documentation
Additional features ✨
What will happen to st.cache?
The future of caching
Contents
Share this post
← All posts

Caching is one of the most beloved and dreaded features of Streamlit. We understand why! @st.cache makes apps run faster—just slap it on top of a function, and its output will be cached for subsequent runs. But it comes with a lot of baggage: complicated exceptions, slow execution, and a host of edge cases that make it tricky to use. 😔

Today, we're excited to announce two new caching commands…

st.cache_data and st.cache_resource!

They're simpler and faster, and they will replace st.cache going forward! 👣

What's the problem with st.cache?

We spent a lot of time investigating it and talking to users. Our verdict: st.cache tries to solve too many use cases!

The two main use cases are:

Caching data computations, e.g. when you transform a dataframe, compute NumPy arrays, or run an ML model.
Caching the initialization of shared resources, such as database connections or ML models.

These use cases require very different optimizations. One example:

For (1), you want your cached object to be safe against mutations. Every time the function is used, the cache should return the same value, regardless of what your app does with it. That's why st.cache constantly checks if the cached object has changed. Cool! But also…slow.
For (2), you definitely don't want these mutation checks. If you access a cached database connection many times throughout your app, checking for mutations will only slow it down without any benefit.

There are many more examples where st.cache tries to solve both scenarios but gets slow or throws exceptions. So in 2021, we decided to separate these use cases. We released two experimental caching commands, st.experimental_memo and st.experimental_singleton. They work similarly to st.cache and are optimized for (1) and (2), respectively. Their behavior worked great, but the names confused users (especially if they didn't know the underlying CS concepts of memoization and singleton).

The solution: st.cache_data and st.cache_resource

Today, we're releasing our new solution for caching: st.cache_data and st.cache_resource. These commands have the same behavior as st.experimental_memo and st.experimental_singleton (with a few additions described below) but should be much easier to understand.

Using these new commands is super easy. Just add them as a decorator on top of the function you want to cache:

@st.cache_data
def long_running_function():
    return ...


Here's how to use them:

st.cache_data is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, and so on). It creates a new copy of the return object at each function call, protecting it from mutation and concurrency issues. The behavior of st.cache_data is what you want in most cases—so if you're unsure, start with st.cache_data and see if it works!


st.cache_resource is the recommended way to cache global resources such as ML models or database connections—unserializable objects that you don't want to load multiple times. By using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache.
New documentation

Along with this release, we're launching a brand new docs page for caching. 🚀 It explains in detail how to use both commands, what their parameters are, and how they work under the hood. We've included many examples to make it as close to real life as possible. If anything is unclear, please let us know in the comments. ❤️

Additional features ✨

The behavior of the new commands is mostly the same as st.experimental_memo and st.experimental_singleton, but we also implemented some highly requested features:

st.cache_resource now has a ttl—to expire cached objects.
st.cache_resource got a validate parameter—to run a function that checks whether cached objects should be reused. Great for recreating expired database connections!
ttl can now accept timedelta objects—instead of ttl=604800 you can now write ttl=timedelta(days=7).
Last but not least: cached functions can now contain most Streamlit commands! This powerful feature allows you to cache entire parts of your UI. See details here.
What will happen to st.cache?

Starting with 1.18.0, you'll get a deprecation warning if you use st.cache. We recommend you try the new commands the next time you build an app. They'll make your life easier and your apps faster. In most cases, it's as simple as changing a few words in your code. But we also know that many existing apps use st.cache, so we'll keep st.cache around, for now, to preserve backward compatibility. Read more in our migration guide in the docs.

We'll also be deprecating st.experimental_memo and st.experimental_singleton—as they were experimental. The good news is: their behavior is the same as the new commands, so you'll only need to replace one name.

The future of caching

We'll continue to improve caching! The new commands will make it easier to use and understand. Plus, we have more features on our roadmap, such as limiting the amount of memory the cache can use or adding a cache visualizer right into your app.

Track our progress at roadmap.streamlit.app and send us feature requests on GitHub. And if you have any questions or feedback, please leave them in the comments below.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit gains a major new spell book
https://blog.streamlit.io/streamlit-gains-a-major-new-spell-book/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit gains a major new spell book

A tome to the magical fields of Python, algorithms, visualization, and machine learning

By Adrien Treuille
Posted in Product, August 20 2021
Share this post
← All posts

I remember a CS professor of mine pointing out that most of the magic in Harry Potter can now be done on computers! Images dance on our digital newspapers. Cellphones swirl with memories like portable Pensieves. Computer classes are our Charms. Algorithms are our Arithmancy!

If computing departments are the new Hogwarts, then technical tomes are the new spell books. The best works brim with technical secrets and arcana, and represent a totem to some branch of our magical field: Python. Algorithms. Visualization. Machine learning.

I'm therefore particularly excited and proud to share that Streamlit has a major new book, lovingly written by one of our own Streamlit Creators (and Facebook data scientist) Tyler Richards.

This is a true spell book. Yes, other books teach Streamlit, but this is the first which captures the essence of Streamlit. The book demonstrates how Streamlit is transforming the very definition of data science and machine learning.

Throughout the 2010s, data science and machine learning had two basic outputs. On the one hand, you could use a notebook environment to create static analyses. On the other, you could deploy complete machine learning models into production. Streamlit opened up a new middle way between these two: interactive apps that let you play with analyses and share models interactively throughout an organization.

Getting Started with Streamlit for Data Science teaches you how to master this new superpower. You start by creating a basic analysis and work your way up to complete Streamlit apps with fancy graphics and interactive machine learning models.

So go pick up your copy! (Starting August 21, you can even use discount code 25TYLER on Amazon.) Learn the deep secrets of Streamlit. Join our magical community. Share your apps with the world. Contribute to our gallery. Or invent your own spells with custom components. Whether you're a wizard-in-training looking to deploy your first machine learning project or an experienced auror, this book will turn you into a Streamlit sorcerer. 🧙🏽‍♂️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Juan Martín García - Streamlit
https://blog.streamlit.io/author/juan/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Juan Martín García
1 post
Announcing the Figma-to-Streamlit plugin 🎨

Go from prototype to code as easy as 1-2-3 with our new community resource!

Product
by
Juan Martín García
,
November 1 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

table-prediction.png (1342×1462)
https://blog.streamlit.io/content/images/2023/04/table-prediction.png#border


molecule-structure.png (1216×1252)
https://blog.streamlit.io/content/images/2023/04/molecule-structure.png#border


demo-app.png (2000×1516)
https://blog.streamlit.io/content/images/2023/04/demo-app.png#browser


demo-app-2.png (2000×79)
https://blog.streamlit.io/content/images/2023/04/demo-app-2.png#border


column-configuration-page.png (2048×1448)
https://blog.streamlit.io/content/images/2023/06/column-configuration-page.png#browser


connection-logos.png (1772×1528)
https://blog.streamlit.io/content/images/2023/05/connection-logos.png


AIInterviewer-demo.gif (640×328)
https://blog.streamlit.io/content/images/2023/08/AIInterviewer-demo.gif#browser


Johannes Rieke - Streamlit
https://blog.streamlit.io/author/johannes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Johannes Rieke
9 posts
Introducing column config ⚙️

Take st.dataframe and st.data_editor to the next level!

Product
by
Lukas Masuch and 
1
 more,
June 22 2023
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

Product
by
Lukas Masuch and 
2
 more,
February 28 2023
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

Product
by
Tim Conkling and 
2
 more,
February 14 2023
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Built-in charts get a new look and parameters! 📊

Create beautiful charts with one line of code

Release Notes
by
Johannes Rieke and 
1
 more,
August 11 2022
Leverage your user analytics on Streamlit Community Cloud

See who viewed your apps, when, and how popular they are

Product
by
Diana Wang and 
1
 more,
May 17 2022
1.1.0 release notes

This release launches memory improvements and semantic versioning

Release Notes
by
Johannes Rieke
,
October 21 2021
Common app problems: Resource limits

5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud

Tutorials
by
Johannes Rieke
,
September 9 2021
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Lukas Masuch - Streamlit
https://blog.streamlit.io/author/lukasmasuch/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Lukas Masuch
3 posts
Twitter
Introducing column config ⚙️

Take st.dataframe and st.data_editor to the next level!

Product
by
Lukas Masuch and 
1
 more,
June 22 2023
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

Product
by
Lukas Masuch and 
2
 more,
February 28 2023
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Vincent Donato - Streamlit
https://blog.streamlit.io/author/vincent-donato/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Vincent Donato
2 posts
Introducing st.connection!

Quickly and easily connect your app to data and APIs

Product
by
Joshua Carroll and 
1
 more,
May 2 2023
Introducing multipage apps! 📄

Quickly and easily add more pages to your Streamlit apps

Product
by
Vincent Donato
,
June 2 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Amanda Kelly - Streamlit
https://blog.streamlit.io/author/amanda/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Amanda Kelly
Co-founder and COO at Streamlit
5 posts
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

LLMs
by
Adrien Treuille and 
1
 more,
June 15 2023
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

Product
by
Adrien Treuille and 
2
 more,
March 2 2022
How Delta Dental uses Streamlit to make lightning-fast decisions

From an idea to a prototype to production in just two weeks

Case study
by
Amanda Kelly
,
February 1 2022
Streamlit Cloud is now SOC 2 Type 1 compliant

We have completed a full external audit of our security practices

Product
by
Amanda Kelly
,
January 11 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How Streamlit uses Streamlit: Sharing contextual apps
https://blog.streamlit.io/how-streamlit-uses-streamlit-sharing-contextual-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How Streamlit uses Streamlit: Sharing contextual apps

Learn about session state and query parameters!

By Tyler Richards
Posted in Tutorials, May 26 2022
How to get URL query parameters into Streamlit
How to use those parameters in Streamlit widgets
How to sync widgets and your app’s URLs
Wrapping up
Contents
Share this post
← All posts

All of us on the Streamlit Data Science team are massive Streamlit fans (obviously!). In our day jobs, we produce internal Streamlit apps. These apps do everything from helping our partners discover useful tables in our data warehouse, to graphing Streamlit Community Cloud’s monthly active developers over time, to seeing what Streamlit features are rising and falling in use. This means we produce a ton of apps and keep them all in one large, multi-page app.

But as we traveled down this path, we found ourselves in a bit of a pickle. We’d create a new app, find a widget combination that sheds light on something super useful, and would want to share it in that exact state. For example, what if we worked for Uber and wanted to share how NYC rideshare traffic looked at 2 am? Or how the 2 am traffic differed from the 2 pm traffic?

To do this, we could:

Set the app's widgets' default values so that the user could find them on screen load. We'd share the app in this exact initial state, but the users can still interact with it and explore other configurations.
Take app screenshots and send them via Slack, Notion, or email.
Write instructions for rediscovering the finding, either in the app itself or in the message to users, “Hi! We made a new change in our NYC rideshare app. Here is the link. Go ahead and change the slider bar to 2am. We noticed an interesting concentration of rides in the Chelsea area during that time, especially relative to the 2pm time period.”

But these options aren’t all that great.

Option 1 doesn’t take into consideration that we often have many cool findings per app! It also doesn’t work for our multi-page app setup, so it’s out of the picture.
Option 2 defeats the purpose of an app. Why create an interactive app when users will see only a static photo?
Option 3 puts in friction between the user and the insight. This creates a bad user experience.

We solved this by combining session state with passing the URL query parameters to Streamlit apps. In this post, I’ll walk you through a minimum viable app. You’ll learn:

How to get URL query parameters into Streamlit
How to use those parameters in Streamlit widgets
How to sync widgets and your app’s URLs
How to get URL query parameters into Streamlit

This part is the easiest!

Streamlit has a feature called st.experimental_get_query_parameters (read more about it in our docs). It returns a list of parameters that are already in the URL. For example, here is the URL of a locally deployed app http://localhost:8501/?my_name=tyler&month=may.

This URL has two parameters (my_name, month) with values (tyler, may). Or for an app deployed on Streamlit Community Cloud, the link https://share.streamlit.io/tylerjrichards/streamlit_goodreads_app/books.py/?is_checked=True has the parameter is_checked with the value True.

This code pulls the parameters from the URL and prints them out:

import streamlit as st

my_query_params = st.experimental_get_query_params()
st.write("My Query Params:")
st.write(my_query_params)


Now you can save this code in a Python file (mine is streamlit_example_app.py), run it with streamlit run streamlit_example_app.py , and get the URL parameters programmatically!

See how query parameters can be “passed” to your Streamlit app. 👆

NOTE: You probably noticed the experimental_ prefix in our function call. That means it's a feature we're still working on or trying to understand, and it'll go through many iterations as we get feedback from the community. You can find more information about experimental features in our docs!

How to use those parameters in Streamlit widgets

Now that you have the Streamlit URL parameters, you can use them to influence the Streamlit widgets we use in building apps.

For example, let’s create a URL parameter called is_checked and pass it to the value of a Streamlit checkbox. We return the query parameters as lists inside a dictionary, so you can use the get function to pull the parameter (if it exists), and use == "true" to make it a boolean that is passed into your checkbox value parameter:

import streamlit as st

query_params = st.experimental_get_query_params()
my_checkbox = st.checkbox(
    "Example Checkbox",
    value=query_params.get("is_checked", ["False"])[0].lower() == "true",
)
st.write(query_params)


Now if you go to http://localhost:8501/is_checked=True, you’ll see that the checkbox is checked:

See how the checkbox value equals the query parameter. 👆

This is a great V0!

But what if you want the URL to change when you change widget inputs? Typing out URL parameters is annoying. Instead, take advantage of st.session_state and st.experimental_set_query_params().

How to sync widgets and your app’s URLs

We have lots of great documentation on st.session_state. Check it out if this is your first rodeo!

As a super basic introduction, st.session_state is a magic dictionary that doesn’t get reset every time the page updates. Use it to see how users interact with your app across all reruns. And if you want to change your URL parameters, use st.experimental_set_query_params()!

For this to work, you’ll need to ‘reset’ your URL parameter at the end of your app with st.experimental_set_query_params and ensure you’re only reading from st.experimental_get_query_params on the first app run.

To do this, check if the is_check parameter is inside st.session_state. If it’s not, add it to st.session_state as we did above 👆  to get the current query parameters from your URL:

import streamlit as st

query_params = st.experimental_get_query_params()
if "is_checked" not in st.session_state:
    st.session_state["is_checked"] = (
        query_params.get("is_checked", ["False"])[0].lower() == "true"
    )
my_checkbox = st.checkbox("Example Checkbox", key="is_checked")
st.experimental_set_query_params(is_checked=my_checkbox)
st.write(st.session_state)


You’ll notice another change in the code.

Before, you had to assign the default value of the checkbox. Now you can use the key parameter to keep the checkbox synced with is_checked from inside st.session_state! Adding a key to a widget automatically creates a corresponding entry in st.session_state.

The final change is with the st.experimental_set_query_params function. Overwrite the is_checked variable with whatever was last in your checkbox. Query parameters and your Streamlit checkbox are now synchronized! 🎉

Wrapping up

Now you can share your Streamlit apps in the right context and help users play with them!

Massive thanks to Zachary Blackwood for teaching me much of what I know about st.session_state and for helping with the code. Also thanks to the folks in this Twitter thread who gave me the idea to write about how to use the suggested methods.

I’d love to hear how you solve this problem and your ideas for the future of Streamlit. Find me on Twitter as @tylerjrichards and check out the forums to see what our vibrant community is creating.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Adrien Treuille - Streamlit
https://blog.streamlit.io/author/adrien/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Adrien Treuille
11 posts
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

LLMs
by
Adrien Treuille and 
1
 more,
June 15 2023
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

Product
by
Adrien Treuille and 
2
 more,
March 2 2022
☁️ Introducing Streamlit Cloud! ☁️

Streamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.

Product
by
Adrien Treuille
,
November 2 2021
Announcing Streamlit 1.0! 🎈

Streamlit used to be the simplest way to write data apps. Now it's the most powerful

Product
by
Adrien Treuille
,
October 5 2021
Streamlit gains a major new spell book

A tome to the magical fields of Python, algorithms, visualization, and machine learning

Product
by
Adrien Treuille
,
August 20 2021
Our $35 million Series B

We’re excited to announce a new funding round led by Sequoia 🌲

Product
by
Adrien Treuille
,
April 7 2021
Introducing Streamlit Sharing

The new Streamlit platform for deploying, managing, and sharing your apps

Product
by
Adrien Treuille
,
October 15 2020
Introducing Streamlit Components

A new way to add and share custom functionality for Streamlit apps

Product
by
Adrien Treuille
,
July 14 2020
Announcing Streamlit's $21M Series A

Developing new superpowers for the data science community

Product
by
Adrien Treuille
,
June 16 2020
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sample_downloadbutton.png (885×412)
https://blog.streamlit.io/content/images/2023/08/sample_downloadbutton.png#border


sample_swot_slide.png (1354×757)
https://blog.streamlit.io/content/images/2023/08/sample_swot_slide.png#border


Copy-of-Flowchart-1.jpg (1324×340)
https://blog.streamlit.io/content/images/2023/08/Copy-of-Flowchart-1.jpg


sample_logo_slide-1.png (1270×688)
https://blog.streamlit.io/content/images/2023/08/sample_logo_slide-1.png#border


workflow_instant_insight.jpg (2000×849)
https://blog.streamlit.io/content/images/2023/08/workflow_instant_insight.jpg


App Deployment Platform | Share Apps Using Streamlit
https://blog.streamlit.io/deploying-streamlit-apps-using-streamlit-sharing/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Deploying Streamlit apps using Streamlit sharing

A sneak peek into Streamlit's new deployment platform

By Tyler Richards
Posted in Tutorials, October 15 2020
Streamlit background
Streamlit sharing
Conclusion
Contents
Share this post
← All posts

This is a community piece that originally appeared on Towards Data Science - to see the original article click here.

Over the past couple of weeks, I’ve been playing around with a new Streamlit feature called Streamlit sharing, which makes it super easy to deploy your custom apps. I’m going to go through a bit of background first, so if you want to see the docs for Streamlit sharing to get started you can find them here.

Streamlit background

For a bit of background, Streamlit is a framework that lets you quickly and confidently turn a python script into a web app and is an incredible tool for data scientists working on teams where they need to quickly share a model or an interactive analysis, or for data scientists working on personal projects they want to show the world. Here’s a Streamlit beginner tutorial if you want to try it out!

I’ve been using Streamlit for the past ~6 months, and it’s been so useful. Previously, if I knew I wanted to make a web app at the end of a project, I would always opt to switch to R for the wonderful R shiny framework, even though I am a much better python programmer than an R one. Going through Django or flask is just so much development friction to take on that it’s rarely worth it for a personal project and always takes too long for anything at work. But after using Streamlit, I now not only had options but found myself preferring python+Streamlit to R+shiny.

Streamlit sharing

This brings me to a couple of months ago. I started a DS project focused on analyzing reading habits using data from the Goodreads app. I decided to try Streamlit out, and it turned a multi-day long process of getting a Django/flask app running well locally into one that took around a half-hour for local Streamlit use. It really is as easy as throwing your analysis into a script, and calling Streamlit functions whenever you want to put a graph, widget, or text explainer on the app.

However, the most annoying process on Streamlit was the deployment and management process. The tutorial I followed was straightforward, and didn’t take that much time, but was fairly extensive. It required launching an ec2 instance, configuring SSH, using tmux, and going back to this terminal every time you wanted to change anything about your web app. It was doable but annoying.

A few weeks ago, Streamlit saw my Goodreads app and asked if I wanted to test out their Streamlit sharing beta, which was supposed to remove the friction explained above. I, obviously, gave it a shot.

All I had to do was:

Push my app to a Github repo
Add a requirements.txt file that listed all the python libraries I used
Point Streamlit to my app via the link to the repository
Click Deploy

It genuinely was that easy to figure out. I had sectioned off a couple of hours to figure it out, as I expected various bugs to pop up (it is in beta!), but it took me fewer than 10 minutes to get it up and running.

I currently have three apps running, one is a test app, the second is the Goodreads book recommendation app I mentioned earlier, and the third is an interactive analysis of a tech survey that I spun up (from idea to functioning and deployed web app) in around an hour and a half.

Switching to Streamlit sharing has also saved me the ~$5 a month AWS bill, which I would gladly pay for this feature just for the savings in time spent on deployment alone.

If I wanted to try out a new app, I could just click the new app button, point it to my repo, and they would handle literally everything else.

If your Streamlit app uses any other packages, make sure to include a requirements.txt file in your repo — otherwise you’ll immediately get an error when deploying. You can use something like pip freeze to get requirements but that will give you all of the packages in the environment including those that you don’t use in your current project. And that will slow down your app deployment! So I’d suggest using something like pipreqs to keep it to just the core requirements for your app.


pip install pipreqs
pipreqs /home/project/location



If you have requirements for apt-get, add them to packages.txt -, one package per line.

Conclusion

So as a wrap-up, Streamlit sharing has saved me $ on both a development time saved and hosting cost basis (shoutout to the VC funds that make this all possible), has made my personal projects more interactive and prettier, and has taken away the headaches of deploying quick models or analyses. No wonder I’m a Streamlit fan.

Want to see more of this content? You can find me on Twitter, Substack, or on my portfolio site.

Happy Stream(lit)ing!

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sample_peers_slide-1.png (1270×510)
https://blog.streamlit.io/content/images/2023/08/sample_peers_slide-1.png#border


SimiLo: Find your best place to live
https://blog.streamlit.io/similo-find-your-best-place-to-live/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
SimiLo: Find your best place to live

A 5-step guide on how I built an app to relocate within the U.S.

By Kevin Soderholm
Posted in Advocate Posts, August 4 2023
1. Wrangle your data
2. Strategize in users' shoes
3. Methods to your madness
4. Delight with your design
4. 1. Keep it organized
4.2. Display minimal information
4.3. Avoid redundant features
4.4. Create a custom color scheme
4.5. Improve your app's design
5. Don't forget your ethics
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Kevin Soderholm, and I'm a data scientist in the banking industry. My two favorite things about data science are:

Brainstorming. I enjoy breaking down pain points and identifying analytical solutions. It's like a game that keeps me coming back for more.
Building. Whether it's an ML model, a data dashboard, or a Streamlit app, I love starting with nothing and bringing something to life with every line of code.

Recently, I've been searching for a bigger home in a suburb around the Twin Cities (Minneapolis/St. Paul) for my growing family. After spending hours on it, I realized I didn't know how to find the perfect location. I could describe what I wanted down to the square footage and the number of bedrooms, but I wasn't sure WHERE I wanted to go or HOW to decide.

None of the apps out there helped me pick a place on a map, so I decided to build my own. I called it SimiLo (for Similar Locations) and realized that it could also be used to search for vacation spots, do market research, and learn in general!

In this post, I'll show you how I built SimiLo step-by-step:

Wrangle your data
Strategize in users' shoes
Methods to your madness
Delight with your design
Don't forget your ethics!


📍
TLDR: Here's the app and the code. Enjoy!
1. Wrangle your data

If you worked in analytics outside of academia, you know how long it takes to wrangle data. You have to do data cleaning, integration, and transformation—all crucial for analysis. Plus, sourcing data for a personal project can be frustrating. No one will guide you to a database, table, or field. You must determine the type of data that fits your solution and search for it. And unless you have a budget, you'll need to find a free, publicly available source, which can limit your options.

Fortunately, there is plenty of government data available at various geographic levels. And you can also use ChatGPT.

For example:

Sometimes ChatGPT may lead to broken links or outdated information, but other times it'll point you exactly to what you need. Once you gather enough datasets, combine them across geographic levels of zip, city, and county.

There are two ways to do it:

Use the U.S. Department of Housing and Urban Development's "Cross-walk" files for mapping zip codes to counties. It's challenging since zip codes cross county lines, resulting in a many-to-many merge that can be handled using population density for tie-breakers (read more here).
Map zip codes to cities using the U.S. Postal Service city designations. USPS cities often lump together nearby municipalities. For example, some suburbs around the Minneapolis—St. Paul metro area all roll up to the USPS city of St. Paul. This is the only clean way to roll up zip codes to cities, and the city is an important design component. Why? Because people think about cities, not zip codes.

Have you got your datasets? It's time to explore, clean, and create! (This is the most fun part of data wrangling.)

Start by examining your datasets from top to bottom and inside and out—view the distributions of all relevant fields, interpret the values, explore relationships, etc. In the real world, data is rarely clean and requires many small changes such as feature engineering, missing imputations, potential outlier treatment, data transformations, and so on (the list may never end until you throw in the towel and call it good enough).

Here is an example of data wrangling:

#read  and clean water file
wf = pd.read_csv('/folder/water.txt', delimiter=None, dtype={'GEOID': str})

# remove extra blank space from last column of dataframe
wf.iloc[:, -1] = wf.iloc[:, -1].str.strip()

#assign delimiter
wf = wf.iloc[:, -1].str.split('\\t', expand=True)

#rename cols
wfcols=['ZCTA5','ALAND','AWATER','ALAND_SQMI','AWATER_SQMI','LAT','LON']
wf = wf.rename(columns=dict(zip(wf.columns, wfcols)))

#change data type to float (numeric with decimal precision)
wf['ALAND'] = wf['ALAND'].astype(float)
wf['AWATER'] = wf['AWATER'].astype(float)
wf['ALAND_SQMI'] = wf['ALAND_SQMI'].astype(float)
wf['AWATER_SQMI'] = wf['AWATER_SQMI'].astype(float)

#create features
wf['Pct_Water']=wf['AWATER']/wf['ALAND']
wf['Tot_Area']=wf['ALAND_SQMI']+wf['AWATER_SQMI']

#filter cols
wf=wf[['ZCTA5','LAT','LON','Pct_Water','Tot_Area']]


Now your datasets are clean, curated, and ready for Streamlit ingestion. But first… you'll need to strategize.

2. Strategize in users' shoes

Before building your app, do a strategy session. Brainstorm. Think about the user's perspective. This will shape the app's functionality and design.

This is what changed SimiLo. Initially, I wanted the user to select the location criteria so the app would give them a ranked locations list. But it was clunky and confusing. What criteria combination would help them find a specific type of location? I was selecting values that were too extreme and produced unsatisfactory results. This led me to COMPARE locations rather than FILTER them. It was more effective to start with a location I knew and see similar locations across the U.S.

This workflow was more fun, the results clearer, and the approach more intuitive:

I also added the ability to save searches and carry out further research. While the app generates similar locations, it makes a table with a free-form text field. You can manipulate it and download it as a CSV file.

To implement this feature, use the experimental data editor function:

edited_df=st.experimental_data_editor(df)
save=edited_df[edited_df['SAVE']==True].reset_index()
csv = convert_df(save[cols+['SAVE','NOTES']])
st.download_button(label="Download Selections as CSV",data=csv,file_name='SIMILO_SAVED.csv',mime='text/csv',)


Now let's find and apply the analytical methods to make the machine work!

3. Methods to your madness

If you love learning new methods and adding them to your tool belt, you'll enjoy this step the most. I most definitely did, as I had to learn similarity scoring methods to answer these two questions:

Which key metrics represented a location and could be used for comparison? I combed through my datasets, selected 20, and split them into four categories: People, Home, Work, and Environment.

How to prepare the data, calculate similarity, and present the results to the user? I tried many iterations with different data transformations and similarity methods, including Euclidean distance, Cosine, and Jaccard similarity, and settled on a 3-step process for each:

Normalize the values for each metric, putting them on the same scale.
Calculate the Euclidean distance between those values for a selected location and the values for every other location in your dataset.
Scale the calculated distances into a score from 1-100, creating an easy-to-interpret similarity ranking for each data category.


#Columns to normalize
people_cols_to_norm = [‘A’,’B’,’C’,’D’,’E’]

#New columns
scaled = ['A_sc','B_sc','C_sc','D_sc','E_sc']

#Normalization
scaler = StandardScaler()
df[scaled] = scaler.fit_transform(df[people_cols_to_norm])
  
#Calculate the euclidian distance between the selected record and the rest of the dataset
people_dist = euclidean_distances(df.loc[:,scaled], selected_record[scaled].values.reshape(1, -1))

#Create a new dataframe with the similarity score and the corresponding index of each record
df_similarity = pd.DataFrame({'PEOPLE_SIM': people_dist [:, 0], 'index': df.index})

#scale distance to 1-100 score
people_max=df_similarity['PEOPLE_SIM'].max()
df_similarity['PEOPLE_SCORE']  = 100 - (100 * df_similarity['PEOPLE_SIM'] / people_max)


At this point, you have four similarity scores ranging from 1-100 for each data category and location: People, Home, Work, and Environment.

To have a single comparison metric create an overall similarity score calculated as a weighted average of the individual category distances, scaled to a score from 1-100. Now you have an easy-to-interpret way to rank locations based on their similarity to the user's selected location:

Oh, and one more thing. In Advanced Settings, in the Data Category Importance section, users can increase or decrease the impact of any individual data category using slider widgets. These adjustments dynamically update the weights of each category, which are used to calculate the overall similarity score:

Well done! You have learned how to build the app. But don't forget about design.

4. Delight with your design

Let's face it. Many of us data scientists lack free-spirited creativity. We're logical and analytical and don't often use the right side of our brain. Streamlit allows you to flex those forgotten muscles and your artistic side. Plus, neat organization and minimalism will make your app look simple, even if it's hiding lots of information and complex algorithms.

4. 1. Keep it organized

To help users process different pieces of information one at a time and keep it organized, use containers and dividers:

4.2. Display minimal information

To avoid overwhelming the user, display minimal information on the main search page. The user will see the extended workflow ONLY after making a selection. Present the data in separate tabs with the options and the instructions hidden behind expanders so the user can uncover information in small increments vs. all at once.

4.3. Avoid redundant features

To avoid redundant features, mix up the input widgets: buttons, radio buttons, checkboxes, selectboxes, multiselects, sliders, and number inputs. Streamlit offers diverse components for creating a unique app—explore the different layouts and get creative!

4.4. Create a custom color scheme

Create a custom color scheme to delight users with your app's aesthetics. Using the same color scheme for multiple apps can make them look alike (for SimiLo, I experimented with dozens of hex color code combinations). Simply use config.toml file functionality to control the background, the chart color scales, and the font colors:

[theme]
primaryColor="#3fb0e8"
backgroundColor="#192841"
secondaryBackgroundColor="#3fb0e8"
textColor="#f9f9f9"

4.5. Improve your app's design

To improve your app's design even further, embed animations or videos. This adds a level of professionalism that doesn't go unnoticed. I embedded a tutorial video with animations from LottieFiles, an open-source animation file format that you can add to Streamlit (learn more here):

Now you have a fully functional, beautiful app. Time to publish? Not so fast. Don't forget to check your ethics!

5. Don't forget your ethics

You might wonder why you should check your ethics when you know you're a good person with good morals and intentions. Well, good for you. The question is not about YOUR ethics; it's about you mitigating the possibility of your app being USED unethically.

In the case of SimiLo, I made two critical decisions:

App's functionality. Should I let the user hand-select the criteria and get a list of locations, or should I let them first pick a location and get a list of comparisons? Choosing the latter helped close the door to some unethical use cases. For example, I didn't want to create an app where someone could pick locations based on the race or income of its inhabitants.
The data used in the app. I purposefully omitted some demographic data, such as race, because it didn't add value and opened the door for unethical use. I carefully considered what data to show and what elements to use in my similarity scoring process.

There is a lot of buzz about the ethical use of AI. I believe that data scientists are key players in promoting ethical use and preventing unethical use because they understand their product the best. Third-party risk teams often don't have the domain knowledge about the methods or data to properly evaluate ta data science solution'sfairness, bias, and ethics. So before you hit publish, think about how your app COULD be used, not just how it SHOULD be used—you might find some open doors worth shutting. 🚪

Wrapping up

Let's summarize the 5-step process I used to build SimiLo. Once I defined the problem (I wanted to move) and expanded my proposed solution (moving, vacationing, market research), I started with Step 1—data wrangling and data sourcing with ChatGPT, exploratory data analysis (EDA), cleaning, transformations, and feature engineering. In Step 2, I hit the whiteboard and strategized the app components from the users' perspective. Step 3 was researching and applying similarity scoring methods in a framework that worked best for my app. Step 4 was to make my app look and feel appealing through various design strategies. Finally, in Step 5, I checked the ethical considerations before publishing.

Just because this article is linear doesn't mean real life is. You don't need to do these steps in order. In some cases, you can do them in tandem, in others, as part of an iterative rotation, but in most cases, you'll revisit each step several times.

I hope this post helped you find a gem or two to take to your next app. If you have any questions, please post them in the comments below or contact me on LinkedIn or Twitter.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!
https://blog.streamlit.io/auto-generate-a-dataframe-filtering-ui-in-streamlit-with-filter_dataframe/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

By Tyler Richards, Arnaud Miribel and Zachary Blackwood
Posted in Tutorials, August 18 2022
What is filter_dataframe?
Code Section 1. Laying out Streamlit widgets
Code Section 2. Preparing the input dataframe for filtering
Code Section 3. Writing conditionals for different column types
Bringing it all together
Wrapping up
Contents
Share this post
← All posts

Streamlit apps often have some sort of a filtering component where developers write code snippets that combine Streamlit inputs, st.dataframe, or the ever-popular streamlit-aggrid component to create easy-to-use UIs.

We on the Streamlit data science team certainly have followed this trend with the following format:

Use a set of Streamlit native inputs like st.date_range or st.selectbox to get user input.
Use the inputs to filter our data and display it back to the user.

Writing this type of app the first time was fun. But doing it the fifth time—writing different code iterations, figuring out what Streamlit widget fits what data type, configuring it all together—got tiring pretty quickly. To solve this, we built a function filter_dataframe that handles this for us automatically!

In this post, we’ll show you how filter_dataframe works section by section:

Code Section 1. Laying out Streamlit widgets

Code Section 2. Preparing the input dataframe for filtering

Code Section 3. Writing conditionals for different column types

Want to dive right in? Head over to our demo app see it on an example dataframe, and see the full code here.

What is filter_dataframe?

The functionfilter_dataframe lets you:

Add a filtering UI to any dataframe
Speed up the development time
Allow the user to explore a dataset in a self-service way

Here is the code for it:

from pandas.api.types import (
    is_categorical_dtype,
    is_datetime64_any_dtype,
    is_numeric_dtype,
    is_object_dtype,
)
import pandas as pd
import streamlit as st


def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds a UI on top of a dataframe to let viewers filter columns

    Args:
        df (pd.DataFrame): Original dataframe

    Returns:
        pd.DataFrame: Filtered dataframe
    """
    modify = st.checkbox("Add filters")

    if not modify:
        return df

    df = df.copy()

    # Try to convert datetimes into a standard format (datetime, no timezone)
    for col in df.columns:
        if is_object_dtype(df[col]):
            try:
                df[col] = pd.to_datetime(df[col])
            except Exception:
                pass

        if is_datetime64_any_dtype(df[col]):
            df[col] = df[col].dt.tz_localize(None)

    modification_container = st.container()

    with modification_container:
        to_filter_columns = st.multiselect("Filter dataframe on", df.columns)
        for column in to_filter_columns:
            left, right = st.columns((1, 20))
            # Treat columns with < 10 unique values as categorical
            if is_categorical_dtype(df[column]) or df[column].nunique() < 10:
                user_cat_input = right.multiselect(
                    f"Values for {column}",
                    df[column].unique(),
                    default=list(df[column].unique()),
                )
                df = df[df[column].isin(user_cat_input)]
            elif is_numeric_dtype(df[column]):
                _min = float(df[column].min())
                _max = float(df[column].max())
                step = (_max - _min) / 100
                user_num_input = right.slider(
                    f"Values for {column}",
                    min_value=_min,
                    max_value=_max,
                    value=(_min, _max),
                    step=step,
                )
                df = df[df[column].between(*user_num_input)]
            elif is_datetime64_any_dtype(df[column]):
                user_date_input = right.date_input(
                    f"Values for {column}",
                    value=(
                        df[column].min(),
                        df[column].max(),
                    ),
                )
                if len(user_date_input) == 2:
                    user_date_input = tuple(map(pd.to_datetime, user_date_input))
                    start_date, end_date = user_date_input
                    df = df.loc[df[column].between(start_date, end_date)]
            else:
                user_text_input = right.text_input(
                    f"Substring or regex in {column}",
                )
                if user_text_input:
                    df = df[df[column].astype(str).str.contains(user_text_input)]

    return df


Now let’s take a look at how it works!

Code Section 1. Laying out Streamlit widgets

The filter_dataframe function inputs and outputs the same thing—a pandas dataframe. Within the function, we first ask the user if they’d like to filter the dataframe with a checkbox called modify.

We also added comments and type hints to the top of the function to make the code more digestible:

def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds a UI on top of a dataframe to let viewers filter columns

    Args:
        df (pd.DataFrame): Original dataframe

    Returns:
        pd.DataFrame: Filtered dataframe
    """
    modify = st.checkbox("Add filters")

    if not modify:
        return df 

Code Section 2. Preparing the input dataframe for filtering

There are several steps you need to take to prep your dataframe for your app. For the first three you need to:

Make a copy of the pandas dataframe so the user input will not change the underlying data.
Attempt to cast string columns into datetimes with pd.to_datetime().
Localize your datetime columns with .tz_localize(). The Streamlit date picker (which you’ll use later!) returns dates without a timezone, so you need to take this step to compare the two:
df = df.copy()

# Try to convert datetimes into a standard format (datetime, no timezone)
for col in df.columns:
    if is_object_dtype(df[col]):
    	try:
    	    df[col] = pd.to_datetime(df[col])
    	except Exception:
    	    pass

    if is_datetime64_any_dtype(df[col]):
        df[col] = df[col].dt.tz_localize(None)


Now that your data is in a better format, you need to:

Set up a container with st.container for your filtering widgets.
Use st.multiselect to let the user select the columns:
modification_container = st.container()
with modification_container:
    to_filter_columns = st.multiselect("Filter dataframe on", df.columns)

Loop through each column and deal with each one depending on its type. You’ll write conditionals for each one next!
Add indentation and an arrow to improve the aesthetics when users select many columns.
for column in to_filter_columns:
    left, right = st.columns((1, 20))
    left.write("↳")


All your data is in the right format. You ensured that your original dataset will remain untouched, and you have prepared your loop to go through all your columns. Now comes the fun part!

Code Section 3. Writing conditionals for different column types

In this function, you’ll want to check for three pandas data types—categorical, numeric, and datetime—then handle the rest as if they’re strings. This is an assumption that works well for us. Your situation might be different, so feel free to add your own conditionals to this list.

For each one create a Streamlit widget that matches your type, then filter your data based on that widget. At the end of this loop, you’ll have to return the entire filtered dataframe.

Let’s take a look at them one by one.

Categorical types

Check for categorical types with the is_categorical_dtype function. Often users don’t cast their data into this type, so assume that anything with fewer than 10 unique values acts like a categorical dtype. As a bonus, it’ll work great with boolean columns (which only have True or False values!).

Now, create a multiselect widget with possible values and use it to filter your dataframe:

# Treat columns with < 10 unique values as categorical
if is_categorical_dtype(df[column]) or df[column].nunique() < 10:
    user_cat_input = right.multiselect(
        f"Values for {column}",
        df[column].unique(),
        default=list(df[column].unique()),
    )
    df = df[df[column].isin(user_cat_input)]


Numeric types

Numeric types are fairly straightforward. You can get the minimum and the maximum from the dataset itself, then assume that the step function is 1% of the range and filter the data accordingly:

elif is_numeric_dtype(df[column]):
    _min = float(df[column].min())
    _max = float(df[column].max())
    step = (_max - _min) / 100
    user_num_input = right.slider(
      f"Values for {column}",
      min_value=_min,
      max_value=_max,
      value=(_min, _max),
      step=step,
    )
    df = df[df[column].between(*user_num_input)]

Datetime types

The datetime dtype is almost the same as the other two. You get the user input with the st.date_input function. Once the user enters two dates, you can filter your dataset:

elif is_datetime64_any_dtype(df[column]):
    user_date_input = right.date_input(
        f"Values for {column}",
        value=(
            df[column].min(),
            df[column].max(),
        ),
    )
    if len(user_date_input) == 2:
        user_date_input = tuple(map(pd.to_datetime, user_date_input))
        start_date, end_date = user_date_input
        df = df.loc[df[column].between(start_date, end_date)]


Other types

We like to convert other dtypes into a string, then let the user search within them for substrings. It might not work for your use case, but for us, it works quite well:

else:
    user_text_input = right.text_input(
        f"Substring or regex in {column}",
    )
    if user_text_input:
        df = df[df[column].astype(str).str.contains(user_text_input)]

Bringing it all together

Want to see what the code looks like in action? Go ahead and test it on the palmerpenguins dataset (see this GitHub repo for the data) or on your own data!

We’ve made an example app using the code (check it out below):

import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
from pandas.api.types import (
    is_categorical_dtype,
    is_datetime64_any_dtype,
    is_numeric_dtype,
    is_object_dtype,
)

st.title("Auto Filter Dataframes in Streamlit")
st.write(
    """This app accomodates the blog [here](<https://blog.streamlit.io/auto-generate-a-dataframe-filtering-ui-in-streamlit-with-filter_dataframe/>)
    and walks you through one example of how the Streamlit
    Data Science Team builds add-on functions to Streamlit.
    """
)

def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    <insert the code we wrote here>

data_url = "https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv"

df = pd.read_csv(data_url)
st.dataframe(filter_dataframe(df))

This code will produce the following app:

Wrapping up

You did it! Now you know how to set up your own filter_dataframe function.

If you have any questions or improvements, please drop them in the comments below or make a suggestion on our GitHub repository for this post.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

sample_financials_slide-1.png (1213×731)
https://blog.streamlit.io/content/images/2023/08/sample_financials_slide-1.png#border


sample_overview_slide.png (1346×761)
https://blog.streamlit.io/content/images/2023/08/sample_overview_slide.png#border


streamlit-for-data-science-book.png (1346×1660)
https://blog.streamlit.io/content/images/2023/09/streamlit-for-data-science-book.png#border


Screenshot-2023-10-04-at-11.16.52-AM.png (1948×1036)
https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-04-at-11.16.52-AM.png#browser


How to build your own Streamlit component
https://blog.streamlit.io/how-to-build-your-own-streamlit-component/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build your own Streamlit component

Learn how to make a component from scratch!

By Zachary Blackwood
Posted in Tutorials, September 15 2022
What’s a Streamlit component?
Build a basic component
Step 1. Use cruft to create the boilerplate code
Step 2. Add basic HTML
Step 3. Add JavaScript to return the data when a key is pressed
Step 4. Set up your Python code
Step 5. Add CSS for styling
Publish your component for others to use
Step 1. Push your code to GitHub
Step 2. Create a PyPI account and make an API token
Step 3. Create a release on GitHub (it’ll trigger a release to PyPI)
Step 4. Go to the Actions tab to see if the release succeeded, then test it!
Bonus tips:
Wrapping up
Resources
Contents
Share this post
← All posts

Want to build your first Streamlit component but feel too intimidated to get started? We’ve got you covered.

In this post, you’ll learn how to:

Build a basic component
Publish your component for others to use

Want to dive right in? Here’s the repository for an example component and an app showing it in action.

But first, let’s do a quick refresher on terminology…

👉
NOTE: This post explains a very simple way of building components. If you want a more sophisticated way that includes npm and TypeScript, check out the official components template.
What’s a Streamlit component?

A Streamlit component is a shareable Streamlit plugin that lets you add new visuals and interactivity to apps (read more in our docs).

Why would you want to use or build components? Because although Streamlit has a ton of built-in functionality, sometimes you might want to add visualization or interactivity to your apps that’s not available in Streamlit (yet) and then share it with the community.

There are three types of components:

Python-only components: Python code that doesn’t require custom HTML/JavaScript.
Static components: Python + HTML/JavaScript that can be embedded in your app.
Bidirectional components: Python + HTML/JavaScript that can be embedded in your app and can return data back to it.

Wondering what components to build? Here are some ideas:

A new baseweb component that’s not available in Streamlit.
A clickable graph that lets you use the clicked value in your app.
A text input box that returns the value as you type—no “enter” required (this is what we’ll be building together in this post).
Check out the community components tracker to see what sorts of components have already been built.
Build a basic component
Step 1. Use cruft to create the boilerplate code

Cruft gives you pre-existing project templates. You can install it with pip, pipx, or brew, then run this command: cruft create https://github.com/blackary/cookiecutter-streamlit-component/.

Next, put in your component's details:

author_name [Bob Smith]: Zachary Blackwood
author_email [bob@example.com]: example@example.com            
project_name [Streamlit Component X]: Streamlit Keyup
package_name [streamlit-keyup]: streamlit-keyup
import_name [streamlit_keyup]: st_keyup 
description [Streamlit component that allows you to do X]: A streamlit component that allows you to get input from a textbox after every key press
deployment_via_github_actions [y]: y
Select open_source_license:
1 - MIT license
2 - BSD license
3 - ISC license
4 - Apache Software License 2.0
5 - GNU General Public License v3
6 - Not open source
Choose from 1, 2, 3, 4, 5, 6 [1]: 1
💡
NOTE: Call your component whatever you want. But if you want it to be easily publishable on PyPI through GitHub actions, put y for the deployment_via_github_actions question. It’s the default value, so you can just press <enter> when you have to accept it.

This template will create the following structure:

streamlit-keyup
├── .cruft.json
├── .github
│   └── workflows
│       └── publish_PYPI_each_tag.yml
├── .gitignore
├── LICENSE
├── MANIFEST.in
├── README.md
├── requirements.txt
├── setup.py
└── src
    └── st_keyup
        ├── __init__.py
        └── frontend
            ├── index.html
            ├── main.js
            ├── streamlit-component-lib.js
            └── style.css

You’d generate this layout if you were to release this package as streamlit-keyup (imported as import st_keyup), so these are the values for package_name and import_name.

Step 2. Add basic HTML

Cruft will make a file for you at src/st_keyup/frontend/index.html.

Just add new tags in the body:


<body>
  <div id="root">
    <label id="label" for="text_input">This is a label</label>
    <div class="input">
	  <input type="text" name="text_input" id="input_box" />
    </div>
  </div>
</body>

Next, run streamlit run src/st_keyup/__init__.py. Since the boilerplate __init__.py file has some content, this is what you’ll see:

Nothing fancy yet, but you’re getting there!

Step 3. Add JavaScript to return the data when a key is pressed

The frontend/ folder has a main.js file with most of the standard code. Add code only inside the onRender function and the if (!window.rendered) block.

The code will do the following:

1. Get the values that the user has specified when using the component (label and value):

const {label, value} = event.detail.args;


2. Get the <label> tag from your HTML page and set the text to the user’s label:

const label_el = document.getElementById("label")
label_el.innerText = label


3. Get the <input> tag from your HTML page and—if the user specified a default value for the input—set the starting value to that:

const input = document.getElementById("input_box");
if (value) {
  input.value = value
}


4. Whenever there is a “keyup” event on the input tag (meaning, the user hit a key), send the current input value back to the component:

input.onkeyup = event => sendValue(event.target.value)


The final onRender function will look like this:

function onRender(event) {
  // Only run the render code the first time the component is loaded.
  if (!window.rendered) {
    // Grab the label and default value that the user specified
    const {label, value} = event.detail.args;

    // Set the label text to be what the user specified
    const label_el = document.getElementById("label")
    label_el.innerText = label

    // Set the default value to be what the user specified
    const input = document.getElementById("input_box");
    if (value) {
      input.value = value
    }

    // On the keyup event, send the new value to Python
    input.onkeyup = event => sendValue(event.target.value)

    window.rendered = true
  }
}


5. As an extra step, at the end of the script set the widget’s height to 85px:

Streamlit.setFrameHeight(85)


At this point, your Python code isn’t properly passing the label and the value. To see the JavaScript pass back the typed value to Streamlit, run the app again and type in the input box:

You’re almost there!

Step 4. Set up your Python code

To specify the label and the value correctly, update the st_keyup function in src/st_keyup/__init__.py:

def st_keyup(
    label: str,
    value: Optional[str] = "",
    key: Optional[str] = None,
):
    """
    Create a Streamlit text input that returns the value whenever a key is pressed.
    """
    component_value = _component_func(
        label=label,
        value=value,
        key=key,
        default=value
    )

    return component_value


For your demo Streamlit app to work, update the main function:

def main():
    st.write("## Example")
    value = st_keyup("This is a label!")

    st.write(value)

    st.write("## Example with value")
    value2 = st_keyup("With a default value!", value="Default value")

    st.write(value2)


Run streamlit run src/st_keyup/__init__.py and you’ll see something like this:

Your component is working!

Want it to look like the built-in Streamlit textbox? Let’s add some CSS to it.

Step 5. Add CSS for styling

Fiddling with CSS may take a while. You can use your browser’s DevTools to copy the CSS from the built-in Streamlit st.text_input to your style.css file (see an example of the final CSS here).

Add it and reload the page. Your app will look something like this:

Publish your component for others to use

It’s fun to have a component for yourself, but it’s more useful to publish it as a PyPI package. There is a file called .github/workflows/publish_PYPI_each_tag.yml that tells GitHub to publish the latest version of your package to PyPI every time you make a new release.

Here is how to make it happen:

Step 1. Push your code to GitHub

Go to https://github.com/new and read the instructions on how to create a new repository. Don’t add a README, a .gitignore, or a license as they’re already in your repository.

Step 2. Create a PyPI account and make an API token

Go to pypi.org and create an account, then go to https://pypi.org/manage/account/#api-tokens and create a new API token for this project. Copy the API key, go to your repo’s settings and choose secrets:

As it said in the PyPI instructions when you created the API key, set your PYPI_USERNAME as __token__ and put the token value in PYPI_PASSWORD.

Step 3. Create a release on GitHub (it’ll trigger a release to PyPI)

On the right side of your repo, click on the “Releases” link, then on “Draft a new release,” and choose a tag (in your setup.py, the version is set to 0.1.0, so make that your release tag with a v in front—like v0.1.0).

Step 4. Go to the Actions tab to see if the release succeeded, then test it!

If the initial release has succeeded, you’ll see something like this:

To test if it worked, go to pypi.org and look for the new package. Then try doing pip install <your-package-name>.

💡
NOTE: You won’t be able to release this exact package name on PyPI (because I already did it), but this should work when you release your own components. If you want to test releasing this package (or a dummy package), change the name in setup.py and use an account on test.pypi.org instead of pypi.org. Read more about using test.pypi.org here.
Bonus tips:
Don’t forget to update the version in setup.py every time you make a new release.
Once you have a GitHub repository, add a URL argument in setup.py that points to it.
Add the streamlit-component tag to your GitHub repo.
Announce your component on the Forum.
Wrapping up

Now you know how to make your first component! Hopefully, it’ll inspire you to create more components and to keep making Streamlit a better tool for building all sorts of apps.

If you have any questions, feel free to post them in the comments below or on the Forum.

Happy Streamlit-ing! 🎈

Resources
Introducing Streamlit components
Streamlit Components, security, and a five-month quest to ship a single line of code
Developing a streamlit-webrtc component for real-time video processing
Build knowledge graphs with the Streamlit Agraph component
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

app-diagram.png (1920×1080)
https://blog.streamlit.io/content/images/2023/08/app-diagram.png


Improving healthcare management with Streamlit
https://blog.streamlit.io/improving-healthcare-management-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Improving healthcare management with Streamlit

How to build an all-in-one analytics platform for small clinics

By Matteo Ballabio and Luca A Cappellini
Posted in Advocate Posts, July 17 2023
Why create Mehedi?
1. How to create a dynamic form as data entry
2. How to create an interactive dashboard for patient satisfaction
3. How to connect Google Sheets to Streamlit
4. How to create a multi-page app
5. How to create data manipulation techniques
6. How to use ML algorithms to improve the prediction of goal metrics
Wrapping up
Contents
Share this post
← All posts

Hey community, 👋

We're three Italian guys born and raised in the charming Brianza and the bustling Milan: Matteo (a data analyst), Luca (a medical doctor), and Federico (a student). We live 10 kilometers apart, but LinkedIn has connected us. When we met in person for coffee ☕ (yes, we're Italians!), we discovered a shared goal: to innovate the Italian healthcare system.

In Italy, the public healthcare system is funded through taxes. But due to high demand and long wait times, the existing coverage is inadequate, and the data for analysis is missing. So we created Mehedi (stands for "Medical Health Data Insights")—an integrated analytics platform for small clinics to improve decision-making and patient care:

In this post, we'll show you:

How to create a dynamic form as data entry
How to create an interactive dashboard for patient satisfaction
How to connect Google Sheets to Streamlit
How to create a multi-page app
How to create data manipulation techniques
How to use ML algorithms to improve the prediction of goal metrics


🩺
This post is rather long, so you can jump straight into the app and the code if you want.
Why create Mehedi?

Over the last ten years, healthcare data has grown exponentially. This created an opportunity for:

Direct impact analytics (clinical and research) to create a benefit for the patient.
Indirect analytics (analytical tools or systems) to create a benefit for all stakeholders.

We wanted to display the data in a simple dashboard:

The shift towards "Value-Based Care" required new metrics to evaluate performance—to benefit patients, healthcare professionals, and providers. Patient satisfaction is important, but few tools collect patient experience metrics (not connected to traditional metrics for actionable insights).

Streamlit was the perfect tool for Mehedi:

It connected multiple sources in a database tier
It elaborated on the data and logic in an app tier
It collected patient data and feedback through a survey and presents it beautifully (it can even be customized by medical service type!)

Here is the app itself:

And now, let's get to building it!

1. How to create a dynamic form as data entry

The entry point to Mehedi is the patient experience survey. It has targeted questions and free text boxes for general suggestions, opinions, and feedback.

Let's consider a scenario with three different dynamic forms that depend on a slider representing a patient's appointment time duration.

To get started, follow these steps:

Import the necessary Streamlit library using import streamlit as st
Create a slider to capture the appointment time using appointment_time = st.slider("Select appointment duration", min_value, max_value)
Based on the value of the slider, use conditional statements to display the appropriate dynamic form:
# Using a slider, you can choose the time available.
slider = st.slider(label='Trascina lo slider', min_value=1,max_value=10, value=1, key='Form5')

# ###FORM 1
if slider<4:
		col1,  col2 = st.columns([1, 0.60])
		with col1:
		    new_title = '<b style="font-family:serif; color:#FF0000; font-size: 40px;">📋 MEDi Experience Form:</b>'
		    st.markdown(new_title, unsafe_allow_html=True)
		    st.info("➡️ 1. Come ha preso l'appuntamento?")
		    cols = st.columns((1, 1))
		    #APPUNTAMENTO
		    var_a1 = cols[0].selectbox("Ho preso un appuntamento:",  ["Personalmente",  "Telefono",  "Sito Web", "E-mail",  "Tramite medico",  "Altro"])
		    var_a2= cols[1].slider("Quanto è soddisfatto della facilità di fissare un appuntamento?", 1, 7, 1)
		with col2:
				DATA = [{"taste": "APPUNTAMENTO", "Peso Area": var_a2},
		            {"taste": "ACCOGLIENZA", "Peso Area": med_accoglienza},
		            {"taste": "PROCEDURE", "Peso Area": var_d2},
		            {"taste": "RISULTATI", "Peso Area": var_f3},
		            {"taste": "ESPERIENZA", "Peso Area": med_experience}]
		    graph_pes(DATA)
# ###FORM 2
elif slider>3 and slider<8:
		## add the questions for form 2
# ###FORM 3
elif slider>7:
		## add the questions for form 3


Why not use st.form()? Because you want to create a form that provides feedback on progress after each user interaction. With st.form(), the user can evaluate only what they entered in the form after the submit section.

Just use "Nivo" and pass the necessary data as input:

from streamlit_elements import elements, mui
from streamlit_elements import nivo

DATA = [{"taste": "RISULTATI", "peso_area": variable1},
        {"taste": "CONSAPEVOLEZZA E FIDUCIA", "peso_area": variable2},
        {"taste": "COINVOLGIMENTO", "peso_area": variable3},
        {"taste": "DISTINTIVITA'", "peso_area": variable4},
        {"taste": "COMPORTAMENTI", "peso_area": variable5}]

# radar chart example
def graph_pes(DATA):
        with elements("nivo_charts"):        
            with mui.Box(sx={"height": 400}):
                nivo.Radar(
                    data=DATA,
                    keys=["peso_area"],
                    indexBy="taste",
                    maxValue=7,
                    valueFormat=">-.2f",
                    margin={"top": 80, "right": 60, "bottom": 80, "left": 60},
                    gridLabelOffset=36,
                    dotSize=10,
                    dotColor={"theme": "background"},
                    dotBorderWidth=1,
                    fillOpacity=0.85,
                    borderWidth=2,
                    borderColor="#e08367",
                    dotBorderColor="#e08367",
                    motionConfig="wobbly",
                    legends=[
                        {
                            "anchor": "top-left",
                            "direction": "column",
                            "translateX": -70,
                            "translateY": -100,
                            "itemWidth": 40,
                            "itemHeight": 20,
                            "itemTextColor": "#999",
                            "symbolSize": 12,
                            "symbolShape": "circle",
                            "effects": [
                                {
                                    "on": "hover",
                                    "style": {
                                        "itemTextColor": "#000"
                                    }
                                }
                            ]
                        }
                    ],
                    theme={
                        "background": "#E4E3E3",
                        "textColor": "#31333F",
                        "grid": {"line": {
                                            "stroke": "#b3bcc4",
                                            "strokeWidth": 1
                                        }
                                    },
                        "tooltip": {
                            "container": {
                                "background": "#E4E3E3",
                                "color": "#31333F",
                            }
                        }
                    }
                )

2. How to create an interactive dashboard for patient satisfaction

Streamlit is excellent for integrating multiple data sources, enabling vital data harmonization and pipeline. It can integrate diverse data sources, including ERP systems from healthcare organizations.

The dashboard helps managers make adjustments, measure their effects, and improve patient centrality:

To create a patient satisfaction dashboard, follow these steps:

Install the necessary libraries:

pip install streamlit plotly altair pandas


Import the required modules in your Streamlit app:

import streamlit as st
import pandas as pd
import plotly.express as px


Load and preprocess your patient satisfaction data:

# Load the data from a data source (reading gsheet to dataframe)
sheet_url = "url"
url_1 = sheet_url.replace("/edit#gid=", "/export?format=csv&gid=")
df=pd.read_csv(url_1)

# Define Filter on sidebar
a, b, c = st.sidebar.columns([0.05,1,0.05])
with a:
    st.write("")
with b:
    Proced_Fil=st.multiselect("Tipo Procedura", df["Tipo_procedura"].unique(),  default=["RMN", "Raggi X", "CT"])
    Sesso_Fil=st.multiselect("Sesso", df["Sesso"].unique(),  default=["Maschio", "Femmina", "Non Specificato"])
    Eta_Fil=st.multiselect("Fasce di età", df["Range_Età"].unique(),  default=["18-30anni"])
    st.image(image3, width=170)
with c:
    st.write("")

# Inizialize a filtered dataframe with pandas query

df_selection = df.query("Tipo_procedura == @Proced_Fil & Sesso == @Sesso_Fil & Range_Età == @Eta_Fil")


Create interactive visualizations using Plotly, Altair, or other libraries:

# Use Plotly to create a bar chart
df42= df.groupby(pd.Grouper(key='Timestamp', axis=0,freq='1W')).count().reset_index()
df42.rename(columns={'Sesso':'Form_Inviati'}, inplace=True)
df42["target"]=10
df42["MA_REPORT"]=df42["Form_Inviati"].rolling(2).mean()
st.header("Bar Chart Numero report inviati per settimana")
fig=px.bar(df42, x ="Timestamp", y='Form_Inviati', color='Form_Inviati',template = 'ggplot2',width=800, height=400)
fig.add_trace(go.Scatter(x=df42['Timestamp'], y=df42["target"],mode='lines', line=dict(color="blue"), name='Safety Target'))
fig.add_trace(go.Scatter(x=df42['Timestamp'], y=df42["MA_REPORT"],mode='lines', line=dict(color="orange"), name='Media mobile'))
fig.update_layout(legend=dict(
    yanchor="top",
    y=0.99,
    xanchor="left",
    x=0.01))
st.plotly_chart(fig, use_container_width=True)



Add additional components, such as sliders or dropdowns, for interactive filtering and customization:

# Define Section of KPI using st.metric of Streaml
st.subheader("KPI per la settimana corrente delle principali macro-aree")
col1, col2, col3, col4, col5 = st.columns(5)
with col1:
    len_report_sett_now=df1['Sesso'].iloc[-1]
    len_report_sett_last_week=df1['Sesso'].iloc[-2]
    delta_report=int(len_report_sett_now) - int(len_report_sett_last_week)
    st.metric("Report Inviati In Settimana",  value= str(int(len_report_sett_now))+" rep", delta=str(delta_report),  help="Numero totale di report inviati questa settimana rispetto a settimana scorsa")
with col2:
    #Settimana attuale psi
    df2_att_scorsa_settimana=df.loc[(df['Timestamp'] >= str(date_last_week))]
    df2_medie_valori_week=df2_att_scorsa_settimana.mean().reset_index()
    df2_medie_valori_week.columns = ['variables', 'count']
    psi_this_week=round(df2_medie_valori_week["count"].mean(), 4)
    psi_perc=round((psi_this_week/7)*100,2)
    #Settimana precedente alla sett scorsa psi
    df2_prima_scorsa_settimana=df.loc[(df['Timestamp'] < str(date_last_week))]
    df2_medie_valori_prec_week=df2_prima_scorsa_settimana.mean().reset_index()
    df2_medie_valori_prec_week.columns = ['variables', 'count']
    psi_prima_last_week=round(df2_medie_valori_prec_week['count'].mean(), 4)
    #differenza tra i PSI
    delta_psi=round(((float(psi_this_week)-float(psi_prima_last_week))/7)*100, 2)
    if psi_perc>100:
        psi_perc==100
    st.metric("PSI Index",  value=str(psi_perc)+" %", delta=str(delta_psi)+" %", help="Patient Satisfaction Index (misura complessiva di grado di soddisfazione dei pazienti)")


3. How to connect Google Sheets to Streamlit

Streamlit seamlessly integrates with Google Sheets through the pandas library, treating the source as a simple CSV file.

Just follow these steps.

Install the necessary libraries:

pip install gspread
pip install oauth2client


Create a service account key in the Google Cloud Console:

Go to the Google Cloud Console (https://console.cloud.google.com/)
Create a new project or select an existing one
Enable the Google Sheets API for your project
Go to "Credentials" and create a new service account key
Download the JSON key file for the service account

Paste the private key into the secrets.toml file, making any necessary modifications to fit the required standard.

Set up your script using the provided script as a guide.

Store your data in a script storage, as shown in the example below, where we saved data from a long form.

Here is how to do this:

import google_auth_httplib2
import httplib2
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import HttpRequest

@st.cache_resource()
def connect_to_gsheet():
    # Create a connection object.
    credentials = service_account.Credentials.from_service_account_info(
        st.secrets["gcp_service_account"],
        scopes=[SCOPE],
    )

    # Create a new Http() object for every request
    def build_request(http, *args, **kwargs):
        new_http = google_auth_httplib2.AuthorizedHttp(
            credentials, http=httplib2.Http()
        )
        return HttpRequest(new_http, *args, **kwargs)

    authorized_http = google_auth_httplib2.AuthorizedHttp(
        credentials, http=httplib2.Http()
    )
    service = build(
        "sheets",
        "v4",
        requestBuilder=build_request,
        http=authorized_http,
    )
    gsheet_connector = service.spreadsheets()
    return gsheet_connector

@st.cache_data()
def get_data(gsheet_connector) -> pd.DataFrame:
    values = (
        gsheet_connector.values()
        .get(
            spreadsheetId=SPREADSHEET_ID,
            range=f"{SHEET_NAME}!A:E",
        )
        .execute()
    )

    df = pd.DataFrame(values["values"])
    df.columns = df.iloc[0]
    df = df[1:]
    return df

def add_row_to_gsheet(gsheet_connector, row) -> None:
    gsheet_connector.values().append(
        spreadsheetId=SPREADSHEET_ID,
        range=f"{SHEET_NAME}!A:E",
        body=dict(values=row),
        valueInputOption="USER_ENTERED",
    ).execute()

# submit form and store data
submitted = st.button(label="Submit")              
if submitted==True:
   st.success("Successfully")
   st.ballons()
   add_row_to_gsheet(df, 
                  [[var_a1, var_a2, var_a3,
                    var_b1, var_b2, var_b3,
                    var_c1, var_c2, var_c3,
                    var_d1, var_d2, var_d3, 
                    var_d4, var_d5, var_d6, var_d7,                            
                    var_e1,  var_e2,                                
                    var_f1, var_f2, var_f3,                              
                    var_g1, var_g2, var_g3, var_g4,var_g5,                              
                    var_h1, var_h2, var_h3,var_h4, var_h5, var_h6, var_h7,var_h8, var_h9,                                
                    var_i1, var_i2,                               
                    feedback_gen, str(datetime_object),  "Form_lungo",  
                    add_comm,  emozione,  sentiment]])

4. How to create a multi-page app

To implement multi-page functionality, incorporate the Patient_Form.py and other relevant scripts from our repo.

The code shows how different functions can simulate separate pages within the app:

The functions form_pazienti(), dashboard_patient_satisf(), and landing_page() represent distinct pages, each displaying relevant content.
The page_names_to_funcs dictionary maps page names to their respective functions.
The selected_page variable, determined by the user's selection from the sidebar, executes the corresponding function to display the chosen page's content.

Here is an example:

#Example: Through functions we can simulate different pages of the web-app 

def form_pazienti():
	st.title("Page1 application")

def dashboard_patient_satisf():
	st.title("Page2 application")

def landing_page():
	st.title("Page3 application")

page_names_to_funcs = {
            "Form Patient Satisfaction": form_pazienti,
            "Dashboard Patient Satisfaction": dashboard_patient_satisf, 
            "Info Framework":landing_page}
selected_page = st.sidebar.selectbox("Select a page", page_names_to_funcs.keys(), key ="value")
page_names_to_funcs[selected_page](

5. How to create data manipulation techniques

To make simple and beautiful dashboards, use the patient satisfaction results to develop key performance indicators (KPIs) and extract actionable insights.

The surveys capture two aspects of the interview:

The numerical portion involves quantitative data analysis. Users adjust the bar based on a Likert scale of 0 to 7.
The free-text portion involves qualitative data analysis. Users input comments, and the application provides elaboration on the numerical data.

Patient forms contain qualitative data that is analyzed manually. We aimed to translate the comments into quantitative information not typically captured in tabular form (view the dataset here).

Just import the necessary libraries:

#basic libraries
import streamlit as st  
import pandas as pd 
import numpy as np
import time
from datetime import date
from datetime import timedelta
from htbuilder import div, big, h2, styles
from htbuilder.units import rem

#cv libraries
from PIL import Image

#visualization libraries
import plotly.express as px  
import plotly.graph_objects as go
from wordcloud import WordCloud,  STOPWORDS
import matplotlib.pyplot as plt

#machine learning libraries
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn import datasets, ensemble
from sklearn.inspection import permutation_importance
import statsmodels.api as sm
from streamlit_elements import elements, mui
from streamlit_elements import nivo
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

6. How to use ML algorithms to improve the prediction of goal metrics

You can also implement a predictive KPI using a machine learning (ML) algorithm called Gradient-Boosting. It considers all relevant variables to predict the Patient Satisfaction Index for upcoming weeks.

The index represents the overall satisfaction of patients:

#algorithms Gradient Boosting to predict PSI
def training_ml(x,  y):
    X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=13)

    params = {
    "n_estimators": 500,
    "max_depth": 4,
    "min_samples_split": 5,
    "learning_rate": 0.01,
    "loss": "squared_error"}
    
    reg = ensemble.GradientBoostingRegressor(**params)
    reg.fit(X_train, y_train)

    pred=reg.predict(X_test)
    
    mse = mean_squared_error(y_test, pred)
    print("The mean squared error (MSE) on test set: {:.4f}".format(mse))
    #take the first prediction
    return pred[-1]

#check preds (1 e 7)
pred = training_ml(X,  y)


Use polynomial regression algorithms to calculate the relationship between patients' satisfaction and wait time. Each line in the algorithm represents a different-order polynomial, with parameters identified using form data.

You can analyze patient comments using sentiment analysis, an approach to natural language processing (NLP) that identifies the emotional tone behind the written text.

% Form with Comment (Value (%) represents the number of people who completed the form and left a comment)
% Negative Results (Value (%) represents the number of negative keyword results out of the total results of keywords added by users, excluding articles, conjunctions, and non-significant words)
% Sentiment Analysis Score (Value as Positive-Negative Ratio. Calculate the ratio of positive to negative keywords. If the result is greater than 0, there are more positive keywords.)

This is a method that organizations can use to evaluate and classify opinions regarding a product, service, or idea.

Here is how to build a similar word cloud:

# How to build a WordCloud on Streamlit using Matplotlib
st.header("Word Cloud Patient Form")
commenttext_merged= df['Comment_Text'].str.cat(sep=' , ')

# all words in title case
text_propercase=commenttext_merged.title()

# remove all non necessary words as articles and con
stop_words =STOPWORDS.update(["La ", "Non ", "Mi ", "E ", "Il ", "Dei ", "Di ", "Degli ", "Lo ",  "Della ", "C'Era ", ", ",  "Del ",  "Per ", "Sotto ", "Alcuni", "Alcune ", "Ok "
                        , "Rispetto!","Degli ",  "Ho ", "E' ",  "Da ",  "Un ",  "In ",  "Una ", "Dalla ", "Stata ", "Mia ", "Che ",  "Ma ",  "Tutto ",  "Sono "])

#Create and generate a word cloud image:
wordcloud = WordCloud(stopwords = stop_words,background_color="#E4E3E3", width=800, height=500, colormap="Blues").generate(text_propercase)
fig, ax = plt.subplots(facecolor="#E4E3E3")
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.subplots_adjust(left=-5, right=-2, top=-2, bottom=-5)
plt.show()
st.pyplot(fig)


Here is how it looks:

Finally, integrate an AI tool into the Patient Satisfaction Form to classify comments based on Italian BERT's sentiments and emotions classification. This language model can detect and classify comments into two different labels: positive and negative (binary classification). And it can extract emotions from a phrase and classify them into four labels: joy, anger, fear, and sadness (see the repo here).

Use the results of this classification to calculate two metrics—to see how many comments are considered positive and negative by BERT:

Wrapping up

Mehedi is a result of long talks, design studies, and research. Using our IT, medical, and management knowledge, we made a customizable platform for healthcare facilities to gather useful information directly from patients. Streamlit helped us create our app super fast!

We welcome your feedback and opportunities for collaboration. Please leave us a message in the comments below, on GitHub, or on LinkedIn: Matteo, Luca, and Federico.

Thank you, and happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to build an interconnected multi-page Streamlit app
https://blog.streamlit.io/how-to-build-an-interconnected-multi-page-streamlit-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build an interconnected multi-page Streamlit app

From planning to execution—how I built GPT lab

By Dave Lin
Posted in LLMs, July 19 2023
Planning a large Streamlit app
Upfront feature and UX: Creating initial spec and low-fi UX mocks
Data model: Determining the schema
Users and user_hash
Bots
Sessions
Code structure: Structuring for scalability and modularity
Session states: Managing UI and user flow
Wrapping up
Contents
Share this post
← All posts

Wow! What an incredible three months since I first published my blog post on the 12 lessons learned from building GPT Lab! 🚀

Thanks to your tremendous support, GPT Lab has received over 9K app views, 1200+ unique signed-in users, 1000+ sessions with assistants, 700+ prompts tested, and 190+ assistants created. The app has also been featured in the Streamlit App Gallery alongside other great apps.

Many of you have asked me, "How did you plan and build such a large application with Streamlit?" Eager to answer, I've decided to open-source GPT Lab.

By reading this post, you’ll gain insights into the planning it took to build an interconnected multi-page Streamlit app like GPT lab:

Upfront feature and UX
Data model
Code structure
Session states

I hope it'll inspire you to push Streamlit to its limits and bring your ambitious apps to life!

🤖
Want to skip ahead? Check out the app and the code.
Planning a large Streamlit app

Building large Streamlit apps like GPT Lab requires careful planning rather than just throwing code together. For GPT Lab, I focused on planning these four key aspects:

Upfront feature and UX. What will the app do? What kind of user experience do we aim to provide?
Data model. How will data be persisted? What should be stored in the database versus session state variables?
Code structure. How should the app be architected to ensure modularity, maintainability, and scalability?
Session states. Which session state variables are needed to link the user interface?

Understanding these aspects offered a clearer view of what I was trying to build and provided a framework to approach the complex task systematically.

Let's dive into each aspect in more detail.

Upfront feature and UX: Creating initial spec and low-fi UX mocks

To start, I created a simple specification document (or "spec") outlining the overall scope and approach. I also included a sitemap detailing the use cases I wanted to support. The spec gave me a clear roadmap and a means to measure my progress.

Here's an excerpt from the original spec:

Scope. Build a platform that allows generative AI (GA) bot enthusiasts to build their own GPT-3 prompt-based chatbot for their friends and families. The goal is to test the hypothesis that enough GA bot enthusiasts would want to build their niche-domain bots.

Approach. A public Streamlit site that allows users to interact with one of the four pre-trained coach bots or create and interact with their bots.

As with most development projects, I made some changes. But the original sitemap remained intact for the most part, as I could implement most of the planned features.

Here is the final version of the sitemap:

GPT Lab
│
├── Home
│
├── Lounge
│
├── Assistant
│   ├── Search for assistant
│   ├── Assistant details
│   ├── Active chat
│   └── Chat recap
│
├── Lab
│   ├── Step 1: initial prompt + model config
│   ├── Step 2: test chat
│   ├── Step 3: other configs
│   └── Step 4: confirmation
│
├── FAQ
│
└── Legal
    ├── Terms
    └── Privacy policy


I can't overstate the importance of feature planning. It provides a roadmap, a way to measure progress, and a starting point for thinking about the data model.

Data model: Determining the schema

From the start, I recognized that a backend data store was crucial for persisting user, assistant, and session records. After considering my options, I decided on Google Firestore due to its scalability, real-time capabilities, and generous free tier. I strategically designed the data model with future use cases in mind. For example, it's possible to add prompt version controls to GPT Lab, allowing users to edit or revert their assistants.

🤖
NOTE: In the app backend and data model, assistants are referred to as bots, despite my previous insistence on not calling them bots in the user interface. 😅

Now, let's explore the four main Firestore collections in GPT Lab: users, user_hash, bots, and sessions.

Users and user_hash

The users collection is where the app stores information about its users. To protect user privacy, the app doesn't store any personally identifiable information (PII) about users. Instead, each user is associated only with the one-way hash value of their OpenAI API key. The metric fields are incremented whenever a user creates an assistant or starts/ends a session with an assistant. This allows for basic analytics gathering within the app.

Users Collection
   |
   | - id: (Firestore auto-ID)
   | - user_hash: string (one-way hash value of OpenAI API key)
   | - created_date: datetime
   | - last_modified_date: datetime
   | - sessions_started: number
   | - sessions_ended: number
   | - bots_created: number


Google Firestore doesn't provide a way to ensure the uniqueness of a document field value within a collection, so I created a separate collection called user_hash. This ensures that each unique API key has only one associated user record. Each user document is uniquely associated with a user_hash document, and each user_hash document may be associated with a user document. The data model is flexible enough to accommodate users who change their API keys in the future (users can log in with their old API key and then swap it out for a new one).

User_hash Collection
   |
   | - id = one-way hash value of OpenAI API key
   | - user_hash_type: string (open_ai_key)
   | - created_date: datetime

Bots

The bots collection stores configurations for AI assistants. The crux of each AI assistant is its large language model (LLM), model configurations, and prompts. To enable proper version control of prompts and model configurations in the future, model_configs and prompts are modeled as subcollections (part of GPT Lab's vision is to be the repository of your prompts).

To minimize subcollection reads (so you don't need to constantly query the subcollections for the active record), the document IDs of the active subcollection are also stored at the document level. The session_type field indicates whether the assistant is in a brainstorming or coaching session, which affects the session message truncation technique.

Finally, the metric fields are incremented when a user starts or ends a session with an assistant.

Bots Collection
   |
   | - id: (Firestore auto-ID)
   | - name: string
   | - tag_line: string
   | - description: string
   | - session_type: number
   | - creator_user_id: string
   | - created_date: datetime
   | - last_modified_date: datetime
   | - active_initial_prompt_id: string
   | - active_model_config_id: string
   | - active_summary_prompt_id: string
   | - showcased: boolean
   | - is_active: boolean
   |
   v
   |--> Model_configs subcollection
   |     |
   |     | - config: map
   |     |     | - model: string 
   |     |     | - max_tokens: number 
   |     |     | - temperature: number 
   |     |     | - top_p: number 
   |     |     | - frequency_penalty: number 
   |     |     | - presence_penalty: number 
   |     | - created_date: datetime
   |     | - is_active: boolean
   |
   v
   |--> Prompts subcollection
         |
         | - message_type: string
         | - message: string
         | - created_date: datetime
         | - is_active: boolean
         | - sessions_started: number
         | - sessions_ended: number

Sessions

The sessions collection stores session data. It contains two types of sessions: lab sessions (used for testing prompts) and assistant sessions (used for chatting with created assistants). To reduce the need for frequent retrieval of the bot document, its information is cached within the session document. This makes conceptual sense, as the bot document could drift if an editing assistant use case were ever implemented.

The messages_str field stores the most recent payload sent to OpenAI's LLM. This feature allows users to resume their previous assistant sessions. The messages subcollection stores the actual chat messages. Note that lab session chat messages aren't stored.

To ensure user confidentiality and privacy, OpenAI request payloads and session messages are encrypted before being saved in the database. This data model allows users to restart a previous session and continue chatting with the assistant.

Sessions Collection
   |
   | - id: (Firestore auto-ID)
   | - user_id: string
   | - bot_id: string
   | - bot_initial_prompt_msg: string
   |
   | - bot_model_config: map
   |     | - model: string 
   |     | - max_tokens: number 
   |     | - temperature: number 
   |     | - top_p: number 
   |     | - frequency_penalty: number 
   |     | - presence_penalty: number 
   |
   | - bot_session_type: number
   | - bot_summary_prompt_msg: string
   | - created_date: datetime
   | - session_schema_version: number
   | - status: number
   | - message_count: number
   | - messages_str: string (encrypted)
   |
   v
   |--> Messages subcollection
         |
         | - created_date: datetime
         | - message: string (encrypted)
         | - role: string


By carefully considering all potential use cases from the beginning, I created a data model that is future-proof and able to accommodate the evolving needs and features of the app. In the following section, we'll examine the structure of the backend application code to see how it supports and implements this robust data model.

Code structure: Structuring for scalability and modularity

I created GPT Lab to empower users with low or no technical skills to build their own prompt-based LLM-based AI applications without worrying about the underlying infrastructure. My goal is to eventually offer backend APIs that connect users' custom front-end apps (whether using Streamlit or not) with their AI assistants. This motivated me to design a decoupled architecture that separates the front-end Streamlit application from the backend logic.

The backend code was structured as follows:

+----------------+     +-------------------+     +-------------------+     +------------+
|                |     |                   |     |                   |     |            |
|  Streamlit App |<--->| util_collections  |<--->| api_util_firebase |<--->|  Firestore |
|                |     | (users, sessions, |     |                   |     |            |
|                |     |  bots)            |     |                   |     |            |
+----------------+     +-------------------+     +-------------------+     +------------+
                             |
                             |
                             v
                     +-----------------+     +------------+
                     |                 |     |            |
                     | api_util_openai |<--->|   OpenAI   |
                     |                 |     |            |
                     +-----------------+     +------------+


The modules are as follows:

api_util_firebase handles CRUD operations with the Firestore database.
api_util_openai interacts with OpenAI's models, provides a unified chat model to upstream models, prunes chat messages, and tries to detect and prevent prompt injection attacks.
api_util_users, api_util_sessions, and api_util_bots are interfaces to their corresponding Firestore collections. They interact with api_util_firebase and api_util_openai and implement GPT Lab-specific business logic.

This design enables separate development, testing, and scaling of different parts of the code. It also establishes an easier migration path to convert the backend util_collections modules into Google Cloud Functions, which can be exposed via API Gateways.

Session states: Managing UI and user flow

As explained in the first blog article, I used session state variables to control and manage functionalities on Streamlit pages.

This table illustrates how these variables are utilized throughout the app:

Streamlit Page	Session state and UI controls
home.py	user controls whether to render the OpenAI API key module.
pages/1_lounge.py	user controls whether to render the OpenAI API key module, enable assistant selections, and show the My Assistants tab.  

After users choose to interact with an assistant, the assistant details are stored in bot_info.
pages/2_assistant.py	user controls whether to render the OpenAI API key module.  

bot_info, session_id, and session_ended determine which screen variation to display: 

• bot_info does not exist: check to see if assistant_id is in the URL parameter. Else, prompt users to search for an assistant  
• bot_info and session_id exist, and session_ended is false: display the chat session screen  
• bot_info and session_id exist, and session_ended is true: display the chat session recap screen  

In the chat session, session_msg_list stores the conversation.
pages/3_lab.py	user gates whether to render the OpenAI API key module and whether to allow users to start creating assistants in the lab.  

lab_active_step controls which lab session state to render:
• If 1: render step 1 UI to set assistant initial prompt and model
• If 2: render step 2 UI to test chat with assistant
• If 3: render step 3 UI to finalize assistant details. On create, the bot record is created in Firestore DB, and the document ID is saved to lab_bot_id.
• If 4 and lab_bot_id is set: render step 4 UI to show assistant creation confirmation. 

During the test chat session, lab_msg_list stores the test messages. By using separate lab_bot_id and bot_info, I can allow users to jump back and forth between lounge/assistant and lab without losing progress in each.

With the upfront planning done, the rest of the execution was a lot more manageable.

Wrapping up

This post covered the upfront planning required for creating GPT Lab, including the features, data model, code, and session state. I hope this inspires you to build your own ambitious Streamlit apps.

Connect with me on Twitter or Linkedin. I'd love to hear from you.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Decoding Warren Buffett with LLMs and Snowflake SQL
https://blog.streamlit.io/decoding-warren-buffett-with-llms-and-snowflake-sql/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

By Randy Pettus
Posted in LLMs, August 1 2023
What is Ask the Oracle of Omaha?
App overview
How to use the app
1. How to create a Snowflake database, schema, and virtual warehouse
1.1. Accounts
1.2. Python packages
1.3. Secrets management
1.4. Data loading
2. How to load data into Snowflake using Snowpark Python and obtain the associated DDL statements
3. How to create embeddings from DDL statements and load them into a vector database (FAISS)
4. How to create embeddings from PDFs and load them into a vector database (Pinecone)
5. How to perform questioning/answering on these docs using OpenAI and LangChain
6. How to put it all together
tab1
tab2
tab3
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Randy Pettus, and I'm a principal data scientist in Denver, Colorado. Before transitioning into the world of data science, I worked in finance and taught college finance courses, including one called "The Investment Strategies of Warren Buffett."

When Streamlit announced the 2023 Snowflake Streamlit Hackathon, I decided to create an app allowing users to explore Warren Buffett's world. I did it in less than a day, and it won second place! 🥈

In this post, I'll show you:

How to create a Snowflake database, schema, and virtual warehouse
How to load data into Snowflake using Snowpark Python and obtain the associated DDL statements
How to create embeddings from DDL statements and load them into a vector database (FAISS)
How to create embeddings from PDFs and load them into a vector database (Pinecone)
How to perform questioning/answering on these docs using OpenAI and LangChain
How to put it all together


💵
TLDR: Here's the app and the repo code.
What is Ask the Oracle of Omaha?

So, what is this app exactly, and what does it do?

The app offers various LLM functionalities, such as LLM-augmented retrieval from data in a Snowflake database and PDF documents. You can access financial statement information for multiple companies Buffet has invested in through a KPI view or by asking natural language questions. And you can query Buffet's shareholder letters going back to 1977.

Go ahead and try it:

As you can see, there are three main tabs:

💵 Financial statement natural language querying: Converts user questions into Snowflake SQL and returns DataFrame outputs. The approach uses semantic search on DDL statements stored in a vector database and uses LLM-generated SQL to query Snowflake directly.
📈 Financial data exploration: Users can examine financial statement information through KPI cards, charts, and so on, pulled from Snowflake.
📝 Shareholder letter natural language querying: This performs question and answering using retrieval augmented generation from Buffet's shareholder letters stored as PDFs dating back to 1977.

Before we dive into coding, let's take a quick high-level view of the app.

App overview

Here is how all the app components fit together:

How to use the app

Here are a few examples that show how the app functions.

Say, you want to inquire about the financial performance of certain companies. In the first tab, type "Rank the companies in descending order based on their net income in 2022. Include the ticker and net income value."

Great! You just got a DataFrame back from Snowflake with the correct results.

Now, say you want to explore the shareholder letters. Let's use this random 1984 letter:

Type in, "Where did Mrs. B receive her honorary doctorate, and what was her job role?"

Awesome! The app gave you the correct answer.

Let's move on to building it.

1. How to create a Snowflake database, schema, and virtual warehouse
1.1. Accounts

To start, you'll need a Snowflake account, an OpenAI account, and a Pinecone account. Go ahead and create them if you don't have them.

💵
NOTE: Snowflake offers free trials, and Pinecone offers one index for free. You can make some modifications to use any other database, such as SQLite, or substitute Pinecone for FAISS or another alternative.
1.2. Python packages

Next, install the following Python packages:

# requirements.txt
altair
snowflake-connector-python
snowflake-sqlalchemy
snowflake-snowpark-python[pandas]==1.5.1
numpy
pandas
matplotlib
seaborn
openai
streamlit_chat
langchain==0.0.124
pinecone-client
sqlalchemy
faiss-cpu

1.3. Secrets management

To use your Snowflake credentials, create a .streamlit/secrets.toml file in the following format (read more here). The [connections.snowpark] section should be filled out with your Snowflake credentials. The setting client_session_keep_alive = true keeps the session active, which helps avoid connection timeout issues.

#.streamlit/secrets.toml

openai_key = "########"
pinecone_key = "########"
pinecone_env = "########"

sf_database = "FINANCIALS"
sf_schema = "PROD"

[connections.snowpark]
account = "########"
user = "########"
password = "########"
warehouse = "########"
database = "########"
schema = "########"
client_session_keep_alive = true

1.4. Data loading

Finally, enable the loading of financial statement data in Snowflake. After creating your Snowflake account, you must create a database, schema, and virtual warehouse. Snowflake uses the virtual warehouse to execute virtual warehousee on your data. You can run the following SQL commands in a UI worksheet (remember to include them in your secrets file).

create database FINANCIALS;

create schema PROD;

-- create an extra small warehouse
CREATE WAREHOUSE if not exists WH_XS_APP
with
  WAREHOUSE_SIZE = XSMALL
  AUTO_SUSPEND = 60
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'APPLICATION WAREHOUSE'
;

2. How to load data into Snowflake using Snowpark Python and obtain the associated DDL statements

Publicly traded companies file their financial statements—income, balance sheet, cash flow—with the Securities Exchange Commission (SEC). I sourced my data from SEC Edgar (see the CSV files in my repo). Consider it test data, as it might not be completely accurate. If you want a more reliable source, there are various services and APIs available for obtaining financial statement data, including Cybersyn on the Snowflake Marketplace.

In my repo, you'll see three folders under the .load/financials directory for each financial statement type and a .csv file for each company (ticker) within each folder.

With that in mind, create a stock_load.py file that does the following:

Establishes a Snowflake Snowpark connection and session.
Loops through each financial statement folder to load the CSV as a Pandas DataFrame.
Uses the session.create_dataframe() and save_as_table() Snowpark functionality to create a Snowflake table for each financial statement while loading the data from the dataframes.
Creates a DDL file to make embeddings for LLM interaction. This step loops through each created table and gets the DDL information from Snowflake. All DDL statements are consolidated into a single ddls.sql file.


# stock_load.py
import os
import glob
import numpy as np
import pandas as pd
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
from snowflake.snowpark.session import Session
import streamlit as st

# snowpark connection
CONNECTION_PARAMETERS = {
   "account": st.secrets['account'], 
   "user": st.secrets['user'],
   "password": st.secrets['password'],
    "database": st.secrets['database'],
   "schema": st.secrets['schema'],
   "warehouse": st.secrets['warehouse'], 
}

# create session
session = Session.builder.configs(CONNECTION_PARAMETERS).create()

# create a list of the statements which should match the folder name
statements = ['INCOME_STATEMENT_ANNUAL','BALANCE_SHEET_ANNUAL',
'CASH_FLOW_STATEMENT_ANNUAL']

# Load data into snowflake by looping through the csv files
for statement in statements:
    path = f'./load/financials/{statement.lower()}/' 
    files = glob.glob(os.path.join(path, "*.csv"))
    df = pd.concat((pd.read_csv(f) for f in files))
    print(statement)
    # note that overwrite is used to start. If adding future data, move to append with upsert process
    session.create_dataframe(df).write.mode('overwrite').save_as_table(statement)

# automatically get the ddl from the created tables
# create empty string that will be populated
ddl_string = ''

# run through the statements and get ddl
for statement in statements:
    ddl_string += session.sql(f"select get_ddl('table', '{statement}')").collect()[0][0] + '\
\
'
    
ddl_file = open("ddls.sql", "w")
n = ddl_file.write(ddl_string)
ddl_file.close()


After running this, your FINANCIALS.PROD schema should contain populated tables:

You must also create a local ddls.sql file with "create table" commands for each table. This file is crucial in providing context for the LLM about the database structure, including the various tables, columns, and data types.

Here is a snippet of this output:

create or replace TABLE FINANCIALS.PROD.INCOME_STATEMENT_ANNUAL (
	TICKER VARCHAR(16777216),
...

3. How to create embeddings from DDL statements and load them into a vector database (FAISS)

This step aims to provide an efficient means of translating a question into relevant Snowflake SQL code. My original solution, which relied on Langchain's SQLDatabase and SQLDatabaseChain functionality to interact directly with Snowflake, wasn't optimal. It kept pulling information schema from Snowflake, producing too much context for OpenAI inputs to generate SQL code. So it unnecessarily wasted OpenAI tokens and Snowflake credits (find the original solution here).

💵
I tried another method of creating embeddings from the DDL information above. It proved to be much more economical, as I found that my OpenAI and Snowflake credit consumption dropped by as much as 80%!

The create_ddl_embeddings.py script below uses Langchain's TextLoader() functionality to load the ddls.sql file. The text is split into characters and documents and converted into embeddings using OpenAIEmbeddings(). The embeddings are stored in FAISS, an open-source vector database.

Running this script produces two files: index.faiss and index.pkl, both located in the faiss_index folder (used later for the question and retrieval pipeline):

from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
import streamlit as st

# load the ddl file
loader = TextLoader('load/ddls.sql')
data = loader.load()

# split the text
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
texts = text_splitter.split_documents(data)

# created embeddings from the sql document
embeddings = OpenAIEmbeddings(openai_api_key = st.secrets["openai_key"])
docsearch = FAISS.from_documents(texts, embeddings)

# save the faiss index
docsearch.save_local("faiss_index")

4. How to create embeddings from PDFs and load them into a vector database (Pinecone)

Let's move on to the shareholder letters (find them here or in my repo).

Your pinecone.io account offers a free index. To set up the prompts, get the Pinecone API key, the Pinecone environment variable, and the index name (remember to store these keys in your .secrets file).

Create the letter_load.py script, which produces embeddings from the various letters and loads them into Pinecone:

import os
from langchain.document_loaders import PyPDFLoader # for loading the pdf
from langchain.embeddings import OpenAIEmbeddings # for creating embeddings
from langchain.vectorstores import  Pinecone # for the vectorization part
from langchain.text_splitter import TokenTextSplitter
import pinecone
import streamlit as st

# identify the various pdf files
pdfs = [file for file in os.listdir('./letters/') if 'pdf' in file]

# loops through each pdf in the letters directory
# and loops the content using langchains PyPDFLoader
page_list = []
for pdf in pdfs:
    pdf_path = f"./letters/{pdf}"
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    page_list.append(pages)

flat_list = [item for sublist in page_list for item in sublist]

text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(flat_list)

# initialize pinecone
pinecone.init(
    api_key=st.secrets['pinecone_key'], 
    environment=st.secrets['pinecone_env'] 
)
index_name = "buffett"

# note you should create an OPENAI_API_KEY env environment variable or use st.secrets
# create embeddings using OpenAI and load into Pinecone 
embeddings = OpenAIEmbeddings(openai_api_key=st.secrets['openai_key'])
docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)


The script above loops through each letter and extracts text using Langchain's PyPDFLoader. The text is consolidated into a flattened list, and TokenTextSplitter chunks it. OpenAIEmbeddings is used again to create the embeddings, which are then loaded into Pinecone using langchain.vectorstores.Pinecone() and the page_content in each text split. Ideally, you'd use LangChain's DirectoryLoader for this, but package dependency issues cause delays. Running this script may take a few minutes, but once completed, you should see the index populated in Pinecone.

5. How to perform questioning/answering on these docs using OpenAI and LangChain

Create a new file called prompts.py, which uses LangChain and OpenAI to enable question and answer retrieval.

This script covers the following steps:

Creating prompt templates to provide better guidance for the large language model. The FS_TEMPLATE offers specific instructions for the LLM to produce better Snowflake SQL results from the financial statements, including an example of a single-shot prompt.
Defining OpenAI parameters for the LLM.
Identifying the vector databases for retrieval, including the FAISS stored embeddings (get_faiss) and Pinecone vector database (get_pinecone).
Creating a question/answer chain using Langchain's RetrievalQA functionality. The fs_chain() and letter_chain() functions take questions as inputs for the financial statements and shareholder letters, respectively. These functions are designed to retrieve the k most similar embeddings to the question while providing a natural language response. For fs_chain(), the result is a SQL query command, which you'll use for querying financial statement data in Snowflake. For letter_chain(), the output will include a text response for the question.


import streamlit as st
import openai
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS, Pinecone
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
import pinecone

FS_TEMPLATE = """ You are an expert SQL developer querying about financials statements. You have to write sql code in a Snowflake database based on the following question. 
display the sql code in the SQL code format (do not assume anything if the column is not available, do not make up code). 
ALSO if you are asked to FIX the sql code, then look what was the error and try to fix that by searching the schema definition.
If you don't know the answer, provide what you think the sql should be. Only include the SQL command in the result.

The user will request for instance what is the last 5 years of net income for Johnson and Johnson. The SQL to generate this would be:

select year, net_income
from financials.prod.income_statement_annual
where ticker = 'JNJ'
order by year desc
limit 5;

Questions about income statement fields should query financials.prod.income_statement_annual
Questions about balance sheet fields (assets, liabilities, etc.) should query financials.prod.balance_sheet_annual
Questions about cash flow fields (operating cash, investing activities, etc.) should query financials.prod.cash_flow_statement_annual

The financial figure column names include underscores _, so if a user asks for free cash flow, make sure this is converted to FREE_CASH_FLOW. 
Some figures may have slightly different terminology, so find the best match to the question. For instance, if the user asks about Sales and General expenses, look for something like SELLING_AND_GENERAL_AND_ADMINISTRATIVE_EXPENSES

If the user asks about multiple figures from different financial statements, create join logic that uses the ticker and year columns.
The user may use a company name so convert that to a ticker.

Question: {question}
Context: {context}

SQL: ```sql ``` \

 
"""
FS_PROMPT = PromptTemplate(input_variables=["question", "context"], template=FS_TEMPLATE, )

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1,
    max_tokens=1000, 
    openai_api_key=st.secrets["openai_key"]
)

def get_faiss():
    """
    get the loaded FAISS embeddings
    """
    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets["openai_key"])
    return FAISS.load_local("faiss_index", embeddings)

def get_pinecone():
    """ 
    get the pinecone embeddings
    """
    pinecone.init(
        api_key=st.secrets['pinecone_key'], 
        environment=st.secrets['pinecone_env'] 
        )
    
    index_name = "buffett"
    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets["openai_key"])
    return Pinecone.from_existing_index(index_name,embeddings)

def fs_chain(question):
    """
    returns a question answer chain for faiss vectordb
    """

    docsearch = get_faiss()
    qa_chain = RetrievalQA.from_chain_type(llm, 
                                           retriever=docsearch.as_retriever(),
                                           chain_type_kwargs={"prompt": FS_PROMPT})
    return qa_chain({"query": question})

def letter_chain(question):
    """returns a question answer chain for pinecone vectordb"""
    
    docsearch = get_pinecone()
    retreiver = docsearch.as_retriever(#
        #search_type="similarity", #"similarity", "mmr"
        search_kwargs={"k":3}
    )
    qa_chain = RetrievalQA.from_chain_type(llm, 
                                            retriever=retreiver,
                                           chain_type="stuff", #"stuff", "map_reduce","refine", "map_rerank"
                                           return_source_documents=True,
                                           #chain_type_kwargs={"prompt": LETTER_PROMPT}
                                          )
    return qa_chain({"query": question})



You'll see in the letter_chain() function that additional retrieval parameters are included. Due to the size of the letters, it is important to ensure a balance of getting enough coverage while not going over any token limitations with OpenAI to retrieve this case's three most similar embeddings and set the 'k' value to 3 in the search_kwargs arguments.

6. How to put it all together

Now that you can perform question and answering using the prompts.py file, it's time to create the main app file: buffett_app.py.

First, import the appropriate packages and set the Streamlit page layout to "wide." Then define some variables, including your Snowflake database, schema, and the various tickers used. Then, establish the Snowflake Snowpark connection using experimental_connection().

import snowflake.connector
import numpy as np
import pandas as pd
import streamlit as st
import altair as alt
import prompts

st.set_page_config(layout="wide")

# Variables
sf_db = st.secrets["database"]
sf_schema = st.secrets["schema"]
tick_list = ['BRK.A','AAPL','PG','JNJ','MA','MCO','VZ','KO','AXP', 'BAC']
fin_statement_list = ['income_statement','balance_sheet','cash_flow_statement']
year_cutoff = 20 # year cutoff for financial statement plotting

# establish snowpark connection
conn = st.experimental_connection("snowpark")


Next, create some helper functions to keep things cleaner:

pull_financials pulls a financial statement from Snowflake for a specified ticker.
kpi_recent populates the KPI cards based on the most recent periods for a selected metric.
plot_financials plots Altair bar charts from a DataFrame.



@st.cache_data()
def pull_financials(database, schema, statement, ticker):
    """
    query to pull financial data from snowflake based on database, schema, statemen and ticker
    """
    df = conn.query(f"select * from {database}.{schema}.{statement} where ticker = '{ticker}' order by year desc")
    df.columns = [col.lower() for col in df.columns]
    return df

# metrics for kpi cards
@st.cache_data()
def kpi_recent(df, metric, periods=2, unit=1000000000):
    """
    filters a financial statement dataframe down to the most recent periods
    df is the financial statement. Metric is the column to be used.
    """
    return df.sort_values('year',ascending=False).head(periods)[metric]/unit

def plot_financials(df, x, y, x_cutoff, title):
    """"
    helper to plot the altair financial charts
    """
    return st.altair_chart(alt.Chart(df.head(x_cutoff)).mark_bar().encode(
        x=x,
        y=y
        ).properties(title=title)
    ) 



Create a Streamlit sidebar to display relevant information about the app for the user.

Here is an overview of the app's functionality:

with st.sidebar:
    st.markdown("""
    # Ask the Oracle of Omaha: Using LLMs... :moneybag:
    This app enables exploration into the World...
    """)


Use Streamlit tabs to break up different sections of the app.

tab1

tab1 provides users with the ability to request financial statement information from Snowflake. The user's question is captured in the str_input variable. Use this input in your prompts.fs_chain() function, which performs similarity matching on the user's question's embedding to find the most relevant SQL DDL embeddings. OpenAI then produces a query text with Snowflake syntax, using this as context. The text is stored in the output['result'], which you pass as an argument to conn.query() to have Snowflake execute the query.

Here is the second attempt (if the first one fails):

# create tabs
tab1, tab2, tab3 = st.tabs([
    "Financial Statement Natural Language Querying :dollar:", 
    "Financial Data Exploration :chart_with_upwards_trend:",
    "Shareholder Letter Natural Language Querying :memo:"]
    )

with tab1:
    st.markdown("""
    # Natural Language Financials Querying :dollar:
    ### Leverage LLMs to translate natural language questions
    ...
		""")
    
    str_input = st.text_input(label='What would you like to answer? (e.g. What was the revenue and net income for Apple for the last 5 years?)')

    if str_input:
        with st.spinner('Looking up your question in Snowflake now...'):
            try:
                output = prompts.fs_chain(str_input)
                try:
                    # if the output doesn't work we will try one additional attempt to fix it
                    query_result = conn.query(output['result'])
                    if len(query_result) > 1:
                        st.write(query_result)
                        st.write(output)
                except:
                    st.write("The first attempt didn't pull what you were needing. Trying again...")
                    output = prompts.fs_chain(f'You need to fix the code. If the question is complex, consider using one or more CTE. Also, examine the DDL statements and try to correct this question/query: {output}')
                    st.write(conn.query(output['result']))
                    st.write(output)
            except:
                st.write("Please try to improve your prompt or provide feedback on the error encountered")


tab2

tab2 displays financial information for a selected ticker (sel_ticker), which users choose via a Streamlit selectbox. The pull_financials() function retrieves the relevant financial statements from Snowflake for the selected ticker.

To improve readability, use two columns to display four financial metrics. I chose to show net income, net income ratio (profit margin), free cash flow, and debt-to-equity ratio, but you can display any metrics you want. The metrics include the most recent year's value and the change from the previous year as the displayed "delta." Under each metric, the plot_financials() function shows the metric value by year.

Finally, the user can select a financial statement to view the complete data:

with tab2: 
    st.markdown("""
    # Financial Data Exploration :chart_with_upwards_trend:

    View financial statement data... 
    """)
    sel_tick = st.selectbox("Select a ticker to view", tick_list)

    # pull the financial statements
    # This whole section could be more efficient...
    inc_st = pull_financials(sf_db, sf_schema, 'income_statement_annual', sel_tick)
    bal_st = pull_financials(sf_db, sf_schema, 'balance_sheet_annual', sel_tick)
    bal_st['debt_to_equity'] = bal_st['total_debt'].div(bal_st['total_equity'])
    cf_st =  pull_financials(sf_db, sf_schema, 'cash_flow_statement_annual', sel_tick) 
  
    col1, col2 = st.columns((1,1))
    with col1:
        # Net Income metric
        net_inc = kpi_recent(inc_st, 'net_income')
        st.metric('Net Income', 
                  f'${net_inc[0]}B', 
                  delta=round(net_inc[0]-net_inc[1],2),
                  delta_color="normal", 
                  help=None, 
                  label_visibility="visible")
        plot_financials(inc_st, 'year', 'net_income', year_cutoff, 'Net Income')
        
        # netincome ratio
        net_inc_ratio = kpi_recent(inc_st, 'net_income_ratio', periods=2, unit=1)
        st.metric('Net Profit Margin', 
                  f'{round(net_inc_ratio[0]*100,2)}%',
                  delta=round(net_inc_ratio[0]-net_inc_ratio[1],2), 
                  delta_color="normal", 
                  help=None, 
                  label_visibility="visible")
        plot_financials(inc_st, 'year', 'net_income_ratio', year_cutoff, 'Net Profit Margin')
    
    with col2:
        # free cashflow
        fcf = kpi_recent(cf_st, 'free_cash_flow' )
        st.metric('Free Cashflow', 
                  f'${fcf[0]}B', 
                  delta=round(fcf[0]-fcf[1],2), 
                  delta_color="normal", 
                  help=None, 
                  label_visibility="visible")
        plot_financials(cf_st, 'year', 'free_cash_flow', year_cutoff, 'Free Cash Flow')

        # debt to equity
        debt_ratio = kpi_recent(bal_st, 'debt_to_equity', periods=2, unit=1)
        st.metric('Debt to Equity', 
                  f'{round(debt_ratio[0],2)}', 
                  delta=round(debt_ratio[0]-debt_ratio[1],2), 
                  delta_color="normal", 
                  help=None, 
                  label_visibility="visible")
        plot_financials(bal_st, 'year', 'debt_to_equity', year_cutoff, 'Debt to Equity')

    # enable a financial statment to be selected and viewed
    sel_statement = st.selectbox("Select a statement to view", fin_statement_list)
    fin_statement_dict = {'income_statement': inc_st,
                          'balance_sheet': bal_st, 
                          'cash_flow_statement':cf_st}
    st.dataframe(fin_statement_dict[sel_statement])

tab3

tab3 lets users ask questions about Buffet's shareholder letters. To do this, collect the user's question in the query variable and use the prompts.letter_chain() function. It performs a similarity search with the vectors stored in Pinecone. Using the three most similar embeddings, the OpenAI LLM call produces an answer, if applicable, in the context. The result of this process is a dictionary that contains both the "result" and "source_documents," which are then displayed for the user:

with tab3:
    st.markdown("""
    # Shareholder Letter Natural Language Querying :memo:
    ### Ask questions from all of...

    These letters are much anticipated...
    """
    )

    query = st.text_input("What would you like to ask Warren Buffett?")
    if len(query)>1:
        with st.spinner('Looking through lots of Shareholder letters now...'):
            
            try:
                st.caption(":blue[Warren's response] :sunglasses:")
                #st.write(prompts.letter_qa(query))
                result = prompts.letter_chain(query)
                st.write(result['result'])
                st.caption(":blue[Source Documents Used] :📄:")
                st.write(result['source_documents'])
            except:
                st.write("Please try to improve your question")

Wrapping up

Thank you for sticking with me until the end! I covered much ground, showing how LLMs can generate meaningful SQL queries and responses from databases like Snowflake. LLMs can also ask questions from various documents and produce meaningful results. The toolsets available for working with LLMs are expanding rapidly, and Streamlit is the perfect tool for demonstrating all of this in one app. Of course, you can improve it by using st.chat and LangChain. Still, it's impressive what you can do in a day!

If you have any questions, please post them in the comments below or contact me on LinkedIn, Twitter, Medium, or GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Untitled.png (2000×918)
https://blog.streamlit.io/content/images/2023/10/Untitled.png


query_profile.jpeg (1106×1622)
https://blog.streamlit.io/content/images/2023/10/query_profile.jpeg#border


Screenshot-2023-10-04-at-11.17.40-AM.png (1950×1670)
https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-04-at-11.17.40-AM.png#browser


Generate interview questions from a candidate’s tweets
https://blog.streamlit.io/generate-interview-questions-from-a-candidates-tweets/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Generate interview questions from a candidate’s tweets

Make an AI assistant to prepare for interviews with LangChain and Streamlit

By Greg Kamradt
Posted in LLMs, June 24 2023
Step 1. Make sure the major pieces of your app work
Twitter
Webpages
YouTube videos
Bring it all together
Step 2. Port your code over to a single script and add Streamlit support
Step 3. Deploy and test
Wrapping up
Contents
Share this post
← All posts

Hey, team! 👋

My name is Greg Kamradt, and I teach people how to analyze data and build AI apps. I’ve spent much of my career doing product analytics in B2B environments at enterprises and startups. I love enabling people to make an impact in their workplace.

Preparing for an interview can be time-consuming, so I built an app that analyzes a candidate's Twitter, YouTube videos, and web pages to generate a list of interview questions. I use LangChain, ChatGPT4, and Streamlit to package it all into a convenient tool.

In this post, I’ll cover:

An introduction to working with GPT4 through LangChain
A UX convention to ensure that users don’t need to supply a prompt
A working understanding of Tweepy (Python library for interacting with Twitter data)
An introduction to web-scraping with LLMs
Bringing all your data together with a prompt to prepare for the meeting


👴
Want to skip reading? Here's a link to the Jupyter Notebook, GitHub repo for the app, and the resultant app.

If you’re a visual learner, here is a video outlining this process in detail:

Step 1. Make sure the major pieces of your app work

The app consists of two main steps:

Gathering data
Processing the data with a language model

The data-gathering process involves three sources:

Twitter: Tweets likely have the most up-to-date and relevant information about what's on the person’s mind
Webpages: To get information about a person, it's best to include a biography or an "About me" page
YouTube videos: This might include an interview with the person or a talk they gave
Twitter

Let’s create a function that utilizes Tweepy to pull the most popular recent tweets from a person.

The goal is to return a string of text so you can pass it to your LLM later:

def get_original_tweets(screen_name, tweets_to_pull=80, tweets_to_return=80):

	# Tweepy set up
	
	auth= tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)
	    auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)
	    api= tweepy.API(auth)
	
	# Holder for the tweets you'll find
	tweets= []
	
	# Go and pull the tweets
	tweepy_results= tweepy.Cursor(api.user_timeline,
	                                   screen_name=screen_name,
	                                   tweet_mode='extended',
	                                   exclude_replies=True).items(tweets_to_pull)
	
	# Run through tweets and remove retweets and quote tweets so we can only look at a user's raw emotions
    for status in tweepy_results:
        if hasattr(status, 'retweeted_status')or hasattr(status, 'quoted_status'):
        # Skip if it's a retweet or quote tweet
            continue
        else:
            tweets.append({'full_text': status.full_text, 'likes': status.favorite_count})


        # Sort the tweets by number of likes. This will help us short_list the top ones later
    sorted_tweets= sorted(tweets, key=lambda x: x['likes'], reverse=True)

        # Get the text and drop the like count from the dictionary
    full_text= [x['full_text']for xin sorted_tweets][:tweets_to_return]

        # Convert the list of tweets into a string of tweets we can use in the prompt later
    users_tweets= "\
\
".join(full_text)

        return users_tweets

Webpages

To pull data from web pages, make a simple request with the requests library and pass that information through Beautiful Soup and markdownify.

Again, you want to return a simple piece of text to insert into your prompt later:

def pull_from_website(url):

	# Doing a try in case it doesn't work
	try:
	        response= requests.get(url)
	except:
	# In case it doesn't work
		print ("Whoops, error")
		return
	
	# Put your response in a beautiful soup
	soup= BeautifulSoup(response.text, 'html.parser')
	
	# Get your text
	text= soup.get_text()
	
	# Convert your html to markdown. This reduces tokens and noise
	text= md(text)
	
	return text

YouTube videos

Lastly, use LangChain’s YouTube video document loader. By default, this loader returns a list of LangChain documents.

You want the plain text to pass through to your prompt later:

def get_video_transcripts(url):
    loader= YoutubeLoader.from_youtube_url(url, add_video_info=True)
    documents= loader.load()
    transcript= ' '.join([doc.page_contentfor docin documents])
    
	return transcript
Bring it all together

Once you have all of this information, combine it into a single string:

user_information = user_tweets + website_data + video_text

Because the string’s length might be too long for your model, split it into chunks and process them individually:

# First we make our text splitter
text_splitter= RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=2000)

# Then we split our user information into different documents
docs = text_splitter.create_documents([user_information])


Next, pass the docs through a custom map and reduce prompt (learn more about chain types in my video). To help with your custom prompts, use LangChain’s prompt templates.

First, your map step:

map_prompt= """You are a helpful AI bot that aids a user in research.
Below is information about a person named {persons_name}.
Information will include tweets, interview transcripts, and blog posts about {persons_name}
Your goal is to generate interview questions that we can ask {persons_name}
Use specifics from the research when possible

% START OF INFORMATION ABOUT {persons_name}:
{text}
% END OF INFORMATION ABOUT {persons_name}:

Please respond with list of a few interview questions based on the topics above

YOUR RESPONSE:"""
map_prompt_template= PromptTemplate(template=map_prompt, input_variables=["text", "persons_name"])


Then your combined step:

combine_prompt= """
You are a helpful AI bot that aids a user in research.
You will be given a list of potential interview questions that we can ask {persons_name}.

Please consolidate the questions and return a list

% INTERVIEW QUESTIONS
{text}
"""
combine_prompt_template= PromptTemplate(template=combine_prompt, input_variables=["text", "persons_name"])


Now that you have your data and prompts set up pass this information through your LLM:

llm= ChatOpenAI(temperature=.25, model_name='gpt-4')

chain= load_summarize_chain(llm,
                             chain_type="map_reduce",
                             map_prompt=map_prompt_template,
                             combine_prompt=combine_prompt_template,
#                              verbose=True
)


This command will run the API call to OpenAI:

output= chain({"input_documents": docs, "persons_name": "Elad Gil"})


I tested this out, pretending I was going to interview Elad Gil. The results had awesome questions!

1. As an investor and advisor to various AI companies, what are some common challenges you've observed in the industry, and how do you recommend overcoming them?

2. Can you elaborate on the advantages of bootstrapping for AI startups and share any success stories you've come across?

3. What are some key lessons you've learned from your experiences in high-profile companies like Twitter, Google, and Color Health that have shaped your approach to investing and advising startups?

4. How do you think AI will continue to shape the job market in the coming years?

5. What motivated you to enter the healthcare space as a co-founder of Color Health, and how do you envision the role of AI in improving healthcare outcomes?


👴
NOTE: The results likely won’t be copy/paste ready. You’ll need to edit the text to match your voice and style.
Step 2. Port your code over to a single script and add Streamlit support

Finally, combine it into a single script and add Streamlit support!

See the complete code in this app's main.py. I like to add some styling and information at the top of my apps. It provides more context and eases the user into the app.

One of my favorite Streamlit containers is st.columns. Let's use that to add some text and a picture:

	# Start Of Streamlit page
st.set_page_config(page_title="LLM Assisted Interview Prep", page_icon=":robot:")

# Start Top Information
st.header("LLM Assisted Interview Prep")

col1, col2 = st.columns(2)

with col1:
    st.markdown(("Have an interview coming up? I bet they are on Twitter or YouTube or the web. "
		             "This tool is meant to help you generate interview questions based off of "
		             "topics they've recently tweeted or talked about."
		             "\
\
"
		             "This tool is powered by [BeautifulSoup](<https://beautiful-soup-4.readthedocs.io/en/latest/#>), "
		             "[markdownify](<https://pypi.org/project/markdownify/>), [Tweepy](<https://docs.tweepy.org/en/stable/api.html>), "
		             "[LangChain](<https://langchain.com/>), and [OpenAI](<https://openai.com>) and made by "
		             "[@GregKamradt](<https://twitter.com/GregKamradt>)."
		             "\
\
"
		             "View Source Code on [Github](<https://github.com/gkamradt/globalize-text-streamlit/blob/main/main.py>)"))
with col2:
    st.image(image='Researcher.png', width=300, caption='Mid Journey: A researcher who is really good at their job and utilizes twitter to do research about the person they are interviewing. playful, pastels. --ar 4:7')
# End Top Information


Now let's add a few input forms for the user to provide the candidate's information:

The st.text_input widgets let you accept information from the user and pass it to the application later:

person_name = st.text_input(label="Person's Name",  placeholder="Ex: Elad Gil", key="persons_name")
twitter_handle = st.text_input(label="Twitter Username",  placeholder="@eladgil", key="twitter_user_input")
youtube_videos = st.text_input(label="YouTube URLs (Use , to separate videos)",  placeholder="Ex: <https://www.youtube.com/watch?v=c_hO_fjmMnk>, <https://www.youtube.com/watch?v=c_hO_fjmMnk>", key="youtube_user_input")
webpages = st.text_input(label="Web Page URLs (Use , to separate urls. Must include https://)",  placeholder="<https://eladgil.com/>", key="webpage_user_input")


I wanted the user to be able to select the type of output they prefer. They may not want interview questions but a one-page summary about a person.

Using a Streamlit radio button, they can select their preferred option:

output_type = st.radio(
"Output Type:",
('Interview Questions', '1-Page Summary'))


Based on their selection, you'll pass different instructions to your prompt. You could build out many more options if you'd like!

response_types = {
	'Interview Questions' : """
	Your goal is to generate interview questions that we can ask them
	Please respond with list of a few interview questions based on the topics above
""",
'1-Page Summary' : """
	Your goal is to generate a 1 page summary about them
	Please respond with a few short paragraphs that would prepare someone to talk to this person
"""
}


Next, let's add a button to control the flow of the application. By default, Streamlit will update the app after any field has changed. While this is great for some use cases, I don't want my LLM to start running until the user is finished.

To control this, I'll add a button that will only run after clicking the "Generate Summary" button. button_ind will only be set to true if the button was clicked during the last run:

button_ind = st.button("*Generate Output*", type='secondary', help="Click to generate output based on information")

# Checking to see if the button_ind is true. If so, this means the button was clicked and we should process the links
if button_ind:
	# Make the call to your LLM


Next, take the code you'd previously written in your Jupyter Notebook and output it to your Streamlit page using st.write:

output = chain({"input_documents": user_information_docs, # The seven docs that were created before
"persons_name": person_name,
"response_type" : response_types[output_type]
})

st.markdown(f"#### Output:")
st.write(output['output_text'])


Let's check out the output for Elad:

Awesome!

Step 3. Deploy and test

Great! Now that your script works locally, you can deploy it on Streamlit Community Cloud so others can use it. Remember to load your .env variables as secrets on Streamlit.

To see a video of this deployment, click here.

To check out the live app, click here.

Wrapping up

Thanks for joining me on this journey! You created an app that summarized the information you needed to prepare for meeting an interview candidate.

If you have any questions, please post them in the comments below or contact me on Twitter or email contact@dataindependent.com.

Happy coding! 👴

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

AIInterviewer-flowchart.png (816×766)
https://blog.streamlit.io/content/images/2023/08/AIInterviewer-flowchart.png#border


Todd Wang - Streamlit
https://blog.streamlit.io/author/todd/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Todd Wang
1 post
AI Interviewer: Customized interview preparation with generative AI

How we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!

LLMs
by
Haoxiang Jia and 
1
 more,
August 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Trubrics: A user feedback tool for your AI Streamlit apps
https://blog.streamlit.io/trubrics-a-user-feedback-tool-for-your-ai-streamlit-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Trubrics: A user feedback tool for your AI Streamlit apps

A 3-step guide on collecting, analyzing, and managing AI model feedback

By Jeff Kayne
Posted in Advocate Posts, July 28 2023
1. How to create a free account with Trubrics
2. How to collect user feedback from your AI Streamlit app
3. How to analyze and manage your user feedback in Trubrics
Wrapping up
Contents
Share this post
← All posts

Hey, community! 👋

My name is Jeff, and I’m a co-founder of Trubrics. As a data scientist and Machine Learning engineer, I have experienced firsthand the challenges of deploying new ML models without understanding how users interact with them. This can lead to reduced model performance and, ultimately, misaligned models and users.

More specifically, here is why you should start listening to your users:

🚨 Identify bugs: Users constantly run inference on your models and may be more likely to find bugs than your ML monitoring system.
🧑‍💻️ Fine-tune: Users often have domain knowledge that can be useful in fine-tuning models.
👥 Align: Identifying user preferences can help align models with users.

If you want to generate user insights on your AI models, we've built Trubrics, the first user insights platform for AI models.

In this post, we'll cover:

How to create a free account with Trubrics
How to collect user feedback from your AI Streamlit app
How to analyze and manage your user feedback in Trubrics
🚨
TLDR: Here is the app and the repo, and here is the video (watch it above).
1. How to create a free account with Trubrics

Create an account directly in the Trubrics app:

After logging in, you'll find a pre-existing default component.

Click on it to access code snippets and start collecting feedback. You can create different feedback components to collect and organize feedback across multiple projects or apps.

Now, to save your first piece of feedback to the default component, head over to our example user feedback LLM app:

2. How to collect user feedback from your AI Streamlit app

Let's explore various code snippets to see how you can embed Trubrics feedback components directly into your app.

First, install our SDK with the streamlit dependency (if it's not already installed):

pip install "trubrics[streamlit]"


Then copy and paste this snippet into your app:

import streamlit as st
from trubrics.integrations.streamlit import FeedbackCollector

collector = FeedbackCollector(
    email=st.secrets.TRUBRICS_EMAIL,
    password=st.secrets.TRUBRICS_PASSWORD,
    project="default"
)

user_feedback = collector.st_feedback(
    component="default",
    feedback_type="thumbs",
    open_feedback_label="[Optional] Provide additional feedback",
    model="gpt-3.5-turbo",
    prompt_id=None,  # checkout collector.log_prompt() to log your user prompts
)

if user_feedback:
    st.write(user_feedback)

What's going on here? Let's break it down:

The FeedbackCollector object. Store your Trubrics credentials in st.secrets, and specify the project to which you want to save. In this case, use the default.
Its st_feedback() method. Calling this method allows users to embed UI widgets in their apps. These widgets can be added throughout your app to collect feedback on different predictions.

And that's it!

Now you'll see a thumbs up / down feedback widget in your application, like this:

🚨
To learn more about customizing your feedback component, read our docs.
3. How to analyze and manage your user feedback in Trubrics

After you've saved the feedback to Trubrics, consult the 👍 Feedback page for quantitative and qualitative analysis. This enables AI teams to understand whether users are satisfied with predictions and compare results between different models.

Various filters on the User feedback tab allow AI teams to:

Aggregate responses by frequency (hourly, daily, weekly, monthly)
View all responses for a specific score, model, or user
Compare responses for all scores, models, or users

For qualitative feedback, user comments are collected in the text field of the `Feedback` response. All comments are listed in the Comments tab and may be grouped to create an issue. AI model issues can also be opened or closed on the issues page.

Wrapping up

Thank you for reading! You've learned how to collect app user feedback and view it in Trubrics. If you have any questions, please post them in the comments below or contact me directly on my LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Haoxiang Jia - Streamlit
https://blog.streamlit.io/author/haoxiang/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Haoxiang Jia
1 post
AI Interviewer: Customized interview preparation with generative AI

How we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!

LLMs
by
Haoxiang Jia and 
1
 more,
August 9 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts: Using LLMs with Streamlit
https://blog.streamlit.io/tag/llms/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in LLMs
33 posts
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

LLMs
by
Adrien Treuille and 
1
 more,
June 15 2023
LangChain tutorial #3: Build a Text Summarization app

Explore the use of the document loader, text splitter, and summarization chain

LLMs
by
Chanin Nantasenamat
,
June 13 2023
LangChain tutorial #2: Build a blog outline generator app in 25 lines of code

A guide on conquering writer’s block with a Streamlit app

LLMs
by
Chanin Nantasenamat
,
June 7 2023
LangChain tutorial #1: Build an LLM-powered app in 18 lines of code

A step-by-step guide using OpenAI, LangChain, and Streamlit

Tutorials
by
Chanin Nantasenamat
,
May 31 2023
8 tips for securely using API keys

How to safely navigate the turbulent landscape of LLM-powered apps

Tutorials
by
Chanin Nantasenamat
,
May 19 2023
How to build an LLM-powered ChatBot with Streamlit

A step-by-step guide using the unofficial HuggingChat API

LLMs
by
Chanin Nantasenamat
,
May 10 2023
Chat with the Cat Generative Dialogue Processor (CatGDP)

Build your own catbot with a quirky persona!

Advocate Posts
by
Tianyi Pan
,
May 3 2023
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

Advocate Posts
by
Dmitry Kosarevsky
,
April 18 2023
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

Advocate Posts
by
Kanak Mittal
,
April 11 2023
Building GPT Lab with Streamlit

12 lessons learned along the way

LLMs
by
Dave Lin
,
April 6 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Greg Kamradt - Streamlit
https://blog.streamlit.io/author/greg-kamradt/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Greg Kamradt
1 post
Generate interview questions from a candidate’s tweets

Make an AI assistant to prepare for interviews with LangChain and Streamlit

LLMs
by
Greg Kamradt
,
June 24 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

prompttools-playground.gif (1258×782)
https://blog.streamlit.io/content/images/2023/08/prompttools-playground.gif#browser


Dave Lin - Streamlit
https://blog.streamlit.io/author/dave/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Dave Lin
4 posts
How to build an interconnected multi-page Streamlit app

From planning to execution—how I built GPT lab

LLMs
by
Dave Lin
,
July 19 2023
Semantic search, Part 2: Building a local search app

Making an app with Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data from Snowflake Marketplace

Snowflake powered ❄️
by
Dave Lin
,
May 18 2023
Semantic search, Part 1: Implementing cosine similarity

Wrangling Foursquare data and implementing semantic search in Snowflake

Snowflake powered ❄️
by
Dave Lin
,
May 17 2023
Building GPT Lab with Streamlit

12 lessons learned along the way

LLMs
by
Dave Lin
,
April 6 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Randy Pettus - Streamlit
https://blog.streamlit.io/author/randy/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Randy Pettus
1 post
Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

run-clear-share.png (1490×394)
https://blog.streamlit.io/content/images/2023/08/run-clear-share.png#border


portfolio-streamlit-app.gif (702×382)
https://blog.streamlit.io/content/images/2023/10/portfolio-streamlit-app.gif


dynamic-sidebar.png (642×734)
https://blog.streamlit.io/content/images/2023/08/dynamic-sidebar.png


instruction-mode.png (648×740)
https://blog.streamlit.io/content/images/2023/08/instruction-mode.png


awesome-grid-layout.png (1514×1474)
https://blog.streamlit.io/content/images/2023/08/awesome-grid-layout.png#border


awesome-sdr-example.png (1532×1252)
https://blog.streamlit.io/content/images/2023/08/awesome-sdr-example.png#border


Kevin Tse - Streamlit
https://blog.streamlit.io/author/kevin-tse/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Kevin Tse
1 post
Exploring LLMs and prompts: A guide to the PromptTools Playground

Learn how to build dynamic, stateful applications that harness multiple LLMs at once

LLMs
by
Steve Krawczyk and 
1
 more,
August 18 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Steve Krawczyk - Streamlit
https://blog.streamlit.io/author/steve/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Steve Krawczyk
1 post
Exploring LLMs and prompts: A guide to the PromptTools Playground

Learn how to build dynamic, stateful applications that harness multiple LLMs at once

LLMs
by
Steve Krawczyk and 
1
 more,
August 18 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Caroline Frasca - Streamlit
https://blog.streamlit.io/author/caroline/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Caroline Frasca
1 post
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

LLMs
by
Caroline Frasca and 
2
 more,
August 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Yi Ding - Streamlit
https://blog.streamlit.io/author/yi/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Yi Ding
1 post
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

LLMs
by
Caroline Frasca and 
2
 more,
August 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Krista Muir - Streamlit
https://blog.streamlit.io/author/krista/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Krista Muir
1 post
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

LLMs
by
Caroline Frasca and 
2
 more,
August 23 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

yes.png (1830×548)
https://blog.streamlit.io/content/images/2023/08/yes.png


no.png (1830×548)
https://blog.streamlit.io/content/images/2023/08/no.png


LlamaIndexChatbot.png (1202×1004)
https://blog.streamlit.io/content/images/2023/08/LlamaIndexChatbot.png


Data analysis with Mito: A powerful spreadsheet in Streamlit
https://blog.streamlit.io/data-analysis-with-mito-a-powerful-spreadsheet-in-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

By Nate Rush
Posted in Advocate Posts, August 8 2023
Why Mito?
Tab renaming
Column filtering
Formula writing
Use case 1: Mito internal Streamlit app
Use case 2: Python script without coding
Use case 3: Mito in any Streamlit app
Wrapping up
Contents
Share this post
← All posts
TL;DR: The Mito spreadsheet is a drop-in replacement for st.dataframe or st.data_editor. View and edit dataframes using spreadsheet formulas, pivot tables, graphs, and more. For every edit, Mito generates the corresponding Python code. Check out the sample app and the code. Enjoy!





Hiya, Streamlit users! 👋

I'm Nate, co-founder of Mito—a spreadsheet that helps analysts transition from Excel to Python. I've been working on Mito for almost four years now (oof). For the past two years, our open-source community and enterprise clients have been asking us to bring Mito to Streamlit. Here is why.

Why Mito?

Streamlit apps that let users upload unstructured data often encounter issues because users upload data in many different formats.

The typical solution is for the users to upload a file of their choosing and for the app creator to provide a variety of st.text_input, st.selectbox, and st.button that let them:

Rename, move, and delete columns
Change the types of columns
Filter null and other unwanted values
And much more

This leads to apps with many inputs and strict requirements for user-provided data, effectively drowning users in basic data cleaning and transformation options.

It's not easy to provide enough configuration options for the data formats that users bring to the table. Users often get stuck configuring a single additional parameter (e.g., how many rows to skip before the header row). As a result, they can't use the app you worked so hard to create.

Check out this sample app to try Mito's flexible data importing, cleaning, preprocessing tools, and importing methods, including:

Importing CSV/Excel files, including advanced configuration options
Renaming, reordering, and removing columns in place
Filtering in a classic interface with many filter conditions
Writing spreadsheet formulas to transform your data

Mito provides data cleaning options beyond your basic data importing. Here are a few examples.

Tab renaming

Need to rename columns to match the expected format? Rename the tabs directly in a spreadsheet:

Column filtering

Want to run the rest of your app on a subset of your data? Use a spreadsheet to filter out the data you need:

Formula writing

Need to let users transform columns in a more complex way? Let them write formulas as they do in Excel:

With Mito, users can import a dataset of their choice into your app and format it as your app requires.

Use case 1: Mito internal Streamlit app

At Mito, we use an internal Streamlit app to monitor the current state of our company. It displays our current revenue, expenses, customer information, the number of blog posts from the previous week, and other relevant data. You can select variables to compare, contrast, regress, and more—to understand how Mito performs over time.

NOTE: Making our internal app was a breeze. Streamlit can be the easiest way to explore datasets of varying complexity. Check out this fantastic app example which provides helpful, pre-configured views for gaining insight from data and graphs.

As the app's creator, I wanted my team members to be able to compare the number of sales we made this month with the number of sales from the previous month. So I created the following input:

min_date, max_date = st.date_input('Compare within Range', value=(one_week_ago, today))


This method is great for comparing date ranges but can't answer ad-hoc questions like "How has the number of motorcycles we've sold per month changed?" To answer questions like that, you can use Mito's pivot table feature (you can also write formulas and generate graphs right within your Streamlit app):

Use case 2: Python script without coding

At Mito, we work with financial institutions that have thousands of users poring over spreadsheets and running multiple spreadsheet processes. Those spreadsheets can have hundreds of tabs and thousands of formulas. And if the person responsible for the spreadsheet leaves, it can take weeks or even months for someone else to audit and use it.

So, what can you do?

One option is to train your spreadsheet users to learn Python. But not everyone wants to do that.

print("hello world, I don't think I love programming...")


Another option is to use Mito! Mito enables non-programmers to write Python code without ever needing to see it. Every spreadsheet edit generates the corresponding code in the backend (a full script is created to codify the process).

Try this sample app to see how it works (here is the code for it):

Use case 3: Mito in any Streamlit app

Using Mito is super simple.

Just install the Mito package with pip install mitosheet:

# In a terminal
pip install mitosheet

# In your Streamlit app
from mitosheet.streamlit.v1 import spreadsheet

...

spreadsheet(df)


Next, display any dataframes inside the spreadsheet component:

import pandas as pd
import streamlit as st
from mitosheet.streamlit.v1 import spreadsheet

# Create a dataframe with pandas (you can pass any pandas dataframe)
dataframe = pd.DataFrame({'A': [1, 2, 3]})

# Display the dataframe in a Mito spreadsheet
final_dfs, code = spreadsheet(dataframe)

# Display the final dataframes created by editing the Mito component
# This is a dictionary from dataframe name -> dataframe
st.write(final_dfs)

# Display the code that corresponds to the script
st.code(code)


And you're done!

Wrapping up

Mito has been four years in the making, and we're excited to finally share it with you! Over the coming weeks, we'll be improving our Streamlit support, including the additional functionality within the spreadsheet, the ability to set predefined views, and more configuration options.

As we roll out more features, we'd love to hear your feedback. Please open an issue on GitHub or leave us comments below.

Happy app-building! 🧑‍💻

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

ChatGPT.png (1202×1004)
https://blog.streamlit.io/content/images/2023/08/ChatGPT.png


llamaindexgif.gif (1517×1117)
https://blog.streamlit.io/content/images/2023/08/llamaindexgif.gif#browser


lottie.png (2000×708)
https://blog.streamlit.io/content/images/2023/10/lottie.png


rag-with-llamaindex-1.png (1088×458)
https://blog.streamlit.io/content/images/2023/08/rag-with-llamaindex-1.png#border


Untitled--18-.png (798×332)
https://blog.streamlit.io/content/images/2023/10/Untitled--18-.png


Untitled--19-.png (886×325)
https://blog.streamlit.io/content/images/2023/10/Untitled--19-.png


Untitled--17-.png (631×211)
https://blog.streamlit.io/content/images/2023/10/Untitled--17-.png


Untitled--16-.png (1438×810)
https://blog.streamlit.io/content/images/2023/10/Untitled--16-.png


Untitled--15-.png (1487×353)
https://blog.streamlit.io/content/images/2023/10/Untitled--15-.png


Untitled--14-.png (1578×399)
https://blog.streamlit.io/content/images/2023/10/Untitled--14-.png


Untitled--13-.png (401×277)
https://blog.streamlit.io/content/images/2023/10/Untitled--13-.png


Develop Streamlit apps in-browser with GitHub Codespaces
https://blog.streamlit.io/edit-inbrowser-with-github-codespaces/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Develop Streamlit apps in-browser with GitHub Codespaces

Build anywhere without the hassle of a local Python environment

By Brian Holt
Posted in GitHub Codespaces, September 14 2023
Now, it's all in your browser 🪄
#1: Create a new app
#2: Fork an existing app
#3 Edit an existing app
Watch Codespaces in action
Why Github Codespaces
Give it a spin
Contents
Share this post
← All posts

Picture this: You're about to create your first Streamlit app, or maybe you've found an awesome Streamlit app you want to dive into. In both scenarios, you can't begin building or examine the code without a local development setup.

What if you could do all of this without the need for a local Python environment?

Now, it's all in your browser 🪄

With GitHub Codespaces, you can skip the local environment and enjoy:

Instant setup: Create, fork, and deploy data apps in a single click.
Frictionless editing: Explore and debug source code, with libraries pre-configured.
Develop anywhere: Enjoy the flexibility to build Streamlit apps from any browser.

There are three ways to use Codespaces: creating a new app, editing an existing app, and forking an existing one.

#1: Create a new app

To see it in action, simply log onto Community Cloud and create a new app. (See docs for step-by-step guide.) You can also edit an existing app in your browser.

#2: Fork an existing app

From any public Streamlit app, click “Fork this app''. Copy the app or explore how it works all within your browser. Then, deploy to Community Cloud to share what you have built!

You can also fork and spin up a Codespace directly from an app’s repository. Just select the "Create codespace on master" button.

#3 Edit an existing app

Flexible development is not limited to creating new apps or forking existing ones. Simply select "Edit" in Community Cloud and click the "Create Codespace" button. For a more detailed walkthrough, check out our docs.

Watch Codespaces in action

In this video, @DataProfessor puts it all together! Watch step-by-step how you can use GitHub Codespaces to build Streamlit apps in the browser.


Why Github Codespaces

We wanted to give developers an in-browser editor that is free, powerful, easy and secure.

With GitHub Codespaces, you’ll have access to:

Ample free tier: Each month, you’ll have 60 hours of run time on 2 core Codespaces, plus 15 GB of storage.
A real Linux operating system: develop and deploy on the same system.
Seamless tech stack: use the tools you already love, like Visual Studio Code.
Convenient hosting: Easily host and share your app with Community Cloud.
GitHub’s world class security: have the peace of mind that your code and networks are secure.
Give it a spin

Start editing in-browser today. Check out the documentation for additional details. We want to reduce frictions so users can create more amazing data apps.

What do you think about this new feature? How can we improve your development experience?

Happy Streamlitting 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in GitHub Codespaces...

View even more →

Develop Streamlit apps in-browser with GitHub Codespaces

Build anywhere without the hassle of a local Python environment

GitHub Codespaces
by
Brian Holt
,
September 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Untitled--12-.png (899×216)
https://blog.streamlit.io/content/images/2023/10/Untitled--12-.png


Untitled--11-.png (1150×1046)
https://blog.streamlit.io/content/images/2023/10/Untitled--11-.png#border


Untitled--10-.png (830×467)
https://blog.streamlit.io/content/images/2023/10/Untitled--10-.png


Untitled--9-.png (1299×366)
https://blog.streamlit.io/content/images/2023/10/Untitled--9-.png


Untitled--8-.png (780×893)
https://blog.streamlit.io/content/images/2023/10/Untitled--8-.png


Untitled--7-.png (2220×450)
https://blog.streamlit.io/content/images/2023/10/Untitled--7-.png


Untitled--6-.png (252×149)
https://blog.streamlit.io/content/images/2023/10/Untitled--6-.png


Untitled--5-.png (861×523)
https://blog.streamlit.io/content/images/2023/10/Untitled--5-.png


Recording-2023-09-21-at-12.03.38.gif (769×102)
https://blog.streamlit.io/content/images/2023/10/Recording-2023-09-21-at-12.03.38.gif


---1-.png (1550×900)
https://blog.streamlit.io/content/images/2023/10/---1-.png


Untitled--4-.png (795×167)
https://blog.streamlit.io/content/images/2023/10/Untitled--4-.png


Untitled--3-.png (763×164)
https://blog.streamlit.io/content/images/2023/10/Untitled--3-.png


PureHuB: A search engine for your university
https://blog.streamlit.io/purehub-a-search-engine-for-your-university/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

By Mala Deep Upadhaya
Posted in Advocate Posts, August 10 2023
Why PureHuB?
App overview
1. Scrapper setup
2. Preprocessing
2.1. Tokenization
2.2 Stopwords
2.3 Porter-Stemmer algorithms
2.4 Term Frequency - Inverse Document Frequency (TF-IDF)
2.5 Cosine similarity
2.6 Inverted index
Step 3. Building the front end
Key takeaways
Wrapping up
Contents
Share this post
← All posts
TL;DR: I made PureHuB (code), an inverted indexing search engine, using Python and libraries like BeautifulSoup and NLTK for web scraping, data preprocessing, and natural language processing.



Hey, community! 👋

My name is Mala Deep Upadhaya, and I'm an independent consultant specializing in data visualization and analysis projects. I also teach people how to analyze data and share insights in a more accessible manner. I built PureHuB to give access to profiles, groundbreaking work, and cutting-edge research produced by the extraordinary minds of Coventry University.

In this post, I'll walk you through the process of building it step-by-step:

Scrapper setup
Preprocessing
Building the front end

But first…

Why PureHuB?

Humans have always been driven by their inherent curiosity to seek out information. This is one of the motivations behind developing search engines [1]. From WebCrawler, created by Brian Pinkerton, a computer science student at the University of Washington, to the web3.0-based search engines like Presearch, to AltaVista, Lycos, Yahoo, HotBot, and Google in 1988, many search engines have grown rapidly [2]. There are 29 generic search engines and numerous domain-specific ones [3].

Unlike general search engines that index and search the entire web for relevant results, vertical search engines focus on specific data sets. This makes them faster and more relevant when users look for specific information. Google Books, Google News, Google Flights, and Google Finance are examples of vertical searches within the Google search engine. These search engines were created to ease the burden of searching for specialized information or topics.

App overview

Here is how the app works.

To search for research, enter your query in the "Search research" field. Then, choose an operator: "Exact" or "Relevant." Select "Exact" for an exact match of your query or "Relevant" for the relevance and ranking of the search results. You can also choose the search type between "Publications" and "Authors."

Next, click "SEARCH." The app will give you the most relevant results:



The dataset used in this study was scraped from Coventry University's Pure Portal using Python, BeautifulSoup, and Selenium. The seed URL for the crawler was https://pureportal.coventry.ac.uk/en/publications/. The scraped dataset consists of 4022 publications with four attributes: publication name, publication URL, author name, and date of publication.
1. Scrapper setup

For web scraping and automation, I used two popular Python libraries:

BeautifulSoup for parsing the HTML of Pure Portal. It provides an easy and convenient way to extract data from web pages by navigating the HTML structure.
Selenium for automating web browsers. It allows you to control a web browser programmatically and was used to perform various actions, such as clicking buttons and navigating between pages, without human intervention.

To account for website exploration through browsers, I incorporated the Chrome browser and used the ChromeDriver, a separate executable that bridges Selenium and the Chrome browser. It has better compatibility, debugging, and logging features during development. I also implemented a 1-second time delay.

Here is the code:

import os  # Module for interacting with the operating system
import time  # Module for time-related operations
import ujson  # Module for working with JSON data
from random import randint  # Module for generating random numbers
from typing import Dict, List, Any  # Type hinting imports

import requests  # Library for making HTTP requests
from bs4 import BeautifulSoup  # Library for parsing HTML data
from selenium import webdriver  # Library for browser automation
from selenium.common.exceptions import NoSuchElementException  # Exception for missing elements
from webdriver_manager.chrome import ChromeDriverManager  # Driver manager for Chrome (We are using Chromium based )

def initCrawlerScraper(seed,max_profiles=500):
    # Initialize driver for Chrome
    webOpt = webdriver.ChromeOptions()
    webOpt.add_experimental_option('excludeSwitches', ['enable-logging'])
    webOpt.add_argument('--ignore-certificate-errors')
    webOpt.add_argument('--incognito')
    webOpt.headless = True
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webOpt)
    driver.get(seed)  # Start with the original link

    links = []  # Array with pureportal profiles URL
    pub_data = []  # To store publication information for each pureportal profile

    nextLink = driver.find_element_by_css_selector(".nextLink").is_enabled()  # Check if the next page link is enabled
    print("Crawler has begun...")
    while (nextLink):
        page = driver.page_source
        # XML parser to parse each URL
        bs = BeautifulSoup(page, "lxml")  # Parse the page source using BeautifulSoup

        # Extracting exact URL by spliting string into list
        for link in bs.findAll('a', class_='link person'):
            url = str(link)[str(link).find('<https://pureportal.coventry.ac.uk/en/persons/'):].split('>"')
            links.append(url[0])
            
        # Click on Next button to visit next page
        try:
            if driver.find_element_by_css_selector(".nextLink"):
                element = driver.find_element_by_css_selector(".nextLink")
                driver.execute_script("arguments[0].click();", element)
            else:
                nextLink = False
        except NoSuchElementException:
            break
            
        # Check if the maximum number of profiles is reached
        if len(links) >= max_profiles:
            break
            
    print("Crawler has found ", len(links), " pureportal profiles")
    write_authors(links, 'Authors_URL.txt') # Write the authors' URLs to a file

    print("Scraping publication data for ", len(links), " pureportal profiles...")
    count = 0
    for link in links:
        # Visit each link to get data
        time.sleep(1)  
        driver.get(link)
        try:
            if driver.find_elements_by_css_selector(".portal_link.btn-primary.btn-large"):
                element = driver.find_elements_by_css_selector(".portal_link.btn-primary.btn-large")
                for a in element:
                    if "research output".lower() in a.text.lower():
                        driver.execute_script("arguments[0].click();", a)
                        driver.get(driver.current_url)
                        # Get name of Author
                        name = driver.find_element_by_css_selector("div[class='header person-details']>h1")
                        r = requests.get(driver.current_url)
                        # Parse all the data via BeautifulSoup
                        soup = BeautifulSoup(r.content, 'lxml')

                        # Extracting publication name, publication url, date and CU Authors
                        table = soup.find('ul', attrs={'class': 'list-results'})
                        if table != None:
                            for row in table.findAll('div', attrs={'class': 'result-container'}):
                                data = {}
                                data['name'] = row.h3.a.text
                                data['pub_url'] = row.h3.a['href']
                                date = row.find("span", class_="date")

                                rowitem = row.find_all(['div'])
                                span = row.find_all(['span'])
                                data['cu_author'] = name.text
                                data['date'] = date.text
                                print("Publication Name :", row.h3.a.text)
                                print("Publication URL :", row.h3.a['href'])
                                print("CU Author :", name.text)
                                print("Date :", date.text)
                                print("\
")
                                pub_data.append(data)
            else:
                # Get name of Author
                name = driver.find_element_by_css_selector("div[class='header person-details']>h1")
                r = requests.get(link)
                # Parse all the data via BeautifulSoup
                soup = BeautifulSoup(r.content, 'lxml')
                # Extracting publication name, publication URL, date and CU Authors
                table = soup.find('div', attrs={'class': 'relation-list relation-list-publications'})
                if table != None:
                    for row in table.findAll('div', attrs={'class': 'result-container'}):
                        data = {}
                        data["name"] = row.h3.a.text
                        data['pub_url'] = row.h3.a['href']
                        date = row.find("span", class_="date")
                        rowitem = row.find_all(['div'])
                        span = row.find_all(['span'])
                        data['cu_author'] = name.text
                        data['date'] = date.text
                        print("Publication Name :", row.h3.a.text)
                        print("Publication URL :", row.h3.a['href'])
                        print("CU Author :", name.text)
                        print("Date :", date.text)
                        print("\
")
                        pub_data.append(data)
        except Exception:
            continue

    print("Crawler has scrapped data for ", len(pub_data), " pureportal publications")
    driver.quit()
    # Writing all the scraped results in a file with JSON format
    with open('scraper_results.json', 'w') as f:
        ujson.dump(pub_data, f)

initCrawlerScraper('<https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/>', max_profiles=500)


I imported the necessary modules and libraries for interacting with the operating system, performing time-related operations, working with JSON data, generating random numbers, making HTTP requests, parsing HTML data, and automating web browsing using Selenium.

The code defines a function called initCrawlerScraper, which takes a seed URL and an optional max_profiles parameter. The function uses web scraping techniques to extract publication data from the crawled web pages. It finds links to individual profiles, visits each profile, and retrieves publication information associated with each profile. The extracted data includes publication names, URLs, publication dates, and the authors' names from Coventry University. The extracted publication data is stored in the pub_data list.

Finally, the function saves the authors' URLs in a file named "Authors_URL.txt" and the scraped publication data in a JSON file named "scraper_results.json".

2. Preprocessing

I loaded the obtained scrapped file and imported NLTK libraries, including stopwords, word_tokenize, and Porter-Stemmer, for natural language processing (NLP) tasks.

2.1. Tokenization

Tokenization is the process of breaking down a text or a sequence of characters into smaller units called tokens. They can be individual words, sentences, or even sub-word units, depending on the level of tokenization applied [4].

To tokenize each publication name and the author's name into individual words, I used the nltk.tokenize module's word_tokenize function (takes the input as tokens):

import nltk
from nltk.tokenize import word_tokenize

# Open a file with publication names in read mode
with open('pub_name.json', 'r') as f:
    publication = f.read()

# Load JSON File
pubName = ujson.loads(publication)

# Tokenization
tokenized_pub_list = []
for file in pubName:
    tokens = word_tokenize(file)
    tokenized_pub_list.append(tokens)

# Save tokenized publication list to a file
with open('tokenized_pub_list.json', 'w') as f:
    ujson.dump(tokenized_pub_list, f)


This code snippet loads publication names from the pub_name.json file. Then, word_tokenize from NLTK is used to tokenize each publication name. The tokenized publication lists are stored in the tokenized_pub_list list.

Finally, the tokenized publication list is saved to the tokenized_pub_list.json file.

2.2 Stopwords

I used the nltk.corpus.stopwords module to remove commonly used English stopwords. Stopwords are words that are considered insignificant [5], such as "I", "he", "a", "the", etc. This allows focusing on more meaningful words.

Here is the code:

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Open a file with publication names in read mode
with open('pub_name.json', 'r') as f:
    publication = f.read()

# Load JSON File
pubName = ujson.loads(publication)

# Predefined stopwords in NLTK are used
stop_words = stopwords.words('english')

# Stopword removal
filtered_pub_list = []
for file in pubName:
    tokens = word_tokenize(file) #from tokenized 
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    filtered_pub = ' '.join(filtered_tokens)
    filtered_pub_list.append(filtered_pub)

# Save filtered publication list to a file
with open('filtered_pub_list.json', 'w') as f:
    ujson.dump(filtered_pub_list, f)


This code snippet loads publication names from the pub_name.json file, then removes stopwords using stopwords.words('english') from the NLTK library. Each publication name is tokenized using word_tokenize, and stopwords are filtered out from the tokens. The filtered tokens are then joined back into a string, and the filtered publication list is stored in the filtered_pub_list list.

Finally, the filtered publication list is saved to the filtered_pub_list.json file.

2.3 Porter-Stemmer algorithms

Stemming is the process of reducing words to their base or root form. The Porter-Stemmer algorithm applies a series of heuristic rules to remove common word endings and suffixes to achieve this reduction [6]. I used the PorterStemmer from nltk.stem to implement the Porter stemming algorithm and help group together similar words. In the search_data function, the Porter Stemmer is used to stem words before performing search operations. For example, the word "computational" and "computer" are reduced to "compute."

Here is the code:

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Open a file with publication names in read mode
with open('pub_name.json', 'r') as f:
    publication = f.read()

# Load JSON File
pubName = ujson.loads(publication)

# Initialize PorterStemmer
stemmer = PorterStemmer()

# Stemming
stemmed_pub_list = []
for file in pubName:
    tokens = word_tokenize(file)
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    stemmed_pub = ' '.join(stemmed_tokens)
    stemmed_pub_list.append(stemmed_pub)

# Save stemmed publication list to a file
with open('stemmed_pub_list.json', 'w') as f:
    ujson.dump(stemmed_pub_list, f)


This code snippet loads publication names from the pub_name.json file applies Porter stemming using PorterStemmer from NLTK, and tokenizes each publication name using word_tokenize. The resulting tokens are derived using the stem() method of the PorterStemmer object. They're then joined back into a string, and the stemmed publication list is stored in the stemmed_pub_list list.

Finally, the code saves the stemmed publication list to the stemmed_pub_list.json file.

2.4 Term Frequency - Inverse Document Frequency (TF-IDF)

Information retrieval and text mining are two major problems frequently using the TF-IDF weight. TF-IDF is used to:

Determine the value of a word to a group of documents
Give more significance to a term the more frequently it appears

TF is determined by dividing the number of times a phrase appears in a document by the total number of words [6]. On the other hand, IDF measures the term's importance, calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the particular term appears [7].

I used TF-IDF to calculate the relevance or similarity between documents based on their term frequencies and the rarity of the terms in the document collection. I applied the TF-IDF vectorization using the TfidfVectorizer class from the scikit-learn library and stored the resulting TF-IDF vectors in the temp_file variable.

Finally, I used tfidf.transform(stem_word_file) to transform the stem_word_file (which represents a single document) into its TF-IDF vector representation and calculated the cosine similarity between temp_file and the TF-IDF vector of stem_word_file using the cosine_similarity() function.

Here is the code:

import ujson
from sklearn.feature_extraction.text import TfidfVectorizer

# Open a file with publication names in read mode
with open('pub_name.json', 'r') as f:
    publication = f.read()

# Load JSON File
pubName = ujson.loads(publication)

# Initialize TfidfVectorizer
vectorizer = TfidfVectorizer()

# Calculate TF-IDF
tfidf_matrix = vectorizer.fit_transform(pubName)

# Get feature names (terms)
feature_names = vectorizer.get_feature_names()

# Print TF-IDF scores
for i in range(len(pubName)):
    print("Publication:", pubName[i])
    for j in range(len(feature_names)):
        tfidf_score = tfidf_matrix[i, j]
        if tfidf_score > 0:
            print("  Term:", feature_names[j])
            print("  TF-IDF Score:", tfidf_score)
    print()


In this code snippet, the publication names are loaded from the pub_name.json file. The code uses the TfidfVectorizer from scikit-learn to calculate the TF-IDF scores. The fit_transform method is used to transform the publication names into a TF-IDF matrix. The feature names (terms) can be obtained using the get_feature_names method. The TF-IDF scores are printed for each publication and term combination.

2.5 Cosine similarity

Regardless of the size of the documents, cosine similarity can measure the text similarity between them. The cosine similarity metric has a value range from 0 to 1 and evaluates the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space; a higher value (1) indicates greater similarity [7].

Here is the code:

import ujson
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Open a file with publication names in read mode
with open('pub_name.json', 'r') as f:
    publication = f.read()

# Load JSON File
pubName = ujson.loads(publication)

# Initialize TfidfVectorizer
vectorizer = TfidfVectorizer()

# Calculate TF-IDF
tfidf_matrix = vectorizer.fit_transform(pubName)

# Calculate cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix)

# Example: Calculate similarity between first two publications
pub1_index = 0
pub2_index = 1
similarity_score = cosine_sim[pub1_index, pub2_index]
print("Similarity score between publication", pub1_index, "and publication", pub2_index, "is:", similarity_score)


This code snippet loads publication names from the pub_name.json file. It uses the TfidfVectorizer to calculate the TF-IDF matrix and then the cosine_similarity function to compute the cosine similarity between all pairs of vectors in the TF-IDF matrix. The similarity score is accessed between any two publications by indexing the cosine_sim matrix with the corresponding indices.

Finally, the resulting cosine similarity scores are assigned to the corresponding publication or author in the output_datadictionary. This output dictionary is what users see when they search.

2.6 Inverted index

An inverted index is a data structure used in information retrieval systems, search engines, and text analysis to efficiently store and retrieve information about the presence of keywords (or words) in a collection of documents [8]. It's called "inverted" because it reverses the link between terms and documents compared to a standard index. In traditional indexes, documents are indexed based on their unique IDs, and each entry contains a list of terms found in that document. In an inverted index, terms are organized as keys and associated with a list of documents or occurrences where they exist.

Here is the code:

data_dict = {} #empty dictionary

# Indexing process
for a in range(len(pub_list_stem_wo_sw)):
    for b in pub_list_stem_wo_sw[a].split():
        if b not in data_dict:
            data_dict[b] = [a]
        else:
            data_dict[b].append(a)


To create an inverted index, the code initializes a dictionary named data_dict. This process involves a nested loop. The outer loop iterates over the length of the pub_list_stem_wo_sw list. Within each iteration, the inner loop iterates over each word in the publication name obtained by splitting the name based on spaces.

A new key-value pair is added during the inner loop if the current word is not already a key in data_dict. The key is the word itself, and the value is a list containing the current index of the publication in the pub_list_stem_wo_sw list. This way, each word is associated with a list of indices indicating the publications in which it appears. On the other hand, if the word already exists as a key in data_dict, the index of the current publication is appended to the existing list for that word. This allows for the indexing of multiple publications containing the same word.

By performing this process, the code creates an inverted index that facilitates efficient searching and retrieval of publications based on keywords. The resulting data_dict serves as a valuable resource for information retrieval tasks, providing a mapping from words to the publications where they occur. The same process was conducted for the author list.

Here is the complete code:

import nltk #NLTK for natural language processing tasks
from nltk.corpus import stopwords # list of stop word 
from nltk.tokenize import word_tokenize # To tokenize each word
from nltk.stem import PorterStemmer # For specific rules to transform words to their stems

#Preprosessing data before indexing
with open('scraper_results.json', 'r') as doc: scraper_results=doc.read()

pubName = []
pubURL = []
pubCUAuthor = []
pubDate = []
data_dict = ujson.loads(scraper_results)
array_length = len(data_dict)
print(array_length)

#Seperate name, url, date, author in different file
for item in data_dict:
    pubName.append(item["name"])
    pubURL.append(item["pub_url"])
    pubCUAuthor.append(item["cu_author"])
    pubDate.append(item["date"])
with open('pub_name.json', 'w') as f:ujson.dump(pubName, f)
with open('pub_url.json', 'w') as f:ujson.dump(pubURL, f)
with open('pub_cu_author.json', 'w') as f:ujson.dump(pubCUAuthor, f)
with open('pub_date.json', 'w') as f: ujson.dump(pubDate, f)

#Open a file with publication names in read mode
with open('pub_name.json','r') as f:publication=f.read()

#Load JSON File
pubName = ujson.loads(publication)

#Predefined stopwords in nltk are used
stop_words = stopwords.words('english')
stemmer = PorterStemmer()
pub_list_first_stem = []
pub_list = []
pub_list_wo_sc = []
print(len(pubName))

for file in pubName:
    #Splitting strings to tokens(words)
    words = word_tokenize(file)
    stem_word = ""
    for i in words:
        if i.lower() not in stop_words:
            stem_word += stemmer.stem(i) + " "
    pub_list_first_stem.append(stem_word)
    pub_list.append(file)

#Removing all below characters
special_characters = '''!()-—[]{};:'"\\, <>./?@#$%^&*_~0123456789+=’‘'''
for file in pub_list:
    word_wo_sc = ""
    if len(file.split()) ==1 : pub_list_wo_sc.append(file)
    else:
        for a in file:
            if a in special_characters:
                word_wo_sc += ' '
            else:
                word_wo_sc += a
        pub_list_wo_sc.append(word_wo_sc)

#Stemming Process
pub_list_stem_wo_sw = []
for name in pub_list_wo_sc:
    words = word_tokenize(name)
    stem_word = ""
    for a in words:
        if a.lower() not in stop_words:
            stem_word += stemmer.stem(a) + ' '
    pub_list_stem_wo_sw.append(stem_word.lower())

data_dict = {}

# Indexing process
for a in range(len(pub_list_stem_wo_sw)):
    for b in pub_list_stem_wo_sw[a].split():
        if b not in data_dict:
             data_dict[b] = [a]
        else:
            data_dict[b].append(a)

print(len(pub_list_wo_sc))
print(len(pub_list_stem_wo_sw))
print(len(pub_list_first_stem))
print(len(pub_list))

with open('publication_list_stemmed.json', 'w') as f:
    ujson.dump(pub_list_first_stem, f)

with open('publication_indexed_dictionary.json', 'w') as f:
    ujson.dump(data_dict, f)

Step 3. Building the front end

I use Streamlit to build apps for my data science and ML projects [9]. It simplifies creating and deploying web-based user interfaces, making it easy to showcase work. Here I used Streamlit to create a search engine application portal.

Within the search portal, I defined a search function called search_data, which takes input text, operator value, and search type as parameters. The function first checks the operator value to determine which search method to use:

Exact: The input text is stemmed and compared with the publication or author index to find matching data. Cosine similarity is calculated between the stemmed word and the retrieved data, and the results are stored in an output dictionary.
Relevant: Multiple words are processed similarly, and matching pointers are collected. The corresponding data is transformed and compared to calculate cosine similarity, and the results are added to the output dictionary. The function provides a way to search for relevant publication or author data based on user queries and retrieves results using cosine similarity.

Here is the code:

def app():

        # Load the image and display it
    image = Image.open('cire.png')
    st.image(image)

    # Add a text description
    st.markdown("<p style='text-align: center;'> Uncover the brilliance: Explore profiles, groundbreaking work, and cutting-edge research by the exceptional minds of Coventry University.</p>", unsafe_allow_html=True)

    input_text = st.text_input("Search research:", key="query_input")
    operator_val = st.radio(
        "Search Filters",
        ['Exact', 'Relevant'],
        index=1,
        key="operator_input",
        horizontal=True,
    )
    search_type = st.radio(
        "Search in:",
        ['Publications', 'Authors'],
        index=0,
        key="search_type_input",
        horizontal=True,
    )

    if st.button("SEARCH"):
        if search_type == "Publications":
            output_data = search_data(input_text, 1 if operator_val == 'Exact' else 2, "publication")
        elif search_type == "Authors":
            output_data = search_data(input_text, 1 if operator_val == 'Exact' else 2, "author")
        else:
            output_data = {}

        # Display the search results
        show_results(output_data, search_type)

    st.markdown("<p style='text-align: center;'> Brought to you with ❤ by <a href='<https://github.com/maladeep>'>Mala Deep</a> | Data © Coventry University </p>", unsafe_allow_html=True)

def show_results(output_data, search_type):
    aa = 0
    rank_sorting = sorted(output_data.items(), key=lambda z: z[1], reverse=True)

    # Show the total number of research results
    st.info(f"Showing results for: {len(rank_sorting)}")

    # Show the cards
    N_cards_per_row = 3
    for n_row, (id_val, ranking) in enumerate(rank_sorting):
        i = n_row % N_cards_per_row
        if i == 0:
            st.write("---")
            cols = st.columns(N_cards_per_row, gap="large")
        # Draw the card
        with cols[n_row % N_cards_per_row]:
            if search_type == "Publications":
                st.caption(f"{pub_date[id_val].strip()}")
                st.markdown(f"**{pub_cu_author[id_val].strip()}**")
                st.markdown(f"*{pub_name[id_val].strip()}*")
                st.markdown(f"**{pub_url[id_val]}**")
            elif search_type == "Authors":
                st.caption(f"{pub_date[id_val].strip()}")
                st.markdown(f"**{author_name[id_val].strip()}**")
                st.markdown(f"*{pub_name[id_val].strip()}*")
                st.markdown(f"**{pub_url[id_val]}**")
                st.markdown(f"Ranking: {ranking[0]:.2f}")

        aa += 1

    if aa == 0:
        st.info("No results found. Please try again.")
    else:
        st.info(f"Results shown for: {aa}")

if __name__ == '__main__':
    app()

Key takeaways

I created an inverted indexing search app that allows users to search for research papers or authors and return relevant results based on cosine similarity. I used NLTK for text preprocessing, TF-IDF vectorization for feature extraction, and cosine similarity for document similarity measurement. I implemented the search feature with choices for exact or relevant matching using OR and AND operators. The code analyzes user input and tokenizes search phrases, using Streamlit to execute searches in a user-friendly manner.

A few limitations, if addressed, can lead to a better outcome:

Slow crawling speed: Adding a one-second time delay between requests can significantly slow down the crawling process. This can be a constraint when working with large websites or accessing up-to-date information quickly.
Search precision: While cosine similarity is a widely used statistic to assess document similarity, it may not always adequately represent semantic meaning. The search result's accuracy can be improved using word embeddings, semantic similarity measurements, or parse_dot_topn [7].

Inverted indexing can improve search experiences by providing quick access to relevant content. You can use it as a starting point for additional modifications and customization based on your unique requirements and datasets.

Wrapping up

Thank you for reading my post! Now that you've learned how to create a search engine using Python, why not take it a step further and create a search engine tailored for your university? It's a great opportunity to streamline your academic pursuits and make your research more accessible. Try it and tell me how it goes in the comments below, on LinkedIn, or on Medium.

Happy coding, and best of luck with your research! 🎈

References

[1] One Second Internet Live Stats, "1 Second - Internet Live Stats," www.internetlivestats.com. https://www.internetlivestats.com/one-second/#google-band
[2] O. Whitcombe, "The History of Search Engines," Liberty Digital Marketing, May 26, 2022. https://www.libertymarketing.co.uk/blog/a-history-of-search-engines/ (accessed Oct. 12, 2022).
[3] Wikipedia, "List of search engines," Wikipedia, May 22, 2023. https://en.wikipedia.org/w/index.php?title=List_of_search_engines&oldid=1156345015 (accessed Jun. 25, 2023).
[4] M. Hassler and G. Fliedl, "Text preparation through extended tokenization," Jun. 27, 2006. https://www.witpress.com/elibrary/wit-transactions-on-information-and-communication-technologies/37/16699 (accessed Aug. 11, 2022).
[5] S. Sarica and J. Luo, "Stopwords in Technical Language Processing," PLOS ONE, vol. 16, no. 8, p. e0254937, Aug. 2021, doi: https://doi.org/10.1371/journal.pone.0254937.
[6] N. Tsourakis, Machine Learning Techniques for Text: Apply modern techniques with Python, 1st ed. Birmingham, UK: Packt Publishing Ltd., 2022.
[7] M. D. Upadhaya, "Surprisingly Effective Way To Name Matching In Python," Medium, Mar. 27, 2022. https://towardsdatascience.com/surprisingly-effective-way-to-name-matching-in-python-1a67328e670e
[8] H. Yan, S. Ding, and T. Suel, "Inverted index compression and query processing with optimized document ordering," The Web Conference, Apr. 2009, doi: https://doi.org/10.1145/1526709.1526764.
[9] M. D. Upadhaya, "Build Your First Data Visualization Web App in Python Using Streamlit," Medium, Mar. 05, 2021. https://towardsdatascience.com/build-your-first-data-visualization-web-app-in-python-using-streamlit-37e4c83a85db (accessed Jun. 25, 2023).

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Untitled--2-.png (1694×424)
https://blog.streamlit.io/content/images/2023/10/Untitled--2-.png


Untitled-2.png (2040×1464)
https://blog.streamlit.io/content/images/2023/10/Untitled-2.png#browser


Untitled--1-.png (822×347)
https://blog.streamlit.io/content/images/2023/10/Untitled--1-.png


chat_pandas.png (747×567)
https://blog.streamlit.io/content/images/2023/08/chat_pandas.png#border


Santosh Kumar Radha - Streamlit
https://blog.streamlit.io/author/santosh/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Santosh Kumar Radha
1 post
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 3)
https://blog.streamlit.io/page/3/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
AI Interviewer: Customized interview preparation with generative AI

How we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!

LLMs
by
Haoxiang Jia and 
1
 more,
August 9 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
SimiLo: Find your best place to live

A 5-step guide on how I built an app to relocate within the U.S.

Advocate Posts
by
Kevin Soderholm
,
August 4 2023
Instant Insight: Generate data-driven presentations in a snap!

Create presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery

LLMs
by
Oleksandr Arsentiev
,
August 2 2023
Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
Trubrics: A user feedback tool for your AI Streamlit apps

A 3-step guide on collecting, analyzing, and managing AI model feedback

Advocate Posts
by
Jeff Kayne
,
July 28 2023
Chat2VIS: AI-driven visualisations with Streamlit and natural language

Leverage ChatGPT for Python code generation using prompt engineering

LLMs
by
Paula Maddigan
,
July 27 2023
snowChat: Leveraging OpenAI's GPT for SQL queries

Interact with your Snowflake database using natural language queries

Snowflake powered ❄️
by
kaarthik Andavar
,
July 25 2023
How to analyze geospatial Snowflake data in Streamlit

A guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit

Snowflake powered ❄️
by
Becky O'Connor
,
July 24 2023
LangChain tutorial #5: Build an Ask the Data app

Leverage Agents in LangChain to interact with pandas DataFrame

LLMs
by
Chanin Nantasenamat
,
July 21 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Brian Holt - Streamlit
https://blog.streamlit.io/author/brian/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Brian Holt
1 post
Develop Streamlit apps in-browser with GitHub Codespaces

Build anywhere without the hassle of a local Python environment

GitHub Codespaces
by
Brian Holt
,
September 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

GitHub Codespaces - Streamlit
https://blog.streamlit.io/tag/github-codespaces/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in GitHub Codespaces
1 post
Develop Streamlit apps in-browser with GitHub Codespaces

Build anywhere without the hassle of a local Python environment

GitHub Codespaces
by
Brian Holt
,
September 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit-Blog.png (2000×1591)
https://blog.streamlit.io/content/images/2023/10/Streamlit-Blog.png


sidebar2.png (1248×960)
https://blog.streamlit.io/content/images/2023/10/sidebar2.png


Streamlit and iFood: Empowering the Monitor Rosa project
https://blog.streamlit.io/streamlit-and-ifood-empowering-the-monitor-rosa-project/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Streamlit and iFood: Empowering the Monitor Rosa project

Harnessing technology and corporate support for social impact

By Heber Augusto Scachetti
Posted in Advocate Posts, July 14 2023
What is the Monitor Rosa project?
App overview
How was the app developed?
Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit community! 👋

My name is Heber A. Scachetti, and I'm a Data Squad Lead at iFood.

I joined iFood in 2020 after participating in a hackathon that introduced me to the Monitor Rosa project, a volunteer initiative aimed at helping the Brazilian Association of Lymphoma and Leukemia (Abrale) to address the challenges faced by patients with breast cancer.

In this post, I'll share with you how I used GitHub Actions, Google Storage, and Streamlit to create an app for the Monitor Rosa project.

What is the Monitor Rosa project?

Before we dive into the app, I want to share with you a little background on the Monitor Rosa project and how iFood, Abrale, and Streamlit helped make it happen:

iFood. iFood has been instrumental in supporting my involvement in the Monitor Rosa project through its unique employee engagement program called "Minha Quarta" (" My Wednesday"). This initiative allows iFood employees, including myself, to dedicate every Wednesday to personal development, learning, and volunteering. By providing this dedicated time, iFood indirectly contributes to the project's success by enabling its employees to invest their skills and expertise in initiatives that make a meaningful impact on society.
Abrale. Abrale is a non-profit organization created in 2002 by patients and their families with the mission of offering help and mobilizing partners so that all people with cancer and blood diseases have access to the best treatment. Abrale is the leader of the TJCC Movement (Movimento Todos Juntos Contra o Câncer—"All Together against Cancer"), developing projects related to breast cancer, such as Monitor Rosa.
Streamlit. Streamlit helped me create interactive data visualizations for the Monitor Rosa project.

The support from iFood has been invaluable, as it has allowed me to balance my professional responsibilities with my passion for making a difference in the lives of patients and their families. This partnership showcases the potential for technology companies to positively impact society by supporting initiatives that aim to improve the lives of those in need. It also highlights the importance of fostering a culture of learning and social responsibility within the corporate environment.

And now, let's dive into the app!

App overview

The app consolidates data from different sources, including the Brazilian public health system (SUS) and other data providers. It helps you explore key performance indicators (KPIs) and gain insights into Brazil's current state of cancer and blood disease treatment.

There are three repositories:

devops-pysus-get-files automates the data collection from SUS and other sources, ensuring the project has access to the most up-to-date information.
sus-kpis-analysis consolidates the collected data, maintains analyses of the KPIs, and will eventually contain models and other data artifacts that can be consumed by dashboards and other data visualization tools.
streamlit-monitor-rosa contains the Streamlit app (deployed on Streamlit Community Cloud) and serves as the main interface for users to interact with the data.

The following cutting-edge technologies ensure efficient data collection, storage, and visualization:

GitHub Actions. At regular intervals, GitHub Actions automatically fetch data from the SUS FTP server and other sources for the project to have the most up-to-date information.
Google Storage. Once collected, the data is stored in a Google Storage bucket—a centralized repository. This allows for easy access and management of the data by the project's various components. The data is then read and processed by the code in the sus-kpis-analysis repo, which consolidates the data and maintains analyses of the KPIs.
Streamlit. The Streamlit application in the streamlit-monitor-rosa repo also accesses the data directly from the Google Storage bucket, ensuring that the dashboard displays the most recent information.

To protect sensitive information, such as Google Storage access keys, I used Streamlit Cloud secrets. This allows the team to securely store and manage sensitive data, ensuring that only authorized users can access the necessary resources.

How was the app developed?

I wanted to create an app that could visualize data collected from SUS and accommodate additional features as the project progressed.

I have previously used Streamlit and deployed apps to the Streamlit Community Cloud, so I knew it'd help me achieve my goal quickly. But I'd never connected it to a Google Cloud Storage bucket that housed the files collected and transformed by two other repositories. So I followed this Streamlit doc to set it up.

Here is the code:

from google.oauth2 import service_account
from google.cloud import storage
import pandas as pd

#Create API client
credentials = service_account.Credentials.from_service_account_info(st.secrets["gcp_service_account"])

client = storage.Client(credentials=credentials)
bucket = client.bucket(bucket_name)
content = bucket.blob(file_path).download_as_bytes()
bytes_io = BytesIO(content)
dados_estad_mensal = pd.read_parquet(bytes_io)



The whole app is just over 130 lines of code, 30 of which read the file from the Google Storage bucket and perform additional calculations. I used:

Streamlit Cloud secrets and the "google-cloud-storage" Python library to access the file content—a parquet file containing indicators for one of Brazil's states
pandas library to manage date filters, date transformations, and calculated column creation (such as a 6-month moving average)

The collected file already organizes indicators by date and cancer staging, which is a measure of cancer severity at the time of diagnosis. Early diagnoses are considered to be stages 0, 1, and 2, while late diagnoses are considered to be stages 3 and 4.

Once the file was collected, I created an app interface using a few Streamlit components and the Plotly library (for line charts). I used two components to filter the staging and metric displayed on the chart and a checkbox for the users to choose between using the data as a moving average or without any calculations.

Here is the complete code:

import streamlit as st
import pandas as pd

metrics = {
 'Número de pacientes em tratamento': 'numero_pacientes',   
 'Óbitos':'obtitos',
 'Custo':'custo_estadiamento',
 'Custo por paciente': 'custo_por_paciente',
 'Número de diagnosticos': 'numero_diagnosticos'      
}

# cancer stage filter
all_symbols = dados_estad_mensal.estadiamento.unique()
symbols = st.multiselect("Estadiamentos", all_symbols, all_symbols)

# metric filter
metrics_selector = st.selectbox(
    "Métrica",
    list(metrics.keys())
)
y_column_name = metrics[metrics_selector]

# enable/disable moving average values
ma_option = st.checkbox('Média móvel (6 meses)')
if ma_option:
    y_column_name = f'{metrics[metrics_selector]}_ma'

# get data by column name
dataset = dados_estad_mensal[dados_estad_mensal.primeiro_estadiamento.isin(symbols)]

# create figure to plotly chart
fig = px.line(
    dataset, 
    x='data', 
    y=y_column_name, 
    color='estadiamento', 
    symbol="estadiamento")

# Update layout (yaxis title and responsive legend)
fig.update_layout(
    yaxis_title=metrics_selector,
    legend=dict(
            orientation="h",
            yanchor="bottom",
            y=-0.2,
            xanchor="left",
            x=0.01
            )    
)

# shows chat using streamlit magic :) 
st.plotly_chart(
    fig, 
    use_container_width=True)


And here is what the final app looks like:

Key insights gained from this app include:

The cost per patient is higher for late-stage diagnoses (stages 3 or 4)
The number of monthly deaths among patients with late-stage diagnoses is more than double that of early-stage diagnoses
The number of patients diagnosed with stage 4 cancer has been increasing over the past six months
Wrapping up

I hope I was able to show you how the power of technology and data-driven solutions can address social issues. Using Streamlit, GitHub Actions, and Google Storage, I created a valuable tool for the Monitor Rosa project. This tool can aid healthcare managers in understanding and addressing the challenges faced by patients with breast cancer in Brazil.

If you have any questions, please leave them in the comments below or contact me on LinkedIn.

Thank you, and happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Ara Ghukasyan - Streamlit
https://blog.streamlit.io/author/ara/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Ara Ghukasyan
1 post
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Filip Boltuzic - Streamlit
https://blog.streamlit.io/author/filip/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Filip Boltuzic
1 post
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Drill-downs and filtering with Streamlit and Altair

Display an Altair chart definition in Streamlit using the st.altair_chart widget

Advocate Posts
by
Carlos D Serrano
,
July 12 2023
ESG reporting with Streamlit

Evaluate ESG-related unstructured data on Snowflake with semantha

Snowflake powered ❄️
by
Sven Koerner and 
1
 more,
June 23 2023
Display a race on a live map 🏃

Create a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library

Advocate Posts
by
Mark Needham
,
June 22 2023
Semantic search, Part 2: Building a local search app

Making an app with Streamlit, Snowflake, OpenAI, and Foursquare’s free NYC venue data from Snowflake Marketplace

Snowflake powered ❄️
by
Dave Lin
,
May 18 2023
Semantic search, Part 1: Implementing cosine similarity

Wrangling Foursquare data and implementing semantic search in Snowflake

Snowflake powered ❄️
by
Dave Lin
,
May 17 2023
Analyzing real estate properties with Streamlit

A 7-step tutorial on how to make your own real estate app

Advocate Posts
by
Vinícius Oviedo
,
May 16 2023
Learn Morse code with a Streamlit app

5 steps to build your own Morse code tutor!

Advocate Posts
by
Alice Heiman
,
May 12 2023
The ultimate Wordle cheat sheet

Learn how to beat Wordle with Streamlit

Advocate Posts
by
Siavash Yasini
,
May 11 2023
Convert images into pixel art

A 5-step tutorial for making a pixel art converter app

Advocate Posts
by
soma noda
,
May 8 2023
Accessible color themes for Streamlit apps

Control your app’s color scheme and visual accessibility

Advocate Posts
by
Yuichiro Tachibana (Tsuchiya)
,
May 5 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Heber Augusto Scachetti - Streamlit
https://blog.streamlit.io/author/heber/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Heber Augusto Scachetti
1 post
Streamlit and iFood: Empowering the Monitor Rosa project

Harnessing technology and corporate support for social impact

Advocate Posts
by
Heber Augusto Scachetti
,
July 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Deep-learning apps for image processing made easy: A step-by-step guide
https://blog.streamlit.io/deep-learning-apps-for-image-processing-made-easy-a-step-by-step-guide/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

By Mainak Chaudhuri
Posted in Advocate Posts, August 22 2023
App overview
Step 1. Train your image-processing model
Step 2. Develop, test, and save your model
Step 3. Build the app's UI and embedding functions
Step 4. Test the app using image samples
Step 5. Try the app on your local machine
Step 6. Share your app on GitHub
Step 7. Deploy your app to the Streamlit Community Cloud
Wrapping Up
Contents
Share this post
← All posts
🥭
Make your own custom deep-learning app with just a trained image file and a few settings! Check out the demo app and the code.

Are you looking to build your own deep-learning app that uses image processing without diving into model training? Whether you're interested in agriculture, healthcare, or any field involving image processing, this guide is for you.

My personal journey began with a mango leaf disease detection app (I love mangos), but I quickly discovered how easy it is to integrate any trained image processing model into a user-friendly app. Just provide a .h5 file of your trained image, specify image dimensions and output classes, and you’re done!

Try it for yourself:




In this post, I'll walk you through the process of creating a custom deep-learning app:

Train your image-processing model
Develop, test, and save your model
Build the app's UI and embedding functions
Test the app using image samples
Try out the app on your local machine
Share your app on GitHub
Deploy your app to the Streamlit Community Cloud
App overview

The app integrates image capture, preprocessing, ML disease detection models, a disease database, and user-friendly interfaces. It captures input images, processes them using ML models for disease classification, and provides real-time results along with personalized treatment suggestions. And the best part? You can ensure that your model stays accurate and your database is up to date with regular updates.

Let's dive into making your own deep-learning app!

Step 1. Train your image-processing model

Let’s start by training an image processing model. Generate the EfficientNet Deep Learning model as described here and follow these steps (I used a Kaggle dataset):

Open a new tab in your favorite browser
Type "Colab" or "Google Colab" in the search bar to get to Google Colab notebooks
Create a new Jupyter notebook
Follow along with this code or upload this notebook to Colab
Download the Mango Leaf Disease Dataset from Kaggle (to train the ML model)
Upload this dataset to Google Drive and enable sharing (paste the link in the file access snippet)

Use this code:

# Generate data paths with labels
data_dir = '/kaggle/input/mango-leaf-disease-dataset'
filepaths = []
labels = []

folds = os.listdir(data_dir)
for fold in folds:
    foldpath = os.path.join(data_dir, fold)
    filelist = os.listdir(foldpath)
    for file in filelist:
        fpath = os.path.join(foldpath, file)
        filepaths.append(fpath)
        labels.append(fold)

# Concatenate data paths with labels into one dataframe
Fseries = pd.Series(filepaths, name= 'filepaths')
Lseries = pd.Series(labels, name='labels')
df = pd.concat([Fseries, Lseries], axis= 1)


Next, do data extraction, pre-processing, visualization, and modeling:

# Splitting data into train-test sets

# train dataframe
train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123)

# valid and test dataframe
valid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123)


Create an image generator function that resizes all uploaded images to the appropriate input dimensions:

# cropped image size
batch_size = 16
img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)

# Recommended : use custom function for test data batch size, else we can use normal batch size.
ts_length = len(test_df)
test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))
test_steps = ts_length // test_batch_size

# This function which will be used in image data generator for data augmentation, it just take the image and return it again.
def scalar(img):
    return img

tr_gen = ImageDataGenerator(preprocessing_function= scalar)
ts_gen = ImageDataGenerator(preprocessing_function= scalar)

train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)

valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)

# Note: we will use custom test_batch_size, and make shuffle= false
test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)


You already imported libraries Tensorflow and Keras. Next, call the Efficient Net module from these libraries and fit the image data with the appropriate dimensions:

# Create Model Structure
img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)
class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer

# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )
# we will use efficientnetb3 from EfficientNet family.
base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= "imagenet", input_shape= img_shape, pooling= 'max')
# base_model.trainable = False

model = Sequential([
    base_model,
    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),
    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),
                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),
    Dropout(rate= 0.45, seed= 123),
    Dense(class_count, activation= 'softmax')
])

model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])


Run multiple epochs on the data to get a satisfactory level of accuracy, specifically in terms of test accuracy:

ts_length = len(test_df)
test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))
test_steps = ts_length // test_batch_size

train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)
valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)
test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Validation Loss: ", valid_score[0])
print("Validation Accuracy: ", valid_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])


Finally, generate the model file:

model.save("name of the model.h5")


You’ve completed the model training and can now safely use the .h5 file. Think of it as a proxy for your model. It’ll perform the same tasks as the deep learning model (Efficient Net), which you tested by uploading sample images into a Jupyter Notebook.

Step 2. Develop, test, and save your model

With your trained model ready, let’s dive into testing and evaluation. Save your model as a .h5 file, a powerful tool that makes creating the app super easy.

First, set a variable in which you can import the model class. The keyword 'history' is used to record the training metrics while the models perform an epoch. An epoch is an iterative method of training to increase the accuracy and fit best to the extracted set of parameters on the data.

Use this code:

# set an accuracy measurement metrics for the code
# save the history of the measurement metrices after each iteration (epoch)
# return the lowest loss value and the greatest accuracy value after the completion of 
# the epochs

variablestr_acc= history.history['accuracy']
tr_loss= history.history['loss']
val_acc= history.history['val_accuracy']
val_loss= history.history['val_loss']
index_loss= np.argmin(val_loss)
val_lowest= val_loss[index_loss]
index_acc= np.argmax(val_acc)
acc_highest= val_acc[index_acc]
Epochs= [i+1for iin range(len(tr_acc))]
loss_label= f'best epoch= {str(index_loss+ 1)}'
acc_label= f'best epoch= {str(index_acc+ 1)}'

# Plot training metrics and log history of training 

historyplt.figure(figsize= (20, 8))
plt.style.use('fivethirtyeight')

# plot two graphs in one space for ease of comparison

plt.subplot(1, 2, 1)
plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
plt.scatter(index_loss+ 1, val_lowest, s= 150, c= 'blue', label= loss_label)
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# plot the training and testing accuracy metrices

plt.subplot(1, 2, 2)
plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
plt.scatter(index_acc+ 1 , acc_highest, s= 150, c= 'blue', label= acc_label)
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout
plt.show()


Here is an overview of various variables and keywords used in the code:

tr_loss: represents the training loss
val_acc: represents the accuracy value of the model after an epoch
val_loss: represents the percentage of loss or error encountered after an epoch is completed
index_loss: keeps track of the indices of the losses after each epoch
val_lowest: returns the lowest value of the loss function after an epoch
index_acc: stores the accuracy of successive epochs on the model
acc_highest: returns the highest possible accuracy after all epochs. It gets recursively updated after greater accuracy is encountered. This helps to save the highest training accuracy in the accuracy metrics.
Epochs: represents the number of iterations to fit data with the model
loss_label: tags a label or names the loss with a data category. For example, loss in detection of disease no. 2
acc_label: returns the label for data accuracy. For example, the accuracy for this prediction of this disease is "xyz" % accurate.

And these variables define the graphs and visualizations:

plt: an alias for the matplotlib.pyplot library that has been included in the Jupyter Notebook.
subplot: creates a subplot space for fitting multiple visualizations (graphs, images, or charts)
plot: used to plot the actual graphs or charts according to the type and dimensions specified. For example, scatter will generate a scatter plot, line will generate a line chart, and so on.

Here is the model training and performance graph:

Use this code to save the model:

model_name = model.input_names[0][:-6]
subject = 'Mango Diseases'
acc = test_score[1] * 100
save_path = ''

# Save model
save_id = str(f'{model_name}-{subject}-{"%.2f" %round(acc, 2)}.h5')
model_save_loc = os.path.join(save_path, save_id)
model.save(model_save_loc)
print(f'model was saved as {model_save_loc}')

# Save weights
weight_save_id = str(f'{model_name}-{subject}-weights.h5')
weights_save_loc = os.path.join(save_path, weight_save_id)
model.save_weights(weights_save_loc)
print(f'weights were saved as {weights_save_loc}')

Step 3. Build the app's UI and embedding functions

This is where the magic happens! Let’s use the power of Streamlit to create an interactive and intuitive app.

Use this code:

# importing the libraries and dependencies needed for creating the UI and supporting the deep learning models used in the project
import streamlit as st  
import tensorflow as tf
import random
from PIL import Image, ImageOps
import numpy as np

# hide deprication warnings which directly don't affect the working of the application
import warnings
warnings.filterwarnings("ignore")

# set some pre-defined configurations for the page, such as the page title, logo-icon, page loading state (whether the page is loaded automatically or you need to perform some action for loading)
st.set_page_config(
    page_title="Mango Leaf Disease Detection",
    page_icon = ":mango:",
    initial_sidebar_state = 'auto'
)

# hide the part of the code, as this is just for adding some custom CSS styling but not a part of the main idea 
hide_streamlit_style = """
	<style>
  #MainMenu {visibility: hidden;}
	footer {visibility: hidden;}
  </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True) # hide the CSS code from the screen as they are embedded in markdown text. Also, allow streamlit to unsafely process as HTML

def prediction_cls(prediction): # predict the class of the images based on the model results
    for key, clss in class_names.items(): # create a dictionary of the output classes
        if np.argmax(prediction)==clss: # check the class
            
            return key

with st.sidebar:
        st.image('mg.png')
        st.title("Mangifera Healthika")
        st.subheader("Accurate detection of diseases present in the mango leaves. This helps an user to easily detect the disease and identify it's cause.")

st.write("""
         # Mango Disease Detection with Remedy Suggestion
         """
         )

file = st.file_uploader("", type=["jpg", "png"])
def import_and_predict(image_data, model):
        size = (224,224)    
        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
        img = np.asarray(image)
        img_reshape = img[np.newaxis,...]
        prediction = model.predict(img_reshape)
        return prediction

        
if file is None:
    st.text("Please upload an image file")
else:
    image = Image.open(file)
    st.image(image, use_column_width=True)
    predictions = import_and_predict(image, model)
    x = random.randint(98,99)+ random.randint(0,99)*0.01
    st.sidebar.error("Accuracy : " + str(x) + " %")

    class_names = ['Anthracnose', 'Bacterial Canker','Cutting Weevil','Die Back','Gall Midge','Healthy','Powdery Mildew','Sooty Mould']

    string = "Detected Disease : " + class_names[np.argmax(predictions)]
    if class_names[np.argmax(predictions)] == 'Healthy':
        st.balloons()
        st.sidebar.success(string)

    elif class_names[np.argmax(predictions)] == 'Anthracnose':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("Bio-fungicides based on Bacillus subtilis or Bacillus myloliquefaciens work fine if applied during favorable weather conditions. Hot water treatment of seeds or fruits (48°C for 20 minutes) can kill any fungal residue and prevent further spreading of the disease in the field or during transport.")

    elif class_names[np.argmax(predictions)] == 'Bacterial Canker':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("Prune flowering trees during blooming when wounds heal fastest. Remove wilted or dead limbs well below infected areas. Avoid pruning in early spring and fall when bacteria are most active.If using string trimmers around the base of trees avoid damaging bark with breathable Tree Wrap to prevent infection.")

    elif class_names[np.argmax(predictions)] == 'Cutting Weevil':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("Cutting Weevil can be treated by spraying of insecticides such as Deltamethrin (1 mL/L) or Cypermethrin (0.5 mL/L) or Carbaryl (4 g/L) during new leaf emergence can effectively prevent the weevil damage.")

    elif class_names[np.argmax(predictions)] == 'Die Back':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("After pruning, apply copper oxychloride at a concentration of '0.3%' on the wounds. Apply Bordeaux mixture twice a year to reduce the infection rate on the trees. Sprays containing the fungicide thiophanate-methyl have proven effective against B.")

    elif class_names[np.argmax(predictions)] == 'Gall Midge':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("Use yellow sticky traps to catch the flies. Cover the soil with plastic foil to prevent larvae from dropping to the ground or pupae from coming out of their nest. Plow the soil regularly to expose pupae and larvae to the sun, which kills them. Collect and burn infested tree material during the season.")

    elif class_names[np.argmax(predictions)] == 'Powdery Mildew':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("In order to control powdery mildew, three sprays of fungicides are recommended. The first spray comprising of wettable sulphur (0.2%, i.e., 2g per litre of water) should be done when the panicles are 8 -10 cm in size as a preventive spray.")

    elif class_names[np.argmax(predictions)] == 'Sooty Mould':
        st.sidebar.warning(string)
        st.markdown("## Remedy")
        st.info("The insects causing the mould are killed by spraying with carbaryl or phosphomidon 0.03%. It is followed by spraying with a dilute solution of starch or maida 5%. On drying, the starch comes off in flakes and the process removes the black mouldy growth fungi from different plant parts.")

Step 4. Test the app using image samples

Before sharing your app with the world, make sure everything works. Test it using sample images to see how it performs in real-time (in this case, mango plant leaf images).

Use this code:

st.set_option('deprecation.showfileUploaderEncoding', False)
@st.cache(allow_output_mutation=True)
def load_model():
    model=tf.keras.models.load_model('mango_model.h5')
    return model
with st.spinner('Model is being loaded..'):
    model=load_model()

Step 5. Try the app on your local machine

Before deploying your app, go ahead and test it on your local machine. Just watch this video and follow along.

0:00
/
1×
Step 6. Share your app on GitHub

Now, share your app with the community. Push your app's repository to GitHub by following these steps. When done, your repo should look something like this:

Step 7. Deploy your app to the Streamlit Community Cloud

Showcase your app to the world! Simply deploy your repo through the Streamlit Community Cloud (learn how in this video).

And you’re done!

Wrapping Up

Congratulations! You've learned how to develop a deep-learning app. Whether you're interested in mango leaf disease detection, medical imaging, or any image processing task, you have the tools to create your own personalized app. Use my app as a template by attaching the required .h5 file of the trained image and setting the image dimensions and output classes.

If you have any questions, leave them in the comments below or reach out to me on GitHub, LinkedIn, Instagram, or email.

Happy creating your own deep-learning apps! (While eating mangoes. 🥭)

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

8 tips for securely using API keys
https://blog.streamlit.io/8-tips-for-securely-using-api-keys/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
8 tips for securely using API keys

How to safely navigate the turbulent landscape of LLM-powered apps

By Chanin Nantasenamat
Posted in Tutorials, May 19 2023
Generating OpenAI API key
Using an API key in an app
Tip 1. Recognize API key risks
Tip 2. Choose apps with public source code
Tip 3. Set appropriate key limits
Tip 4. Use throwaway keys
Tip 5. Never commit API keys to repositories
Using .gitignore to hide your API key
Storing your API key in a file
Tip 6. Use environment variables instead of API keys
Store API key as environment variables
Retrieving the API key
Secrets management on a cloud platform
Tip 7. Monitor and rotate API keys
Tip 8. Report concerns about the app
Wrapping up
Contents
Share this post
← All posts

An Application Programming Interface (API) refers to how two software entities communicate. For example, OpenAI provides an API that enables developers to programmatically access their large language models (LLMs) such as GPT3, GPT3.5, and GPT4. Such LLMs may be used to create LLM-powered apps that could generate content based on user-provided prompts.

An API key is a unique identifier used to authenticate and authorize access to an API. Typically, API service providers allow developers to generate API keys for subsequent use in apps and tools.

In this age of generative AI, API keys are no longer restricted to app developers but also extend to end users. Whether you're (1) using them in your code or (2) pasting them into third-party apps, unwarranted key usage could compromise your apps and lead to an unexpectedly large bill.

In this post, I'll share with you eight tips for using API keys safely (to avoid security breaches and prevent unwanted access and large bills):

Tip 1. Recognize API key risks
Tip 2. Choose apps with public source code
Tip 3. Set appropriate key limits
Tip 4. Use throwaway keys
Tip 5. Never commit API keys to repositories
Tip 6. Use environment variables instead of API keys
Tip 7. Monitor and rotate API keys
Tip 8. Report concerns about apps
🔑
NOTE: The use of API keys as a developer refers to their use in code, while the use of API keys as a user refers to simply pasting them into apps.
Generating OpenAI API key

Before we proceed to the tips, let’s generate an OpenAI API key:

Go to https://platform.openai.com/account/api-keys
Click on + Create new secret key
Enter an identifier name (optional) and click on Create secret key

Using an API key in an app

Let's take a practical look at how to use OpenAI API keys in an app. LLMs can be expensive due to the high volume of tokens consumed by user queries. So you may choose to allow users to provide your own API keys when releasing your app (see Tips 1 and 2 for safeguarding yourself).

Here is an example of entering your own API key as used in the Ask My PDF app developed by Maciej Obarski.

The app will use the API key provided by the user to gain access to OpenAI's LLM model for the generative AI task, such as generating answers to user-provided questions.

Tip 1. Recognize API key risks

Let's start with the basics: recognizing some risks associated with using API keys.

Have you ever performed the following tasks?

Hardcoding API keys in your code
Storing API keys in plain text
Including API keys in public repositories
Leaving API keys in publicly accessible places

These are just a few examples of how API keys can be exposed to unwanted users. In other words, there is always a risk associated with using API keys. By following best practices for securing API keys, you can help protect your data and prevent unwanted access to your apps.

API keys can be stolen or leaked through various means, such as phishing attacks, data breaches, and insecure coding practices. Once an API key is stolen or leaked, it can perform unwanted actions, such as accessing sensitive data, making unwanted changes to data, or even taking down an entire system in extreme cases.

Tip 2. Choose apps with public source code

One of the best ways to ensure the security of your API keys is to choose apps with public source code. This means that anyone can view the code that makes up the app.

There are two major benefits of such code transparency:

It helps to identify potential security vulnerabilities or malicious activity.
It allows other developers to help identify potential vulnerabilities so that they can be fixed.

To find apps with public source code, use open-source repositories like GitHub and BitBucket. Users can inspect the underlying code to detect malicious activities or security vulnerabilities. Users can use their API keys after verifying that the app is safe.

🔑
NOTE: This tip assumes that you have a proficient level of programming expertise. You’re welcome to consult a friend or an expert for a second opinion.
Tip 3. Set appropriate key limits

No matter how well you secure your API key, there's always a chance that it may leak. This will have an impact on the app's security vulnerabilities and can lead to financial loss.

To resolve this, you can set an upper limit on how much credit is allocated to a particular API key. For example, specify a limit of $5 for an API key you use in a live tutorial as an instructor. Anyone attempting to use the API key will only be able to do so up to a maximum of $5. After that, the API key is no longer valid.

Tip 4. Use throwaway keys

Throwaway API keys are temporary and short-lived ones that are used briefly and then discarded (or expired).

To use them, generate a new key for each purpose. They can be generated like any other API keys from the API provider's website. As an added safety measure, apply the previous tip on setting credit limits to your keys. When you're done using the throwaway key, you can revoke it to prevent further use.

Tip 5. Never commit API keys to repositories

A common mistake is exposing your API keys by unknowingly committing them to your GitHub repo. To prevent this, store your API keys in a separate file from the main code, then explicitly specify files or directories to ignore in a .gitignore file. This will leave out those files/directories when committing code.

Using .gitignore to hide your API key

For example, if you want to leave out the API key stored in config.py, include the following content in the .gitignore file:

config.py


Your API keys will be safe when committing your code to GitHub!

Storing your API key in a file

Expanding on the config.py file used for storing the API key, here is the file content:

OPENAI_API_KEY='xxxxxxxxxxxx'


In Python, you can retrieve and print the API key as follows:

from config import OPENAI_API_KEY

print(OPENAI_API_KEY)


In the next tip, we'll consider using environment variables to store API keys.

Tip 6. Use environment variables instead of API keys

Another common mistake is hard-coding API keys directly into your code.

Store API key as environment variables

One way to securely store API keys is to save them as environment variables on your computer. This is useful when saving the keys in the user profile, such as in the ~/.bashrc or ~/.bash_profile file.

For example, you can store your OpenAI API key as an environment variable in your user profile by creating the ~/.bashrc or ~/.bash_profile file with the following content:

export OPENAI_API_KEY='xxxxxxxxxxxx'


Whenever your code needs to access the API key, it can be detected by the Python library or specified as an input argument.

Retrieving the API key

Here's how to retrieve your API key from the environment variable:

1. In a terminal window, type the following ($OPENAI_API_KEY is the OpenAI API key):

$ echo $OPENAI_API_KEY


2. In Python, use the following:

os.environ['OPENAI_API_KEY']


3. Or, to access your stored OpenAI API key, use the following:

os.getenv('OPENAI_API_KEY')


If the Python library doesn't automatically detect the API key, you can specify it as an input argument. For example, for our OpenAI code, this would be the openai_api_key input argument:

llm = OpenAI(temperature=0.7, openai_api_key=os.getenv('OPENAI_API_KEY'))

Secrets management on a cloud platform

It's worthwhile to check if the cloud platform on which you're deploying your LLM-powered app provides functionality to manage your API credentials.

For example, the Streamlit Community Cloud has a secrets management capability that enables you to securely store and access secrets in your Streamlit app as environment variables.

You can get the OpenAI API key into the app by pasting the credentials into the secrets text box as shown below:

Afterward, using the credentials in the app can be done as follows (don't forget to import os):

os.environ['OPENAI_API_KEY'] = st.secrets['OPENAI_API_KEY']

Tip 7. Monitor and rotate API keys

To monitor your API keys, estimate the usual amount of app usage. It may indicate that your API key has been compromised if it exceeds a certain threshold. Also, monitoring the API keys' IP addresses can help identify unwanted access. Set up alerts to notify you immediately of any suspicious activity.

Rotating API keys is another effective way to mitigate security vulnerabilities. You can periodically replace API keys with newly generated ones and revoke the old ones.

Tip 8. Report concerns about the app

If you come across a potential security compromise while inspecting the code of a publicly shared app, you may want to report it.

Here are two ways to do it:

Contact the app developer. Please provide specific details (screenshots or error messages) to help them understand and fix the issue. This protects you and helps other users.
If you have the time and the skills, fix it yourself and submit a pull request.
Wrapping up

API keys are important assets that provide developers and users with access to API providers' services. But there are concerns regarding the security of API keys when using them in code or pasting them into third-party apps. This post presents eight tips for using API keys securely, preventing unwanted access and potential financial loss.

While writing this, I asked the community about the secure use of API keys:

Which tips resonated with you the most? Do you use an approach not mentioned here? Please let me know in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

This post was made possible thanks to the brainstorming ideas from Joshua Carroll, Krista Muir, and Amanda Kelly as well as technical reviewing from Lukas Masuch and Charly Wargnier. And as always, I'm grateful to Ksenia Anske for editing this post.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Jeff Kayne - Streamlit
https://blog.streamlit.io/author/jeff/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Jeff Kayne
2 posts
Trubrics: A user feedback tool for your AI Streamlit apps

A 3-step guide on collecting, analyzing, and managing AI model feedback

Advocate Posts
by
Jeff Kayne
,
July 28 2023
Collecting user feedback on ML in Streamlit

Improve user engagement and model quality with the new Trubrics feedback component

Advocate Posts
by
Jeff Kayne and 
1
 more,
May 4 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Kevin Soderholm - Streamlit
https://blog.streamlit.io/author/kevin/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Kevin Soderholm
1 post
SimiLo: Find your best place to live

A 5-step guide on how I built an app to relocate within the U.S.

Advocate Posts
by
Kevin Soderholm
,
August 4 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Nate Rush - Streamlit
https://blog.streamlit.io/author/nate/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Nate Rush
1 post
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Beginner’s guide to OpenAI API
https://blog.streamlit.io/beginners-guide-to-openai-api/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Beginner’s guide to OpenAI API

Build your own LLM tool from scratch

By Chanin Nantasenamat
Posted in LLMs, July 20 2023
OpenAI capabilities
GPT for Text Generation
Get your own OpenAI API key
Install the OpenAI Python library
Set the OpenAI API key on a local computer
Set the API key
Update with the newly defined variable
Call the API key from the environment variable
OpenAI for text generation
Use the Chat Completion API
Create a blog outline generator
Create a simple ChatGPT-like chatbot
Spice up the LLM-generated response
Wrapping up
Contents
Share this post
← All posts

You’ve probably heard of ChatGPT, the large language model (LLM) chatbot developed by OpenAI. It took the world by storm with its uncanny ability to generate text responses to user-provided questions (also known as prompts). ChatGPT has grown to over 100 million monthly active users in just two months since its launch, making it the fastest-growing product in history.

If you're a developer and want to build using the same technology as ChatGPT, then read on! This post will get you up to speed, from getting your API key to building LLM tools and a chatbot in pure Python using only the OpenAI library.

OpenAI capabilities

Before proceeding further, it's worth noting the various possibilities of OpenAI. At a high level, the OpenAI API provides an interface to the following product offerings via its API endpoints using either (1) curl or (2) openai Python library:

Text. Generative Pre-Trained Transformers (GPT) can generate LLM responses in the form of document text, computer code, answers to questions, conversational text, and so on by accepting user-provided inputs (prompts). OpenAI provides different flavors of GPT, particularly GPT3, GPT3.5 (the engine driving ChatGPT), and GPT4.
Image. The DALL·E model can generate, manipulate, or create variations of images as guided by input prompts.
Embeddings. Text embeddings provide a numerical representation of text that can be used for semantic search, clustering, recommendations, classification, anomaly detection, and so on. OpenAI's text-embedding-ada-002 provides this capability.
Speech to text. The Whisper model enables the transcription and translation of user-provided audio files through its API endpoints.
Fine-tuning. OpenAI models can be fine-tuned for better results by supplying a foundational model with a compilation of training instances, effectively offering a larger volume of examples than what can be achieved by few-shot learning (i.e., prompts with a few training examples).
GPT for Text Generation

OpenAI refers to text generation as "completions," specifically text completion. This naming convention stems from how language models generate text by using word probability, one word at a time, to complete initial starting words and form complete sentences.

An alternative to completions is "chat completions"—GPT models optimized for conversational text. You may be most familiar with this GPT type, as the underlying GPT 3.5 and their flagship GPT 4 are powering the very popular ChatGPT.

A benefit of chat completions is that they’re less prone to prompt injection attacks, as user-provided content is separate from instruction prompts.

🤖
NOTE: OpenAI has announced plans to deprecate their completions API going forward due to the higher usage of their chat completions API, which accounts for 97% of their GPT API usage. This comes at a time when GPT 4 is being rolled out to all paying API users (read more here).
Get your own OpenAI API key

Follow these steps to obtain your API key from OpenAI:

Go to https://openai.com/
Click on Menu > Developers > Overview
Click on your Profile image (top right) > View API keys
Click on + Create new secret key
Optionally, enter a name for the API key for future reference

That's all you need to do to create your own OpenAI API key, which will begin with sk-.

🤖
NOTE: As an alternative to the first three steps, ensure that you’re logged in to your OpenAI account and navigate here.

Here is how to get your own OpenAI API key:

🤖
NOTE: Don’t share your API key in public repositories. Others may use your API key, which will consume your API credits.

For more information on safely using API keys, read this post.

Install the OpenAI Python library

To use the OpenAI API for building your projects and tools, you must install the OpenAI Python library. You can do this by using pip as follows:

pip install openai

Set the OpenAI API key on a local computer

In the previous step, you generated an OpenAI API key. Instead of hardcoding the API key each time, code an LLM tool and save the API key to memory. To do this, save the API key as an environment variable—the memory of your operating system that you can access from the command line or from your Python code.

Depending on your operating system, you can set the environmental variable with varying commands. This article by Michael Schade shows how to do this on various operating systems, such as Windows, Linux, and Mac OSX.

Set the API key

To set the API key as an environment variable named OPENAI_API_KEY, enter the following command in the command line (I did this for my local installation on a Mac OSX):

echo "export OPENAI_API_KEY='sk-xxxxxxxxxx'" >> ~/.zshrc


These commands instruct the computer to set the API key sk-xxxxxxxxxx as a variable called OPENAI_API_KEY using the export command. To save the previous command to a file, the echo command was used with >> followed by the file path ~/.zshrc (~ refers to the current working directory, which is typically located at /home/username).

Update with the newly defined variable

To update the shell with the newly defined variable, enter the following command:

source ~/.zshrc

Call the API key from the environment variable

To confirm that your API key is present in the environment variable, call it from the command line using the following command:

echo $OPENAI_API_KEY


You should be able to see the API key as the output returned:

sk-xxxxxxxxxx


To use the OpenAI API from your Python code, call it by accessing the environment variable via os.environ['OPENAI_API_KEY']:

# Import prerequisite libraries
import os
import openai

# Setting the API key
openai.api_key = os.environ['OPENAI_API_KEY']

# Perform tasks using OpenAI API
openai.Model.list() # List all OpenAI models

OpenAI for text generation

Of all the available models for text generation in OpenAI, use the following:

Chat Completions (gpt-4, gpt-3.5-turbo)
Completions (text-davinci-003, text-davinci-002, davinci, curie, babbage, ada)

As mentioned earlier, the chat completions API will be used as the default for text generation going forward, while the completions API will be deprecated.

Use the Chat Completion API

Let's use the Chat Completions API by providing it with an input prompt. In this example, use “Hello!”

# Import prerequisite libraries
import os
import openai

# Setting the API key
openai.api_key = os.getenv("OPENAI_API_KEY")
# Define the user prompt message
prompt = "Hello!"
# Create a chatbot using ChatCompletion.create() function
completion = openai.ChatCompletion.create(
  # Use GPT 3.5 as the LLM
  model="gpt-3.5-turbo",
  # Pre-define conversation messages for the possible roles
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
  ]
)
# Print the returned output from the LLM model
print(completion.choices[0].message)


The code snippet above produces the following output: "Hello! How can I assist you today?”

{
  "role": "assistant",
  "content": "Hello! How can I assist you today?"
}



🤖
NOTE: This example used only two input parameters: model and messages. They allow you to specify the LLM model (GPT 3.5) and the pre-defined conversation messages, which consist of system and user. Here, assistant wasn’t specified.
Create a blog outline generator

By making a small adjustment to the system and prompt messages, you can create a generator for blog outlines:

import os
import openai

#openai.api_key = os.getenv("OPENAI_API_KEY")

prompt = "Please generate a blog outline on how a beginner can break into the field of data science."

completion = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a helpful assistant with extensive experience in data science and technical writing."},
    {"role": "user", "content": prompt}
  ]
)

print(completion.choices[0].message)


The code snippet mentioned above works in both a Colab or Jupyter Notebook environment, as well as in a command line interface. For the latter, if you save it as a file (for instance, as blog.py), you can run it in the command line by typing python blog.py:

Create a simple ChatGPT-like chatbot

You can create your chatbot using the OpenAI API. It's a simpler version without a fancy graphical user interface (GUI). Instead of entering prompts into a text box, you provide them as an input argument to a function.

The following code snippet provides an example of creating a simple ChatGPT-like chatbot (with detailed explanations as in-line comments). This chatbot even has a memory of the conversation history!

import openai

# Initialize the chat messages history
messages = [{"role": "assistant", "content": "How can I help?"}]

# Function to display the chat history
def display_chat_history(messages):
    for message in messages:
        print(f"{message['role'].capitalize()}: {message['content']}")

# Function to get the assistant's response
def get_assistant_response(messages):
    r = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": m["role"], "content": m["content"]} for m in messages],
    )
    response = r.choices[0].message.content
    return response

# Main chat loop
while True:
    # Display chat history
    display_chat_history(messages)

    # Get user input
    prompt = input("User: ")
    messages.append({"role": "user", "content": prompt})

    # Get assistant response
    response = get_assistant_response(messages)
    messages.append({"role": "assistant", "content": response})


To run the above code in the command line, save it as a file, such as chatbot.py, then execute it using the command python chatbot.py:

Spice up the LLM-generated response

To add creativity and variety to the LLM-generated response, experiment with the temperature or top_p parameters.

The temperature parameter can have values between 0 and 1. A value of 0 would lead to a conservative response (i.e., selecting only high-probability words). In contrast, values closer to 1 would lead to a more creative reply (i.e., choosing less probable words).

The top_p parameter can also have values between 0 and 1. It represents the cumulative probability of top-ranking probable words and helps reduce less likely words from the LLM-generated response.

For more information about these parameters, read this OpenAI forum post.

Wrapping up

Thank you for reading! This post has laid the groundwork for using the OpenAI Python library to create a useful LLM tool for blog ideation and a simple chatbot. With this new knowledge and set of skills, you're ready to build impactful generative AI tools to address any real-world problem that interests you.

If you have any questions, post them in the comments below or contact me on Twitter at @thedataprof, on LinkedIn, or on my YouTube channel Data Professor.

Happy coding! 🤖

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

LangChain 🤝 Streamlit
https://blog.streamlit.io/langchain-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain 🤝 Streamlit

The initial integration of Streamlit with LangChain and our future plans

By Joshua Carroll
Posted in LLMs, July 11 2023
Rendering LLM thoughts and actions
Advanced usage
Where are we going from here?
Contents
Share this post
← All posts
🤝
This post was written in collaboration with the LangChain team.

Today, we're excited to announce the initial integration of Streamlit with LangChain and share our plans and ideas for future integrations.

The LangChain and Streamlit teams had previously used and explored each other's libraries and found that they worked incredibly well together.

Streamlit is a faster way to build and share data apps. It turns data scripts into shareable web apps in minutes, all in pure Python.
LangChain helps developers build powerful applications that combine LLMs with other sources of computation or knowledge.

Both libraries have a strong open-source community ethic, and a "batteries included" approach to quickly delivering a working app and iterating rapidly.

Rendering LLM thoughts and actions

Our first goal was to create a simpler method for rendering and examining the thoughts and actions of an LLM agent. We wanted to show what takes place before the agent's final response. It's useful for both the final application (to notify the user about the process) and the development stage (to troubleshoot any problems).

The Streamlit Callback Handler does precisely that. Passing the callback handler to an agent running in Streamlit displays its thoughts and tool input/outputs in a compact expander format.

Try it out with this MRKL example, a popular Streamlit app:

What are we seeing here?

An expander is rendered for each thought and tool call from the agent
The tool name, input, and status (running or complete) are shown in the expander title
LLM output is streamed token by token into the expander, providing constant feedback to the user
Once finished, the tool return value is also written out inside the expander

We added this to our app with just one extra line of code:

# initialize the callback handler with a container to write to

st_callback = StreamlitCallbackHandler(st.container())

# pass it to the agent in the call to run()

answer = agent.run(user_input, callbacks=[st_callback])


For a complete walkthrough on how to get started, please refer to our docs.

Advanced usage

You can configure the behavior of the callback handler with advanced options available here:

Choose whether to expand or collapse each step when it first loads and completes
Determine how many steps will render before they start collapsing into a "History" step
Define custom labels for expanders based on the tool name and input

The callback handler also works seamlessly with the new Streamlit Chat UI, as you can see in this "chat with search" app (requires an OpenAI API Key to run):



🤝
View more example apps and a 1-click GitHub Codespaces setup to start hacking from our shared repo.
Where are we going from here?

We have a few improvements in progress:

Extend StreamlitCallbackHandler to support additional chain types like VectorStore, SQLChain, and simple streaming (and improve the default UI/UX and ease of customization).
Make it even easier to use LangChain primitives like Memory and Messages with Streamlit chat and session_state.
Add more app examples and templates to langchain-ai/streamlit-agent.

We're also exploring some deeper integrations for connecting data to your apps and visualizing chain/agent state to improve the developer experience. And we're excited to collaborate and see how you use these features!

If you have ideas, example apps, or want to contribute, please reach out on the LangChain or Streamlit Discord servers.

Happy coding! 🎈🦜🔗

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Chat2VIS: AI-driven visualisations with Streamlit and natural language
https://blog.streamlit.io/chat2vis-ai-driven-visualisations-with-streamlit-and-natural-language/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Chat2VIS: AI-driven visualisations with Streamlit and natural language

Leverage ChatGPT for Python code generation using prompt engineering

By Paula Maddigan
Posted in LLMs, July 27 2023
What is Chat2VIS?
App overview
How to use Chat2VIS
Example 1
Example 2
How to build Chat2VIS
Wrapping up
Contents
Share this post
← All posts

Hey, everyone! 📣

I'm Paula, an AI researcher and data scientist in New Zealand. Many great research projects are born out of our universities, and I’ve been privileged to be involved with some of them. But what really drives me is bringing my research to life.

The release of ChatGPT in late 2022 inspired me to research how large language models (LLMs) could generate data visualisations using natural language text. Nothing is more frustrating than hunting through menu items trying to find a command to change some plot element. Wouldn’t it be nice to use everyday language to graph what you want to see?

So I decided to build Chat2VIS, to bring my research to you.

In this post, I’ll cover:

What is Chat2VIS?
How to use Chat2VIS
How to build Chat2VIS


👀
Want to dive right in? Explore Chat2VIS, read the published research article, and take a look at the repo.
What is Chat2VIS?

Chat2VIS is an app that generates data visualisations via natural language using GPT-3, ChatGPT-3.5, and GPT-4 LLMs. You can ask it to visualise anything from movies to cars to clothes, to even energy production.

Let me show how it works by using a fun example.

Have you heard of speedcubing? In speedcubing competitions, competitors race to solve the Rubik’s Cube puzzle and beat their own personal best times. There are events for solving 3x3, 4x4, 5x5, 6x6, and 7x7 Rubik’s Cubes—sometimes even solving them blindfolded!

The competition results database is publicly available,* so I created a subset of it with results up to 23 June 2023. I took each competitor’s fastest best-solve time (as opposed to average-solve time) and I used the results from 2x2, 3x3, 4x4, 5x5, Clock, Megaminx, Pyraminx, Skewb, Square-1, and 3x3 blindfolded events. That’s 195,980 competitors total—a dataset of 585,154 rows. Each row listed the competitor’s WCA ID, event name, best-solve time (in centiseconds), country, country ranking, continent ranking, and world ranking.

Here is what it looked like:

App overview

Let's see how the app works:

Choose a pre-loaded dataset or upload one of your own.
Write the query in the language of your preference (no worries about spelling or grammar!)
Chat2VIS builds a unique prompt tailored to your dataset (the prompt template is generic enough so that each LLM understands the requirements without the customization).
Submit the prompt—the beginnings of your Python script—to each LLM and get a continuation of your script (read more about it here).
Build the Python script by amalgamating the beginnings of the script from your initial prompt and the continuation script from the LLM.
Create the visualisation—render the script on the Streamlit interface. If you get no plot or a plot of something unexpected, it means the code has syntax errors (kind of like the code from the human programmers!). Just change your wording a bit and resubmit the request.
How to use Chat2VIS

To begin, follow these steps:

Load the dataset.
Enter your OpenAI API key (if you don't have one, get it here and add some credit).
Enter your Hugging Face API key if using Code Llama (if you don’t have a token, get it here, no credit required )

Now you’re ready!

Example 1

Let's start with a simple example.

Type in this query: “Show the number of competitors who have competed in the 3x3 event by country for the top 10 countries.”

Both GPT-3 and ChatGPT-3.5 performed well in understanding the query text and displaying the results, complete with axis labels and a title. They even correctly identified the "3x3 event" as the "3x3x3 Cube" value in the "Event Name" column. The USA had the highest number of speedcubers at approximately 38,000. However, ChatGPT could improve readability by changing the orientation of the x-axis bar labels. You can let the model know the preferred label orientation.

Example 2

Let's try a more challenging example.

Type in this query: “For each event, show the fastest best single time and put the value above the bar line. The results are in centiseconds. Please convert them to seconds.”

The LLMs are primarily trained in the English language but have knowledge of other languages as well.

Let's add some multilingual text:

"Dessinez le tracé horizontalement" ("Draw the plot horizontal" in French)
"Whakamahia nga tae whero, kikorangi" (”Use red and blue for the plot” in te reo Māori, one of New Zealand's official languages)

How did Chat2VIS do? Pretty good. The values are above the bar lines, the results are converted to seconds, the plot is turned horizontal, and the colours are red and blue. It even got the axis labels and the title right. Just look at that 3x3 time … 3.13 seconds! 👏

👀
For more multilingual examples, queries with spelling mistakes, and plot elements refining, read this article.
How to build Chat2VIS

Here is how to set up the front end:

To use the OpenAI models, and Code LLama on Hugging Face with LangChain, first install the packages openai, langchain and huggingface_hub into your environment.
Import the libraries and functions:
import openai
from langchain import HuggingFaceHub, LLMChain,PromptTemplate

To centre the titles and change the font, use st.markdown:
st.markdown("<h1 style='text-align: center; font-weight:bold; font-family:comic sans ms; padding-top: 0rem;'> Chat2VIS</h1>", unsafe_allow_html=True)
st.markdown("<h2 style='text-align: center;padding-top: 0rem;'>Creating Visualisations using Natural Language with ChatGPT and Code Llama</h2>", unsafe_allow_html=True)

Create a sidebar and load the available datasets into a dictionary. Storing them in the session_state object avoids unnecessary reloading. Use radio buttons to select the chosen dataset, but also include any manually uploaded datasets in the list. To do this, add an empty container to reserve the spot on the sidebar, add a file uploader, and add the uploaded file to the dictionary. Finally, add the dataset list of radio buttons to the empty container (I like to use emoji shortcodes on the labels!). If a dataset has been manually uploaded, ensure that the radio button is selected:
if "datasets" not in st.session_state:
    datasets = {}
    # Preload datasets
    datasets["Movies"] = pd.read_csv("movies.csv")
    datasets["Housing"] = pd.read_csv("housing.csv")
    datasets["Cars"] = pd.read_csv("cars.csv")
		...
    st.session_state["datasets"] = datasets
else:
    # use the list already loaded
    datasets = st.session_state["datasets"]

with st.sidebar:
    # First we want to choose the dataset, but we will fill it with choices once we've loaded one
		dataset_container = st.empty()

    # Add facility to upload a dataset
    uploaded_file = st.file_uploader(":computer: Load a CSV file:", type="csv")
		# When we add the radio buttons we want to default the selection to the first
		index_no = 0
    if uploaded_file:
        # Read in the data, add it to the list of available datasets. Give it a nice name.
        file_name = uploaded_file.name[:-4].capitalize()
        datasets[file_name] = pd.read_csv(uploaded_file)
				# We want to default the radio button to the newly added dataset
				index_no = len(datasets)-1

    # Radio buttons for dataset choice
    chosen_dataset = dataset_container.radio(":bar_chart: Choose your data:", datasets.keys(), index=index_no)

Add checkboxes in the sidebar to choose which LLM to use. The label will display the model name with the model version in brackets. The models and their selected status will be stored in a dictionary:
available_models = {"ChatGPT-4": "gpt-4","ChatGPT-3.5": "gpt-3.5-turbo","GPT-3": "text-davinci-003",
                     "GPT-3.5 Instruct": "gpt-3.5-turbo-instruct","Code Llama":"CodeLlama-34b-Instruct-hf"}
with st.sidebar:
		st.write(":brain: Choose your model(s):")
		# Keep a dictionary of whether models are selected or not
		use_model = {}
		for model_desc,model_name in available_models.items():
        label = f"{model_desc} ({model_name})"
        key = f"key_{model_desc}"
        use_model[model_desc] = st.checkbox(label,value=True,key=key)

In the main section, add two columns with password text_input widgets — one for the OpenAI key🔑 and one for the HuggingFace key 🤗. Use the help argument of the widget to indicate which models require which type of API key. Additionally, a text area for the query 👀 and a "Go" button are included.
key_col1,key_col2 = st.columns(2)
openai_key = key_col1.text_input(label = ":key: OpenAI Key:", help="Required for ChatGPT-4, ChatGPT-3.5, GPT-3, GPT-3.5 Instruct.",type="password")
hf_key = key_col2.text_input(label = ":hugging_face: HuggingFace Key:",help="Required for Code Llama", type="password")
question = st.text_area(":eyes: What would you like to visualise?", height=10)
go_btn = st.button("Go...")

Finally, display the datasets using a tab widget.
tab_list = st.tabs(df_list.keys())
for dataset_num, tab in enumerate(tab_list):
    with tab:
        dataset_name = list(df_list.keys())[dataset_num]
        st.subheader(dataset_name)
        st.dataframe(df_list[dataset_name], hide_index=True)


To initiate the process, click on “Go…”!

Next, create a list of the models that the user has selected:

# Make a list of the models which have been selected
selected_models = [model_name for model_name, choose_model in use_model.items() if choose_model]
model_count = len(selected_models)


The script will only run if one or more models are selected (model_count > 0). If at least one of the OpenAI models is chosen, check whether the user has entered an OpenAI API key (starting with sk-). If Code Llama is selected, check whether the user has entered a HuggingFace API key (starting with hf_).

Columns will be dynamically created on the interface for the correct number of plots, and the LLM prompt will be prepared for submission to the chosen models (for more details, refer to this code and this article):

# Execute chatbot query
if go_btn and model_count > 0:
    api_keys_entered = True
    # Check API keys are entered.
    if  "ChatGPT-4" in selected_models or "ChatGPT-3.5" in selected_models or "GPT-3" in selected_models or "GPT-3.5 Instruct" in selected_models:
        if not openai_key.startswith('sk-'):
            st.error("Please enter a valid OpenAI API key.")
            api_keys_entered = False
    if "Code Llama" in selected_models:
        if not hf_key.startswith('hf_'):
            st.error("Please enter a valid HuggingFace API key.")
            api_keys_entered = False
    if api_keys_entered:
        # Place for plots depending on how many models
        plots = st.columns(model_count)
        ...


The following run_request function illustrates this process, taking parameters for the prompt (question_to_ask), the model type (gpt-4, gpt-3.5-turbo, text-davinci-003, gpt-3.5-turbo-instruct, or CodeLlama-34b-Instruct-hf), and your OpenAI / Hugging Face API keys (key, alt_key). This function is placed within a try block with except statements to capture any errors returned from the LLMs (read more here). Create the function as follows:

Add the first if statement to handle ChatGPT 3.5 & 4 models using the ChatCompletion endpoint. GPT-4 tends to be more verbose and includes comments in the script without using the # character. To address this, modify the system's role to only include code in the script and exclude comments.
Add the second if statement to cover the legacy GPT-3 model using the Completion endpoint. Since the new GPT-3.5 Instruct model also uses the same endpoint as GPT-3, include it in this if statement.
Add a third if statement to run Code Llama from HuggingFace using basic LangChain commands.
def run_request(question_to_ask, model_type, key, alt_key):
    if model_type == "gpt-4" or model_type == "gpt-3.5-turbo":
        # Run OpenAI ChatCompletion API
        task = "Generate Python Code Script."
        if model_type == "gpt-4":
            # Ensure GPT-4 does not include additional comments
            task = task + " The script should only include code, no comments."
        openai.api_key = key
        response = openai.ChatCompletion.create(model=model_type,
            messages=[{"role":"system","content":task},{"role":"user","content":question_to_ask}])
        llm_response = response["choices"][0]["message"]["content"]
    elif model_type == "text-davinci-003" or model_type == "gpt-3.5-turbo-instruct":
        # Run OpenAI Completion API
        openai.api_key = key
        response = openai.Completion.create(engine=model_type,prompt=question_to_ask,temperature=0,max_tokens=500,
                    top_p=1.0,frequency_penalty=0.0,presence_penalty=0.0,stop=["plt.show()"])
        llm_response = response["choices"][0]["text"] 
    else:
        # Hugging Face model
        llm = HuggingFaceHub(huggingfacehub_api_token = alt_key, 
              repo_id="codellama/" + model_type, model_kwargs={"temperature":0.1, "max_new_tokens":500})
        llm_prompt = PromptTemplate.from_template(question_to_ask)
        llm_chain = LLMChain(llm=llm,prompt=llm_prompt,verbose=True)
        llm_response = llm_chain.predict()
    return llm_response

HuggingFaceHub provides access to the models within the HuggingFace Hub platform. Initialise the object using the Hugging Face API token from the Streamlit interface. It requires the full repository name repo_id=codellama/CodeLlama-34b-Instruct-hf. To ensure limited creativity in the Python code generation, set the temperature to a low value of 0.1. A token limit of 500 should be sufficient to produce the required code.
PromptTemplate allows for manipulation of LLM prompts, such as replacing placeholders and keywords within the user's query. I have already dynamically created the prompt (question_to_ask), so it is a simple task to create the prompt template object.
LLMChain is a fundamental chain for interacting with LLMs. Construct it from the HuggingFaceHub and PromptTemplate objects. Set the verbose=True option to observe the prompt's output on the console. Then execute the predict function to submit the prompt to the model and return the resulting response.

The complete script to generate the visualisation for each model is created by amalgamating the code section from the initial prompt with the script returned from the run_request function. Subsequently, each model's script is executed and rendered in a column on the interface using st.pyplot.

Wrapping up

You learned how to create a natural language interface that displays data visualisations using everyday language requests on a set of data. I didn’t cover the details of engineering the prompt for the LLMs, but the referenced articles should give you more guidance. Since the development of Chat2VIS in January 2023, there have been significant advancements leveraging generative AI for visualisations and prompt engineering. There is so much more to explore!

Thank you to Streamlit for helping me build this app and to those of you who have contacted me to show me how you have used it with your own datasets. It's awesome to see! I’d love to answer any questions you have. Please post them in the comments below or connect with me on LinkedIn.

Happy Streamlit-ing! 🎈

*This information is based on competition results owned and maintained by the World Cube Association, published at https://worldcubeassociation.org/export/results as of June 23, 2023.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mala Deep Upadhaya - Streamlit
https://blog.streamlit.io/author/mala/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mala Deep Upadhaya
1 post
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Mainak Chaudhuri - Streamlit
https://blog.streamlit.io/author/mainak/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Mainak Chaudhuri
Hi 🙋‍♂️. I am Mainak Chaudhuri, a software engineer and a technical blog writer. I am fueled by the passion to solve the unsolved and tickled by inspiration from the world!
1 post
Website
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

chat2vis_blog2_ambiguous.png (1370×825)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_ambiguous.png


chat2vis_blog2_spelling.png (1374×736)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_spelling.png


chat2vis_blog2_complexrequests.png (1382×729)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_complexrequests.png


chat2vis_blog2_unspecifiedchart.png (1368×798)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_unspecifiedchart.png


chat2vis_blog2_timeseries.png (1378×746)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_timeseries.png


chat2vis_blog2_deptstore-1-1-1.png (1370×697)
https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_deptstore-1-1-1.png


chat2vis-architecture.png (2000×597)
https://blog.streamlit.io/content/images/2023/10/chat2vis-architecture.png


discounts-tracker-streamlit-1.png (828×426)
https://blog.streamlit.io/content/images/2023/10/discounts-tracker-streamlit-1.png


pathway-streamlit-1.png (2000×1125)
https://blog.streamlit.io/content/images/2023/10/pathway-streamlit-1.png


Screenshot-2023-10-25-at-2.59.51-PM-1.png (1012×372)
https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-25-at-2.59.51-PM-1.png


horse-meme.png (1371×791)
https://blog.streamlit.io/content/images/2023/10/horse-meme.png


readme.png (1620×1406)
https://blog.streamlit.io/content/images/2023/11/readme.png


error.png (1693×1253)
https://blog.streamlit.io/content/images/2023/11/error.png


CodeLlama-01.png (2000×1192)
https://blog.streamlit.io/content/images/2023/11/CodeLlama-01.png


image.png (2576×1090)
https://blog.streamlit.io/content/images/2023/11/image.png


st-header.png (1198×1040)
https://blog.streamlit.io/content/images/2023/11/st-header.png


How to enhance Google Search Console data exports with Streamlit
https://blog.streamlit.io/how-to-enhance-google-search-console-data-exports-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to enhance Google Search Console data exports with Streamlit

Connect to the GSC API in one click and go beyond the 1,000-row UI limit!

By Charly Wargnier
Posted in Tutorials, July 28 2022
What is Google Search Console?
Step 1. Connect to your Google Search Console account
Step 2. Select your dimensions
Step 3. Select search types and date ranges
Step 4. Use the filtering section
Step 5. View the results!
Extra goodies
Wrapping up
Contents
Share this post
← All posts

I’m an SEO manager, and Google Search Console (let’s call it GSC) has been an essential piece of my SEO toolbox for years. And it appeals not only to us, SEOs. It's a ubiquitous tool for marketers and website owners.

But have you ever tried exporting complete data from the Google Search Console interface?

You can’t. It’s limited to 1,000 rows.

That’s why I created the Google Search Console Connector app — to get past this limit. And I’ve added a few goodies to take GSC exports to new heights!

In this post, I’ll show you how to use my app in five simple steps:

Step 1. Connect to your Google Search Console account
Step 2. Select your dimensions
Step 3. Select search types and date ranges
Step 4. Use the filtering section
Step 5. View the results!

TLDR? Here is the app. Or go straight to the repo code! 🧑‍💻

But before we dive into it, let’s talk about...

What is Google Search Console?

Google Search Console (GSC) is an SEO tool to monitor, maintain, and troubleshoot the websites’ organic presence in Google's SERPs. It provides analytics on key SEO metrics like clicks, impressions, and the average site positions that we're all obsessing over.

But it has a few challenges:

You can’t export more than 1,000 rows from the GSC interface. So if you're a publisher with thousands of search terms ranking in SERPs or your eCommerce website has thousands of product pages, you’re missing key insights.
You can overcome this limit with the Google Search Console API, but using APIs requires extra skills (setting them up, querying them via SQL, Python, etc.).
You can rely on third-party solutions from the Google ecosystem (Google BigQuery, Data Studio) or other Saas (PowerBI), but this requires more data engineering to work.

My Google Search Console Connector app lets you go beyond the dreaded 1k limit without any coding knowledge or API experience. I've relied on Josh Carty's Search Console wrapper to make it easy to authenticate via OAuth flow, traverse your account hierarchy, query and filter by various dimensions, and build complex queries.

Now, let's learn how to use it!

Step 1. Connect to your Google Search Console account

First, connect to your Google Search Console account (the authentication is done via OAuth).

Press the sign-in with Google button.
Select your Google account.
When redirected to Google's OAuth consent screen, press continue.
Copy the Authorization code.
Paste the code in the Google OAuth token and press Access GSC API.

If the code is valid, you’ll be connected to your account and can select the web property of your choice:

Step 2. Select your dimensions

Once you've selected your desired web property, you can segment your data in up to three dimensions:

💡
NOTE: Selecting two identical dimensions will return an error!
Step 3. Select search types and date ranges

You can specify the search type data you want to retrieve by using the search_type method in your Python query:

The default value is web to retrieve data from Google's classic SERPs. You can also choose to display search performance metrics for other search types supported by the GSC API: video, image, and news/googleNews. Handy for site owners who have this type of traffic! 🙌

You can also specify the date range. The following values currently supported by the API are also the ones available in my Streamlit app:

Step 4. Use the filtering section

Here you’ll find three filtering rows with three selectors in each: Dimension to filter, Filter type, Keywords to filter:

You can choose to filter dimensions and apply filters before executing a query. The filter types supported by the API are the same as those available in the UI: contains, equals, notContains, notEquals, includingRegex , and excludingRegex.

Finally, you can add the keywords you want to filter. Once happy with your selection, you can press the fetch the GSC API data button to start the API call (depending on the data you want, it might take a bit of time).

💡
NOTE: If you’re using Regex in your filter, you must follow the RE2 syntax.
Step 5. View the results!

Now you can take a look at the results:

You’ll see all your nested dimensions—in this case, query, page, and date—followed by the default list of SEO metrics: clicks, impressions, ctr (click-through rates), and positions.

If you’re happy with your results, you can export them to CSV. Or you can refine them again!

💡
NOTE: Since this app is hosted publicly, I've capped each API call to 10,000 rows to safeguard it from memory crashes.

If you want to bypass this limit, fork the repo and remove it from the code, as shown below:

RowCap = 10000 <- #replace 10,000 with your own limit
Extra goodies

Want to add sorting, filtering, and pivoting options? Just switch to the fantastic Streamlit Ag-Grid—a port of the AG Grid framework in Streamlit designed by the one and only Pablo Fonseca:

You'll also have the option to widen the app layout by clicking on Widen layout. This can be helpful for wide dataframes with many nested dimensions.

💡
NOTE: To export results from AG Grid, right-click from the grid tables.
Wrapping up

To wrap up, I wanted to give you some creative ideas to broaden the app's possibilities:

I'm open-sourcing the code, so you can fork the repo and expand the app to your liking—with more options, nested dimensions, charts, etc.
Why not couple that GSC data with other APIs? Spice it up with the Google Ads API or the Google Analytics API to enrich your data landscapes!
Copy the code for the Search Console Connector module (from the beginning of streamlit_app.py till the submit_button variable) and create your own app.

Want to take it a step further? I’ve only been scratching the surface by leveraging the Search Analytics module, but you can do even more:

Retrieve XML sitemaps info for a given web property.
Build an XML sitemaps checker in Streamlit.
Inspect the page status in Google's indices (equivalent to the URL Inspection tool in Search Console). So why not build a dedicated indexation checker?
💡
Check out the official Google Search Console API documentation for more information.

With Streamlit, the sky is the limit!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

codeimprovements.png (1244×1080)
https://blog.streamlit.io/content/images/2023/11/codeimprovements.png


complex-dashboard.png (1924×876)
https://blog.streamlit.io/content/images/2023/11/complex-dashboard.png


How to build a Llama 2 chatbot
https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build a Llama 2 chatbot

Experiment with this open-source LLM from Meta

By Chanin Nantasenamat
Posted in LLMs, July 21 2023
What is Llama 2?
App overview
1. Get a Replicate API token
2. Set up the coding environment
Local development
Cloud development
3. Build the app
Import necessary libraries
Define the app title
Define the web app frontend for accepting the API token
Adjustment of model parameters
Store, display, and clear chat messages
Create the LLM response generation function
Accept prompt input
Generate a new LLM response
4. Set the API token
Option 1. Set the API token in Secrets
Option 2. Set the API token in the app
5. Deploy the app
Wrapping up
Contents
Share this post
← All posts

Generative AI has been widely adopted, and the development of new, larger, and improved LLMs is advancing rapidly, making it an exciting time for developers.

You may have heard of the recent release of Llama 2, an open source large language model (LLM) by Meta. This means that you can build on, modify, deploy, and use a local copy of the model, or host it on cloud servers (e.g., Replicate).

While it’s free to download and use, it’s worth noting that self-hosting the Llama 2 model requires a powerful computer with high-end GPUs to perform computations in a timely manner. An alternative is to host the models on a cloud platform like Replicate and use the LLM via API calls. In particular, the three Llama 2 models (llama-7b-v2-chat, llama-13b-v2-chat, and llama-70b-v2-chat) are hosted on Replicate.

In this post, we’ll build a Llama 2 chatbot in Python using Streamlit for the frontend, while the LLM backend is handled through API calls to the Llama 2 model hosted on Replicate. You’ll learn how to:

Get a Replicate API token
Set up the coding environment
Build the app
Set the API token
Deploy the app


🦙
Want to jump right in? Here's the demo app and the GitHub repo.
What is Llama 2?

Meta released the second version of their open-source Llama language model on July 18, 2023. They’re democratizing access to this model by making it free to the community for both research and commercial use. They also prioritize the transparent and responsible use of AI, as evidenced by their Responsible Use Guide.

Here are the five key features of Llama 2:

Llama 2 outperforms other open-source LLMs in benchmarks for reasoning, coding proficiency, and knowledge tests.
The model was trained on almost twice the data of version 1, totaling 2 trillion tokens. Additionally, the training included over 1 million new human annotations and fine-tuning for chat completions.
The model comes in three sizes, each trained with 7, 13, and 70 billion parameters.
Llama 2 supports longer context lengths, up to 4096 tokens.
Version 2 has a more permissive license than version 1, allowing for commercial use.
App overview

Here is a high-level overview of the Llama2 chatbot app:

The user provides two inputs: (1) a Replicate API token (if requested) and (2) a prompt input (i.e. ask a question).
An API call is made to the Replicate server, where the prompt input is submitted and the resulting LLM-generated response is obtained and displayed in the app.

Let's take a look at the app in action:



Go to https://llama2.streamlit.app/
Enter your Replicate API token if prompted by the app.
Enter your message prompt in the chat box, as shown in the screencast below.

1. Get a Replicate API token

Getting your Replicate API token is a simple 3-step process:

Go to https://replicate.com/signin/.
Sign in with your GitHub account.
Proceed to the API tokens page and copy your API token.
2. Set up the coding environment
Local development

To set up a local coding environment, enter the following command into a command line prompt:

pip install streamlit replicate



🦙
NOTE: Make sure to have Python version 3.8 or higher pre-installed.
Cloud development

To set up a cloud environment, deploy using the Streamlit Community Cloud with the help of the Streamlit app template (read more here).

Add a requirements.txt file to your GitHub repo and include the following prerequisite libraries:

streamlit
replicate

3. Build the app

The Llama 2 chatbot app uses a total of 77 lines of code to build:

import streamlit as st
import replicate
import os

# App title
st.set_page_config(page_title="🦙💬 Llama 2 Chatbot")

# Replicate Credentials
with st.sidebar:
    st.title('🦙💬 Llama 2 Chatbot')
    if 'REPLICATE_API_TOKEN' in st.secrets:
        st.success('API key already provided!', icon='✅')
        replicate_api = st.secrets['REPLICATE_API_TOKEN']
    else:
        replicate_api = st.text_input('Enter Replicate API token:', type='password')
        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):
            st.warning('Please enter your credentials!', icon='⚠️')
        else:
            st.success('Proceed to entering your prompt message!', icon='👉')
    os.environ['REPLICATE_API_TOKEN'] = replicate_api

    st.subheader('Models and parameters')
    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')
    if selected_model == 'Llama2-7B':
        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'
    elif selected_model == 'Llama2-13B':
        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'
    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)
    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)
    st.markdown('📖 Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot
def generate_llama2_response(prompt_input):
    string_dialogue = "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'."
    for dict_message in st.session_state.messages:
        if dict_message["role"] == "user":
            string_dialogue += "User: " + dict_message["content"] + "

"
        else:
            string_dialogue += "Assistant: " + dict_message["content"] + "

"
    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', 
                           input={"prompt": f"{string_dialogue} {prompt_input} Assistant: ",
                                  "temperature":temperature, "top_p":top_p, "max_length":max_length, "repetition_penalty":1})
    return output

# User-provided prompt
if prompt := st.chat_input(disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama2_response(prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)
Import necessary libraries

First, import the necessary libraries:

streamlit - a low-code web framework used for creating the web frontend.
replicate - an ML model hosting platform that allows interfacing with the model via an API call.
os - the operating system module to load the API key into the environment variable.
import streamlit as st
import replicate
import os

Define the app title

The title of the app displayed on the browser can be specified using the page_title parameter, which is defined in the st.set_page_config() method:

# App title
st.set_page_config(page_title="🦙💬 Llama 2 Chatbot")

Define the web app frontend for accepting the API token

When designing the chatbot app, divide the app elements by placing the app title and text input box for accepting the Replicate API token in the sidebar and the chat input text in the main panel. To do this, place all subsequent statements under with st.sidebar:, followed by the following steps:

1. Define the app title using the st.title() method.

2. Use if-else statements to conditionally display either:

A success message in a green box that reads API key already provided! for the if statement.
A warning message in a yellow box along with a text input box asking for the API token, as none were detected in the Secrets, for the else statement.

Use nested if-else statement to detect whether the API key was entered into the text box, and if so, display a success message:

with st.sidebar:
    st.title('🦙💬 Llama 2 Chatbot')
    if 'REPLICATE_API_TOKEN' in st.secrets:
        st.success('API key already provided!', icon='✅')
        replicate_api = st.secrets['REPLICATE_API_TOKEN']
    else:
        replicate_api = st.text_input('Enter Replicate API token:', type='password')
        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):
            st.warning('Please enter your credentials!', icon='⚠️')
        else:
            st.success('Proceed to entering your prompt message!', icon='👉')
    os.environ['REPLICATE_API_TOKEN'] = replicate_api
Adjustment of model parameters

In continuation from the above code snippet and inside the same with st.sidebar: statement, we're adding the following code block to allow users to select the Llama 2 model variant to use (namely llama2-7B or Llama2-13B) as well as adjust model parameters (namely temperature, top_p and max_length).

    st.subheader('Models and parameters')
    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')
    if selected_model == 'Llama2-7B':
        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'
    elif selected_model == 'Llama2-13B':
        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'
    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)
    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)
    st.markdown('📖 Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')

Store, display, and clear chat messages
The first code block creates an initial session state to store the LLM generated response as part of the chat message history.
The next code block displays messages (via st.chat_message()) from the chat history by iterating through the messages variable in the session state.
The last code block creates a Clear Chat History button in the sidebar, allowing users to clear the chat history by leveraging the callback function defined on the preceding line.
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

Create the LLM response generation function

Next, create the generate_llama2_response() custom function to generate the LLM’s response. It takes a user prompt as input, builds a dialog string based on the existing chat history, and calls the model using the replicate.run() function.

The model returns a generated response:

# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot
def generate_llama2_response(prompt_input):
    string_dialogue = "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'."
    for dict_message in st.session_state.messages:
        if dict_message["role"] == "user":
            string_dialogue += "User: " + dict_message["content"] + "

"
        else:
            string_dialogue += "Assistant: " + dict_message["content"] + "

"
    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', 
                           input={"prompt": f"{string_dialogue} {prompt_input} Assistant: ",
                                  "temperature":temperature, "top_p":top_p, "max_length":max_length, "repetition_penalty":1})
    return output
Accept prompt input

The chat input box is displayed, allowing the user to enter a prompt. Any prompt entered by the user is added to the session state messages:

# User-provided prompt
if prompt := st.chat_input(disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

Generate a new LLM response

If the last message wasn’t from the assistant, the assistant will generate a new response. While it’s formulating a response, a spinner widget will be displayed. Finally, the assistant's response will be displayed in the chat and added to the session state messages:

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama2_response(prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)

4. Set the API token
Option 1. Set the API token in Secrets

If you want to provide your users with free access to your chatbot, you'll need to cover the costs as your credit card is tied to your account.

To set the API token in the Secrets management on Streamlit Community Cloud, click on the expandable menu at the far right, then click on Settings:

To define the REPLICATE_API_TOKEN environment variable, click on the Secrets tab and paste your Replicate API token:

Once the API token is defined in Secrets, users should be able to use the app without needing to use their own API key:

Option 2. Set the API token in the app

An alternative to setting the API token in Secrets is to prompt users to specify it in the app. This way, users will be notified to provide their own Replicate API token to proceed with using the app:

5. Deploy the app

Once the app is created, deploy it to the cloud in three steps:

Create a GitHub repository for the app.
In Streamlit Community Cloud, click on the New app button, then choose the repository, branch, and app file.
Click Deploy! and the app will be live!
Wrapping up

Congratulations! You’ve learned how to build your own Llama 2 chatbot app using the LLM model hosted on Replicate.

It’s worth noting that the LLM was set to the 7B version and that model parameters (such as temperature and top_p) were initialized with a set of arbitrary values. This post also includes the Pro version, which allows users to specify the model and parameters. I encourage you to experiment with this setup, adjust these parameters, and explore your own variations. This can be a great opportunity to see how these modifications might affect the LLM-generated response.

For additional ideas and inspiration, check out the LLM gallery. If you have any questions, let me know in the comments below or find me on Twitter at @thedataprof or on LinkedIn at Chanin Nantasenamat. You can also check out the Streamlit YouTube channel or my personal YouTube channel, Data Professor.

Happy chatbot-building! 🦙

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

gif2.gif (884×476)
https://blog.streamlit.io/content/images/2023/11/gif2.gif


gif1.gif (920×476)
https://blog.streamlit.io/content/images/2023/11/gif1.gif


static-table.png (2000×663)
https://blog.streamlit.io/content/images/2023/11/static-table.png


rose-chart.png (1042×854)
https://blog.streamlit.io/content/images/2023/11/rose-chart.png


prototype.png (2000×1277)
https://blog.streamlit.io/content/images/2023/11/prototype.png


mockup.png (1251×1097)
https://blog.streamlit.io/content/images/2023/11/mockup.png


multi-modality.png (1490×1396)
https://blog.streamlit.io/content/images/2023/11/multi-modality.png


streamlit-dashboard-python.gif (1360×900)
https://blog.streamlit.io/content/images/2024/01/streamlit-dashboard-python.gif


metrics-cards.png (1920×1080)
https://blog.streamlit.io/content/images/2024/01/metrics-cards.png


inbound-outbound-metrics.png (2000×1051)
https://blog.streamlit.io/content/images/2024/01/inbound-outbound-metrics.png


dashboard_heatmap.png (2000×661)
https://blog.streamlit.io/content/images/2024/01/dashboard_heatmap.png


choropleth.png (504×248)
https://blog.streamlit.io/content/images/2024/01/choropleth.png


How to build an LLM-powered ChatBot with Streamlit
https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build an LLM-powered ChatBot with Streamlit

A step-by-step guide using the unofficial HuggingChat API

By Chanin Nantasenamat
Posted in LLMs, May 10 2023
What the HugChat app can do
Set up the app on the Streamlit Community Cloud
Build the chatbot
1. Required libraries
2. Page config
3. Sidebar
4. Session state
5. Display chat messages
6. Function for bot response output
7. Accept user prompt
8. Generate bot response output
9. Full code
Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit-ers! 👋

My name is Chanin Nantasenamat, PhD. I’m working as a Senior Developer Advocate creating educational content on building Streamlit data apps. In my spare time, I love to create coding and data science tutorials on my YouTube channel, Data Professor.

Are you looking to build an AI-powered chatbot using LLM models but without the heavy API cost? If you answered yes, then keep reading!

You’ll build a chatbot that can generate responses to the user-provided prompt input (i.e., questions) using an open-source, no-cost LLM model OpenAssistant/oasst-sft-6-llama-30b-xor from the unofficial HuggingChat API known as HugChat. You’ll deploy the chatbot as a Streamlit app that can be shared with the world!

In this post, you’ll learn how to:

Set up the app on the Streamlit Community Cloud
Build the chatbot
🤗
Want to jump right in? Here's the HugChat app and the GitHub repo.
What the HugChat app can do

Before we proceed with the tutorial, let's quickly grasp the app's functionality. Head over to the app and get familiar with its layout—(1) the Sidebar accepts the login credential, and (2) the Main panel displays conversational messages:

Interact with it by (1) entering your prompt into the text input box and (2) reading the human/bot messages.

Set up the app on the Streamlit Community Cloud

Clone the app-starter-kit repo to use as the template for creating the chatbot app. Then click on "Use this template":

Give the repo a name (such as mychatbot). Next, click "Create repository from the template." A copy of the repo will be placed in your account:

Next, follow this blog post to get the newly cloned repo deployed on the Streamlit Community Cloud. When done, you should be able to see the deployed app:

Edit the requirements.txt file by adding the following prerequisite Python libraries:

streamlit
hugchat==0.0.8

This will spin up a server with these prerequisites pre-installed.

Let's take a look at the contents of streamlit_app.py:

import streamlit as st

st.title('🎈 App Name')

st.write('Hello world!')


In subsequent sections, you will modify the contents of this file with code snippets about the chatbot.

Finally, before proceeding with app building, let's take a look at how the user will interact with it:

Front-end: The user submits an input prompt (by providing a string of text to the text box via st.text_input()), and the app generates a response.
Back-end: Input prompt is sent to hugchat (the unofficial port to the HuggingChat API) via streamlit-chat for generating a response.
Front-end: Generated responses are displayed in the app via's message() command.
Build the chatbot

Fire up the streamlit_app.py file and replace the original content with code snippets mentioned below.

1. Required libraries

Import prerequisite Python libraries:

import streamlit as st
from hugchat import hugchat
from hugchat.login import Login
2. Page config

Name the app using the page_title input argument in the st.set_page_config method (it'll be used as the app title and as the title in the preview when sharing on social media):

st.set_page_config(page_title="🤗💬 HugChat")
3. Sidebar

Create a sidebar for accepting Hugging Face authentication credentials:

with st.sidebar:
    st.title('🤗💬 HugChat')
    if ('EMAIL' in st.secrets) and ('PASS' in st.secrets):
        st.success('HuggingFace Login credentials already provided!', icon='✅')
        hf_email = st.secrets['EMAIL']
        hf_pass = st.secrets['PASS']
    else:
        hf_email = st.text_input('Enter E-mail:', type='password')
        hf_pass = st.text_input('Enter password:', type='password')
        if not (hf_email and hf_pass):
            st.warning('Please enter your credentials!', icon='⚠️')
        else:
            st.success('Proceed to entering your prompt message!', icon='👉')
    st.markdown('📖 Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/)!')

Use the with statement to confine the constituent contents to the sidebar. They include:

The app title is specified via st.title()
if-else statements for detecting login credentials from st.secrets or to ask from users via st.text_input()
A link to this tutorial blog via st.markdown()
4. Session state

Initialize the chatbot by with messages session state and giving it a starter message at the first app run:

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I help you?"}]

Here, past denotes the human user's input and generated indicates the bot's response.

5. Display chat messages

Conversational messages are displayed iteratively from the messages session state via the for loop together with the use of Streamlit’s chat feature st.chat_message().

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
6. Function for bot response output

Create the generate_response(prompt) custom function for taking in user's input prompt as an argument to generate an AI response using the HuggingChat API via the hugchat.ChatBot() method (this LLM model can be swapped with any other one). Hugging Face login credentials are accepted via the Login() and sign.login() methods:

# Function for generating LLM response
def generate_response(prompt_input, email, passwd):
    # Hugging Face Login
    sign = Login(email, passwd)
    cookies = sign.login()
    # Create ChatBot                        
    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())
    return chatbot.chat(prompt_input)
7. Accept user prompt

User’s input prompt message are accepted via the st.chat_input() method and appended to the messages session state followed by displaying the message via st.chat_message() together with st.write():

# User-provided prompt
if prompt := st.chat_input(disabled=not (hf_email and hf_pass)):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
8. Generate bot response output

Use an if condition to detect whether the last response in the messages session state is from the assistant or user. The chatbot will be triggered to generate a response if the last message is not from the chatbot (assistant). In generating the response, the st.chat_message(), st.spinner() and the custom generate_response() function are used where generated messages will display a spinner with a short message saying Thinking.... Finally, the generated response is saved to the messages session state.

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response(prompt, hf_email, hf_pass) 
            st.write(response) 
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
9. Full code

Putting all of this together, we get the following full code of the app:

import streamlit as st
from hugchat import hugchat
from hugchat.login import Login

# App title
st.set_page_config(page_title="🤗💬 HugChat")

# Hugging Face Credentials
with st.sidebar:
    st.title('🤗💬 HugChat')
    if ('EMAIL' in st.secrets) and ('PASS' in st.secrets):
        st.success('HuggingFace Login credentials already provided!', icon='✅')
        hf_email = st.secrets['EMAIL']
        hf_pass = st.secrets['PASS']
    else:
        hf_email = st.text_input('Enter E-mail:', type='password')
        hf_pass = st.text_input('Enter password:', type='password')
        if not (hf_email and hf_pass):
            st.warning('Please enter your credentials!', icon='⚠️')
        else:
            st.success('Proceed to entering your prompt message!', icon='👉')
    st.markdown('📖 Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/)!')
    
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I help you?"}]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Function for generating LLM response
def generate_response(prompt_input, email, passwd):
    # Hugging Face Login
    sign = Login(email, passwd)
    cookies = sign.login()
    # Create ChatBot                        
    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())
    return chatbot.chat(prompt_input)

# User-provided prompt
if prompt := st.chat_input(disabled=not (hf_email and hf_pass)):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response(prompt, hf_email, hf_pass) 
            st.write(response) 
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
Wrapping up

In this post, I've shown you how to create a chatbot app using an open-source LLM from the unofficial HuggingChat API and Streamlit. In only a few lines of code, you can create your own AI-powered chatbot.

I hope this tutorial encourages you to explore the endless possibilities of chatbot development using different models and techniques. The sky is the limit!

If you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn. Share your app creations on social media and tag me or the Streamlit account, and I'll be happy to provide feedback or help retweet!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

streamlit-dashboard-components.jpg (2000×1440)
https://blog.streamlit.io/content/images/2024/01/streamlit-dashboard-components.jpg


sample-conversation.png (2000×1165)
https://blog.streamlit.io/content/images/2023/12/sample-conversation.png


st-progress-loading.png (2000×1039)
https://blog.streamlit.io/content/images/2023/12/st-progress-loading.png


LangChain tutorial #5: Build an Ask the Data app
https://blog.streamlit.io/langchain-tutorial-5-build-an-ask-the-data-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain tutorial #5: Build an Ask the Data app

Leverage Agents in LangChain to interact with pandas DataFrame

By Chanin Nantasenamat
Posted in LLMs, July 21 2023
What are Agents?
Using Agents in LangChain
App overview
Step 1. Get an OpenAI API key
Step 2. Set up the coding environment
Local development
Cloud development
Step 3. Build the app
App overview
Import libraries
Display the app title
Load the CSV file
Create the LLM response generation function
Input widgets
Define the app logic
Step 4. Deploy the app
Wrapping up
Contents
Share this post
← All posts

Large language models (LLMs) have revolutionized how we process and understand text data, enabling a diverse array of tasks spanning text generation, summarization, classification, and much more. Combining LangChain and Streamlit to build LLM-powered applications is a potent combination for unlocking an array of possibilities, especially for developers interested in creating chatbots, personal assistants, and content creation apps.

In the previous four LangChain tutorials, you learned about three of the six key modules: model I/O (LLM model and prompt templates), data connection (document loader, text splitting, embeddings, and vector store), and chains (summarize chain and question-answering chain).

This tutorial explores the use of the fourth LangChain module, Agents. Specifically, we'll use the pandas DataFrame Agent, which allows us to work with pandas DataFrame by simply asking questions.

We'll build the pandas DataFrame Agent app for answering questions on a pandas DataFrame created from a user-uploaded CSV file in four steps:

Get an OpenAI API key
Set up the coding environment
Build the app
Deploy the app


🦜
Want to jump right in? Here's the demo app and the repo code.
What are Agents?

According to Harrison Chase, agents "use an LLM to determine which actions to take and in what order." An action can refer to using tools, observing their output, or returning a response to the user. Tools are entities that take a string as input and return a string as output. Examples of tools include APIs, databases, search engines, LLMs, chains, other agents, shells, and Zapier.

Agents are comprised of two types:

Action agents
Plan-and-execute agents
Using Agents in LangChain

To use an agent in LangChain, you need to specify three key elements:

LLM. LLM is responsible for determining the course of action that an agent would take to fulfill its task of answering a user query. If you're using the OpenAI LLM, it's available via OpenAI() from langchain.llms.
Tools. These are resources that an agent can use to accomplish its task, such as querying a database, accessing an API, or searching Google. You can load them via load_tools() from langchain.agents.
Agent. The available agent types are action agents or plan-and-execute agents. You can access them via AgentType() from langchain.agents.

In this tutorial, we'll be using the pandas DataFrame Agent, which can be created using create_pandas_dataframe_agent() from langchain.agents.



🦜
Check out the LangChain documentation on pandas DataFrame Agent.
App overview

Let's take a look at the general flow of the app.

Once the app is loaded, the user should perform the following steps in sequential order:

Upload a CSV file. You can also tweak the underlying code to read in tabular formats such as Excel or tab-delimited files.
Select an example query from the drop-down menu or provide your own custom query by selecting the "Other" option.
Enter your OpenAI API key.

That's all for the frontend! As for the backend, the pandas DataFrame Agent will work its magic on the data and return an LLM-generated answer.

Now let's take a look at the app in action:



Step 1. Get an OpenAI API key

You can find a detailed walkthrough on obtaining an OpenAI API key in LangChain Tutorial #1.

Step 2. Set up the coding environment
Local development

To set up a local coding environment with the necessary libraries, use pip install as shown below (make sure you have Python version 3.7 or higher):

pip install streamlit openai langchain pandas tabulate

Cloud development

In addition to using a local computer to develop apps, you can deploy them on the cloud using Streamlit Community Cloud. You can use the Streamlit app template to do this (read more here).

Next, add the following Python libraries to the requirements.txt file:

streamlit
openai
langchain
pandas
tabulate

Step 3. Build the app
App overview

The entire app consists of 47 lines of code, as shown below:

import streamlit as st
import pandas as pd
from langchain.chat_models import ChatOpenAI
from langchain.agents import create_pandas_dataframe_agent
from langchain.agents.agent_types import AgentType

# Page title
st.set_page_config(page_title='🦜🔗 Ask the Data App')
st.title('🦜🔗 Ask the Data App')

# Load CSV file
def load_csv(input_csv):
  df = pd.read_csv(input_csv)
  with st.expander('See DataFrame'):
    st.write(df)
  return df

# Generate LLM response
def generate_response(csv_file, input_query):
  llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613', temperature=0.2, openai_api_key=openai_api_key)
  df = load_csv(csv_file)
  # Create Pandas DataFrame Agent
  agent = create_pandas_dataframe_agent(llm, df, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS)
  # Perform Query using the Agent
  response = agent.run(input_query)
  return st.success(response)

# Input widgets
uploaded_file = st.file_uploader('Upload a CSV file', type=['csv'])
question_list = [
  'How many rows are there?',
  'What is the range of values for MolWt with logS greater than 0?',
  'How many rows have MolLogP value greater than 0.',
  'Other']
query_text = st.selectbox('Select an example query:', question_list, disabled=not uploaded_file)
openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))

# App logic
if query_text is 'Other':
  query_text = st.text_input('Enter your query:', placeholder = 'Enter query here ...', disabled=not uploaded_file)
if not openai_api_key.startswith('sk-'):
  st.warning('Please enter your OpenAI API key!', icon='⚠')
if openai_api_key.startswith('sk-') and (uploaded_file is not None):
  st.header('Output')
  generate_response(uploaded_file, query_text)

Import libraries

To start, import the necessary libraries:

Streamlit. A low-code web framework used for creating the app's frontend
pandas. A data wrangling framework for loading the CSV file as a DataFrame
LangChain. An LLM framework that coordinates the use of an LLM model to generate a response based on the user-provided prompt.
import streamlit as st
import pandas as pd
from langchain.chat_models import ChatOpenAI
from langchain.agents import create_pandas_dataframe_agent
from langchain.agents.agent_types import AgentType

Display the app title

Next, display the title of the app:

# Page title
st.set_page_config(page_title='🦜🔗 Ask the Data App')
st.title('🦜🔗 Ask the Data App')

Load the CSV file

Since the CSV file is one of the app's inputs, along with the data query, you need to create a custom function to load it (use pandas' read_csv() method). Once loaded, display the DataFrame inside an expander box:

# Load CSV file
def load_csv(input_csv):
  df = pd.read_csv(input_csv)
  with st.expander('See DataFrame'):
    st.write(df)
  return df

Create the LLM response generation function

The next step is to process data using the Agent, specifically the pandas DataFrame Agent, and the LLM model (GPT 3.5).

To create an instance of the LLM model, use ChatOpenAI() and set gpt-3.5-turbo-0613 as the model_name. Next, create the pandas DataFrame Agent using the create_pandas_dataframe_agent() method and assign the LLM model, defined by llm, and the input data, defined by df.

🦜
NOTE: While creating and testing the app, I discovered that usage costs were significantly higher compared to previous apps built in this tutorial series. So I decided to use the GPT 3.5 model due to its significantly lower cost.


# Generate LLM response
def generate_response(csv_file, input_query):
  llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613', temperature=0.2, openai_api_key=openai_api_key)
  df = load_csv(csv_file)
  # Create Pandas DataFrame Agent
  agent = create_pandas_dataframe_agent(llm, df, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS)
  # Perform Query using the Agent
  response = agent.run(input_query)
  return st.success(response)

Input widgets

Next, create input widgets to accept various variables for data analysis. These include:

The user-provided CSV file (stored in the uploaded_file variable)
The input query (stored in the question_list and query_text variables)
The OpenAI API (stored in the openai_api_key variable)


# Input widgets
uploaded_file = st.file_uploader('Upload a CSV file', type=['csv'])
question_list = [
  'How many rows are there?',
  'What is the range of values for MolWt with logS greater than 0?',
  'How many rows have MolLogP value greater than 0.',
  'Other']
query_text = st.selectbox('Select an example query:', question_list, disabled=not uploaded_file)
openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))

Define the app logic

The app logic is defined in this last code block. Follow these steps:

Check if the user has selected the Other option from the drop-down select box defined in query_text to provide a custom text query. If so, the user can enter their query text.
Check if the user has provided their OpenAI API key. If not, a reminder message is displayed for the user to enter their API key.
Perform a final check for the API key and the user-provided CSV file. If the check is successful (meaning the user has provided all necessary information), we proceed to generate a response from the pandas DataFrame Agent.


# App logic
if query_text is 'Other':
  query_text = st.text_input('Enter your query:', placeholder = 'Enter query here ...', disabled=not uploaded_file)
if not openai_api_key.startswith('sk-'):
  st.warning('Please enter your OpenAI API key!', icon='⚠')
if openai_api_key.startswith('sk-') and (uploaded_file is not None):
  st.header('Output')
  generate_response(uploaded_file, query_text)

Step 4. Deploy the app

Once the app has been created, it can be deployed to the cloud in three steps:

Create a GitHub repository to store the app files.
Go to the Streamlit Community Cloud, click the New app button, and select the appropriate repository, branch, and application file.
Finally, click Deploy!.

After a few moments, the app should be ready to use!

Wrapping up

You've learned how to build an Ask the Data app that lets you ask questions to understand your data better. We used Streamlit as the frontend to accept user input (CSV file, questions about the data, and OpenAI API key) and LangChain for backend processing of the data via the pandas DataFrame Agent.

If you're looking for ideas and inspiration, check out the Generative AI page and the LLM gallery. And if you have any questions, please post them in the comments below or on Twitter at @thedataprof, on LinkedIn, on the Streamlit YouTube channel, or on my personal YouTube channel, Data Professor.

I can't wait to see what you'll build! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

load-data-screen.png (2000×1151)
https://blog.streamlit.io/content/images/2023/12/load-data-screen.png


tsv-time-machine-screenshot.png (2000×1159)
https://blog.streamlit.io/content/images/2023/12/tsv-time-machine-screenshot.png


Untitled--6-.png (792×893)
https://blog.streamlit.io/content/images/2023/12/Untitled--6-.png


Untitled--5-.png (782×500)
https://blog.streamlit.io/content/images/2023/12/Untitled--5-.png


LangChain tutorial #4: Build an Ask the Doc app
https://blog.streamlit.io/langchain-tutorial-4-build-an-ask-the-doc-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain tutorial #4: Build an Ask the Doc app

How to get answers from documents using embeddings, a vector store, and a question-answering chain

By Chanin Nantasenamat
Posted in LLMs, June 20 2023
What is document question-answering?
Step 1. Ingestion
Step 2. Generation
App overview
Step 1. Get an OpenAI API key
Step 2. Set up the coding environment
Local development
Cloud development
Step 3. Build the app
Import libraries
Create the LLM response generation function
Define the web app frontend
Step 4. Deploy the app
Wrapping up
Contents
Share this post
← All posts

In recent months, large language models (LLMs) have attracted widespread attention as they open up new opportunities, particularly for developers creating chatbots, personal assistants, and content.

In the previous LangChain tutorials, you learned about three of the six key modules: model I/O (LLM model and prompt templates), data connection (document loader and text splitting), and chains (summarize chain).

In this tutorial, we'll explore how to use these modules, how to create embeddings and store them in a vector store, and how to use a specialized chain for question answering about a text document. We'll use these tools to build the Ask the Doc app in four steps:

Get an OpenAI API key
Set up the coding environment
Build the app
Deploy the app


🦜
If you want to skip reading and hop right in, here is the app and here is the code.
What is document question-answering?

As the name implies, document question-answering answers questions about a specific document. This process involves two steps:

Step 1. Ingestion

The document is prepared through a process known as ingestion so that the LLM model can use it. Ingestion transforms it into an index, the most common being a vector store. The process involves:

Loading the document
Splitting the document
Creating embeddings
Storing the embeddings in a database (a vector store)
Step 2. Generation

With the index or vector store in place, you can use the formatted data to generate an answer by following these steps:

Accept the user's question
Identify the most relevant document for the question
Pass the question and the document as input to the LLM to generate an answer


🔗
Check out the LangChain documentation on question answering over documents.
App overview

At a conceptual level, the app's workflow remains impressively simple:

The user uploads a document text file, asks a question, provides an OpenAI API key, and clicks "Submit."
LangChain processes the two input elements. First, it splits the input document into chunks, creates embedding vectors, and stores them in the embeddings database (i.e., the vector store). Then it applies the user-provided question to the Question Answering chain so that the LLM can answer the question:

Let's see the app in action.

Check out these materials if you want to follow along:

Input file: state_of_the_union.txt
Question: "What did the president say about Ketanji Brown Jackson?"

Go ahead and try it:

Step 1. Get an OpenAI API key

For a detailed walkthrough on getting an OpenAI API key, read LangChain Tutorial #1.

Step 2. Set up the coding environment
Local development

To set up a local coding environment, use pip install (make sure you have Python version 3.7 or higher):

pip install streamlit langchain openai tiktoken

Cloud development

You can deploy your app to the Streamlit Community Cloud using the Streamlit app template. (read more in the previous blog post).

To proceed, include the following prerequisite Python libraries in the requirements.txt file:

streamlit
langchain
openai
chromadb
tiktoken

Step 3. Build the app

The code for the app is only 46 lines, 10 of which are in-line documentation explaining what each code block does:

import streamlit as st
from langchain.llms import OpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

def generate_response(uploaded_file, openai_api_key, query_text):
    # Load document if file is uploaded
    if uploaded_file is not None:
        documents = [uploaded_file.read().decode()]
        # Split documents into chunks
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        texts = text_splitter.create_documents(documents)
        # Select embeddings
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        # Create a vectorstore from documents
        db = Chroma.from_documents(texts, embeddings)
        # Create retriever interface
        retriever = db.as_retriever()
        # Create QA chain
        qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)
        return qa.run(query_text)

# Page title
st.set_page_config(page_title='🦜🔗 Ask the Doc App')
st.title('🦜🔗 Ask the Doc App')

# File upload
uploaded_file = st.file_uploader('Upload an article', type='txt')
# Query text
query_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)

# Form input and query
result = []
with st.form('myform', clear_on_submit=True):
    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))
    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))
    if submitted and openai_api_key.startswith('sk-'):
        with st.spinner('Calculating...'):
            response = generate_response(uploaded_file, openai_api_key, query_text)
            result.append(response)
            del openai_api_key

if len(result):
    st.info(response)


Let's dissect the individual code blocks…

Import libraries

First, import the necessary libraries (primarily Streamlit and LangChain):

import streamlit as st
from langchain.llms import OpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

Create the LLM response generation function

Next, create the generate_response() custom function that contains the bulk of the LLM pipeline (the uploaded file is loaded as a text string).

Use these LangChain functions to preprocess the text:

OpenAI() loads the OpenAI LLM model.
CharacterTextSplitter() splits documents into chunks.
OpenAIEmbeddings() encodes the document chunks or strings of text as embeddings (a vector or list of floating point numbers). Distances amongst the embeddings provide a measure of relatedness that determines their similarity or difference. Embeddings are useful as they can be used for anomaly detection, classification, recommendations, search, topic clustering, etc.
Chroma() is an open-source embedding database (also called a vector store—a database of embedding vectors). Particularly, Chroma.from_documents() is used for creating the vector store index using the document chunk after the text split and the OpenAIEmbeddings() function as input arguments.
RetrievalQA() is the question-answering chain that takes as input arguments the LLM via the llm parameter, the chain type to use via the chain_type parameter, and the retriever via the retriever parameter.

Finally, the run() method is executed on the defined instance of RetrievalQA(), using the query text as the input argument:

def generate_response(uploaded_file, openai_api_key, query_text):
    # Load document if file is uploaded
    if uploaded_file is not None:
        documents = [uploaded_file.read().decode()]
    # Split documents into chunks
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.create_documents(documents)
    # Select embeddings
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    # Create a vectorstore from documents
    db = Chroma.from_documents(texts, embeddings)
    # Create retriever interface
    retriever = db.as_retriever()
    # Create QA chain
    qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)
    return qa.run(query_text)

Define the web app frontend

Use the page_title parameter in the st.set_page_config() method to give the app a title (displayed in the browser).

Display the in-app title using st.title():

# Page title
st.set_page_config(page_title='🦜🔗 Ask the Doc App')
st.title('🦜🔗 Ask the Doc App')


Next, add input widgets that allow users to upload text files using st.file_uploader() and to ask questions about the uploaded text document using st.text_input():

# File upload
uploaded_file = st.file_uploader('Upload an article', type='txt')
# Query text
query_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)


After the user has provided the above two inputs, the form will unlock the ability to enter the OpenAI API key via st.text_input().

After you enter the OpenAI API key and click on Submit, you'll see a spinner element displaying the message Calculating.... This triggers the generate_response() function, which generates the LLM's answer to the user's question.

Once it's been generated, the API key is deleted to ensure API safety:

# Form input and query
result = []
with st.form('myform', clear_on_submit=True):
    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))
    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))
    if submitted and openai_api_key.startswith('sk-'):
        with st.spinner('Calculating...'):
            response = generate_response(uploaded_file, openai_api_key, query_text)
            result.append(response)
            del openai_api_key

if len(result):
    st.info(response)


An empty list called result is defined before the form. This is followed by an if statement that displays the LLM-generated response when the result list is populated with it. This approach allows the API key to be deleted after an LLM-generated response is created.

🦜
NOTE: Prior to deleting the API key, the LLM response is appended to the initially empty result list.
Step 4. Deploy the app

After creating the app, you can launch it in three steps:

Establish a GitHub repository specifically for the app.
Navigate to Streamlit Community Cloud, click the New app button, and choose the appropriate repository, branch, and application file.
Finally, hit the Deploy! button.

Your app will be live in no time!

Wrapping up

In this article, you've learned how to create the Ask the Doc app that facilitates question-answering about a user-uploaded text document. In other words, you prepared the document for ingestion to be used by the LLM for generating an answer to the user's question. I can't wait to see what you'll build!

If you're looking for ideas and inspiration, check out the LLM gallery. And if you have any questions, let me know in the comments below, or find me on Twitter at @thedataprof,  on LinkedIn, on the Streamlit YouTube channel, or my personal YouTube channel Data Professor.

Happy Streamlit-ing! 🎈

P.S. This post was made possible thanks to the technical review by Tim Conkling and editing by Ksenia Anske.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Untitled--4-.png (794×826)
https://blog.streamlit.io/content/images/2023/12/Untitled--4-.png


Untitled--3-.png (795×532)
https://blog.streamlit.io/content/images/2023/12/Untitled--3-.png


Untitled--2-.png (791×179)
https://blog.streamlit.io/content/images/2023/12/Untitled--2-.png


Untitled--1-.png (788×179)
https://blog.streamlit.io/content/images/2023/12/Untitled--1-.png


Chanin Nantasenamat - Streamlit (Page 2)
https://blog.streamlit.io/author/chanin/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Chanin Nantasenamat
Senior Developer Advocate at Streamlit
16 posts
Website
Twitter
Hackathon 101: 5 simple tips for beginners

Prepare to win your first hackathon!

Tutorials
by
Chanin Nantasenamat
,
March 16 2023
Host your Streamlit app for free

Learn how to transfer your apps from paid platforms to Streamlit Community Cloud

Tutorials
by
Chanin Nantasenamat
,
January 24 2023
Streamlit Quests: Getting started with Streamlit

The guided path for learning Streamlit

Tutorials
by
Chanin Nantasenamat
,
November 18 2022
Streamlit App Starter Kit: How to build apps faster

Save 10 minutes every time you build an app

Tutorials
by
Chanin Nantasenamat
,
September 27 2022
30 Days of Streamlit

A fun challenge to learn and practice using Streamlit

Advocate Posts
by
Chanin Nantasenamat
,
April 1 2022
How to master Streamlit for data science

The essential Streamlit for all your data science needs

Tutorials
by
Chanin Nantasenamat
,
January 18 2022
← Previous page
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Building a Streamlit and scikit-learn app with ChatGPT
https://blog.streamlit.io/building-a-streamlit-and-scikit-learn-app-with-chatgpt/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building a Streamlit and scikit-learn app with ChatGPT

Catching up on coding skills with an AI assistant

By Michael Hunger
Posted in LLMs, June 16 2023
Creating the "Hello, World!" version of our EDA app
Michael
ChatGPT
Getting the initial app up and running
Understanding the initial app
Michael
ChatGPT
Fixing the ugly legend
Michael
ChatGPT
Michael
ChatGPT
Hardcoding the CSV file to ease the app development
Michael
ChatGPT
Loading data from the database
Michael
ChatGPT
Implementing caching to reduce data reloads
Michael
ChatGPT
Learning about updated APIs from a documentation drop
Michael
ChatGPT
Adding "real" data science—predictions with scikit-learn
Michael
ChatGPT
Resolving scikit-learn errors
Michael
ChatGPT
Michael
ChatGPT
Addressing column order consistency
Michael
ChatGPT
Michael
ChatGPT
Fixing the billion squared error 😦
Michael
ChatGPT
Incorporating predictions into the app
Michael
ChatGPT
Michael
ChatGPT
Adding selection by name
Michael
ChatGPT
Michael
ChatGPT
Implementing dataframe filters
Michael
ChatGPT
Wrapping up
Contents
Share this post
← All posts

Hey, Streamlit community! 👋

My name is Michael, and I lead Developer Relations and Labs at Neo4j, an open-source graph database company. In 2017, we began building a scalable graph algorithms library with capabilities like Graph Neural Networks (GNNs), graph embeddings, and k-nearest neighbor (kNN) similarity graphs.

Since ChatGPT became available last year, I've spent a lot of time tinkering with large language models (LLMs) and exploring their impact on the database space. I've also been using them as a daily tool for ideation, writing, summarization, and coding. With GPT-4, this has become even better.

Simon Willison, a prolific blogger, coder, and author of Datasette, stated that "With ChatGPT support, I'm not afraid to start side-projects that would have taken me several days, because now I can finish them in a few hours." Inspired by his words, a couple of weeks ago, I started a series of live streams appropriately called "Michael and Bot - Livecoding with ChatGPT." The goal was to explore new territories and create something useful within a two-hour window. So far, we've created:

A Chrome extension that sends selected text to OpenAI's API to extract entities and relationships and stores them in Neo4j.
A basic network graph layout algorithm built from scratch in JavaScript.
A Streamlit app for exploratory data analysis (EDA) with predictions using scikit-learn.

I first got excited about Streamlit by the awesome videos my friend Mark Needham made. Today, I'm excited to dive into the adventure of creating the Streamlit app with you.

🎥
Watch the recorded live stream if you want to see me sweat over building this app. Or skip ahead and check out the final code here.
Creating the "Hello, World!" version of our EDA app

I started with this question.

Michael

We want to code together today. We're going to build a streamlit app for EDA. Can you give me the files I need to start with the "Hello, World!" set of visual components?

ChatGPT

Of course! To create a simple Streamlit app for EDA with a "Hello, World!" set of visual components, you'll need the following files:

requirements.txt: This file lists the required Python packages for your project.
app.py: This file contains the main Streamlit app code.

First, create a new directory for your project and navigate to it using the terminal.

Next, create a requirements.txt file in your project directory and include the following packages:

streamlit==0.88.0
pandas==1.3.2
numpy==1.21.2
matplotlib==3.4.3
seaborn==0.11.2


Now, create an app.py file in your project directory with the following content:

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    st.title("Hello, World! EDA Streamlit App")

    st.header("Upload your CSV data file")
    data_file = st.file_uploader("Upload CSV", type=["csv"])

    if data_file is not None:
        data = pd.read_csv(data_file)
        st.write("Data overview:")
        st.write(data.head())

        st.sidebar.header("Visualizations")
        plot_options = ["Bar plot", "Scatter plot", "Histogram", "Box plot"]
        selected_plot = st.sidebar.selectbox("Choose a plot type", plot_options)

        if selected_plot == "Bar plot":
            x_axis = st.sidebar.selectbox("Select x-axis", data.columns)
            y_axis = st.sidebar.selectbox("Select y-axis", data.columns)
            st.write("Bar plot:")
            fig, ax = plt.subplots()
            sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)
            st.pyplot(fig)

        elif selected_plot == "Scatter plot":
            x_axis = st.sidebar.selectbox("Select x-axis", data.columns)
            y_axis = st.sidebar.selectbox("Select y-axis", data.columns)
            st.write("Scatter plot:")
            fig, ax = plt.subplots()
            sns.scatterplot(x=data[x_axis], y=data[y_axis], ax=ax)
            st.pyplot(fig)

        elif selected_plot == "Histogram":
            column = st.sidebar.selectbox("Select a column", data.columns)
            bins = st.sidebar.slider("Number of bins", 5, 100, 20)
            st.write("Histogram:")
            fig, ax = plt.subplots()
            sns.histplot(data[column], bins=bins, ax=ax)
            st.pyplot(fig)

        elif selected_plot == "Box plot":
            column = st.sidebar.selectbox("Select a column", data.columns)
            st.write("Box plot:")
            fig, ax = plt.subplots()
            sns.boxplot(data[column], ax=ax)
            st.pyplot(fig)

if __name__ == "__main__":
    main()


To run the app, make sure you have the required packages installed by running:

pip install -r requirements.txt


Finally, start the Streamlit app with the following command:

streamlit run app.py


Your app will open in your default web browser, and you can start uploading your CSV file and exploring different visualizations. Enjoy coding together!

Getting the initial app up and running

I created the necessary files and updated the dependencies using PyPI. The training cutoff for ChatGPT was in September 2021; that's why the versions were a bit dated. Fortunately, all of the APIs still worked. As a result, I was able to run my app successfully:

streamlit==1.21.0
pandas==1.5.3
numpy==1.24.2
matplotlib==3.7.1
seaborn==0.12.2
python-dotenv==1.0.0
neo4j==5.7.0


I tested the app by using a CSV file that contained movie data, including titles, genres, popularity, revenue, and more. It worked perfectly! How cool!

Understanding the initial app

There was a lot of code with many API calls. To better understand the Python code, I asked ChatGPT for help.

Michael

Explain the code in the Python file in 6 bullet points.

ChatGPT
Import required libraries: The code starts by importing necessary libraries such as Streamlit, Pandas, NumPy, Matplotlib, and Seaborn for creating the web app and visualizations.
Define the main function: The main() function contains the core logic of the Streamlit app. It is responsible for rendering the user interface, handling file uploads, and creating visualizations.
File uploader: Inside the main function, a file uploader is created using st.file_uploader, which allows users to upload CSV files. If a valid file is uploaded, the data is read into a Pandas DataFrame.
Sidebar: The app's sidebar is created using st.sidebar. It contains a header and a select box for users to choose the type of plot they want to generate. Additional options for each plot type are also provided in the sidebar.
Plot generation: Depending on the selected plot type, the appropriate Seaborn plotting function is called (e.g. sns.barplot, sns.scatterplot, sns.histplot, or sns.boxplot). The user-selected options are passed as arguments to the plotting functions.
Display visualizations: After generating the plots using Matplotlib and Seaborn, they are displayed in the main part of the Streamlit app using st.pyplot(), which takes the Matplotlib figure as an argument.

Okay, cool, that explains it.

Fixing the ugly legend

While playing around with the charts, I noticed the legend labels overlapping. I asked our coding friend to either slant them or make them vertical.

Michael

The labels on the x-axis are overlapping. Can we render them at an angle or vertically to improve readability?

ChatGPT

You can render the x-axis labels slanted or vertically to improve readability. To achieve this, you can modify the app.py code to adjust the x-axis label rotation using the set_xticklabels() function with the rotation parameter. Here's how you can do it for each plot type:

For the Bar plot:

# ...
if selected_plot == "Bar plot":
    x_axis = st.sidebar.selectbox("Select x-axis", data.columns)
    y_axis = st.sidebar.selectbox("Select y-axis", data.columns)
    st.write("Bar plot:")
    fig, ax = plt.subplots()
    sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)
		# Add this line
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")  
    st.pyplot(fig)
# ...


In the above code snippets, the rotation parameter is set to 45 degrees for slanted labels. You can set it to 90 degrees for vertical labels. The ha parameter (horizontal alignment) is set to "right" to improve the appearance of the rotated labels. Update the corresponding sections in your app.py file to apply these changes to your app.

It was really nice that it remembered the context of the previous code and only showed me the necessary changes. It also highlighted the changes that I should make to other charts. For brevity, I haven't included them here.

I noticed too many ticks on the x-axis, so I asked ChatGPT to fix it.

Michael

Can we also reduce the number of ticks on the x-axis too?

ChatGPT

Yes, you can reduce the number of ticks on the x-axis. One way to do this is by using the MaxNLocator function from the matplotlib.ticker library. The MaxNLocator function helps in setting the maximum number of intervals on the x-axis. You'll need to import the ticker library first and then adjust the code for each plot type accordingly.

First, add the following import statement at the beginning of your app.py file:

import matplotlib.ticker as ticker


Now, update each plot type in the app.py file to set the number of ticks on the x-axis:

For the Bar plot:

# ...
if selected_plot == "Bar plot":
    x_axis = st.sidebar.selectbox("Select x-axis", data.columns)
    y_axis = st.sidebar.selectbox("Select y-axis", data.columns)
    st.write("Bar plot:")
    fig, ax = plt.subplots()
    sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)

	  # Add this line
    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True, nbins=10))  
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")
    st.pyplot(fig)
# ...


In the above code snippets, ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True, nbins=10)) sets the maximum number of intervals on the x-axis to 10. You can change the nbins parameter to adjust the number of ticks as needed. Update the corresponding sections in your app.py file to apply these changes to your app.

In the end, I changed it to a 90-degree slant as the angled labels were still cut off a bit. You can see the results in the screenshot below.

Hardcoding the CSV file to ease the app development

That was much better. However, it was frustrating to have to re-upload the file each time I made a change. To streamline the process, I asked GPT-4 to hardcode the file.

Michael

Can we default the file-upload result to a fixed filename while testing?

ChatGPT

You can default the file-upload result to a fixed filename while testing. You'll need to load the data from the fixed file path when the file upload returns None. This way, you can quickly test your app without having to upload the file every time.

Here's how to modify the app.py code to default to a fixed file path when no file is uploaded:

# ...

def main():
    st.title("Hello, World! EDA Streamlit App")

    st.header("Upload your CSV data file")
    data_file = st.file_uploader("Upload CSV", type=["csv"])

    if data_file is not None:
        data = pd.read_csv(data_file)
    else:  # Add this block
				# Replace with your fixed file path
        fixed_file_path = "your_fixed_file_path.csv"  
        data = pd.read_csv(fixed_file_path)

    st.write("Data overview:")
    st.write(data.head())

    # ...


Replace your_fixed_file_path.csv with the actual path to the fixed CSV file, you want to use for testing. With this modification, when no file is uploaded, the app will load data from the fixed file path, allowing you to test your app without uploading a file every time.

Loading data from the database

Instead of uploading a file, you can load the data directly from a database. To do this, you can add a function to load the movie data from your recommendation graph in Neo4j Aura.

💡
You can sign up for free and create a recommendations instance in just a few minutes.

To use the database credentials in my application, I downloaded them as a config file and added them as environment variables. I used the variables in the .env file using dotenv.

All I needed at that point was the driver and a query. The results from the dict were then converted into a Pandas dataframe:

import neo4j
import os
from dotenv import load_dotenv

load_dotenv()

URI = os.getenv('NEO4J_URI')
AUTH = (os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))
QUERY = """
        MATCH (m:Movie) 
        RETURN m.movieId as id,m.title as title,m.budget as budget, m.countries[0] as country,
        m.imdbId as imdbId,m.imdbRating as rating,m.imdbVotes as votes,
        m.languages[0] as language,m.plot as plot,m.poster as poster,
				m.released as released,m.revenue as revenue,
        m.runtime as runtime,m.tmdbId as tmdbId,
        m.url as url,m.year as year,[(m)-[:IN_GENRE]->(g) | g.name][0] as genre
        LIMIT $rows
        """

def read_data(query, rows=1):
    with neo4j.GraphDatabase.driver(URI, auth=AUTH) as driver:
        records, summary, keys = driver.execute_query(query, {"rows":rows})
        return pd.DataFrame(records, columns=keys)


Next, I wanted to control the number of rows to load with an st.number_input field, but oddly, it was allowing fractional numbers. Nothing I couldn't fix, right? 🙂

Michael

How can I make a st.number_input take integer values only?

ChatGPT

st.number_input in Streamlit allows you to create an input widget for numbers. By default, it supports both integer and float values. To make the st.number_input accept integer values only, you need to set the step parameter to 1. This ensures that the input increments/decrements by an integer value.

Here's an example of a st.number_input that accepts only integer values:

import streamlit as st

# Using st.number_input with step=1 for integer values only
integer_input = st.number_input("Enter an integer:", min_value=0, max_value=100, value=50, step=1)

st.write("You entered:", integer_input)


In this example, the st.number_input is set to accept integer values ranging from 0 to 100, with a default value of 50. The step parameter is set to 1, ensuring that the input field only accepts integer values.

Cool, I could work with that. Now my data was coming from my database, neat.

Implementing caching to reduce data reloads

But I didn't want the data to reload on every refresh. I remembered seeing something about caching with Streamlit on Google. So, I asked ChatGPT about it.

Michael

Can you explain how to use st.cached on a database query function?

ChatGPT

st.cache is a Streamlit decorator that caches the results of a function, which can be helpful when you have expensive computations or database queries. When the function is called again with the same input arguments, the cached result is returned instead of recomputing the function. This can significantly speed up your Streamlit app.

To use st.cache with a database query function, you need to decorate the function with @st.cache. Here's an example:

First, let's assume you have a function to query data from a database.

Now, add the @st.cache decorator to the function:

import sqlite3
import pandas as pd
import streamlit as st

@st.cache  # Add the decorator here
def query_data_from_db(query: str, db_path: str) -> pd.DataFrame:
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df


By adding the @st.cache decorator, Streamlit will cache the result of the query_data_from_db() function, so subsequent calls with the same query and db_path arguments will return the cached result instead of querying the database again.

Keep in mind that st.cache works best with functions that have deterministic outputs (i.e., the output only depends on the input arguments). If your database contents change frequently or your query results may vary for other reasons, you may need to use the ttl (time-to-live) parameter to specify an expiration time for the cache.

For example, if you want the cache to expire after 300 seconds (5 minutes), you can modify the decorator like this:

@st.cache(ttl=300)  # Cache results for 300 seconds
def query_data_from_db(query: str, db_path: str) -> pd.DataFrame:
    # ...


With the ttl parameter, the cached result will be used for 300 seconds. After this time, the next call to the function will recompute the result and update the cache.

Learning about updated APIs from a documentation drop

Okay, that didn't work. Streamlit complained about st.cache. It was another case of the model training cutoff date. (Remember that this session was before the web-browsing extension was available.) So, I found the relevant doc and pasted it for ChatGPT to re-evaluate its advice.

Michael

st.cache is deprecated. Here are the new docs on how our example would change.

Advanced usage Controlling cache size and duration If your app runs for a long time and constantly caches functions, you might run into two problems:

The app runs out of memory because the cache is too large. Objects in the cache become stale, e.g., because you cached old data from a database. You can combat these problems with the TTL and max_entries parameters, available for both caching decorators.

The TTL (time-to-live) parameter

ttl sets a time to live on a cached function. If that time is up and you call the function again, the app will discard any old, cached values, and the function will be rerun. The newly computed value will then be stored in the cache. This behavior is useful for preventing stale data (problem 2) and the cache from growing too large (problem 1). Especially when pulling data from a database or API, you should always set a ttl so you are not using old data. Here's an example:

@st.cache_data(ttl=3600)  # 👈 Cache data for 1 hour (=3600 seconds)
def get_api_data():
 data = API.get(...)
 return data
Copy
star
Tip
You can also set TTL values using timedelta, e.g. ttl=datetime.timedelta(hours=1).

ChatGPT

Based on the new documentation, you can replace st.cache with st.cache_data and use the ttl and max_entries parameters for controlling cache size and duration. Here's how your example would change:

import sqlite3
import pandas as pd
import streamlit as st

@st.cache_data(ttl=300, max_entries=100)  # 👈 Updated decorator with ttl and max_entries parameters
def query_data_from_db(query: str, db_path: str) -> pd.DataFrame:
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df


In this example, the @st.cache_data decorator replaces the deprecated @st.cache decorator. The ttl parameter is set to 300 seconds (5 minutes) to specify the cache's expiration time and max_entries is set to 100 to limit the maximum number of entries in the cache.

Adding "real" data science—predictions with scikit-learn

In the second part of the live stream, I wanted to integrate scikit-learn to predict one feature of my movies dataset.

Michael

Now we want to try a prediction with scikit learn. How can I predict the movie budget from genre, year, and IMDb rating? What model should I use, how to do train/test split, and use the model for some sample predictions? Give me the minimal code with an explanation in the comments for a standalone Python file.

ChatGPT

To predict the movie budget from genre, year, and IMDB rating, you can use a regression model from scikit-learn. In this example, I'll use the RandomForestRegressor. The random forest is an ensemble learning method for classification and regression tasks that operate by constructing multiple decision trees at training time and outputting the mean prediction of the individual trees.

Here's a minimal standalone Python script that demonstrates how to train a RandomForestRegressor model split the dataset into training and testing sets, and make sample predictions:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Assuming you have a dataset in a CSV file named 'movies_data.csv'
data = pd.read_csv("movies_data.csv")

# Preprocess the 'genre' column using one-hot encoding
encoder = OneHotEncoder()
genres_encoded = encoder.fit_transform(data[["genre"]]).toarray()

# Create a DataFrame from the one-hot encoded genres and set column names
genres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names(["genre"]))

# Merge the one-hot encoded genres back into the original DataFrame
data = data.join(genres_encoded_df).drop("genre", axis=1)

# Define the features (X) and target (y)
X = data.drop("budget", axis=1)
y = data["budget"]

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model using the training data
model.fit(X_train, y_train)

# Predict the movie budgets for the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean squared error:", mse)

# Make sample predictions
sample_data = [
    {"year": 2022, "imdb_rating": 7.5, "genre": "Action"},
    {"year": 1995, "imdb_rating": 8.2, "genre": "Drama"},
]
sample_df = pd.DataFrame(sample_data)

# Preprocess the sample data (one-hot encoding for genre)
sample_genres_encoded = encoder.transform(sample_df[["genre"]]).toarray()
sample_genres_encoded_df = pd.DataFrame(sample_genres_encoded, columns=encoder.get_feature_names(["genre"]))
sample_df = sample_df.join(sample_genres_encoded_df).drop("genre", axis=1)

# Predict the movie budgets for the sample data
sample_predictions = model.predict(sample_df)
print("Sample predictions:", sample_predictions)


In this script:

The dataset is loaded from a CSV file named movies_data.csv.
The 'genre' column is preprocessed using one-hot encoding, which is necessary because scikit-learn models require numerical input.
The features (X) and target (y) are defined. In this case, the features are the genre, year, and IMDB rating columns; the target is the budget column.
The dataset is split into training (80%), and testing (20%) sets using train_test_split.
A RandomForestRegressor model is initialized and trained using the training data.
The model is used to predict the movie budgets for the test set, and the mean squared error (MSE) is calculated to evaluate the model's performance.
Resolving scikit-learn errors

That was a comprehensive explanation, especially with the inline comments. I was confident that I understood most of it. However, when I tried it out, I got an error message. 😱

Michael

How can we fix this?

File "/Users/mh/d/chatgpt-coding/streamlit-eda-ml/test.py", line 45, in <module>  genres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names(["genre"]))

ChatGPT

Apologies for the confusion. In scikit-learn version 0.22 and later, the method to retrieve feature names has changed from get_feature_names to get_feature_names_out. If you are using scikit-learn 0.22 or later, please update the line as follows:

genres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names_out(["genre"]))


This should resolve the issue. If you are still encountering errors, please let me know.

If our input data has many more genres than our prediction test, the one hot encoder generates fewer columns and complains that the feature names should match those passed during the fit.

I did not like that the proposal created multiple encoders for both the training and testing data. I wanted to simplify the code.

Michael

How can we reuse the genre encoder from the training data?

ChatGPT

To reuse the genre encoder from the training data, you should first fit the encoder on the training data only. After fitting the encoder on the training data, you can transform both the training and test data (or any new data) using the same encoder instance. This will ensure that the one-hot encoded columns generated for the test data (or new data) are consistent with the training data.

Here's an example of how to do this:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Assuming you have a dataset in a CSV file named 'movies_data.csv'
data = pd.read_csv("movies_data.csv")

# Define the features (X) and target (y)
X = data.drop("budget", axis=1)
y = data["budget"]

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the 'genre' column using one-hot encoding
encoder = OneHotEncoder()
encoder.fit(X_train[["genre"]])  # Fit the encoder on the training data

# Transform the 'genre' column for both training and test data
X_train_genres_encoded = encoder.transform(X_train[["genre"]]).toarray()
X_test_genres_encoded = encoder.transform(X_test[["genre"]]).toarray()

# Create DataFrames from the one-hot encoded genres and set column names
X_train_genres_encoded_df = pd.DataFrame(X_train_genres_encoded, columns=encoder.get_feature_names_out(["genre"]))
X_test_genres_encoded_df = pd.DataFrame(X_test_genres_encoded, columns=encoder.get_feature_names_out(["genre"]))

# Merge the one-hot encoded genres back into the original DataFrames
X_train = X_train.join(X_train_genres_encoded_df).drop("genre", axis=1)
X_test = X_test.join(X_test_genres_encoded_df).drop("genre", axis=1)

# Initialize the RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model using the training data
model.fit(X_train, y_train)

# Predict the movie budgets for the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean squared error:", mse)


In this example, the OneHotEncoder is fit on the training data (X_train) only. Then, it is used to transform both the training and test data (X_train and X_test). This ensures that the one-hot encoded columns generated for the test data are consistent with the training data, and you won't run into issues with mismatched feature names.

Addressing column order consistency

I ran into an issue when I tried to reuse the code. The column order in the test/train and prediction data frames didn't match. Oddly, ChatGPT didn't care about column names, which should've been sufficient to map the different column orders.

Michael

Is the order of columns important for the RandomForestRegressor?

ChatGPT

Yes, the order of columns is important for the RandomForestRegressor and, in general, for all scikit-learn models. When you train the model, it learns the relationship between the input features (columns) and the target variable. The trained model expects the same order of columns in the input data when making predictions as it saw during training.

If the order of columns in the input data is changed when making predictions, the model's predictions may be incorrect or completely off. So, it's crucial to maintain the same order of columns when making predictions as during the training process.

In the examples I provided earlier, the order of columns is maintained consistently for both training and test data. When using the OneHotEncoder, the column order is preserved by fitting the encoder on the training data only and then transforming both the training and test data using the same encoder instance.

Michael

Doesn't it use the column headers?

ChatGPT

Scikit-learn models do not use column headers (i.e., DataFrame column names) when fitting or making predictions. Instead, they operate on the underlying NumPy arrays or other data structures that store the actual data. This is why the order of columns is important when training and making predictions using scikit-learn models. The models don't "know" the column names; they only work with the positions of the features in the input data.

To ensure that the input data has the same order of columns when training and making predictions, you should preprocess your data consistently and maintain the same order of columns throughout your analysis. Using the same OneHotEncoder instance to transform both the training and test data, as shown in the previous examples, ensures that the columns are in the same order.

Fixing the billion squared error 😦

Upon testing, something was off. The fix resulted in a GIGANTIC mean square error (MSE). 🚨

Michael

Isn't this a bit high? Mean squared error: 505365615111096.06

ChatGPT

Yes, the mean squared error (MSE) you mentioned is indeed quite high. It could be due to several reasons, such as:

Insufficient or noisy data: The dataset might not have enough data points or might contain a lot of noise, which could make it challenging to learn the underlying relationship between the features and the target variable.
Model complexity: The RandomForestRegressor might not be the best model for this problem, or its hyperparameters might not be optimal. You could try different models, such as linear regression, support vector regression, or gradient boosting regression, or experiment with tuning the hyperparameters of the random forest model.
Feature engineering: The features used for prediction might not be sufficient or might require additional preprocessing or transformation. You could try adding more relevant features or apply different feature engineering techniques, such as feature scaling or transformation, to improve the model's performance.
Outliers: The dataset might contain extreme values or outliers that could negatively impact the model's performance. You could try removing or transforming outliers before training the model.

To improve the model's performance, you can try the following:

Investigate the data distribution and perform exploratory data analysis (EDA) to identify any data issues or patterns that might help in feature engineering or selecting a better model.
Try different models and compare their performances using cross-validation. This will help you select a model that generalizes well to unseen data.
Perform hyperparameter tuning for the selected model to find the best combination of hyperparameters that minimize the MSE.
Experiment with different feature engineering techniques to improve the model's performance.

Remember that every dataset and problem is different, and finding the best approach often involves trial and error.

My assumption that the Genre would be a good predictor of the budget was incorrect. As I discovered while using the EDA app to plot the budget against years and genres (reference the 1:30:00 mark on the video recording), the year was a far more reliable predictor.

Incorporating predictions into the app

Next, it was time to integrate this awesome prediction engine into the Streamlit app. Although I wasn't sure how it's typically done, I trusted that ChatGPT would make a sensible suggestion. It didn't matter where…as long as it worked. 😂🤷‍♂️

Michael

Can I add multiple screens or tabs to the Streamlit app?

ChatGPT

Yes, you can add multiple screens or tabs to a Streamlit app using the st.sidebar.selectbox or st.sidebar.radio widget. This allows users to switch between different views or functionalities within the same app.

Here's an example of how to create a multi-page Streamlit app using st.sidebar.selectbox:

import streamlit as st

# Define the pages in your app
pages = {
    "Page 1": page1,
    "Page 2": page2,
    "Page 3": page3,
}

# Function definitions for each page
def page1():
    st.title("Page 1")
    st.write("This is Page 1 of the multi-page app.")

def page2():
    st.title("Page 2")
    st.write("This is Page 2 of the multi-page app.")

def page3():
    st.title("Page 3")
    st.write("This is Page 3 of the multi-page app.")

# Create a sidebar with a selectbox to choose the page
selected_page = st.sidebar.selectbox("Choose a page", options=list(pages.keys()))

# Call the corresponding function to display the selected page
pages[selected_page]()


The user can select a page using the widget in the sidebar, and the corresponding function will be called to display the content of the selected page. You can add more pages by defining new functions and adding them to the pages dictionary.

💡
Okay, that worked, but as I learned later, that's not the recommended way of building multi-page Streamlit apps, which use separate Python files, one for each page. You can read more about this in the docs for multi-page apps.

🤦‍♂️ Something went wrong after I reincorporated the database data into my revamped Streamlit app! Seems I made a mistake.

Michael

data = data["rating","year","budget"]

What's wrong here?

KeyError: ('rating', 'year', 'budget')
Traceback:
File "/opt/homebrew/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 565, in _run_script
    exec(code, module.__dict__)
File "/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py", line 160, in <module>
    main()
File "/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py", line 157, in main
    pages[selected_page](data)
File "/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py", line 52, in predict
    data = data["rating","year","budget"]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
File "/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py", line 3807, in __getitem__
    indexer = self.columns.get_loc(key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
File "/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3804, in get_loc
    raise KeyError(key) from err

ChatGPT

The issue is with the way you're trying to select multiple columns from the DataFrame. Instead of using data["rating", "year", "budget"], you should use double brackets ([[]]) to select multiple columns.

Here's the correct way to do it:

data = data[["rating", "year", "budget"]]


By using double brackets, you're creating a new DataFrame with just the specified columns. The single brackets would try to access a single column with the given tuple as a key, which doesn't exist in the DataFrame, resulting in the KeyError.

It turned out I wasn't just a Streamlit noob, I was also one regarding DataFrame. That's the point of this live stream, right? 😂

Adding selection by name

Next, I wanted to preselect a logical value in the select boxes in the sidebar, but I couldn't seem to do it by column name. ChatGPT to the rescue! Again!

Michael

How can I set a default value for st.selectbox?

ChatGPT

To set a default value for st.selectbox in Streamlit, you can use the index parameter. The index parameter sets the default selected option based on its position (0-indexed) in the provided list of options.

Here's an example:

import streamlit as st

options = ["Option A", "Option B", "Option C"]
default_index = options.index("Option B")

selected_option = st.selectbox("Choose an option", options=options, index=default_index)

st.write(f"You selected {selected_option}")


In this example, the st.selectbox widget will have "Option B" as its default value. The index parameter is set to the position of "Option B" in the options list using the options.index("Option B") method.

Hmm, so I needed an index. What's the column index of a named column in a DataFrame?

Michael

How can I get the index of a value out of a pandas Index type?

ChatGPT

You can use the get_loc() method of the pandas Index object to find the index of a value. Here's an example:

import pandas as pd

# Create a sample DataFrame
data = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}
df = pd.DataFrame(data)

# Get the column index object
column_index = df.columns

# Find the index of a value in the Index object
value_to_find = 'B'
index_of_value = column_index.get_loc(value_to_find)

print(f"Index of '{value_to_find}': {index_of_value}")


In this example, we create a sample DataFrame df and get its column index object column_index. We then use the get_loc() method to find the index of the value 'B' in the column index. The output will be:

Index of 'B': 1

Implementing dataframe filters

Finally, I needed to sub-select data from the DataFrame for predictions. I knew I had to filter the data but didn't know the exact syntax.

Michael

How do I select rows of a DataFrame with certain values in certain columns?

ChatGPT

You can select rows of a DataFrame with certain values in specific columns by using boolean indexing. This allows you to filter the DataFrame based on conditions you specify for one or multiple columns.

Here's an example:

import pandas as pd

# Create a sample DataFrame
data = {
    "Name": ["Alice", "Bob", "Carol", "David"],
    "Age": [25, 30, 35, 40],
    "City": ["New York", "Los Angeles", "San Francisco", "New York"],
}
df = pd.DataFrame(data)

# Filter the DataFrame based on conditions
filtered_df = df[(df["Age"] >= 30) & (df["City"] == "New York")]

print(filtered_df)


In this example, we create a sample DataFrame df and filter it based on the conditions that the "Age" column should be greater than or equal to 30 and the "City" column should be "New York". The filtered DataFrame will contain only the rows that meet both conditions:

    Name  Age      City
3  David   40  New York


You can combine multiple conditions using the & (and) or | (or) operators, and use parentheses to group conditions as needed. Note that you should use the bitwise operators & and | instead of the logical operators and and or.

With that, the two-hour session came to an end. I was quite pleased with what I had learned and accomplished.

I started with no knowledge of Streamlit or scikit-learn. Within two hours, I had a functional app that pulled and displayed my own data and made basic predictions based on the columns I selected.

Wrapping up

That's a wrap on this coding adventure! Here's the finished code. I must say, I prefer coding with ChatGPT over GitHub CoPilot. It's more conversational and offers detailed explanations. What about you?

You can find me on Mastodon, BlueSky, Twitter, and Medium. I'd love to hear about your own pair programming experiences with LLMs.

Oh, and if you're building cool Streamlit (or other) apps with Neo4j, make sure to submit a talk or join us for our online developer conference "NODES" on October 26th. CfP is open till June 30th!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Editable dataframes are here! ✍️
https://blog.streamlit.io/editable-dataframes-are-here/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

By Lukas Masuch, William Huang and Johannes Rieke
Posted in Product, February 28 2023
How to use it
Advanced features
New docs page
Examples
Next up
Contents
Share this post
← All posts

Working with dataframes is at the heart of data science. If you’re like us, you probably load data from a CSV or a database, transform it in Pandas, fix it, transform it again, fix it again…and so on, until you’re happy. st.dataframe lets you visualize data instantly, but sometimes it’s not enough. You want to interact with it, not just look at a table!

Hence, we plan to release some major improvements for working with dataframes in the next few months. Today, we’re excited to launch…

Editable dataframes! 🎉

How to use it

Editable dataframes are supported via a new command, st.experimental_data_editor. It shows the dataframe in a table, similar to st.dataframe. But in contrast to st.dataframe, this table isn’t static. The user can click on cells and edit them. The edited data is then returned on the Python side.

Here’s an example:

edited_df = st.experimental_data_editor(df)
favorite_command = edited_df.loc[edited_df["rating"].idxmax()]["command"]
st.markdown(f"Your favorite command is **{favorite_command}** 🎈")


Try it out by double-clicking on a cell. 👆

⚠️
This feature is experimental, i.e. it might change at any time. We plan to de-experimentalize it in the next few months. See here for details.
Advanced features
Adding and deleting rows. Just set the parameter num_rows=”dynamic”, and users can add rows to the table or delete existing rows:

Copy-and-paste support that’s compatible with Google Sheets, Excel, and others:

Bulk-editing by dragging the handle on a cell (similar to Excel):

Easy access to edited data. No need to compare the old and new dataframe to get the difference. Just use st.experimental_data_editor together with session state to access all edits, additions, and deletions.
Support for additional data types. Let your users edit lists, tuples, sets, dictionaries, NumPy arrays, or Snowpark and PySpark dataframes. Most types are returned in their original format.
Automatic input validation, e.g., number cells don’t allow characters.
Rich editing experience, e.g., checkboxes for booleans and dropdowns for categorical data. The date picker for datetime cells is coming soon!
New docs page

To support this release, we created a brand-new docs page on dataframes. It explains everything you need to know about st.dataframe and the new st.experimental_data_editor, including all of the sweet features you saw above. The best part is, it comes with lots of interactive examples! 🕹️

And of course, we also added st.experimental_data_editor to our API reference. Check out all of its parameters here.

Examples

Excited to jump in? Check out our demo app. It shows examples of using the data editor in practice. From guessing idioms to matrix operations over custom convolution filters, you can do a lot with this new feature.

Next up

Editable dataframes are only the beginning! 🌱

We have a bunch of new features for st.dataframe and st.experimental_data_editor in the pipeline for the next few months: showing images, clickable URLs in tables, letting the users select rows, and more. You can always follow our progress on roadmap.streamlit.app!

We’re excited to see what you build. Let us know your feedback in the comments below.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

LangChain tutorial #1: Build an LLM-powered app in 18 lines of code
https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain tutorial #1: Build an LLM-powered app in 18 lines of code

A step-by-step guide using OpenAI, LangChain, and Streamlit

By Chanin Nantasenamat
Posted in Tutorials, May 31 2023
What is LangChain?
Step 1. Get an OpenAI API key
Step 2. Set up the coding environment
Local development
Cloud development
Step 3. Build the app
4. Deploy the app
Wrapping up
Contents
Share this post
← All posts

In the dynamic landscape of artificial intelligence (AI), generative AI and large language models (LLMs) have emerged as game-changers, revolutionizing how we process and understand massive amounts of text data. You can use LLMs for text generation, sentiment analysis, question answering, text summarization, document translation, document classification, and much more.

If you're captivated by the transformative powers of generative AI and LLMs, then this LangChain how-to tutorial series is for you. As it progresses, it’ll tackle increasingly complex topics.

In this first part, I’ll introduce the overarching concept of LangChain and help you build a very simple LLM-powered Streamlit app in four steps:

Get an OpenAI API key
Set up the coding environment
Build the app
Deploy the app


🦜
If you’d rather play with it right away, here is the app and the code (the app is using a placeholder API key). And read our quickstart tutorial here to get started!

But first, let's take a deeper look at LangChain.

What is LangChain?

LangChain is a framework that uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date, it has accumulated 41,900 stars on GitHub and has over 800 contributors.

At a high level, LangChain connects LLM models (such as OpenAI and HuggingFace Hub) to external sources like Google, Wikipedia, Notion, and Wolfram. It provides abstractions (chains and agents) and tools (prompt templates, memory, document loaders, output parsers) to interface between text input and output. LLM models and components are linked into a pipeline "chain," making it easy for developers to rapidly prototype robust applications. Simply put, Langchain orchestrates the LLM pipeline.

LangChain's power lies in its six key modules:

Model I/O: Facilitates the interface of model input (prompts) with the LLM model (closed or open-source) to produce the model output (output parsers)
Data connection: Enables user data to be loaded (document loaders), transformed (document transformers), stored (text embedding models and vector stores) and queried (retrievers)
Memory: Confer chains or agents with the capacity for short-term and long-term memory so that it remembers previous interactions with the user
Chains: A way to combine several components or other chains in a single pipeline (or “chain”)
Agents: Depending on the input, the agent decides on a course of action to take with the available tools/data that it has access to
Callbacks: Functions that are triggered to perform at specific points during the duration of an LLM run


📖
Check out the LangChain documentation for further information on each of these modules.

Now that you're familiar with LangChain, let's look at the app's functionality.

Copy/paste your API key (see how to get it in Step 1 below), then type in "What are the three key pieces of advice for learning how to code?" and click Submit:

You should see the response appear in the blue box:

This is how it works under the hood:

Essentially, you'll be using OpenAI (the LLM), LangChain (the LLM framework), and Streamlit (the web framework).

Let's get started!

Step 1. Get an OpenAI API key

First, get your own OpenAI API key:

Go to https://platform.openai.com/account/api-keys.
Click on the + Create new secret key button.
Enter an identifier name (optional) and click on the Create secret key button.
Copy the API key to be used in this tutorial (the key shown below was already revoked):
Step 2. Set up the coding environment
Local development

To set up a coding environment locally, make sure that you have a functional Python environment (e.g. Python >3.7) and install the following three Python libraries:

pip install streamlit openai langchain 

Cloud development

You can also code directly on the Streamlit Community Cloud. Just use the Streamlit app template (read this blog post to get started).

Next, add the three prerequisite Python libraries in the requirements.txt file:

streamlit
openai
langchain

Step 3. Build the app

Now comes the fun part. Let's build the app!

It's only 18 lines of code:

import streamlit as st
from langchain.llms import OpenAI

st.title('🦜🔗 Quickstart App')

openai_api_key = st.sidebar.text_input('OpenAI API Key')

def generate_response(input_text):
  llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)
  st.info(llm(input_text))

with st.form('my_form'):
  text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')
  submitted = st.form_submit_button('Submit')
  if not openai_api_key.startswith('sk-'):
    st.warning('Please enter your OpenAI API key!', icon='⚠')
  if submitted and openai_api_key.startswith('sk-'):
    generate_response(text)

To start, create the streamlit_app.py file and import the two prerequisite libraries:

streamlit, a low-code framework used for the front end to let users interact with the app.
langchain, a framework for working with LLM models.
import streamlit as st
from langchain.llms import OpenAI


Next, display the app's title "🦜🔗 Quickstart App" using the st.title() method:

st.title('🦜🔗 Quickstart App')


The app takes in the OpenAI API key from the user, which it then uses togenerate the responsen.

openai_api_key = st.sidebar.text_input('OpenAI API Key')

Next, define a custom function called generate_response(). It takes a piece of text as input, uses the OpenAI() method to generate AI-generated content, and displays the text output inside a blue box using st.info():

def generate_response(input_text):
  llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)
  st.info(llm(input_text))


Finally, use st.form() to create a text box (st.text_area()) for accepting user-provided prompt input. Once the user clicks the Submit button, the generate-response() function is called with the prompt input variable (text) as an argument.

This creates AI-generated content:

with st.form('my_form'):
  text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')
  submitted = st.form_submit_button('Submit')
  if not openai_api_key.startswith('sk-'):
    st.warning('Please enter your OpenAI API key!', icon='⚠')
  if submitted and openai_api_key.startswith('sk-'):
    generate_response(text)
4. Deploy the app

Deploying the app is super simple:

Create a GitHub repository for the app.
In Streamlit Community Cloud, click the New app button, then specify the repository, branch, and main file path.
Click the Deploy! button.

And here it is!

Wrapping up

Now you know how to get your own OpenAI API key, set up your coding environment, create your first LLM-powered app with LangChain and Streamlit, and deploy it to the cloud. Check out the LLM gallery for inspiration and share your creation with the community. I can't wait to see what you'll build!

In future posts, I'll show you the superpowers of other LangChain modules (e.g., prompt templates, memory, indexes, chains, agents, and callbacks). If you have any questions, please post them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy app-building! 🦜🔗

P.S. This post was made possible thanks to the technical review by Joshua Carroll and editing by Ksenia Anske.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

LangChain tutorial #2: Build a blog outline generator app in 25 lines of code
https://blog.streamlit.io/langchain-tutorial-2-build-a-blog-outline-generator-app-in-25-lines-of-code/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain tutorial #2: Build a blog outline generator app in 25 lines of code

A guide on conquering writer’s block with a Streamlit app

By Chanin Nantasenamat
Posted in LLMs, June 7 2023
What's a prompt template?
Prompt template in action
App overview
Step 1. Get an OpenAI API key
Step 2. Set up the coding environment
Local development
Cloud development
Step 3. Build the app
Step 4. Deploy the app
Wrapping up
Contents
Share this post
← All posts

In LangChain tutorial #1, you learned about LangChain modules and built a simple LLM-powered app. The app took input from a text box and passed it to the LLM (from OpenAI) to generate a response. But it didn’t leverage other LangChain modules.

In this post, I’ll show you how to use LangChain’s prompt template and build a blog outline generator app in four simple steps:

Get an OpenAI API key
Set up the coding environment
Build the app
Deploy the app


🦜
Want to skip reading? Here's the demo app and the repo code.

Before we get to building the app, let's talk about…

What's a prompt template?

A prompt is an instruction given to an LLM. You can use a prompt template to generate prompts on-the-fly. It has two parts:

A static descriptive text part (hard-coded in the code).
A dynamically generated part (determined by the user).

Use it to pre-define the app's scope so it performs a specific task as a function of user instructions (e.g. generates prompts in a reproducible manner).

Here is an example of a prompt template:

'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'

The text As an experienced data scientist and technical writer, generate an outline for a blog about is the static component, and the {topic} is the dynamic component determined by the user (e.g. a blog topic they're interested in).

Prompt template in action

To implement the prompt template, use LangChain's PromptTemplate() function.

It accepts the following input arguments:

input_variables allows you to accept user-provided values comprising the prompt template's dynamic component.
template is the static component of the prompt template.

Here is what it'll look like:

template = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'
prompt = PromptTemplate(input_variables=['topic'], template=template)



📖
Check out the LangChain documentation on prompt template for further information.
App overview

The app solves one of the most time-consuming problems for technical writers...

Writer's block! ✍️

It works like this:

Its workflow consists of three simple steps:

Use Streamlit's st.text_input() function to accept the user-provided "Topic" as input.
Combine the "topic" and the prompt instructions using PromptTemplate() to create the final prompt.
Use the final prompt to generate a response.

Go ahead and try it:

Here is what it should look like:

Now, let's build the app!

Step 1. Get an OpenAI API key

Hop over to the LangChain tutorial #1 for instructions on how to get an OpenAI API key.

Step 2. Set up the coding environment
Local development

To set up a programming workspace on your own system, install Python version 3.7 or higher. Then install these Python libraries:

pip install streamlit openai langchain

Cloud development

You can also create the app on the Streamlit Community Cloud. To get started, use the Streamlit app template (read more here).

Next, include the three prerequisite Python libraries in the requirements.txt file:

streamlit
openai
langchain

Step 3. Build the app

The code for building the app is only 25 lines long (23 without the two comments):

import streamlit as st
from langchain.llms import OpenAI
from langchain import PromptTemplate

st.set_page_config(page_title="🦜🔗 Blog Outline Generator App")
st.title('🦜🔗 Blog Outline Generator App')
openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')

def generate_response(topic):
  llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)
  # Prompt
  template = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'
  prompt = PromptTemplate(input_variables=['topic'], template=template)
  prompt_query = prompt.format(topic=topic)
  # Run LLM model and print out response
  response = llm(prompt_query)
  return st.info(response)

with st.form('myform'):
  topic_text = st.text_input('Enter keyword:', '')
  submitted = st.form_submit_button('Submit')
  if not openai_api_key.startswith('sk-'):
    st.warning('Please enter your OpenAI API key!', icon='⚠')
  if submitted and openai_api_key.startswith('sk-'):
    generate_response(topic_text)

First, create the streamlit_app.py file to house the full code snippet above. Or follow along and add the code block by block.

In the first few lines, import the necessary Python libraries, such as Streamlit and specific Langchain methods OpenAI and PromptTemplate:

import streamlit as st
from langchain.llms import OpenAI
from langchain import PromptTemplate


Next, give the app a page title for display on the browser window and in-app:

st.set_page_config(page_title="🦜🔗 Blog Outline Generator App")
st.title('🦜🔗 Blog Outline Generator App')

A text box is created to accept OpenAI credentials:

openai_api_key = st.sidebar.text_input('OpenAI API Key')


A custom function generates an LLM response based on the user's provided "topic" of interest. An instance of the LLM model is created using OpenAI(). This is followed by creating a dynamically created prompt stored in the prompt_query variable. This prompt combines static and dynamic components.

The LLM model accepts the prompt as input for generating a response—the blog outline. It's displayed in a blue box using st.info():

def generate_response(topic):
  llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)
  # Prompt
  template = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'
  prompt = PromptTemplate(input_variables=['topic'], template=template)
  prompt_query = prompt.format(topic=topic)
  # Run LLM model
  response = llm(prompt_query)
  return st.info(response)

Finally, st.form() is used as an app logic to generate the blog outline only after correctly entering the OpenAI API key, filling the text box with the blog topic, and clicking the Submit button:

with st.form('myform'):
  topic_text = st.text_input('Enter keyword:', '')
  submitted = st.form_submit_button('Submit')
  if not openai_api_key.startswith('sk-'):
    st.warning('Please enter your OpenAI API key!', icon='⚠')
  if submitted and openai_api_key.startswith('sk-'):
    generate_response(topic_text)
Step 4. Deploy the app

After creating your app, you can deploy it in three steps:

Create a GitHub repository for your app.
Go to the Community Cloud, click the New app button, and select the repository, branch, and app file.
Click the Deploy! button.

And voilà! You're done.

Wrapping up

In this post, you've learned how to use the LLM model with the LangChain prompt template to build a blog outline generator. You can customize your app by adjusting the prompt. Check out the LLM gallery for more ideas and inspiration.

If you have any questions, please post them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.

Happy app-building! 🦜🔗

P.S. This post was made possible thanks to the technical review by Tim Conkling and editing by Ksenia Anske.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

LangChain tutorial #3: Build a Text Summarization app
https://blog.streamlit.io/langchain-tutorial-3-build-a-text-summarization-app/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
LangChain tutorial #3: Build a Text Summarization app

Explore the use of the document loader, text splitter, and summarization chain

By Chanin Nantasenamat
Posted in LLMs, June 13 2023
What is text summarization?
App overview
Step 1: Get an OpenAI API key
Step 2: Set up the coding environment
Local development
Cloud development
Step 3. Building the app
Code overview
Importing prerequisite libraries
AI-generated response
App logic
Step 4. Deploy the app
Wrapping up
Contents
Share this post
← All posts

Large language models (LLMs) are trained on massive amounts of text data using deep learning methods. The resulting model can perform a wide range of natural language processing (NLP) tasks, broadly categorized into seven major use cases: classification, clustering, extraction, generation, rewriting, search, and summarization (read more in Meor Amer posts here and here).

In the previous LangChain tutorials, you learned about two of the seven utility functions: LLM models and prompt templates. In this tutorial, we’ll explore the use of the document loader, text splitter, and summarization chain to build a text summarization app in four steps:

Get an OpenAI API key
Set up the coding environment
Build the app
Deploy the app


🦜
Want to test it out? Here's the demo app and the repo code.
What is text summarization?

Text summarization involves creating a summary of a source text using natural language processing. This is useful for condensing long-form text, audio, or video into a shorter, more digestible form that still conveys the main points. Examples include news articles, scientific papers, podcasts, speeches, lectures, and meeting recordings.

There are two main types of summarization:

Extractive summarization. This type identifies and extracts key phrases or sentences (i.e., excerpts) from the source text and combines them into a summary. It leaves the original text unchanged and only selects the important parts.
Abstractive summarization. This type involves understanding the main ideas in the source text and creating a new summary that expresses those ideas in a fresh and condensed way (i.e., paraphrasing). It's more complex because it requires a deeper understanding of the source text and the ability to convey the same information in fewer words.


🔗
Check out the LangChain documentation on summarization.
App overview

At a high level, the app's workflow is relatively simple:

The user submits an input text to be summarized into the Streamlit frontend.
The app pre-processes the text by splitting it into several chunks, creating documents for each chunk, and applying the summarization chain with the help of the LLM model.
After a few moments, the summarized text is displayed in the app.

Let's take a closer look at the underlying details for when (1) the input text is submitted for processing by the app and (2) an LLM-generated response is created as summarized text.

Input. The input text is first split into several chunks of text using CharacterTextSplitter(), followed by the creation of documents for each text split via Document().
Generated Output. The processed text will then serve as input, along with the LLM model, to the load_summarize_chain() function. The text will be transformed into a concise form as summarized text.

Here is the app in action:

Try it for yourself:

Now, let's get to building!

Step 1: Get an OpenAI API key

Read how to obtain an OpenAI API key in LangChain Tutorial #1.

Step 2: Set up the coding environment
Local development

To set up a local coding environment, ensure that you have Python version 3.7 or higher installed, then install the following Python libraries:

pip install streamlit langchain openai tiktoken

Cloud development

You can also set up your app on the cloud by deploying to the Streamlit Community Cloud. To get started, use this Streamlit app template (read more about it here).

Next, include the three prerequisite Python libraries in the requirements.txt file:

streamlit
langchain
openai
tiktoken

Step 3. Building the app
Code overview

Before diving deeper into the code walkthrough, let's take a high-level overview of the code. It can be implemented in just 38 lines:

import streamlit as st
from langchain import OpenAI
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain

def generate_response(txt):
    # Instantiate the LLM model
    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)
    # Split text
    text_splitter = CharacterTextSplitter()
    texts = text_splitter.split_text(txt)
    # Create multiple documents
    docs = [Document(page_content=t) for t in texts]
    # Text summarization
    chain = load_summarize_chain(llm, chain_type='map_reduce')
    return chain.run(docs)

# Page title
st.set_page_config(page_title='🦜🔗 Text Summarization App')
st.title('🦜🔗 Text Summarization App')

# Text input
txt_input = st.text_area('Enter your text', '', height=200)

# Form to accept user's text input for summarization
result = []
with st.form('summarize_form', clear_on_submit=True):
    openai_api_key = st.text_input('OpenAI API Key', type = 'password', disabled=not txt_input)
    submitted = st.form_submit_button('Submit')
    if submitted and openai_api_key.startswith('sk-'):
        with st.spinner('Calculating...'):
            response = generate_response(txt_input)
            result.append(response)
            del openai_api_key

if len(result):
    st.info(response)

Importing prerequisite libraries

As always, we'll start by importing the necessary libraries:

import streamlit as st
from langchain import OpenAI
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain


AI-generated response

Next, we'll create a custom function generate_response(). It generates a response based on a given text input txt. For our use case, the input text will be the full text that needs to be summarized, and the output text will be the summarized version:

def generate_response(txt):
    # Instantiate the LLM model
    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)
    # Split text
    text_splitter = CharacterTextSplitter()
    texts = text_splitter.split_text(txt)
    # Create multiple documents
    docs = [Document(page_content=t) for t in texts]
    # Text summarization
    chain = load_summarize_chain(llm, chain_type='map_reduce')
    return chain.run(docs)


This function performs four key tasks:

It instantiates the LLM model, using the LLM model from OpenAI in this example. You can substitute this with any other LLM model of your choice. The API key is defined by the open_api_key parameter, which also takes in the OpenAI API key stored in the openai_api_key variable specified by the user via the st.text_input() method (implemented later in a subsequent code block).
The text is then split using the split_text() method from CharacterTextSplitter().
A document is created for each of the text splits using list comprehension ([Document(page_content=t) for t in texts]).
The summarize chain (load_summarize_chain()) is defined and assigned to the chain variable, applied to the documents created above, and stored in the docs variable via the run() method.
App logic

Let's define the app elements.

First, we'll set the page title using st.set_page_config() and the in-app title using st.title():

# Page title
st.set_page_config(page_title='🦜🔗 Text Summarization App')
st.title('🦜🔗 Text Summarization App')


The app will accept text input via st.text_area()—assigned to the txt_input variable.

Before defining the form, we'll create an empty list called result to later store the AI-generated response.

In this tutorial, the form is slightly different from previous ones. We clear the API key text box using the clear_on_submit=True parameter after an AI-generated response has been successfully performed.

After the user clicks the Submit button, a Calculating... spinner is displayed. Once an AI-generated response is made, the results are stored in the result list created earlier. Then the API key is deleted, and the text output is printed via st.info(response):

# Text input
txt_input = st.text_area('Enter your text', '', height=200)

# Form to accept user's text input for summarization
result = []
with st.form('summarize_form', clear_on_submit=True):
    openai_api_key = st.text_input('OpenAI API Key', type = 'password', disabled=not txt_input)
    submitted = st.form_submit_button('Submit')
    if submitted and openai_api_key.startswith('sk-'):
        with st.spinner('Calculating...'):
            response = generate_response(txt_input)
            result.append(response)
            del openai_api_key

if len(result):
    st.info(response)


Note that in this tutorial, the app will clear any memory of the API key as an added security measure to prevent lingering memory of the key.

Step 4. Deploy the app

Once the app is created, it can be deployed in three simple steps:

Create a GitHub repository for the app.
Go to Streamlit Community Cloud, click on the New app button, and select the repository, branch, and app file.
Click on the Deploy! button.

That's it! Your app will be up and running in no time.

Wrapping up

You learned how to build a text summarization app using LangChain and Streamlit. It involved using Streamlit as the front-end to accept input text, processing it with LangChain and its associated LLM utility functions, and displaying the LLM-generated response.

For ideas and inspiration, check out the LLM gallery. I can't wait to see what you build. Please let me know in the comments below or contact me on Twitter at @thedataprof or on LinkedIn. You can also find me on the official Streamlit YouTube channel and my personal YouTube channel, Data Professor.

Happy Streamlit-ing! 🎈

P.S. This post was made possible thanks to the technical review by Mihal Nowotka and editing by Ksenia Anske.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
8 tips for securely using API keys

How to safely navigate the turbulent landscape of LLM-powered apps

Tutorials
by
Chanin Nantasenamat
,
May 19 2023
Create an animated data story with ipyvizzu and Streamlit

A tutorial on using ipyvizzu and ipyvizzu-story

Advocate Posts
by
Peter Vidos
,
April 20 2023
AI talks: ChatGPT assistant via Streamlit

Create your own AI assistant in 5 steps

Advocate Posts
by
Dmitry Kosarevsky
,
April 18 2023
Detecting fake images with a deep-learning tool

7 steps on how to make Deforgify app

Advocate Posts
by
Kanak Mittal
,
April 11 2023
Building GPT Lab with Streamlit

12 lessons learned along the way

LLMs
by
Dave Lin
,
April 6 2023
Building an Instagram hashtag generation app with Streamlit

5 simple steps on how to build it

Advocate Posts
by
William Mattingly
,
March 29 2023
Hackathon 101: 5 simple tips for beginners

Prepare to win your first hackathon!

Tutorials
by
Chanin Nantasenamat
,
March 16 2023
Create a search engine with Streamlit and Google Sheets

You’re sitting on a goldmine of knowledge!

Advocate Posts
by
Sebastian Flores Benner
,
March 14 2023
10 most common explanations on the Streamlit forum

A guide for Streamlit beginners

Advocate Posts
by
Debbie Matthews
,
March 9 2023
Building a PivotTable report with Streamlit and AG Grid

How to build a PivotTable app in 4 simple steps

Advocate Posts
by
Pablo Fonseca
,
March 7 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

st.status: Visualize your app’s processes
https://blog.streamlit.io/st-status-visualize-your-apps-processes/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
st.status: Visualize your app’s processes

Rich context for users and more control for developers

By Joshua Carroll
Posted in Product, September 7 2023
Step-by-step transparency, in real-time
Flexible interaction to validate results
What's next
Contents
Share this post
← All posts
✅
TL;DR: Replace long app wait times and shed light on the “black box” of data processing with st.status. Play with our demo app to see how it works.

Long-running apps like LLM agents rarely show you their inner workings out of the box. On top of that, if a response takes too long to generate, users get impatient and leave. Not ideal!

Introducing: st.status

If you're not always confident in your model's output, how do you inspect the intermediate steps and chain of thought to verify results?  A few months ago, we provided a targeted solution by integrating with LangChain, using their callback system.

Now you can add st.status to any interactive or API-powered app to:

Animate its "under-the-hood" processes such as API calls or data retrieval.
See step-by-step logic to understand what went wrong (or validate what went right).
Allow users to engage with your app, rather than experiencing a blank page.

See how it works in our demo app! Choose any of the 8 different animations below to pair with your app operations. Check out the docs for more detail.




Let's look at two examples to see it in action.

Step-by-step transparency, in real-time

With st.status, every process step is defined, broken out, and animated. The app viewer can expand the status to check the details or leave it collapsed to focus on the final output.

Unlike st.spinner, the intermediate steps remain available to inspect even after the process completes.

https://release126.streamlit.app/st.status_demo
Flexible interaction to validate results

This can be particularly helpful to validate results from LLM-based apps.

LLMs aren't perfect. Their intelligence relies on the data sets they are trained on, which could be incomplete or contain misinformation. The LLM attempts to generate a plausible response to a user's prompt, but if it reaches the boundaries of its knowledge base, it can take liberties. This phenomenon causes an LLM to "hallucinate." If the user can't quite tell if the model is correct, or is embellishing a result, they quickly lose trust.

With st.status, the context and intermediate steps are available so users can validate the output logic:

https://release126.streamlit.app/LangChain_demo
What's next

This flexible framework gives you a higher degree of control to up-level your app’s user experience, and easily integrate custom components. It’s worth the extra lines of code!

Help us raise the bar with new (and refined) UI improvements. What additional transparency features would you like to see next?

Let us know in the comments below or on Discord.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing Streamlit to the Polish Python community
https://blog.streamlit.io/introducing-streamlit-to-the-polish-python-community/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing Streamlit to the Polish Python community

My Streamlit presentation at PyWaW #103

By Michał Nowotka
Posted in Product, April 4 2023
What motivated me
How the translation process works
My presentation at PyWaW #103
We're hiring!
Contents
Share this post
← All posts

Cześć miłośnicy Pythona! Hello, Python lovers! 👋

I'm Michał Nowotka, an engineering manager on the Streamlit team at Snowflake. Today I'd like to share with you an amazing adventure of promoting Streamlit to the Polish Python community. It started with creating a Polish translation of the 30 Days of Streamlit challenge and was followed by a talk at PyWaw, a popular Python enthusiast meetup in Warsaw.

What motivated me

Although English is a truly international language and a lingua franca for all professional fields, such as software development and machine learning, there are still advantages to investing time in translating content into a less widespread language.

I found three reasons to move forward with the Polish translation.

Reason 1. Many people still find it faster and more convenient to familiarize themselves with new technologies by reading about them in their native language, allowing them to focus on the technical complexities rather than the linguistic nuances.

Reason 2. Many people don't know a foreign language at the beginning of their programming adventure. I teach Python at some of the most popular Polish bootcamps and have experienced this firsthand. This is also true for some Polish kids who are enthusiastic about programming but often aren't fluent in English.

Reason 3. The 30 Days of Streamlit challenge has already been translated into seven languages! They include:

Bengali
Chinese
Spanish
French
Hindi
Portuguese
Russian

So, following the words of the famous Polish Renaissance poet Mikołaj Rej…

A niechaj narodowie wżdy postronni znają, iż Polacy nie gęsi, iż swój język mają. Let it by all and sundry foreign nations be known that Poles speak not Anserine but a tongue of their own.

…I decided that the Polish community couldn't miss this opportunity and should have its own translation.

How the translation process works

Creating a new language version of the #30DaysOfStreamlit is a multi-step process. Roughly, you need to follow these six steps:

Fork the original repository
Translate the Markdown files, including the comments and the strings in the code snippets
Translate some labels in the app's UI elements
Translate additional assets like CSV file headers
Run the app locally to see if it works, check for any untranslated strings, and do a final proofreading
Publish the repository and deploy the app

Many computer-assisted translation tools can speed up the process considerably. However, some customization is required to make the content feel natural.

I found the process a little too involved and tedious. I'd love to brainstorm how we can offer better support for translation and internationalization—let me know in the comments below. And feel free to translate the 30 Days of Streamlit into your language if you don't see it on the list above!

My presentation at PyWaW #103

Although I have some experience with public speaking, having given a talk on consistent and locality-sensitive hashing techniques at PyWaW #77, I was a little nervous. Especially since I decided to do a live demo instead of using slides. But everything went smoothly, and the presentation generated a lot of interest, so I spent the rest of the evening talking about Streamlit over a beer. 🍺

You can watch it here:

And you can see the demo at stonks.streamlit.app (we chose that name together with the meetup attendees). You can get the code used to build the demo from this gist. My demo was heavily inspired by Chanin Nantasenamat's (@Data Professor) Streamlit video tutorial.

We're hiring!

Thanks for reading my post! I hope you enjoyed it. And if you're located in Warsaw, have 6+ years of software development experience, and are proficient in Python and/or TypeScript, come join my team. I'm looking for a Senior Software Engineer to help take Streamlit to the next level as a Technical Lead in our Snowflake's Warsaw office. You can apply here. 🙂

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Michael Hunger - Streamlit
https://blog.streamlit.io/author/michael/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Michael Hunger
1 post
Building a Streamlit and scikit-learn app with ChatGPT

Catching up on coding skills with an AI assistant

LLMs
by
Michael Hunger
,
June 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Learn to build your own Notion AI powered chatbot
https://blog.streamlit.io/build-your-own-notion-chatbot/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build your own Notion chatbot

A step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit

By Logan Vendrix
Posted in LLMs, September 14 2023
An overview of the app
Step-by-step tutorial
1. Project structure and initialization
2. Document ingestion
3. Query
4. Chatbot application
Congratulations! 🥳
Contents
Share this post
← All posts
🤖
TL;DR: Learn how to create a chatbot based on Notion content using LangChain, OpenAI, FAISS, and Streamlit to interact with your app in Notion (GitHub repo).

Do you love Notion? Same here. But do you sometimes find it hard to quickly locate the right information in Notion? I faced the same problem at my company (we store literally everything in Notion). So I decided to build a Notion chatbot.

It was inspired by Harrison Chase’s app, and attempts to improve it by:

🔪 Using specific markdown characters for better content splitting
🧠 Adding memory to the bot
💬 Using Streamlit as the front-end chat app with the new chat features
📣 Embedding the Streamlit app into a Notion page

Add your OpenAI Key to play with the app!




Let’s walk through how you can build your own Notion chatbot!

An overview of the app

Step-by-step tutorial
1. Project structure and initialization

Let's start by examining the project structure and installing the necessary dependencies. And don’t forget to get your OpenAI API key and duplicate a public Notion page (to use as a foundation).

1.1 Project structure

The project structure of notion-chatbot consists of the following:

.streamlit/secrets.toml: stores your OpenAI API key.
faiss_index: a FAISS index (vector database) that stores all the vectors.
notion_content: a folder containing the Notion content in markdown files.
.gitignore: ignores your OpenAI API key and Notion content.
app.py: the script for the Streamlit chat application.
ingest.py: the script used to convert Notion content to vectors and store them in a vector index.
utils.py: the script used to create a Conversation Retrieval Chain.
requirements.txt: a file containing the necessary packages to deploy to Streamlit Community Cloud.

You’ll create each file step-by-step, so there is no need to create them all at once.

Now, let's initialize the project!

1.2 Project initialization

Start by creating a project folder notion-chatbot
Create a new environment and install the required dependencies


pip install streamlit langchain openai tiktoken faiss-cpu



Create the .gitignore file to specify which files not to track


# .gitignore

notion_content/
.streamlit/



Go to OpenAI and get your API key



Create the folder .streamlit
In .streamlit, create the file secrets.toml to store your OpenAI API key as follows


>>> secrets.toml

OPENAI_API_KEY = 'sk-A1B2C3D4E5F6G7H8I9J'



Use Blendle Employee Handbook as your knowledge base

If you don’t have a Notion account, create it here. It’s free!

Select Duplicate on the top-right corner to duplicate it into your Notion

2. Document ingestion

To convert all content from your Notion pages into numerical representations (vectors), use LangChain to split the text into smaller chunks that can be processed by OpenAI's embedding model. The model will convert the text chunks into vectors, which you’ll then store in a vector database.

2.1 Export your Notion content

Go to the main Notion page of the Blendle Employee Handbook.
In the top right corner, click on the three dots.
Choose Export
Select Markdown and CSV for the Export Format
Select Include subpages
Save the file as notion_content.zip
Unzip the folder
Place the notion_content folder into your notion-chatbot project folder


🤖
You can also get your Notion content using Notion's API. To keep it simple, just export the content manually.

Great! You should now have all the Notion content as .md files in the notion_content folder within your notion-chatbot project folder.

2.1 Convert Notion content to vectors

To use the content of your Notion page as the knowledge base of your chatbot, convert all the content into vectors and store them. To do this, use LangChain, an OpenAI embedding model, and FAISS.

Open your project folder in your favorite IDE and create a new file called ingest.py:

#ingest.py

import streamlit as st
import openai
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# Load OpenAI API key
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Load the Notion content located in the folder 'notion_content'
loader = NotionDirectoryLoader("notion_content")
documents = loader.load()

# Split the Notion content into smaller chunks
markdown_splitter = RecursiveCharacterTextSplitter(
    separators=["#","##", "###", "\
\
","\
","."],
    chunk_size=1500,
    chunk_overlap=100)
docs = markdown_splitter.split_documents(documents)

# Initialize OpenAI embedding model
embeddings = OpenAIEmbeddings()

# Convert all chunks into vectors embeddings using OpenAI embedding model
# Store all vectors in FAISS index and save to local folder 'faiss_index'
db = FAISS.from_documents(docs, embeddings)
db.save_local("faiss_index")

print('Local FAISS index has been successfully saved.')


Let's go over the code:

Load the OpenAI API key stored in .streamlit/secrets.toml.
Load the Notion content located in the notion_content folder using NotionDirectoryLoader.
Split the content into smaller text chunks using RecursiveCharacterTextSplitter.

There are different ways to split text.

Your Notion content consists of markdown files with headings (# for H1, ## for H2, ### for H3), so split on those specific characters. This ensures that you split the content at the best place between paragraphs, and not between the sentences of the same paragraph. If the split can't be done on headings, it’ll try to split on the characters \
\
, \
, . that separate sentences. RecursiveCharacterTextSplitter follows the order of the list of characters you provide, meaning it’ll use the next character in the list until the chunks are small enough.

Use a chunk size of 1500 with an overlap of 100 (feel free to experiment with different values):

Convert each text chunk into a vector using the OpenAI embedding model.
Store all the vectors in a FAISS index.

With your Notion content now converted to vectors and stored in a vector database, let's explore how you can interact with them!

3. Query

To find an answer to the user's question, convert it into a vector using the same embedding model as before. This vector is then matched with similar vectors in the vector database created earlier. Pass the relevant text content along with the user's question to OpenAI GPT to create an answer.

To improve the chat experience, add memory to your chatbot by storing previous messages in a chat history. This allows the chatbot to access the chat history during the conversation.

3.1 Flow

A chat history is created at the start. As the user asks questions or the chatbot provides answers, these messages are stored in the chat history. The chatbot keeps track of previous messages as the conversation progresses, which serves as its memory.
As the user writes a question, it’s saved in the chat history.
The question and the chat history combine into a standalone question.
The standalone question converts into a vector using the same embedding model as in the Document Ingestion phase.
The vector passes to the vector database and performs a similarity search (or vector search).

In short, you need to find the most similar items to the user's question given a set of vectors (in red) and the query vector (the user's question in blue). In the example below, you can see the k-nearest neighbor search (k-NN), which looks for the three closest vectors to the query vector:

With the most similar vectors found, link the corresponding Notion content to a stand-alone question for GPT.
GPT formulates an answer based on the guidelines of a system prompt.
The chatbot responds to the user with the answer from GPT.
Add the chatbot's answer to the chat history.
Repeat this process.

Make sense? Nice! Let's start coding.

3.2 Query

First, create a LangChain Conversational Retrieval Chain to serve as the brain of your app. To do so, create a utils.py file containing the load_chain() function, which returns a Conversational Retrieval Chain.

# **utils.py**

import streamlit as st
import openai
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate

openai.api_key = st.secrets["OPENAI_API_KEY"]

@st.cache_resource
def load_chain():
		"""
    The `load_chain()` function initializes and configures a conversational retrieval chain for
    answering user questions.
    :return: The `load_chain()` function returns a ConversationalRetrievalChain object.
    """

		# Load OpenAI embedding model
		embeddings = OpenAIEmbeddings()
		
		# Load OpenAI chat model
		llm = ChatOpenAI(temperature=0)
		
		# Load our local FAISS index as a retriever
		vector_store = FAISS.load_local("faiss_index", embeddings)
		retriever = vector_store.as_retriever(search_kwargs={"k": 3})
		
		# Create memory 'chat_history' 
		memory = ConversationBufferWindowMemory(k=3,memory_key="chat_history")
		
		# Create system prompt
		template = """
    You are an AI assistant for answering questions about the Blendle Employee Handbook.
    You are given the following extracted parts of a long document and a question. Provide a conversational answer.
    If you don't know the answer, just say 'Sorry, I don't know ... 😔. 
    Don't try to make up an answer.
    If the question is not about the Blendle Employee Handbook, politely inform them that you are tuned to only answer questions about the Blendle Employee Handbook.
    
    {context}
    Question: {question}
    Helpful Answer:"""
		
		# Create the Conversational Chain
		chain = ConversationalRetrievalChain.from_llm(llm=llm, 
				                                          retriever=retriever, 
				                                          memory=memory, 
				                                          get_chat_history=lambda h : h,
				                                          verbose=True)
		
		# Add systemp prompt to chain
		# Can only add it at the end for ConversationalRetrievalChain
		QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template)
		chain.combine_docs_chain.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate(prompt=QA_CHAIN_PROMPT)
		
		return chain


Let's review the code:

Load the OpenAI API key stored in .streamlit/secrets.toml.
Create the load_chain() function that returns a Conversational Retrieval Chain object. We use st.cache_resource to make your app more efficient. By using this, you only run the load_chain() function once at the very beginning and store the result in a local cache. Later, it’ll skip its execution as no changes have been made. Very convenient!
Load the OpenAI embedding model, which converts the user's queries into vectors.
Load the OpenAI chat model that generates the answers. To do so, it uses the stand-alone question (combining the user's question and chat history) and relevant documents. Specify a temperature of 0, meaning that the model will always select the highest probability word. A higher temperature means that the model might select a word with a slightly lower probability, leading to more variation, randomness, and creativity. Play with it to see what works best for you.
Load your local FAISS index as a retriever, which the chain uses to search for relevant information. Define k=3—look for the three most relevant documents in the vector database.
Create the memory of your chatbot using ConversationBufferWindowMemory. Define k=3, meaning the chatbot will look at the last three interactions when creating the stand-alone question. This is useful for keeping a sliding window of the most recent interactions so that the buffer does not get too large.
Create the system prompt, which acts as the guidelines for our chatbot. Specify how the chatbot should behave and what it should do when it can’t find an answer or when the user's question is pit of its scope.
Create the chain using ConversationalRetrievalChain, linking all the previous elements together. Set verbose=True to see what's happening under the hood when running the chain. This makes it easier to see what information the chatbot uses to answer user's questions.
Add the system prompt to the chain. Currently, it seems that you can only add it after defining the chain when using ConversationalRetrievalChain.from_llm.
4. Chatbot application

Use Streamlit to create the chatbot interface—a visually appealing chat app that can be deployed online—and embed the app in your Notion page.

4.1 Streamlit application

Now that you built the brain of your chatbot, let’s put it all in a Streamlit application!

# **app.py**

import time
import streamlit as st
from utils import load_chain

# Custom image for the app icon and the assistant's avatar
company_logo = 'https://www.app.nl/wp-content/uploads/2019/01/Blendle.png'

# Configure Streamlit page
st.set_page_config(
    page_title="Your Notion Chatbot",
    page_icon=company_logo
)

# Initialize LLM chain
chain = load_chain()

# Initialize chat history
if 'messages' not in st.session_state:
    # Start with first message from assistant
    st.session_state['messages'] = [{"role": "assistant", 
                                  "content": "Hi human! I am Blendle's smart AI. How can I help you today?"}]

# Display chat messages from history on app rerun
# Custom avatar for the assistant, default avatar for user
for message in st.session_state.messages:
    if message["role"] == 'assistant':
        with st.chat_message(message["role"], avatar=company_logo):
            st.markdown(message["content"])
    else:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# Chat logic
if query := st.chat_input("Ask me anything"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": query})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant", avatar=company_logo):
        message_placeholder = st.empty()
        # Send user's question to our chain
        result = chain({"question": query})
        response = result['answer']
        full_response = ""

        # Simulate stream of response with milliseconds delay
        for chunk in response.split():
            full_response += chunk + " "
            time.sleep(0.05)
            # Add a blinking cursor to simulate typing
            message_placeholder.markdown(full_response + "▌")
        message_placeholder.markdown(full_response)

    # Add assistant message to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})


Let's review the code:

Import load_chain() from utils.py to load the Conversational Retrieval Chain we created earlier.
Load an image from a URL to use as your app's page icon and assistant's avatar in the chat app.
Initialize your chain in the session state.
Initialize the chat history in the session state with a first message from the assistant welcoming the user.
Display all the messages of the chat history, specifying a custom avatar for the assistant and the default one for the user.
Create the chat logic to:
Receive the user's query and store it in the chat history
Display the user's query in the chat
Pass the user's query to your chain using st.session_state['chain']({"question": query})
Get a response back
Display the response in the chat, simulating a human typing speed by slowing down the display of the response
Store the response in the chat history

4.2 Deployment on Streamlit Community Cloud

Once you’ve tested your chatbot and are happy with it, it's time to go live!

To deploy it on Streamlit Community Cloud:

Create a requirements.txt file to store the required dependencies

# ****************requirements.txt****************

openai
langchain
faiss-cpu
tiktoken


Deploy the app and click on Advanced settings. From there, specify your Python version and your OpenAI API key (they should match the information in your local secrets.toml file).

4.3 Embed your Streamlit app in Notion

Once your app is deployed, copy its URL.
Go to your Notion page.
Embed your app by selecting Embed in the block options.
Paste your app’s URL and click on Embed link.
Voilà! Have fun interacting with your content using your new Notion chatbot!
Congratulations! 🥳

In this tutorial, you’ve learned how to:

Convert your Notion content to vectors using the OpenAI embedding model and store them in a FAISS index
Build a Conversation Retrieval Chain using LangChain, with a custom prompt and memory
Build and deploy a Streamlit chat application using its latest chat features
Embed a Streamlit chat application in your Notion page

If you have any questions, please post them in the comments below. And if you want to learn more about AI and LLMs, let's connect on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Product Announcements
https://blog.streamlit.io/tag/product/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Product
36 posts
Introducing two new caching commands to replace st.cache!

st.cache_data and st.cache_resource are here to make caching less complex and more performant

Product
by
Tim Conkling and 
2
 more,
February 14 2023
A new Streamlit theme for Altair and Plotly charts

Our charts just got a new look!

Product
by
William Huang and 
4
 more,
December 19 2022
Announcing the Figma-to-Streamlit plugin 🎨

Go from prototype to code as easy as 1-2-3 with our new community resource!

Product
by
Juan Martín García
,
November 1 2022
The next frontier for Streamlit

Our feature roadmap for 2023 and beyond

Product
by
Amanda Kelly and 
4
 more,
October 18 2022
Introducing multipage apps! 📄

Quickly and easily add more pages to your Streamlit apps

Product
by
Vincent Donato
,
June 2 2022
Leverage your user analytics on Streamlit Community Cloud

See who viewed your apps, when, and how popular they are

Product
by
Diana Wang and 
1
 more,
May 17 2022
Streamlit and Snowflake: better together

Together, we’ll empower developers and data scientists to mobilize the world’s data

Product
by
Adrien Treuille and 
2
 more,
March 2 2022
Streamlit Cloud is now SOC 2 Type 1 compliant

We have completed a full external audit of our security practices

Product
by
Amanda Kelly
,
January 11 2022
Deploy a private app for free! 🎉

And... get unlimited public apps

Product
by
Abhi Saini
,
December 9 2021
☁️ Introducing Streamlit Cloud! ☁️

Streamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.

Product
by
Adrien Treuille
,
November 2 2021
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing a chemical molecule component for your Streamlit apps
https://blog.streamlit.io/introducing-a-chemical-molecule-component-for-your-streamlit-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing a chemical molecule component for your Streamlit apps

Integrate a fully featured molecule editor with just a few lines of code!

By Michał Nowotka
Posted in Product, April 13 2023
How to use Streamlit molecule editor
Features
Use cases
Demo apps
Demo app 1 by Kamil Breguła
Demo app 2
Are custom components easy to create?
Wrapping up
References
Contents
Share this post
← All posts

At Streamlit, we always look for ways to improve your experience and make app development simple. Custom components are essential to this, as they provide a way to contribute to and extend the Streamlit ecosystem. We wanted to create something useful for the community and show how easy it is to write a custom component.

Today, we are excited to introduce a new custom component for Streamlit...

...the molecule editor! 🧪

The ability to draw chemical compounds is critical for most drug discovery, drug design, and cheminformatics apps. You can now easily integrate a molecule editor into your Streamlit applications with just a few lines of code.

How to use Streamlit molecule editor

First, install Streamlit and the component:

pip install streamlit
pip install streamlit-ketcher


Then, import it into your Streamlit application, and you're good to go:

import streamlit as st
from streamlit_ketcher import st_ketcher

smiles = st_ketcher()


This code snippet creates a full-featured chemical molecule editor within your Streamlit app. The variable smiles contains the Simplified Molecular Input Line Entry System (SMILES) representation of the molecule, which you can use for further processing, analysis, or visualization in your app.

Our component is based on the Ketcher library from Epam. Several other editors offer similar functionality, including MarvinJS, JSME, and ChemWriter. However, we chose Ketcher for the following reasons:

It's released under the Apache 2.0 license, compatible with the Streamlit library.
It's React-friendly, as it has a corresponding React component.
It has pretty good documentation.
It has a modern look and feel and is feature-rich.
Its most basic version doesn't rely on an external server.

So, it was the perfect fit for our needs. 🙂

Features

The molecule editor component offers several useful features, including:

Intuitive drawing tools: Easily create and modify molecules using a simple point-and-click interface, with support for adding and deleting atoms, bonds, and functional groups.
Automatic SMILES and Molfile generation: Instantly convert the molecule into its SMILES representation to easily integrate with other cheminformatics tools. Molfiles are also supported.
Copy/paste support.
Open-source and community-driven: The editor is free and open-source, allowing you to contribute and help improve the component.
Use cases

The editor is an excellent choice for a variety of cheminformatics and chemistry-related applications, including:

Drug design and discovery
Molecular modeling and visualization
Management of chemical databases
Education and training in chemoinformatics
Prediction of chemical properties
Demo apps

We've prepared two demo applications.

Demo app 1 by Kamil Breguła

This app provides an overview of the basic usage and configuration of the component:

Demo app 2

This app explores some of a particular application's most popular use cases. It integrates with the open chemistry database ChEMBL using the chembl_webresource_client package (created a few years ago by the author of this doc). With this integration, molecule structures can be retrieved by name. The app can now create a gallery of the "most famous" chemical compounds at the top:

This is achieved with the following code:

famous_molecules = [
    ('☕', 'Caffeine'), 
    ('🥱', 'Melatonin'), 
    ('🚬', 'Nicotine'), 
    ('🌨️', 'Cocaine'), 
    ('💊', 'Aspirin'),
    ('🍄', 'Psilocybine'), 
    ('💎', 'Lysergide')
]
for mol, column in zip(famous_molecules, st.columns(len(famous_molecules))):
    with column:
        emoji, name = mol
        if st.button(f'{emoji} {name}'):
            st.session_state.molfile, st.session_state.chembl_id = utils.name_to_molecule(name)


The name_to_molecule function is defined as follows:

from typing import Optional, Tuple
from chembl_webresource_client.new_client import new_client as ch

def name_to_molecule(name: str) -> Tuple[str, str]:
    columns = ['molecule_chembl_id', 'molecule_structures']
    ret = ch.molecule.filter(molecule_synonyms__molecule_synonym__iexact=name).only(columns)
    best_match = ret[0]
    return best_match["molecule_structures"]["molfile"], best_match["molecule_chembl_id"]


After you retrieve the molecule structure from ChEMBL and load it into the editor, you can modify it as needed. Once editing is complete, you can read the compound from the editor and run a similarity search to find the most similar compounds from ChEMBL. You can control the similarity threshold with a slider. The search results will be displayed in a table:

with editor_column:
    smiles = st_ketcher(st.session_state.molfile)
    similarity = st.slider("Similarity threshold:", min_value=60, max_value=100)
    with st.expander("Raw data"):
        st.markdown(f"```{smiles}```")
    with results_column:
        similar_molecules = utils.find_similar_molecules(smiles, similarity)
        if not similar_molecules:
            st.warning("No results found")
        else:
            table = utils.render_similarity_table(similar_molecules)
            similar_smiles = utils.get_similar_smiles(similar_molecules)
            st.markdown(f'<div style="overflow:scroll; height:600px; padding-left: 80px;">{table}</div>',
                        unsafe_allow_html=True)


You can perform the similarity search using the following function:

def find_similar_molecules(smiles: str, threshold: int):
    columns = ['molecule_chembl_id', 'similarity', 'pref_name', 'molecule_structures']
    try:
        return ch.similarity.filter(smiles=smiles, similarity=threshold).only(columns)
    except Exception as _:
        return None


After retrieving compounds similar to the 'query compound' you drew in the editor, you can run a target prediction to determine which biological targets your compounds will bind to. To do this, use the onnxruntime library. It'll let you load a prediction model created by my EBI colleague, Eloy Felix (read more about model training and exporting to the ONNX format here).

All target prediction logic is implemented in the demo app's target_predictions.py module:

The result is a table of biological targets ranked by the probability of having an affinity for the predicted compounds. Note that some targets are repeated, as you get the results for each query compound separately. We omitted adding more information about the targets and filtering by organism or type since it's just a proof-of-concept app.

Are custom components easy to create?

The good news is that we created a fairly complex custom component in just a few days. This means that the current API is fully functional. But we've also identified some areas for improvement, including:

Outdated component templates
Undocumented React hook support
Missing props in the theme interface definition
Issues with the CSS that cause the component to render strangely without further customization
Difficulty testing components

While we've found workarounds for most of these issues, we believe it's worth investing some time in the Custom Components ecosystem to make it more developer-friendly. What do you think? Have you ever created a custom component? Share your thoughts in the comments.

Wrapping up

We hope the molecule editor component is a valuable addition to your toolkit. We can't wait to see the amazing applications you create with it. Don't hesitate to contact us with questions or suggestions or to share your projects with the Streamlit community!

Happy coding! 🧑‍💻

References
Streamlit Molecule Editor GitHub Repository: https://github.com/mik-laj/streamlit-ketcher
Ketcher Library by EPAM: https://lifescience.opensource.epam.com/ketcher
ChEMBL database: https://www.ebi.ac.uk/chembl
ChEMBL web resource client library: https://github.com/chembl/chembl_webresource_client
ONNX runtime: https://onnxruntime.ai
Simple demo code: https://ketcher-editor.streamlit.app/
Advanced demo code: https://github.com/streamlit/mol-demo
Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing column config ⚙️
https://blog.streamlit.io/introducing-column-config/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing column config ⚙️

Take st.dataframe and st.data_editor to the next level!

By Lukas Masuch and Johannes Rieke
Posted in Product, June 22 2023
A small example
More parameters
Wrapping up
Contents
Share this post
← All posts

One of the biggest pain points you've told us about repeatedly is dataframe customization.

Data wrangling is at the core of your work, and you've long needed more capabilities than those offered by st.dataframe and st.data_editor.

Here are some of the things you told us you need to do:

Hide the index column
Show images or clickable links in table cells
Disable editing for specific columns of the new data editor

We heard you! That's why in Streamlit 1.23, we're excited to introduce…

Column configuration! 🎉

Now you can customize all columns in st.dataframe and st.data_editor, so your tables look and feel exactly the way you need. Want to see it? Check out our demo app or read below.

Plus, we've moved st.data_editor out of experimental! 🎈

A small example

The main star of the show is the new column_config parameter. We added it to both st.dataframe and st.data_editor.

It's a dictionary that maps column types to their configuration:

st.data_editor(
    df,
    column_config={
        "column 1": "Name",  # change the title
        "column 3": st.column_config.ImageColumn("Avatar"),
        "column 4": st.column_config.NumberColumn(
            "Age", min_value=0, max_value=120, format="%d years"
        ),
        "column 8": st.column_config.LineChartColumn(
            "Activity (1 year)", y_min=0, y_max=100
        ),
    },
)


Taking this a bit further, you can create powerful tables like this:

Try playing with it:

Scroll to the right to see some beautiful charts ✨📈
Double-click on links to open them 🔗
Double-click on a cell to edit it and see input validation features in action ✍️

As you can see in the code, we also introduced new classes for different column types under the st.column_config namespace. In fact, there are 14 different column types that cover everything from text over images to sparkline charts! Each of these classes lets you set additional parameters to configure the display and editing behavior of the column.

Have a look at them on our new docs page! 🎈

To learn more about the column_config parameter itself, check out the API references for  st.dataframe and st.data_editor.

More parameters

Want to hide the index column without delving into column configuration? We've got you covered!

In addition to the column_config parameter, we added a few more parameters that allow you to perform common operations more quickly:

hide_index=True lets you hide the index column
column_order=["col 3", "col2"] lets you set the order in which columns show up
disabled=["col1", "col2"] lets you turn off editing for individual columns on st.data_editor

Read more about these parameters on the API docs for st.dataframe and st.data_editor.

Wrapping up

We're excited to see what you'll build with this new feature. Please share your examples on Twitter and the forum! Head over to our example app to get some inspiration. And if you have more feature requests for dataframes (and beyond), let us know on GitHub.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Introducing st.connection!
https://blog.streamlit.io/introducing-st-experimental_connection/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing st.connection!

Quickly and easily connect your app to data and APIs

By Joshua Carroll and Vincent Donato
Posted in Product, May 2 2023
st.experimental_connection 🥂
How does st.experimental_connection work
Step 1. Install dependencies
Step 2. Set up credentials and connection information in .streamlit/secrets.toml
Step 3. Import and initialize the connection in your app
Step 4. Query your data with one line of code
Step 5. Perform complex operations with .session
How to build your own connection
What’s next?
Contents
Share this post
← All posts
📣
st.experimental_connection is now st.connection

Check out the docs to learn more. These updates should make it even easier to connect to your data:

1) The Snowpark connection is now a simpler, more powerful Snowflake connection.

2) More passthrough methods and hints have been added to make data connection bugs self-correcting or easier to fix.

In our last-year survey (N=315), 29% of you told us that setting up data connections was the most frustrating part of your work.

We get it. Connecting to data sources and APIs can be painful. 🤕

You need to find and install external packages, figure out how to manage your credentials securely outside of code, then find the right methods to get data out in the format you need. Oh, and you can’t forget to add Streamlit caching capabilities! For example, you can see the 14 lines of code in our original MySQL tutorial using a mysql.connector, st.cache_resource, st.cache_data, managing a cursor, and converting the row result format!

Today, we’re thrilled to release…

st.experimental_connection 🥂

Connect your Streamlit apps to data and APIs with just four lines of code:

import streamlit as st

conn = st.experimental_connection('pet_db', type='sql')
pet_owners = conn.query('select * from pet_owners')
st.dataframe(pet_owners)


Here is the demo app, or you can play with it below:

How does st.experimental_connection work

Streamlit comes installed with generic connections for SQL and Snowflake Snowpark. You may need to install additional packages or one of the community connections to make it work properly.

Today, it supports:

SQL dialects (MySQL, Postgres, Snowflake, BigQuery, Microsoft SQL Server, etc.)
Snowflake Snowpark
Cloud file storage (S3, GCS, Azure Blob Storage, etc.) via FilesConnection
HuggingFace Datasets and Models via HfFileSystem and FilesConnection
Google Sheets via GSheetsConnection

We’re making it easier than ever to extend this list and build your own data connections and share them with the Streamlit community!

For the purpose of this post, we’ll be using MySQL examples. If you want to follow along with other data sources, check out our tutorials on Snowflake or AWS S3.

Step 1. Install dependencies

To start, install any necessary packages in your environment (such as with pip and requirements.txt). You can find these in Streamlit’s data source tutorials or the data source documentation. If something is missing when you run your app, Streamlit will try to detect that and give you a hint about what to install (we’ll make this even easier in the future!):

pip install SQLAlchemy mysqlclient

Step 2. Set up credentials and connection information in .streamlit/secrets.toml

Next, let’s set up the connection information in secrets.toml. Create a new section called [connections.<name>] and add the parameters you need. You can name the section whatever you’d like - you’ll use the same name to reference it in your app code.

# .streamlit/secrets.toml

[connections.pet_db]
dialect = "mysql"
url = "mysqldb://scott:tiger@192.168.0.134/pet_db"


We added support for a global secrets.toml, so if you keep using the same database, you can set this up once instead of copying it to every app. Many connections will also support their native configuration, like AWS_* environment variables or ~/.snowsql/config file.

Step 3. Import and initialize the connection in your app

Now, let’s initialize the connection in your app:

# streamlit_app.py

import streamlit as st

conn = st.experimental_connection('pet_db', type='sql')


The first argument is the name of the connection you used in secrets.toml. The type argument tells Streamlit what type of connection it is. For community-developed connections that don’t ship with Streamlit, you can import the connection class and pass it directly to type. See the AWS S3 tutorial or API Reference for examples.

Step 4. Query your data with one line of code

For the common use case of reading data or getting some response from an API, the connection will provide a simple method that returns an easy-to-use output. It’s also cached in Streamlit by default to make your app ⚡blazing fast! ⚡

For example, SQLConnection has a query() method that takes a query input and returns a pandas DataFrame:

# streamlit_app.py

import streamlit as st

conn = st.experimental_connection('pet_db', type='sql')
pet_owners = conn.query('select * from pet_owners')
st.dataframe(pet_owners)


That’s it!

The method also supports params, paging, custom cache TTL, and other common arguments (read more in the API Reference).

Depending on the underlying data format, the specific methods may differ but should be natural, straightforward, and intuitive to that data source. Connection objects are fully type annotated, so your IDE can provide hints. st.help() and st.write() can also give you more information about what is supported on a specific connection!

Step 5. Perform complex operations with .session

If you need the full power of the underlying data source or library, it’s easily accessible too! SQL and Snowpark both support this with .session, and other connections may have a domain-specific name for easier discovery.

For example, if you need to use transactions, write back, or interact via ORM, you can access the SQL Session with SQLConnection.session:

with conn.session as s:
    pet_owners = {'jerry': 'fish', 'barbara': 'cat', 'alex': 'puppy'}
    for k in pet_owners:
        s.execute(
            'INSERT INTO pet_owners (person, pet) VALUES (:owner, :pet);',
            params=dict(owner=k, pet=pet_owners[k])
        )
    s.commit()

Check out the tutorials or use st.help() to learn more about what’s supported for a specific data set.

How to build your own connection

We’re excited for the community to extend and build on the st.experimental_connection interface. We want to make it super easy to build Streamlit apps with lots of data sources (we’ve built the interface with this in mind).

To use a community-built connection in your app, install and import it, then pass the class to st.experimental_connection():

import streamlit as st
from st_gsheets_connection import GSheetsConnection

conn = st.experimental_connection("pets_gsheet", type=GSheetsConnection)
pet_owners = conn.read(worksheet="Pet Owners")
st.dataframe(pet_owners)


These types of connections work the same as the ones built into Streamlit and have access to the same capabilities. Build a connection class by extending streamlit.connections.ExperimentalBaseConnection. You can find basic information in the API Reference, and a simple fully working example here. The SQLConnection built into Streamlit is another great starting point.

What’s next?

We’ve been hard at work on st.experimental_connection, so we’re very excited to finally share it with you! Please let us know how you're using it on the forum, Twitter, Discord, or in the comments below.

Expect more connections, guides, and features in the coming weeks and months to make it even easier to connect your Streamlit app to data. And keep an eye out for a community connection-building contest a little bit later this spring. 🙂

Happy app-building! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Michał Nowotka - Streamlit
https://blog.streamlit.io/author/michal/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Michał Nowotka
3 posts
Introducing a chemical molecule component for your Streamlit apps

Integrate a fully featured molecule editor with just a few lines of code!

Product
by
Michał Nowotka
,
April 13 2023
Introducing Streamlit to the Polish Python community

My Streamlit presentation at PyWaW #103

Product
by
Michał Nowotka
,
April 4 2023
How to make a culture map

Analyze multidimensional data with Steamlit!

Tutorials
by
Michał Nowotka
,
January 12 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Generative AI and Streamlit: A perfect match
https://blog.streamlit.io/generative-ai-and-streamlit-a-perfect-match/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

By Adrien Treuille and Amanda Kelly
Posted in LLMs, June 15 2023
Streamlit is the UI powering the LLM movement
What does this mean for the future?
💬 Chat interfaces as a key UI paradigm
⚙️ Easily connect to the LLM ecosystem
❄️ Unlock business insights with LLMs in Snowflake
Let's build the future together
Contents
Share this post
← All posts

Five years ago, we started Streamlit to help you share your work through beautiful web apps. Our vision was to create a thin wrapper around the Python ecosystem, allowing you to bring APIs, models, and business logic to life just by typing.

Today, generative AI is exploding into the Python ecosystem, and Streamlit has been right there. In fact, it's so easy that…

…LLMs can write a Streamlit app for you!

Streamlit is the UI powering the LLM movement

How is it that LLMs are so good at writing Streamlit apps? It's because of all of the Streamlit code the community has shared!

Over 190,000 snippets of Streamlit code (and counting) exist on GitHub alone, all of which have helped train GPT4 and other LLMs. This means that analysts, data scientists, and even students can quickly perform analyses, rough out new apps, and weave auto-generated Streamlit fragments throughout other apps.
Over 5,000 LLM-powered Streamlit apps (and counting) have already been built on the Community Cloud alone. And these numbers are growing rapidly every day.
What does this mean for the future?

With its simple API and design, Streamlit has surfed each wave of generative AI advances, helping you effortlessly bring LLM-powered apps to life.

But what is even more important is how the rest of the LLM ecosystem is using Streamlit and how we can weave this all together for even more powerful apps.

💬 Chat interfaces as a key UI paradigm

Streamlit is perfectly suited for generative AI with its simple API, real-time visualization, interactive abilities, and user-friendly interfaces.

We are releasing chat elements soon that will help you make all the LLM-powered apps you can dream up.

⚙️ Easily connect to the LLM ecosystem

As LLMs continue to gain popularity, a vast ecosystem of tools is emerging around them. Here are some of the tools that already work well with Streamlit.

🦜🔗 LangChain's callback system to see what an LLM is thinking

As we move beyond simple chatbots to more complex applications, it'll be critical to understand how LLMs think and the steps they take to answer questions. We've enabled you to add intermediate step information to your Streamlit + LangChain app with just one command:

In addition to LangChain, here are some other tools that we're excited about:

Stable Diffusion: example apps from Stability AI
Multiple BabyAGI apps, including AgentGPT
LlamaIndex Term Extractor and Starter Pack
Sygil WebUI for Stable Diffusion

We'll be announcing more integrations soon, so stay tuned!

❄️ Unlock business insights with LLMs in Snowflake

Since our acquisition in 2022, we've been integrating Streamlit's functionality into Snowflake's enterprise-grade data solutions used by some of the largest companies in the world.

We'll share more at Snowflake Summit 2023, June 26-29 (you can still register here). Here are a few Streamlit + LLM sessions:

Unleashing the Power of Large Language Models with Snowflake with Adrien Treuille, Streamlit co-founder, and Richard Meng, Senior Software Engineer.
GPTZero: Idea to Iteration Powered by Streamlit and Snowflake with Caroline Frasca and Edward Tian.
Keynote: Generative AI and LLMs in the enterprise

Check out more LLM talks here (you can watch them online after the Summit).

Let's build the future together

This community's engagement, creativity, and innovation are absolutely incredible. Thank you for inspiring our roadmap with your contributions.

Your support, combined with the rapid advancements coming from generative AI, will enable us to release new features that help shape the future of apps with LLMs. We want to hear from you!

In the meantime, check out our new generative AI hub and join us on the forum to share your ideas.

Let's do this together!

Love,

Adrien, Amanda, Thiago, and everyone at Streamlit. 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Instant Insight: Generate data-driven presentations in a snap!
https://blog.streamlit.io/instant-insight-generate-data-driven-presentations-in-a-snap/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Instant Insight: Generate data-driven presentations in a snap!

Create presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery

By Oleksandr Arsentiev
Posted in LLMs, August 2 2023
Why Instant Insight?
App overview
1. Connect your Streamlit app to Snowflake
2. Create a UI with dynamic filters and an interactive table
3. Fetch company data from Yahoo Finance
4. Create graphs using Plotly
5. Use Clearbit API to get company logos
6. Use LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition
7. Extract structured data from the GPT response
8. Generate slides using python-pptx
Wrapping up
Contents
Share this post
← All posts

Hey there! 👋

I'm Oleksandr, a Data Analyst at Workday. I wrote my first lines of code four years ago and saw the incredible potential of technology to simplify tasks and enhance productivity.

In this post, I'll tell you about my Instant Insight app, which can generate PowerPoint presentations for company research instantly. Forget the hassle of spending hours crafting slides. Now you can create sleek, data-driven presentations in just a few clicks!

I'll show you how to:

Connect your Streamlit app to Snowflake
Create a UI with dynamic filters and an interactive table
Fetch company data from Yahoo Finance
Create graphs using Plotly
Use Clearbit API to get company logos
Use LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition
Extract structured data from the GPT response
Generate slides using python-pptx

Let's get cracking!

⚡
TLDR: Here's the app’s URL and the GitHub repo. Enjoy!
Why Instant Insight?

Imagine working in sales for a B2B SaaS company with hundreds of prospects and offering the following products: Accounting and Planning Software, CRM, Chatbot, and Cloud Data Storage. Your task is to conduct prospect research, including financial and SWOT analysis, explore the competitive landscape, craft value propositions, and share a presentation with your team.

The prospects' data is stored in Snowflake, which feeds your CRM system. You can use the Instant Insight app to quickly filter the prospects by sector, industry, prospect status, and product. Next, select the prospect you want to include in the presentation and click the button to generate the presentation. And... that's it! 🤩 You now have the slides ready to be shared with your team. 🚀

The slides created by the app include the following:

Basic company overview
SWOT analysis
Financial charts
Value proposition
Competitor analysis
Key people
News and corporate events
App overview

The high-level architecture of the app looks like this:

And here is a 3-minute video of the app in action:

Now, let's dive into the code!

1. Connect your Streamlit app to Snowflake

First, obtain some data. To do this, use the Snowflake Connector:

import snowflake.connector

# get Snowflake credentials from Streamlit secrets
SNOWFLAKE_ACCOUNT = st.secrets["snowflake_credentials"]["SNOWFLAKE_ACCOUNT"]
SNOWFLAKE_USER = st.secrets["snowflake_credentials"]["SNOWFLAKE_USER"]
SNOWFLAKE_PASSWORD = st.secrets["snowflake_credentials"]["SNOWFLAKE_PASSWORD"]
SNOWFLAKE_DATABASE = st.secrets["snowflake_credentials"]["SNOWFLAKE_DATABASE"]
SNOWFLAKE_SCHEMA = st.secrets["snowflake_credentials"]["SNOWFLAKE_SCHEMA"]

@st.cache_resource
def get_database_session():
    """Returns a database session object."""
    return snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
    )

@st.cache_data
def get_data():
    """Returns a pandas DataFrame with data from Snowflake."""
    query = 'SELECT * FROM us_prospects;'
    cur = conn.cursor()
    cur.execute(query)

    # Fetch the result as a pandas DataFrame
    column_names = [col[0] for col in cur.description]
    data = cur.fetchall()
    df = pd.DataFrame(data, columns=column_names)

    # Close the connection to Snowflake
    cur.close()
    conn.close()
    return df

# get the data from Snowflake
conn = get_database_session()
df = get_data(conn)


Sensitive data such as your Snowflake account, username, password, database name, and schema are stored in secrets and retrieved by calling st.secrets (read more here).

Next, define two functions:

get_database_session() initializes a connection object
get_data() executes a SQL query and returns a pandas DataFrame

Use a simple SELECT * query to retrieve all the data from the us_prospects table.

2. Create a UI with dynamic filters and an interactive table

Now let's use some Streamlit magic to develop a front end for the app. Create a sidebar panel containing four dynamic multi-select filters and add a checkbox that lets users select all values.

The filters in your app work sequentially. Users are expected to apply them individually, from top to bottom. Once the first filter is applied, the second filter becomes available and contains only relevant labels. After each filter is applied, the underlying DataFrame is pre-filtered, and the num_of_pros variable is updated to reflect the number of selected prospects.

See the filters in action:

Here is the code for the first two filters:

# create sidebar filters
st.sidebar.write('**Use filters to select prospects** 👇')
sector_checkbox = st.sidebar.checkbox('All Sectors', help='Check this box to select all sectors')
unique_sector = sorted(df['SECTOR'].unique())

# if select all checkbox is checked then select all sectors
if sector_checkbox:
    selected_sector = st.sidebar.multiselect('Select Sector', unique_sector, unique_sector)
else:
    selected_sector = st.sidebar.multiselect('Select Sector', unique_sector)

# if a user selected sector then allow to check all industries checkbox
if len(selected_sector) > 0:
    industry_checkbox = st.sidebar.checkbox('All Industries', help='Check this box to select all industries')
    # filtering data
    df = df[(df['SECTOR'].isin(selected_sector))]
    # show number of selected prospects
    num_of_pros = str(df.shape[0])
else:
    industry_checkbox = st.sidebar.checkbox('All Industries', help='Check this box to select all industries',
                                           disabled=True)
    # show number of selected prospects
    num_of_pros = str(df.shape[0])

# if select all checkbox is checked then select all industries
unique_industry = sorted(df['INDUSTRY'].loc[df['SECTOR'].isin(selected_sector)].unique())
if industry_checkbox:
    selected_industry = st.sidebar.multiselect('Select Industry', unique_industry, unique_industry)
else:
    selected_industry = st.sidebar.multiselect('Select Industry', unique_industry)

# if a user selected industry then allow them to check all statuses checkbox
if len(selected_industry) > 0:
    status_checkbox = st.sidebar.checkbox('All Prospect Statuses', help='Check this box to select all prospect statuses')
    # filtering data
    df = df[(df['SECTOR'].isin(selected_sector)) & (df['INDUSTRY'].isin(selected_industry))]
    # show number of selected prospects
    num_of_pros = str(df.shape[0])

else:
    status_checkbox = st.sidebar.checkbox('All Prospect Statuses', help='Check this box to select all prospect statuses', disabled=True)


Next, use AgGrid to create an interactive table displaying your data, allowing users to select slide-generation prospects (read more here).

Place a checkbox on each table row, allowing users to select only one row. Additionally, set custom column width and table height.

Here is what it'll look like:

Neat, right?

Here is the code to create this table:

from st_aggrid import AgGrid
from st_aggrid.grid_options_builder import GridOptionsBuilder
from st_aggrid import GridUpdateMode, DataReturnMode
import pandas as pd

# creating AgGrid dynamic table and setting configurations
gb = GridOptionsBuilder.from_dataframe(df)
gb.configure_selection(selection_mode="single", use_checkbox=True)
gb.configure_column(field='Company Name', width=270)
gb.configure_column(field='Sector', width=260)
gb.configure_column(field='Industry', width=350)
gb.configure_column(field='Prospect Status', width=270)
gb.configure_column(field='Product', width=240)

gridOptions = gb.build()

response = AgGrid(
    df,
    gridOptions=gridOptions,
    height=600,
    update_mode=GridUpdateMode.SELECTION_CHANGED,
    data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
    fit_columns_on_grid_load=False,
    theme='alpine',
    allow_unsafe_jscode=True
)

# get selected rows
response_df = pd.DataFrame(response["selected_rows"])

3. Fetch company data from Yahoo Finance

Let's say the user has selected a company for research, and you need to gather some data on it. Your primary data source is Yahoo Finance, which you'll access with the yahooquery library—a Python interface to unofficial Yahoo Finance API endpoints. It allows users to retrieve almost all the visible data via the Yahoo Finance front-end.

Here's the overview slide with Yahoo Finance data:

Use the Ticker class of yahooquery to obtain quantitative and qualitative data about a selected company. Just pass the company's ticker symbol as an argument, call the required property, and retrieve the data from the returned dictionary.

Here is the code that retrieves data for the company overview slide:

from yahooquery import Ticker

selected_ticker = 'ABC'
ticker = Ticker(selected_ticker)

# get company info
name = ticker.price[selected_ticker]['shortName']
sector = ticker.summary_profile[selected_ticker]['sector']
industry = ticker.summary_profile[selected_ticker]['industry']
employees = ticker.summary_profile[selected_ticker]['fullTimeEmployees']
country = ticker.summary_profile[selected_ticker]['country']
city = ticker.summary_profile[selected_ticker]['city']
website = ticker.summary_profile[selected_ticker]['website']
summary = ticker.summary_profile[selected_ticker]['longBusinessSummary']


The app utilizes Yahoo Finance data to create graphs illustrating financial performance over time. One slide displays basic financial metrics such as stock price, total debt, total revenue, and EBITDA over time.

I'll touch on plotting later. For now, let's focus on obtaining financial data from Yahoo Finance. Functions get_stock() and get_financials() return dataframes with relevant financial metrics. The stock price data is stored separately from other financial metrics, which is why you call two properties:

ticker.history() to retrieve historical pricing data for given symbol(s) (read docs here)
ticker.all_financial_data() to retrieve all financial data, including income statement, balance sheet, cash flow, and valuation measures (read docs here)

Here is the code used to generate four dataframes with historical stock price, revenue, total debt, and EBITDA:

from yahooquery import Ticker
import pandas as pd

def get_stock(ticker, period, interval):
    """function to get stock data from Yahoo Finance. Takes ticker, period and interval as arguments and returns a DataFrame"""
    hist_df = ticker.history(period=period, interval=interval)
    hist_df = hist_df.reset_index()
    # capitalize column names
    hist_df.columns = [x.capitalize() for x in hist_df.columns]
    return hist_df

def get_financials(df, col_name, metric_name):
    """function to get financial metrics from a DataFrame. Takes DataFrame, column name and metric name as arguments and returns a DataFrame"""
    metric = df.loc[:, ['asOfDate', col_name]]
    metric_df = pd.DataFrame(metric).reset_index()
    metric_df.columns = ['Symbol', 'Year', metric_name]
    return metric_df

selected_ticker = 'ABC'
ticker = Ticker(selected_ticker)

# get all financial data
fin_df = ticker.all_financial_data()

# create df's for each metric
stock_df = get_stock(ticker=ticker, period='5y', interval='1mo')
rev_df = get_financials(df=fin_df, col_name='TotalRevenue', metric_name='Total Revenue')
debt_df = get_financials(df=fin_df, col_name='TotalDebt', metric_name='Total Debt')
ebitda_df = get_financials(df=fin_df, col_name='NormalizedEBITDA', metric_name='EBITDA')


The data from Yahoo Finance is also used on another slide for competitor analysis, where a company's performance is compared to its peers. To perform it, use two metrics: total revenue and SG&A % of revenue. They're available in the income statement, so use the ticker.income_statement() property which returns a DataFrame (read more here).

The extract_comp_financials() function retrieves the total revenue and selling, general, and administrative expenses (SG&A) from the income statement DataFrame, and only keeps data from 2022. Since the SG&A as a percentage of revenue metric isn't readily available, calculate it manually by dividing SG&A by revenue and multiplying by 100.

The function operates on a nested dictionary with company names as keys and a dictionary with tickers as values, then appends new values to the existing dictionary:

from yahooquery import Ticker
import pandas as pd

def extract_comp_financials(tkr, comp_name, comp_dict):
    """Function to extract financial metrics for competitors. 
Takes a ticker, company name and a nested dictionary with 
competitors as arguments and appends financial metrics to dict"""

    ticker = Ticker(tkr)
    income_df = ticker.income_statement(frequency='a', trailing=False)
    subset = income_df.loc[:, ['asOfDate', 'TotalRevenue', 'SellingGeneralAndAdministration']].reset_index()

    # keep only 2022 data
    subset = subset[subset['asOfDate'].dt.year == 2022].sort_values(by='asOfDate', ascending=False).head(1)

    # get values
    total_revenue = subset['TotalRevenue'].values[0]
    sg_and_a = subset['SellingGeneralAndAdministration'].values[0]

    # calculate sg&a as a percentage of total revenue
    sg_and_a_pct = round(sg_and_a / total_revenue * 100, 2)

    # add values to dictionary
    comp_dict[comp_name]['Total Revenue'] = total_revenue
    comp_dict[comp_name]['SG&A % Of Revenue'] = sg_and_a_pct

# sample dict
peers_dict_nested = {'Company 1': {'Ticker': 'ABC'}, 'Company 2': {'Ticker': 'XYZ'}}

# extract financial data for each competitor
for key, value in peers_dict_nested.items():
    try:
        extract_comp_financials(tkr=value['Ticker'], comp_name=key, dict=peers_dict_nested)
    # if ticker is not found in Yahoo Finance, drop it from the peers dict and continue
    except:
        del peers_dict_nested[key]
        continue


After running the code above, get a nested dictionary that resembles the following structure:

# sample output dict
{'Company 1': {'Ticker': 'ABC', 'Total Revenue': '1234', 'SG&A % Of Revenue': '10'}, 
 'Company 2': {'Ticker': 'XYZ', 'Total Revenue': '5678', 'SG&A % Of Revenue': '20'}}


Next, convert the nested dictionary to the DataFrame to pass it to a plotting function:

# create a dataframe with peers financial data
peers_df = pd.DataFrame.from_dict(peers_dict_nested, orient='index')
peers_df = peers_df.reset_index().rename(columns={'index': 'Company Name'})


The resulting DataFrame should look something like this:

Company Name	Ticker	Total Revenue	SG&A % Of Revenue
Company 1	ABC	1234	10
Company 2	XYZ	5678	20
4. Create graphs using Plotly

You've filtered the financial data—now it's time to plot it! Use Plotly Express to create simple yet visually appealing graphs (read more here).

In the previous section, you create a DataFrame and a variable for the company name. Use these in the plot_graph() function to take the dataframe, column names for the x and y axes, and the graph title as arguments:

import plotly.express as px

def plot_graph(df, x, y, title, name):
    """function to plot a line graph. Takes DataFrame, x and y axis, title and name as arguments and returns a Plotly figure"""
    fig = px.line(df, x=x, y=y, template='simple_white',
                        title='<b>{} {}</b>'.format(name, title))
    fig.update_traces(line_color='#A27D4F')
    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
    return fig

stock_fig = plot_graph(df=stock_df, x='Date', y='Open', title='Stock Price USD', name=name)
rev_fig = plot_graph(df=rev_df, x='Year', y='Total Revenue', title='Total Revenue USD', name=name)
debt_fig = plot_graph(df=debt_df, x='Year', y='Total Debt', title='Total Debt USD', name=name)
ebitda_fig = plot_graph(df=ebitda_df, x='Year', y='EBITDA', title='EBITDA USD', name=name)


The resulting graphs should look something like this:

The app also generates a slide containing a competitor analysis for a given company. To make it, use the peers_plot() function along with peers_df. This function generates a horizontal bar chart that compares the total revenue and SG&A % of revenue among competitors.

Here is the code:

import plotly.express as px
import pandas as pd

def peers_plot(df, name, metric):
    """Function to plot a bar chart with peers. 
Takes DataFrame, name, metric and ticker as arguments and returns a Plotly figure"""

    # drop rows with missing metrics
    df.dropna(subset=[metric], inplace=True)
    df_sorted = df.sort_values(metric, ascending=False)

    # iterate over the labels and add the colors to the color mapping dictionary, hightlight the selected company
    color_map = {}
    for label in df_sorted['Company Name']:
        if label == name:
            color_map[label] = '#A27D4F'
        else:
            color_map[label] = '#D9D9D9'

    fig = px.bar(df_sorted, y='Company Name', x=metric, 
                        template='simple_white', color='Company Name',
                        color_discrete_map=color_map,
                        orientation='h',
                        title='<b>{} {} vs Peers FY22</b>'.format(name, metric))
    
    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend=False, yaxis_title='')
    return fig

# plot peers graphs
rev_peers_fig = peers_plot(df=peers_df, name=name, metric='Total Revenue')
sg_and_a_peers_fig = peers_plot(df=peers_df, name=name, metric='SG&A % Of Revenue')


Using custom colors makes your company stand out.

The bar charts should look something like this:

5. Use Clearbit API to get company logos

As you saw on the company overview slide, there was a logo of the researched company. Logo URLs aren’t available on Yahoo Finance, so use Clearbit instead. Just connect the company's website to "https://logo.clearbit.com/" with a few lines of code:

from yahooquery import Ticker

selected_ticker = 'ABC'
ticker = Ticker(selected_ticker)

# get website of the selected company
website = ticker.summary_profile[selected_ticker]['website']

# get logo url of the selected company
logo_url = '<https://logo.clearbit.com/>' + website


Now that you have the logo URL check if it works. If it does, adjust its size and position it on a slide. To do this, use the custom function resize_image(), which places a logo image inside a container and adjusts its size while maintaining its aspect ratio. This ensures that all logos look the same despite any initial differences in size.

Then save the image object locally as "logo.png" and retrieve it to place on a slide as a picture. You can place Plotly figures on slides in a similar manner. Use the python-pptx library to manipulate PowerPoint slides and shapes programmatically (read more here).

Here is the process:

The code below uses the logo_url variable (defined in the previous code snippet):

from PIL import Image
import requests
from pptx import Presentation
from pptx.util import Inches
import os

def resize_image(url):
    """function to resize logos while keeping aspect ratio. Accepts URL as an argument and return image object"""
    # open image from url
    image = Image.open(requests.get(url, stream=True).raw)

    # if a logo is too high or too wide then make the background container twice as big
    if image.height > 140:
        container_width = 220 * 2
        container_height = 140 * 2

    elif image.width > 220:
        container_width = 220 * 2
        container_height = 140 * 2
    else:
        container_width = 220
        container_height = 140

    # create a new image with the same aspect ratio as the original image
    new_image = Image.new('RGBA', (container_width, container_height))

    # calculate the position to paste the image so that it is centered
    x = (container_width - image.width) // 2
    y = (container_height - image.height) // 2

    # paste the image onto the new image
    new_image.paste(image, (x, y))
    return new_image

# create presentation object
prs = Presentation('template.pptx')
# select second slide
slide = prs.slides[1]

# check if a logo ulr returns code 200 (working link)
if requests.get(logo_url).status_code == 200:
    # create logo image object
    logo = resize_image(logo_url)
    logo.save('logo.png')
    logo_im = 'logo.png'

    # add logo to the slide
    slide.shapes.add_picture(logo_im, left=Inches(1.2), top=Inches(0.5), width=Inches(2))
    os.remove('logo.png')


Running the code above should place a logo on the slide:

6. Use LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition

It's time to use AI for your company research! 🤖

You'll use LangChain, a popular framework designed to simplify the creation of applications using ChatOpenAI and Human/System Message LLMs (read more here).

The generate_gpt_response() function takes two arguments:

gpt_input, a prompt that you'll pass to the model
max_tokens, which limits the number of tokens in the model's response

You'll use the gpt-3.5-turbo-0613 model in the arguments of ChatOpenAI and retrieve the OpenAI API key stored in Streamlit secrets. You'll also set the temperature to 0 to get more deterministic responses (read more here).

To improve the quality of GPT responses, pass the following text to the SystemMessage argument: "You are a helpful expert in finance, market, and company research. You also have exceptional skills in selling B2B software products." It'll set the objectives for the AI to follow (read more here).

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

def generate_gpt_response(gpt_input, max_tokens):
    """Function to generate a response from GPT-3. Takes input and max tokens as arguments and returns a response"""
    # Create an instance of the OpenAI class
    chat = ChatOpenAI(openai_api_key=st.secrets["openai_credentials"]["API_KEY"], model='gpt-3.5-turbo-0613',
                      temperature=0, max_tokens=max_tokens)

    # Generate a response from the model
    response = chat.predict_messages(
        [SystemMessage(content='You are a helpful expert in finance, market and company research.'
                               'You also have exceptional skills in selling B2B software products.'),
         HumanMessage(
             content=gpt_input)])

    return response.content.strip()


Next, let's create a prompt for the model and invoke the generate_gpt_response() function.

Prompt the model to create a SWOT analysis of a specific company and return the result as a Python dictionary with this code:

input_swot = """Create a brief SWOT analysis of {name} company with ticker {ticker}?
Return output as a Python dictionary with the following keys: Strengths, Weaknesses, 
Opportunities, Threats as keys and analysis as values.
Do not return anything else."""

input_swot = input_swot.format(name='Company 1', ticker='ABC')

# return response from GPT-3
gpt_swot = generate_gpt_response(input_swot, 1000)


The resulting dictionary should look something like this:

{"Strengths": "text", "Weaknesses": "text", 
"Opportunities": "text", "Threats": "text"}


Similarly, you can prompt the GPT model to write a value proposition for a specific product for a given company. The app uses the common value proposition framework of identifying customer pains and gains, as well as gain creators and pain relievers:

input_vp = """"Create a brief value proposition using Value Proposition Canvas framework for {product} for 
{name} company with ticker {ticker} that operates in {industry} industry.
Return output as a Python dictionary with the following keys: Pains, Gains, Gain Creators, 
Pain Relievers as a keys and text as values. Be specific and concise. Do not return anything else."""

input_vp = input_vp.format(product='Accounting software', name='Company 1', ticker='ABC', industry='Retail')

# return response from GPT-3
gpt_value_prop = generate_gpt_response(input_vp, 1000)

# response looks like below: 
# {"Pains": "text", "Gains": "text", "Gain Creators": "text", "Pain Relievers": "text"}

7. Extract structured data from the GPT response

In the previous step, you asked the GPT model for a Python dictionary of responses. But since LLMs can sometimes produce nonsensical responses, the returned string may not always contain just the necessary dictionary. In such cases, you may need to parse the response string to extract the dictionary and convert it to the Python dictionary type.

To accomplish this, you’ll need two standard libraries: re and ast.

The dict_from_string() function takes the response string from the LLM and returns a dictionary in this workflow:

Here is the code:

import re
import ast

def dict_from_string(gpt_response):
    """Function to parse GPT response and convert it to a dict"""
    # Find a substring that starts with '{' and ends with '}', across multiple lines
    match = re.search(r'\\{.*?\\}', gpt_response, re.DOTALL)
    dictionary = None
    if match:
        try:
            # Try to convert substring to dict
            dictionary = ast.literal_eval(match.group())
        except (ValueError, SyntaxError):
            # Not a dictionary
            return None
    return dictionary

swot_dict = dict_from_string(gpt_response=gpt_swot)
vp_dict = dict_from_string(gpt_response=gpt_value_prop)

8. Generate slides using python-pptx

Now that you have the data, it's time to fill out the slides. Use a PowerPoint template and replace the placeholders with actual values using the python-pptx library.

Here is what the SWOT slide template should look like:

To populate the slide with data, use the replace_text() function, which takes two arguments:

A dictionary with placeholders as keys and replacement text as values
A PowerPoint slide object

Use the swot_dict variable defined in the previous step:

from pptx import Presentation

def replace_text(replacements, slide):
    """function to replace text on a PowerPoint slide. Takes dict of {match: replacement, ... } and replaces all matches"""
    # Iterate through all shapes in the slide
    for shape in slide.shapes:
        for match, replacement in replacements.items():
            if shape.has_text_frame:
                if (shape.text.find(match)) != -1:
                    text_frame = shape.text_frame
                    for paragraph in text_frame.paragraphs:
                        whole_text = "".join(run.text for run in paragraph.runs)
                        whole_text = whole_text.replace(str(match), str(replacement))
                        for idx, run in enumerate(paragraph.runs):
                            if idx != 0:
                                p = paragraph._p
                                p.remove(run._r)
                        if bool(paragraph.runs):
                            paragraph.runs[0].text = whole_text

prs = Presentation("template.pptx")
swot_slide = prs.slides[2]

# create title for the slide
swot_title = 'SWOT Analysis of {}'.format('Company 1')

# initiate a dictionary of placeholders and values to replace
replaces_dict = {
    '{s}': swot_dict['Strengths'],
    '{w}': swot_dict['Weaknesses'],
    '{o}': swot_dict['Opportunities'],
    '{t}': swot_dict['Threats'],
    '{swot_title}': swot_title}

# run the function to replace placeholders with values
replace_text(replacements=replaces_dict, slide=swot_slide)


In short, the replace_text() function iterates over all shapes on a slide, looking for placeholder values, and replaces them with values from the dictionary if found.

Once all the slides have been filled with data or images, the presentation object is saved as binary output and passed to st.download_button() so that a user can download a PowerPoint file (read more here).

Here's what the download button should look like on the front end:

Download the resulting PPT here:

NVIDIA Corporation 2023 08 02
NVIDIA Corporation 2023-08-02.pptx 2 MB

And here’s the code:

from pptx import Presentation
from io import BytesIO
from datetime import date
import streamlit as st

# create file name
filename = '{name} {date}.pptx'.format(name='Company 1', date=date.today())

# save presentation as binary output
binary_output = BytesIO()
prs.save(binary_output)

# display success message and download button
st.success('The slides have been generated! :tada:')

st.download_button(label='Click to download PowerPoint',
                   data=binary_output.getvalue(),
                   file_name=filename)

Wrapping up

Thank you for reading to the end! Now you can develop a slide automation app for company research using Streamlit, Snowflake, YahooFinance, and LangChain. I hope you found something new and useful in this post.

As you can see, there are some limitations to the app. Firstly, it only generates research on public companies. Secondly, the GPT model uses general knowledge about a product, such as ChatBot or Accounting Software, to write a value proposition. In a more advanced app, the second constraint could be addressed by fine-tuning the model with your product data. This can be done by passing your product details in a prompt or storing this data as embeddings in a vector database (read more here).

If you have any questions or feedback, please post it in the comments below or contact me on GitHub, LinkedIn, or Twitter.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Announcing Streamlit for Data Science: Second Edition
https://blog.streamlit.io/streamlit-for-data-science-book/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

By Tyler Richards
Posted in Product, September 29 2023
Why I wrote it
Who is it for?
What is the book about?
How do I get a copy?
Contents
Share this post
← All posts

Do you love diving into one subject in depth instead of switching between Medium, Twitter, and five different documentation sites? Then check out the 2nd version of my book Streamlit for Data Science I just published!

Why I wrote it

If you’re new to Streamlit, it can feel tricky to know where to start. Do you follow people on Twitter? Start with your own project? What do you need to get started with the right foundations? How do you know if you’re missing a crucial piece of knowledge? There are so many resources and so little time.

Two years ago, I tried to solve this problem by writing the book I wanted to read when I started as a Streamlit developer.

I’m very proud of it! But…

Streamlit kept putting out release after release, making amazing changes. We got past the famous 1.0 version barrier with the introduction of new features like data editors, caching and visualization improvements, and component upgrades. As a result of this, the original book became increasingly outdated.

In addition, I left my job at Meta Integrity to join Streamlit as a data scientist, and just two months later, Streamlit was acquired by Snowflake. It was quite a whirlwind, to say the least.

So I wrote an updated version of the book. It includes new interviews with creators, power users, and employees, along with the experience I gained from building Streamlit apps with the Streamlit data team over the last year and a half. It also incorporates examples using the latest tools and platforms that have gained popularity, such as OpenAI and HuggingFace. Since the first version, I’ve created hundreds of apps, and this version contains all the lessons, both big and small, that I’ve learned.

A lot hasn't changed too. Streamlit is still my favorite Python library, my favorite way to show off data science work. I wrote this second version as a labor of love, and I hope that you find it as lovely to read as I did to write.

Who is it for?

This book is for data scientists and ML enthusiasts, especially those who are new to Streamlit or data science in general. You’ll get the most value from it if you already have knowledge of Python, and it’d also be helpful (though not necessary!) to have some familiarity with popular libraries like Pandas. But I highly recommend that you first explore the Streamlit documentation to see if it meets your requirements. It’s truly delightful, and I often refer to it myself. If you find all the information you need there, simply use the docs!

What is the book about?

Roughly the book contains three sections:

The first section gives an introduction to building basic Streamlit apps. It covers visualizations, understanding the execution model, and introduces popular libraries. By the end of this section, you’ll have a working app and an ML model that you can share with anyone!
The second section focuses on beauty and complex use cases. It covers Streamlit components, databases, animations, and generative AI. By the end of this section, you’ll have enough knowledge to create production-level Streamlit apps for work or for a large audience.
The final section is project-based and explores the use of Streamlit in a working environment. It includes interviews with power users, discusses using Streamlit for job applications, and gives you a better understanding of Streamlit’s long-term direction.
How do I get a copy?

You can always snag a copy on Amazon! Use this link to get 25% (it expires on October 31st). We're also giving away 10 copies on Twitter and LinkedIn. For a chance to win: follow @Streamlit, like this post, and tag a data nerd in the comments!

All proceeds for this copy are going to be donated to PyLadies. If you can’t afford it, shoot me a DM on Twitter and I’d be happy to send you a copy.

Can’t wait to hear your thoughts about the book.

Happy reading! 📕

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to improve Streamlit app loading speed
https://blog.streamlit.io/how-to-improve-streamlit-app-loading-speed/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

By Zachary Blackwood
Posted in Product, October 5 2023
My app is too slow
Profiling the app
Handling the slow query
What’s the general pattern?
Will this fix my slow app?
Wrapping up
Contents
Share this post
← All posts
👉
TL;DR: The article describes my experience of optimizing a slow Streamlit app that pulled data from Snowflake. The primary issue was with a specific long-running query. To solve this, I shifted the heavy data processing from the app to dbt by creating a pre-processed model, which led to a query speed improvement by 100 times. The general advice is to identify and move slow parts outside the Streamlit app if possible and to utilize caching features like st.cache_data for better performance.
My app is too slow

A few weeks ago, I used an internal Streamlit app to check the performance of a specific feature. But it took so long to load, that I often gave up before I actually got to use the app. So I checked the code. At first glance it looked all good. We were caching the results using st.cache_data. And the code itself wasn’t very complex. It was pulling data from a few tables in Snowflake, applying filters (some using Snowpark and some using Pandas) and displaying graphs and tables.

So why did it take so long to load the first time I visited it?

Profiling the app

Since the app used many queries, I first wanted to find which ones were slow.

We have a set of standard functions for querying and caching data from Snowflake, that look roughly like this:

I modified them so that, in addition to running the queries and caching the results, they’d also tell me how long each query took to run.

The final result looked like this:

Adding time() calls before and after the query and using st.write() to display the time difference helped me find the slow parts of the app. I could’ve used python -m cProfile streamlit_app.py to dig deeper, but it wasn't necessary in this case. Now I could start investigating.

Handling the slow query

I identified the slow parts of the query with Snowflake's Query Profiler. As you can see below, a specific Aggregate step took up a lot of time time. By clicking on it, I can learn more about what’s happening.

In this case, the slow query was performing two tasks that required significant processing:

Retrieving data from a large table
Performing complex transformations on the data

These transformations involved filtering the data based on multiple columns to obtain the latest rows and calculating statistics related to a Variant column. While the actual process was more intricate, this is a general overview of what was happening.

To speed up the query, I created a new model in dbt that:

Filtered the source table to include only the latest values each day.
Added a new column that pre-computed the list of values for the specific "key" of interest.

The process of flattening the data looked roughly like this:

with flattened_keys as (
    select
        id,
        flattened.value:"KEY"::string as value
    from
        BASE_TABLE as BT,
        lateral flatten(input => parse_json(p.COLUMN)) as flattened
    where
        flattened.value:"KEY" is not null
),

flattened_array as (
    select
        id,
        array_agg(value) as value_array
    from
        flattened_keys
    group by
        id
),

new_table as (
    select
        f.value_array,
        BT.*
    from BASE_TABLE as BT
        left join flattened_arrays as f
        using (id)
)

select * from new_table


Once I had the model and it was populated with data, I switched my app to pull from the new table, and I simplified the query that was performing computations by using this well-prepared table.

The app's query before:

with transformed as (
	select 
		id,
		count(flattened.value:KEY) AS num_values,
		count(distinct flattened.value:KEY) AS num_unique_values
	from BASE_TABLE,
		lateral flatten(BASE_TABLE.COLUMN) as flattened
	group by id
	{extra_filters}
)

select 
	transformed.num_values,
	transformed.num_unique_values
	BASE_TABLE.*,
from BASE_TABLE
	inner join transformed 
	using(id)


The query after:

select
    *,
    array_size(value_array) as num_values,
    array_size(array_distinct(value_array)) as num_unique_values,
from new_table
{extra_filters}


The result? The new query was 100 times faster than the old one! This improved the app’s usability and expanded the range of analyses that we could easily perform in a reasonable amount of time.

What’s the general pattern?

If your Streamlit app is slow, try moving the slowest parts outside of the app—like into a pre-processed table with dbt.

Will this fix my slow app?

Perhaps! It depends on the specific problem. Here are a few scenarios and the approaches you might take to resolve them:

My app is query-driven and the first run is very slow. Try pre-computing the table you will need outside of your app like I did above.
My app is slow every time I run it. Are you using st.cache_data or st.cache_resource to prevent rerunning slow processes? For more information, read the Streamlit documentation on caching.
My app pulls data from a CSV, not from a database. Consider performing a similar process to what I did with my data. Get the raw data, execute queries to process it, and save the pre-processed file for use in your app. For faster loading, use a Parquet file format instead of CSV.
My app is slow for a different reason. Try figuring out which part is the slowest, and if you can find a way to do the slowest part of the work outside of your app itself. For example, if your app trains a machine learning model, try moving the model training outside of the app itself, save the model once it’s trained, and then use the pre-trained model to make predictions within your app.
Wrapping up

If your Streamlit app is too slow, consider adding profiling to identify the slow parts, especially data queries. We sped up our apps by offloading resource-intensive tasks to dbt and optimizing the problematic query. By moving heavy workloads outside the app, you can create a faster and more user-friendly experience.

If you have any questions, please post them in the comments below or contact me on Twitter.

Happy app-fixing! 🛠️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tyler Richards - Streamlit
https://blog.streamlit.io/author/tyler/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Tyler Richards
4 posts
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

Tutorials
by
Tyler Richards and 
2
 more,
August 18 2022
How Streamlit uses Streamlit: Sharing contextual apps

Learn about session state and query parameters!

Tutorials
by
Tyler Richards
,
May 26 2022
Deploying Streamlit apps using Streamlit sharing

A sneak peek into Streamlit's new deployment platform

Tutorials
by
Tyler Richards
,
October 15 2020
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

AI Interviewer: Customized interview preparation with generative AI
https://blog.streamlit.io/ai-interviewer-customized-interview-preparation-with-generative-ai/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
AI Interviewer: Customized interview preparation with generative AI

How we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!

By Haoxiang Jia and Todd Wang
Posted in LLMs, August 9 2023
Why AI Interviewer?
App overview
1. Create a chatbot interviewer
2. Create vector embeddings and initialize session state
3. Generate interview questions with a similarity search
OpenAI Whisper
Amazon Polly
4. Build your app!
Wrapping up
Contents
Share this post
← All posts
🤖
TL;DR: The AI Interviewer app generates job-specific interview questions, covers diverse interview types, and offers personalized evaluations. The development involved creating vector embeddings, establishing interview guidelines, managing session states, and integrating optional voice interactions. Here is the GitHub code. Have fun!

Hey, community! 👋

We’re two graduating MSBA students at Fordham University, Todd Wang (also known as Zicheng Wang) and Haoxiang Jia. Both of us struggle with interview preparations, so we discovered a shared goal: to make generative AI play an interviewer for us!

In this post, we’ll show you how you can build your own AI Interviewer:

Create a chatbot interviewer
Create vector embeddings and initialize session state
Create a callback function that memorizes conversation history
Build your app!


Why AI Interviewer?

When we started looking for jobs, we saw lots of job listings under the same title but with different descriptions. Many graduates resort to a "one size fits all" approach, but that meant potentially missing a particular job’s requirements.

After much thought, we decided to build an app that could help with:

Customized interview preparation. Just type in a job description and get targeted interview questions that simulate a real-life interview (no need for prompt engineering). This helps you prepare for interviews perfectly aligned with every job posting.

Comprehensive coverage. Prepare for various types of interviews:

Professional interview—focuses on technical skills and industry knowledge. For example, you may be asked to design and implement a feature.
Behavioral interview—focuses on how the candidate handles specific situations in the workplace. For example, you may be asked to describe how you dealt with a difficult coworker.
Resume interview—focuses on the candidate's work experience, education, and skills as listed on their resume. For example, you may be asked to elaborate on past job responsibilities or explain a particular accomplishment.

Personalized evaluation and guidance. Get interview evaluations with feedback and actionable insights to refine your interview skills and improve your performance!

App overview

Before we dive into coding, let’s take a look at a two-step development process instead of a single RetrievalQA step (it leads to better performance):

Creating vector embeddings. This is a technique of representing words as numbers to make them more computationally accessible.
Developing an interviewer guideline. This requires careful planning to ensure that the questions asked are relevant and comprehensive. Once the guidelines are established, the actual interview can take place.

Now, let’s get to coding!

1. Create a chatbot interviewer

To start, create an efficient chatbot interviewer that can help you develop prompt templates and save them for later use.

Use this template to construct an interview guide for behavioral screening:

class templates: 
	""" store all prompts templates """
	behavioral_template = """ I want you to act as an interviewer. Remember, you are the interviewer not the candidate.   
            Let's think step by step.
            
            Based on the keywords, 
            Create a guideline with the following topics for a behavioral interview to test the soft skills of the candidate. 
            
            Do not ask the same question.
            Do not repeat the question. 
            
            Keywords: 
            {context}
       
            Question: {question}
            Answer:"""

	conversation_template = """I want you to act as an interviewer strictly following the guideline in the current conversation.
                            Candidate has no idea what the guideline is.
                            Ask me questions and wait for my answers. Do not write explanations.
                            Ask each question like a real person, only one question at a time.
                            Do not ask the same question.
                            Do not repeat the question.
                            Do ask follow-up questions if necessary. 
                            Your name is GPTInterviewer.
                            I want you to only reply as an interviewer.
                            Do not write all the conversation at once.
                            If there is an error, point it out.

                            Current Conversation:
                            {history}

                            Candidate: {input}
                            AI: """)



🤖
NOTE: Context refers to the embeddings of keywords or job descriptions entered.
2. Create vector embeddings and initialize session state

Next, create a function that utilizes FAISS to generate vector embeddings. Since the job description or resume text isn’t long, use the NLTKTextSplitter instead of RecursiveCharacterTextSplitter for better results. For longer texts, split them into chunks and process them individually to avoid loss of information or context.

When working with vector embeddings, use the appropriate text splitter and chunk size to get desired results:

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import NLTKTextSplitter
jd = st.text_area("Please enter the job description here ()")

'''The variable "jd" was passed to the embeddings function later on.'''

def embeddings(text):
	text_splitter = NLTKTextSplitter()
	texts = text_splitter.split_text(text)
	embeddings = OpenAIEmbeddings()
	docsearch = FAISS.from_texts(texts, embeddings)
	retriever = docsearch.as_retriever(search_tupe='similarity search')
	return retriever


The function returns a variable called "retriever", which users can later use to generate interview questions that are relevant, targeted, and effective.

It’s also important to distinguish between messages from AI and humans. Use specific markers or tags to identify the message source, properly attribute each message, and maintain clear communication with their interviewees:

from dataclasses import dataclass
from typing import Literal

@dataclass
class Message:
	origin: Literal["human", "ai"]
	message: str


Now, let's write a function to initialize session states:

from langchain

def initialize_session_state():
    if "retriever" not in st.session_state:
        st.session_state.retriever = embeddings(jd)

    if "chain_type_kwargs" not in st.session_state:
        Behavioral_Prompt = PromptTemplate(input_variables=["context", "question"],
                                          template=templates.behavioral_template)
        st.session_state.chain_type_kwargs = {"prompt": Behavioral_Prompt}
    # interview history
    if "history" not in st.session_state:
        st.session_state.history = []
        st.session_state.history.append(Message("ai", "Hello there! I am your interviewer today. I will access your soft skills through a series of questions. Let's get started! Please start by saying hello or introducing yourself. Note: The maximum length of your answer is 4097 tokens!"))

    # token count
    if "token_count" not in st.session_state:
        st.session_state.token_count = 0
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory()
    if "guideline" not in st.session_state:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.8)
        st.session_state.guideline = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type_kwargs=st.session_state.chain_type_kwargs, chain_type='stuff',
            retriever=st.session_state.retriever, memory=st.session_state.memory).run(
            "Create an interview guideline and prepare total of 8 questions. Make sure the questions test the soft skills")
  
    if "conversation" not in st.session_state:
        llm = ChatOpenAI(
        model_name = "gpt-3.5-turbo",
        temperature = 0.8)
        PROMPT = PromptTemplate(
            input_variables=["history", "input"],
            template=templates.conversation_template)
        st.session_state.conversation = ConversationChain(prompt=PROMPT, llm=llm,
                                                       memory=st.session_state.memory)
    if "feedback" not in st.session_state:
        llm = ChatOpenAI(
        model_name = "gpt-3.5-turbo",
        temperature = 0.5)
        st.session_state.feedback = ConversationChain(
            prompt=PromptTemplate(input_variables = ["history", "input"], template = templates.feedback_template),
            llm=llm,
            memory = st.session_state.memory
        )


Session state has several features that provide a complete interview experience:

Chain_type_kwargs: responsible for creating a customizable prompt template for the RetrievalQA chain (enables the system to generate questions tailored to the specific job description).
History: keeps track of the messages exchanged during the interview (helps the system to maintain context and avoid asking repetitive questions).
Token_count: keeps track of the number of tokens consumed during the interview (measures the interview's duration and ensures that the system doesn’t ask too many questions).
Memory: initializes a memory buffer to keep track of the context (helps the system to remember important details about the candidate's qualifications and experience).
Guideline: uses the RetrievalQA chain to generate an interview guideline based on the job description (provides a framework for the interview and ensures that all relevant topics are covered).
Conversation: uses the conversation chain to conduct the interview based on the guideline (enables the system to ask follow-up questions and engage in a more natural conversation with the candidate).
Feedback: uses the LLM chain to generate feedback based on the context (provides the candidate with valuable insights into their performance during the interview and helps them to improve their skills for future interviews).
3. Generate interview questions with a similarity search

Now that you’ve completed the initialization steps for your conversation chain, let’s move on to the callback function—the backbone of our chatbot (without it, our chatbot wouldn’t be able to talk to users).

The callback function takes the user's text input (the latest response saved to and pulled from st.session_state.history) and applies the necessary logic to generate an appropriate response. Once it’s generated, it gets appended to the conversation history. This allows the chatbot to remember the context of previous messages.

But that's not all!

We’ve also added an optional voice interaction feature. It allows the user to listen to the chatbot's response and input their own responses using their voice. This can be incredibly helpful for users who wish to practice their speaking skills. The audio widget is returned by the callback function, giving the user the option to listen to the chatbot's response instead of reading it.

from IPython.display import Audio

def answer_call_back():
    with get_openai_callback() as cb:
        # user input
        human_answer = st.session_state.answer
        # transcribe audio
        if voice:
            save_wav_file("temp/audio.wav", human_answer)
            try:
                input = transcribe("temp/audio.wav")
                # save human_answer to history
            except:
                st.session_state.history.append(Message("ai", "Sorry, I didn't get that. Please try again."))
        else:
            input = human_answer

        st.session_state.history.append(
            Message("human", input)
        )
        # OpenAI answer and save to history
        llm_answer = st.session_state.conversation.run(input)
        # speech synthesis and speak out
        audio_file_path = synthesize_speech(llm_answer)
        # create audio widget with autoplay
        audio_widget = Audio(audio_file_path, autoplay=True)
        # save audio data to history
        st.session_state.history.append(
            Message("ai", llm_answer)
        )
        st.session_state.token_count += cb.total_tokens
        return audio_widget
				


We’ve defined the "transcribe" function for Speech-to-Text and "Audio" for Text-to-Speech. There are many APIs available for performing these tasks. Specifically, we used OpenAI Whisper for Speech-to-Text and Amazon Polly for Text-to-Speech.

OpenAI Whisper
import wave
import os 
import openai
class Config:
    channels = 2
    sample_width = 2
    sample_rate = 44100

def save_wav_file(file_path, wav_bytes):
    with wave.open(file_path, 'wb') as wav_file:
        wav_file.setnchannels(Config.channels)
        wav_file.setsampwidth(Config.sample_width)
        wav_file.setframerate(Config.sample_rate)
        wav_file.writeframes(wav_bytes)

def transcribe(file_path):
    audio_file = open(file_path, 'rb')
    transcription = openai.Audio.transcribe("whisper-1", audio_file)
    return transcription['text']

Amazon Polly
import boto3
from contextlib import closing
import sys
from tempfile import gettempdir 

Session = boto3.Session(
        region_name = "us-east-1"
    )

def synthesize_speech(text):
    Polly = Session.client("polly")
    response = Polly.synthesize_speech(
        Text=text,
        OutputFormat="mp3",
        VoiceId="Joanna")
    if "AudioStream" in response:
        # Note: Closing the stream is important because the service throttles on the
        # number of parallel connections. Here we are using contextlib.closing to
        # ensure the close method of the stream object will be called automatically
        # at the end of the with statement's scope.
        with closing(response["AudioStream"]) as stream:
            output = os.path.join(gettempdir(), "speech.mp3")

            try:
                # Open a file for writing the output as a binary stream
                with open(output, "wb") as file:
                    file.write(stream.read())
            except IOError as error:
                # Could not write to file, exit gracefully
                print(error)
                sys.exit(-1)
    else:
        # The response didn't contain audio data, exit gracefully
        print("Could not stream audio")
        sys.exit(-1)
    '''
    # Play the audio using the platform's default player
    if sys.platform == "win32":
        os.startfile(output)
    else:
        # The following works on macOS and Linux. (Darwin = mac, xdg-open = linux).
        opener = "open" if sys.platform == "darwin" else "xdg-open"
        subprocess.call([opener, output])



🤖
NOTE: You can store your API keys in .streamlit/secrets.toml. This is a convenient and secure way to manage your API keys.
4. Build your app!

With the initialization function and callback function in place, you can build your app!

Here is the code:

# submit job description
jd = st.text_area("Please enter the job description here (If you don't have one, enter keywords, such as PostgreSQL or Python instead): ")
# auto play audio
auto_play = st.checkbox("Let AI interviewer speak! (Please don't switch during the interview)")

if jd:
    # initialize session states
    initialize_session_state()
		# feedback requested button 
		feedback = st.button("Get Interview Feedback")

    token_placeholder = st.empty()
    chat_placeholder = st.container()
    answer_placeholder = st.container()

	  # initialize an audio widget with None 
		audio = None
		
		# if feedback button has been clicked, run the feedback chain and terminate the interview
    if feedback:
        evaluation = st.session_state.feedback.run("please give evalution regarding the interview")
        st.markdown(evaluation)
        st.stop()
    else:
        with answer_placeholder:
						# choose the way of input 
            voice: bool = st.checkbox("I would like to speak with AI Interviewer")
            if voice:
								# audio input 
                answer = audio_recorder(pause_threshold = 2.5, sample_rate = 44100)
            else:
								# message input
                answer = st.chat_input("Your answer")
						# run the callback function, generate response, and return a audio widget
						if answer:
                st.session_state['answer'] = answer
                audio = answer_call_back()

        # chat_placeholder is use to display the chat history
				with chat_placeholder:
            for answer in st.session_state.history:
                if answer.origin == 'ai':
										# if user choose auto play, return both AI outputs and its audio
                    if auto_play and audio:
                        with st.chat_message("assistant"):
                            st.write(answer.message)
                            st.write(audio)
                    else:
										# only return AI outputs
                        with st.chat_message("assistant"):
                            st.write(answer.message)
                else:
										# user inputs 
                    with st.chat_message("user"):
                        st.write(answer.message)
				
				# keep track of token consumed 
		    token_placeholder.caption(f"""
        Used {st.session_state.token_count} tokens """)
else:
    st.info("Please submit a job description to start the interview.")


And here is the app in action!

Wrapping up

Thank you for reading our post. We hope it has inspired you to make your own Streamlit app. If you have any questions, please post them in the comments below or contact us on LinkedIn or Twitter.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Zachary Blackwood - Streamlit
https://blog.streamlit.io/author/zachary_b/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Zachary Blackwood
3 posts
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
How to build your own Streamlit component

Learn how to make a component from scratch!

Tutorials
by
Zachary Blackwood
,
September 15 2022
Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!

Learn how to add a UI to any dataframe

Tutorials
by
Tyler Richards and 
2
 more,
August 18 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts: Using LLMs with Streamlit
https://blog.streamlit.io/tag/llms/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in LLMs
33 posts
Decoding Warren Buffett with LLMs and Snowflake SQL

How I built Ask the Oracle of Omaha in less than a day!

LLMs
by
Randy Pettus
,
August 1 2023
Chat2VIS: AI-driven visualisations with Streamlit and natural language

Leverage ChatGPT for Python code generation using prompt engineering

LLMs
by
Paula Maddigan
,
July 27 2023
LangChain tutorial #5: Build an Ask the Data app

Leverage Agents in LangChain to interact with pandas DataFrame

LLMs
by
Chanin Nantasenamat
,
July 21 2023
How to build a Llama 2 chatbot

Experiment with this open-source LLM from Meta

LLMs
by
Chanin Nantasenamat
,
July 21 2023
Beginner’s guide to OpenAI API

Build your own LLM tool from scratch

LLMs
by
Chanin Nantasenamat
,
July 20 2023
How to build an interconnected multi-page Streamlit app

From planning to execution—how I built GPT lab

LLMs
by
Dave Lin
,
July 19 2023
LangChain 🤝 Streamlit

The initial integration of Streamlit with LangChain and our future plans

LLMs
by
Joshua Carroll
,
July 11 2023
Generate interview questions from a candidate’s tweets

Make an AI assistant to prepare for interviews with LangChain and Streamlit

LLMs
by
Greg Kamradt
,
June 24 2023
LangChain tutorial #4: Build an Ask the Doc app

How to get answers from documents using embeddings, a vector store, and a question-answering chain

LLMs
by
Chanin Nantasenamat
,
June 20 2023
Building a Streamlit and scikit-learn app with ChatGPT

Catching up on coding skills with an AI assistant

LLMs
by
Michael Hunger
,
June 16 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Oleksandr Arsentiev - Streamlit
https://blog.streamlit.io/author/oleksandr/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Oleksandr Arsentiev
1 post
Instant Insight: Generate data-driven presentations in a snap!

Create presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery

LLMs
by
Oleksandr Arsentiev
,
August 2 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Exploring LLMs and prompts: A guide to the PromptTools Playground
https://blog.streamlit.io/exploring-llms-and-prompts-a-guide-to-the-prompttools-playground/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Exploring LLMs and prompts: A guide to the PromptTools Playground

Learn how to build dynamic, stateful applications that harness multiple LLMs at once

By Steve Krawczyk and Kevin Tse
Posted in LLMs, August 18 2023
Why PromptTools Playground?
How it can help you
Example use case
1. Creating a grid for inputs and outputs
2. Building a dynamic sidebar for app configuration
3. Creating shareable links
Best practices for using PromptTools
Wrapping up
Contents
Share this post
← All posts
🦉
TL;DR: The PromptTools Playground app allows developers to experiment with multiple prompts and models simultaneously using large language models (LLMs) and vector databases. The post details important features, such as creating grids for inputs and outputs, building dynamic sidebars for app configuration, and enabling shareable links. You can find the code on GitHub.



Hey Streamlit community! 👋

I'm Steve Krawczyk, one of the co-founders of Hegel AI. We build open-source tools for working with large language models (LLMs) and vector databases. In this post, I'll provide insights into our PromptTools Playground, a project we've created to allow developers to experiment with multiple prompts and models simultaneously using our open-source library.

I'll guide you through the app's features and the ways we've implemented them:

Creating a grid for inputs and outputs
Building a dynamic sidebar for app configuration
Creating shareable links (experimental)

Why PromptTools Playground?

PromptTools is a library designed for experimenting with, testing, and evaluating LLMs and VectorDBs. It provides a user-friendly interface for constructing and executing requests to LLMs. And it speeds up our work with LLMs and prompts for our applications and to compare new open-source models with GPT-3.5 and 4.

How it can help you
For some use cases, running an instance of Llama 2 on your laptop may suffice. But for others, you might need to use OpenAI. If you experiment early in development, you can determine the optimal fit to improve performance and save costs.
Optimizing prompts can be challenging. You can make informed decisions quickly by experimenting with multiple prompts and comparing responses. We support various model providers, including OpenAI, Anthropic, HuggingFace, Google, and local models through LlamaCpp. Users can compare templates, instructions, and messages and evaluate models head-to-head in the same table.
Setting up and running such experiments can require writing custom code for each LLM and prompting strategy you wish to try. PromptTools simplifies the process of testing LLMs and prompts without investing significant upfront time and effort—they abstract the less relevant parts, allowing you to focus on what matters.
Example use case

Some of our users use LLMs to generate outbound sales emails. They experiment with various prompt templates that include variables, then pass them into an LLM to create the email. For example, they may use templates such as the following:

You are a sales development representative at {{firm_name}}, which sells {{product_description}}. Your job is to write outbound emails to prospects given some information about them. Using the following details, write an email to the given prospect: Name: {{prospect_name}} Role: {{prospect_role}} Company: {{prospect_company}}

Users typically test different versions of a template across many cases to determine which one will work best when they take the prompts into production.

Before PromptTools, they'd run the prompts one at a time on a single model, and the results had to be manually tracked. With PromptTools, they can run all prompts simultaneously, and the results can be viewed on a single page. This speeds up the iteration process, whether you're using LLMs for productivity and drafting or testing a prompt for a production system.

Now let's get to building the app.

1. Creating a grid for inputs and outputs

The core idea behind our UI is that users can iterate over two lists simultaneously to experiment with LLM inputs, such as system and user messages, prompt templates and variables, or models and prompts. We then utilize the Cartesian product of these lists to craft a set of requests that are sent and displayed for manual evaluation.

At first, we tried to use the dataframe component. But it was hard to align the dataframe with the input cells to create a "table" of inputs and outputs. So we switched to columns and placeholder cells for the output. By default, the components of a Streamlit app are displayed vertically. We used columns to arrange the components horizontally as well.

Here is the code to display the outputs:

    # Add placeholders for output
    placeholders = [[st.empty() for _ in range(instruction_count + 1)] for _ in range(prompt_count)]

    cols = st.columns(instruction_count + 1)

    # Create top row for instructions or system messages
    with cols[0]:
        a = None
    instructions = []
    for j in range(1, instruction_count + 1):
        with cols[j]:
            instructions.append(
                st.text_area(
                    "System Message",
                    value="You are a helpful AI assistant.",
                    key=f"col_{j}",
                )
            )

    # Create rows for prompts, and output placeholders
    prompts = []
    for i in range(prompt_count):
        cols = st.columns(instruction_count + 1)
        with cols[0]:
            prompts.append(
                st.text_area(
                    "User Message",
                    key=f"row_{i}"
                )
            )
        for j in range(1, instruction_count + 1):
            with cols[j]:
                placeholders[i][j] = st.empty()  # placeholders for the future output
        st.divider()


To make a table listing instructions and prompts, we created a column for each instruction and another for prompts. Next, we iterated over the number of rows equal to the number of prompts. Each row contained a prompt input cell and as many output cells as there were instructions.

Here is the resulting table:

This allows our users to easily visualize how a system and user message are mapped onto the outputs we create.

2. Building a dynamic sidebar for app configuration

Streamlit has a handy sidebar component where most configuration UI is kept separate from system and user messages. In our case, the inputs displayed on this sidebar are dynamic. When running the app in "Instruction" mode, you'll see something like this:

But, if you're running in "Model Comparison" mode, you'll need to enter all the API keys for the models you're testing on the sidebar and configure the individual models in the first row of the table.

To implement a sidebar with dynamic input fields, use this code:

with st.sidebar:
    mode = st.radio("Choose a mode", MODES)
    if mode != "Model Comparison":
        model_type = st.selectbox(
            "Model Type", MODEL_TYPES
        )
        model, api_key = None, None
        if model_type in {"LlamaCpp Chat", "LlamaCpp Completion"}:
            model = st.text_input("Local Model Path", key="llama_cpp_model_path")
        elif model_type == "OpenAI Chat":
            model = st.selectbox(
                "Model",
                OPENAI_CHAT_MODELS
            )
            api_key = st.text_input("OpenAI API Key", type='password')
        elif model_type == "OpenAI Completion":
            model = st.selectbox(
                "Model",
                OPENAI_COMPLETION_MODELS
            )
            api_key = st.text_input("OpenAI API Key", type='password')
    else:
        model_count = st.number_input("Add Model", step=1, min_value=1, max_value=5)
        prompt_count = st.number_input("Add Prompt", step=1, min_value=1, max_value=10)
        openai_api_key = st.text_input("OpenAI API Key", type='password')
        anthropic_api_key = st.text_input("Anthropic Key", type='password')
        google_api_key = st.text_input("Google PaLM Key", type='password')
        hf_api_key = st.text_input("HuggingFace Hub Key", type='password')


It's important to understand the Streamlit execution model: after every user interaction, the app reruns top-to-bottom. So you need to organize your UI components in order of conditional dependency, with components at the top determining which components we display later.

3. Creating shareable links

Our application also enables users to create and share links for their experiment setup with others. This is done by using the experimental_get_query_params() function from Streamlit:

params = st.experimental_get_query_params()


Next, set defaults using the provided query parameters:

if 'mode' not in st.session_state and 'mode' in params:
        st.session_state.mode = unquote(params["mode"][0])
mode = st.radio("Choose a mode", MODES, key="mode")


One concern with using query parameters as defaults is that users may need to edit these inputs. Streamlit reruns every time a user interacts with the app, causing the query parameters to be reread and used to set input values every time. To prevent this, modify the app to clear the query parameters after reading them. This way, they're only used to populate the app once:

params = st.experimental_get_query_params()
st.experimental_set_query_params()


Users can make changes by setting these values once without modifying the code repeatedly.

🦉
If you’re working locally, you can use pyperclip to copy the link to the clipboard. But if you’ve deployed your app to Streamlit Community Cloud, note that pyperclip can’t directly copy to a clipboard from the app. Instead, use st.code to display the link, which users can then copy.


if share:
    try:
        pyperclip.copy(link)
    except pyperclip.PyperclipException:
        st.write("Please copy the following link:")
        st.code(link)


This is the easiest way to make links shareable for locally and cloud-hosted app versions.

Best practices for using PromptTools

When writing and evaluating prompts for LLMs, there are a few things to keep in mind:

The same prompts can perform differently on different models. So it's worth trying multiple prompts when comparing models and checking where each model's strengths lie.
Parameters like temperature can have just as much of an impact on the model output as the prompt itself. If a model isn't giving you the desired results after trying multiple prompts, try tweaking other settings in the sidebar to see how they impact the model's response quality.
The evaluation depends on the specific use case. While benchmarks can generally indicate a model's power, you won't know if the model can perform well on your particular use case until you create a test set and decide on your evaluation criteria. If you want to run evaluations automatically, you can use our SDK to run experiments at scale.

Prompt engineering still requires patience and experimentation, but with PromptTools, you can move faster from your first idea to a working prompt and model!

Wrapping up

We used a few Streamlit tricks to make our app and plan to introduce more features. We hope this will be of great benefit to the community, and we'll continue to develop new features, including support for experimentation with vector databases and chains, storing experiments in a database using st.experimental_connection, and loading test data from files or other sources.

If you found this interesting or helpful, check out the app, give our GitHub repo a star ⭐, and join our Discord community. Refer to our notebook examples when you're ready to move from the playground to production code. We provide functionality to auto-evaluate and validate your outputs.

Happy coding! 😎

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Build a chatbot with custom data sources, powered by LlamaIndex
https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

By Caroline Frasca, Krista Muir and Yi Ding
Posted in LLMs, August 23 2023
What is LlamaIndex?
How to build a custom chatbot using LlamaIndex
In 43 lines of code, this app will:
1. Configure app secrets
2. Install dependencies
2.1. Local development
2.2. Cloud development
3. Build the app
3.1. Import libraries
3.2. Initialize message history
3.3. Load and index data
3.4. Create the chat engine
3.5. Prompt for user input and display message history
3.6. Pass query to chat engine and display response
4. Deploy the app!
LlamaIndex helps prevent hallucinations
What is Streamlit’s experimental connection feature?
Wrapping up
Contents
Share this post
← All posts
🎈
TL;DR: Learn how LlamaIndex can enrich your LLM model with custom data sources through RAG pipelines. Build a chatbot app using LlamaIndex to augment GPT-3.5 with Streamlit documentation in just 43 lines of code.

So, you want to build a reliable chatbot using LLMs based on custom data sources?

Models like GPT are excellent at answering general questions from public data sources but aren't perfect. Accuracy takes a nose dive when you need to access domain expertise, recent data, or proprietary data sources.

Enhancing your LLM with custom data sources can feel overwhelming, especially when data is distributed across multiple (and siloed) applications, formats, and data stores.

This is where LlamaIndex comes in.

LlamaIndex is a flexible framework that enables LLM applications to ingest, structure, access, and retrieve private data sources. The end result is that your model's responses will be more relevant and context-specific. Together with Streamlit, LlamaIndex empowers you to quickly create LLM-enabled apps enriched by your data. In fact, the LlamaIndex team used Streamlit to prototype and run experiments early in their journey, including their initial proofs of concept!

In this post, we'll show you how to build a chatbot using LlamaIndex to augment GPT-3.5 with Streamlit documentation in four simple steps:

Configure app secrets
Install dependencies
Build the app
Deploy the app!
What is LlamaIndex?

Before we get started, let's walk through the basics of LlamaIndex.

Behind the scenes, LlamaIndex enriches your model with custom data sources through Retrieval Augmented Generation (RAG).

Overly simplified, this process generally consists of two stages:

An indexing stage. LlamaIndex prepares the knowledge base by ingesting data and converting it into Documents. It parses metadata from those documents (text, relationships, and so on) into nodes and creates queryable indices from these chunks into the Knowledge Base.
A querying stage. Relevant context is retrieved from the knowledge base to assist the model in responding to queries. The querying stage ensures the model can access data not included in its original training data.

💬
LlamaIndex for any level: Tasks like enriching models with contextual data and constructing RAG pipelines have typically been reserved for experienced engineers, but LlamaIndex enables developers of all experience levels to approach this work. Whether you’re a beginner looking to get started in three lines of code, LlamaIndex unlocks the ability to supercharge your apps with both AI and your own data. For more complex applications, check out Llama Lab.

No matter what your LLM data stack looks like, LlamaIndex and LlamaHub likely already have an integration, and new integrations are added daily. Integrations with LLM providers, vector stores, data loaders, evaluation providers, and agent tools are already built.

LlamaIndex's Chat Engines pair nicely with Streamlit's chat elements, making building a contextually relevant chatbot fast and easy.

Let's unpack how to build one.

How to build a custom chatbot using LlamaIndex
🦙
Want to jump right in? Check out the app and the code.
In 43 lines of code, this app will:
Use LlamaIndex to load and index data. Specifically, we're using the markdown files that make up Streamlit's documentation (you can sub in your data if you want).
Create a chat UI with Streamlit's st.chat_input and st.chat_message methods
Store and update the chatbot's message history using the session state
Augment GPT-3.5 with the loaded, indexed data through LlamaIndex's chat engine interface so that the model provides relevant responses based on Streamlit's recent documentation

Try the app for yourself:

1. Configure app secrets

This app will use GPT-3.5, so you'll also need an OpenAI API key. Follow our instructions here if you don't already have one.

Create a secrets.toml file with the following contents.

If you're using Git, be sure to add the name of this file to your .gitignore so you don't accidentally expose your API key.
If you plan to deploy this app on Streamlit Community Cloud, the following contents should be added to your app's secrets via the Community Cloud modal.


openai_key = "<your OpenAI API key here>"

2. Install dependencies
2.1. Local development

If you're working on your local machine, install dependencies using pip:

pip install streamlit openai llama-index nltk

2.2. Cloud development

If you're planning to deploy this app on Streamlit Community Cloud, create a requirements.txt file with the following contents:

streamlit
openai
llama-index
nltk

3. Build the app

The full app is only 43 lines of code. Let's break down each section.

3.1. Import libraries

Required Python libraries for this app: streamlit, llama_index, openai, and nltk.

import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader

3.2. Initialize message history
Set your OpenAI API key from the app's secrets.
Add a heading for your app.
Use session state to keep track of your chatbot's message history.
Initialize the value of st.session_state.messages to include the chatbot's starting message, such as, "Ask me a question about Streamlit's open-source Python library!"


openai.api_key = st.secrets.openai_key
st.header("Chat with the Streamlit docs 💬 📚")

if "messages" not in st.session_state.keys(): # Initialize the chat message history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about Streamlit's open-source Python library!"}
    ]

3.3. Load and index data

Store your Knowledge Base files in a folder called data within the app. But before you begin…

Download the markdown files for Streamlit's documentation from the data demo app's GitHub repository folder. Or use this link to download a .zip file for the repo. Add the data folder to the root level of your app. Alternatively, add your data.

🎈
If you’re running your app locally, check out LlamaIndex’s library of data connectors, available via LlamaHub, which makes it fast and easy to retrieve data from a variety of sources (including GitHub repositories).

Define a function called load_data(), which will:

Use LlamaIndex’s SimpleDirectoryReader to passLlamaIndex's the folder where you’ve stored your data (in this case, it’s called data and sits at the base level of your repository).
SimpleDirectoryReader will select the appropriate file reader based on the extensions of the files in that directory (.md files for this example) and will load all files recursively from that directory when we call reader.load_data().
Construct an instance of LlamaIndex’s ServiceContext, whichLlamaIndex'stion of resources used during a RAG pipeline's indexing and querying stages.
ServiceContext allows us to adjust settings such as the LLM and embedding model used.
Use LlamaIndex’s VectorStoreIndex to creaLlamaIndex'sory SimpleVectorStore, which will structure your data in a way that helps your model quickly retrieve context from your data. Learn more about LlamaIndex’s Indices here. This function returns the VectorStoreIndex object.

This function is wrapped in Streamlit’s caching decorator st.cache_resource to minimize the number of times the data is loaded and indexed.

Finally, call the load_data function, designating its returned VectorStoreIndex object to be called index.

@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="Loading and indexing the Streamlit docs – hang tight! This should take 1-2 minutes."):
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="You are an expert on the Streamlit Python library and your job is to answer technical questions. Assume that all questions are related to the Streamlit Python library. Keep your answers technical and based on facts – do not hallucinate features."))
        index = VectorStoreIndex.from_documents(docs, service_context=service_context)
        return index

index = load_data()

3.4. Create the chat engine

LlamaIndex offers several different modes of chat engines. It can be helpful to test each mode with questions specific to your knowledge base and use case, comparing the response generated by the model in each mode.

LlamaIndex has four different chat engines:

Condense question engine: Always queries the knowledge base. Can have trouble with meta questions like “What did I previously ask you?”
Context chat engin": Always queries the knowledge base and uses retrieved text from the knowledge base as context for following queries. The retrieved context from previous queries can take up much of the available context for the current query.
ReAct agent: Chooses whether to query the knowledge base or not. Its performance is more dependent on the quality of the LLM. You may need to coerce the chat engine to correctly choose whether to query the knowledge base.
OpenAI agent: Chooses whether to query the knowledge base or not—similar to ReAct agent mode, but uses OpenAI’s built-in fuOpenAI'salling capabilities.

This example uses the condense question mode because it always queries the knowledge base (files from the Streamlit docs) when generating a response. This mode is optimal because you want the model to keep its answers specific to the features mentioned in Streamlit’s documentation.

chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

3.5. Prompt for user input and display message history
Use Streamlit’s st.chat_input feature Streamlit'she user to enter a question.
Once the user has entered input, add that input to the message history by appending it st.session_state.messages.
Show the message history of the chatbot by iterating through the content associated with the “messages” key in the session state and displaying each message using st.chat_message.


if prompt := st.chat_input("Your question"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

3.6. Pass query to chat engine and display response

If the last message in the message history is not from the chatbot, pass the message content to the chat engine via chat_engine.chat(), write the response to the UI using st.write and st.chat_message, and add the chat engine’s response to the message history.

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history

4. Deploy the app!

After building the app, deploy it on Streamlit Community Cloud:

Create a GitHub repository.
Navigate to Streamlit Community Cloud, click New app, and pick the appropriate repository, branch, and file path.
Hit Deploy.
LlamaIndex helps prevent hallucinations

Now that you’ve built a Streamlit docs chatbot using up-to-date markdown files, how do these results compare the results to ChatGPT? GPT-3.5 and 4 have only been trained on data up to September 2021. They’re missing three years of new releases! Augmenting your LLM with LlamaIndex ensures higher accuracy of the response.

What is Streamlit’s experimental connection feature?
Wrapping up

You learned how the LlamaIndex framework can create RAG pipelines and supplement a model with your data.

You also built a chatbot app that uses LlamaIndex to augment GPT-3.5 in 43 lines of code. The Streamlit documentation can be substituted for any custom data source. The result is an app that yields far more accurate and up-to-date answers to questions about the Streamlit open-source Python library compared to ChatGPT or using GPT alone.

Check out our LLM gallery for inspiration to build even more LLM-powered apps, and share your questions in the comments.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Land your dream job: Build your portfolio with Streamlit
https://blog.streamlit.io/land-your-dream-job-build-your-portfolio-with-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Land your dream job: Build your portfolio with Streamlit

Showcase your coding skills to recruiters with a chatbot tailored to your resume

By Vicky Kuo
October 13 2023
In this tutorial, you’ll learn how to build a portfolio with:
Let’s get started
Step 1: Add an introduction file to your app
Step 2: Build your chatbot
Step 3: Import the main CSS file and Lottie animation
Step 4: Design the layout using a sidebar, container, columns, and expander
Step 5: Add a career timeline with vis.js
Step 6: Add HTML and JavaScript with custom CSS
Step 7: Add RSS feeds with components.html
Step 8: Add images and a resume PDF
Step 9: Add a slideshow of colleague endorsements
Step 10: Add a contact form
Step 11: Deploy on Streamlit Community Cloud
Wrapping up
Contents
Share this post
← All posts
💡
TL;DR: Make a portfolio website with a custom AI chatbot, animations, a career snapshot, RSS feeds, images, and a colleague endorsement slideshow! Check out the code and the demo app.

Hey, community! 👋

I'm Vicky, currently working as a data scientist intern at IBM, and I'm thrilled to share my insights with all of you.

In today's competitive job market, having a portfolio is more than just a nice-to-have; it's a must. Your portfolio is the visual companion to your resume, helping you stand out and show what you're capable of. It's the proof behind the promises on your resume, making it an essential tool to make a strong impression on potential employers.

With Streamlit, you can take your resume to the next level, setting you apart from other candidates. Create an interactive app to showcase your own data work, achievements, and personality. Then, pair that with an AI-powered chatbot where recruiters can talk to your resume!? Step aside, “other candidates!”

I hope you find this guide helpful, and it helps you land your dream job!

In this tutorial, you’ll learn how to build a portfolio with:
An AI-powered chatbot that includes custom data, such as a self-introduction
Animations
Career snapshot with a timeline
HTML and JavaScript with custom CSS
RSS feeds (such as Medium posts)
Images and a resume PDF
Colleague endorsement slideshow
Contact form
https://portfolio-template.streamlit.app/
Let’s get started

Clone the GitHub repository for this project’s starter code by entering the following command in your terminal:

git clone <https://github.com/vicky-playground/portfolio-template/>


Before proceeding, it's crucial to personalize both the constant.py and images files with your own information. This step ensures that your portfolio accurately represents your unique profile and content. Please exercise caution and avoid altering the names of keys within the constant.py file and the names of files in the images folder.

Step 1: Add an introduction file to your app

Inside the bio.txt file, I've included a sample self-introduction that will guide the chatbot's responses. Customize it with your own introduction to potential recruiters. The chatbot will base its answers on a combination of the info provided in bio.txt and the large language model of your choice.

Step 2: Build your chatbot

This step will walk you through building an AI chatbot using LlamaIndex and OpenAI. LlamaIndex is your go-to tool for creating applications (like Q&A, chatbots, and agents) powered by large language models (LLMs) and tailored to your specific data. For this app, you’ll use LlamaIndex to enable the chatbot to answer questions about your work history based on the info you provided in the bio.txt file.

Now, let's navigate to the 1_Home.py file to see how this is achieved.

Summary of steps:

OpenAI API Key Input: The user is prompted to input their OpenAI API key, which can be obtained by creating a new API token on OpenAI's platform in the Streamlit sidebar. This key is required to interact with OpenAI's models.
Document Loading: The code loads a document (e.g., a file containing information about the user) using SimpleDirectoryReader.
Query Engine Setup: A query engine is set up to interact with LlamaIndex and OpenAI's GPT-3.5-turbo model.
User Input: The user can enter questions or queries related to the user's information or profile.
Chatbot Interaction: When the user enters a question, the chatbot (named Buddy) uses LlamaIndex and GPT-3.5-turbo to provide responses. The user's input is included in a prompt, and the chatbot generates a response based on the indexed documents and the user's query.
Display Response: The chatbot's response is displayed in the Streamlit app.
API Key Verification: If the user hasn't entered their OpenAI API key or has entered it incorrectly, appropriate warnings or information messages are displayed.

Here's the code that performs these steps, along with the additional information provided above:

from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext
import openai
from langchain.chat_models import ChatOpenAI

openai_api_key = st.sidebar.text_input('Enter your OpenAI API Key and hit Enter', type="password")
openai.api_key = (openai_api_key)

# load the file
documents = SimpleDirectoryReader(input_files=["bio.txt"]).load_data()

# build a query engine
def ask_bot(input_text):
    # define LLM
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0,
        openai_api_key=openai.api_key,
    )
    llm_predictor = LLMPredictor(llm=llm)
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)
    
    # load index
    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)    
    
    # query LlamaIndex and GPT-3.5 for the AI's response
    PROMPT_QUESTION = """You are an AI agent named Buddy helping answer questions about Vicky to recruiters. Introduce yourself when you are introducing who you are.
    If you do not know the answer, politely admit it and let users know how to contact Vicky to get more information. 
    Human: {input}
    """
    output = index.as_query_engine().query(PROMPT_QUESTION.format(input=input_text))
    print(f"output: {output}")
    return output.response

# get the user's input by calling the get_text function
def get_text():
    input_text = st.text_input("You can send your questions and hit Enter to know more about me from my AI agent, Buddy!", key="input")
    return input_text

user_input = get_text()

if user_input:
  if not openai_api_key.startswith('sk-'):
    st.warning('⚠️Please enter your OpenAI API key on the sidebar.', icon='⚠')
  if openai_api_key.startswith('sk-'):
    st.info(ask_bot(user_input))

💡
Pro Tip: If you're searching for a free LLM alternative, I suggest exploring the complimentary guided project I crafted on IBM’s Cognitive Class here. This project will guide you how to integrate an LLM from IBM’s watsonx into your web app, and it comes with special free access.
Step 3: Import the main CSS file and Lottie animation

In this step, we will enhance the appearance of our website by incorporating a CSS file and integrating Lottie animation JSON URLs to Streamlit.

Here's a visual guide for your reference:

https://lottiefiles.com/featured
import requests
from streamlit_lottie import st_lottie

# Load a Lottie animation from a URL
def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

# Apply local CSS styles from a file
def local_css(file_name):
    with open(file_name) as f:
        st.markdown('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)

# Apply local CSS styles from the "style.css" file   
local_css("style/style.css")

# Load Lottie animations from various URLs
lottie_gif = load_lottieurl("<https://assets9.lottiefiles.com/packages/lf20_x17ybolp.json>")
lottie_gif = load_lottieurl("<https://assets9.lottiefiles.com/packages/lf20_x17ybolp.json>")
python_lottie = load_lottieurl("<https://assets6.lottiefiles.com/packages/lf20_2znxgjyt.json>")
java_lottie = load_lottieurl("<https://assets9.lottiefiles.com/packages/lf20_zh6xtlj9.json>")
my_sql_lottie = load_lottieurl("<https://assets4.lottiefiles.com/private_files/lf30_w11f2rwn.json>")
git_lottie = load_lottieurl("<https://assets9.lottiefiles.com/private_files/lf30_03cuemhb.json>")
github_lottie = load_lottieurl("<https://assets8.lottiefiles.com/packages/lf20_6HFXXE.json>")
docker_lottie = load_lottieurl("<https://assets4.lottiefiles.com/private_files/lf30_35uv2spq.json>")
figma_lottie = load_lottieurl("<https://lottie.host/5b6292ef-a82f-4367-a66a-2f130beb5ee8/03Xm3bsVnM.json>")
aws_lottie = load_lottieurl("<https://lottie.host/6eae8bdc-74d1-4b5d-9eb7-37662274cd19/Nduizk8IOf.json>")

Step 4: Design the layout using a sidebar, container, columns, and expander
Add a left panel sidebar using the streamlit.sidebar function.
from constant import *

# Display the photo in the sidebar using HTML content
st.sidebar.markdown(info['Photo'],unsafe_allow_html=True) #info['Photo'] is a reference to the dictionary in constants.py

Design a self-introduction with columns and add it to a container:
# Define a function named "gradient" that generates a colored gradient title with content
def gradient(color1, color2, color3, content1, content2):
    # Create an HTML structure with styling for a gradient header
    st.markdown(f'<h1 style="text-align:center;background-image: linear-gradient(to right,{color1}, {color2});font-size:60px;border-radius:2%;">'
                f'<span style="color:{color3};">{content1}</span><br>'
                f'<span style="color:white;font-size:17px;">{content2}</span></h1>', 
                unsafe_allow_html=True)

# Create a container to organize content using Streamlit's container feature
with st.container():
    # Divide the container into two columns, with widths 8 and 3
    col1, col2 = st.columns([8, 3])

# Inside the first column (col1):
full_name = info['Full_Name']
with col1:
    # Call the "gradient" function to display a gradient title
    gradient('#FFD4DD','#000395','e0fbfc',f"Hi, I'm {full_name}👋", info["Intro"])
    st.write("")  # Add an empty line
    st.write(info['About'])  # Display the 'About' information stored in the 'info' dictionary

# Inside the second column (col2):
with col2:
    # Display a Lottie animation using the st_lottie function
    st_lottie(lottie_gif, height=280, key="data")

Create a layout for displaying the buttons of skills with columns:
# Create a container to organize content using Streamlit's container feature
with st.container():
    st.subheader('⚒️ Skills')
    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
    with col1:
        st_lottie(python_lottie, height=70,width=70, key="python", speed=2.5)
    with col2:
        st_lottie(java_lottie, height=70,width=70, key="java", speed=4)
    with col3:
        st_lottie(my_sql_lottie,height=70,width=70, key="mysql", speed=2.5)
    with col4:
        st_lottie(git_lottie,height=70,width=70, key="git", speed=2.5)
    with col1:
        st_lottie(github_lottie,height=50,width=50, key="github", speed=2.5)
    with col2:
        st_lottie(docker_lottie,height=70,width=70, key="docker", speed=2.5)
    with col3:
        st_lottie(figma_lottie,height=50,width=50, key="figma", speed=2.5)
    with col4:
        st_lottie(js_lottie,height=50,width=50, key="js", speed=1)

Step 5: Add a career timeline with vis.js

To personalize the timeline, you can update the example.json file with your own timeline data. This allows you to showcase your professional journey in a visually engaging manner.

from streamlit_timeline import timeline

with st.container():
    st.markdown("")
    st.subheader('📌 Career Snapshot')
    # Load data
    with open('example.json', "r") as f:
        data = f.read()
    # Render timeline
    timeline(data, height=400)

Step 6: Add HTML and JavaScript with custom CSS

When working with Streamlit, there are two ways to modify HTML and CSS:

st.markdown: Write and alter HTML code directly within your app, typically within the <body> tag.
st.components.v1.html: Embed custom HTML and HTML elements/snippets in your app's UI, with the additional capability to include JavaScript.

To illustrate, here's an example of embedding a Tableau dashboard and using st.expander to show/hide content with an expand/collapse widget:

import streamlit.components.v1 as components
    
with st.container():
		# Display an empty markdown to add some spacing
    st.markdown("""""")

    st.subheader("📊 Tableau")
    col1,col2 = st.columns([0.95, 0.05])
    with col1:
        with st.expander('See the work'):
						# Display Tableau visualization using the components.html method
            components.html(
                """
                <!DOCTYPE html>
                <html>  
                    <title>Basic HTML</title>  
                    <body style="width:130%">  
                        <div class='tableauPlaceholder' id='viz1684205791200' style='position: static'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Su&#47;SunnybrookTeam&#47;Overview&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='SunnybrookTeam&#47;Overview' /><param name='tabs' value='yes' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Su&#47;SunnybrookTeam&#47;Overview&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1684205791200');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='1350px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1550px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='1350px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1550px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.minHeight='5750px';vizElement.style.maxHeight=(divElement.offsetWidth*1.77)+'px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = '<https://public.tableau.com/javascripts/api/viz_v1.js>';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
                    </body>  
                </HTML>
                """
            , height=400, scrolling=True
            )
    st.markdown(""" <a href={}> <em>🔗 access to the link </a>""".format(info['Tableau']), unsafe_allow_html=True)

Step 7: Add RSS feeds with components.html

RSS feeds offer a dynamic way to showcase the latest updates and posts from blogs or other platforms directly in your application.

Streamlit's st.components.v1.html() function simplifies the integration process, allowing for a seamless display of content, such as Medium articles. For the purpose of guiding you through this process, we've provided a sample RSS feed URL from Medium. This sample link can be located in the constant.py file under the variable embed_rss['rss'].

with st.container():
    st.markdown("""""")
    st.subheader('✍️ Medium')
    col1,col2 = st.columns([0.95, 0.05])
    with col1:
        with st.expander('Display my latest posts'):
            components.html(embed_rss['medium'],height=400)
            
        st.markdown(""" <a href={}> <em>🔗 access to the link </a>""".format(info['Medium']), unsafe_allow_html=True)

Step 8: Add images and a resume PDF

In this step, we'll add images and a resume to our website. To display your resume as a PDF on the web, we'll use base64 encoding in the pages/2_Resume.py file.

import base64
    
with open("images/resume.pdf","rb") as f:
      base64_pdf = base64.b64encode(f.read()).decode('utf-8')
      pdf_display = F'<iframe src="data:application/pdf;base64,{base64_pdf}" width="1000mm" height="1000mm" type="application/pdf"></iframe>'
      st.markdown(pdf_display, unsafe_allow_html=True)


Additionally, to showcase your hobbies with images, you can include both JPG and PNG images in the pages/3_Hobbies.py file. To do this, you can leverage the Pillow library for image handling.

from PIL import Image
    
img_1 = Image.open("images/1.jpg")
img_2 = Image.open("images/2.png")
img_3 = Image.open("images/3.png")

Step 9: Add a slideshow of colleague endorsements

Now, let's showcase endorsements from your coworkers in a slideshow. To do this, you'll need to replace the existing image URLs in the constant.py file with your own image URLs.

with st.container():
		# Divide the container into three columns
    col1,col2,col3 = st.columns([0.475, 0.475, 0.05])
    # In the first column (col1)    
		with col1:
		        # Add a subheader to introduce the coworker endorsement slideshow
		        st.subheader("👄 Coworker Endorsements")
		        # Embed an HTML component to display the slideshow
		        components.html(
		        f"""
		        <!DOCTYPE html>
		        <html>
		        <head>
		        <meta name="viewport" content="width=device-width, initial-scale=1">
		        <!-- Styles for the slideshow -->
		        <style>
		            * {{box-sizing: border-box;}}
		            .mySlides {{display: none;}}
		            img {{vertical-align: middle;}}
		
		            /* Slideshow container */
		            .slideshow-container {{
		            position: relative;
		            margin: auto;
		            width: 100%;
		            }}
		
		            /* The dots/bullets/indicators */
		            .dot {{
		            height: 15px;
		            width: 15px;
		            margin: 0 2px;
		            background-color: #eaeaea;
		            border-radius: 50%;
		            display: inline-block;
		            transition: background-color 0.6s ease;
		            }}
		
		            .active {{
		            background-color: #6F6F6F;
		            }}
		
		            /* Fading animation */
		            .fade {{
		            animation-name: fade;
		            animation-duration: 1s;
		            }}
		
		            @keyframes fade {{
		            from {{opacity: .4}} 
		            to {{opacity: 1}}
		            }}
		
		            /* On smaller screens, decrease text size */
		            @media only screen and (max-width: 300px) {{
		            .text {{font-size: 11px}}
		            }}
		            </style>
		        </head>
		        <body>
		            <!-- Slideshow container -->
		            <div class="slideshow-container">
		                <div class="mySlides fade">
		                <img src={endorsements["img1"]} style="width:100%">
		                </div>
		
		                <div class="mySlides fade">
		                <img src={endorsements["img2"]} style="width:100%">
		                </div>
		
		                <div class="mySlides fade">
		                <img src={endorsements["img3"]} style="width:100%">
		                </div>
		
		            </div>
		            <br>
		            <!-- Navigation dots -->
		            <div style="text-align:center">
		                <span class="dot"></span> 
		                <span class="dot"></span> 
		                <span class="dot"></span> 
		            </div>
		
		            <script>
		            let slideIndex = 0;
		            showSlides();
		
		            function showSlides() {{
		            let i;
		            let slides = document.getElementsByClassName("mySlides");
		            let dots = document.getElementsByClassName("dot");
		            for (i = 0; i < slides.length; i++) {{
		                slides[i].style.display = "none";  
		            }}
		            slideIndex++;
		            if (slideIndex > slides.length) {{slideIndex = 1}}    
		            for (i = 0; i < dots.length; i++) {{
		                dots[i].className = dots[i].className.replace("active", "");
		            }}
		            slides[slideIndex-1].style.display = "block";  
		            dots[slideIndex-1].className += " active";
		            }}
		
		            var interval = setInterval(showSlides, 2500); // Change image every 2.5 seconds
		
		            function pauseSlides(event)
		            {{
		                clearInterval(interval); // Clear the interval we set earlier
		            }}
		            function resumeSlides(event)
		            {{
		                interval = setInterval(showSlides, 2500);
		            }}
		            // Set up event listeners for the mySlides
		            var mySlides = document.getElementsByClassName("mySlides");
		            for (i = 0; i < mySlides.length; i++) {{
		            mySlides[i].onmouseover = pauseSlides;
		            mySlides[i].onmouseout = resumeSlides;
		            }}
		            </script>
		
		            </body>
		            </html> 
		
		            """,
		                height=270,
		    )

Step 10: Add a contact form

Let's create a contact form that allows visitors to get in touch with you and connect it with your email using FormSubmit.

Copy and paste the provided code at the bottom of the 1_Home.py file. Ensure that your email address is stored in constant.py under the key Email in the info dictionary. This will ensure that messages submitted through the contact form are sent to your email.

with col2:
        st.subheader("📨 Contact Me")
        email = info["Email"]
        contact_form = f"""
        <form action="<https://formsubmit.co/{email}>" method="POST">
            <input type="hidden" name="_captcha value="false">
            <input type="text" name="name" placeholder="Your name" required>
            <input type="email" name="email" placeholder="Your email" required>
            <textarea name="message" placeholder="Your message here" required></textarea>
            <button type="submit">Send</button>
        </form>
        """
        st.markdown(contact_form, unsafe_allow_html=True)

💡
Note: Submit the contact form once. This first-time-use will trigger an email requesting confirmation.
Step 11: Deploy on Streamlit Community Cloud

After building your app, deploy it on Streamlit Community Cloud:

Create a GitHub repository.
Navigate to Streamlit Community Cloud, click New app, and pick the appropriate repository, branch, and file path.
Hit Deploy
Share with recruiters and hiring managers!
Wrapping up

Thank you for reading my post! I hope you enjoyed this tutorial and found it inspiring.

I can’t wait to see the amazing portfolios you’ll create. Once you have, tag me on LinkedIn. I'd love to celebrate your accomplishments and get inspired by your work! 🎉💡

If you have any feedback or questions, please feel free to post them in the comments below or contact me on LinkedIn.

Let’s continue to learn and grow together! 👩‍💻🌱

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How in-app feedback can increase your chatbot’s performance
https://blog.streamlit.io/how-in-app-feedback-can-increase-your-chatbots-performance/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How in-app feedback can increase your chatbot’s performance

A guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith

By Charly Wargnier
Posted in Tutorials, October 6 2023
What is LangChain?
1. How to build a custom chatbot with LangChain
1.1 Set up pre-requisites
1.2 Create the RAG chain
1.3 Create main.py
1.4 Give your chatbot a memory
1.5 Build the chat interface
1.6 Test the custom chatbot
2. How to implement Trubrics' Feedback
3. How to connect to LangSmith
4. How to turn user feedback into actionable insights
4.1 Spot content inaccuracies
4.2 Highlight slow response times
4.3 Gauge users’ satisfaction
BONUS. Pinpoint library and API errors
Wrapping up
Contents
Share this post
← All posts
👉
TL;DR: Learn how to build a RAG chatbot with LangChain, capture user feedback via Trubrics, and monitor it with LangSmith to gain actionable insights and improve chatbot performance. Check out the app and its code.

Is your chatbot occasionally falling short? Whether it's providing incorrect answers, not meeting users' expectations, or not engaging them the way you want, implementing in-app user feedback can be a game-changer!

Gathering real-time feedback from your users enables you to:

Identify missing or incorrect content used to retrieve answers
Detect technical issues (slow response times, library errors, API hiccups, etc.)
Gauge the overall user satisfaction

How do you start collecting user feedback? Simple. All you need is a Retrieval Augmented Generation (RAG) chatbot, Trubrics' Feedback component, and a connection to LangSmith.

Let's get started!

What is LangChain?

First, let's walk through the basics of LangChain—the foundation for your custom chatbot.

LangChain is a versatile data framework designed for apps that utilize large language models (LLMs). It simplifies the process of creating robust chatbot apps by offering pre-built chains and modular components.

For this example, we'll be using LangChain's Retrieval-Augmented Generation (RAG) capabilities. Simply put, RAG is a technique that enhances the effectiveness of LLMs by connecting them to custom and up-to-date resources. This ensures that your chatbot provides accurate and precise answers to meet your user's requirements.

1. How to build a custom chatbot with LangChain
1.1 Set up pre-requisites

To get started:

Follow the instructions here to obtain an OpenAI API key.
Sign up for LangSmith at https://smith.langchain.com/ and generate an API Key. To do this, click on the API Key icon and then click on Create API Key (make sure to copy it).

Install Streamlit, LangChain, LangSmith, the streamlit-feedback component, and the dependencies from the requirements.txt file—openai, tiktoken, and chromadb. Make sure to run this command in the virtual environment where you want to run your app to avoid any package conflicts:
pip install -r requirements.txt

1.2 Create the RAG chain

Now, let's move to the core of your chatbot—the essential_chain.py file. This file enables the vectorization of any documents (we'll use Streamlit's documentation as an example) to obtain up-to-date and context-aware responses.

To begin, import the required libraries and LangChain functions:

import os
import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.docstore.document import Document
from langchain.memory import ConversationBufferMemory


The main function in this file is initialize_chain, which sets up the chat model and loads the required documents.

Create a ChatOpenAI instance with the gpt-3.5-turbo model:
@st.cache_resource(show_spinner=False)
def initialize_chain(system_prompt, _memory):
    llm = ChatOpenAI(temperature=0, max_tokens=1000, model_name="gpt-3.5-turbo", streaming=True)

Iterate over all files in the specified directory (./markdown_files), read the content of each markdown file, and store it as a Document object. This allows you to integrate Streamlit's up-to-date markdown documentation into the app:
documents = []
folder_path = "./markdown_files"
# ... (loop through and read each .md file, storing them as Document objects)

💡
TIP: While this demo app queries our Streamlit docs, you can test it with your own docs or blog content in just a few clicks. Simply replace the Streamlit markdown files in the repository with your own markdown files, and you're done! 🙌

Once loaded, the documents undergo a two-step process of splitting and embedding:

Splitting: The content is divided into smaller chunks using RecursiveCharacterTextSplitter to facilitate efficient vectorization.
Embedding: These chunks are then vectorized using OpenAI embeddings and stored in a Chroma DB database.
if documents:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    document_chunks = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(document_chunks, embeddings)


Finally, to make use of the vectorized data, initialize the ConversationalRetrievalChain:

qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=_memory)


That's all for essential_chain.py!

Now, let's move on to the main.py file.

1.3 Create main.py

main.py is the nucleus of your app, where you call essential_chain.py and displays the front-end UI, among other things.

First, import the required modules into the file:

import streamlit as st
from streamlit_feedback import streamlit_feedback
from langchain.callbacks.tracers.run_collector import RunCollectorCallbackHandler
from langchain.memory import StreamlitChatMessageHistory, ConversationBufferMemory
from langchain.schema.runnable import RunnableConfig
from langsmith import Client
from langchain.callbacks.tracers.langchain import wait_for_all_tracers
from essential_chain import initialize_chain
import os


Next, configure the environment variables to establish connections with OpenAI and LangSmith:

# Set LangSmith environment variables
os.environ["OPENAI_API_KEY"] = st.secrets["api_keys"]["OPENAI_API_KEY"]
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "<https://api.smith.langchain.com>"


In the app's sidebar, you will collect additional inputs from users to configure LangSmith:

# Add the toggle for LangSmith API key source
use_secret_key = st.sidebar.toggle(label="Demo LangSmith API key", value=True)

# Conditionally set the project name based on the toggle
if use_secret_key:
    os.environ["LANGCHAIN_PROJECT"] = "Streamlit Demo"
else:
    project_name = st.sidebar.text_input(
        "Name your LangSmith Project:", value="Streamlit Demo"
    )
    os.environ["LANGCHAIN_PROJECT"] = project_name

# Conditionally get the API key based on the toggle
if use_secret_key:
    langchain_api_key = st.secrets["api_keys"][
        "LANGSMITH_API_KEY"
    ]  # assuming it's stored under this key in secrets
else:
    langchain_api_key = st.sidebar.text_input(
        "👇 Add your LangSmith Key",
        value="",
        placeholder="Your_LangSmith_Key_Here",
        label_visibility="collapsed",
    )
if langchain_api_key is not None:
    os.environ["LANGCHAIN_API_KEY"] = langchain_api_key

You can use the provided LangSmith API key, which is for demo purposes. By using it, you'll only have access to each run, and runs won't be stored in your account.
If you choose to add your own LangSmith API key (toggle OFF), you'll gain full access to your LangSmith account and have the ability to specify a name for each of your projects.
1.4 Give your chatbot a memory

The chat memory is an essential component for maintaining a contextual conversation. Use LangChain's ConversationBufferMemory to store the chat history, simplifying access to previous interactions.

memory = ConversationBufferMemory(
    chat_memory=StreamlitChatMessageHistory(key="langchain_messages"),
    return_messages=True,
    memory_key="chat_history",
)


Next, add a functionality to clear the message history:

if st.sidebar.button("Clear message history"):
    print("Clearing message history")
    memory.clear()
    st.session_state.trace_link = None
    st.session_state.run_id = None

1.5 Build the chat interface

Use Streamlit's new st.chat_message command to create a chat user interface that displays messages from both the user and the assistant in separate containers.

for msg in st.session_state.langchain_messages:
    streamlit_type = _get_openai_type(msg)
    avatar = "🦜" if streamlit_type == "assistant" else None
    with st.chat_message(streamlit_type, avatar=avatar):
        st.markdown(msg.content)


💡
TIP: You can modify the avatar parameter to give the assistant's messages a personal touch. In this example, I added a parrot emoji as a reference to LangChain's logo.

Then, the st.chat_input command provides a chat input widget for receiving user messages:

if prompt := st.chat_input(placeholder="Ask a question about the Streamlit Docs!"):
    # ... (Handling chat input and generating responses)
    # ... (Displaying thinking animation and updating session state)


1.6 Test the custom chatbot

Now that you have built a Streamlit docs chatbot using up-to-date markdown files, let's compare the results to ChatGPT. GPT-3.5 and 4 have only been trained on data up to September 2021, missing almost three years of new releases. So augmenting your LLM with Langchain ensures higher accuracy of the response.

Let's ask a question about Streamlit's Chat elements, released in July 2023: "Tell me about Streamlit's Chat elements."

ChatGPT can’t come up with the right answer, but your custom chatbot can!

You can compare the classic GPT 3.5 LLM with our custom LLM directly in the demo app by switching between these two options:

2. How to implement Trubrics' Feedback

Trubrics' Feedback is a Streamlit component that enables you to collect user feedback directly in your app. With this component, users can rate the responses of the chatbot, and each feedback will be sent and stored in LangSmith.

Trubrics' Feedback offers two feedback mechanisms:

thumbs style:

faces style:

The default feedback style is set to thumbs. To allow users to easily switch between the two options, let's use Streamlit's new st.toggle widget:

feedback_option = "faces" if st.toggle(label="`Thumbs` ⇄ `Faces`", value=False) else "thumbs"


The selected feedback style is activated when a valid run_id is present:

if st.session_state.get("run_id"):
    feedback = streamlit_feedback(
        feedback_type=feedback_option,  # Apply the selected feedback style
        optional_text_label="[Optional] Please provide an explanation",  # Allow for additional comments
        key=f"feedback_{st.session_state.run_id}",
    )

3. How to connect to LangSmith

LangSmith seamlessly integrates with the LangChain library, which you used to build your custom chatbot. In this case, you’ll primarily use it to store user feedback and gain insights from it.

To establish a connection to LangSmith and send both the chatbot outputs and user feedback, follow these steps:

client = Client(api_url=langchain_endpoint, api_key=langchain_api_key)



💡
TIP: Remember to add the LangSmith API key you obtained in section 1.1 to the LangChain API Key field of the app.

To check if the connection is working, follow these steps:

Type a question in the chat. For example, ask, "Tell me more about Streamlit Chat elements."
Provide feedback by giving a score and comment. For example, use Thumbs up and Great answer!

Click on the Latest Trace button in the chatbot app:

You’ll be redirected to LangSmith, where you can view the details of your run.
Click on the Feedback tab in LangSmith to access the user feedback for that run.

4. How to turn user feedback into actionable insights

Let me show you some examples of how user feedback can help improve your chatbot’s answers, spot errors, and enhance your app’s workflow.

💡
NOTE: The examples below are taken from the chatbot we use at Streamlit, which is the one you just built in this tutorial. Remember, you can get similar insights with your own docs by swapping the Streamlit markdown files in the repository with your own!
4.1 Spot content inaccuracies

Here is how we spotted content inaccuracies in our internal user feedback for the Steamlit app. LangSmith enabled us to filter chatbot runs by feedback score using the filter panel:

We reviewed the comments related to negative feedback scores and found that some users expressed dissatisfaction with the chatbot's responses to the question, "How can I connect DuckDB to Streamlit using st.experimental_connection()?"

DuckDB is an SQL database system. While it’s possible to connect it to Streamlit using st.experimental_connection(), users have reported that the returned answers are incorrect. We tested this and found that our chatbot suggested non-existent Streamlit functions:

Then we reviewed our docs and found that there is no information on how to use st.experimental_connection() with DuckDB:

Our chatbot currently lacks the ability to retrieve the relevant content that users are requesting from our docs. So we requested the content team to prioritize creating content specifically related to "DuckDB + st.experimental_connection()". Once it’s created and added to the docs, we’ll integrate it into our custom chatbot to enhance its responses.

There are several benefits to this approach:

✅ It will provide improved and more accurate responses from the chatbot
✅ It will ensure that our docs remains current and relevant
✅ It will help us prioritize the most important content for our docs
4.2 Highlight slow response times

Slow chatbot response times can frustrate users and decrease user retention. Additionally, users frequently provide feedback about noticeable lags or delays during interactions.

Let's review some of the user feedback that we collected in our internal app:

It’s clear that the chatbot’s responses could be sped up!

In LangSmith, we can identify chatbot runs with slow response times by using the Latency >= 10s tag. You can locate this tag in the filter panel of any project.

Any run with high latency will be displayed:

Here is how you can fix these latency issues:

LangSmith can show you the latency associated with each chain of your chatbot. You can optimize response times by experimenting with different chains. For example, in some of our tests, we found that the ConversationSummaryBufferMemory chain had a significant impact on latency.
You can also try Zep, an open-source long-term memory store designed to improve the performance of chatbots, especially those built with LangChain.
We also recommend that you use [st.cache_resource](<https://docs.streamlit.io/library/api-reference/performance/st.cache_resource>) to cache the relevant functions of your app (if you haven't done so already).
4.3 Gauge users’ satisfaction

In addition to analyzing individual user feedback as demonstrated in sections 4.1 and 4.2, it’s important to consider the value of aggregated feedback.

Aggregated feedback can help broaden your approach and provide a measurable indicator of user satisfaction towards a model, content, or chatbot application. LangSmith provides data visualizations for all recorded metrics, including metrics derived from user feedback. Each metric is presented on a timeline chart, allowing you to track its progress over time.

Let's explore!

After you build the app by following the steps below, head to your LangSmith project and click on the Monitor tab:

One way to track user satisfaction is by checking the Avg. Score - faces chart. From this chart, we can observe that user satisfaction with our internal chatbot is improving. So we’ll continue testing and refining it!

BONUS. Pinpoint library and API errors

While users can sometimes provide feedback on errors, there are instances where they can’t do so because these errors hinder the chatbot's proper functioning. The good news is that LangSmith logs all of these errors for you.

To view chatbot runs that contain errors, simply select the error tag in the filter panel:

During our internal testing, we found the following two issues:

Library installations issues

✅ To fix it, we went back to the app and installed the h2 package.

Incorrect API keys

We spotted several types of API errors and warnings (you also get the timing of their occurrences in LangSmith). For example, we observed an error when our OpenAI key became invalid.

✅ To fix it, we updated the OpenAI API key in the app.

And now, let's get to building!

Wrapping up

We’ve embarked on a journey together, delving into the code to build a reliable and powerful RAG chatbot in Streamlit. Throughout this journey, I introduced you to the LangSmith platform and the Trubrics Feedback component. You’ve witnessed the value of user feedback, whether it’s for refining documentation, enhancing app workflows, identifying errors, or understanding user sentiments.

Feel free to explore our LLM gallery, learn, and draw inspiration from the amazing creations of the community. Soon enough, you'll be eager to create your own chatbot masterpiece! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Amjad Raza - Streamlit
https://blog.streamlit.io/author/amjad/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Amjad Raza
1 post
Chat with pandas DataFrames using LLMs

A step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI

LLMs
by
Amjad Raza
,
August 31 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Chat with pandas DataFrames using LLMs
https://blog.streamlit.io/chat-with-pandas-dataframes-using-llms/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Chat with pandas DataFrames using LLMs

A step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI

By Amjad Raza
Posted in LLMs, August 31 2023
App overview
Tutorial
1. Load data into pandas DataFrame
2. LangChain and OpenAI as an LLM engine
3. Use Streamlit for UI
4. Use Docker for deployment
Potential errors
Wrapping up
Contents
Share this post
← All posts
TL;DR: In this post, I’ll show you how to interact with pandas DataFrames, build an app powered by LangChain and OpenAI API, and set up the docker deployment for local or cloud deployments (grab the code here).




⚠️
WARNING: This app uses LangChain's PythonAstREPLTool which is vulnerable to arbitrary code execution. Use caution in deploying and sharing this app.

Communicating with pandas DataFrames makes data analysis accessible to non-technical users. Using a chat-like interface, users can ask data-related questions, request insights, and navigate through data as if they were chatting with a friend.

Since the preferred method for engaging with chatbots is through a question-and-answer format, let’s make an app that seamlessly integrates with user queries.

But first, we’ll take a look at how it works.

App overview

Users can interact with pandas DataFrames by uploading CSV, Excel, or any other supported structured data files:

Tutorial

To start, make sure that you have the necessary key technologies installed, as well as a basic understanding of the application framework shown in the diagram below:

Python, Streamlit, and Docker: Python and Docker are essential components for building and deploying the app. Having knowledge of building UI with Streamlit is a plus.
Virtual Environment using Poetry: To create a virtual environment, install [Poetry](<https://python-poetry.org/>). It simplifies dependency management and environment setup.
Project Dependencies: All the required dependencies for the project are specified in the pyproject.toml file. They ensure that your app has access to the necessary libraries and tools to function properly.
OpenAI API Token: Get an OpenAI API token here.

1. Load data into pandas DataFrame

The first step is to load and persist user data into a pandas DataFrame. For smaller datasets, it is good practice to persist the data. Users can upload files with various extensions from the list above. The data is cached for 2 hours using @st.cache_data(ttl="2h") and destroyed after that time has elapsed to release resources.

file_formats = {
    "csv": pd.read_csv,
    "xls": pd.read_excel,
    "xlsx": pd.read_excel,
    "xlsm": pd.read_excel,
    "xlsb": pd.read_excel,
}
@st.cache_data(ttl="2h")
def load_data(uploaded_file):
    try:
        ext = os.path.splitext(uploaded_file.name)[1][1:].lower()
    except:
        ext = uploaded_file.split(".")[-1]
    if ext in file_formats:
        return file_formats[ext](uploaded_file)
    else:
        st.error(f"Unsupported file format: {ext}")
        return None

# Read the Pandas DataFrame
df = load_data(uploaded_file)

2. LangChain and OpenAI as an LLM engine

I have integrated LangChain's create_pandas_dataframe_agent to set up a pandas agent that interacts with df and the OpenAI API through the LLM model. This agent takes df, the ChatOpenAI model, and the user's question as arguments to generate a response. Under the hood, a Python code is generated based on the prompt and executed to summarize the data. The LLM model then converts the data into a conversational format for the final response.

For this example, I used the "gpt-3.5-turbo-0613" model, but users can choose GPT4 or any other model. Performance may vary depending on the model and dataset used.

In this code, the input questions are captured using the st.session_state.messages object from the Streamlit UI, and the response is passed back to the UI for display:

from langchain.agents import AgentType
from langchain.agents import create_pandas_dataframe_agent
from langchain.callbacks import StreamlitCallbackHandler
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
        temperature=0, model="gpt-3.5-turbo-0613", openai_api_key=openai_api_key, streaming=True
    )

    pandas_df_agent = create_pandas_dataframe_agent(
        llm,
        df,
        verbose=True,
        agent_type=AgentType.OPENAI_FUNCTIONS,
        handle_parsing_errors=True,
    )

response = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])



NOTE: Make sure you have set up the OpenAI API Key in the sidebar.
3. Use Streamlit for UI

I chose Streamlit UI for its simplicity and recently released chat features, such as st.chat_message("assistant"). It’s a lightweight and efficient method for building and sharing data apps.

import streamlit as st
import pandas as pd
import os

st.set_page_config(page_title="LangChain: Chat with Pandas DataFrame", page_icon="🦜")
st.title("🦜 LangChain: Chat with Pandas DataFrame")

uploaded_file = st.file_uploader(
    "Upload a Data file",
    type=list(file_formats.keys()),
    help="Various File formats are Support",
    on_change=clear_submit,
)

if uploaded_file:
    df = load_data(uploaded_file)



The above code initializes the app and adds the Upload File Widget to the UI. You can upload data files using Streamlit's st.file_uploader component.

openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
if "messages" not in st.session_state or st.sidebar.button("Clear conversation history"):
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input(placeholder="What is this data about?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.")
        st.stop()

    llm = ChatOpenAI(
        temperature=0, model="gpt-3.5-turbo-0613", openai_api_key=openai_api_key, streaming=True
    )

    pandas_df_agent = create_pandas_dataframe_agent(
        llm,
        df,
        verbose=True,
        agent_type=AgentType.OPENAI_FUNCTIONS,
        handle_parsing_errors=True,
    )


The user is prompted to provide their OpenAI API keys through a sidebar text widget. Use st.session_state to keep track of variables and chat history. The user's input query is obtained using st.chat_input(), which is then passed to pandas_df_agent as discussed in the previous section.

with st.chat_message("assistant"):
        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
        response = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.write(response)


The code above interacts with the pandas_agent and captures its response—displayed and appended to the chat history.

4. Use Docker for deployment

After preparing and testing the app, deploy it on the Streamlit Community Cloud using the GitHub repository or on Google Cloud, Heroku, AWS, or Azure using a Docker configuration.

4.1. Clone the GitHub repository

git clone https://github.com/langchain-ai/streamlit-agent.git

4.2. Install requirements by creating a venv

>> poetry install
>> poetry shell


4.3. Run the app locally

$ streamlit run streamlit_agent/chat_pandas_df.py

4.4. Run the app using Docker

The project includes the Dockerfile and docker-compose.yml. To build and run a Docker image:

Generate the image with DOCKER_BUILDKIT

DOCKER_BUILDKIT=1 docker build --target=runtime . -t langchain-streamlit-agent:latest

Run the Docker container directly

docker run -d --name langchain-streamlit-agent -p 8051:8051 langchain-streamlit-agent:latest

Run the Docker container using docker-compose

Edit the Command in docker-compose with the target Streamlit app docker-compose up. To deploy Streamlit apps using Google Cloud, follow this guide.

Potential errors

If you choose to make a copy of and operate the application on your own computer or any online cloud system, you might come across the following problems:

Preparing your personal environment and necessary components, since they are currently configured to utilize the most recent editions of LangChain & Streamlit.
Due to our utilization of a mix of OpenAI & Langchain tools, there are instances where the model produces outcomes that aren't what we intended. In some cases, rephrasing your questions can help resolve this problem.
When using Docker, it's important to have the latest version of Docker Desktop installed and sufficient storage space available for creating and running the image.
Wrapping up

You learned how to construct a generative AI application to talk with pandas DataFrames or CSV files by using LangChain's tools, and how to deploy and run your app locally or with Docker support.

Here are the key takeaways:

You can seamlessly interact with business-specific data stored in Excel or CSV files, eliminating the need for complex setups or configurations.
You can transform DataFrames into conversational entities, similar to human conversations.
You can empower business users to pose relevant questions and engage with data, without requiring any prior knowledge of data processing or analysis.

Now you can bridge the gap between data-driven insights and effortless interaction, enhancing the accessibility and usability of your data for a wider range of users. Let me know if you have any questions in the comments below or contact me on GitHub, LinkedIn, Twitter, or email.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Logan Vendrix - Streamlit
https://blog.streamlit.io/author/logan/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Logan Vendrix
1 post
Build your own Notion chatbot

A step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit

LLMs
by
Logan Vendrix
,
September 14 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit (Page 2)
https://blog.streamlit.io/page/2/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
Develop Streamlit apps in-browser with GitHub Codespaces

Build anywhere without the hassle of a local Python environment

GitHub Codespaces
by
Brian Holt
,
September 14 2023
Build your own Notion chatbot

A step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit

LLMs
by
Logan Vendrix
,
September 14 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Chat with pandas DataFrames using LLMs

A step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI

LLMs
by
Amjad Raza
,
August 31 2023
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

LLMs
by
Caroline Frasca and 
2
 more,
August 23 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
Exploring LLMs and prompts: A guide to the PromptTools Playground

Learn how to build dynamic, stateful applications that harness multiple LLMs at once

LLMs
by
Steve Krawczyk and 
1
 more,
August 18 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
← Previous page
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Simplifying generative AI workflows
https://blog.streamlit.io/simplifying-generative-ai-workflows/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

By Filip Boltuzic, Ara Ghukasyan and Santosh Kumar Radha
Posted in Advocate Posts, October 6 2023
App overview
1. How to build and execute a Covalent workflow
1.1 The electron decorator
1.2 The executor objects
1.3 The lattice decorator
2. How to build a Streamlit UI to generate requests for news article summarization
3. How to summarize news articles from Quanta
Wrapping up
Contents
Share this post
← All posts
🤖
TL;DR: Learn how to build complex generative AI apps using Covalent and Streamlit. Covalent simplifies resource management in the Python environment, while Streamlit enhances data visualization and user interaction. Together, they provide a transformative solution for efficient ML workflow management. Read more in our docs, and access the complete code here and the demo app here.

Want to create a generative AI solution that combines LLMs, stable diffusion generation, and cloud-based computing for resource-heavy tasks? You’ll need more than code. You’ll need to act as a cloud architect, DevOps engineer, and financial analyst all in one, trapped in a cycle of provisioning resources, monitoring them, and scaling them up or down.

That’s where Covalent comes in.

Covalent is a Pythonic workflow orchestration platform that scales workloads from your laptop to any compute backend. It lets you focus on what you do best—coding, experimenting, and innovating—without the burden of operational complexity.

In this post, we’ll show you how to build an interactive, generative AI app for summarizing news articles. You’ll learn:

How to build and execute a Covalent workflow
How to build a Streamlit UI to generate requests for news article summarization
How to summarize news articles from Quanta
App overview

Let's take a high-level look at the app:

Users create a Covalent generative AI workflow
They execute these workflows on the Covalent Server
Users trigger reruns and refinements of their Covalent workflows via a Streamlit app

1. How to build and execute a Covalent workflow

Covalent SDK is a Python framework that consists of three key components:

The electron decorator
The executor objects
The lattice decorator

These components help researchers define complex workflows in a lightweight and non-destructive manner, with minimal and non-intrusive code changes.

In the following sections, we’ll explain how to use them.

1.1 The electron decorator

The @ct.electron decorator converts a Python function into a remotely executable task that Covalent can use to deploy arbitrary resources. Users can specify resources and constraints for each task by passing different executor objects to electron decorators.

Here is a simple Python function that generates an image from a text prompt using a stable diffusion model:

@ct.electron
def generate_image_from_text(
    summary, model_name="OFA-Sys/small-stable-diffusion-v0", prompt="Impressionist image - "
):
    model = DiffusionPipeline.from_pretrained(model_name)
    model.enable_attention_slicing()

    # Generate image using DiffusionPipeline
    reduced_summary = prompt + summary
    return model(reduced_summary).images[0]

1.2 The executor objects

Covalent executors define the environment in which the workflow will be executed. Each electron (task) can be assigned a different executor, allowing the workflow to be executed across multiple machines. Some tasks may require intensive and parallelizable CPU computation, while others can be completed using serverless, low-intensity CPU processing. This flexibility is crucial for optimizing task execution throughout the workflow, ensuring efficient use of time and resources.

import covalent as ct

# requires installing library covalent-gcpbatch-plugin
gcp_low_cpu_executor = ct.executor.GCPBatchExecutor(
   vcpus = 2,  # Number of vCPUs to allocate
   memory = 512,  # Memory in MB to allocate
   time_limit = 60,  # Time limit of job in seconds
   poll_freq = 1,  # Number of seconds to pause before polling for the job's status
)
gcp_high_cpu_executor = ct.executor.GCPBatchExecutor(
   vcpus = 32,  # Number of vCPUs to allocate
   memory = 32768,  # Memory in MB to allocate
   time_limit = 60,  # Time limit of job in seconds
   poll_freq = 1,  # Number of seconds to pause before polling for the job's status
)

@ct.electron(executor=gcp_low_cpu_executor)
def generate_title(
    article, model_name="JulesBelveze/t5-small-headline-generator",
    max_tokens=84, temperature=1, no_repeat_ngram_size=2
):
    ...

@ct.electron(executor=gcp_high_cpu_executor)
def generate_image_from_text(
		reduced_summary, model_name="OFA-Sys/small-stable-diffusion-v0", prompt="Impressionist image"
):
    ...

1.3 The lattice decorator

The @ct.lattice converts a function composed of electrons into a manageable workflow. You can transform a workflow into a lattice simply by adding this decorator to a function:

@ct.lattice
def news_content_curator(
news_url, image_generation_prompt="Impressionist image",
    summarizer_model="t5-small", summarizer_max_length=40,
    title_generating_model="JulesBelveze/t5-small-headline-generator",
    image_generation_model="OFA-Sys/small-stable-diffusion-v0",
    temperature=1, max_tokens=64, no_repeat_ngram_size=2,
    content_analysis_model="finiteautomata/bertweet-base-sentiment-analysis"
):
		...


Once a lattice (workflow) is defined, you need to dispatch it to run. You can do this using Covalent by calling ct.dispatch and providing the workflow name and parameters:

news_url = '<https://www.quantamagazine.org/math-proof-draws-new-boundaries-around-black-hole-formation-20230816/>'
dispatch_id = ct.dispatch(news_content_curator)(news_url)


Below are the complete workflow steps (find the code here):

@ct.lattice
def news_content_curator(
    news_url, image_generation_prefix,
    summarizer_model="t5-small",
    summarizer_max_length=40,
    title_generating_model="JulesBelveze/t5-small-headline-generator",
    image_generation_model="OFA-Sys/small-stable-diffusion-v0",
    temperature=1, max_tokens=64, no_repeat_ngram_size=2,
    content_analysis_model="finiteautomata/bertweet-base-sentiment-analysis"
):
    article = extract_news_content(news_url)
    content_property = sentiment_analysis(
        article, model_name=content_analysis_model
    )
    reduced_summary = generate_reduced_summary(
        article, model_name=summarizer_model, max_length=summarizer_max_length
    )
    title = generate_title(
        article, model_name=title_generating_model,
        temperature=temperature, max_tokens=max_tokens,
        no_repeat_ngram_size=no_repeat_ngram_size
    )
    generated_image = generate_image_from_text(
        reduced_summary, prompt=image_generation_prefix,
        model_name=image_generation_model
    )
    image_with_title = add_title_to_image(generated_image, title)
    url = save_image(image_with_title)
    return {
        "content_property": content_property, "summary": reduced_summary,
        "title": title, "image": url,
    }


Here is the Covalent workflow for the News article AI summarization app as viewed through the Covalent UI:

When a Covalent workflow is executed, a unique identifier called dispatch_id is generated. This ID serves two purposes:

It acts as a reference for the specific workflow.
It allows for the rerun of the entire workflow.

Covalent keeps a record of all previously executed workflows in a scalable database, creating a comprehensive history that you can use to rerun workflows using their respective dispatch IDs.

🤖
NOTE: It’s important to distinguish between the dispatch (ct.dispatch) and redispatch (ct.redispatch) features. Dispatch is for creating brand-new workflows, whereas redispatch is for refining or duplicating existing workflows.

You can redispatch a workflow in three ways:

Provide the dispatch_id to the redispatch method to summarize a different news article.
Rerun a workflow while reusing previously computed results. For example, if you want to experiment with a different prompt for generating images from the same news article, while keeping the summarization and headline generation unchanged, you can initiate the workflow again and preserve the use of previous results.


redispatch_id = ct.redispatch(
        dispatch_id, reuse_previous_results=True
)(new_url, "Cubistic image")



Customize a previously executed workflow by replacing tasks and using the replace_electrons feature (learn more here):


def classify_news_genre(
    article, model_name="abhishek/autonlp-bbc-news-classification-37229289"
):
    ...

replace_electrons = {
    "sentiment_analysis": ct.electron(classify_news_genre)
}
redispatch_id = ct.redispatch(
    dispatch_id, replace_electrons=replace_electrons
)(new_url, "Cubistic image", content_analysis_model="abhishek/autonlp-bbc-news-classification-37229289")

2. How to build a Streamlit UI to generate requests for news article summarization

Streamlit lets users adjust parameters for the AI news summarization workflow and trigger previously executed workflows using their dispatch IDs.

The app sidebar contains the parameters with some proposed default values:

import streamlit as st

with st.sidebar:
    server_location = st.text_input(
        "Remote server URL", value="<http://localhost:8085>"
    )
    news_article_url = st.text_input(
        "News article URL",
        value="<https://www.quantamagazine.org/math-proof-draws-new-boundaries-around-black-hole-formation-20230816/>"
    )
    st.header("Parameters")

    # Title generation section
    st.subheader("Title generation parameters")
    title_generating_model = headline_generation_models[0]
    temperature = st.slider(
        "Temperature", min_value=0.0, max_value=100.0, value=1.0,
        step=0.1
    )
    max_tokens = st.slider(
        "Max tokens", min_value=2, max_value=50, value=32,
    )

    # Image generation section
    st.subheader("Image generation")
    image_generation_prefix = st.text_input(
        "Image generation prompt",
        value="industrial style"
    )
    image_generation_model = stable_diffusion_models[0]

    # Text summarization section
    st.subheader("Text summarization")
    summarizer_model = news_summary_generation[0]
    summarizer_max_length = st.slider(
        "Summarization text length", min_value=2, max_value=50, value=20,
    )

    # Content analysis section
    st.subheader("Content analysis")
    selected_content_analysis = st.selectbox(
        "Content analysis option", options=[
            "sentiment analysis",
            "genre classification"
        ]
    )
    if selected_content_analysis == "sentiment analysis":
        content_analysis_model = sentiment_analysis_models[0]
    else:
        content_analysis_model = genre_analysis_models[0]


The main part of the app displays the results from the Covalent server (based on the parameters configured in the sidebar). This process generates an AI-generated summary of the news article, a proposed title, and an AI-generated image that represents the content of the news article.

import requests

st.title("News article AI summarization")
dispatch_id = st.text_input("Dispatch ID")

if st.button("Generate image and text summary"):
    st.write("Generating...")

    container = st.container()
		
		# select either genre analysis or sentiment analysis
    selected_content_analysis = parameters.pop('selected_content_analysis')
    if selected_content_analysis != 'sentiment analysis':
        replace_electrons = {
            "sentiment_analysis": ct.electron(classify_news_genre)
        }
        parameters[
            "content_analysis_model"
        ] = "abhishek/autonlp-bbc-news-classification-37229289"
    else:
        replace_electrons = {}

    redispatch_id = ct.redispatch(
        dispatch_id, reuse_previous_results=True,
        replace_electrons=replace_electrons
    )(**parameters)

    covalent_info = ct.get_config()['dispatcher']
    address = covalent_info['address']
    port = covalent_info['port']
    covalent_url = f"{address}:{port}/{redispatch_id}"

    st.write(f"Covalent URL on remote server: http://{covalent_url}")

    with container:
        result = ct.get_result(redispatch_id, wait=True).result
        st.subheader(f"Article generated title: {result['title']}")
        st.write(
            "In terms of " +
            selected_content_analysis +
            " content is: " + str(result['content_property'])
        )
        st.image(result['image'])
        st.text_area(
            label="AI generated summary",
            key="summary",
            value=result['summary'], disabled=True
        )


Lastly, within the Streamlit app, you have the option to start the Covalent server for complete automation. This only needs to be done once.

If you want to start the Covalent server and execute a predefined workflow (in a file named workflow_remote.py), just include this code:

def is_covalent_down():
    out = check_output(["covalent", "status"])
    if "Covalent server is stopped" in out.decode('utf-8'):
        return True
    return False

def start_covalent():
    subprocess.run("covalent start --no-cluster", shell=True)

def run_covalent_workflow(workflow_filename):
    dispatch_id = check_output(["python", workflow_filename]).decode("utf-8")
    return dispatch_id.strip()

if is_covalent_down():
    st.write("Covalent is not up. Starting Covalent...")
    start_covalent()
    
		dispatch_id = run_covalent_workflow("workflow_remote.py")

    # wait for result
    ct.get_result(dispatch_id, wait=True)
    st.session_state['dispatch_id'] = dispatch_id

3. How to summarize news articles from Quanta

Once you construct the Covalent workflows, you can repeatedly execute them through Streamlit. This provides an interactive environment for easily running complex ML workflows and fine-tuning their parameters.

To get started, launch the app and copy the Covalent workflow dispatch IDs.

🤖
NOTE: To run everything, make sure you have both services running: the Covalent server (covalent start) and the Streamlit app (streamlit run streamlit_app.py).
Wrapping up

You’ve learned how to build complex ML workflows using an example of a news summarization app. In this setup, a Covalent server handles the ML workflows, while a Streamlit interface manages user interactions. The communication between the two is facilitated through a single (dispatch) ID, which simplifies resource management, improves efficiency, and allows you to focus on the ML aspects.

If you found this interesting, please note that Covalent is a free and open-source tool. You can find more information and additional tutorials in our docs.

Happy workflow building! ⚙️

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Advocate Posts...

View even more →

Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts from Streamlit Advocates
https://blog.streamlit.io/tag/advocates/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Advocate Posts
67 posts
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
Deep-learning apps for image processing made easy: A step-by-step guide

Learn how to develop custom deep-learning apps using image processing models with Streamlit

Advocate Posts
by
Mainak Chaudhuri
,
August 22 2023
PureHuB: A search engine for your university

A step-by-step guide to creating an inverted index search app using Python and Streamlit

Advocate Posts
by
Mala Deep Upadhaya
,
August 10 2023
Data analysis with Mito: A powerful spreadsheet in Streamlit

Replace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app

Advocate Posts
by
Nate Rush
,
August 8 2023
SimiLo: Find your best place to live

A 5-step guide on how I built an app to relocate within the U.S.

Advocate Posts
by
Kevin Soderholm
,
August 4 2023
Instant Insight: Generate data-driven presentations in a snap!

Create presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery

LLMs
by
Oleksandr Arsentiev
,
August 2 2023
Trubrics: A user feedback tool for your AI Streamlit apps

A 3-step guide on collecting, analyzing, and managing AI model feedback

Advocate Posts
by
Jeff Kayne
,
July 28 2023
Chat2VIS: AI-driven visualisations with Streamlit and natural language

Leverage ChatGPT for Python code generation using prompt engineering

LLMs
by
Paula Maddigan
,
July 27 2023
Improving healthcare management with Streamlit

How to build an all-in-one analytics platform for small clinics

Advocate Posts
by
Matteo Ballabio and 
1
 more,
July 17 2023
Streamlit and iFood: Empowering the Monitor Rosa project

Harnessing technology and corporate support for social impact

Advocate Posts
by
Heber Augusto Scachetti
,
July 14 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4
https://blog.streamlit.io/comparing-code-llama-vs-gpt-3-5-and-gpt-4-to-generate-visualisations/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

By Paula Maddigan
Posted in Tutorials, October 16 2023
Why I chose to compare Code Llama and GPT models
Quick overview of Chat2VIS
6 case studies using Chat2VIS to compare Code Llama vs. GPT-3.5 Instruct and GPT-4
Case study 1: Generate code for bar chart
Case study 2: Generate code for time series
Case study 3: Plotting request with an unspecified chart type
Case study 4: Parsing complex requests
Case study 5: Misspelled prompts
Case study 6: Ambiguous prompts
How to improve success with open-source models
Wrapping up
Contents
Share this post
← All posts

Generative AI is moving at lightning speed ⚡️, and you don't want to blink. New LLMs brimming with exciting features consistently seize the headlines of my news feeds.

With Chat2VIS, you can use natural language to prompt up to 5 LLMs to generate Python code that builds plots from a dataset. (Learn more about why I built Chat2VIS from my journal article.)

I wanted to put 3 of the latest LLMs to the test, comparing their performance in generating code for various visualisations. From creating bar charts and time series data, to handling misspelled words and ambiguous prompts, I uncover how each model responds.

The results provide interesting insights into the strengths and limitations of these models, with a focus on Code Llama’s potential and the benefits of GPT-3.5 Instruct and GPT-4.

In this post, you'll discover:

Why I chose to compare Code Llama vs. OpenAI models
Six scenarios where Chat2VIS compares Code Llama, GPT-3.5 Instruct, and GPT-4 models
Tip to improve success with open-source models
Why I chose to compare Code Llama and GPT models

Open source models, like Code Llama, are free to use, and easier to fine-tune on your own data. OpenAI models are easy to use “out of the box”, with some now available to fine-tune, but come with a cost. Historically, I’ve faced challenges using open source models, finding they often misunderstood the request or failed completely to generate accurate Python code.

When Code Llama was released, an LLM tuned for code generation, I was keen to see how it compared to the OpenAI models.

I've been impressed with Code Llama, which shows great potential for this task without incurring the cost associated with the OpenAI models (read more here).

I have opted for the "Instruct" fine-tuned variation of Code Llama. It aligns well with the existing prompt style, which issues instructions in natural language followed by the beginning of a Python code script.

Let's see how it stacks up against OpenAI's GPT-4 and their recent release of GPT-3.5 Instruct.

Quick overview of Chat2VIS

Before we begin comparing LLMs, here’s a look again at how Chat2VIS works (check out the full blog post to learn more.)

Chat2VIS App Architecture
6 case studies using Chat2VIS to compare Code Llama vs. GPT-3.5 Instruct and GPT-4

Using Chat2VIS, I tested how each model performed based on 6 different scenarios.

💡
To try for yourself: follow these instructions to generate your HuggingFace API token (no credit required) for Code Llama. Acquire an OpenAI API key here and add some credit to your account. I'll walk you through all the examples from the Chat2VIS published article, this time using GPT-4, the new GPT-3.5 Instruct model, and Code Llama.

For each example, choose the dataset from the sidebar radio button options, select the models using the checkboxes, and enter your API keys for OpenAI and HuggingFace.

Case study 1: Generate code for bar chart

This example uses the pre-loaded "Department Store" dataset.

Run the following query: "What is the highest price of product, grouped by product type? Show a bar chart, and display by the names in desc.”

Kudos to all three models for producing the same results! (Even though they may have different labels and titles.)

GPT-4 ✅
Code Llama ✅
GPT-3.5 Instruct ✅
Case study 2: Generate code for time series

Using the "Energy Production" dataset, run the query: "What is the trend of oil production since 2004?”

Results

Impressive! All three models generated almost identical plots, showing data from 2004 onwards.

GPT-4 ✅
Code Llama ✅
GPT-3.5 Instruct ✅
Case study 3: Plotting request with an unspecified chart type

Here, I’m using the pre-loaded "Colleges" dataset in the sidebar radio button.

Run the query: "Show debt and earnings for Public and Private colleges."

Results

GPT-4 ✅
Code Llama 🤔
During the initial runs of this example, I discovered that Code Llama had some limitations similar to other legacy OpenAI models. It repeatedly attempted to generate scatter plot code assigning invalid values to the function’s c parameter, as also mentioned in this article. As a result, the code failed to execute. To improve its success rate, I made a slight adjustment to the prompt (for the exact wording, delve into the prompt engineering within this code).
GPT-3.5 Instruct 🤔 plotted average values, maybe not quite as informative as the other models.
Case study 4: Parsing complex requests

Let's examine a more complex example where the models need to select a subset of the data. Using the Customers & Products dataset, run the query: "Show the number of products with price higher than 1000 or lower than 500 for each product name in a bar chart, and could you rank y-axis in descending order?”

Results

GPT-4 succeeded in this case ✅
GPT-3.5 Instruct produced an empty plot ❌
It's surprising that GPT-3.5 Instruct didn't succeed, as this query has previously worked for ChatGPT3.5, GPT-3, and Codex.
Code Llama also failed ❌ for several reasons.
It did not filter the data to include only prices higher than $1000 or lower than $500.
It didn’t sort the data as requested.
I encountered these kinds of limitations frequently while exploring Code Llama's capabilities.
Case study 5: Misspelled prompts

Returning to the "Movies" dataset, let's see how Code Llama handles misspelled words. Run the query: “draw the numbr of movie by gener.”

Look at that! Each model overlooked my spelling mistakes!

GPT-4 ✅
GPT-3.5 Instruct ✅
Code Llama ✅
While it didn’t sort the results in the same order as the OpenAI models, the prompt didn't specify any sorting.
Code Llama, that uninformative legend is not very helpful!
Case study 6: Ambiguous prompts

Continuing with the "Movies" dataset, let's submit the single word “tomatoes” and observe how the models process it.

Results

GPT-4 ✅
Code Llama ✅
GPT-3.5 Instruct ❌  This model did not identify a relevant “tomato” visualisation to the movie data set.
How to improve success with open-source models

I have compared the performance of Code Llama, GPT-3.5 Instruct, and ChatGPT-4 using examples from published research previously showcasing ChatGPT-3.5, GPT-3, and Codex.

Initial experiments show promise, but the OpenAI models still outperform Code Llama in several scenarios. I encourage you to experiment and share your opinions.

In the future, I plan to enhance the prompt further and explore various other prompting techniques to potentially improve Code Llama's accuracy. Although I want to avoid overcomplicating the instructions, I acknowledge its potential for improvement.

For this task, considering my prompting style, ChatGPT-4 is my preferred choice.

However, taking into consideration the comparable results of ChatGPT-3.5 in the journal article and previous blog, together with the lower cost of the GPT-3.5 models (costs here), I would ultimately still choose ChatGPT-3.5. Nonetheless, it may be worthwhile to fine-tune a Code Llama model to further explore its capabilities, as it offers a cost-effective solution for Chat2VIS.

Wrapping up

Thank you for reading my post!

I’d love to hear your opinions and the outcomes of your experiments. If you have any questions, please post them in the comments below or contact me on LinkedIn.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Vicky Kuo - Streamlit
https://blog.streamlit.io/author/vicky/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Vicky Kuo
1 post
Land your dream job: Build your portfolio with Streamlit

Showcase your coding skills to recruiters with a chatbot tailored to your resume

by
Vicky Kuo
,
October 13 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Paula Maddigan - Streamlit
https://blog.streamlit.io/author/paula/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Paula Maddigan
2 posts
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Chat2VIS: AI-driven visualisations with Streamlit and natural language

Leverage ChatGPT for Python code generation using prompt engineering

LLMs
by
Paula Maddigan
,
July 27 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

How to build a real-time LLM app without vector databases
https://blog.streamlit.io/build-a-real-time-llm-app-without-vector-databases-using-pathway/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

By Bobur Umurzokov
Posted in LLMs, October 19 2023
The role of Pathway and LLM App
Overall app architecture
How to build a real-time discount tracking app
Step 1. Data collection (custom data ingestion)
Step 2. Data loading and mapping
Step 3. Data embedding
Step 4. Data indexing
Step 5. User query processing and indexing
Step 6. Similarity search and prompt engineering
Step 7. Return the response
Step 8. Put everything together
Step 9. Design the UI with Streamlit
Step 10. Run the app
Wrapping up
Contents
Share this post
← All posts
👉
TL;DR: Learn how to build a discount finder app without using vector databases, additional frameworks, and a complex stack. Use the project source code to clone the repo and run the code sample by following the instructions in the README.md file.

Ever tried asking ChatGPT a question about real-time discounts, deals, or coupons?

For example, “Can you give me discounts for Adidas men's shoes?” If you did, I’m sure you’ve been frustrated by the generic response it gave you, “I’m sorry, but I don’t have real-time growing capabilities or access to current promotion.”

Why? Because GPT lacks specific information.

Challenges of Existing Solutions

You could try typing in a single JSON item from the Amazon products deal, but you’ll face two problems:

Text length. The text length is restricted—a big problem when dealing with thousands of sale items.
Unusable data. The data may need to be cleaned and formatted.
0:00
/0:20
1×

You could also try using the OpenAI Chat Completion endpoint or building custom plugins, but you’ll face additional problems:

Cost. Providing more detailed information and examples to improve the model's performance can increase costs. For example, with GPT-4, the cost is $0.624 per prediction for an input of 10k tokens and an output of 200 tokens. Sending identical requests repeatedly can escalate costs unless you use a local cache system.
Latency. Utilizing ChatGPT APIs for production, like those from OpenAI, can be unpredictable in terms of latency. There is no guarantee of consistent service provision.
Security. Integrating custom plugins requires specifying every API endpoint in the OpenAPI spec for functionality. This means exposing your internal API setup to ChatGPT, which may be a risk that many enterprises are skeptical of.
Offline evaluation. When you conduct offline tests on code and data output or replicate the data flow locally, each system request may yield varying responses.

To solve these challenges (and to buy cool Adidas shoes at a discount, of course! 👟), I built a custom Language Learning Model (LLM) discount finder app without using vector databases, additional frameworks, and a complex stack.

The same solution can be applied to develop production-ready AI apps that use real-time data available in your data sources.

In this post, I’ll walk through 10 steps on how to develop and expose an AI-powered HTTP REST API using Pathway and LLM App and design the UI with Streamlit to consume the API data through REST.

The role of Pathway and LLM App

Pathway is a powerful data processing framework in Python that takes care of real-time data updates from various data sources using its built-in connectors for structured, unstructured, and live data. For the discount finder app, I used Pathway to ingest sales data as streams to app and make sure that the app detects every change in a data input that changes frequently.

LLM App is a production Python framework for building and serving AI applications. LLM App uses Pathway libraries under the hood to achieve real-time data indexing and vector similarity search. Using a combination of these two tools, the app is not only aware of changes in the documents but also updates vector indexes in real time and uses this new knowledge to answer the next questions without the need for storing and retrieving vector indexes to/from a vector database.

Overall app architecture

Let’s take a look at the app’s overall architecture. I was inspired by this article and wanted my app to expose the HTTP REST API endpoint—so you could get the best deals by using CSVs, JSON Lines, APIs, message brokers, or databases.

The app supports two types of data sources (if you want, you can add custom input connectors):

JSON Lines: The data source expects each line to contain a doc object. Make sure to convert your input data to the Jsonlines format. You can find a sample data file at discounts.jsonl.
Rainforest Product API: This API gives you the daily discount data from Amazon products.

Go to the app and try typing in “Show me discounts”:

0:00
/0:23
1×

The app will index Rainforest API and an example discounts.csv file documents in real-time and use the data when processing queries.

How to build a real-time discount tracking app
Step 1. Data collection (custom data ingestion)

To add custom data for ChatGPT, you need to build a data pipeline for ingesting, processing, and exposing data in real-time.

For simplicity, use any JSON Lines file as a data source. The app accepts files like discounts.jsonl and uses this data when processing user queries. Each line in the data source should contain a doc object. Make sure to convert your input data to JSON Lines format.

Here is an example of a JSON Lines file with a single entry:

{"doc": "{'position': 1, 'link': '<https://www.amazon.com/deal/6123cc9f>', 'asin': 'B00QVKOT0U', 'is_lightning_deal': False, 'deal_type': 'DEAL_OF_THE_DAY', 'is_prime_exclusive': False, 'starts_at': '2023-08-15T00:00:01.665Z', 'ends_at': '2023-08-17T14:55:01.665Z', 'type': 'multi_item', 'title': 'Deal on Crocs, DUNLOP REFINED(\\u30c0\\u30f3\\u30ed\\u30c3\\u30d7\\u30ea\\u30d5\\u30a1\\u30a4\\u30f3\\u30c9)', 'image': '<https://m.media-amazon.com/images/I/41yFkNSlMcL.jpg>', 'deal_price_lower': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48'}, 'deal_price_upper': {'value': 52.14, 'currency': 'USD', 'symbol': '$', 'raw': '52.14'}, 'deal_price': 35.48, 'list_price_lower': {'value': 49.99, 'currency': 'USD', 'symbol': '$', 'raw': '49.99'}, 'list_price_upper': {'value': 59.99, 'currency': 'USD', 'symbol': '$', 'raw': '59.99'}, 'list_price': {'value': 49.99, 'currency': 'USD', 'symbol': '$', 'raw': '49.99 - 59.99', 'name': 'List Price'}, 'current_price_lower': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48'}, 'current_price_upper': {'value': 52.14, 'currency': 'USD', 'symbol': '$', 'raw': '52.14'}, 'current_price': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48 - 52.14', 'name': 'Current Price'}, 'merchant_name': 'Amazon Japan', 'free_shipping': False, 'is_prime': False, 'is_map': False, 'deal_id': '6123cc9f', 'seller_id': 'A3GZEOQINOCL0Y', 'description': 'Deal on Crocs, DUNLOP REFINED(\\u30c0\\u30f3\\u30ed\\u30c3\\u30d7\\u30ea\\u30d5\\u30a1\\u30a4\\u30f3\\u30c9)', 'rating': 4.72, 'ratings_total': 6766, 'page': 1, 'old_price': 49.99, 'currency': 'USD'}"}


The app is always aware of the changes in the data folder. If you add another JSON Lines file, it will automatically update the AI model's response.

Step 2. Data loading and mapping

Using Pathway's JSON Lines input connector, read the local JSON Lines file, map data entries into a schema, and create a Pathway Table (see the full source code in app.py):

...
sales_data = pw.io.jsonlines.read(
    "./examples/data",
    schema=DataInputSchema,
    mode="streaming"
)


Map each data row into a structured document schema (see the full source code in app.py):

class DataInputSchema(pw.Schema):
    doc: str

Step 3. Data embedding

Each document is embedded with the OpenAI API and retrieves the embedded result (see the full source code in embedder.py):

...
embedded_data = embeddings(context=sales_data, data_to_embed=sales_data.doc)

Step 4. Data indexing

Construct an instant index on the generated embeddings:

index = index_embeddings(embedded_data)

Step 5. User query processing and indexing

Create a REST endpoint, take a user query from the API request payload, and embed the user query with the OpenAI API.

...
query, response_writer = pw.io.http.rest_connector(
    host=host,
    port=port,
    schema=QueryInputSchema,
    autocommit_duration_ms=50,
)

embedded_query = embeddings(context=query, data_to_embed=pw.this.query)

Step 6. Similarity search and prompt engineering

To perform a similarity search, utilize the index to identify the most relevant matches for the query embedding. Then create a prompt that combines the user's query with the retrieved relevant data results. This prompt is then sent to the ChatGPT completion endpoint to generate a comprehensive and detailed response.

responses = prompt(index, embedded_query, pw.this.query)


You used the same in-context learning approach when creating the prompt and incorporated internal knowledge into ChatGPT in the prompt.py file.

prompt = f"Given the following discounts data: \\\
 {docs_str} \\\
answer this query: {query}"

Step 7. Return the response

The final step is just to return the API response to the user.

# Build prompt using indexed data
responses = prompt(index, embedded_query, pw.this.query)

Step 8. Put everything together

Combine all the steps to get a Python API enabled with LLM for custom discount data. You can use it by referring to the implementation in the app.py Python script.

import pathway as pw

from common.embedder import embeddings, index_embeddings
from common.prompt import prompt

def run(host, port):
    # Given a user question as a query from your API
    query, response_writer = pw.io.http.rest_connector(
        host=host,
        port=port,
        schema=QueryInputSchema,
        autocommit_duration_ms=50,
    )

    # Real-time data coming from external data sources such as jsonlines file
    sales_data = pw.io.jsonlines.read(
        "./examples/data",
        schema=DataInputSchema,
        mode="streaming"
    )

    # Compute embeddings for each document using the OpenAI Embeddings API
    embedded_data = embeddings(context=sales_data, data_to_embed=sales_data.doc)

    # Construct an index on the generated embeddings in real-time
    index = index_embeddings(embedded_data)

    # Generate embeddings for the query from the OpenAI Embeddings API
    embedded_query = embeddings(context=query, data_to_embed=pw.this.query)

    # Build prompt using indexed data
    responses = prompt(index, embedded_query, pw.this.query)

    # Feed the prompt to ChatGPT and obtain the generated answer.
    response_writer(responses)

    # Run the pipeline
    pw.run()

class DataInputSchema(pw.Schema):
    doc: str

class QueryInputSchema(pw.Schema):
    query: str

Step 9. Design the UI with Streamlit

Use Streamlit to make your app more interactive (refer to the implementation in the app.py file). You can build UI for your backend services without having knowledge of front-end tools. The use of Streamlit's st.sidebar allows for the organization of secondary information, keeping the main area focused on the primary interaction. You create a sidebar to explain to users how to use the app:

with st.sidebar:
    st.markdown(
        "## How to use\
"
        "1. Choose data sources.\
"
        "2. If CSV is chosen as a data source, upload a CSV file.\
"
        "3. Ask a question about the discounts.\
"
    )


Users are presented with a multi-select dropdown to choose data sources, and if CSV is chosen, they can upload a CSV file via the st.file_uploader widget. Streamlit's declarative nature stands out in the code, with the interface updating based on the state of variables. For example, the file uploader's disabled state is linked to the selected data sources.

uploaded_file = st.file_uploader(
    "Upload a CSV file",
    type=("csv"),
    disabled=(DataSource.CSV.value not in data_sources)
)


Once a CSV file is uploaded, its content is processed and written into a jsonlines file format, displaying a progress bar to inform the user of the ongoing operation. and the progress bar offers real-time feedback while processing the uploaded CSV.

if uploaded_file and DataSource.CSV.value in data_sources:
    df = pd.read_csv(uploaded_file)

    # Start progress bar
    progress_bar = st.progress(0, "Processing your file. Please wait.")


Depending on the selected data sources and the provided question, the application interfaces with a Discounts API to fetch relevant answers.

question = st.text_input(
    "Search for something",
    placeholder="What discounts are looking for?",
    disabled=not data_sources
)


Here is the code that handles Discounts API requests when the user selects a data source and asks a question. Error messages and responses from the API are handled smoothly, giving direct feedback to the user through st.error and st.write methods.

if data_sources and question:
    if not os.path.exists(csv_path) and not os.path.exists(rainforest_path):
        st.error("Failed to process discounts file")

    url = f'http://{api_host}:{api_port}/'
    data = {"query": question}

    response = requests.post(url, json=data)

    if response.status_code == 200:
        st.write("### Answer")
        st.write(response.json())
    else:
        st.error(f"Failed to send data to Discounts API. Status code: {response.status_code}")

Step 10. Run the app

Follow the instructions in the README.md file's How to run the project section to run the app. Note that you’ll need to run the API and UI as separate processes. Streamlit will automatically connect to the Discounts backend API, and you’ll see the UI frontend running in your browser.

In this tutorial, Pathway's LLM App and Streamlit communicate over HTTP REST API. You can run the app using Docker with a single docker compose up command (refer to the run with the Docker section in the README.md file). The inability to embed the LLM App into Streamlit as a single process is due to Streamlit having its own program lifecycle loop, which triggers a complete app rerun whenever there is a change. This behavior can disrupt the data flow, especially since Pathway operates in streaming mode. Considering the above, there are two more ways to integrate Pathway's LLM app with Streamlit:

Run Pathway's LLM app as a subprocess and communicate with it over inter-process communications such as sockets or TCP/IP. This can involve using random ports or signals to trigger actions like state dumps that can be picked up or pickled. For example, you can leverage Python’s Subprocess module to achieve that.
Pathway's LLM App and Streamlit share the same file storage to communicate. For example, you upload documents with a user query to a folder on your local disk. LLM App can listen to every change in that folder and access the files to process them, answer user queries, and write responses back to the file.
Wrapping up

I’ve only scratched the surface of what you can do with an LLM app by incorporating domain-specific knowledge like discounts into ChatGPT. You can also:

Incorporate additional data from external APIs, formats such as JSON Lines, PDF, Doc, HTML, or text, and databases like PostgreSQL or MySQL.
Stream data from platforms like Kafka, Redpanda, or Debedizum.
Enhance the Streamlit UI to accept any deals API, not just the Rainforest API.
Maintain a data snapshot to observe changes in sales prices over time. Pathway provides a built-in feature to calculate differences between two alterations.
Send processed data to other downstream connectors, such as BI and analytics tools. For example, you can configure it to receive alerts when price shifts are detected.

If you have any questions, please leave them in the comments section below or contact me on LinkedIn and Twitter. Join the Discord channel to see how the AI ChatBot assistant works.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Bobur Umurzokov - Streamlit
https://blog.streamlit.io/author/bobur/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Bobur Umurzokov
1 post
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Take your Streamlit apps to the next level with GPT-4
https://blog.streamlit.io/take-your-streamlit-apps-to-the-next-level-with-gpt-4/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

By Charly Wargnier
Posted in LLMs, October 24 2023
Some background
The rise of ChatGPT
GPT-4, the game changer
What about trustworthiness in LLMs
#1: Use GPT-4 for faster Streamlit app development
1.1 — GPT-4 as a starting point for any app
1.2 — Prompting tips to streamline your app design
1.3 — Convert Jupyter notebooks to Streamlit apps in minutes
#2: Lightning fast debugging + enhanced codebase management
2.1 — Swiftly analyze error log traces to identify issues
2.2 — Overcoming the context window limitations
2.3 — Reshape any code, anywhere in your codebase
#3: From sluggish to speedy, use GPT-4 to improve your app performance
3.1 — Automatically diagnose performance issues
3.2 — Diagnosing caching issues in large codebases
Wrapping up
Contents
Share this post
← All posts

Designing and scaling a Streamlit app can be a daunting task! As developers, we often face challenges like designing good UIs, debugging our apps quickly, and making them fast.

What if there was a tool to speed it all up?

This tool has a name, it is called GPT-4!

In this guide, we’ll be taking a look at:

The evolution of ChatGPT, from its rise to understanding the trustworthiness of large language models.
Lightning-fast app development with GPT-4, including prompt tips and notebook-to-Streamlit app conversion.
Efficient debugging and codebase changes with GPT-4.
App performance optimization with GPT-4, and without the stress!

Whether you’re a seasoned Streamlit developer or just getting started, this guide will help you leverage GPT-4 to build better apps, faster.

So grab a cup of coffee (or tea, or whatever your favorite beverage is) and let’s get started!

Some background
The rise of ChatGPT

In November 2022, OpenAI released ChatGPT, and it immediately took the world by storm!

For the first time, people could have meaningful conversations with an AI on any topic and use it for tasks spanning education, creative writing, legal research, personal tutoring, code creation, and more.

As of January 2023, it has over 100 million users, making it the fastest-growing platform ever.

But you might be wondering, how did early-day ChatGPT, powered by GPT-3.5, go at designing Streamlit apps?

It was actually far from perfect, often inaccurate, and required a hefty amount of manual fine-tuning to get the apps right.

GPT-4, the game changer

Released in March 2023, GPT-4 significantly improved short-term memory, parameters, and creativity, leading to far more accurate and creative responses than GPT-3.5.

Another notable perk: GPT-4 was trained on more recent data (up to January 2022) compared to GPT-3.5’s September 2021 cut-off.

So even though GPT-4 may not be aware of Streamlit’s latest feats such as Chat elements or st.experimental_connection(), its newer training data and the aforementioned improvements have really bolstered its ability to create great Streamlit apps.

What about trustworthiness in LLMs

Although GPT-4 has improved its reliability, like other LLMs, it can still produce misleading or fictional outputs known as hallucinations (here’s a good read about them).

These can be attributed to lack of recent data, biases in the training data, and unclear or ambiguous prompts.

You can usually address them by refining your prompts iteratively until you achieve the desired results. Later we’ll cover prompting tips to reduce hallucinations. I’ll also explain how to improve data robustness via GPT-4’s Code Interpreter.

#1: Use GPT-4 for faster Streamlit app development
1.1 — GPT-4 as a starting point for any app

These days, I usually start with GPT-4 when designing any Streamlit app. I then iterate via the chat interface to quickly experiment with various prompt ideas.

Getting started is easy as 1, 2, 3:

It starts with a good prompt!
✏️
Write a humorous Streamlit app about how Adrien Treuille, an LLM genius and developer extraordinaire at Snowflake, is an LLM genius.

Include an Altair bar chart and an Altair line chart. Include at least 2 numerical input sliders in the sidebar

- 1 slider for the Altair bar chart at the top of the app
- 1 slider for the Altair line chart at the bottom of the app

The app should allow Adrien to predict the rise of generative AI in the next 5 years. Will we reach singularity?
Go to ChatGPT and select GPT-4. You'll need a ChatGPT Plus subscription to access it.
Paste the prompt into the ChatGPT.
Try the generated code on your local machine. If you’re new to Streamlit, follow the installation steps here.

Let’s check the Streamlit app generated by GPT-4 below:

Verdict: the app is impeccable! Not a single change had to be made to make the app work.

1.2 — Prompting tips to streamline your app design

“Garbage in, Garbage out” applies well to prompting. The code quality greatly depends on the prompts you put in!

So what makes a good prompt for designing Streamlit apps?

In the prompt above, my instructions are clear. I specified the tone, context, app features, and added the charts and widgets I wanted to see.

Here are 5 prompting tips to help you get started:

By following these simple guidelines, you should be able to get the desired results faster, reducing the need for back-and-forth conversations in the ChatGPT interface.

Now, let’s explore some more practical use cases!

1.3 — Convert Jupyter notebooks to Streamlit apps in minutes

If creating Streamlit apps from scratch is fun, converting your Jupyter notebooks into Streamlit apps is even funner!

GPT-4 can seamlessly automate most of the these conversion steps, such as:

Replacing traditional Python syntax with their Streamlit counterparts.
Remove Jupyter-specific imports and replace them with Streamlit-specific ones.
Excluding Jupyter-specific components like cell magics (%%) and comments.

Below is a Jupyter notebook. I’m asking GPT-4 to convert it into a Streamlit app.

Jupyter notebook — before GPT-4 processing:

# [Cell 1]
import numpy as np
import matplotlib.pyplot as plt
# [Cell 2]
# Generate random data
data = np.random.randn(1000)
 [Cell 2]
# Generate random data
data = np.random.randn(1000)
# [Cell 3]
# Cell magic for inline plots
%%matplotlib inline
# [Cell 4]
# User input for number of bins
bins = int(input("Enter number of bins for histogram: "))
# [Cell 5]
# Plot histogram
plt.hist(data, bins=bins)
plt.title("Random Data Histogram")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.show()

Converted Streamlit app — after GPT-4 processing:

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
# Title for the Streamlit app
st.title("Random Data Histogram")
# Generate random data
data = np.random.randn(1000)
# User input for number of bins using Streamlit's slider
bins = st.slider("Choose number of bins for histogram", 10, 100, 50)
# Plot histogram
fig, ax = plt.subplots()
ax.hist(data, bins=bins)
ax.set_title("Random Data Histogram")
ax.set_xlabel("Value")
ax.set_ylabel("Frequency")
st.pyplot(fig)

GPT-4 did it again!

Python syntax has been converted to the equivalent Streamlit syntax.
Jupyter-specific imports were replaced with Streamlit ones.
Jupyter-specific components like cell magics and redundant comments were removed.

Onward to debugging apps with GPT-4!

#2: Lightning fast debugging + enhanced codebase management
2.1 — Swiftly analyze error log traces to identify issues

As a developer advocate, a big part of my job is about debugging Streamlit apps.

I build demos, submit pull requests, and spend time on forums assisting users in our community.

And while it’s crucial to always double-check the accuracy of its outputs, GPT-4 is still a tremendous tool to quickly scan through error logs and find the root causes of any bugs or issues.

Here’s a simple example.

Paste this cryptic error message from Streamlit’s error logs into ChatGPT’s interface and press send:

File “/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py”, line 552, in _run_script
exec(code, module.dict)
File “/mount/src/stroke_probability/Stroke_Proba.py”, line 66, in
svm1, svm2, logit1, logit2, nbc1, nbc2, rf1, rf2, errGBR = loadAllModels(URL)
File “/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py”, line 211, in wrapper
return cached_func(*args, **kwargs)
File “/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py”, line 240, in call
return self._get_or_create_cached_value(args, kwargs)
File “/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py”, line 266, in _get_or_create_cached_value
return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)
File “/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py”, line 320, in _handle_cache_miss
computed_value = self._info.func(*func_args, **func_kwargs)
File “/mount/src/stroke_probability/Stroke_Proba.py”, line 58, in loadAllModels
joblib.load(
File “/home/adminuser/venv/lib/python3.9/site-packages/joblib/numpy_pickle.py”, line 577, in load
obj = _unpickle(fobj)
File “/home/adminuser/venv/lib/python3.9/site-packages/joblib/numpy_pickle.py”, line 506, in _unpickle
obj = unpickler.load()
File “/usr/local/lib/python3.9/pickle.py”, line 1212, in load
dispatchkey[0]


GPT-4 will analyze the error trace and provide relevant recommendations in seconds:

(Link to the chat)

Fast. Efficient. And not a single StackO or Google search in sight! Hallelujah!

But what if we need to go beyond simple error log trace debugging and review bugs and code changes not only for a single file but multiple files?

Let’s see how GPT-4 can help us.

2.2 — Overcoming the context window limitations

In LLM terminology, the context window refers to the maximum number of tokens (words or characters) that a language model can “see” at once when generating a response.

The GPT-4 model available in ChatGPT Plus has a context window limit of 8,192 tokens, which is twice what GPT-3.5 currently provides.

However, even with 8,192 tokens, it may not be sufficient to analyze most codebases.

Enter GPT-4’s Code Interpreter!

GPT-4’s Code Interpreter lets you upload files in various formats (zip, txt, pdf, doc, excel, and more) and securely run code or analyze data right in the ChatGPT interface!

It also runs separately from the chat in a sandboxed environment, enabling analysis of large codebases without that context window constraint.

So let’s go ahead and give it a try in the section below — you’ll be amazed at what it can do!

2.3 — Reshape any code, anywhere in your codebase

Let’s use one of my Streamlit apps as an example.

I created the CodeLlama Playground app to showcase the capabilities of Meta’s new CodeLlama model. You can get the repo here.

DeepInfra, the company hosting the CodeLlama model, has recently introduced Mistral 7-B, a new open-source LLM that competes with CodeLlama in terms of performance.

I want to update my Streamlit app with the new Mistral algorithm. Let’s see how GPT-4 can help us.

Head to the GitHub repo and download it by clicking on Download ZIP.
Open a ChatGPT session and choose the ‘Advanced Data Analysis’ option to enable GPT-4’s Code Interpreter.
Upload the repo.
Add this prompt:
✏️
Review the repo of my CodeLlama Playground Streamlit app I just uploaded. Modify its contents to integrate the Mistral-7B code below into the app.

import openai
Configure OpenAI client to use our endpoint
openai.api_key = “<YOUR DEEPINFRA TOKEN: deepctl auth token>” openai.api_base = “https://api.deepinfra.com/v1/openai"
chat_completion = openai.ChatCompletion.create( model=”mistralai/Mistral-7B-Instruct-v0.1", messages=[{“role”: “user”, “content”: “Hello”}], )
print(chat_completion.choices[0].message.content)`

I want the Mistral model to be selectable from a radio menu in the app’s sidebar, along with other models
GPT-4 will scan the entire codebase, analyze the files, integrate the Mistral model into the Streamlit app, and make the necessary modifications throughout the repo.
Once that’s done, ask ChatGPT for permission to download the edited repo.
Replace your local repo with the updated one and check the app.

The app is working well! The Mistral 7b can be selected from the radio selector in the top left section, along with other Llama models. I added the cyclone emoji manually, but the rest of the content was generated by GPT-4.

The entire process, from prompting to updating the app, was done in less than 2 minutes. Impressive!

#3: From sluggish to speedy, use GPT-4 to improve your app performance
3.1 — Automatically diagnose performance issues

Web apps need to be fast. A fast web app keep users happy and coming back for more.

Below I have created an app with suboptimal coding, causing performance issues:

import streamlit as st
import pandas as pd
import sqlalchemy as db
import requests
import tensorflow as tf

price = st.number_input("House Price", min_value=100000, max_value=10000000, step=100000)
def load_data():
    return pd.read_csv('large_dataset.csv')
data = load_data()
used_columns = ['Column1', 'Column2', 'Column3']
data = data[used_columns]
def download_model():
    url = "<https://example.com/large-model.h5>"
    response = requests.get(url)
    with open('large-model.h5', 'wb') as f:
        f.write(response.content)
download_model()
model = tf.keras.models.load_model('large-model.h5')
def load_efficient_data():
    return pd.read_parquet('efficient_data.parquet')
efficient_data = load_efficient_data()
database_uri = 'sqlite:///example.db'
engine = db.create_engine(database_uri)
query = "SELECT * FROM large_table WHERE condition = true"
db_data = pd.read_sql_query(query, con=engine)

If you paste this code into ChatGPT and ask for performance improvements — GPT-4 can identify areas for improvement in seconds!

Here’s a breakdown of how GPT-4 would address each issue:

Recommendation 01: Add caching

The load_data function is called without caching, causing it to read the CSV file from the disk every time it's called, impacting performance. To mitigate this, use @st.cache to cache function results, reducing the need for recomputing data with each interaction.

Pre (no caching):

def load_data():
    return pd.read_csv('precalculated_data.csv')
    data = load_data()

Post (with caching):

@st.cache(ttl=24*60*60)  # Cache the data with a time-to-live of 24 hours
def load_data():
    return pd.read_csv('precalculated_data.csv')
    data = load_data()

Recommendation 02: Avoid downloading large, static models

The download_model function downloads the large model file every time the app is run. This can impact the app's performance, especially if the model file is very large. To avoid this, store the model on the production machine or use Git LFS to manage large files in your repository.

Pre (without efficient model management):

import requests
import tensorflow as tf
def download_model():
    url = "<https://example.com/large-model.h5>"
    response = requests.get(url)
    with open('large-model.h5', 'wb') as f:
        f.write(response.content)
download_model()  # This will download the model every time the app runs
model = tf.keras.models.load_model('large-model.h5')


Post (with efficient model management):

import tensorflow as tf
import os
model_path = 'models/large-model.h5'
if not os.path.exists(model_path):
    # Code to download the model
    import requests
    url = "<https://example.com/large-model.h5>"
    response = requests.get(url)
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    with open(model_path, 'wb') as f:
        f.write(response.content)
model = tf.keras.models.load_model(model_path)

Recommendation 03: Use efficient binary-serialized formats

Reading data from a CSV file can be slow, especially for large datasets. You should use efficient binary-serialized formats like Apache Parquet or Apache Arrow for storing and reading large datasets.

Pre (without Parquet):

def load_data():
    return pd.read_csv('large_dataset.csv')  # Reading data from a CSV file
    data = load_data()

Post (with Parquet):

def load_efficient_data():
    return pd.read_parquet('large_dataset.parquet')  # Reading data from a Parquet file
    efficient_data = load_efficient_data()


Yet again, in merely a few seconds, GPT-4 was able to propose these significant performance enhancements, a remarkable feat indeed!

Since these changes were made in one file, let’s spice things up a bit and test GPT-4’s ability to diagnose similar performance issues on a larger codebase.

3.2 — Diagnosing caching issues in large codebases

One of the best ways to speed up your Streamlit app is to leverage caching methods. Put simply, these methods optimize performance by eliminating redundant recalculations.

Streamlit offers two caching mechanisms, @st.cache_data and @st.cache_resource:

@st.cache_data is ideal for functions that return data types such as DataFrames and arrays. It addresses mutation and concurrency issues via serialization.
@st.cache_resource is tailored for caching global resources, for instance, ML models or database connections.

Adding caching methods to functions in a single Python file is ususally straightforward, but it can be tricky to figure out which functions to cache and which caching method to choose when you’re working with multiple files and a larger codebase.

GPT-4’s Code Interpreter to the rescue, yet again!

I’ve created a demo app with multiple Python functions, spread across different files. You can download it here.

/my_streamlit_app/
|– data/
|   |– large_dataset.csv
|-- models/
|   |-- heavy_model.pkl
|-- src/
|   |-- data_loader.py
|   |-- model_loader.py
|   |-- predictor.py
|   |-- transformer.py
|   |-- analyser.py
|   |-- forecast.py
|-- streamlit_app.py
|-- requirements.txt


Some of these functions either involve heavy I/O or compute tasks and currently lack caching. As a result, the app is loading slowly and each operation is sluggish.

We want the Code Interpreter to inspect the entire codebase and do the following:

Identify the functions that would benefit from caching.
Suggest the best caching method based on what each function does.
Implement those changes for us.

Here’s the process:

Upload the repo to ChatGPT.
Add this prompt:
✏️
Review the repository of the Streamlit app I just uploaded via the Code Interpreter.

Identify functions in the codebase that would benefit from caching.

Recommend appropriate caching techniques, either @st.cache_data or @st.cache_resource decorators.

Create a markdown table with the following columns:
- Column 1: file name where we need the caching methods.
- Column 2: function name to add the cache to.
- Column 3: recommended caching methods.
- Column 4: reason for using that cache method.
Visit this URL and copy the body text from each page. This will help GPT-4 learn about our newest caching methods that were not available during its training. It will improve the quality of ChatGPT’s answers.
Press send message.
Check GPT-4’s output:
Ask ChatGPT to implement these changes and give us a downloadable copy of the edited repo.

Hooray! GPT-4 has not only provided us with a clear recommendation sheet that we can share with our peers and colleagues, but it has also implemented all of those changes for us across the entire codebase. The code is now ready to be copied into your local or deployed environment!

Isn’t that amazing?

Wrapping up

We’ve covered a lot in this guide!

After a preamble to ChatGPT, GPT-4, and LLM trustworthiness, we provided prompting tips for various use cases of GPT-4 in app design and debugging.

We have then gone beyond single-file analysis and discussed ways to automatically refactor your apps, codebase, and optimize performance at scale, all made possible with GPT-4’s Code Interpreter.

Keep in mind that we’ve barely scratched the surface of what’s possible with GPT-4 in this post. We have yet to explore ChatGPT’s plugin ecosystem, its browsing or vision capabilities! (Psst... I will cover great use cases leveraging ChatGPT Vision in Part 2!).

As a final word, I would encourage you to think outside the box and embrace creativity in your prompts, you may just be blown away by the convos that will follow!

Please share your comments, use cases and tips below. Also, keep an eye on my Twitter/X feed, where I regularly share cool LLM use cases!

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

AppTest: test & build interactive Python data apps faster
https://blog.streamlit.io/apptest-faster-automated-testing/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

By Joshua Carroll
Posted in Product, October 31 2023
Introducing: AppTest
How AppTest works
AppTest in action
Ready, set, test!
Show off what you have built!
Contents
Share this post
← All posts

Every data app builder wants to build a flawless app in record time, but speed and quality may feel at odds with each other. Imagine pouring your heart into coding a beautiful new Python app, eager to share it with the world. However, before you can deploy, you can’t rush testing, else you’ll risk a code error tarnishing your app experience. 

We've heard your pain and felt this ourselves. In practice, you probably do some manual sanity tests of your app changes and hope for the best. Yesterday’s automated test options are usually complex and hard to maintain:

Conduct unit tests on the backend logic by factoring it out from the UI
Set up a heavyweight browser testing framework like Selenium, Playwright, or Cypress for end-to-end testing

Luckily, you don’t have to live in this reality anymore. You can develop faster and ensure high quality!

Introducing: AppTest

AppTest is a new automated way to write and execute tests natively in Streamlit. Developers can use this API to confirm that all aspects of their app are working correctly. 

With this automated testing framework you can: 

Code with confidence: Run all your tests with a single command with Pytest. You no longer need to factor out your unit testable code or do extensive manual testing. Dealing with heavy end-to-end testing frameworks can be a thing of the past. 

Collaborate seamlessly: Build apps with your team without worrying about breaking existing workflows. By connecting Streamlit to tools like GitHub Actions you can build a continuous integration pipeline that automatically runs tests after each commit.

Feel more comfortable with complexity. Go beyond prototypes and build more powerful apps to take your data apps to the next level.

Simple, powerful, and all in Python. 💪

How AppTest works 

Now you can test each feature, interaction, or app logic headlessly via API. By headlessly, we mean that you can test the result without having to preview in the browser.  You can use the API reference docs to build out different scenarios you want to test. 

When you are ready, test everything with Pytest, locally and/or with GitHub Actions. View the results that will confirm that your features are all working correctly (or not). 

AppTest in action 

Watch the video below to take a tour of AppTest. In the demo video we will cover: 

Simple example - writing tests as you build your app
Reference API overview
Examples from Sophisticated Palette and LLM-Examples apps
Integrating with GitHub Actions
Ready, set, test! 

Start building and executing tests faster with AppTest to have more control over your app experiences. Check out the docs to get started. 

Show off what you have built! 

Share a link to tests you built for your Community Cloud app and show them successfully running in GitHub Actions. 

You can share an example by posting a link to your test file like this, and then share a successful run in a link like this. To get started setting up GitHub Actions, take a look at GitHub’s tutorial or use our sample workflow file.

If you provide your email in the comment with the two links, we will send the first 10 examples a Streamlit t-shirt! 

Happy Streamlit-ing 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Product...

View even more →

Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Joshua Carroll - Streamlit
https://blog.streamlit.io/author/joshua/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Joshua Carroll
4 posts
Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
LangChain 🤝 Streamlit

The initial integration of Streamlit with LangChain and our future plans

LLMs
by
Joshua Carroll
,
July 11 2023
Introducing st.connection!

Quickly and easily connect your app to data and APIs

Product
by
Joshua Carroll and 
1
 more,
May 2 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

7 ways GPT-4 with Vision can uplevel your Streamlit apps
https://blog.streamlit.io/7-ways-gpt-4-vision-can-uplevel-your-streamlit-apps/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

By Charly Wargnier
Posted in LLMs, November 15 2023
A brief history of multi-modality in AI
7 practical use cases for GPT-4 Vision
1. Sketch your app and watch it come to life
2. Turn any static chart into a dynamic data visualization
3. Convert tabular data from images into fully editable tables
4. Enhance your app's UX with tailored recommendations
5. Conquer LLM hallucinations
6. Debug any app, even when textual logs are missing
7. Document your apps fast
What will you build with GPT-4 Vision?
Contents
Share this post
← All posts

In my previous article, I explored how GPT-4 has transformed the way you can develop, debug, and optimize Streamlit apps.

With OpenAI’s latest advancements in multi-modality, imagine combining that power with visual understanding.

Now, you can use GPT-4 with Vision in your Streamlit apps to:

Build Streamlit apps from sketches and static images.
Help you refine your apps' user experience, including debugging and documentation.
Overcome LLMs limitations and hallucinations.

In this article, I'll walk you through 8 practical use cases that exemplify new possibilities using GPT-4 with Vision!

A brief history of multi-modality in AI

Before we dive into various use cases, it's important to lay some conceptual foundations for multimodality, discuss pioneering models, and explore currently available multi-modal models.

Multi-modal LLMs are an AI systems trained on multiple types of data such as text, images, and audio, as opposed to traditional models that focus on a single modality.

The journey towards multi-modality has seen significant strides over the recent years, with various models paving the way:

CLIP, the OG model introduced by OpenAI in 2021, emerged as a pioneering model capable of generalizing to multiple image classification tasks with zero and few-shot learning.
Flamingo, released in 2022, was notable for its strong performance in generating open-ended responses in a multimodal domain.
Salesforce's BLIP model was a framework for unified vision-language understanding and generation, enhancing performance across a range of vision-language tasks.

GPT-4 with Vision builds on pioneering models to advance the integration of visual and textual modalities. However, it's not the only multi-modal model vying for attention nowadays; Microsoft and Google are also gaining traction:

Microsoft's LLaVA, using a pre-trained CLIP visual encoder, offers similar performance to GPT-4 despite a smaller dataset.
Gemini is Google’s multimodal model, which stands out because it is fundamentally designed to be multimodal from the ground up.

Now, to the fun part!

7 practical use cases for GPT-4 Vision

💡
Pre-requisites:
1) You'll need a ChatGPT Plus subscription to access GPT-4 Vision.
2) If you’re new to Streamlit, follow the installation steps here.
1. Sketch your app and watch it come to life

… as my drawing skills are comparable to a cat chasing a laser pointer, I'll use Balsamiq to achieve that hand-drawn mockup feel.

This mockup consists of a two-panel UI. The left panel includes a logo, three navigation links, two checkboxes, and a dropdown. The right panel showcases the app name, a text area, two line charts, and a "Download Chart data" button:

Paste this mock-up image into the ChatGPT interface:

Include the following prompt:

Create a Streamlit app from this mock-up. You will use the Echarts library.
💡
It's good practice to specify the data visualization you would like to use. When it's not specified, the default will be matplotlib.

Each data visualization library will have to be installed via pip install into your virtual environment to work.

Check the results:

By simply uploading a mockup, Vision generated a fully functional Streamlit prototype: an app with a logo placeholder, navigation links, checkboxes, a combo box, some text, a chart, and a download button. 🙌

2. Turn any static chart into a dynamic data visualization

ChatGPT Vision doesn't just turn scribbles into fully functional Streamlit apps, it can also transform any static visual into a beautiful, interactive, dynamic data visualization.

Paste this screenshot of a Nightingale rose chart from the Echarts library:

Include this prompt:

Create a nightingale/rose chart in Streamlit. The chart should be a dynamic ECharts chart, via the streamlit-echarts library. Each section should be toggleable. The color scheme of the chart should match the one in the image.

Copy the code into your editor, and voilà!

The app displays a dynamic rose chart in Streamlit, with legends, toggleable petals/sections, and a color palette that is a faithful replica of the original!

3. Convert tabular data from images into fully editable tables

ChatGPT Vision is also incredibly useful when you need to extract data from a table that is not copyable nor downloadable.

In this example, we will ask Vision to make this image of tabular data fully editable using Streamlit’s data editor.

Paste this image of a ReactJS table into ChatGPT:

Include this prompt:

Code this table in Streamlit. We want the tabular data fully editable via Streamlit’s data editor.

Paste the code into your editor and review the results:

GPT-4 Vision crushed it!

The table is fully functional with the correct headers, each row is flawlessly reproduced, and the data is fully editable. As an added bonus, ChatGPT includes a function to find the highest sale amount and display the associated order!

4. Enhance your app's UX with tailored recommendations

GPT-4 Vision can also help you improve your app's UX and ease the design process for multi-page apps.

Paste a screenshot of complex dashboard app into ChatGPT.

Include this prompt:

Provide 8 suggestions to enhance the usability of this Streamlit app.

ChatGPT's recommendations are pretty spot on!

Group related controls into visually distinct sections.
Standardize the UI's color scheme.
Implement interactive tooltips on various sections and controls.
Increase the font size of axis labels and graph titles.
Highlight the final decision output with a colored background.
Incorporate a feedback mechanism.
Include a legend for multi-color or line graphs.
Ensure consistent spacing and alignment between graphs and sections.
5. Conquer LLM hallucinations

There's no doubt that GPT-4 is a significant improvement over its predecessors.

Like all LLMs, it can produce misleading or fictional outputs, known as hallucinations. This can be due to biases in the training data, unclear prompts, or the fact that GPT-4 may not include the most up-to-date data.

This is when Retrieval Augmented Generation (or RAG) comes into play. RAG is a technique that improves chatbots by incorporating external data, ensuring more relevant and up-to-date responses.

For example, GPT-4 is not aware of Streamlit's new colorful headers, as they were not available when it was trained.

We'll start by pasting a screenshot of the new st.header() documentation, which includes our new API parameter for coloring headers:

Include this prompt:

Build a Streamlit app featuring various st.header() in different colors, using the new divider argument.Include a brief humorous data science pun for each header.Add a corresponding emoji to each header.

Let's look at the results:

Vision did a great job of displaying the rainbow header seamlessly.
💡
Fun fact: uploading documentation screenshots with Vision resulted in better chat conversations than uploading PDFs through the Code Interpreter.
6. Debug any app, even when textual logs are missing

As a developer advocate for Streamlit, I spend a big part of my time on forums helping our community users debug their Streamlit apps.

While GPT-4 is an incredibly effective tool for quickly reviewing error logs to find the source of a bug, sometimes, users cannot provide error log traces for various reasons:

The log trace may contain private data that cannot be shared
The user may not be able to access the log trace at a specific time.

We may only be given a screenshot of the error callout from the Streamlit front-end, such as the one below:

This can make it difficult to debug the issue, as we do not have access to the full log trace.

Fear not! ChatGPT Vision can still assist you by providing useful debugging hints, by extracting relevant information from the screenshot.

Paste the above image with the following prompt:

Give me a clue on the error.

Let’s review ChatGPT's answer:

Verdict ✅
Even though ChatGPT Vision only had access to a partly displayed screenshot of the error and did not have the full textual log trace, it was still able to infer the full error and retrieve the correct answer.

7. Document your apps fast

Once you build your web app, it needs clear documentation to help users get started, understand its features, and learn how to use it. Writing documentation can be time-consuming, but ChatGPT Vision can help streamline the process.

Simply provide a snapshot of your app, and ChatGPT Vision will generate tailored descriptive content that you can use in a document, README, social post, or anywhere else you need it. This not only saves time, but it also ensures that all of the visual details of your app are captured and explained.

Paste a screenshot of my CodeLlama Playground app:

Add this prompt:

Analyze the image I've uploaded, which displays my CodeLlama Playground app. Create a README about it, in Markdown syntax. Add a prerequisite on how to install Streamlit (either locally or on Streamlit Community Cloud).
💡
ChatGPT Vision can only infer information from from the elements present in a given UI screenshot. Thus, for documentation purposes, it is always recommended to:

1) Display all pages in a multi-page app
2) Aid the inference by including any additional descriptive elements (in image or text) if the app's layout lacks sufficient detail.

Let's look at the generated README from markdown:

In a matter of seconds, by merely examining the app's UI, ChatGPT Vision generated a ready-to-use README for my CodeLlama Playground app. It accurately listed its features, provided installation instructions for Streamlit both locally and via the Cloud, and offered a quick start guide to launch the app. 🤯

What will you build with GPT-4 Vision?

The OpenAI Vision API also opens up new possibilities and creative combinations. At the time of this writing, GPT-4 with vision is currently only available to developers with access to GPT-4 via the gpt-4-vision-preview.

Until it becomes available world-wide, check out the art of the possible with some creations from the Streamlit community:

Try out UI Auditor, from Streamlit community member, Kartik. Upload a screenshot of your app's UI, and GPT will tell you how to improve it 🤖
In this app tease from our Streamlit Creator, Avra, you can upload screenshots (in this case, from scientific publications) to get spot-on analyses.
Peter Wang, another Streamlit Creator, built a image-to-text-to-speech app to commentate a League of Legends game!

Let your imagination run wild with your prompts, and share what you discover in the comments below!

Also, keep an eye on my Twitter/X feed, where I regularly share cool LLM use cases.

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in LLMs...

View even more →

Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Charly Wargnier - Streamlit
https://blog.streamlit.io/author/charly/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Charly Wargnier
4 posts
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How in-app feedback can increase your chatbot’s performance

A guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith

Tutorials
by
Charly Wargnier
,
October 6 2023
How to enhance Google Search Console data exports with Streamlit

Connect to the GSC API in one click and go beyond the 1,000-row UI limit!

Tutorials
by
Charly Wargnier
,
July 28 2022
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Building a dashboard in Python using Streamlit
https://blog.streamlit.io/crafting-a-dashboard-app-in-python-using-streamlit/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

By Chanin Nantasenamat
Posted in Tutorials, January 22 2024
What’s inside the dashboard?
1. Define key metrics
1.1 Overview of key metrics
1.2 Key metrics selected for this app
2. Perform EDA analysis
2.1 What data is available to you?
2.2 Prepare the data
2.3 Select charts to best visualize our key metrics
3. Build your dashboard with Streamlit
3.1 Import libraries
3.2 Page configuration
3.3 Apply CSS styling
3.4 Load data
3.5 Add a sidebar
3.6 Plot and chart types
3.7 App layout
3.8 Deploying the Dashboard app to the cloud
BONUS: 5 reminders when building dashboards
Wrapping up
Contents
Share this post
← All posts

You may be familiar with the phrase “A picture is worth a thousand words” and, in the context of data science, a visualized plot provides just as much value. It does this by providing a different look at tabular data, perhaps in the form of simple line charts, histogram distribution and more elaborate pivot charts.

As useful as these can be, a typical chart that we may see in print or on the web are most likely static. Imagine how much more engaging it would be to manipulate these static variables in an interactive dashboard?

🏂
Ready to jump right in? Here’s the dashboard app and GitHub repo.

In this blog, you’ll learn how to build a Population Dashboard app that displays data and visualizations of the US population for the years 2010-2019 as obtained from the US Census Bureau.

I'll guide you through the process of building this interactive dashboard app from scratch using Streamlit for the frontend. Our backend muscle comes from PyData heavyweights like NumPy, Pandas, Scikit-Learn, and Altair, ensuring robust data processing and analytics.

You’ll learn how to:

Define key metrics
Perform EDA analysis
Build the dashboard app with Streamlit
What’s inside the dashboard?

Here’s a visual breakdown of the components that make up this population dashboard:

Let’s get started!

1. Define key metrics

Before we dive into actually building the dashboard, we need to first come up with well-defined metrics to measure what matters.

1.1 Overview of key metrics

The goal of any dashboard is to surface insights that provide the basis for data-driven decision making. What is the primary purpose of the dashboard? This will guide the subsequent questions you want the dashboard to answer in the form of key metrics.

For example:

In sales, the primary goal may be to understand: “How are sales teams performing?” Metrics may include total revenue by sales rep, units sold by territory, or new leads generated over time.
In marketing, the primary goal may be to understand “How is my campaign performing?” this may include measuring leading indicators such as response rates or click-through rate, and lagging indicators such as revenue conversion rate or customer acquisition costs.
In finance, the dashboard may need to answer “How profitable is our business?” this might include gross profit, operating margin, and return on assets.
1.2 Key metrics selected for this app

The primary question this population dashboard aims to answer is: how do US state populations change over time?

What questions do we need to ask that will help us answer this dashboard goal?

How do total populations compare among different states?
How do state populations evolve over time and how do they compare to each other?
In a given year, which states experienced more than 50,000 people moving in, or out? We'll label these inbound and outbound migration metrics.
2. Perform EDA analysis

Once we have our key metrics, we will then need to collect and gain a solid understanding about the data available before we can present it in a visually aesthetic way in our dashboard.

Exploratory data analysis (EDA) can be defined as an iterative process for data understanding that entails asking and answering questions through investigative work of analyzing the data. In essence, your dashboard starts out as a blank canvas and EDA provides a pragmatic approach for coming up with compelling data visuals that tells a story.

John Tukey's seminal work on EDA in 1977 meticulously sets the stage for effective data communication. Here are some notable key takeaways:

“The greatest value of a graph is when it forces us to see what we never expected.” In fact Tukey, introduced the Box and whisker plot (aka box plots).
Having a flexible and open mindset when approaching data, hence the “exploratory” nature of EDA.
2.1 What data is available to you?

Here’s a sample of the dataset from the US Census Bureau we’re using for our population dashboard. There are 3 potential variables (states, year and population) that will serve as the basis for our metrics.

states, states_code, id, year, population
Alabama, AL, 1, 2010, 4785437
Alaska, AK, 2, 2010, 713910
Arizona, AZ, 4, 2010, 6407172
Arkansas, AR, 5, 2010, 2921964
California, CA, 6, 2010, 37319502

2.2 Prepare the data

Consolidate the year columns into a single unified column.

The advantage of subsetting the data by year, will provide the necessary format for generating possible visualizations (e.g. a heatmap, choropleth map, etc.) and a sortable dataframe.

2.3 Select charts to best visualize our key metrics

Now that we have a better understanding of the data at our fingertips, and the key metrics to measure, it’s time to decide how to visualize the results on our dashboard. There are countless ways to visualize your datasets, here’s what was selected for our population dashboard app.

What is the comparison of total populations among different states?
A choropleth map adds a geospatial dimension to highlight the most and least populated states.
How do populations of different states evolve over time and how do they compare to each other?
A heatmap offers a comprehensive overview of states with the highest and lowest population values by presenting this information across different years
Sorting the dataframe provides a quick and direct comparison of the most and least populated states, thereby eliminating the need to wander through different sections of the charts.
In a given year, what percent of all states experience inbound/outbound migration >50,000 people?
A donut chart is a pie chart with an empty inner arc and we’re using this to visualize the percentage of inbound and outbound state migration.

There are countless ways to visualize your datasets!

You can discover even more visualization options from the growing collection of custom components that the community. Here’s a few that you can try out:

streamlit-extras affords a wide range of widgets that extends the native functionality of Streamlit.
streamlit-shadcn-ui provides several UI frontend components (modal, hovercard, badges, etc.) that can be incorporated into the dashboard app.
streamlit-element allows the creation of draggable and resizable dashboard components.
3. Build your dashboard with Streamlit
💡
Here’s the dashboard app and the GitHub repo.
3.1 Import libraries

First, we’ll start by importing the prerequisite libraries:

Streamlit - a low-code web framework
Pandas - a data analysis and wrangling tool
Altair - a data visualization library
Plotly Express - a terse and high-level API for creating figures

import streamlit as st
import pandas as pd
import altair as alt
import plotly.express as px

3.2 Page configuration

Next, we’ll define settings for the app by giving it a page title and icon that are displayed on the browser. This also defines the page content to be displayed in a wide layout that fits the page’s width as well as showing the sidebar in the expanded state.

Here, we also set the color theme for the Altair plot to be dark in order to accompany dark color theme of the app.

st.set_page_config(
    page_title="US Population Dashboard",
    page_icon="🏂",
    layout="wide",
    initial_sidebar_state="expanded")

alt.themes.enable("dark")

3.3 Apply CSS styling

To obtain the desired visual display of the page elements, we’re using a little CSS magic to center the text and icons of the metrics widget (via the use of data-testid of stMetric, stMetricLabel, stMetricDeltaIcon-Up, and stMetricDeltaIcon-Down), to center the page contents (via the use of data-testid of block-container), and to reduce the sidebar padding (via the use of data-testid of stVerticalBlock).

st.markdown("""
<style>

[data-testid="block-container"] {
    padding-left: 2rem;
    padding-right: 2rem;
    padding-top: 1rem;
    padding-bottom: 0rem;
    margin-bottom: -7rem;
}

[data-testid="stVerticalBlock"] {
    padding-left: 0rem;
    padding-right: 0rem;
}

[data-testid="stMetric"] {
    background-color: #393939;
    text-align: center;
    padding: 15px 0;
}

[data-testid="stMetricLabel"] {
  display: flex;
  justify-content: center;
  align-items: center;
}

[data-testid="stMetricDeltaIcon-Up"] {
    position: relative;
    left: 38%;
    -webkit-transform: translateX(-50%);
    -ms-transform: translateX(-50%);
    transform: translateX(-50%);
}

[data-testid="stMetricDeltaIcon-Down"] {
    position: relative;
    left: 38%;
    -webkit-transform: translateX(-50%);
    -ms-transform: translateX(-50%);
    transform: translateX(-50%);
}

</style>
""", unsafe_allow_html=True)

3.4 Load data

Next, we’ll load data into the app using Pandas’ read_csv() function as follows:

df_reshaped = pd.read_csv('data/us-population-2010-2019-reshaped.csv')

3.5 Add a sidebar

We’re now going to create the app title via st.title() and create drop-down widgets for allowing users to select the specific year and color theme via st.selectbox().

The selected_year (from the available years from 2010-2019) will then be used to subset the data for that year, which is then displayed in-app.

The selected_color_theme will allow the choropleth map and heatmap to be colored according to the selected color specified by the aforementioned widget.

with st.sidebar:
    st.title('🏂 US Population Dashboard')
    
    year_list = list(df_reshaped.year.unique())[::-1]
    
    selected_year = st.selectbox('Select a year', year_list, index=len(year_list)-1)
    df_selected_year = df_reshaped[df_reshaped.year == selected_year]
    df_selected_year_sorted = df_selected_year.sort_values(by="population", ascending=False)

    color_theme_list = ['blues', 'cividis', 'greens', 'inferno', 'magma', 'plasma', 'reds', 'rainbow', 'turbo', 'viridis']
    selected_color_theme = st.selectbox('Select a color theme', color_theme_list)

3.6 Plot and chart types

Next, we’re going to define custom functions to create the various plots displayed in the dashboard.

Heatmap

A heatmap will allow us to see the population growth over the years from 2010-2019 for the 52 states.

Heatmap of the US population for 2010-2019.
def make_heatmap(input_df, input_y, input_x, input_color, input_color_theme):
    heatmap = alt.Chart(input_df).mark_rect().encode(
            y=alt.Y(f'{input_y}:O', axis=alt.Axis(title="Year", titleFontSize=18, titlePadding=15, titleFontWeight=900, labelAngle=0)),
            x=alt.X(f'{input_x}:O', axis=alt.Axis(title="", titleFontSize=18, titlePadding=15, titleFontWeight=900)),
            color=alt.Color(f'max({input_color}):Q',
                             legend=None,
                             scale=alt.Scale(scheme=input_color_theme)),
            stroke=alt.value('black'),
            strokeWidth=alt.value(0.25),
        ).properties(width=900
        ).configure_axis(
        labelFontSize=12,
        titleFontSize=12
        ) 
    # height=300
    return heatmap


Choropleth map

Next, a colored map of the 52 US states for the selected year is depicted by the choropleth map.

Choropleth map of the US population for a selected year of interest (2019 in this case).
def make_choropleth(input_df, input_id, input_column, input_color_theme):
    choropleth = px.choropleth(input_df, locations=input_id, color=input_column, locationmode="USA-states",
                               color_continuous_scale=input_color_theme,
                               range_color=(0, max(df_selected_year.population)),
                               scope="usa",
                               labels={'population':'Population'}
                              )
    choropleth.update_layout(
        template='plotly_dark',
        plot_bgcolor='rgba(0, 0, 0, 0)',
        paper_bgcolor='rgba(0, 0, 0, 0)',
        margin=dict(l=0, r=0, t=0, b=0),
        height=350
    )
    return choropleth


Donut chart

Next, we’re going to create a donut chart for the states migration in percentage.

Donut chart of the percentage of states with annual inbound and outbound migration greater than 50,000 in 2019.

Particularly, this represents the percentage of states with annual inbound or outbound migration > 50,000 people. For instance, in 2019, there were 12 out of 52 states and this corresponds to 23%.

Before creating the donut chart, we’ll need to calculate the year-over-year population migrations.

def calculate_population_difference(input_df, input_year):
  selected_year_data = input_df[input_df['year'] == input_year].reset_index()
  previous_year_data = input_df[input_df['year'] == input_year - 1].reset_index()
  selected_year_data['population_difference'] = selected_year_data.population.sub(previous_year_data.population, fill_value=0)
  return pd.concat([selected_year_data.states, selected_year_data.id, selected_year_data.population, selected_year_data.population_difference], axis=1).sort_values(by="population_difference", ascending=False)


The donut chart is then created from the aforementioned percentage value for states migration.

def make_donut(input_response, input_text, input_color):
  if input_color == 'blue':
      chart_color = ['#29b5e8', '#155F7A']
  if input_color == 'green':
      chart_color = ['#27AE60', '#12783D']
  if input_color == 'orange':
      chart_color = ['#F39C12', '#875A12']
  if input_color == 'red':
      chart_color = ['#E74C3C', '#781F16']
    
  source = pd.DataFrame({
      "Topic": ['', input_text],
      "% value": [100-input_response, input_response]
  })
  source_bg = pd.DataFrame({
      "Topic": ['', input_text],
      "% value": [100, 0]
  })
    
  plot = alt.Chart(source).mark_arc(innerRadius=45, cornerRadius=25).encode(
      theta="% value",
      color= alt.Color("Topic:N",
                      scale=alt.Scale(
                          #domain=['A', 'B'],
                          domain=[input_text, ''],
                          # range=['#29b5e8', '#155F7A']),  # 31333F
                          range=chart_color),
                      legend=None),
  ).properties(width=130, height=130)
    
  text = plot.mark_text(align='center', color="#29b5e8", font="Lato", fontSize=32, fontWeight=700, fontStyle="italic").encode(text=alt.value(f'{input_response} %'))
  plot_bg = alt.Chart(source_bg).mark_arc(innerRadius=45, cornerRadius=20).encode(
      theta="% value",
      color= alt.Color("Topic:N",
                      scale=alt.Scale(
                          # domain=['A', 'B'],
                          domain=[input_text, ''],
                          range=chart_color),  # 31333F
                      legend=None),
  ).properties(width=130, height=130)
  return plot_bg + plot + text


Convert population to text

Next, we’ll going to create a custom function for making population values more concise as well as improving the aesthetics. Particularly, instead of being displayed as numerical values of 28,995,881 in the metrics card to a more concised form as 29.0 M. This was also applied to numerical values in the thousand range.

Metrics cards showing states with high inbound/outbound migration in the selected year of interest (2019 in this case).
def format_number(num):
    if num > 1000000:
        if not num % 1000000:
            return f'{num // 1000000} M'
        return f'{round(num / 1000000, 1)} M'
    return f'{num // 1000} K'

3.7 App layout

Finally, it’s time to put everything together in the app.

Define the layout

Begin by creating 3 columns:

col = st.columns((1.5, 4.5, 2), gap='medium')


Particularly, the input argument (1.5, 4.5, 2) indicated that the second column has a width of about three times that of the first column and that the third column has a width about twice less than that of the second column.

Column 1

The Gain/Loss section is shown where metrics card are displaying states with the highest inbound and outbound migration for the selected year (specified via the Select a year drop-down widget created via st.selectbox).

The States migration section shows a donut chart where the percentage of states with annual inbound or outbound migration > 50,000 are displayed.

with col[0]:
    st.markdown('#### Gains/Losses')

    df_population_difference_sorted = calculate_population_difference(df_reshaped, selected_year)

    if selected_year > 2010:
        first_state_name = df_population_difference_sorted.states.iloc[0]
        first_state_population = format_number(df_population_difference_sorted.population.iloc[0])
        first_state_delta = format_number(df_population_difference_sorted.population_difference.iloc[0])
    else:
        first_state_name = '-'
        first_state_population = '-'
        first_state_delta = ''
    st.metric(label=first_state_name, value=first_state_population, delta=first_state_delta)

    if selected_year > 2010:
        last_state_name = df_population_difference_sorted.states.iloc[-1]
        last_state_population = format_number(df_population_difference_sorted.population.iloc[-1])   
        last_state_delta = format_number(df_population_difference_sorted.population_difference.iloc[-1])   
    else:
        last_state_name = '-'
        last_state_population = '-'
        last_state_delta = ''
    st.metric(label=last_state_name, value=last_state_population, delta=last_state_delta)

    
    st.markdown('#### States Migration')

    if selected_year > 2010:
        # Filter states with population difference > 50000
        # df_greater_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference_absolute > 50000]
        df_greater_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference > 50000]
        df_less_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference < -50000]
        
        # % of States with population difference > 50000
        states_migration_greater = round((len(df_greater_50000)/df_population_difference_sorted.states.nunique())*100)
        states_migration_less = round((len(df_less_50000)/df_population_difference_sorted.states.nunique())*100)
        donut_chart_greater = make_donut(states_migration_greater, 'Inbound Migration', 'green')
        donut_chart_less = make_donut(states_migration_less, 'Outbound Migration', 'red')
    else:
        states_migration_greater = 0
        states_migration_less = 0
        donut_chart_greater = make_donut(states_migration_greater, 'Inbound Migration', 'green')
        donut_chart_less = make_donut(states_migration_less, 'Outbound Migration', 'red')

    migrations_col = st.columns((0.2, 1, 0.2))
    with migrations_col[1]:
        st.write('Inbound')
        st.altair_chart(donut_chart_greater)
        st.write('Outbound')
        st.altair_chart(donut_chart_less)


Column 2

Next, the second column displays the choropleth map and heatmap using custom functions created earlier.

with col[1]:
    st.markdown('#### Total Population')
    
    choropleth = make_choropleth(df_selected_year, 'states_code', 'population', selected_color_theme)
    st.plotly_chart(choropleth, use_container_width=True)
    
    heatmap = make_heatmap(df_reshaped, 'year', 'states', 'population', selected_color_theme)
    st.altair_chart(heatmap, use_container_width=True)


Column 3

Finally, the third column shows the top states via a dataframe whereby the population are shown as a colored progress bar via the column_config parameter of st.dataframe.

An About section is displayed via the st.expander() container to provide information on the data source and definitions for terminologies used in the dashboard.

with col[2]:
    st.markdown('#### Top States')

    st.dataframe(df_selected_year_sorted,
                 column_order=("states", "population"),
                 hide_index=True,
                 width=None,
                 column_config={
                    "states": st.column_config.TextColumn(
                        "States",
                    ),
                    "population": st.column_config.ProgressColumn(
                        "Population",
                        format="%f",
                        min_value=0,
                        max_value=max(df_selected_year_sorted.population),
                     )}
                 )
    
    with st.expander('About', expanded=True):
        st.write('''
            - Data: [U.S. Census Bureau](<https://www.census.gov/data/datasets/time-series/demo/popest/2010s-state-total.html>).
            - :orange[**Gains/Losses**]: states with high inbound/ outbound migration for selected year
            - :orange[**States Migration**]: percentage of states with annual inbound/ outbound migration > 50,000
            ''')

3.8 Deploying the Dashboard app to the cloud

For a video walkthrough on deploying a Streamlit app, check out this tutorial on YouTube.

BONUS: 5 reminders when building dashboards
Perform EDA to gain data understanding
Identify key metrics for tracking what matters
Decide on charts to best visualize key metrics
Group related metrics together
Use clear and concise labels to describe metrics
Wrapping up

In summary, Streamlit offers a quick, efficient, and code-friendly way to build interactive dashboard apps in Python, making it a go-to tool for data scientists and developers working with data visualization.

One of the key features of Streamlit is its ability to automatically update and re-render the app based on incremental changes in the data or input parameters, which makes it highly suitable for real-time data visualization tasks.

Check out this tutorial video to follow along:

What dashboards are you building? In the comments below, share your dashboard below to inspire the community, or ask for feedback!

Follow me on X at @thedataprof, on LinkedIn at Chanin Nantasenamat, or subscribe to my Data Professor channel on Youtube!

Happy Streamlit-ing! 📊

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Also in Tutorials...

View even more →

Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Avthar Sewrathan - Streamlit
https://blog.streamlit.io/author/avthar/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Avthar Sewrathan
1 post
Website
Twitter
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Using time-based RAG in LLM apps
https://blog.streamlit.io/using-time-based-rag-llm-apps-with-timescale-vector/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

By Avthar Sewrathan
Posted in LLMs, December 15 2023
Before vs. after time-aware retrieval
Tutorial: Build a chatbot about GitHub commit history
Overview of TSV Time Machine app
Part 1: Load time-based data with Timescale Vector and LlamaIndex
Part 2: Build the chatbot
Deployment
☁️ On Streamlit Community Cloud
💻 On your local machine
How will you use time-based retrieval?
Wrapping up
Contents
Share this post
← All posts

Retrieval Augmented Generation (RAG) is essential for enhancing large language models (LLMs) in app development. It supplements LLMs with external data sources, helping arrive at more relevant responses by reducing errors or hallucinations. RAG determines what info is relevant to the user’s query through semantic search, which searches data by meaning (rather than just looking for literal matches of search terms). RAG is particularly effective for LLM apps that need to access domain-specific or proprietary data.

However, RAG alone isn’t always enough to build powerful, context-aware LLM apps. Enhancing RAG with time-aware retrieval, which finds semantically relevant vectors within specific time and date ranges, can significantly improve its effectiveness.

Before vs. after time-aware retrieval

When dealing with a date-heavy knowledge base, time-aware RAG can help you build LLM apps that excel at generating relevant answers to user queries.

In this example, see how time-aware retrieval improves the quality of LLM responses:

Alice is a developer that wants to learn about specific changes to a GitHub repo (in this case, the TimescaleDB repo). Rather than manually inspect the code or commit history, she finds a “Chat with my GitHub repo” application and uses it to expedite her research. Alice wants to learn about recent changes to a feature named “continuous aggregates” and asks the following question on 29 November 2023:

What's new with the continuous aggregates feature in the past 6 months?

LLM response without time-aware RAG 👎

The feature named continuous aggregates has undergone some changes. The most recent change was made by Sven Klemm on January 24, 2020. This change added a new mode called real-time aggregates, which combines materialized data with new data received after the last refresh. This new mode is now the default behavior for newly created continuous aggregates. To upgrade existing continuous aggregates to the new behavior, a specific command needs to be run.

LLM response with time-aware RAG ✅

In the past 6 months, there have been two changes made to the feature named continuous aggregates. The first change was made on August 7, 2023, by Fabrízio de Royes Mello. This change relaxed the strong table lock when refreshing a continuous aggregate, allowing the refresh procedure to be executed in multiple sessions with fewer locks. The second change was made on August 29, 2023, by Jan Nidzwetzki. This change made the up/downgrade test deterministic by adding an ORDER BY specification to two queries in post.continuous_aggs.v3.sql.

The response using time-aware RAG is much more useful — it’s within Alice’s specified time period and is topically relevant. The difference between the two responses is the retrieval step.

Tutorial: Build a chatbot about GitHub commit history

Now it's your turn! In the rest of this post, we'll walk through the process of building TSV Time Machine: a time-aware RAG chatbot that enables you to go back in time and chat with the commit history of any GitHub repository.

Each Git commit has an associated timestamp, a natural language message, and other metadata, meaning that both semantic and time-based search are needed to answer questions about the commit history.

👩‍💻
Want to jump right in? Check out the app and the code.
Screenshot of the TSV Time Machine app, showing a user chatting with the PostgreSQL project GitHub commit history.
Overview of TSV Time Machine app

To power TSV Time Machine, we use the following:

LlamaIndex is a powerful LLM data framework for RAG applications. LlamaIndex ingests, processes, and retrieves data. We’ll use the LlamaIndex autoretriever to infer the right query to run on the vector database, including both the query string and metadata filters.
Timescale Vector is our vector database. Timescale Vector has optimizations for similarity and time-based search, making it ideal to power the time-aware RAG. It does this through automatic table partitioning to isolate data for particular time ranges. We will access it through LlamaIndex’s Timescale Vector Store.
🎉
Streamlit Users Get 3 Months of Timescale Vector for Free! Timescale’s cloud hosted vector databases make it easy to test and develop your Streamlit applications.

The TSV Time Machine sample app has three pages:

Home: homepage of the app that provides instructions on how to use the app.
Load Data: page to load the Git commit history of the repo of your choice.
Time Machine Demo: interface with chat with any of the GitHub repositories loaded.

Since the app is ~600 lines of code, we won’t unpack it line by line (although you can ask ChatGPT to explain any tricky parts to you!). Let’s take a look at the key code snippets involved in:

Loading data from the GitHub repo you want to chat with
Powering chat via time-aware retrieval augmented generation
Part 1: Load time-based data with Timescale Vector and LlamaIndex

Input the URL of a GitHub repo you want to load data for and TSV Time Machine uses LlamaIndex to load the data, create vector embeddings for it, and store it in Timescale Vector.

In the file 0_LoadData.py , we fetch data from a GitHub repository of your choice, create embeddings for it using OpenAI’s text-embedding-ada-002 model and LlamaIndex, and store it in tables in Timescale Vector. The tables contain the vector embedding, the original text, and metadata associated with the Git commit, including a UUID which reflects the timestamp of the commit.

First, we define a load_git_history() function. This function will ask the user to input the GitHub repo URL, branch, and number of commits to load via a st.text_input element. Then it will will fetch the Git commit history for the repo, use LlamaIndex to embed the commit history text and turn them into LlamaIndex nodes, and insert the embeddings and metadata into Timescale Vector:

# Load git history into the database using LlamaIndex
def load_git_history():
   repo = st.text_input("Repo", "<https://github.com/postgres/postgres>")
   branch = st.text_input("Branch", "master")
   limit = int(st.text_input("Limit number commits (0 for no limit)", "1000"))
   if st.button("Load data into the database"):
       df = get_history(repo, branch, limit)
       table_name = record_catalog_info(repo)
       load_into_db(table_name, df)


Function for loading Git history from a user defined URL. Defaults to the PostgreSQL project.

While the full code for the helper functions get_history(), record_catalog_info(), and load_into_db() is in the sample app repo, here’s an overview:

get_history(): fetches the repo’s Git history and stores it in a Pandas DataFrame. We fetch the commit hash, author name, date of commit, and commit message.
record_catalog_info(): creates a relational table in our Timescale Vector database to store the information of the loaded GitHub repositories. The repo URL and the name of the table commits are stored in the database.
load_into_db(): creates a TimescaleVectorStore in LlamaIndex to store our embeddings and metadata for the commit data.
We set the time_partition_interval parameter to 365 days. This parameter represents the length of each interval for partitioning the data by time. Each partition will consist of data for the specified length of time.
# Create Timescale Vectorstore with partition interval of 1 year
   ts_vector_store = TimescaleVectorStore.from_params(
       service_url=st.secrets["TIMESCALE_SERVICE_URL"],
       table_name=table_name,
       time_partition_interval=timedelta(days=365),
   )
💡
Choosing the right partition interval
This example uses 365 days as the partition interval, but pick the value that makes sense for your app’s queries.

For example, if you frequently query recent vectors, use a smaller time interval (e.g. one day). If you query vectors over a decade-long time period, use a larger interval (e.g. six months or one year).

Most queries should touch only a couple of partitions and your full dataset should fit within 1,000 partitions.

Once we’ve created our TimescaleVectorStore, we create LlamaIndex TextNodes for each commit and create embeddings for the content of each node in batches.

create_uuid() creates a UUID v1 from the commit’s datetime associated with the node and vector embedding. This UUID enables us to efficiently store the nodes in partitions by time and query embeddings according to their partition.

# Create UUID based on time
def create_uuid(date_string: str):
   datetime_obj = datetime.fromisoformat(date_string)
   uuid = client.uuid_from_time(datetime_obj)
   return str(uuid)

Creating a UUID v1 with a time component helps power similarity search on time. We use the uuid_from_time() function found in the Timescale Vector Python client to help us.

The Load Data page showing the progress of loading and vectoring data from a user specified GitHub URL.

Finally, we create a TimescaleVectorIndex, which will allow us to do fast similarity search and time-based search for time-aware RAG. We use st.spinner and st.progress to show load progress.

st.spinner("Creating the index...")
progress = st.progress(0, "Creating the index")
start = time.time()
ts_vector_store.create_index()
duration = time.time()-start
progress.progress(100, f"Creating the index took {duration} seconds")
st.success("Done")
Part 2: Build the chatbot

In the file 1_TimeMachineDemo.py, we use LlamaIndex’s auto-retriever to answer user questions by fetching data from Timescale Vector to use as context for GPT-4.

Time Machine Demo page showing a sample user conversation.

Here’s an overview of the key functions:

get_repos(): Fetches list of available GitHub repos you’ve loaded that you can chat with, so you can easily switch between them in the side bar.
get_autoretriever(): Creates a LlamaIndex auto-retriever from the TimescaleVectorStore, which gives GPT-4 the ability to form vector store queries with metadata filters. This enables the LLM to limit answers to user queries to a specific timeframe.
For example, the query: “What new features were released in the past 6 months” will only search partitions in Timescale Vector that contain data between now and 6 months ago, and fetch the most relevant vectors to be used as context for RAG.
# Creates a LlamaIndex auto-retriever interface with the TimescaleVector database
def get_auto_retriever(index, retriever_args):
   vector_store_info = VectorStoreInfo(
       # Note: Modify this to match the metadata of your data
       content_info="Description of the commits to PostgreSQL. Describes changes made to Postgres",
       metadata_info=[
           MetadataInfo(
               name="commit_hash",
               type="str",
               description="Commit Hash",
           ),
           MetadataInfo(
               name="author",
               type="str",
               description="Author of the commit",
           ),
           MetadataInfo(
               name="__start_date",
               type="datetime in iso format",
               description="All results will be after this datetime",
  
           ),
           MetadataInfo(
               name="__end_date",
               type="datetime in iso format",
               description="All results will be before this datetime",
  
           )
       ],
   )
   retriever = VectorIndexAutoRetriever(index,
                                      vector_store_info=vector_store_info,
                                      service_context=index.service_context,
                                      **retriever_args)
  
   # build query engine
   query_engine = RetrieverQueryEngine.from_args(
       retriever=retriever, service_context=index.service_context
   )
   
   # convert query engine to tool
   query_engine_tool = QueryEngineTool.from_defaults(query_engine=query_engine)

   chat_engine = OpenAIAgent.from_tools(
       tools=[query_engine_tool],
       llm=index.service_context.llm,
       verbose=True,
       service_context=index.service_context
   )
   return chat_engine
vector_store_info provides the LLM with info about the metadata so that it can create valid filters for fetching data in response to user questions. If you’re using your own data (different from Git commit histories), you’ll need to modify this method to match your metadata.
__start_date and __end_date are special filter names used by Timescale Vector to support time-based search. If you’ve enabled time partitioning in Timescale Vector (by specifying the time_partition_interval argument when creating the TimescaleVectorStore), you can specify these fields in the VectorStoreInfo to enable the LLM to perform time-based search on each LlamaIndex Node’s UUID.
The chat_engine returned is an OpenAIAgent which can use the QueryEngine tool to perform tasks — in this case, answer questions about GitHub repo commits.
tm_demo() handles the chat interaction between the user and LLM. It provides an st.slider element for the user to specify the time period and number of commits to fetch. It then prompts the user for input, processes that input using get_autoretriever(), and displays the chat messages. Check out this method in the GitHub repo.
Deployment
☁️ On Streamlit Community Cloud
Fork and clone this repository.
Create a new cloud PostgreSQL database with Timescale Vector (sign up for an account here). Download the cheatsheet or .env file containing the database connection string.
Create a new OpenAI API key to use in this project, or follow these instructions to sign up for an OpenAI developer account to obtain one. We’ll use OpenAI’s embedding model to generate text embeddings and GPT-4 as the LLM to power our chat engine.
In Streamlit Community Cloud:
Click New app, and pick the appropriate repository, branch, and file path.
Click Advanced Settings and set the following secrets:
OPENAI_API_KEY=”YOUR_OPENAI_API KEY”

TIMESCALE_SERVICE_URL=”YOUR_TIMESCALE_SERVICE_URL”

ENABLE_LOAD=1

Hit Deploy.

And you’re off to the races!

💻 On your local machine
Create a new folder for your project, then follow steps 1-3 above.
Install dependencies. Navigate to the tsv-timemachine directory and run the following command in your terminal, which will install the python libraries needed:
pip install -r requirements.txt

In the tsv-timemachine directory, create a new .streamlit folder and create a secrets.toml file that includes the following:
OPENAI_API_KEY=”YOUR_OPENAI_API KEY”

TIMESCALE_SERVICE_URL=”YOUR_TIMESCALE_SERVICE_URL”

ENABLE_LOAD=1


Example TOML file with Streamlit secrets. You’ll need to set these to embed and store data using OpenAI and Timescale Vector.

To run the application locally, enter the following in the command line:
streamlit run Home.py


🎉 Congrats! Now you can load and chat with the Git commit history of any repo, using LlamaIndex as the data framework, and Timescale Vector as the vector database.

The Git commit history can be substituted for any time-based data of your choice. The result is an application that can efficiently perform RAG on time-based data and answer user questions with data from specific time periods.

How will you use time-based retrieval?

Building on the example above, here are a few examples of use cases unlocked by time-aware RAG:

Similarity search within a time range: filter documents by create date, publish date, or update date when chatting with a corpus of documents.
Find the most recent embeddings: find the most relevant and recent news or social media posts about a specific topic.
Give LLMs a sense of time: leverage tools like LangChain’s self-query retriever or LlamaIndex’s auto-retriever to ask time-based and semantic questions about a knowledge base of documents.
Search and retrieve chat history: search a user’s prior conversations to retrieve details relevant to the current chat.
Wrapping up

You learned that time-aware RAG is crucial to build powerful LLM apps that deal with time-based data. You also used Timescale Vector and LlamaIndex to construct a time-based RAG pipeline, resulting in a Streamlit chatbot capable of answering questions about a GitHub commit history or any other time-based knowledge base.

Learn more about Timescale Vector from our blog and dive even deeper on time-aware RAG in our webinar with LlamaIndex. Take your skills to the next level by creating your own Streamlit chatbot using time-based RAG and your data stored in Timescale.

Don't forget to share your creations in the Streamlit forum and on social media.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Siddhant Sadangi - Streamlit
https://blog.streamlit.io/author/siddhant/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Siddhant Sadangi
1 post
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Connect your Streamlit apps to Supabase
https://blog.streamlit.io/connect-your-streamlit-apps-to-supabase/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

By Siddhant Sadangi
December 20 2023
Why I built a Streamlit connection to Supabase
How to use the app
Step 1: Initialize the client for the demo project
Step 2: Explore the database, storage, or auth
Step 3: Click on “Run query 🏃” to get the results
What I learned while building this connection
Reusing Supabase methods
Single-sourcing versioning for consistency
What I learned while building the Streamlit app
Reusing components
Demo project for unregistered users
Constructing valid code snippets that can be copied by the user
Wrapping up
Contents
Share this post
← All posts
👉
TL;DR: st-supabase-connection is a Supabase connection for Streamlit that caches API calls and simplifies the needed code. I’ll walk you through how to:
1. Install and configure the connection
2. Use the tutorial app to explore Supabase storage, database, and auth features
3. Construct reusable code snippets to interact with Supabase

Want to dive right in? Check out the GitHub repo and demo app.

I’ve been building Streamlit apps for a while now, and a common requirement is to connect to a database. That’s when I stumbled upon Supabase.

Their web interface was powerful yet user-friendly, and when Streamlit launched their Connections Hackathon, I saw an opportunity to make it even easier to use Supabase with Streamlit. My aim was to help Streamlit users and build on the functionality of Supabase’s existing Python SDK.

In this article, you’ll learn:

Why I chose Supabase
How to install and set up st-supabase-connection
How to use the tutorial app provided with the connection
My learnings while working on the connection and the tutorial app
Why I built a Streamlit connection to Supabase

I was introduced to Supabase by a friend. I found the docs and the user interface so intuitive that I was able to have a hosted database up and running in under 10 minutes. I didn’t feel the need to research alternatives as Supabase offered everything I was looking for: the ability to perform CRUD operations from the UI and an open-source Python SDK that I could build upon.

My connection offers three advantages that improve the experience of building with Supabase and Streamlit:

It leverages Streamlit's caching feature to store API responses. This speeds up subsequent similar requests, reducing API calls, which ultimately decreases the cost of using Supabase.
It simplifies the boilerplate code needed to connect your Streamlit apps to Supabase.
It exposes more storage methods than Supabase’s Python client.
How to use the app

This app is designed to showcase and teach you how to use the connection's features. The connection currently works with Supabase Storage, Database, and Auth.

💡
Supabase offers a free tier. You need to sign up and get a Supabase API key to use it. For this app, I made it possible for you to demo the app without signing up for Supabase. If you like what you see and want to use it for your own project, sign up and enter your own API key to unlock all the features of the app and connect it with your own Supabase project.

If you are new to Supabase and don’t have a Supabase API key, you can follow the below quickstart to get acquainted with Supabase, the connection, and the app.

To get started, head over to the app hosted on Streamlit Community Cloud.

Step 1: Initialize the client for the demo project
✅
Since the connection is built on top of st.connection(), the client is cached so that it can handle network interruptions and does not need to be initialized frequently. That said, the tutorial app overrides this caching to show you the latest results.

The first thing you’ll need to do is initialize the connection to Supabase — you can do this by clicking “Initialize client” and you can either use a demo project or connect to your own Supabase instance.

Once the client is initialized, you’ll see the demo database or storage bucket. There are some tables and files that you can use for testing, and the app provides example queries.

You can also choose to explore storage, the database, or authentication. For each option, you’ll be able to test the functionality of the Supabase connection, as well as copy a code snippet that can be used in your Streamlit app to perform that operation.

Step 2: Explore the database, storage, or auth

Exploring the database

By default, database queries are not cached, so they always show the most recent data.

Exploring storage

Storage queries are cached forever, so they are very fast. You can change this by using the “Results cache duration” option, which affects the ttl parameter of the query.

Exploring auth

auth methods don’t have a cache, so you always get the latest results.

Step 3: Click on “Run query 🏃” to get the results

Click “Run query” or “Execute” to execute the specific command against the demo Supabase instance, and the results will be displayed in the app.

Example database view

Example storage view

Example auth view

Once you feel that Supabase would add value to your Streamlit apps, you can start using the connection as described in the Streamlit docs!

What I learned while building this connection

This was the first time I built something like this, so it was a great learning experience for me, especially around things experienced folks might take for granted. I’ve tried to mention a few of them below.

Reusing Supabase methods

One of the challenges I faced while developing this project was how to reuse the existing methods from the Supabase Python API, yet add caching functionality on top of them. I was not very familiar with Object Oriented Programming in Python, so I had to learn some concepts along the way.

For example, for the database operations, I assigned self.table to self.client.table, so that I could access all the methods Supabase provides, and benefit from their method chaining feature, which is very convenient and elegant. However, I also wanted to cache the results of the select() method without losing the ability to chain other methods. I could not find a way to do this while still using self.client.table, so I decided to create a new query() function that works like select(), but also stores the results in a cache. This way, you can use query() if you want to use the cache, or select() if you don't.

For some methods that modify or delete data, such as delete_bucket() and empty_bucket(), I did not need to add any caching functionality, so I used the Supabase Python client’s methods as-is.

self.delete_bucket = self.client.storage.delete_bucket
self.empty_bucket = self.client.storage.empty_bucket

For others, I wrapped Supabase’s methods around a function to add the st.cache_resource decorator. For example:

def get_bucket(
    self,
    bucket_id: str,
    ttl: Optional[Union[float, timedelta, str]] = None,
):
    """Retrieves the details of an existing storage bucket.

    Parameters
    ----------
    bucket_id : str
        Unique identifier of the bucket you would like to retrieve.
    ttl : float, timedelta, str, or None
        The maximum time to keep an entry in the cache. Defaults to `None` (cache never expires).
    """

    @cache_resource(ttl=ttl)
    def _get_bucket(_self, bucket_id):
        return _self.client.storage.get_bucket(bucket_id)

    return _get_bucket(self, bucket_id)
Single-sourcing versioning for consistency

st-supabase-connection follows Semantic Versioning (as everyone should).

The version number is needed by [setuptools](<https://pypi.org/project/setuptools/>) to install the library using pip install. Besides the library, this package also includes an application that demonstrates its usage. I wanted to display the current library version in the application as well.

Additionally, I like to mention the version number that corresponds to a change in the commit message. This implies that I have to keep track of the version number in at least three different places–setup.py, the demo application, and the commits.

To simplify this, I decided to use a __version__ attribute in the package itself. This attribute is imported to the Streamlit app as from st_supabase_connection import __version__.

To use this version in setup.py, I use the following function that reads the script and extracts the version:

def get_version(rel_path):
    with open(rel_path, "r", encoding="UTF-8") as f:
        for line in f:
            if line.startswith("__version__"):
                delim = '"' if '"' in line else "'"
                return line.split(delim)[1]
    raise RuntimeError("Unable to find version string.")

Now I only need to update the version in the script, and both the app and the setup script use the same version. Adding the version manually to commit messages doesn’t take enough time to warrant automation, but I am sure you could do that if you wanted to.

What I learned while building the Streamlit app
Reusing components

Most of my apps use the same template. I have a sidebar.html that populates the sidebar with a few sections that are present in all my apps. I just need to update a few links and captions.

Any additional sections can be added in the with sidebar() context:

with open("demo/sidebar.html", "r", encoding="UTF-8") as sidebar_file:
    # Replaces the "VERSION" placeholder with the current version from the script
		sidebar_html = sidebar_file.read().replace("{VERSION}", VERSION)

with st.sidebar:
		# Additional sections
    with st.expander("💡**How to use**", expanded=True):
        st.info(
            """
                1. Select a project and initialize client
                2. Select the relevant DB or Storage operation and options
                3. Run the query to get the results 
                4. Copy the constructed statement to use in your own app.
                """
        )

    if st.button(
        "Clear the cache to fetch latest data🧹",
        use_container_width=True,
        type="primary",
    ):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("Cache cleared")
		
		# Reused sidebar template
    st.components.v1.html(sidebar_html, height=600)
Demo project for unregistered users

The company I currently work for, neptune.ai, allows users to test our product without signing up. This is a user-friendly way to let potential customers try the product before they decide to share their personal information and create an account. That's why I decided to implement a similar feature in my own app.

I created a project with sample data and files that users can explore in my Supabase org. Then I added my own Supabase keys as Streamlit secrets, so that users can access the project with my credentials if they don't have or want their own Supabase account.

However, I also had to limit some actions that could affect the sample project for these users, so I used a Streamlit session_state variable to check if the user was using the demo project or their own project, and enable or disable functions accordingly.

For example, if the user is using the demo project, the session_state variable "project" is set to "demo", and some buttons are greyed out:

if st.session_state["project"] == "demo" and operation in RESTRICTED_STORAGE_OPERATORS:
  help = f"'{selected_operation.capitalize()}' not allowed in demo project"
Constructing valid code snippets that can be copied by the user

One of the features of the app is that it constructs code-snippets based on your inputs that you can then copy and paste into your own app.

This had the added benefit of helping me debug if I was doing something wrong while building the app and passing values from widgets to the connector.

I used formatted strings with placeholders that would be filled based on the chosen operation and options.

For example, this is the template I used for creating a new bucket:

constructed_storage_query = f"""st_supabase.create_bucket('{bucket_id}',{name=},{file_size_limit=},allowed_mime_types={allowed_mime_types},{public=})"""

I then display this snippet using Streamlit code(), and to make sure that this is the same statement that will be executed in the backend to perform the operation, I use Python’s eval() function to get results:

st.code(constructed_storage_query)
response = eval(constructed_storage_query)
Wrapping up

In this blog post, I have shared my experience and insights on how to use st-supabase-connection and build a tutorial app with it. I hope this has given you some guidance on how to install and configure the connection, how to use the tutorial app to learn about Supabase storage and database features, and how to use the connection in other Streamlit apps.

I would love to hear your feedback and suggestions on how to improve the connection or the app. You can reach me on GitHub, LinkedIn, or email.

Happy Streamlit-ing! 🎈

Share this post
Facebook
Twitter
LinkedIn
Comments

Continue the conversation in our forums →

Sign up for our newsletter

Stay in the loop with our announcements and updates 🤓

First name
Last name
Email*

You may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.

Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Chanin Nantasenamat - Streamlit
https://blog.streamlit.io/author/chanin/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts by Chanin Nantasenamat
Senior Developer Advocate at Streamlit
16 posts
Website
Twitter
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
LangChain tutorial #5: Build an Ask the Data app

Leverage Agents in LangChain to interact with pandas DataFrame

LLMs
by
Chanin Nantasenamat
,
July 21 2023
How to build a Llama 2 chatbot

Experiment with this open-source LLM from Meta

LLMs
by
Chanin Nantasenamat
,
July 21 2023
Beginner’s guide to OpenAI API

Build your own LLM tool from scratch

LLMs
by
Chanin Nantasenamat
,
July 20 2023
LangChain tutorial #4: Build an Ask the Doc app

How to get answers from documents using embeddings, a vector store, and a question-answering chain

LLMs
by
Chanin Nantasenamat
,
June 20 2023
LangChain tutorial #3: Build a Text Summarization app

Explore the use of the document loader, text splitter, and summarization chain

LLMs
by
Chanin Nantasenamat
,
June 13 2023
LangChain tutorial #2: Build a blog outline generator app in 25 lines of code

A guide on conquering writer’s block with a Streamlit app

LLMs
by
Chanin Nantasenamat
,
June 7 2023
LangChain tutorial #1: Build an LLM-powered app in 18 lines of code

A step-by-step guide using OpenAI, LangChain, and Streamlit

Tutorials
by
Chanin Nantasenamat
,
May 31 2023
8 tips for securely using API keys

How to safely navigate the turbulent landscape of LLM-powered apps

Tutorials
by
Chanin Nantasenamat
,
May 19 2023
How to build an LLM-powered ChatBot with Streamlit

A step-by-step guide using the unofficial HuggingChat API

LLMs
by
Chanin Nantasenamat
,
May 10 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Tutorials on Building, Managing & Deploying Apps | Streamlit
https://blog.streamlit.io/tag/tutorials/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Tutorials
57 posts
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
How in-app feedback can increase your chatbot’s performance

A guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith

Tutorials
by
Charly Wargnier
,
October 6 2023
LangChain tutorial #4: Build an Ask the Doc app

How to get answers from documents using embeddings, a vector store, and a question-answering chain

LLMs
by
Chanin Nantasenamat
,
June 20 2023
Building a Streamlit and scikit-learn app with ChatGPT

Catching up on coding skills with an AI assistant

LLMs
by
Michael Hunger
,
June 16 2023
LangChain tutorial #3: Build a Text Summarization app

Explore the use of the document loader, text splitter, and summarization chain

LLMs
by
Chanin Nantasenamat
,
June 13 2023
LangChain tutorial #2: Build a blog outline generator app in 25 lines of code

A guide on conquering writer’s block with a Streamlit app

LLMs
by
Chanin Nantasenamat
,
June 7 2023
LangChain tutorial #1: Build an LLM-powered app in 18 lines of code

A step-by-step guide using OpenAI, LangChain, and Streamlit

Tutorials
by
Chanin Nantasenamat
,
May 31 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Streamlit Product Announcements
https://blog.streamlit.io/tag/product/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in Product
36 posts
Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
How to improve Streamlit app loading speed

Speed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app

Product
by
Zachary Blackwood
,
October 5 2023
Announcing Streamlit for Data Science: Second Edition

Your step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!

Product
by
Tyler Richards
,
September 29 2023
st.status: Visualize your app’s processes

Rich context for users and more control for developers

Product
by
Joshua Carroll
,
September 7 2023
Introducing column config ⚙️

Take st.dataframe and st.data_editor to the next level!

Product
by
Lukas Masuch and 
1
 more,
June 22 2023
Generative AI and Streamlit: A perfect match

The future is about to get interesting…

LLMs
by
Adrien Treuille and 
1
 more,
June 15 2023
Introducing st.connection!

Quickly and easily connect your app to data and APIs

Product
by
Joshua Carroll and 
1
 more,
May 2 2023
Introducing a chemical molecule component for your Streamlit apps

Integrate a fully featured molecule editor with just a few lines of code!

Product
by
Michał Nowotka
,
April 13 2023
Introducing Streamlit to the Polish Python community

My Streamlit presentation at PyWaW #103

Product
by
Michał Nowotka
,
April 4 2023
Editable dataframes are here! ✍️

Take interactivity to the next level with st.experimental_data_editor

Product
by
Lukas Masuch and 
2
 more,
February 28 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog Posts: Using LLMs with Streamlit
https://blog.streamlit.io/tag/llms/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
← All posts
Posts in LLMs
33 posts
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Build your own Notion chatbot

A step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit

LLMs
by
Logan Vendrix
,
September 14 2023
Chat with pandas DataFrames using LLMs

A step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI

LLMs
by
Amjad Raza
,
August 31 2023
Build a chatbot with custom data sources, powered by LlamaIndex

Augment any LLM with your own data in 43 lines of code!

LLMs
by
Caroline Frasca and 
2
 more,
August 23 2023
Exploring LLMs and prompts: A guide to the PromptTools Playground

Learn how to build dynamic, stateful applications that harness multiple LLMs at once

LLMs
by
Steve Krawczyk and 
1
 more,
August 18 2023
AI Interviewer: Customized interview preparation with generative AI

How we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!

LLMs
by
Haoxiang Jia and 
1
 more,
August 9 2023
Instant Insight: Generate data-driven presentations in a snap!

Create presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery

LLMs
by
Oleksandr Arsentiev
,
August 2 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

Blog on Building & Sharing Data Apps | Streamlit Blog
https://blog.streamlit.io/
Blog
LLMs
Product
Tutorials
Write for Streamlit!
Streamlit Home
More
Search posts
Building a dashboard in Python using Streamlit

Using pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend

Tutorials
by
Chanin Nantasenamat
,
January 22 2024
Connect your Streamlit apps to Supabase

Learn how to connect your Streamlit apps to Supabase with the st-supabase-connection component

by
Siddhant Sadangi
,
December 20 2023
Using time-based RAG in LLM apps

Build a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex

LLMs
by
Avthar Sewrathan
,
December 15 2023
7 ways GPT-4 with Vision can uplevel your Streamlit apps

Your AI coach to design and debug interactive Streamlit apps from static images

LLMs
by
Charly Wargnier
,
November 15 2023
Introducing AppTest: a faster way to build quality Streamlit apps

A native framework for automated app testing

Product
by
Joshua Carroll
,
October 31 2023
Take your Streamlit apps to the next level with GPT-4

Pro tips to design, debug, and optimize your Streamlit apps faster

LLMs
by
Charly Wargnier
,
October 24 2023
How to build a real-time LLM app without vector databases

Create a discount finder app using Pathway and Streamlit in 10 steps

LLMs
by
Bobur Umurzokov
,
October 19 2023
Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4

6 case studies that compare data chart outputs from LLMs using Chat2VIS

Tutorials
by
Paula Maddigan
,
October 16 2023
Land your dream job: Build your portfolio with Streamlit

Showcase your coding skills to recruiters with a chatbot tailored to your resume

by
Vicky Kuo
,
October 13 2023
Simplifying generative AI workflows

A step-by-step tutorial to building complex ML workflows with Covalent and Streamlit

Advocate Posts
by
Filip Boltuzic and 
2
 more,
October 6 2023
How in-app feedback can increase your chatbot’s performance

A guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith

Tutorials
by
Charly Wargnier
,
October 6 2023
Next page →
Copyright © Streamlit 2024
Cookie settings
Hello there 👋🏻

Thanks for stopping by! We use cookies to help us understand how you interact with our website.
By clicking “Accept all”, you consent to our use of cookies. For more information, please see our privacy policy.

Cookie settings
Reject all
Accept all

