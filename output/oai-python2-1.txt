Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.0/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.1/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.3/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.2/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.5/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.26.4/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.1/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.2/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.0/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.3/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.4/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.9/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.5/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.6/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.8/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.7/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.27.10/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.28.0/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

openai · PyPI
https://pypi.org/project/openai/0.0.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.0.2

pip install openai==0.0.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Feb 18, 2020

Placeholder package

Navigation
 Project description
 Release history
 Download files
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

Page Not Found (404) · PyPI
https://pypi.org/project/openai/0.28.1/openai/embeddings_utils.py
We looked everywhere but couldn't find this page
Search PyPI
Search

Error code 404
Back to the homepage

And now for something
completely different

View video transcript

openai · PyPI
https://pypi.org/project/openai/0.1.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.1.3

pip install openai==0.1.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.1.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.1.0

pip install openai==0.1.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.1.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.1.1

pip install openai==0.1.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.1.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.1.2

pip install openai==0.1.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.0

pip install openai==0.2.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.3

pip install openai==0.2.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 7, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.1

pip install openai==0.2.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 13, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.4

pip install openai==0.2.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 12, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.5

pip install openai==0.2.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 5, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.2.6/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.2.6

pip install openai==0.2.6
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 24, 2020

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.3.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.3.0

pip install openai==0.3.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 27, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.4.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.4.0

pip install openai==0.4.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 4, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.6.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.6.0

pip install openai==0.6.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 18, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.6.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.6.1

pip install openai==0.6.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 19, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.6.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.6.2

pip install openai==0.6.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 20, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.6.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.6.3

pip install openai==0.6.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 12, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.8.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.8.0

pip install openai==0.8.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 17, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.6.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.6.4

pip install openai==0.6.4
Copy PIP instructions

Newer version available (1.10.0)

Released: May 21, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.7.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.7.0

pip install openai==0.7.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 12, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.9.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.9.0

pip install openai==0.9.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 29, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.9.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.9.1

pip install openai==0.9.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 30, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.9.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.9.2

pip install openai==0.9.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 30, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.9.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.9.4

pip install openai==0.9.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 13, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: all systems operational

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.9.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.9.3

pip install openai==0.9.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 30, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.0/
Skip to main content
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.0

pip install openai==0.10.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 14, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: all systems operational

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.1

pip install openai==0.10.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 14, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.2

pip install openai==0.10.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 29, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.3/
Skip to main content
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.3

pip install openai==0.10.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Aug 31, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.4

pip install openai==0.10.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Sep 9, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.10.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.10.5

pip install openai==0.10.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 1, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.6

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.0

pip install openai==0.11.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 27, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.1

pip install openai==0.11.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 2, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.2

pip install openai==0.11.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 4, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.3

pip install openai==0.11.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 4, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.4

pip install openai==0.11.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 14, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.6/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.6

pip install openai==0.11.6
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 21, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.11.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.11.5

pip install openai==0.11.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 21, 2021

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.12.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.12.0

pip install openai==0.12.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 22, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.15.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.15.0

pip install openai==0.15.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Feb 24, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.16.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.16.0

pip install openai==0.16.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 17, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.13.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.13.0

pip install openai==0.13.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 25, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.14.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.14.0

pip install openai==0.14.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Feb 2, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.18.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.18.0

pip install openai==0.18.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 8, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.18.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.18.1

pip install openai==0.18.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 15, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.19.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.19.0

pip install openai==0.19.0
Copy PIP instructions

Newer version available (1.10.0)

Released: May 25, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.20.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.20.0

pip install openai==0.20.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 16, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.22.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.22.0

pip install openai==0.22.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jul 26, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.22.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.22.1

pip install openai==0.22.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Aug 3, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.23.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.23.0

pip install openai==0.23.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Aug 24, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.23.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.23.1

pip install openai==0.23.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Sep 28, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.0

pip install openai==0.26.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 7, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

Documentation

See the OpenAI API docs.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openapi.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise a openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine tuning

Fine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine tuning are shared in the following Jupyter notebooks:

Classification with fine tuning (a simple notebook that shows the steps required for fine tuning)
Fine tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.24.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.24.0

pip install openai==0.24.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 21, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.25.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.25.0

pip install openai==0.25.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 2, 2022

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description

The author of this package has not provided a project description

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.1

pip install openai==0.26.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 13, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

Documentation

See the OpenAI API docs.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise a openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine tuning

Fine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine tuning are shared in the following Jupyter notebooks:

Classification with fine tuning (a simple notebook that shows the steps required for fine tuning)
Fine tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.3

pip install openai==0.26.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 25, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.2

pip install openai==0.26.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 24, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

Documentation

See the OpenAI API docs.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.5

pip install openai==0.26.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Feb 7, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.26.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.26.4

pip install openai==0.26.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 26, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines.data[0].id)

# create a completion
completion = openai.Completion.create(engine="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list engines
openai api engines.list

# create a completion
openai api completions.create -e ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", engine="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.1

pip install openai==0.27.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 8, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.2

pip install openai==0.27.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 11, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.0

pip install openai==0.27.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Mar 1, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2022-12-01"

# create a completion
completion = openai.Completion.create(engine="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2022-12-01"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.3

pip install openai==0.27.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 3, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-03-15-preview"

# create a completion
completion = openai.Completion.create(deployment_id="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-03-15-preview"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.4

pip install openai==0.27.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 4, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-03-15-preview"

# create a completion
completion = openai.Completion.create(deployment_id="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-03-15-preview"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.5

pip install openai==0.27.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Apr 27, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-03-15-preview"

# create a completion
completion = openai.Completion.create(deployment_id="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-03-15-preview"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.6/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.6

pip install openai==0.27.6
Copy PIP instructions

Newer version available (1.10.0)

Released: May 2, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model="ada", prompt="Hello world")

# print the completion
print(completion.choices[0].text)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-03-15-preview"

# create a completion
completion = openai.Completion.create(deployment_id="deployment-name", prompt="Hello world")

# print the completion
print(completion.choices[0].text)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-03-15-preview"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a completion
openai api completions.create -m ada -p "Hello world"

# create a chat completion
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world!"}])
print(completion.choices[0].message.content)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_completion():
    completion_resp = await openai.Completion.acreate(prompt="This is a test", model="davinci")


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.7/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.7

pip install openai==0.27.7
Copy PIP instructions

Newer version available (1.10.0)

Released: May 19, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the chat completion
print(chat_completion.choices[0].message.content)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat Completions

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)

Completions

Text models such as text-davinci-003, text-davinci-002 and earlier (ada, babbage, curie, davinci, etc.) can be called using the completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.Completion.create(model="text-davinci-003", prompt="Hello world")
print(completion.choices[0].text)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.8/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.8

pip install openai==0.27.8
Copy PIP instructions

Newer version available (1.10.0)

Released: Jun 7, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the chat completion
print(chat_completion.choices[0].message.content)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat Completions

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)

Completions

Text models such as text-davinci-003, text-davinci-002 and earlier (ada, babbage, curie, davinci, etc.) can be called using the completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.Completion.create(model="text-davinci-003", prompt="Hello world")
print(completion.choices[0].text)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-similarity-davinci-001"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this get embeddings notebook.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.9/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.9

pip install openai==0.27.9
Copy PIP instructions

Newer version available (1.10.0)

Released: Aug 22, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the chat completion
print(chat_completion.choices[0].message.content)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(chat_completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat Completions

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)

Completions

Text models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.Completion.create(model="davinci-002", prompt="Hello world")
print(completion.choices[0].text)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-embedding-ada-002"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this embeddings guide.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).

Examples of fine-tuning are shared in the following Jupyter notebooks:

Classification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)
Fine-tuning a model that answers questions about the 2020 Olympics
Step 1: Collecting data
Step 2: Creating a synthetic Q&A dataset
Step 3: Train a fine-tuning model specialized for Q&A

Sync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:

openai wandb sync


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.27.10/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.27.10

pip install openai==0.27.10
Copy PIP instructions

Newer version available (1.10.0)

Released: Aug 30, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the chat completion
print(chat_completion.choices[0].message.content)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(chat_completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat Completions

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)

Completions

Text models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.Completion.create(model="davinci-002", prompt="Hello world")
print(completion.choices[0].text)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-embedding-ada-002"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this embeddings guide.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.

# Create a fine-tuning job with an already uploaded file
openai.FineTuningJob.create(training_file="file-abc123", model="gpt-3.5-turbo")

# List 10 fine-tuning jobs
openai.FineTuningJob.list(limit=10)

# Retrieve the state of a fine-tune
openai.FineTuningJob.retrieve("ft-abc123")

# Cancel a job
openai.FineTuningJob.cancel("ft-abc123")

# List up to 10 events from a fine-tuning job
openai.FineTuningJob.list_events(id="ft-abc123", limit=10)

# Delete a fine-tuned model (must be an owner of the org the model was created in)
openai.Model.delete("ft-abc123")


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.28.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.28.0

pip install openai==0.28.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Sep 1, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

You don't need this source code unless you want to modify the package. If you just want to use the package, just run:

pip install --upgrade openai


Install from source with:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

import openai
openai.api_key = "sk-..."

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the chat completion
print(chat_completion.choices[0].message.content)

Params

All endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(chat_completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure fine-tuning
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

# ...

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Example code

Examples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:

Classification using fine-tuning
Clustering
Code search
Customizing embeddings
Question answering from a corpus of documents
Recommendations
Visualization of embeddings
And more

Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.

Chat Completions

Conversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)

Completions

Text models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

completion = openai.Completion.create(model="davinci-002", prompt="Hello world")
print(completion.choices[0].text)

Embeddings

In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.

To get an embedding for a text string, you can use the embeddings method as follows in Python:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

# choose text to embed
text_string = "sample text"

# choose an embedding
model_id = "text-embedding-ada-002"

# compute the embedding of the text
embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


An example of how to call the embeddings method is shown in this embeddings guide.

Examples of how to use embeddings are shared in the following Jupyter notebooks:

Classification using embeddings
Clustering using embeddings
Code search using embeddings
Semantic text search using embeddings
User and product embeddings
Zero-shot classification using embeddings
Recommendation using embeddings

For more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.

# Create a fine-tuning job with an already uploaded file
openai.FineTuningJob.create(training_file="file-abc123", model="gpt-3.5-turbo")

# List 10 fine-tuning jobs
openai.FineTuningJob.list(limit=10)

# Retrieve the state of a fine-tune
openai.FineTuningJob.retrieve("ft-abc123")

# Cancel a job
openai.FineTuningJob.cancel("ft-abc123")

# List up to 10 events from a fine-tuning job
openai.FineTuningJob.list_events(id="ft-abc123", limit=10)

# Delete a fine-tuned model (must be an owner of the org the model was created in)
openai.Model.delete("ft-abc123")


For more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.

Moderation

OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


See the moderation guide for more details.

Image generation (DALL·E)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")

Audio transcription (Whisper)
import openai
openai.api_key = "sk-..."  # supply your API key however you choose
f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

Async API

Async support is available in the API by prepending a to a network-bound method:

import openai
openai.api_key = "sk-..."  # supply your API key however you choose

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

import openai
from aiohttp import ClientSession

openai.aiosession.set(ClientSession())
# At the end of your program, close the http session
await openai.aiosession.get().close()


See the usage guide for more details.

Requirements
Python 3.7.1+

In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/0.28.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 0.28.1

pip install openai==0.28.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Sep 26, 2023

Python client library for the OpenAI API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: MIT License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: MIT License
Operating System
OS Independent
Programming Language
Python :: 3
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python Library

The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.

You can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.

Installation

To start, ensure you have Python 3.7.1 or newer. If you just want to use the package, run:

pip install --upgrade openai


After you have installed the package, import it at the top of a file:

import openai


To install this package from source to make modifications to it, run the following command from the root of the repository:

python setup.py install

Optional dependencies

Install dependencies for openai.embeddings_utils:

pip install openai[embeddings]


Install support for Weights & Biases:

pip install openai[wandb]


Data libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:

pip install openai[datalib]

Usage

The library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:

export OPENAI_API_KEY='sk-...'


Or set openai.api_key to its value:

openai.api_key = "sk-..."


Examples of how to use this library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for: classification using fine-tuning, clustering, code search, customizing embeddings, question answering from a corpus of documents. recommendations, visualization of embeddings, and more.

Most endpoints support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).

Chat completions

Chat models such as gpt-3.5-turbo and gpt-4 can be called using the chat completions endpoint.

completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
print(completion.choices[0].message.content)


You can learn more in our chat completions guide.

Completions

Text models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.

completion = openai.Completion.create(model="davinci-002", prompt="Hello world")
print(completion.choices[0].text)


You can learn more in our completions guide.

Embeddings

Embeddings are designed to measure the similarity or relevance between text strings. To get an embedding for a text string, you can use following:

text_string = "sample text"

model_id = "text-embedding-ada-002"

embedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']


You can learn more in our embeddings guide.

Fine-tuning

Fine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.

# Create a fine-tuning job with an already uploaded file
openai.FineTuningJob.create(training_file="file-abc123", model="gpt-3.5-turbo")

# List 10 fine-tuning jobs
openai.FineTuningJob.list(limit=10)

# Retrieve the state of a fine-tune
openai.FineTuningJob.retrieve("ft-abc123")

# Cancel a job
openai.FineTuningJob.cancel("ft-abc123")

# List up to 10 events from a fine-tuning job
openai.FineTuningJob.list_events(id="ft-abc123", limit=10)

# Delete a fine-tuned model (must be an owner of the org the model was created in)
openai.Model.delete("ft:gpt-3.5-turbo:acemeco:suffix:abc123")


You can learn more in our fine-tuning guide.

To log the training results from fine-tuning to Weights & Biases use:

openai wandb sync


For more information, read the wandb documentation on Weights & Biases.

Moderation

OpenAI provides a free Moderation endpoint that can be used to check whether content complies with the OpenAI content policy.

moderation_resp = openai.Moderation.create(input="Here is some perfectly innocuous text that follows all OpenAI content policies.")


You can learn more in our moderation guide.

Image generation (DALL·E)

DALL·E is a generative image model that can create new images based on a prompt.

image_resp = openai.Image.create(prompt="two dogs playing chess, oil painting", n=4, size="512x512")


You can learn more in our image generation guide.

Audio (Whisper)

The speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model.

f = open("path/to/file.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", f)

transcript = openai.Audio.translate("whisper-1", f)


You can learn more in our speech to text guide.

Async API

Async support is available in the API by prepending a to a network-bound method:

async def create_chat_completion():
    chat_completion_resp = await openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])


To make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:

from aiohttp import ClientSession
openai.aiosession.set(ClientSession())

# At the end of your program, close the http session
await openai.aiosession.get().close()

Command-line interface

This library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.

# list models
openai api models.list

# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)
openai api chat_completions.create -m gpt-3.5-turbo -g user "Hello world"

# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)
openai api completions.create -m ada -p "Hello world"

# generate images via DALL·E API
openai api image.create -p "two dogs playing chess, cartoon" -n 1

# using openai through a proxy
openai --proxy=http://proxy.com api models.list

Microsoft Azure Endpoints

In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.

import openai
openai.api_type = "azure"
openai.api_key = "..."
openai.api_base = "https://example-endpoint.openai.azure.com"
openai.api_version = "2023-05-15"

# create a chat completion
chat_completion = openai.ChatCompletion.create(deployment_id="deployment-name", model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# print the completion
print(chat_completion.choices[0].message.content)


Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:

Using Azure completions
Using Azure chat
Using Azure embeddings
Microsoft Azure Active Directory Authentication

In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to "azure_ad" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.

from azure.identity import DefaultAzureCredential
import openai

# Request credential
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

# Setup parameters
openai.api_type = "azure_ad"
openai.api_key = token.token
openai.api_base = "https://example-endpoint.openai.azure.com/"
openai.api_version = "2023-05-15"

Credit

This library is forked from the Stripe Python Library.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0b1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0b1

pip install openai==1.0.0b1
Copy PIP instructions

Newer version available (1.10.0)

Released: Sep 29, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License (Apache-2.0)

Author: OpenAI

Requires: Python >=3.7.1, <4.0.0

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: Apache Software License
Programming Language
Python :: 3
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API Library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. It includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="my api key",
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
)
print(completion.choices)


While you can provide an api_key keyword argument, we recommend using python-dotenv and adding OPENAI_API_KEY="my api key" to your .env file so that your API Key is not stored in source control.

Async Usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="my api key",
)


async def main():
    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
    )
    print(completion.choices)


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT]
We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using Types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into json (v1, v2). To get a dictionary, you can call dict(model).

This helps provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to "basic".

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

contents = Path("input.jsonl").read_bytes()
client.files.create(
    file=contents,
    purpose="fine-tune",
)


The async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.

import aiofiles
from openai import OpenAI

client = OpenAI()

async with aiofiles.open("input.jsonl", mode="rb") as f:
    contents = await f.read()

await client.files.create(
    file=contents,
    purpose="fine-tune",
)

Handling errors

When the library is unable to connect to the API (e.g., due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (i.e., 4xx or 5xx response), a subclass of openai.APIStatusError will be raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors will be automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors will all be retried by default.

You can use the max_retries option to configure or disable this:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
)

Timeouts

Requests time out after 10 minutes by default. You can configure this with a timeout option, which accepts a float or an httpx.Timeout:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
)


On timeout, an APITimeoutError is thrown.

Note that requests which time out will be retried twice by default.

Advanced
How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Configuring custom URLs, proxies, and transports

You can configure the following keyword arguments when instantiating the client:

import httpx
from openai import OpenAI

client = OpenAI(
    # Use a custom base URL
    base_url="http://my.test.server.example.com:8083",
    proxies="http://my.test.proxy.example.com",
    transport=httpx.HTTPTransport(local_address="0.0.0.0"),
)


See the httpx documentation for information about the proxies and transport keyword arguments.

Managing HTTP resources

By default we will close the underlying HTTP connections whenever the client is garbage collected is called but you can also manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Versioning

This package generally attempts to follow SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0b2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0b2

pip install openai==1.0.0b2
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 12, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License (Apache-2.0)

Author: OpenAI

Requires: Python >=3.7.1, <4.0.0

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: Apache Software License
Programming Language
Python :: 3
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
)
print(completion.choices)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main():
    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
    )
    print(completion.choices)


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call dict(model).

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

contents = Path("input.jsonl").read_bytes()
client.files.create(
    file=contents,
    purpose="fine-tune",
)


The async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.

import aiofiles
from openai import OpenAI

client = OpenAI()

async with aiofiles.open("input.jsonl", mode="rb") as f:
    contents = await f.read()

await client.files.create(
    file=contents,
    purpose="fine-tune",
)

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0b3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0b3

pip install openai==1.0.0b3
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 17, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License (Apache-2.0)

Author: OpenAI

Requires: Python >=3.7.1, <4.0.0

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
License
OSI Approved :: Apache Software License
Programming Language
Python :: 3
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
)
print(completion.choices)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main():
    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
    )
    print(completion.choices)


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call dict(model).

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

contents = Path("input.jsonl").read_bytes()
client.files.create(
    file=contents,
    purpose="fine-tune",
)


The async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.

import aiofiles
from openai import OpenAI

client = OpenAI()

async with aiofiles.open("input.jsonl", mode="rb") as f:
    contents = await f.read()

await client.files.create(
    file=contents,
    purpose="fine-tune",
)

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0rc1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0rc1

pip install openai==1.0.0rc1
Copy PIP instructions

Newer version available (1.10.0)

Released: Oct 28, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
)
print(completion.choices)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main():
    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
    )
    print(completion.choices)


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "content": "string",
        "role": "system",
    }],
    model="gpt-3.5-turbo",
)

print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion.choices)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT]
The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0rc2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0rc2

pip install openai==1.0.0rc2
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 3, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
)
print(completion.choices)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main():
    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
    )
    print(completion.choices)


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "content": "string",
        "role": "system",
    }],
    model="gpt-3.5-turbo",
)

print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion.choices)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0rc3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0rc3

pip install openai==1.0.0rc3
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 6, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

Documentation

The API documentation can be found here.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.completions.create(
    prompt="Say this is a test",
    model="text-davinci-003",
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.0

pip install openai==1.0.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 6, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Beta Release

[!IMPORTANT]
We're preparing to release version 1.0 of the OpenAI Python library.

This new version will be a major release and will include breaking changes. We're releasing this beta version to give you a chance to try out the new features and provide feedback before the official release. You can install the beta version with:

pip install --pre openai


And follow along with the beta release notes.

Installation
pip install --pre openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.0.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.0.1

pip install openai==1.0.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 6, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.1.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.1.0

pip install openai==1.1.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 6, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.1.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.1.1

pip install openai==1.1.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 6, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.1.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.1.2

pip install openai==1.1.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 8, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.2.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.2.0

pip install openai==1.2.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 9, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

page = client.files.list()

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.2.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.2.1

pip install openai==1.2.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 9, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation
pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.2.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.2.2

pip install openai==1.2.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 9, 2023

Client library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.2.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.2.3

pip install openai==1.2.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 10, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.0

pip install openai==1.3.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 15, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.2.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.2.4

pip install openai==1.2.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 13, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.1

pip install openai==1.3.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 16, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.2

pip install openai==1.3.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 16, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.4/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.4

pip install openai==1.3.4
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 21, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.3/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.3

pip install openai==1.3.3
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 18, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.5/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.5

pip install openai==1.3.5
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 22, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for part in stream:
    print(part.choices[0].delta.content or "")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for part in stream:
    print(part.choices[0].delta.content or "")

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.6/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.6

pip install openai==1.3.6
Copy PIP instructions

Newer version available (1.10.0)

Released: Nov 29, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="My API Key",
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.7/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.7

pip install openai==1.3.7
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 1, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content)


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt="Say this is a test",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content)

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.8/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.8

pip install openai==1.3.8
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 9, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content)


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
async for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content)

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 60s
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.3.9/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.3.9

pip install openai==1.3.9
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 13, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.4.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.4.0

pip install openai==1.4.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 15, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.5.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.5.0

pip install openai==1.5.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 17, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Bloomberg is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.6.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.6.0

pip install openai==1.6.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 20, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview"
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.7.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.7.0

pip install openai==1.7.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 9, 2024

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Capital One is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.6.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.6.1

pip install openai==1.6.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Dec 22, 2023

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.7.1/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.7.1

pip install openai==1.7.1
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 10, 2024

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Red Hat is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The API documentation can be found here.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tunes.create(
        training_file="file-XGinujblHPwGLSztz8cPS8XY",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.7.2/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.7.2

pip install openai==1.7.2
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 12, 2024

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-3.5-turbo",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an APIResponse object.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.8.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.8.0

pip install openai==1.8.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 16, 2024

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Google is a Visionary sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-3.5-turbo",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.

For the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.

A migration script will be provided & the migration in general should be smooth.

.with_streaming_response

The above interface eagerly reads the full response body when you make the request, which may not always be what you want.

To stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.

As such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.

with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)


The context manager is required so that the response will reliably be closed.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.9.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.9.0

pip install openai==1.9.0
Copy PIP instructions

Newer version available (1.10.0)

Released: Jan 21, 2024

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Qube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-3.5-turbo",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.

For the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.

A migration script will be provided & the migration in general should be smooth.

.with_streaming_response

The above interface eagerly reads the full response body when you make the request, which may not always be what you want.

To stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.

As such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.

with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)


The context manager is required so that the response will reliably be closed.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/1.10.0/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.10.0

pip install openai==1.10.0
Copy PIP instructions

Latest version

Released: about 4 hours ago

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
OpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-3.5-turbo",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.

For the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.

A migration script will be provided & the migration in general should be smooth.

.with_streaming_response

The above interface eagerly reads the full response body when you make the request, which may not always be what you want.

To stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.

As such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.

with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)


The context manager is required so that the response will reliably be closed.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

openai · PyPI
https://pypi.org/project/openai/
Skip to main content
 2FA is now required on PyPI  Read more
Search PyPI
Search
Help Sponsors Log in Register
openai 1.10.0

pip install openai
Copy PIP instructions

Latest version

Released: about 4 hours ago

The official Python library for the openai API

Navigation
 Project description
 Release history
 Download files
Project links
Homepage
Repository
Statistics
GitHub statistics:
 Stars: 17970
 Forks: 2459
 Open issues: 48
 Open PRs: 6

View statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery

Meta

License: Apache Software License

Author: OpenAI

Requires: Python >=3.7.1

Maintainers
 atty-openai
 borispower
 christinakim
 ddeville
 dschnurr-openai
 emorikawa-openai
 gdb
 hallacy-openai
 hponde_oai
 jhallard
 kennyhsu
 michelle-openai
 mikaell-openai
 peterz-openai
 rachel-openai
 tomerkOpenAI
Classifiers
Intended Audience
Developers
License
OSI Approved :: Apache Software License
Operating System
MacOS
Microsoft :: Windows
OS Independent
POSIX
POSIX :: Linux
Programming Language
Python :: 3.7
Python :: 3.8
Python :: 3.9
Python :: 3.10
Python :: 3.11
Python :: 3.12
Topic
Software Development :: Libraries :: Python Modules
Typing
Typed
Indeed is a Contributing sponsor of the Python Software Foundation.
PSF Sponsor · Served ethically
Project description
OpenAI Python API library

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.

It is generated from our OpenAPI specification with Stainless.

Documentation

The REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.

Installation

[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.

pip install openai

Usage

The full API of this library can be found in api.md.

import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
)


While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY="My API Key" to your .env file so that your API Key is not stored in source control.

Async usage

Simply import AsyncOpenAI instead of OpenAI and use await with each API call:

import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())


Functionality between the synchronous and asynchronous clients is otherwise identical.

Streaming Responses

We provide support for streaming responses using Server Side Events (SSE).

from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")


The async client uses the exact same interface.

from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")


asyncio.run(main())

Module-level client

[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = "https://..."
openai.default_headers = {"x-foo": "true"}

completion = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.choices[0].message.content)


The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, not in application code.

We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:

It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Using types

Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:

Serializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)
Converting to a dictionary, model.model_dump(exclude_unset=True)

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.

Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

import openai

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)


Or, asynchronously:

import asyncio
import openai

client = AsyncOpenAI()


async def main() -> None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())


Alternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.


Or just work directly with the returned data:

first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # => "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.

Nested params

Nested parameters are dictionaries, typed using TypedDict, for example:

from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Can you generate an example json object describing a fruit?",
        }
    ],
    model="gpt-3.5-turbo-1106",
    response_format={"type": "json_object"},
)

File Uploads

Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).

from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)


The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.

Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.

When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.

All errors inherit from openai.APIError.

import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-3.5-turbo",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)


Error codes are as followed:

Status Code	Error Type
400	BadRequestError
401	AuthenticationError
403	PermissionDeniedError
404	NotFoundError
422	UnprocessableEntityError
429	RateLimitError
>=500	InternalServerError
N/A	APIConnectionError
Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.

You can use the max_retries option to configure or disable retry settings:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in Node.js?",
        }
    ],
    model="gpt-3.5-turbo",
)

Timeouts

By default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:

from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5 * 1000).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-3.5-turbo",
)


On timeout, an APITimeoutError is thrown.

Note that requests that time out are retried twice by default.

Advanced
Logging

We use the standard library logging module.

You can enable logging by setting the environment variable OPENAI_LOG to debug.

$ export OPENAI_LOG=debug

How to tell whether None means null or missing

In an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:

if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')

Accessing raw response data (e.g. headers)

The "raw" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,

from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-3.5-turbo",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)


These methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.

For the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.

A migration script will be provided & the migration in general should be smooth.

.with_streaming_response

The above interface eagerly reads the full response body when you make the request, which may not always be what you want.

To stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.

As such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.

with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-3.5-turbo",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)


The context manager is required so that the response will reliably be closed.

Configuring the HTTP client

You can directly override the httpx client to customize it for your use case, including:

Support for proxies
Custom transports
Additional advanced functionality
import httpx
from openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=httpx.Client(
        proxies="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)

Managing HTTP resources

By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.

Microsoft Azure OpenAI

To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.

[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.

from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.model_dump_json(indent=2))


In addition to the options provided in the base OpenAI client, the following options are provided:

azure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)
azure_deployment
api_version (or the OPENAI_API_VERSION environment variable)
azure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)
azure_ad_token_provider

An example of using the client with Azure Active Directory can be found here.

Versioning

This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:

Changes that only affect static types, without breaking runtime behavior.
Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).
Changes that we do not expect to impact the vast majority of users in practice.

We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.

We are keen for your feedback; please open an issue with questions, bugs, or suggestions.

Requirements

Python 3.7 or higher.

Help
Installing packages
Uploading packages
User guide
Project name retention
FAQs
About PyPI
PyPI on Twitter
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors
Contributing to PyPI
Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits
Using PyPI
Code of conduct
Report security issue
Privacy policy
Terms of use
Acceptable Use Policy

Status: Service Under Maintenance

Developed and maintained by the Python community, for the Python community.
Donate today!

"PyPI", "Python Package Index", and the blocks logos are registered trademarks of the Python Software Foundation.


© 2024 Python Software Foundation
Site map

 English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto
AWS
Cloud computing and Security Sponsor
Datadog
Monitoring
Fastly
CDN
Google
Download Analytics
Microsoft
PSF Sponsor
Pingdom
Monitoring
Sentry
Error logging
StatusPage
Status page

