[
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nIn this module, you've learned how to use Apache Spark to transform data in Azure Synapse Analytics. Using Spark for complex data transformations is a common technique because of the inherent scalability of the Spark platform. You can use code in notebooks to experiment with data transformations, and then include those notebooks in automated pipelines as part of a data integration solution.\n\n Tip\n\nLearn more about using the Spark SQL and DataFrames Guide.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Transform data with Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/6-exercise-transform-data-spark-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Transform data with Spark in Azure Synapse Analytics\n30 minutes\n\nNow it's your chance to use Spark to transform data for yourself. In this exercise, you’ll use a Spark notebook in Azure Synapse Analytics to transform data in files.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich method of the Dataframe object is used to save a dataframe as a file?\n\n \n\ntoFile()\n\nwrite()\n\nsave()\n\n2. \n\nWhich method is used to split the data across folders when saving a dataframe?\n\n \n\nsplitBy()\n\ndistributeBy()\n\npartitionBy()\n\n3. \n\nWhat happens if you drop an external table that is based on existing files?\n\n \n\nAn error – you must delete the files first\n\nThe table is dropped from the metastore but the files remain unaffected\n\nThe table is dropped from the metastore and the files are deleted\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data with SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/4-tramsform-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTransform data with SQL\n5 minutes\n\nThe SparkSQL library, which provides the dataframe structure also enables you to use SQL as a way of working with data. With this approach, You can query and transform data in dataframes by using SQL queries, and persist the results as tables.\n\n Note\n\nTables are metadata abstractions over files. The data is not stored in a relational table, but the table provides a relational layer over files in the data lake.\n\nDefine tables and views\n\nTable definitions in Spark are stored in the metastore, a metadata layer that encapsulates relational abstractions over files. External tables are relational tables in the metastore that reference files in a data lake location that you specify. You can access this data by querying the table or by reading the files directly from the data lake.\n\n Note\n\nExternal tables are \"loosely bound\" to the underlying files and deleting the table does not delete the files. This allows you to use Spark to do the heavy lifting of transformation then persist the data in the lake. After this is done you can drop the table and downstream processes can access these optimized structures. You can also define managed tables, for which the underlying data files are stored in an internally managed storage location associated with the metastore. Managed tables are \"tightly-bound\" to the files, and dropping a managed table deletes the associated files.\n\nThe following code example saves a dataframe (loaded from CSV files) as an external table name sales_orders. The files are stored in the /sales_orders_table folder in the data lake.\n\norder_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')\n\nUse SQL to query and transform the data\n\nAfter defining a table, you can use of SQL to query and transform its data. The following code creates two new derived columns named Year and Month and then creates a new table transformed_orders with the new derived columns added.\n\n# Create derived columns\nsql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\n\n# Save the results\nsql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')\n\n\nThe data files for the new table are stored in a hierarchy of folders with the format of Year=*NNNN* / Month=*N*, with each folder containing a parquet file for the corresponding orders by year and month.\n\nQuery the metastore\n\nBecause this new table was created in the metastore, you can use SQL to query it directly with the %%sql magic key in the first line to indicate that the SQL syntax will be used as shown in the following script:\n\n%%sql\n\nSELECT * FROM transformed_orders\nWHERE Year = 2021\n    AND Month = 1\n\nDrop tables\n\nWhen working with external tables, you can use the DROP command to delete the table definitions from the metastore without affecting the files in the data lake. This approach enables you to clean up the metastore after using SQL to transform the data, while making the transformed data files available to downstream data analysis and ingestion processes.\n\n%%sql\n\nDROP TABLE transformed_orders;\nDROP TABLE sales_orders;\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Partition data files - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/3-partition-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPartition data files\n5 minutes\n\nPartitioning is an optimization technique that enables spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO.\n\nPartition the output file\n\nTo save a dataframe as a partitioned set of files, use the partitionBy method when writing the data.\n\nThe following example creates a derived Year field. Then uses it to partition the data.\n\nfrom pyspark.sql.functions import year, col\n\n# Load source data\ndf = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n\n# Add Year column\ndated_df = df.withColumn(\"Year\", year(col(\"OrderDate\")))\n\n# Partition by year\ndated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\")\n\n\nThe folder names generated when partitioning a dataframe include the partitioning column name and value in a column=value format, as shown here:\n\n Note\n\nYou can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you could partition the order in the example by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value.\n\nFilter parquet files in a query\n\nWhen reading data from parquet files into a dataframe, you have the ability to pull data from any folder within the hierarchical folders. This filtering process is done with the use of explicit values and wildcards against the partitioned fields.\n\nIn the following example, the following code will pull the sales orders, which were placed in 2020.\n\norders_2020 = spark.read.parquet('/partitioned_data/Year=2020')\ndisplay(orders_2020.limit(5))\n\n\n Note\n\nThe partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a Year column - all rows would be from 2020.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Modify and save dataframes - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/2-transform-dataframe",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nModify and save dataframes\n5 minutes\n\nApache Spark provides the dataframe object as the primary structure for working with data. You can use dataframes to query and transform data, and persist the results in a data lake. To load data into a dataframe, you use the spark.read function, specifying the file format, path, and optionally the schema of the data to be read. For example, the following code loads data from all .csv files in the orders folder into a dataframe named order_details and then displays the first five records.\n\norder_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\ndisplay(order_details.limit(5))\n\nTransform the data structure\n\nAfter loading the source data into a dataframe, you can use the dataframe object's methods and Spark functions to transform it. Typical operations on a dataframe include:\n\nFiltering rows and columns\nRenaming columns\nCreating new columns, often derived from existing ones\nReplacing null or other values\n\nIn the following example, the code uses the split function to separate the values in the CustomerName column into two new columns named FirstName and LastName. Then it uses the drop method to delete the original CustomerName column.\n\nfrom pyspark.sql.functions import split, col\n\n# Create the new FirstName and LastName fields\ntransformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Remove the CustomerName field\ntransformed_df = transformed_df.drop(\"CustomerName\")\n\ndisplay(transformed_df.limit(5))\n\n\nYou can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\n\nSave the transformed data\n\nAfter your dataFrame is in the required structure, you can save the results to a supported format in your data lake.\n\nThe following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name.\n\ntransformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\nprint (\"Transformed data saved!\")\n\n\n\n Note\n\nThe Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nApache Spark provides a powerful platform for performing data cleansing and transformation tasks on large volumes of data. By using the Spark dataframe object, you can easily load data from files in a data lake and perform complex modifications. You can then save the transformed data back to the data lake for downstream processing or ingestion into a data warehouse.\n\nAzure Synapse Analytics provides Apache Spark pools that you can use to run Spark workloads to transform data as part of a data ingestion and preparation workload. You can use natively supported notebooks to write and run code on a Spark pool to prepare data for analysis. You can then use other Azure Synapse Analytics capabilities such as SQL pools to work with the transformed data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data with Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nTransform data with Spark in Azure Synapse Analytics\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nData engineers commonly need to transform large volumes of data. Apache Spark pools in Azure Synapse Analytics provide a distributed processing platform that they can use to accomplish this goal.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nUse Apache Spark to modify and save dataframes\nPartition data files for improved performance and scalability.\nTransform data with SQL\nAdd\nPrerequisites\n\nBefore taking this module, you should be familiar with Apache Spark pools in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first.\n\nIntroduction\nmin\nModify and save dataframes\nmin\nPartition data files\nmin\nTransform data with SQL\nmin\nExercise: Transform data with Spark in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark is a key technology used in big data analytics, and the Spark pool support in Azure Synapse Analytics enables you to combine big data processing in Spark with large-scale data warehousing in SQL.\n\nIn this module, you learned how to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition best describes Apache Spark?\n\n \n\nA highly scalable relational database management system.\n\nA virtual server with a Python runtime.\n\nA distributed platform for parallel data processing using multiple languages.\n\n2. \n\nYou need to use Spark to analyze data in a parquet file. What should you do?\n\n \n\nLoad the parquet file into a dataframe.\n\nImport the data into a table in a serverless SQL pool.\n\nConvert the data to CSV format.\n\n3. \n\nYou want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?\n\n \n\n%%spark\n\n%%pyspark\n\n%%sql\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/6-exercise-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Analyze data with Spark\n45 minutes\n\nNow it's your opportunity to use a Spark pool in Azure Synapse Analytics. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a Spark pool to analyze and visualize data from files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/5-visualize-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nVisualize data with Spark\n5 minutes\n\nOne of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Synapse Analytics provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.\n\nUsing built-in notebook charts\n\nWhen you display a dataframe or run a SQL query in a Spark notebook in Azure Synapse Analytics, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here:\n\nThe built-in charting functionality in notebooks is useful when you're working with results of a query that don't include any existing groupings or aggregations, and you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.\n\nUsing graphics packages in code\n\nThere are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n\nFor example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.\n\nfrom matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n\n\nThe Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n\nThe chart produced by the code would look similar to the following image:\n\nYou can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/4-write-spark-code",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nAnalyze data with Spark\n5 minutes\n\nOne of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Synapse Analytics Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java and Microsoft .NET.\n\nExploring data with dataframes\n\nNatively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.\n\n Note\n\nIn addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.\n\nLoading data into a dataframe\n\nLet's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the primary storage account for an Azure Synapse Analytics workspace:\n\nProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nIn a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:\n\n%%pyspark\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n\n\nThe %%pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example:\n\n%%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://container@store.dfs.core.windows.net/products.csv\")\ndisplay(df.limit(10))\n\n\nThe magic %%spark is used to specify Scala.\n\nBoth of these code samples would produce output like this:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nSpecifying a dataframe schema\n\nIn the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nThe following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n\n\nThe results would once again be similar to:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nFiltering and grouping dataframes\n\nYou can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:\n\npricelist_df = df.select(\"ProductID\", \"ListPrice\")\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductID\tListPrice\n771\t3399.9900\n772\t3399.9900\n773\t3399.9900\n...\t...\n\nIn common with most data manipulation methods, select returns a new dataframe object.\n\n Tip\n\nSelecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n\npricelist_df = df[\"ProductID\", \"ListPrice\"]\n\nYou can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n\nbikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\n\nTo group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:\n\ncounts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nCategory\tcount\nHeadsets\t3\nWheels\t14\nMountain Bikes\t32\n...\t...\nUsing SQL expressions in Spark\n\nThe Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\n\nCreating database objects in the Spark catalog\n\nThe Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\n\nOne of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:\n\ndf.createOrReplaceTempView(\"products\")\n\n\nA view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.\n\n Note\n\nWe won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:\n\nYou can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.\nYou can save a dataframe as a table by using its saveAsTable method.\nYou can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.\nUsing the Spark SQL API to query data\n\nYou can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.\n\nbikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n\n\nThe results from the code example would look similar to the following table:\n\nExpand table\nProductID\tProductName\tListPrice\n38\tMountain-100 Silver, 38\t3399.9900\n52\tRoad-750 Black, 52\t539.9900\n...\t...\t...\nUsing SQL code\n\nThe previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this:\n\n%%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n\n\nThe SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:\n\nExpand table\nCategory\tProductCount\nBib-Shorts\t3\nBike Racks\t1\nBike Stands\t1\n...\t...\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/3-use-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark in Azure Synapse Analytics\n3 minutes\n\nYou can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:\n\nBatch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.\nInteractive analytics sessions to explore, analyze, and visualize data.\nRunning Spark code in notebooks\n\nAzure Synapse Studio includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Synapse Studio is similar to that of Jupyter notebooks - a popular open source notebook platform.\n\n Note\n\nWhile usually used interactively, notebooks can be included in automated pipelines and run as an unattended script.\n\nNotebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:\n\nSyntax highlighting and error support.\nCode auto-completion​.\nInteractive data visualizations.\nThe ability to export results.\n\n Tip\n\nTo learn more about working with notebooks in Azure Synapse Analytics, see the Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nAccessing data from a Synapse Spark pool\n\nYou can use Spark in Azure Synapse Analytics to work with data from various sources, including:\n\nA data lake based on the primary storage account for the Azure Synapse Analytics workspace.\nA data lake based on storage defined as a linked service in the workspace.\nA dedicated or serverless SQL pool in the workspace.\nAn Azure SQL or SQL Server database (using the Spark connector for SQL Server)\nAn Azure Cosmos DB analytical database defined as a linked service and configured using Azure Synapse Link for Cosmos DB.\nAn Azure Data Explorer Kusto database defined as a linked service in the workspace.\nAn external Hive metastore defined as a linked service in the workspace.\n\nOne of the most common uses of Spark is to work with data in a data lake, where you can read and write files in multiple commonly used formats, including delimited text, Parquet, Avro, and others.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nApache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become extremely popular in \"big data\" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Databricks, and Azure Synapse Analytics.\n\nThis module explores how you can use Spark in Azure Synapse Analytics to ingest, process, and analyze data from a data lake. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other Synapse analytical runtimes are specific to Azure Synapse Analytics.\n\nAfter completing this module, you'll be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get to know Apache Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/2-get-to-know-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet to know Apache Spark\n3 minutes\n\nApache Spark is distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster.\n\nHow Spark works\n\nApache Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). The SparkContext connects to the cluster manager, which allocates resources across applications using an implementation of Apache Hadoop YARN. Once connected, Spark acquires executors on nodes in the cluster to run your application code.\n\nThe SparkContext runs the main function and parallel operations on the cluster nodes, and then collects the results of the operations. The nodes read and write data from and to the file system and cache transformed data in-memory as Resilient Distributed Datasets (RDDs).\n\nThe SparkContext is responsible for converting an application to a directed acyclic graph (DAG). The graph consists of individual tasks that get executed within an executor process on the nodes. Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.\n\nSpark pools in Azure Synapse Analytics\n\nIn Azure Synapse Analytics, a cluster is implemented as a Spark pool, which provides a runtime for Spark operations. You can create one or more Spark pools in an Azure Synapse Analytics workspace by using the Azure portal, or in Azure Synapse Studio. When defining a Spark pool, you can specify configuration options for the pool, including:\n\nA name for the spark pool.\nThe size of virtual machine (VM) used for the nodes in the pool, including the option to use hardware accelerated GPU-enabled nodes.\nThe number of nodes in the pool, and whether the pool size is fixed or individual nodes can be brought online dynamically to auto-scale the cluster; in which case, you can specify the minimum and maximum number of active nodes.\nThe version of the Spark Runtime to be used in the pool; which dictates the versions of individual components such as Python, Java, and others that get installed.\n\n Tip\n\nFor more information about Spark pool configuration options, see Apache Spark pool configurations in Azure Synapse Analytics in the Azure Synapse Analytics documentation.\n\nSpark pools in an Azure Synapse Analytics Workspace are serverless - they start on-demand and stop when idle.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Apache Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nAnalyze data with Apache Spark in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nApache Spark is a core technology for large-scale data analytics. Learn how to use Spark in Azure Synapse Analytics to analyze and visualize data in a data lake.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\nAdd\nPrerequisites\n\nIf you are not already familiar with Azure Synapse Analytics, consider completing the Introduction to Azure Synapse Analytics module before starting this module.\n\nIntroduction\nmin\nGet to know Apache Spark\nmin\nUse Spark in Azure Synapse Analytics\nmin\nAnalyze data with Spark\nmin\nVisualize data with Spark\nmin\nExercise - Analyze data with Spark\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich authentication method would be the likeliest choice to use for an individual who needs to access your serverless SQL pool who works for an external organization?\n\n \n\nLocal authentication.\n\nSQL Authentication.\n\nMicrosoft Entra ID.\n\n2. \n\nWhich Azure Synapse Studio hub is where you assign administrator privileges to an Azure Synapse workspace?\n\n \n\nManage.\n\nData.\n\nDevelop.\n\n3. \n\nWhich role enables a user to create external table as select (CETAS) against an Azure Data Lake Gen2 data store?\n\n \n\nStorage Blob Data Reader.\n\nStorage Blob Data Contributor.\n\nExecutor.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this lesson, you will learn how you can set up security when using Azure Synapse serverless SQL pools by:\n\nChoosing an authentication method in Azure Synapse serverless SQL pools\nManaging users in Azure Synapse serverless SQL pools\nManaging user permissions in Azure Synapse serverless SQL pools\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage user permissions in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/4-manage-user-permissions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage user permissions in Azure Synapse serverless SQL pools\n3 minutes\n\nTo secure data, Azure Storage implements an access control model that supports both Azure role-based access control (Azure RBAC) and access control lists (ACLs) like Portable Operating System Interface for Unix (POSIX)\n\nYou can associate a security principal with an access level for files and directories. These associations are captured in an access control list (ACL). Each file and directory in your storage account has an access control list. When a security principal attempts an operation on a file or directory, an ACL check determines whether that security principal (user, group, service principal, or managed identity) has the correct permission level to perform the operation.\n\nThere are two kinds of access control lists:\n\nAccess ACLs\n\nControls access to an object. Files and directories both have access ACLs.\n\nDefault ACLs\n\nAre templates of ACLs associated with a directory that determine the access ACLs for any child items that are created under that directory. Files do not have default ACLs.\n\nBoth access ACLs and default ACLs have the same structure.\n\nThe permissions on a container object are Read, Write, and Execute, and they can be used on files and directories as shown in the following table:\n\nLevels of permissions\n\nExpand table\nPermission\tFile\tDirectory\nRead (R)\tCan read the contents of a file\tRequires Read and Execute to list the contents of the directory\nWrite (W)\tCan write or append to a file\tRequires Write and Execute to create child items in a directory\nExecute (X)\tDoes not mean anything in the context of Data Lake Storage Gen2\tRequired to traverse the child items of a directory\nGuidelines in setting up ACLs\n\nAlways use Microsoft Entra security groups as the assigned principal in an ACL entry. Resist the opportunity to directly assign individual users or service principals. Using this structure will allow you to add and remove users or service principals without the need to reapply ACLs to an entire directory structure. Instead, you can just add or remove users and service principals from the appropriate Microsoft Entra security group.\n\nThere are many ways to set up groups. For example, imagine that you have a directory named /LogData which holds log data that is generated by your server. Azure Data Factory (ADF) ingests data into that folder. Specific users from the service engineering team will upload logs and manage other users of this folder, and various Databricks clusters will analyze logs from that folder.\n\nTo enable these activities, you could create a LogsWriter group and a LogsReader group. Then, you could assign permissions as follows:\n\nAdd the LogsWriter group to the ACL of the /LogData directory with rwx permissions.\nAdd the LogsReader group to the ACL of the /LogData directory with r-x permissions.\nAdd the service principal object or Managed Service Identity (MSI) for ADF to the LogsWriters group.\nAdd users in the service engineering team to the LogsWriter group.\nAdd the service principal object or MSI for Databricks to the LogsReader group.\n\nIf a user in the service engineering team leaves the company, you could just remove them from the LogsWriter group. If you did not add that user to a group, but instead, you added a dedicated ACL entry for that user, you would have to remove that ACL entry from the /LogData directory. You would also have to remove the entry from all subdirectories and files in the entire directory hierarchy of the /LogData directory.\n\nRoles necessary for serverless SQL pool users\n\nFor users which need read only access you should assign role named Storage Blob Data Reader.\n\nFor users which need read/write access you should assign role named Storage Blob Data Contributor. Read/Write access is needed if user should have access to create external table as select (CETAS).\n\n Note\n\nIf user has a role Owner or Contributor, that role is not enough. Azure Data Lake Storage gen 2 has super-roles which should be assigned.\n\nDatabase level permission\n\nTo provide more granular access to the user, you should use Transact-SQL syntax to create logins and users.\n\nTo grant access to a user to a single serverless SQL pool database, follow the steps in this example:\n\nCreate LOGIN\n\nuse master\nCREATE LOGIN [alias@domain.com] FROM EXTERNAL PROVIDER;\n\n\nCreate USER\n\nuse yourdb -- Use your DB name\nCREATE USER alias FROM LOGIN [alias@domain.com];\n\n\nAdd USER to members of the specified role\n\nuse yourdb -- Use your DB name\nalter role db_datareader \nAdd member alias -- Type USER name from step 2\n-- You can use any Database Role which exists \n-- (examples: db_owner, db_datareader, db_datawriter)\n-- Replace alias with alias of the user you would like to give access and domain with the company domain you are using.\n\nServer level permission\n\nTo grant full access to a user to all serverless SQL pool databases, follow the step in this example:\n\nCREATE LOGIN [alias@domain.com] FROM EXTERNAL PROVIDER;\nALTER SERVER ROLE sysadmin ADD MEMBER [alias@domain.com];\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage users in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/3-manage-users",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage users in Azure Synapse serverless SQL pools\n3 minutes\n\nYou can give administrator privileges to a user to Azure Synapse serverless SQL pool. To do this you should open the Azure Synapse workspace and do the following steps:\n\nGo to Manage menu\n\nGo to Access control\n\nClick on Add\n\nChoose Synapse Administrator\n\nSelect a User or Security group (a security group is the recommended option here)\n\nClick Apply\n\nNow this user or group is the administrator of the Azure Synapse workspace and serverless SQL pool.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this lesson, you will learn how you can set up security when using Azure Synapse serverless SQL pools\n\nAfter the completion of this lesson, you will be able to:\n\nChoose an authentication method in Azure Synapse serverless SQL pools\nManage users in Azure Synapse serverless SQL pools\nManage user permissions in Azure Synapse serverless SQL pools\nPrerequisites\n\nBefore taking this lesson, it is recommended that the student is able to:\n\nLog into the Azure portal\nExplain the different components of Azure Synapse Analytics\nUse Azure Synapse Studio\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose an authentication method in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/2-choose-authentication-method",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nChoose an authentication method in Azure Synapse serverless SQL pools\n3 minutes\n\nServerless SQL pool authentication refers to how users prove their identity when connecting to the endpoint. Two types of authentication are supported:\n\nSQL Authentication\n\nThis authentication method uses a username and password.\n\nMicrosoft Entra authentication\n\nThis authentication method uses identities managed by Microsoft Entra ID. For Microsoft Entra users, multi-factor authentication can be enabled. Use Active Directory authentication (integrated security) whenever possible.\n\nAuthorization\n\nAuthorization refers to what a user can do within a serverless SQL pool database and is controlled by your user account's database role memberships and object-level permissions.\n\nIf SQL Authentication is used, the SQL user exists only in the serverless SQL pool and permissions are scoped to the objects in the serverless SQL pool. Access to securable objects in other services (such as Azure Storage) can't be granted to a SQL user directly since it only exists in scope of serverless SQL pool. The SQL user needs get authorization to access the files in the storage account.\n\nIf Microsoft Entra authentication is used, a user can sign in to a serverless SQL pool and other services, like Azure Storage, and can grant permissions to the Microsoft Entra user.\n\nAccess to storage accounts\n\nA user that is logged into the serverless SQL pool service must be authorized to access and query the files in Azure Storage. Serverless SQL pool supports the following authorization types:\n\nAnonymous access\n\nTo access publicly available files placed on Azure storage accounts that allow anonymous access.\n\nShared access signature (SAS)\n\nProvides delegated access to resources in storage account. With a SAS, you can grant clients access to resources in storage account, without sharing account keys. A SAS gives you granular control over the type of access you grant to clients who have the SAS: validity interval, granted permissions, acceptable IP address range, acceptable protocol (https/http).\n\nManaged Identity.\n\nIs a feature of Microsoft Entra ID that provides Azure services for serverless SQL pool. Also, it deploys an automatically managed identity in Microsoft Entra ID. This identity can be used to authorize the request for data access in Azure Storage. Before accessing the data, the Azure Storage administrator must grant permissions to Managed Identity for accessing the data. Granting permissions to Managed Identity is done the same way as granting permission to any other Microsoft Entra user.\n\nUser Identity\n\nAlso known as \"pass-through\", is an authorization type where the identity of the Microsoft Entra user that logged into serverless SQL pool is used to authorize access to the data. Before accessing the data, Azure Storage administrator must grant permissions to Microsoft Entra user for accessing the data. This authorization type uses the Microsoft Entra user that logged into serverless SQL pool, therefore it's not supported for SQL user types.\n\nSupported authorization types for database users can be found in the table below:\n\nExpand table\nAuthorization type\tSQL user\tMicrosoft Entra user\nUser Identity\tNot supported\tSupported\nSAS\tSupported\tSupported\nManaged Identity\tNot supported\tSupported\n\nSupported storage and authorization types can be found in the table below:\n\nExpand table\nAuthorization type\tBlob Storage\tADLS Gen1\tADLS Gen2\nUser Identity\tSupported - SAS token can be used to access storage that is not protected with firewall\tNot supported\tSupported - SAS token can be used to access storage that is not protected with firewall\nSAS\tSupported\tSupported\tSupported\nManaged Identity\tSupported\tSupported\tSupported\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Secure data and manage users in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nSecure data and manage users in Azure Synapse serverless SQL pools\nModule\n6 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how you can set up security when using Azure Synapse serverless SQL pools\n\nLearning objectives\n\nAfter the completion of this module, you will be able to:\n\nChoose an authentication method in Azure Synapse serverless SQL pools\nManage users in Azure Synapse serverless SQL pools\nManage user permissions in Azure Synapse serverless SQL pools\nAdd\nPrerequisites\nIt is recommended that students have completed Data Fundamentals before starting this learning path.\nIntroduction\nmin\nChoose an authentication method in Azure Synapse serverless SQL pools\nmin\nManage users in Azure Synapse serverless SQL pools\nmin\nManage user permissions in Azure Synapse serverless SQL pools\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nA lake database can provide the benefits of a relational schema and query interface with the flexibility of file storage in a data lake.\n\nIn this module, you learned how to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nLearn more\n\nTo learn more about lake databases, refer to the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich if the following statements is true of a lake database?\n\n \n\nData is stored in a relational database store and cannot be directly accessed in the data lake files.\n\nData is stored in files that cannot be queried using SQL.\n\nA relational schema is overlaid on the underlying files, and can be queried using a serverless SQL pool or a Spark pool.\n\n2. \n\nYou need to create a new lake database for a retail solution. What's the most efficient way to do this?\n\n \n\nCreate a sample database in Azure SQL Database and export the SQL scripts to create the schema for the lake database.\n\nStart with the Retail database template in Azure Synapse Studio, and adapt it as necessary.\n\nStart with an empty database and create a normalized schema.\n\n3. \n\nYou have Parquet files in an existing data lake folder for which you want to create a table in a lake database. What should you do?\n\n \n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) query to create the table.\n\nConvert the files in the folder to CSV format.\n\nUse the database designer to create a table based on the existing folder.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Analyze data in a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/6-exercise-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Analyze data in a lake database\n45 minutes\n\nNow it's your opportunity to create and use a lake database. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then create a lake database in Azure Synapse Studio.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/5-use-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a lake database\n3 minutes\n\nAfter creating a lake database, you can store data files that match the table schemas in the appropriate folders in the data lake, and query them using SQL.\n\nUsing a serverless SQL pool\n\nYou can query a lake database in a SQL script by using a serverless SQL pool.\n\nFor example, suppose a lake database named RetailDB contains an Customer table. You could query it using a standard SELECT statement like this:\n\nUSE RetailDB;\nGO\n\nSELECT CustomerID, FirstName, LastName\nFROM Customer\nORDER BY LastName;\n\n\nThere is no need to use an OPENROWSET function or include any additional code to access the data from the underlying file storage. The serverless SQL pool handles the mapping to the files for you.\n\nUsing an Apache Spark pool\n\nIn addition to using a serverless SQL pool, you can work with lake database tables using Spark SQL in an Apache Spark pool.\n\nFor example, you could use the following code to insert a new customer record into the Customer table.\n\n%%sql\nINSERT INTO `RetailDB`.`Customer` VALUES (123, 'John', 'Yang')\n\n\nYou could then use the following code to query the table:\n\n%%sql\nSELECT * FROM `RetailDB`.`Customer` WHERE CustomerID = 123\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore database templates - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/3-database-templates",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExplore database templates\n3 minutes\n\nYou can create a Lake database from an empty schema, to which you add definitions for tables and the relationships between them. However, Azure Synapse Analytics provides a comprehensive collection of database templates that reflect common schemas found in multiple business scenarios; including:\n\nAgriculture\nAutomotive\nBanking\nConsumer goods\nEnergy and commodity trading\nFreight and logistics\nFund management\nHealthcare insurance\nHealthcare provider\nManufacturing\nRetail\nand many others...\n\nYou can use one of the enterprise database templates as the starting point for creating your lake database, or you can start with a blank schema and add and modify tables from the templates as required.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/4-create-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a lake database\n3 minutes\n\nYou can create a lake database using the lake database designer in Azure Synapse Studio. Start by adding a new lake database on the Data page, selecting a template from the gallery or starting with a blank lake database; and then add and customize tables using the visual database designer interface.\n\nAs you create each table, you can specify the type and location of the files you want to use to store the underlying data, or you can create a table from existing files that are already in the data lake. In most cases, it's advisable to store all of the database files in a consistent format within the same root folder in the data lake.\n\nDatabase designer\n\nThe database designer interface in Azure Synapse Studio provides a drag-and-drop surface on which you can edit the tables in your database and the relationships between them.\n\nUsing the database designer, you can define the schema for your database by adding or removing tables and:\n\nSpecifying the name and storage settings for each table.\nSpecifying the names, key usage, nullability, and data types for each column.\nDefining relationships between key columns in tables.\n\nWhen your database schema is ready for use, you can publish the database and start using it.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand lake database concepts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/2-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand lake database concepts\n3 minutes\n\nIn a traditional relational database, the database schema is composed of tables, views, and other objects. Tables in a relational database define the entities for which data is stored - for example, a retail database might include tables for products, customers, and orders. Each entity consists of a set of attributes that are defined as columns in the table, and each column has a name and a data type. The data for the tables is stored in the database, and is tightly coupled to the table definition; which enforces data types, nullability, key uniqueness, and referential integrity between related keys. All queries and data manipulations must be performed through the database system.\n\nIn a data lake, there is no fixed schema. Data is stored in files, which may be structured, semi-structured, or unstructured. Applications and data analysts can work directly with the files in the data lake using the tools of their choice; without the constraints of a relational database system.\n\nA lake database provides a relational metadata layer over one or more files in a data lake. You can create a lake database that includes definitions for tables, including column names and data types as well as relationships between primary and foreign key columns. The tables reference files in the data lake, enabling you to apply relational semantics to working with the data and querying it using SQL. However, the storage of the data files is decoupled from the database schema; enabling more flexibility than a relational database system typically offers.\n\nLake database schema\n\nYou can create a lake database in Azure Synapse Analytics, and define the tables that represent the entities for which you need to store data. You can apply proven data modeling principles to create relationships between tables and use appropriate naming conventions for tables, columns, and other database objects.\n\nAzure Synapse Analytics includes a graphical database design interface that you can use to model complex database schema, using many of the same best practices for database design that you would apply to a traditional database.\n\nLake database storage\n\nThe data for the tables in your lake database is stored in the data lake as Parquet or CSV files. The files can be managed independently of the database tables, making it easier to manage data ingestion and manipulation with a wide variety of data processing tools and technologies.\n\nLake database compute\n\nTo query and manipulate the data through the tables you have defined, you can use an Azure Synapse serverless SQL pool to run SQL queries or an Azure Synapse Apache Spark pool to work with the tables using the Spark SQL API.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nData analysts and engineers often find themselves forced to choose between the flexibility of storing data files in a data lake, with the advantages of a structured schema in a relational database. Lake databases in Azure Synapse Analytics provide a way to combine these two approaches and benefit from an explicit relational schema of tables, views, and relationships that is decoupled from file-based storage.\n\nIn this module, you'll learn how to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with the Microsoft Azure portal\nFamiliarity with data lake and data warehouse concepts\nExperience of using SQL to query database tables\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a lake database in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate a lake database in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nWhy choose between working with files in a data lake or a relational database schema? With lake databases in Azure Synapse Analytics, you can combine the benefits of both.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nAdd\nPrerequisites\n\nConsider completing the Explore data analytics in Azure and Get started querying with Transact-SQL learning paths before starting this module. You will need knowledge of:\n\nAnalytical data workloads in Microsoft Azure\nQuerying data with Transact-SQL\nIntroduction\nmin\nUnderstand lake database concepts\nmin\nExplore database templates\nmin\nCreate a lake database\nmin\nUse a lake database\nmin\nExercise - Analyze data in a lake database\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nBy using the CREATE EXTERNAL TABLE AS statement, you can use Azure Synapse serverless SQL pool to transform data as part of a data ingestion pipeline or an extract, transform, and load (ETL) process. The transformed data is persisted in files in the data lake with a relational table based on the file location; enabling you to work with the transformed data using SQL in the serverless SQL database, or directly in the file data lake.\n\nIn this lesson, you learned how to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\n\n Tip\n\nFor more information about using the CETAS statement, see CETAS with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]