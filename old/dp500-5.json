[
  {
    "title": "Define use cases for dataflows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/2-define-use-cases-for-dataflows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDefine use cases for dataflows\n9 minutes\n\nPower BI dataflows enable you to build reusable data tables in a workspace using Power Query Online, and share them for use in other reports and with other users for reuse in other workspaces. Dataflows are objects in a workspace alongside datasets, dashboards, reports, and workbooks. When a Power BI dataflow is refreshed, behind the scenes it loads its data into files located in a data lake, Azure Data Lake Storage Gen 2 (ADLS Gen 2).\n\nPower BI dataflows should be used in Premium capacity for enterprise solutions, to take advantage of features like advanced compute, incremental refresh, and linked and computed entities.\n\n Note\n\nDataflows are supported for Power BI Pro, Premium Per User (PPU), and Power BI Premium users. Learn more about Premium-only features of dataflows.\n\nData used with Power BI is stored in internal storage provided by Power BI by default. With the integration of dataflows and Azure Data Lake Storage Gen 2 (ADLS Gen2), you can store your dataflows in your organization's Azure Data Lake Storage Gen2 account. This essentially allows you to \"bring your own storage\" to Power BI dataflows, and establish a connection at the tenant or workspace level.\n\nWhy use dataflows?\n\nDataflows were designed to promote reusable ETL logic that prevents the need to create additional connections to your data source.\n\nDataflows are a great choice for you if:\n\nThere's no data warehouse in your organization.\nYou want to extend a core dataset or data in the data warehouse with consistent data.\nSelf service users need frequent access to an up-to-date subset of data from the data warehouse without having access to the data warehouse itself.\nYou have slower data sources.\nDataflows extract data once and reuse it multiple times, which can reduce the overall data refresh time for slower data sources.\nComputed entities may be faster than referencing queries with the enhanced compute engine.\nYou have chargeable data sources.\nDataflows can reduce costs associated with data refresh if you're getting data from chargeable data sources.\nDataflows increase control and reduce the number of calls to the source system.\nDatasets refresh against dataflows without affecting source systems.\nYou have different versions of datasets floating around your organization. Dataflows increase consistency between datasets.\nIncreased structural consistency by reducing the chance that users will prepare data differently\nIncreased temporal consistency by having a single set of data extracted from source systems at a single point in time\nShared tables that have no source, such as a standard date dimension, can be standardized across your organization.\nYou want to reduce or hide the complexity of data sources.\nYou can expose common data entities for larger groups of analysts that have already been transformed and simplified.\nYou can also partition data horizontally, using multiple data flows. For example, upstream dataflows contain all data and are available only to a small group of users. Downstream dataflows then contain curated subsets of data, and can be made available to members of appropriate security groups.\nBenefits and limitations\n\nWhile there are notable benefits to using dataflows in your dataset design, there are also a few limitations that users should keep in mind.\n\nBenefits:\n\nReduced load on database queries.\nReduced number of users accessing source data.\nProvides single version of properly structured data for analysts to build reports from.\n\nLimitations:\n\nNot a replacement for a data warehouse.\nRow-level security isn't supported.\nIf not using dataflows in Premium capacity, performance can be an issue.\n\n Important\n\nSee Dataflow considerations and limitations for a complete list of considerations and limitations.\n\nDataflows in Power BI Premium\n\nPower BI Premium was designed for enterprise deployments. Dataflow features available in premium offer substantial performance benefits and include the use of:\n\nEnhanced compute engine\nDirectQuery\nComputed entities\nLinked entities\nIncremental refresh\nOptimize dataflows using the enhanced compute engine\n\nThe enhanced compute engine in Power BI dataflows enables you to optimize the use of dataflows by:\n\nSpeeding up refresh operations when computed entities or linked entities are involved (for example, performing joins, distinct, filters, and group by).\nEnabling DirectQuery connectivity over dataflows using the compute engine.\nAchieve improved performance in the transformation steps of dataflows when entities are cached within the compute engine.\n\n Tip\n\nLearn more about Power BI Premium features of dataflows.\n\nDistinction between dataflows\n\nPerhaps you've also heard of Azure Data Factory dataflows and you're wondering what the best type of dataflow is to use in your scenario.\n\nPower BI dataflows and Azure Data Factory (ADF) wrangling dataflows are often considered to do the same thing: extract data from source systems, transform data, and load the transformed data into a destination. They're both powered by Power Query online, but there are differences in these two types of dataflows. You can implement a solution that works with a combination of the two.\n\nWhen to use ADF wrangling dataflows or Power BI dataflows\n\nData transformation should always be done as close to the source as possible. If your analytics solution includes Azure Data Factory and you have the skills to implement transformations upstream of Power BI, you should.\n\nExpand table\nFeatures\tPower BI dataflows\tData Factory wrangling dataflows\nDestinations\tDataverse or Azure Data Lake Storage\tMany destinations\nPower Query transformation\tAll Power Query functions are supported\tA limited set of functions is supported\nSources\tMany sources are supported\tOnly a few sources\nScalability\tDepends on the Premium capacity and the use of the enhanced compute engine\tHighly scalable\n\n Tip\n\nLearn more about how Microsoft Power Platform dataflows and Azure Data Factory wrangling dataflows relate to each other.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create and manage scalable Power BI dataflows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate and manage scalable Power BI dataflows\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nCreate Power BI transformation logic for reuse across your organization with Power BI dataflows. Learn how to combine Power BI dataflows with Power BI Premium for scalable ETL, and practice creating and consuming dataflows.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI dataflows and use cases.\nDescribe best practices for implementing Power BI dataflows.\nCreate and consume Power BI dataflows.\nAdd\nPrerequisites\nYou'll need knowledge of Power BI data model design including star schema design basics.\nConsider completing the Model data in Power BI learning path.\nIntroduction\nmin\nDefine use cases for dataflows\nmin\nCreate reusable assets\nmin\nImplement best practices\nmin\nExercise: Create a dataflow\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat is the most efficient approach for data transformation when designing data models for scalability?\n\n \n\nData transformation should always be done using Power Query in Power BI.\n\nData transformation should be done as close to the source as possible, before reaching Power BI.\n\nData transformation should be done using DAX, after data is ingested and loaded to the model.\n\n2. \n\nWhich of the following best practices for Power BI data modeling is relevant only to DirectQuery models?\n\n \n\nSet relationships to enforce integrity using the assume referential integrity property on relationships.\n\nUse a star schema as opposed to wide tables.\n\nDisable auto date/time in Power BI Desktop.\n\n3. \n\nWhat are the data model size limits on a dataset with large dataset storage format determined by?\n\n \n\nPower BI Premium 10 GB size limit.\n\nPower BI Premium capacity size or the maximum size set by the administrator.\n\nThere are no size limits if large dataset storage format is enabled.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n2 minutes\n\nYou've learned that scalability and working with large data is achievable in Power BI. The correct model framework, a proper data model, and using Power BI Premium features like large dataset storage enables performant enterprise reporting in Power BI.\n\nLearn more\nPower BI enterprise documentation\nPower BI implementation planning: Advanced data model management\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure large datasets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4-configure-large-datasets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Understand scalability in Power BI \nAdd\nPrevious\nUnit 4 of 7\nNext\nConfigure large datasets\nCompleted\n100 XP\n4 minutes\n\nPower BI datasets store data in a highly compressed, in-memory cache for optimized query performance. Enterprise deployment of an analytics solution using Power BI will likely require Power BI Premium. With the large dataset storage format enabled, dataset sizes are limited only by the capacity size, or a maximum size set by the administrator. This differs from datasets in Power BI Premium, which are limited to 10 GB after compression if large dataset storage format isn't enabled.\n\nLarge datasets can be enabled for all Premium P SKUs, Embedded A SKUs, and with Premium Per User (PPU). The large dataset size limit in Premium is comparable to Azure Analysis Services, in terms of data model size limitations.\n\nThe large dataset feature brings the Power BI dataset cache sizes to parity with Azure Analysis Services model sizes. The large dataset feature enables consolidation of tabular models from SQL Server Analysis Services and Azure Analysis Services on one common platform based on Power BI Premium.\n\n Note\n\nTo use large dataset storage format, the dataset must be stored in a workspace that allocated to Premium capacity.\n\nEnabling the large dataset format enables fast user interactivity and allows data to grow beyond the 10-GB limit. Additionally, the large dataset format can also improve xmla write operation performance, even for datasets that may not be large.\n\n Important\n\nDatasets enabled for large models can't be downloaded as a Power BI Desktop (.pbix) file from the Power BI service. Read more about .pbix download limitations.\n\nEnable large dataset storage format\n\nTo take advantage of the large dataset storage format option, it must be enabled in the Power BI service. Here you can enable large dataset storage format for a single dataset, or for all datasets created in a workspace.\n\nEnable large dataset storage format for a single dataset\n\nIn the dataset settings in the Power BI service, toggle the slider to on and select Apply.\n\nEnable large dataset storage format for all datasets created in a workspace\n\nYou can set the default storage format for all datasets created in a workspace in the workspace settings. In the settings, select Premium, and select Large dataset storage format as the Default storage format.\n\nLarge dataset storage format for a workspace can also be enabled using PowerShell.\n\n Note\n\nSee Configure large datasets to learn more about large models in Power BI Premium including information on checking dataset size, dataset eviction, considerations, and limitations.\n\nNext unit: Exercise: Create a star schema model\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Create a star schema model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4a-exercise-create-star-schema-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Create a star schema model\n45 minutes\n\nNow it's your opportunity to try creating a star schema in Power BI yourself, connecting to an Azure Synapse Analytics dedicated SQL pool. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription, develop a dataset, and create a star schema model in Power BI desktop.\n\n Note\n\nTo complete this lab, you will need both an Azure subscription in which you have administrative access and a Power BI account. If you need a free trial Power BI account, sign up and follow the steps to create an account before continuing with this lab. Launch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement Power BI data modeling best practices - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/3-implement-data-modeling-best-practices",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nImplement Power BI data modeling best practices\n7 minutes\n\nImplementing data modeling best practices is key to performant, scalable data models.\n\nChoose the correct Power BI model framework\n\nChoosing the correct Power BI model framework is at the heart of building any scalable solution.\n\nThe first place to start with your Power BI data model is import mode. Import mode offers you the most options, design flexibility, and delivers fast performance.\n\nUse DirectQuery when your data source stores large volumes of data and/or your report needs to deliver near real-time data.\n\nFinally, use a composite model when you need to:\n\nBoost the query performance of a DirectQuery model.\nDeliver near real-time query results from an import model.\nExtend a Power BI dataset (or Azure Analysis Services model) with other data.\n\nComposite models combine data from more than one DirectQuery source or combine DirectQuery with import data.\n\n Important\n\nReview the Choose a Power BI model framework module for more information on using import, DirectQuery, or composite models.\n\nImplement data modeling best practices\n\nThere are some basic principles to abide by when building any data model. These principles become even more important as data begins to grow.\n\nMost importantly, you want to do as much data preparation work as possible before data reaches Power BI, as far upstream as possible. For example, if you have the opportunity to transform data in the data warehouse, that's where it should be done. Transformation at the source produces consistency for any other solutions built on that data and ensures that your Power BI model doesn't need to do any extra processing. This may require working with your data engineer or other members of the data team and is critically important.\n\nBest practices for import mode:\nAlways start with import mode if you can.\nOnly bring in data you need.\nRemove unnecessary rows and columns.\nOnly process what is absolutely necessary (tables/partitions) given the business requirements.\nAvoid wide tables.\nUse a star schema in Power BI.\nIf your source is a beautifully modeled data warehouse, you're a step ahead.\nBig data is often in wide flat tables. Take advantage of dimensional models for their performance benefits.\nPower BI supports multiple fact tables with different dimensionality and different granularities – you don’t have to put everything into one large table.\nPre-aggregate data before loading it to the model where possible.\nReduce the usage of calculated columns.\nData transformations requiring additional columns should be done as close to the source as possible.\nAvoid high cardinality columns.\nConsider breaking a datetime column into two columns, one for date and one for time.\nUse appropriate data types.\nUse integers instead of strings for ID columns.\nUse surrogate keys for ID columns if necessary.\nLimit the use of bi-directional filters on relationships.\nDisable auto date/time.\nConnect to a date table at the source or create your own date table.\nDisable attribute hierarchies for non-attribute columns.\nIf querying a relational database, query database views rather than tables.\nA view provides an abstraction layer to manage columns, and relates back to the first consideration, pushing transformations as close to the source as possible.\nViews shouldn't contain logic. They should only contain a SELECT statement from a table.\nConsider partitioning and incremental refresh to avoid loading data you don’t need to.\nCheck to ensure query folding is achieved.\nIf query folding isn't possible, you have another opportunity to work with the data engineer to move transformation upstream.\n\n Tip\n\nLearn more about techniques to help reduce the data loaded into import models.\n\nBest practices specific to DirectQuery mode:\nSet relationships to enforce integrity using the Assume referential integrity property on relationships.\nThe Assume Referential Integrity setting on relationships enables queries to use INNER JOIN statements rather than OUTER JOIN.\nLimit the use of bi-directional filters on relationships.\nUse only when necessary.\nLimit the complexity of DAX calculations.\nBecause query folding occurs by default in DirectQuery, more complex DAX measures means added complexity at the source, leading to slow queries.\nThe need for complex DAX also leads back to the key principle of applying transformations as far upstream as possible. You may need to work with the data engineer to apply transformations at the source.\nAvoid the use of calculated columns.\nTransformations requiring additional columns should be done as far upstream as possible, particularly when using DirectQuery.\nAvoid relationships on calculated columns\nAvoid relationships on Unique Identifier columns\nUse dual storage mode for dimensions related to fact tables that are in DirectQuery.\n\n Note\n\nRefer to the DirectQuery model guidance for a complete list of considerations in developing DirectQuery models.\n\nThere's also a tool you can use as you're developing tabular models that will alert you of modeling missteps or changes that would improve model design and performance. The Best Practice Analyzer within Tabular Editor was designed to help you design models that adhere to modeling best practices.\n\nIn the next unit, you'll learn how to configure the large dataset storage format using Power BI Premium.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe the significance of scalable models - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/2-describe-significance-of-scalable-models",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe the significance of scalable models\n4 minutes\n\nOne of the keys to manageable, performant solutions with large data is good model design. We'll discuss model scalability, why it's important, and what tools exist in Power BI to help you and your team accomplish your goals.\n\nWhat is enterprise or large-scale data?\n\nBefore we talk through scalability, let's define what we're talking about. You'll see throughout the module that we refer to enterprise-scale or large-scale data rather than big data. In this module, enterprise-scale or large-scale data refers to tables with a large number of records or rows. Power BI, used with tools like Azure Synapse Analytics, can analyze massive datasets, in the range of trillions of rows or petabytes of data.\n\nIf you're familiar with working with enterprise data, it may be helpful to understand that Power BI is the next generation of Analysis Services. It's the same technology under the hood of Analysis Services and Power BI datasets, the VertiPaq engine.\n\n Tip\n\nTake a look at the Model, query, and explore data in Azure Synapse learning path for more information on data analytics in Azure.\n\nWhat is scalability and why is it important?\n\nScalability in this context refers to building data models that can handle growth in the volume of data. A data model that ingests thousands of rows of data may grow to millions of rows over time, and the model must be designed to accommodate such growth. It's important to consider that your data will grow and/or change, which increases complexity.\n\nScalability must be at the forefront in enterprise solutions to ensure:\n\nFlexibility - models need to be able to accommodate change\nData growth - models must be able to handle an increase in data volume with acceptable report performance\nReduced complexity - models built with scalability in mind will be less complex and easier to manage\nHow do I design for scalability?\n\nThe best approach to building scalable Power BI data models will always be building with data modeling best practices in mind.\n\nBeyond the data model, Power BI Premium was designed specifically for enterprise deployments. Premium capacity offers greater storage capacity and allows for larger individual datasets depending on the SKU. Implementing the premium only large dataset storage feature enables data to grow beyond the Power BI desktop (.pbix) file size limitations.\n\n Tip\n\nAre you planning a Power BI enterprise deployment? Read the Power BI enterprise deployment whitepaper for a full list of enterprise deployment considerations.\n\nAnother important consideration in designing for scalability using Power BI Premium is choosing the right capacity. You'll need to work with your Power BI administrator to determine which Power BI Premium licensing SKU is available to you. If you're having performance issues in Premium capacity, work first to optimize your model, and then work with your Power BI administrator to monitor Power BI Premium capacities.\n\nAt the most basic level, it's important to understand that Premium capacities require sufficient memory for processing. You'll need to double the amount of RAM to process your data model refresh. For example, if you have a 40-GB dataset, you'll need at least 80-GB of memory available. A 40-GB dataset would be best supported by a P3/A6 capacity, which contains 100-GB of memory.\n\n Tip\n\nReview Power BI license types and capabilities. If you're not sure which license type your organization has, check with the Power BI administrator.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nPerhaps you've heard that Power BI can seamlessly handle trillions of rows of data, but you're not able to accomplish that in your own Power BI tenant. Working with high volume and large-scale data can be done in Power BI with the right groundwork in place.\n\nThis module introduces considerations for building enterprise scale, IT-driven solutions. You'll review data modeling best practices and Power BI Premium features for working with large data.\n\nLearning objectives\n\nIn this module, you will:\n\nDescribe the importance of building scalable data models\nImplement Power BI data modeling best practices\nUse the Power BI large dataset storage format\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand scalability in Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUnderstand scalability in Power BI\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nAzure Synapse Analytics\nPower BI\n\nScalable data models enable enterprise-scale analytics in Power BI. Implement data modeling best practices, use large dataset storage format, and practice building a star schema to design analytics solutions that can scale.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe the importance of building scalable data models\nImplement Power BI data modeling best practices\nUse the Power BI large dataset storage format\nAdd\nPrerequisites\n\nConsider completing the Model data in Power BI learning path. You will need knowledge of:\n\nPower BI data model design including star schema design basics\nIntroduction\nmin\nDescribe the significance of scalable models\nmin\nImplement Power BI data modeling best practices\nmin\nConfigure large datasets\nmin\nExercise: Create a star schema model\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nYou can develop your Power BI model based on three different frameworks: import, DirectQuery, and composite. Each framework has its own benefits and limitations, and features to help you optimize your model.\n\nUltimately, you should strive to develop a model that efficiently delivers fast performance with low latency, even for high volume data sources.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nGeoffrey is a data modeler at Adventure Works who developed a DirectQuery model that connects to the data warehouse. To improve the query performance of higher-grain sales queries, Geoffrey added an import aggregation table. What else should Geoffrey do to improve query performance of the higher-grain queries?\n\n \n\nSet related dimension tables as aggregation tables.\n\nSet related dimension tables to dual storage mode.\n\nSet related dimension tables to import storage mode.\n\n2. \n\nBreana is a data modeler at Adventure Works who developed a manufacturing model, which is an import model. Breana needs to ensure that manufacturing reports deliver real-time results. Which type of table should Breana create?\n\n \n\nAggregation table.\n\nHybrid table.\n\nPartitioned table.\n\n3. \n\nMousef is a business analyst at Adventure Works who wants to create a new model by extending the sales dataset, which is delivered by IT. Mousef wants to add a new table of census population data sourced from a web page. Which model framework should Mousef use?\n\n \n\nComposite.\n\nDirectQuery.\n\nLive connection.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose a model framework - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/6-choose-model-framework",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nChoose a model framework\n3 minutes\n\nHere’s some general guidance about choosing the appropriate model framework for your project. It especially applies to enterprise solutions, where data volumes are large, query throughput is high, and fast responsiveness is important.\n\nMost importantly, choose the import model framework whenever possible. This framework offers you the most options, design flexibility, and delivers fast performance. Be sure to apply data reduction techniques to ensure that Power BI loads the least amount of data possible.\n\nChoose the DirectQuery model framework when your data source stores large volumes of data and/or your report needs to deliver near real-time data.\n\nChoose the composite model framework to:\n\nBoost the query performance of a DirectQuery model.\nDeliver near real-time query results from an import model.\nExtend a Power BI dataset (or AAS model) with additional data.\n\nYou can boost the query performance of a DirectQuery model by using aggregation tables, which can use import or DirectQuery storage mode. When using import aggregation tables, be sure to set related dimension tables to use dual storage mode. That way, Power BI can satisfy higher-grain queries entirely from cache.\n\nYou can deliver near real-time query results from in import model by creating a hybrid table. In this case, Power BI adds a DirectQuery partition for the current period.\n\nLastly, you can create specialized models by chaining to a core model by using DirectQuery. This type of development is typically done by a business analyst who extends core models, which IT delivers and supports.\n\n Important\n\nPlan carefully. In Power BI Desktop, it’s always possible to convert a DirectQuery table to an import table. But it’s not possible to convert an import table to a DirectQuery table.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop a composite model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/5-determine-when-to-develop-composite-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop a composite model\n7 minutes\n\nA composite model comprises more than one source group. Typically, there’s always the import source group and a DirectQuery source group.\n\n Note\n\nGenerally, the benefits and limitations associated with import and DirectQuery storage modes apply to composite models.\n\nComposite model benefits\n\nThere are several benefits to developing a composite model.\n\nComposite models provide you with design flexibility. You can choose to integrate data using different storage modes, striking the right balance between imported data and pass-through data. Commonly, enterprise models benefit from using DirectQuery tables on large data sources and by boosting query performance with imported tables. Power BI features that support this scenario are described later in this unit.\n\nComposite models can also boost the performance of a DirectQuery model by providing Power BI with opportunity to satisfy some analytic queries from imported data. Querying cached data almost always performs better than pass-through queries.\n\nLastly, when your model includes DirectQuery tables to a remote model, like a Power BI dataset, you can extend your model with new calculated columns and tables. It results in a specialized model based on a core model. For more information, see Power BI usage scenarios: Customizable managed self-service BI.\n\nComposite model limitations\n\nThere are several limitations related to composite models.\n\nImport (or dual, as described later) storage mode tables still require periodic refresh. Imported data can become out of sync with DirectQuery sourced data, so it’s important to refresh it periodically.\n\nWhen an analytic query must combine imported and DirectQuery data, Power BI must consolidate source group query results, which can impact performance. To help avoid this situation for higher-grain queries, you can add import aggregation tables to your model (or enable automatic aggregations) and set related dimension tables to use dual storage mode. This scenario is described later in this unit.\n\nWhen chaining models (DirectQuery to Power BI datasets), modifications made to upstream models can break downstream models. Be sure to assess the impact of modifications by performing dataset impact analysis first.\n\nRelationships between tables from different source groups are known as limited relationships. A model relationship is limited when the Power BI can’t determine a “one” side of a relationship. Limited relationships may result in different evaluations of model queries and calculations. For more information, see Relationship evaluation.\n\nBoost DirectQuery model performance with import data\n\nWhen there’s a justification to develop a DirectQuery model, you can mitigate some limitations by using specific Power BI features that involve import tables.\n\nImport aggregation tables\n\nYou can add import storage mode user-defined aggregation tables or enable automatic aggregations. This way, Power BI directs higher-grain fact queries to a cached aggregation. To boost query performance further, ensure that related dimension tables are set to use dual storage mode.\n\nAutomatic aggregations are a Premium feature. For more information, see Automatic aggregations.\n\nDual storage mode\n\nA dual storage mode table is set to use both import and DirectQuery storage modes. At query time, Power BI determines the most efficient mode to use. Whenever possible, Power BI attempts to satisfy analytic queries by using cached data.\n\nDual storage mode tables work well with import aggregation tables. They allow Power BI to satisfy higher-grain queries entirely from cached data.\n\nSlicer visuals and filter card lists, which are often based on dimension table columns, render more quickly because they’re queried from cached data.\n\nDeliver real-time data from an import model\n\nWhen you set up an import table with incremental refresh, you can enable the Get the latest data in real-time with DirectQuery option.\n\nBy enabling this option, Power BI automatically creates a table partition that uses DirectQuery storage mode. In this case, the table becomes a hybrid table, meaning it has import partitions to store older data, and a single DirectQuery partition for current data.\n\nWhen Power BI queries a hybrid table, the query uses the cache for older data, and passes through to the data source to retrieve current data.\n\nThis option is only available with a Premium license.\n\nFor more information, see Configure incremental refresh and real-time data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop a DirectQuery model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/4-determine-when-to-develop-directquery-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop a DirectQuery model\n9 minutes\n\nA DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group.\n\nA source group is a set of model tables that relate to a data source. There are two types:\n\nImport – Represents all import storage mode tables including calculated tables. There can only be one import source group in a model.\nDirectQuery – Represents all DirectQuery storage mode tables that relate to a specific data source.\n\n Note\n\nAn import model and a DirectQuery model only comprise a single source group. When there’s more than one source group, the model framework is known as a composite model. Composite models are described in Unit 5.\n\nDirectQuery model benefits\n\nThere are several benefits to developing a DirectQuery model.\n\nModel large or fast-changing data sources\n\nA DirectQuery model is a great framework choice when your source data exhibits volume and/or velocity characteristics. Because DirectQuery tables don’t require refresh, they’re well suited to large data stores, like a data warehouse. It’s impractical and inefficient, if not impossible, to import an entire data warehouse into a model. When the source data changes rapidly and users need to see current data, a DirectQuery model can deliver near real-time query results.\n\nWhen a report queries a DirectQuery model, Power BI passes those queries to the underlying data source.\n\nEnforce source RLS\n\nDirectQuery is also useful when the source database enforces row-level security (RLS). Instead of replicating RLS rules in your Power BI model, the source data base can enforce its rules. This approach works only for some relational databases, and it involves setting up single sign-on for the dataset data source. For more information, see Azure SQL Database with DirectQuery.\n\nData sovereignty restrictions\n\nIf your organization has security policies that restrict data leaving their premises, then it isn’t possible to import data. A DirectQuery model that connects to an on-premises data source may be appropriate. (You can also consider installing Power BI Report Server for on-premises reporting.)\n\nCreate specialized datasets\n\nTypically, DirectQuery mode supports relational database sources. That’s because Power BI must translate analytic queries to native queries understood by the data source.\n\nHowever, there’s one powerful exception. You can connect to a Power BI dataset (or Azure Analysis Services model) and convert it to a DirectQuery local model. A local model is a relative term that describes a model’s relationship to another model. In this case, the original dataset is a remote model, and the new dataset is the local model. These models are chained, which is term used to describe related models. You can chain up to three models in this way.\n\nThis capability to chain models supports the potential to personalize and/or extend a remote model. The simplest thing you can do is rename objects, like tables or columns, or add measures to the local model. You can also extend the model with calculated columns or calculated tables, or add new import or DirectQuery tables. However, these extensions result in the creation of new source groups, which means the model becomes a composite model. That scenario is described in Unit 5.\n\nFor more information, see Using DirectQuery for Power BI datasets and Azure Analysis Services.\n\nDirectQuery model limitations\n\nThere are many limitations related to DirectQuery models that you must bear in mind. Here are the main limitations:\n\nNot all data sources are supported. Typically, only major relational database systems are supported. Power BI datasets and Azure Analysis Services models are supported too.\n\nAll Power Query (M) transformations are not possible, because these queries must translate to native queries that are understood by source systems. So, for example, it’s not possible to use pivot or unpivot transformations.\n\nAnalytic query performance can be slow, especially if source systems aren’t optimized (with indexes or materialized views), or there are insufficient resources for the analytic workload.\n\nAnalytic queries can impact on source system performance. It could result in a slower experience for all workloads, including OLTP operations.\n\nBoost DirectQuery model performance\n\nWhen there’s a justification to develop a DirectQuery model, you can mitigate some limitations in two ways.\n\nData source optimizations\n\nYou can optimize the source database to ensure the expected analytic query workload performs well. Specifically, you can create indexes and materialized views, and ensure the database has sufficient resources for all workloads.\n\n Tip\n\nWe recommend that you always work in collaboration with the database owner. It’s important that they understand the additional workload a DirectQuery model can place on their database.\n\nDirectQuery user-defined aggregation tables\n\nYou can add user-defined aggregation tables to a DirectQuery model. User-defined aggregation tables are special model tables that are hidden (from users, calculations, and RLS). They work best when they satisfy higher-grain analytic queries over large fact tables. When you set the aggregation table to use DirectQuery storage mode, it can query a materialized view in the data source. You can also set an aggregation table to use import storage mode or enable automatic aggregations, and these options are described in Unit 4.\n\nFor more information, see DirectQuery model guidance in Power BI Desktop.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop an import model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/3-determine-when-to-develop-import-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop an import model\n7 minutes\n\nAn import model comprises tables that have their storage mode property set to Import. It includes calculated tables, which you define with a DAX formula, too.\n\nImport model benefits\n\nImport models are the most frequently developed model framework because there are many benefits. Import models:\n\nSupport all Power BI data source types, including databases, files, feeds, web pages, dataflows, and more.\nCan integrate source data. For example, one table sources its data from a relational database while a related table sources its data from a web page.\nSupport all DAX and Power Query (M) functionality.\nSupport calculated tables.\nDeliver the best query performance. That’s because the data cached in the model is optimized for analytic queries (filter, group, and summarize) and the model is stored entirely in memory.\n\nIn short, import models offer you the most options and design flexibility, and they deliver fast performance. For this reason, Power BI Desktop defaults to use import storage mode when you “Get data.”\n\nImport model limitations\n\nDespite the many compelling benefits, there are limitations of import models that you must bear in mind. Limitations are related to model size and data refresh.\n\nModel size\n\nPower BI imposes dataset size restrictions, which limit the size of a model. When you publish the model to a shared capacity, there’s a 1-GB limit per dataset. When this size limit is exceeded, the dataset will fail to refresh. When you publish the model to a dedicated capacity (also known as Premium capacities), it can grow beyond 10-GB, providing you enable the Large dataset storage format setting for the capacity.\n\nYou should always strive to reduce the amount of data stored in tables. This strategy helps to reduce the duration of model refreshes and speed up model queries. There are numerous data reduction techniques that you can apply, including:\n\nRemove unnecessary columns\nRemove unnecessary rows\nGroup by and summarize to raise the grain of fact tables\nOptimize column data types with a preference for numeric data\nPreference for custom columns in Power Query instead of calculated columns in the model\nDisable Power Query query load\nDisable auto date/time\nUse DirectQuery table storage, as described in later units of this module.\n\nFor more information, see Data reduction techniques for Import modeling.\n\n Note\n\nThe 1-GB per dataset limit refers to the compressed size of the Power BI model, not the volume of data being collected from the source system.\n\nData refresh\n\nImported data must be periodically refreshed. Dataset data is only as current as the last successful data refresh. To keep data current, you set up scheduled data refresh, or report consumers can perform an on-demand refresh.\n\nPower BI imposes limits on how often scheduled refresh operations can occur. It’s up to eight times per day in a shared capacity, and up to 48 times per day in a dedicated capacity.\n\nYou should determine whether this degree of latency is tolerable. It often depends on the velocity (or volatility) of the data, and the urgency to keep users informed about the current state of data. When scheduled refresh limits aren’t acceptable, consider using DirectQuery storage tables, or creating a hybrid table. Or take a different approach, and create a real-time dataset instead.\n\n Tip\n\nHybrid tables are described in unit 4. For information about real-time datasets, work through the Monitor data in real-time with Power BI module.\n\nYou must also consider refresh workload and duration. By default, to refresh a table, Power BI removes all data and reloads it again. These operations can place an unacceptable burden on source systems, especially for large fact tables. To reduce this burden, you can set up the incremental refresh feature. Incremental refresh automates the creation and management of time-period partitions, and intelligently update only those partitions that require refresh.\n\nWhen your data source supports incremental refresh, it can result in faster and more reliable refreshes, and reduced resource consumption of Power BI and source systems.\n\nAdvanced data modelers can customize their own partitioning strategy. Automation scripts can create, manage, and refresh table partitions. For more information, see Power BI usage scenarios: Advanced data model management. This usage scenario describes using the XMLA endpoint available with Power BI Premium.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe Power BI model fundamentals - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/2-describe-power-bi-model-fundamentals",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe Power BI model fundamentals\n9 minutes\n\nThis unit introduces Power BI model terms. It’s important that you understand these terms in order to choose the appropriate model framework for your project. This unit describes the following terms:\n\nData model\nPower BI dataset\nAnalytic query\nTabular model\nStar schema design\nTable storage mode\nModel framework\nData model\n\nA Power BI data model is a query-able data resource that’s optimized for analytics. Reports can query data models by using one of two analytic languages: Data Analysis Expressions (DAX) or Multidimensional Expressions (MDX). Power BI uses DAX, while paginated reports can use either DAX or MDX. The Analyze in Excel features uses MDX.\n\n Tip\n\nA data model is also described as semantic model, especially in enterprise scenarios. Commonly, in the context of a data discussion and in this module, a data model is simply referred to as a model.\n\nPower BI dataset\n\nYou develop a Power BI model in Power BI Desktop, and once published to a workspace in the Power BI service, it’s then known as a dataset. A dataset is a Power BI artifact that’s a source of data for visualizations in Power BI reports and dashboards.\n\n Note\n\nNot all datasets originate from models developed in Power BI Desktop. Some datasets represent connections to external-hosted models in AAS or SSAS. Others can represent real-time data structures, including push datasets, streaming datasets, or hybrid datasets. This module is concerned only with models developed in Power BI Desktop.\n\nAnalytic query\n\nPower BI reports and dashboards must query a dataset. When Power BI visualizes dataset data, it prepares and sends an analytic query. An analytic query produces a query result from a model that’s easy for a person to understand, especially when visualized.\n\nAn analytic query has three phases that are executed in this order:\n\nFilter\nGroup\nSummarize\n\nFiltering (sometimes known as slicing) narrows down on a subset of the model data. Filter values aren’t visible in the query result. Most analytic queries apply filters because it’s common to filter by a time period, and usually other attributes. Filtering happens in different ways. In a Power BI report, you can set filters at report, page, or visual level. Report layouts often include slicer visuals to filter visuals on the report page. When the model enforces row-level security (RLS), it applies filters to model tables to restrict access to specific data. Measures, which summarize model data, can also apply filters.\n\nGrouping (sometimes known as dicing) divides query result into groups. Each group is also a filter, but unlike the filtering phase, filter values are visible in the query result. For example, grouping by customer filters each group by customer.\n\nSummarization produces a single value result. Typically, a report visual summarizes a numeric field by using an aggregate function. Aggregate functions include sum, count, minimum, maximum, and others. You can achieve simple summarization by aggregating a column, or you can achieve complex summarization by creating a measure using a DAX formula.\n\nConsider an example: A Power BI report page includes a slicer to filter by a single year. There’s also a column chart visual that shows quarterly sales for the filtered year.\n\nIn this example, the slicer filters the visual by calendar year 2021. The column chart groups by quarters (of the filtered year). Each column is a group that represents a visible filter. The column heights represent the summarized sales values for each quarter of the filtered year.\n\nTabular model\n\nA Power BI model is a tabular model. A tabular model comprises one or more tables of columns. It can also include relationships, hierarchies, and calculations.\n\nStar schema design\n\nTo produce an optimized and easy-to-use tabular model, we recommend you produce a star schema design. Star schema design is a mature modeling approach widely adopted by relational data warehouses. It requires you to classify model tables as either dimension or fact.\n\nDimension tables describe business entities; the things you model. Entities can include products, people, places, and concepts including time itself. Fact tables store observations or events, and can be, for example, sales orders, stock balances, exchange rates, or temperature readings. A fact table contains dimension key columns that relate to dimension tables, and numeric measure columns. A fact table forms the center of a star, and the related dimension tables form the points of the star.\n\nIn an analytic query, dimensions table columns filter or group. Fact table columns are summarized.\n\nFor more information, see Understand star schema and the importance for Power BI.\n\nTable storage mode\n\nEach Power BI model table (except calculated tables) has a storage mode property. The storage mode property can be either Import, DirectQuery, or Dual, and it determines whether table data is stored in the model.\n\nImport – Queries retrieve data that’s stored, or cached, in the model.\nDirectQuery – Queries pass through to the data source.\nDual – Queries retrieve stored data or pass through to the data source. Power BI determines the most efficient plan, striving to use cached data whenever possible.\nModel framework\n\nTable storage mode settings determine the model framework, which can be either import, DirectQuery, or composite. The following units in this module describe each of these frameworks and provides guidance on their use.\n\nAn import model comprises tables that have their storage mode property set to Import.\nA DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group. Source group is described later in this module.\nA composite model comprises more than one source group.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nFor over two decades, Microsoft continues to make deep investments in enterprise business intelligence (BI). Azure Analysis Services (AAS) and SQL Server Analysis Services (SSAS) are based on mature BI data modeling technology used by countless enterprises. The same technology is also at the heart of Power BI data models.\n\nPower BI offers you a choice when designing your model. You can use Power BI Desktop to develop your model, and you can develop it by using different frameworks. These frameworks help to deliver fast performance, near real-time results, or both.\n\nThis module introduces the frameworks, their benefits and limitations, and features to help optimize your models. Lastly, it provides you with guidance to help you choose the right framework and features for your project.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI model fundamentals.\nDetermine when to develop an import model.\nDetermine when to develop a DirectQuery model.\nDetermine when to develop a composite model.\nChoose an appropriate Power BI model framework.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose a Power BI model framework - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nChoose a Power BI model framework\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nDescribe model frameworks, their benefits and limitations, and features to help optimize your Power BI data models.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI model fundamentals.\nDetermine when to develop an import model.\nDetermine when to develop a DirectQuery model.\nDetermine when to develop a composite model.\nChoose an appropriate Power BI model framework.\nAdd\nPrerequisites\nExperience developing Power BI data models, reports, and dashboards.\nIntroduction\nmin\nDescribe Power BI model fundamentals\nmin\nDetermine when to develop an import model\nmin\nDetermine when to develop a DirectQuery model\nmin\nDetermine when to develop a composite model\nmin\nChoose a model framework\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nRelational data warehousing skills are essential in multiple data professional roles, including data engineers, data analysts, and data scientists.\n\nIn this module, you learned how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nLearn more\n\nTo learn more about using Azure Synapse Analytics for relational data warehousing, refer to Synapse POC playbook: Data warehousing with dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nIn which of the following table types should an insurance company store details of customer attributes by which claims will be aggregated?\n\n \n\nStaging table\n\nDimension table\n\nFact table\n\n2. \n\nYou create a dimension table for product data, assigning a unique numeric key for each row in a column named ProductKey. The ProductKey is only defined in the data warehouse. What kind of key is ProductKey?\n\n \n\nA surrogate key\n\nAn alternate key\n\nA business key\n\n3. \n\nWhat distribution option would be best for a sales fact table that will contain billions of records?\n\n \n\nHASH\n\nROUND_ROBIN\n\nREPLICATE\n\n4. \n\nYou need to write a query to return the total of the UnitsProduced numeric measure in the FactProduction table aggregated by the ProductName attribute in the FactProduct table. Both tables include a ProductKey surrogate key field. What should you do?\n\n \n\nUse two SELECT queries with a UNION ALL clause to combine the rows in the FactProduction table with those in the FactProduct table.\n\nUse a SELECT query against the FactProduction table with a WHERE clause to filter out rows with a ProductKey that doesn't exist in the FactProduct table.\n\nUse a SELECT query with a SUM function to total the UnitsProduced metric, using a JOIN on the ProductKey surrogate key to match the FactProduction records to the FactProduct records and a GROUP BY clause to aggregate by ProductName.\n\n5. \n\nYou use the RANK function in a query to rank customers in order of the number of purchases they have made. Five customers have made the same number of purchases and are all ranked equally as 1. What rank will the customer with the next highest number of purchases be assigned?\n\n \n\ntwo\n\nsix\n\none\n\n6. \n\nYou need to compare approximate production volumes by product while optimizing query response time. Which function should you use?\n\n \n\nCOUNT\n\nNTILE\n\nAPPROX_COUNT_DISTINCT\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/6-exercise-explore-data-warehouse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore a data warehouse\n45 minutes\n\nNow it's your opportunity to explore a relational data warehouse. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then explore a data warehouse that has been created for you.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/5-query-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery a data warehouse\n10 minutes\n\nWhen the dimension and fact tables in a data warehouse have been loaded with data, you can use SQL to query the tables and analyze the data they contain. The Transact-SQL syntax used to query tables in a Synapse dedicated SQL pool is similar to SQL used in SQL Server or Azure SQL Database.\n\nAggregating measures by dimension attributes\n\nMost data analytics with a data warehouse involves aggregating numeric measures in fact tables by attributes in dimension tables. Because of the way a star or snowflake schema is implemented, queries to perform this kind of aggregation rely on JOIN clauses to connect fact tables to dimension tables, and a combination of aggregate functions and GROUP BY clauses to define the aggregation hierarchies.\n\nFor example, the following SQL queries the FactSales and DimDate tables in a hypothetical data warehouse to aggregate sales amounts by year and quarter:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter\nORDER BY dates.CalendarYear, dates.CalendarQuarter;\n\n\nThe results from this query would look similar to the following table:\n\nExpand table\nCalendarYear\tCalendarQuarter\tTotalSales\n2020\t1\t25980.16\n2020\t2\t27453.87\n2020\t3\t28527.15\n2020\t4\t31083.45\n2021\t1\t34562.96\n2021\t2\t36162.27\n...\t...\t...\n\nYou can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the DimCustomer table:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        custs.City,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nJOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City\nORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City;\n\n\nThis time, the results include a quarterly sales total for each city:\n\nExpand table\nCalendarYear\tCalendarQuarter\tCity\tTotalSales\n2020\t1\tAmsterdam\t5982.53\n2020\t1\tBerlin\t2826.98\n2020\t1\tChicago\t5372.72\n...\t...\t...\t..\n2020\t2\tAmsterdam\t7163.93\n2020\t2\tBerlin\t8191.12\n2020\t2\tChicago\t2428.72\n...\t...\t...\t..\n2020\t3\tAmsterdam\t7261.92\n2020\t3\tBerlin\t4202.65\n2020\t3\tChicago\t2287.87\n...\t...\t...\t..\n2020\t4\tAmsterdam\t8262.73\n2020\t4\tBerlin\t5373.61\n2020\t4\tChicago\t7726.23\n...\t...\t...\t..\n2021\t1\tAmsterdam\t7261.28\n2021\t1\tBerlin\t3648.28\n2021\t1\tChicago\t1027.27\n...\t...\t...\t..\nJoins in a snowflake schema\n\nWhen using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a DimProduct dimension table from which the product categories have been normalized into a separate DimCategory table. A query to aggregate items sold by product category might look similar to the following example:\n\nSELECT  cat.ProductCategory,\n        SUM(sales.OrderQuantity) AS ItemsSold\nFROM dbo.FactSales AS sales\nJOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey\nJOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey\nGROUP BY cat.ProductCategory\nORDER BY cat.ProductCategory;\n\n\nThe results from this query include the number of items sold for each product category:\n\nExpand table\nProductCategory\tItemsSold\nAccessories\t28271\nBits and pieces\t5368\n...\t...\n\n Note\n\nJOIN clauses for FactSales and DimProduct and for DimProduct and DimCategory are both required, even though no fields from DimProduct are returned by the query.\n\nUsing ranking functions\n\nAnother common kind of analytical query is to partition the results based on a dimension attribute and rank the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL ranking functions such as ROW_NUMBER, RANK, DENSE_RANK, and NTILE. These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition:\n\nROW_NUMBER returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on.\nRANK returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties.\nDENSE_RANK ranks rows in a partition the same way as RANK, but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties.\nNTILE returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, NTILE(4) returns the quartile in which a store's sales volume places it.\n\nFor example, consider the following query:\n\nSELECT  ProductCategory,\n        ProductName,\n        ListPrice,\n        ROW_NUMBER() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,\n        RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,\n        DENSE_RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,\n        NTILE(4) OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile\nFROM dbo.DimProduct\nORDER BY ProductCategory;\n\n\nThe query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table:\n\nExpand table\nProductCategory\tProductName\tListPrice\tRowNumber\tRank\tDenseRank\tQuartile\nAccessories\tWidget\t8.99\t1\t1\t1\t1\nAccessories\tKnicknak\t8.49\t2\t2\t2\t1\nAccessories\tSprocket\t5.99\t3\t3\t3\t2\nAccessories\tDoodah\t5.99\t4\t3\t3\t2\nAccessories\tSpangle\t2.99\t5\t5\t4\t3\nAccessories\tBadabing\t0.25\t6\t6\t5\t4\nBits and pieces\tFlimflam\t7.49\t1\t1\t1\t1\nBits and pieces\tSnickity wotsit\t6.99\t2\t2\t2\t1\nBits and pieces\tFlange\t4.25\t3\t3\t3\t2\n...\t...\t...\t...\t...\t...\t...\n\n Note\n\nThe sample results demonstrate the difference between RANK and DENSE_RANK. Note that in the Accessories category, the Sprocket and Doodah products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a RANK of 5 (there are four products more expensive than it) and a DENSE_RANK of 4 (there are three higher prices).\n\nTo learn more about ranking functions, see Ranking Functions (Transact-SQL) in the Azure Synapse Analytics documentation.\n\nRetrieving an approximate count\n\nWhile the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data.\n\nFor example, the following query uses the COUNT function to retrieve the number of sales for each year in a hypothetical data warehouse:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    COUNT(DISTINCT sales.OrderNumber) AS Orders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe results of this query might look similar to the following table:\n\nExpand table\nCalendarYear\tOrders\n2019\t239870\n2020\t284741\n2021\t309272\n...\t...\n\nThe volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the APPROX_COUNT_DISTINCT function as shown in the following example:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe APPROX_COUNT_DISTINCT function uses a HyperLogLog algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table:\n\nExpand table\nCalendarYear\tApproxOrders\n2019\t235552\n2020\t290436\n2021\t304633\n...\t...\n\nThe counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the APPROX_COUNT_DISTINCT function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration.\n\n Note\n\nSee the APPROX_COUNT_DISTINCT function documentation for more details.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/4-load-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad data warehouse tables\n10 minutes\n\nAt a basic level, loading a data warehouse is typically achieved by adding new data from files in a data lake into tables in the data warehouse. The COPY statement is an effective way to accomplish this task, as shown in the following example:\n\nCOPY INTO dbo.StageProducts\n    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)\nFROM 'https://mydatalake.blob.core.windows.net/data/stagedfiles/products/*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF'\n);\n\nConsiderations for designing a data warehouse load process\n\nOne of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n\nIn most cases, you should implement a data warehouse load process that performs tasks in the following order:\n\nIngest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.\nLoad the data from files into staging tables in the relational data warehouse.\nLoad the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.\nLoad the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.\nPerform post-load optimization by updating indexes and table distribution statistics.\n\nAfter using the COPY statement to load data into staging tables, you can use a combination of INSERT, UPDATE, MERGE, and CREATE TABLE AS SELECT (CTAS) statements to load the staged data into dimension and fact tables.\n\n Note\n\nImplementing an effective data warehouse loading solution requires careful consideration of how to manage surrogate keys, slowly changing dimensions, and other complexities inherent in a relational data warehouse schema. To learn more about techniques for loading a data warehouse, consider completing the Load data into a relational data warehouse module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]