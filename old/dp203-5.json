[
  {
    "title": "Implement Azure Synapse Link with Azure Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nImplement Azure Synapse Link with Azure Cosmos DB\nModule\n9 Units\nFeedback\nIntermediate\nData Engineer\nAzure Cosmos DB\nAzure Synapse Analytics\n\nAzure Synapse Link for Azure Cosmos DB enables HTAP integration between operational data in Azure Cosmos DB and Azure Synapse Analytics runtimes for Spark and SQL.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nConfigure an Azure Cosmos DB Account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Cosmos DB and Azure Synapse Analytics. Consider completing the following modules first:\n\nExplore fundamentals of Azure Cosmos DB\nExplore fundamentals of large-scale data warehousing\nIntroduction\nmin\nEnable Cosmos DB account to use Azure Synapse Link\nmin\nCreate an analytical store enabled container\nmin\nCreate a linked service for Cosmos DB\nmin\nQuery Cosmos DB data with Spark\nmin\nQuery Cosmos DB with Synapse SQL\nmin\nExercise - Implement Azure Synapse Link for Cosmos DB\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/5-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nHTAP solutions enable extensive, near-realtime data analytics without impacting the performance of operational data stores. Azure Synapse Link offers multiple ways to create HTAP solutions that integrate operational data in commonly used data stores with Azure Synapse Analytics.\n\nIn this module, you learned to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\n\nFor more information about Azure Synapse Link, see the following articles in the Azure Synapse Analytics documentation:\n\nWhat is Azure Synapse Link for Azure Cosmos DB?\nWhat is Azure Synapse Link for SQL?\nWhat is Azure Synapse Link for Dataverse?\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe Azure Synapse Link - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/3-azure-synapse-link",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe Azure Synapse Link\n6 minutes\n\nHTAP solutions are supported in Azure Synapse Analytics through Azure Synapse Link; a general term for a set of linked services that support HTAP data synchronization into your Azure Synapse Analytics workspace.\n\nAzure Synapse Link for Cosmos DB\n\nAzure Cosmos DB is a global-scale NoSQL data service in Microsoft Azure that enables applications to store and access operational data by using a choice of application programming interfaces (APIs).\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables you to run near-real-time analytics over operational data stored in a Cosmos DB container. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:\n\nAn Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.\nThe container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.\nAzure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.\nAzure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.\nAzure Synapse Link for SQL\n\nMicrosoft SQL Server is a popular relational database system that powers business applications in some of the world's largest organizations. Azure SQL Database is a cloud-based platform-as-a-service database solution based on SQL Server. Both of these relational database solutions are commonly used as operational data stores.\n\nAzure Synapse Link for SQL enables HTAP integration between data in SQL Server or Azure SQL Database and an Azure Synapse Analytics workspace.\n\nIn the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:\n\nAn Azure SQL Database or SQL Server instance contains a relational database in which transactional data is stored in tables.\nAzure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.\nThe replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.\nAzure Synapse Link for Dataverse\n\nMicrosoft Dataverse is data storage service within the Microsoft Power Platform. You can use Dataverse to store business data in tables that are accessed by Power Apps, Power BI, Power Virtual Agents, and other applications and services across Microsoft 365, Dynamics 365, and Azure.\n\nAzure Synapse Link for Dataverse enables HTAP integration by replicating table data to Azure Data Lake storage, where it can be accessed by runtimes in Azure Synapse Analytics - either directly from the data lake or through a Lake Database defined in a serverless SQL pool.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Dataverse architecture are illustrated:\n\nBusiness applications store data in Microsoft Dataverse tables.\nAzure Synapse Link for Dataverse replicates the table data to an Azure Data Lake Gen2 storage account associated with an Azure Synapse workspace.\nThe data in the data lake can be used to define tables in a lake database and queried using a serverless SQL pool, or read directly from storage using SQL or Spark.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/4-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich of the following descriptions matches a hybrid transactional/analytical processing (HTAP) architecture.\n\n \n\nBusiness applications store data in an operational data store, which is also used to support analytical queries for reporting.\n\nBusiness applications store data in an operational data store, which is synchronized with low latency to a separate analytical store for reporting and analysis.\n\nBusiness applications store operational data in an analytical data store that is optimized for queries to support reporting and analysis.\n\n2. \n\nYou want to use Azure Synapse Analytics to analyze operational data stored in a Cosmos DB for NoSQL container. Which Azure Synapse Link service should you use?\n\n \n\nAzure Synapse Link for SQL\n\nAzure Synapse Link for Dataverse\n\nAzure Synapse Link for Azure Cosmos DB\n\n3. \n\nYou plan to use Azure Synapse Link for Dataverse to analyze business data in your Azure Synapse Analytics workspace. Where is the replicated data from Dataverse stored?\n\n \n\nIn an Azure Synapse dedicated SQL pool\n\nIn an Azure Data Lake Gen2 storage container.\n\nIn an Azure Cosmos DB container.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nHybrid Transactional / Analytical Processing (HTAP) is a style of data processing that combines transactional data processing, such as is typically found in a business application, with analytical processing, such as is used in a business intelligence (BI) or reporting solution. The data access patterns and storage optimizations used in these two kinds of workload are very different, so usually a complex extract, transform, and load (ETL) process is required to copy data out of transactional systems and into analytical systems; adding complexity and latency to data analysis. In an HTAP solution, the transactional data is replicated automatically, with low-latency, to an analytical store, where it can be queried without impacting the performance of the transactional system.\n\nIn Azure Synapse Analytics, HTAP capabilities are provided by multiple Azure Synapse Link services, each connecting a commonly used transactional data store to your Azure Synapse Analytics workspace and making the data available for processing using Spark or SQL.\n\nAfter completing this module, you'll be able to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand hybrid transactional and analytical processing patterns - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/2-understand-patterns",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand hybrid transactional and analytical processing patterns\n6 minutes\n\nMany business application architectures separate transactional and analytical processing into separate systems with data stored and processed on separate infrastructures. These infrastructures are commonly referred to as OLTP (online transaction processing) systems working with operational data, and OLAP (online analytical processing) systems working with historical data, with each system is optimized for their specific task.\n\nOLTP systems are optimized for dealing with discrete system or user requests immediately and responding as quickly as possible.\n\nOLAP systems are optimized for the analytical processing, ingesting, synthesizing, and managing large sets of historical data. The data processed by OLAP systems largely originates from OLTP systems and needs to be loaded into the OLAP systems by ETL (Extract, Transform, and Load) batch processes.\n\nDue to their complexity and the need to physically copy large amounts of data, this approach creates a delay in data being available to analyze in OLAP systems.\n\nHybrid Transactional / Analytical Processing (HTAP)\n\nAs more businesses move to digital processes, they increasingly recognize the value of being able to respond to opportunities by making faster and well-informed decisions. HTAP (Hybrid Transactional/Analytical processing) enables business to run advanced analytics in near-real-time on data stored and processed by OLTP systems.\n\nThe following diagram illustrates the generalized pattern of an HTAP architecture:\n\nA business application processes user input and stores data in a transactional database that is optimized for a mix of data reads and writes based on the application's expected usage profile.\nThe application data is automatically replicated to an analytical store with low latency.\nThe analytical store supports data modeling, analytics, and reporting without impacting the transactional system.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Plan hybrid transactional and analytical processing using Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nPlan hybrid transactional and analytical processing using Azure Synapse Analytics\nModule\n5 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how hybrid transactional / analytical processing (HTAP) can help you perform operational analytics with Azure Synapse Analytics.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of data analytics and Azure services for data. Consider completing the Azure Data Fundamentals certification first.\n\nIntroduction\nmin\nUnderstand hybrid transactional and analytical processing patterns\nmin\nDescribe Azure Synapse Link\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark offers data engineers a powerful platform for transforming and processing data. The ability to include Spark notebooks in a pipeline enables you to automate Spark processing and integrate it into a data integration workflow.\n\n Tip\n\nTo learn more about using Spark notebooks in an Azure Synapse Analytics pipeline, see Transform data by running a Synapse notebook in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use an Apache Spark notebook in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/5-exercise-use-spark-notebooks-pipeline",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use an Apache Spark notebook in a pipeline\n30 minutes\n\nNow it's your chance to integrate spark into an Azure Synapse Analytics pipeline. In this exercise, you'll create a pipeline that includes a notebook activity, and configure parameters for the notebook.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhat kind of pool is required to run a Synapse notebook in a pipeline?\n\n \n\nA Dedicated SQL pool\n\nA Data Explorer pool\n\nAn Apache Spark pool\n\n2. \n\nWhat kind of pipeline activity encapsulates a Synapse notebook?\n\n \n\nNotebook activity\n\nHDInsight Spark activity\n\nScript activity\n\n3. \n\nA notebook cell contains variable declarations. How can you use them as parameters?\n\n \n\nAdd a %%Spark magic at the beginning of the cell\n\nToggle the Parameters cell setting for the cell\n\nUse the var keyword for each variable declaration\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use parameters in a notebook - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/4-notebook-parameters",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse parameters in a notebook\n5 minutes\n\nParameters enable you to dynamically pass values for variables in the notebook each time it's run. This approach provides flexibility, enabling you to adjust the logic encapsulated in the notebook for each run.\n\nCreate a parameters cell in the notebook\n\nTo define the parameters for a notebook, you declare and initialize variables in a cell, which you then configure as a Parameters cell by using the toggle option in the notebook editor interface.\n\nInitializing a variable ensures that it has a default value, which will be used if the parameter isn't set in the notebook activity.\n\nSet base parameters for the notebook activity\n\nAfter defining a parameters cell in the notebook, you can set values to be used when the notebook is run by a notebook activity in a pipeline. To set parameter values, expand and edit the Base parameters section of the settings for the activity.\n\nYou can assign explicit parameter values, or use an expression to assign a dynamic value. For example, the expression @pipeline().RunId returns the unique identifier for the current run of the pipeline.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a Synapse notebook activity in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/3-use-notebook-activity",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a Synapse notebook activity in a pipeline\n3 minutes\n\nTo run a Spark notebook in a pipeline, you must add a notebook activity and configure it appropriately. You'll find the Notebook activity in the Synapse section of the activities pane in the Azure Synapse Analytics pipeline designer.\n\n Tip\n\nYou can also add a notebook to a pipeline from within the notebook editor.\n\nTo configure the notebook activity, edit the settings in the properties pane beneath the pipeline designer canvas. Notebook activity specific settings include:\n\nNotebook: The notebook you want to run. You can select an existing notebook in your Azure Synapse Analytics workspace, or create a new one.\nSpark pool: The Apache Spark pool on which the notebook should be run.\nExecutor size: The node size for the worker nodes in the pool, which determines the number of processor cores and the amount of memory allocated to worker nodes.\nDynamically allocate executors: Configures Spark dynamic allocation, enabling the pool to automatically scale up and down to support the workload.\nMin executors: The minimum number of executors to be allocated.\nMax executors: The maximum number of executors to be allocated.\nDriver size: The node size for the driver node.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWith Azure Synapse Analytics pipelines, you can orchestrate data transfer and transformation activities and build data integration solutions across multiple systems. When you're working with analytical data in a data lake, Apache Spark provides a scalable, distributed processing platform that you can use to process huge volumes of data efficiently.\n\nThe Synapse Notebook activity enables you to run data processing code in Spark notebooks as a task in a pipeline; making it possible to automate big data processing and integrate it into extract, transform, and load (ETL) workloads.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Synapse Notebooks and Pipelines - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/2-understand-notebooks-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Synapse Notebooks and Pipelines\n5 minutes\n\nAzure Synapse Pipelines enable you to create, run, and manage data integration and data flow activities. While many of these activities are built-into the Azure Synapse Pipeline platform and run natively in the integration runtime for your pipeline, you can also use external processing resources to perform specific tasks. One such external resource is an Apache Spark pool in your Azure Synapse Analytics workspace on which you can run code in a notebook.\n\nIt's common in big data analytics solutions for data engineers to use Spark notebooks for initial data exploration and interactive experimentation when designing data transformation processes. When the transformation logic has been completed, you can perform some final code optimization and refactoring for maintainability, and then include the notebook in a pipeline. The pipeline can then be run on a schedule or in response to an event (such as new data files being loaded into the data lake).\n\nThe notebook is run on a Spark pool, which you can configure with the appropriate compute resources and Spark runtime for your specific workload. The pipeline itself is run in an integration runtime that orchestrates the activities in the pipeline, coordinating the external services needed to run them.\n\n Tip\n\nThere are several best practices that can help make working with Spark notebooks more efficient and effective. Some of these include:\n\nKeep your code organized: Use clear and descriptive variable and function names, and organize your code into small, reusable chunks.\nCache intermediate results: Spark allows you to cache intermediate results, which can significantly speed up the performance of your notebook.\nAvoid unnecessary computations: Be mindful of the computations you are performing and try to avoid unnecessary steps. For example, if you only need a subset of your data, filter it out before running any further computations.\nAvoid using collect() unless necessary: When working with large datasets, it is often better to perform operations on the entire dataset rather than bringing the data into the driver node using the collect() method.\nUse Spark UI for monitoring and debugging: Spark's web-based user interface (UI) provides detailed information about the performance of your Spark jobs, including task execution times, input and output data sizes, and more.\nKeep your dependencies version-consistent and updated: when working with Spark, it is important to keep dependencies version-consistent across your cluster and to use the latest version of Spark and other dependencies if possible.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark Notebooks in an Azure Synapse Pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Spark Notebooks in an Azure Synapse Pipeline\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nApache Spark provides data engineers with a scalable, distributed data processing platform, which can be integrated into an Azure Synapse Analytics pipeline.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nDescribe notebook and pipeline integration.\nUse a Synapse notebook activity in a pipeline.\nUse parameters with a notebook activity.\nAdd\nPrerequisites\n\nBefore starting this module, you should have experience of using Apache Spark in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first:\n\nIntroduction\nmin\nUnderstand Synapse Notebooks and Pipelines\nmin\nUse a Synapse notebook activity in a pipeline\nmin\nUse parameters in a notebook\nmin\nExercise - Use an Apache Spark notebook in a pipeline\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Analytics provides data integration services through the creation of pipelines. By using pipelines, you can implement complex extract, transform, and load (ETL) solutions that support enterprise data analytics.\n\n Tip\n\nTo learn more about developing and debugging pipelines, see Iterative development and debugging with Azure Data Factory and Synapse Analytics pipelines\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhat does a pipeline use to access external data source and processing resources?\n\n \n\nData Explorer pools\n\nLinked services\n\nExternal tables\n\n2. \n\nWhat kind of object should you add to a data flow to define a target to which data is loaded?\n\n \n\nSource\n\nTransformation\n\nSink\n\n3. \n\nWhat must you create to run a pipeline at scheduled intervals?\n\n \n\nA control flow\n\nA trigger\n\nAn activity\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Run a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/5-run-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Transfer and transform data with Azure Synapse Analytics pipelines  Build a data pipeline in Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 8\nNext\nRun a pipeline\nCompleted\n100 XP\n5 minutes\n\nWhen you’re ready, you can publish a pipeline and use a trigger to run it. Triggers can be defined to run the pipeline:\n\nImmediately\nAt explicitly scheduled intervals\nIn response to an event, such as new data files being added to a folder in a data lake.\n\nYou can monitor each individual run of a pipeline in the Monitor page in Azure Synapse Studio.\n\nThe ability to monitor past and ongoing pipeline runs is useful for troubleshooting purposes. Additionally, when combined with the ability to integrate Azure Synapse Analytics and Microsoft Purview, you can use pipeline run history to track data lineage data flows.\n\n Tip\n\nTo learn more about integration between Azure Synapse Analytics and Microsoft Purview, consider completing the Integrate Microsoft Purview and Azure Synapse Analytics module.\n\nNext unit: Exercise - Build a data pipeline in Azure Synapse Analytics\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/6-exercise-build-data-pipeline-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Build a data pipeline in Azure Synapse Analytics\n45 minutes\n\nNow it's your chance to build an Azure Synapse Analytics pipeline. In this exercise, you'll implement a run an Azure Synapse Analytics pipeline that transfers and transforms data.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a pipeline in Azure Synapse Studio - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/3-create-pipeline-azure-synapse-studio",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a pipeline in Azure Synapse Studio\n5 minutes\n\nYou can create a pipeline in Azure Synapse Studio by using shortcuts on the Home page, but the primary place where pipelines are created and managed is the Integrate page.\n\nWhen you create a pipeline in Azure Synapse Studio, you can use the graphical design interface.\n\n.\n\nThe pipeline designer includes a set of activities, organized into categories, which you can drag onto a visual design canvas. You can select each activity on the canvas and use the properties pane beneath the canvas to configure the settings for that activity.\n\nTo define the logical sequence of activities, you can connect them by using the Succeeded, Failed, and Completed dependency conditions, which are shown as small icons on the right-hand edge of each activity.\n\nDefining a pipeline with JSON\n\nWhile the graphical development environment is the preferred way to create a pipeline, you can also create or edit the underlying JSON definition of a pipeline. The following code example shows the JSON definition of a pipeline that includes a Copy Data activity:\n\n{\n  \"name\": \"CopyPipeline\",\n  \"properties\": {\n    \"description\": \"Copy data from a blob to Azure SQL table\",\n    \"activities\": [\n      {\n        \"name\": \"CopyFromBlobToSQL\",\n        \"type\": \"Copy\",\n        \"inputs\": [\n          {\n            \"name\": \"InputDataset\"\n          }\n        ],\n        \"outputs\": [\n          {\n            \"name\": \"OutputDataset\"\n          }\n        ],\n        \"typeProperties\": {\n          \"source\": {\n            \"type\": \"BlobSource\"\n          },\n          \"sink\": {\n            \"type\": \"SqlSink\",\n            \"writeBatchSize\": 10000,\n            \"writeBatchTimeout\": \"60:00:00\"\n          }\n        },\n        \"policy\": {\n          \"retry\": 2,\n          \"timeout\": \"01:00:00\"\n        }\n      }\n    ]\n  }\n}\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Define data flows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/4-define-data-flows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDefine data flows\n5 minutes\n\nA Data Flow is a commonly used activity type to define data flow and transformation. Data flows consist of:\n\nSources - The input data to be transferred.\nTransformations – Various operations that you can apply to data as it streams through the data flow.\nSinks – Targets into which the data will be loaded.\n\nWhen you add a Data Flow activity to a pipeline, you can open it in a separate graphical design interface in which to create and configure the required data flow elements.\n\nAn important part of creating a data flow is to define mappings for the columns as the data flows through the various stages, ensuring column names and data types are defined appropriately. While developing a data flow, you can enable the Data flow debug option to pass a subset of data through the flow, which can be useful to test that your columns are mapped correctly.\n\n Tip\n\nTo learn more about implementing a Data Flow activity, see Data Flow activity in Azure Data Factory and Azure Synapse Analytics in the Azure documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWith the wide range of data stores available in Azure, there's the need to manage and orchestrate the movement data between them. In fact, you'll usually want to automate extract, transform, and load (ETL) workloads as a regular process in a wider enterprise analytical solution. Pipelines are a mechanism for defining and orchestrating data movement activities. In this module, you'll be introduced to Azure Synapse Analytics pipelines, their component parts, and how to implement and run a pipeline in Azure Synapse Studio.\n\n Note\n\nAzure Synapse Analytics pipelines are built on the same technology as Azure Data Factory, and offer a similar authoring experience. The authoring processes described in this module are also applicable to Azure Data Factory. For a detailed discussion of the differences between Azure Synapse Analytics pipelines and Azure Data Factory, see Data integration in Azure Synapse Analytics versus Azure Data Factory.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand pipelines in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/2-understand-pipelines-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand pipelines in Azure Synapse Analytics\n6 minutes\n\nPipelines in Azure Synapse Analytics encapsulate a sequence of activities that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical design tools in Azure Synapse Studio enable you to build complex pipelines with minimal or no coding required.\n\nCore pipeline concepts\n\nBefore building pipelines in Azure Synapse Analytics, you should understand a few core concepts.\n\nActivities\n\nActivities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence.\n\nActivities can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows that apply transformations to the data as part of an extract, transfer, and load (ETL) operation. Additionally, there are activities that encapsulate processing tasks on specific systems, such as running a Spark notebook or calling an Azure function. Finally, there are control flow activities that you can use to implement loops, conditional branching, or manage variable and parameter values.\n\nIntegration runtime\n\nThe pipeline requires compute resources and an execution context in which to run. The pipeline's integration runtime provides this context, and is used to initiate and coordinate the activities in the pipeline.\n\nLinked services\n\nWhile many of the activities are run directly in the integration runtime for the pipeline, some activities depend on external services. For example, a pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database. To enable secure connections to the external services used by your pipelines, you must define linked services for them.\n\n Note\n\nLinked services are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.\n\nDatasets\n\nMost pipelines process data, and the specific data that is consumed and produced by activities in a pipeline is defined using datasets. A dataset defines the schema for each data object that will be used in the pipeline, and has an associated linked service to connect to its source. Activities can have datasets as inputs or outputs.\n\n Note\n\nSimilarly to linked services, datasets are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Transfer and transform data with Azure Synapse Analytics pipelines \n900 XP\nBuild a data pipeline in Azure Synapse Analytics\n1 hr 11 min\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Data Factory\nAzure Synapse Analytics\n\nPipelines are the lifeblood of a data analytics solution. Learn how to use Azure Synapse Analytics pipelines to build integrated data solutions that extract, transform, and load data across diverse systems.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nDescribe core concepts for Azure Synapse Analytics pipelines.\nCreate a pipeline in Azure Synapse Studio.\nImplement a data flow activity in a pipeline.\nInitiate and monitor pipeline runs.\nStart\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Synapse Analytics and data analytics solutions in general. Consider completing the Introduction to Azure Synapse Analytics module first.\n\nThis module is part of these learning paths\nTransfer and transform data with Azure Synapse Analytics pipelines\nIntroduction\n1 min\nUnderstand pipelines in Azure Synapse Analytics\n6 min\nCreate a pipeline in Azure Synapse Studio\n5 min\nDefine data flows\n5 min\nRun a pipeline\n5 min\nExercise - Build a data pipeline in Azure Synapse Analytics\n45 min\nKnowledge check\n3 min\nSummary\n1 min\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/10-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this module, you have learned how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nIn this module, you have:\n\nUnderstood network security options for Azure Synapse Analytics\nConfigured Conditional Access\nConfigured Authentication\nManaged authorization through column and row level security\nManaged sensitive data with Dynamic Data masking\nImplemented encryption in Azure Synapse Analytics\nUnderstood advanced data security options for Azure Synapse Analytics\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/9-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou want to configure a private endpoint. You open up Azure Synapse Studio, go to the manage hub, and see that the private endpoints are greyed out. Why is the option not available?\n\n \n\nAzure Synapse Studio doesn't support the creation of private endpoints.\n\nA Conditional Access policy has to be defined first.\n\nA managed virtual network hasn't been created.\n\n2. \n\nYou require an Azure Synapse Analytics Workspace to access an Azure Data Lake Store using the benefits of the security provided by Microsoft Entra ID. What is the best authentication method to use?\n\n \n\nStorage account keys.\n\nShared access signatures.\n\nManaged identities.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement encryption in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/8-implement-encryption",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nImplement encryption in Azure Synapse Analytics\n3 minutes\n\nIn this section, we will go through Transparent Data Encryption and TokenLibrary for Apache Spark.\n\nWhat is transparent data encryption\n\nTransparent data encryption (TDE) is an encryption mechanism to help you protect Azure Synapse Analytics. It will protect Azure Synapse Analytics against threats of malicious offline activity. The way TDE will do so is by encrypting data at rest. TDE performs real-time encryption as well as decryption of the database, associated backups, and transaction log files at rest without you having to make changes to the application. In order to use TDE for Azure Synapse Analytics, you will have to manually enable it.\n\nWhat TDE does is performing I/O encryption and decryption of data at the page level in real time. When a page is read into memory, it is decrypted. It is encrypted before writing it to disk. TDE encrypts the entire database storage using a symmetric key called a Database Encryption Key (DEK). When you start up a database, the encrypted Database Encryption Key is decrypted. The DEK will then be used for decryption and re-encryption of the database files in the SQL Server database engine. The DEK is protected by the Transparent Data Encryption Protector. This protector can be either a service-managed certificated, which is referred to as service-managed transparent data encryption, or an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption).\n\nWhat is important to understand is that for Azure Synapse Analytics, this TDE protector is set on the server level. There it is inherited by all the databases that are attached or aligned to that server. The term server refers both to server and instance.\n\nService-managed transparent data encryption\n\nAs stated above, the DEK that is protected by the Transparent Encryption protector can be service-managed certificates which we call service-managed TDE. When you look in Azure, that default setting means that the DEK is protected by a built-in certificate unique for each server with encryption algorithm AES256. When a database is in a geo-replicated relationship then primary and the geo-secondary database are protected by the primary database's parent server key. If the databases are connected to the same server, they will also have the same built-in AES 256 certificate. As Microsoft we automatically rotate the certificates in compliance with the internal security policy. The root key is protected by a Microsoft internal secret store. Microsoft also seamlessly moves and manages the keys as needed for geo-replication and restores.\n\nTransparent data encryption with bring your own key for customer-managed transparent data encryption\n\nAs stated above, the DEK that is protected by the Transparent Data Encryption Protector can also be customer managed by bringing an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption). This is also referred to as Bring Your Own Key (BYOK) support for TDE. When this is the scenario that is applicable to you, the TDE Protector that encrypts the DEK is a customer-managed asymmetric key. It is stored in your own and managed Azure Key Vault. Azure Key Vault is Azure's cloud-based external key management system. This managed key never leaves the key vault. The TDE Protector can be generated by the key vault. Another option is to transfer the TDE Protector to the key vault from, for example, an on-premise hardware security module (HSM) device. Azure Synapse Analytics needs to be granted permissions to the customer-owned key vault in order to decrypt and encrypt the DEK. If permissions of the server to the key vault are revoked, a database will be inaccessible, and all data is encrypted.\n\nBy using Azure Key Vault integration for TDE, you have control over the key management tasks such as key rotations, key backups, and key permissions. It also enables you to audit and report on all the TDE protectors when using the Azure Key Vault functionality. The reason for using Key Vault is that it provides you with a central key management system where tightly monitored HSMs are leveraged. It also enables you to separate duties of management of keys and data in order to meet compliance with security policies.\n\nManage transparent data encryption in the Azure portal.\n\nFor Azure Synapse Analytics, you can manage TDE for the database in the Azure portal after you've signed in with the Azure Administrator or Contributor account. The TDE settings can be found under your user database.\n\nIt is by default that the service-managed TDE is used and therefore a TDE certificate is automatically generated for the server that contains that database.\n\nMoving a transparent data encryption protected database\n\nIn some use cases you need to move a database that is protected with TDE. Within Azure, there is no need to decrypt the databases. The TDE settings on the source database or primary database, will be inherited on the target. Some of the operations within Azure that inherited the TDE are:\n\nGeo-restore\nSelf-service point-in-time restore\nRestoration of a deleted database\nActive geo-replication\nCreation of a database copy\nRestore of backup file to Azure SQL Managed Instance\n\nIf you export a TDE-protected database, the exported content is not encrypted. This will be stored in an unencrypted BACPAC file. You need to make sure that you protect this BACPAC file and enable TDE as soon as the import of the bacpac file in the new database is finished.\n\nSecuring your credentials through linked services with TokenLibrary for Apache Spark\n\nIt is quite a common pattern to access data from external sources. Unless the external data source allows anonymous access, it is highly likely that you need to secure your connection with a credential, secret, or connection string.\n\nWithin Azure Synapse Analytics, the integration process is simplified by providing linked services. Doing so, the connection details can be stored in the linked service or an Azure Key Vault. If the Linked Service is created, Apache spark can reference the linked service to apply the connection information in your code. When you want to access files from the Azure Data Lake Storage Gen 2 within your Azure Synapse Analytics Workspace, it uses AAD passthrough for the authentication. Therefore, there is no need to use TokenLibrary. However, to connect to other linked services, you are enabled to make a direct call to the TokenLibrary.\n\nAn example can be found below: In order to connect to other linked services, you are enabled to make a direct call to TokenLibrary by retrieving the connection string. In order to retrieve the connection string, use the getConnectionString function and pass in the linked service name.\n\nScala\nCopy\n// Scala\n// retrieve connectionstring from TokenLibrary\n\nimport com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n\nval connectionString: String = TokenLibrary.getConnectionString(\"<LINKED SERVICE NAME>\")\nprintln(connectionString)\n\nPython\nCopy\n# Python\n# retrieve connectionstring from TokenLibrary\n\nfrom pyspark.sql import SparkSession\n\nsc = SparkSession.builder.getOrCreate()\ntoken_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\nconnection_string = token_library.getConnectionString(\"<LINKED SERVICE NAME>\")\nprint(connection_string)\n\n\nIf you want to Get the connection string as map and parse specific values from a key in the connection string, you can find an example below:\n\nTo parse specific values from a key=value pair in the connection string such as\n\nDefaultEndpointsProtocol=https;AccountName=<AccountName>;AccountKey=<AccountKey>\n\nuse the getConnectionStringAsMap function and pass the key to return the value.\n\nScala\nCopy\n// Linked services can be used for storing and retreiving credentials (e.g, account key)\n// Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\nimport com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n\nval accountKey: String = TokenLibrary.getConnectionStringAsMap(\"<LINKED SERVICE NAME\">).get(\"<KEY NAME>\")\nprintln(accountKey)\n\nPython\nCopy\n# Linked services can be used for storing and retreiving credentials (e.g, account key)\n# Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\nfrom pyspark.sql import SparkSession\n\nsc = SparkSession.builder.getOrCreate()\ntoken_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\naccountKey = token_library.getConnectionStringAsMap(\"<LINKED SERVICE NAME>\").get(\"<KEY NAME>\")\nprint(accountKey)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage sensitive data with Dynamic Data Masking - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/7-manage-sensitive-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 7 of 10\nNext\nManage sensitive data with Dynamic Data Masking\nCompleted\n100 XP\n8 minutes\n\nAzure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics support Dynamic Data Masking. Dynamic Data Masking ensures limited data exposure to nonprivileged users, such that they can't see the data that is being masked. It also helps you in preventing unauthorized access to sensitive information that has minimal impact on the application layer. Dynamic Data Masking is a policy-based security feature. It will hide the sensitive data in a result set of a query that runs over designated database fields.\n\nLet's give you an example how it works. Let's say you work at a bank as a service representative in a call center. Due to compliance, any caller must identify themselves by providing several digits of their credit card number. In this scenario, the full credit card number shouldn't be fully exposed to the service representative in the call center. You can define a masking rule that masks all but the last four digits of a credit card number so that you would get a query that only gives as a result the last four digits of the credit card number. This is just one example that could be equally applied to a variety of personal data such that compliance isn't violated. For Azure Synapse Analytics, the way to set up a Dynamic Data Masking policy is using PowerShell or the REST API. The configuration of the Dynamic Data Masking policy can be done by the Azure SQL Database admin, server admin, or SQL Security Manager roles.\n\nIn Azure Synapse Analytics, you can find Dynamic Data Masking here;\n\nLooking into Dynamic Data Masking Policies:\n\nSQL users excluded from Dynamic Data Masking Policies\n\nThe following SQL users or Microsoft Entra identities can get unmasked data in the SQL query results. Users with administrator privileges are always excluded from masking, and will see the original data without any mask.\n\nMasking rules - Masking rules are a set of rules that define the designated fields to be masked including the masking function that is used. The designated fields can be defined using a database schema name, table name, and column name.\n\nMasking functions - Masking functions are a set of methods that control the exposure of data for different scenarios.\n\nSet up Dynamic Data Masking for your database in Azure Synapse Analytics using PowerShell cmdlets\n\nIn this part, we're going to look into Dynamic Data Masking for a database in Azure Synapse Analytics using PowerShell cmdlets.\n\nData masking policies\nGet-AzSqlDatabaseDataMaskingPolicy\n\nThe Get-AzSqlDatabaseDataMaskingPolicy gets the data masking policy for a database.\n\nThe syntax for the Get-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:\n\nCopy\nGet-AzSqlDatabaseDataMaskingPolicy [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the Get-AzSqlDatabaseDataMaskingPolicy cmdlet does, is getting the data masking policy of an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group you deployed the database in\nServerName: sql server name\nDatabaseName : name of the database\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet-AzSqlDatabaseDataMaskingPolicy\n\nThe Set-AzSqlDatabaseDataMaskingPolicy sets data masking for a database.\n\nThe syntax for the Set-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:\n\nCopy\nSet-AzSqlDatabaseDataMaskingPolicy [-PassThru] [-PrivilegedUsers <String>] [-DataMaskingState <String>]\n [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Set-AzSqlDatabaseDataMaskingPolicy cmdlet does is setting the data masking policy for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nIn addition, you'll need to set the DataMaskingState parameter to specify whether data masking operations are enabled or disabled.\n\nIf the cmdlet succeeds and the PassThru parameter is used, it will return an object describing the current data masking policy in addition to the database identifiers.\n\nDatabase identifiers can include, ResourceGroupName, ServerName, and DatabaseName.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nData masking rules\nGet-AzSqlDatabaseDataMaskingRule\n\nThe Get-AzSqlDatabaseDataMaskingRule Gets the data masking rules from a database.\n\nThe syntax for the Get-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nCopy\nGet-AzSqlDatabaseDataMaskingRule [-SchemaName <String>] [-TableName <String>] [-ColumnName <String>]\n [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Get-AzSqlDatabaseDataMaskingRule cmdlet does it getting either a specific data masking rule or all of the data masking rules for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nYou'd also have to specify the RuleId parameter to specify which rule this cmdlet returns.\n\nIf you don't provide RuleId, all the data masking rules for that Azure SQL database are returned.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nNew-AzSqlDatabaseDataMaskingRule\n\nThe New-AzSqlDatabaseDataMaskingRule creates a data masking rule for a database.\n\nThe syntax for the New-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nCopy\nNew-AzSqlDatabaseDataMaskingRule -MaskingFunction <String> [-PrefixSize <UInt32>] [-ReplacementString <String>]\n [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru] -SchemaName <String>\n -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the New-AzSqlDatabaseDataMaskingRule cmdlet does is creating a data masking rule for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nProviding the TableName and ColumnName is necessary in order to specify the target of the rule.\n\nThe MaskingFunction parameter is necessary to define how the data is masked.\n\nIf MaskingFunction has a value of Number or Text, you can specify the NumberFrom and NumberTo parameters, for number masking, or the PrefixSize, ReplacementString, and SuffixSize for text masking.\n\nIf the command succeeds and the PassThru parameter is used, the cmdlet returns an object describing the data masking rule properties in addition to the rule identifiers.\n\nRule identifiers can be, for example, ResourceGroupName, ServerName, DatabaseName, and RuleID.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nRemove-AzSqlDatabaseDataMaskingRule\n\nThe Remove-AzSqlDatabaseDataMaskingRule removes a data masking rule from a database.\n\nThe syntax for the Remove-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nCopy\nRemove-AzSqlDatabaseDataMaskingRule [-PassThru] [-Force] -SchemaName <String> -TableName <String>\n -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Remove-AzSqlDatabaseDataMaskingRule cmdlet does, is it removes a specific data masking rule from an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule that needs to be removed:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\nRuleId : identifier of the rule\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet-AzSqlDatabaseDataMaskingRule\n\nThe Set-AzSqlDatabaseDataMaskingRule Sets the properties of a data masking rule for a database.\n\nThe syntax for the Set-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nCopy\nSet-AzSqlDatabaseDataMaskingRule [-MaskingFunction <String>] [-PrefixSize <UInt32>]\n [-ReplacementString <String>] [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru]\n -SchemaName <String> -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the Set-AzSqlDatabaseDataMaskingRule cmdlet does is setting a data masking rule for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\nRuleId : identifier of the rule\n\nYou can provide any of the parameters of SchemaName, TableName, and ColumnName to retarget the rule.\n\nSpecify the MaskingFunction parameter to modify how the data is masked.\n\nIf you specify a value of Number or Text for MaskingFunction, you can specify the NumberFrom and NumberTo parameters for number masking or the PrefixSize, ReplacementString, and SuffixSize parameters for text masking.\n\nIf the command succeeds, and if you specify the PassThru parameter, the cmdlet returns an object that describes the data masking rule properties and the rule identifiers.\n\nRule identifiers can be, ResourceGroupName, ServerName, DatabaseName, and RuleId.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet up Dynamic Data Masking for your database in Azure Synapse Analytics using the REST API\n\nFor setting up Dynamic Data Masking in Azure Synapse Analytics, you can also make use of the REST API. It will enable you to programmatically manage data masking policy and rules.\n\nThe REST API will support the following operations:\n\nData masking policies\nCreate Or Update\n\nThe Create Or Update masking policy using the REST API will create or update a database data masking policy.\n\nIn HTTP the following request can be made: > Note: The date of the API will change over time and the version you use will be determined by your needs and the funtionality requred.\n\nHTTP\nCopy\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\nGet\n\nThe Get policy, gets a database data masking policy.\n\nIn HTTP the following request can be made:\n\nHTTP\nCopy\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\n\nData masking rules\n\nCreate Or Update\n\nThe Create or Update masking rule creates or updates a database data masking rule.\n\nIn HTTP the following request can be made:\n\nHTTP\nCopy\nPUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules/{dataMaskingRuleName}?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\ndataMaskingRuleName: the name of the rule for data masking\napi version: version of the api that is used.\nList By Database\n\nThe List By Database request gets a list of database data masking rules.\n\nIn HTTP the following request can be made:\n\nHTTP\nCopy\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\nNext unit: Implement encryption in Azure Synapse Analytics\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Manage authorization through column and row level security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/6-exercise-manage-authorization-through-column-row-level-security",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Manage authorization through column and row level security\n9 minutes\n\nIn this exercise, examples are shown how you can manage authorization through column and row level security.\n\nAn example of column level security\n\nThe following example shows how to restrict TestUser from accessing the SSN column of the Membership table:\n\nCreate Membership table with SSN column used to store social security numbers:\n\nCREATE TABLE Membership\n  (MemberID int IDENTITY,\n   FirstName varchar(100) NULL,\n   SSN char(9) NOT NULL,\n   LastName varchar(100) NOT NULL,\n   Phone varchar(12) NULL,\n   Email varchar(100) NULL);\n\n\nAllow TestUser to access all columns except for the SSN column, which has the sensitive data:\n\nGRANT SELECT ON Membership(MemberID, FirstName, LastName, Phone, Email) TO TestUser;\n\n\nQueries executed as TestUser will fail if they include the SSN column:\n\nSELECT * FROM Membership;\n\n-- Msg 230, Level 14, State 1, Line 12\n-- The SELECT permission was denied on the column 'SSN' of the object 'Membership', database 'CLS_TestDW', schema 'dbo'.\n\nAn example of row level security\n\nThis scenario gives you an example for row level security on an Azure Synapse external table.\n\nThis short example creates three users and an external table with six rows. It then creates an inline table-valued function and a security policy for the external table. The example shows how select statements are filtered for the various users.\n\nPrerequisites\nYou must have a SQL pool. See Create a Synapse SQL pool\nThe server hosting your SQL pool must be registered with AAD and you must have an Azure storage account with Storage Blog Contributor permissions. Follow the steps here.\nCreate a file system for your Azure Storage account. Use Storage Explorer to view your storage account. Right click on containers and select Create file system.\n\nOnce you have the prerequisites in place, create three user accounts that will demonstrate different access capabilities.\n\n--run in master\nCREATE LOGIN Manager WITH PASSWORD = '<user_password>'\nGO\nCREATE LOGIN Sales1 WITH PASSWORD = '<user_password>'\nGO\nCREATE LOGIN Sales2 WITH PASSWORD = '<user_password>'\nGO\n\n--run in master and your SQL pool database\nCREATE USER Manager FOR LOGIN Manager;  \nCREATE USER Sales1  FOR LOGIN Sales1;  \nCREATE USER Sales2  FOR LOGIN Sales2 ;\n\n\nCreate a table to hold data.\n\nCREATE TABLE Sales  \n    (  \n    OrderID int,  \n    SalesRep sysname,  \n    Product varchar(10),  \n    Qty int  \n    );  \n\n\nPopulate the table with six rows of data, showing three orders for each sales representative.\n\nINSERT INTO Sales VALUES (1, 'Sales1', 'Valve', 5);\nINSERT INTO Sales VALUES (2, 'Sales1', 'Wheel', 2);\nINSERT INTO Sales VALUES (3, 'Sales1', 'Valve', 4);\nINSERT INTO Sales VALUES (4, 'Sales2', 'Bracket', 2);\nINSERT INTO Sales VALUES (5, 'Sales2', 'Wheel', 5);\nINSERT INTO Sales VALUES (6, 'Sales2', 'Seat', 5);\n-- View the 6 rows in the table  \nSELECT * FROM Sales;\n\n\nCreate an Azure Synapse external table from the Sales table you just created.\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = '<user_password>';\n\nCREATE DATABASE SCOPED CREDENTIAL msi_cred WITH IDENTITY = 'Managed Service Identity';\n\nCREATE EXTERNAL DATA SOURCE ext_datasource_with_abfss WITH (TYPE = hadoop, LOCATION = 'abfss://<file_system_name@storage_account>.dfs.core.windows.net', CREDENTIAL = msi_cred);\n\nCREATE EXTERNAL FILE FORMAT MSIFormat  WITH (FORMAT_TYPE=DELIMITEDTEXT);\n  \nCREATE EXTERNAL TABLE Sales_ext WITH (LOCATION='<your_table_name>', DATA_SOURCE=ext_datasource_with_abfss, FILE_FORMAT=MSIFormat, REJECT_TYPE=Percentage, REJECT_SAMPLE_VALUE=100, REJECT_VALUE=100)\nAS SELECT * FROM sales;\n\n\nGrant SELECT for the three users on the external table Sales_ext that you created.\n\nGRANT SELECT ON Sales_ext TO Sales1;  \nGRANT SELECT ON Sales_ext TO Sales2;  \nGRANT SELECT ON Sales_ext TO Manager;\n\n\nCreate a new schema, and an inline table-valued function, you may have completed this in example A. The function returns 1 when a row in the SalesRep column is the same as the user executing the query (@SalesRep = USER_NAME()) or if the user executing the query is the Manager user (USER_NAME() = 'Manager').\n\nCREATE SCHEMA Security;  \nGO  \n  \nCREATE FUNCTION Security.fn_securitypredicate(@SalesRep AS sysname)  \n    RETURNS TABLE  \nWITH SCHEMABINDING  \nAS  \n    RETURN SELECT 1 AS fn_securitypredicate_result\nWHERE @SalesRep = USER_NAME() OR USER_NAME() = 'Manager';  \n\n\nCreate a security policy on your external table using the inline table-valued function as a filter predicate. The state must be set to ON to enable the policy.\n\nCREATE SECURITY POLICY SalesFilter_ext\nADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)\nON dbo.Sales_ext  \nWITH (STATE = ON);\n\n\nNow test the filtering predicate, by selecting from the Sales_ext external table. Sign in as each user, Sales1, Sales2, and manager. Run the following command as each user.\n\nSELECT * FROM Sales_ext;\n\n\nThe Manager should see all six rows. The Sales1 and Sales2 users should only see their sales.\n\nAlter the security policy to disable the policy.\n\nALTER SECURITY POLICY SalesFilter_ext  \nWITH (STATE = OFF);  \n\n\nNow the Sales1 and Sales2 users can see all six rows.\n\nConnect to the Azure Synapse database to clean up resources\n\nDROP USER Sales1;\nDROP USER Sales2;\nDROP USER Manager;\n\nDROP SECURITY POLICY SalesFilter_ext;\nDROP TABLE Sales;\nDROP EXTERNAL TABLE Sales_ext;\nDROP EXTERNAL DATA SOURCE ext_datasource_with_abfss ;\nDROP EXTERNAL FILE FORMAT MSIFormat;\nDROP DATABASE SCOPED CREDENTIAL msi_cred; \nDROP MASTER KEY;\n\n\nConnect to logical master to clean up resources.\n\nDROP LOGIN Sales1;\nDROP LOGIN Sales2;\nDROP LOGIN Manager;\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]