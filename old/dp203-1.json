[
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/07-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou want to connect to an Azure Databricks workspace from Azure Data Factory. What must you define in Azure Data Factory?\n\n \n\nA global parameter\n\nA linked service\n\nA customer managed key\n\n2. \n\nYou need to run a notebook in the Azure Databricks workspace referenced by a linked service. What type of activity should you add to a pipeline?\n\n \n\nNotebook\n\nPython\n\nJar\n\n3. \n\nYou need to use a parameter in a notebook. Which library should you use to define parameters with default values and get parameter values that are passed to the notebook?\n\n \n\nnotebook\n\nargparse\n\ndbutils.widget\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/08-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nNotebooks are one of the most common ways that data engineers and data analysts implement data ingestion and processing logic in Azure Databricks. Using Azure Data Factory to run notebooks in a pipeline enables you to create data processing solutions that can be run on-demand, at scheduled intervals, or in response to a specific event.\n\nIn this module, you learned how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Run an Azure Databricks Notebook with Azure Data Factory - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/06-exercise-databricks-factory",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Run an Azure Databricks Notebook with Azure Data Factory\n40 minutes\n\nNow it's your chance to explore how to use Azure Data Factory to run a notebook in Azure Databricks for yourself.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use parameters in a notebook - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/05-notebook-parameters",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse parameters in a notebook\n3 minutes\n\nYou can use parameters to pass variable values to a notebook from the pipeline. Parameterization enables greater flexibility than using hard-coded values in the notebook code.\n\nUsing parameters in a notebook\n\nTo define and use parameters in a notebook, use the dbutils.widgets library in your notebook code.\n\nFor example, the following Python code defines a variable named folder and assigns a default value of data:\n\ndbutils.widgets.text(\"folder\", \"data\")\n\n\nTo retrieve a parameter value, use the get function, like this:\n\nfolder = dbutils.widgets.get(\"folder\")\n\n\nThe get function will retrieve the value for the specific parameter that was passed to the notebook. If no such parameter was passed, it will get the default value of the variable you declared previously.\n\nPassing output values\n\nIn addition to using parameters that can be passed in to a notebook, you can pass values out to the calling application by using the notebook.exit function, as shown here:\n\npath = \"dbfs:/{0}/products.csv\".format(folder)\ndbutils.notebook.exit(path)\n\nSetting parameter values in a pipeline\n\nTo pass parameter values to a Notebook activity, add each parameter to the activity's Base parameters, as shown here:\n\nIn this example, the parameter value is explicitly specified as a property of the Notebook activity. You could also define a pipeline parameter and assign its value dynamically to the Notebook activity's base parameter; adding a further level of abstraction.\n\n Tip\n\nFor more information about using parameters in Azure Data Factory, see How to use parameters, expressions and functions in Azure Data Factory in the Azure Data Factory documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a Notebook activity in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/04-notebook-activity",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a Notebook activity in a pipeline\n5 minutes\n\nAfter you've created a linked service in Azure Data Factory for your Azure Databricks workspace, you can use it to define the connection for a Notebook activity in a pipeline.\n\nTo use a Notebook activity, create a pipeline and from the Databricks category, add a Notebook activity to the pipeline designer surface.\n\nUse the following properties of the Notebook activity to configure it:\n\nExpand table\nCategory\tSetting\tDescriptions\nGeneral\tName\tA unique name for the activity.\n\tDescription\tA meaningful description.\n\tTimeout\tHow long the activity should run before automatically canceling.\n\tRetries\tHow many times should Azure Data Factory try before failing.\n\tRetry interval\tHow long to wait before retrying.\n\tSecure input and output\tDetermines if input and output values are logged.\nAzure Databricks\tAzure Databricks linked service\tThe linked service for the Azure Databricks workspace containing the notebook.\nSettings\tNotebook path\tThe path to the notebook file in the Workspace.\n\tBase parameters\tUsed to pass parameters to the notebook.\n\tAppend libraries\tRequired code libraries that aren't installed by default.\nUser properties\t\tCustom user-defined properties.\nRunning a pipeline\n\nWhen the pipeline containing the Notebook activity is published, you can run it by defining a trigger. You can then monitor pipeline runs in the Monitor section of Azure Data Factory Studio.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a linked service for Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/03-linked-service",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a linked service for Azure Databricks\n5 minutes\n\nTo run notebooks in an Azure Databricks workspace, the Azure Data Factory pipeline must be able to connect to the workspace; which requires authentication. To enable this authenticated connection, you must perform two configuration tasks:\n\nGenerate an access token for your Azure Databricks workspace.\nCreate a linked service in your Azure Data Factory resource that uses the access token to connect to Azure Databricks.\nGenerating an access token\n\nAn access token provides an authentication method for Azure Databricks as an alternative to credentials on the form of a user name and password. You can generate access tokens for applications, specifying an expiration period after which the token must be regenerated and updated in the client applications.\n\nTo create an Access token, use the Generate new token option on the Developer tab of the User Settings page in Azure Databricks portal.\n\nCreating a linked service\n\nTo connect to Azure Databricks from Azure Data Factory, you need to create a linked service for Azure Databricks compute. You can create a linked service in the Linked services page in the Manage section of Azure Data Factory Studio.\n\nWhen you create an Azure Databricks linked service, you must specify the following configuration settings:\n\nExpand table\nSetting\tDescription\nName\tA unique name for the linked service\nDescription\tA meaningful description\nIntegration runtime\tThe integration runtime used to run activities in this linked service. See Integration runtime in Azure Data Factory for more details.\nAzure subscription\tThe Azure subscription in which Azure Databricks is provisioned\nDatabricks workspace\tThe Azure Databricks workspace\nCluster\tThe Spark cluster on which activity code will be run. You can have Azure Databricks dynamically provision a job cluster on-demand or you can specify an existing cluster in the workspace.\nAuthentication type\tHow the linked connection will be authenticated by Azure Databricks. For example, using an access token (in which case, you need to specify the access token you generated for your workspace).\nCluster configuration\tThe Databricks runtime version, Python version, worker node type, and number of worker nodes for your cluster.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Databricks notebooks and pipelines - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/02-databricks-notebooks",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Databricks notebooks and pipelines\n3 minutes\n\nIn Azure Databricks, you can use notebooks to run code written in Python, Scala, SQL, and other languages to ingest and process data. Notebooks provide an interactive interface in which you can run individual code cells and use Markdown to include notes and annotations.\n\nIn many data engineering solutions, code that is written and tested interactively can later be incorporated into an automated data processing workload. On Azure, such workloads are often implemented as pipelines in Azure Data Factory, in which one or more activities are used to orchestrate a series of tasks that can be run on-demand, at scheduled intervals, or in response to an event (such as new data being loaded into a folder in a data lake). Azure Data Factory supports a Notebook activity that can be used to automate the unattended execution of a notebook in an Azure Databricks workspace.\n\n Note\n\nThe same Notebook activity is available in pipelines built in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Databricks enables data engineers to use code in notebooks to ingest and process data. While notebooks are designed to be used interactively, you can use them to encapsulate activities in a data ingestion or processing pipeline that is orchestrated using Azure Data Factory.\n\nIn this module, you'll learn how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\n\n Note\n\nWhile this module focuses on using Azure Databricks notebooks in an Azure Data Factory pipeline, the same principles and techniques apply when using an Azure Synapse Analytics pipeline.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Run Azure Databricks Notebooks with Azure Data Factory - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nRun Azure Databricks Notebooks with Azure Data Factory\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Data Factory\nAzure Databricks\n\nUsing pipelines in Azure Data Factory to run notebooks in Azure Databricks enables you to automate data engineering processes at cloud scale.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nUnderstand Azure Databricks notebooks and pipelines\nmin\nCreate a linked service for Azure Databricks\nmin\nUse a Notebook activity in a pipeline\nmin\nUse parameters in a notebook\nmin\nExercise - Run an Azure Databricks Notebook with Azure Data Factory\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/07-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Databricks SQL enables you to create a scalable relational SQL Warehouse in Azure Databricks. Data analysts can use the SQL Warehouse to query tables and create dashboards of data visualizations.\n\nIn this module, you learned how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use a SQL Warehouse in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/05-exercise-databricks-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use a SQL Warehouse in Azure Databricks\n30 minutes\n\nNow it's your chance to explore Azure Databricks SQL for yourself. In this exercise, you'll use a SQL Warehouse in Azure Databricks to query tables and create a dashboard.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/06-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich of the following workloads is best suited for Azure Databricks SQL?\n\n \n\nRunning Scala code in notebooks to transform data.\n\nQuerying and visualizing data in relational tables.\n\nTraining and deploying machine learning models.\n\n2. \n\nWhich statement should you use to create a database in a SQL warehouse?\n\n \n\nCREATE VIEW\n\nCREATE SCHEMA\n\nCREATE GROUP\n\n3. \n\nYou need to share data visualizations, including charts and tables of data, with users in your organization. What should you create?\n\n \n\nA table\n\nA query\n\nA dashboard\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create databases and tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/03-databases-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate databases and tables\n3 minutes\n\nAfter creating and starting a SQL Warehouse, you can start to work with data in tables.\n\nDatabase schema\n\nAll SQL Warehouses contain a default database schema named default. You can use create tables in this schema in order to analyze data. However, if you need to work with multiple tables in a relational schema, or you have multiple analytical workloads where you want to manage the data (and access to it) separately, you can create custom database schema. To create a database, use the SQL editor to run a CREATE DATABASE or CREATE SCHEMA SQL statement. These statements are equivalent, but CREATE SCHEMA is preferred, as shown in this example:\n\nCREATE SCHEMA salesdata;\n\n\n Tip\n\nFor more information, see CREATE SCHEMA in the Azure Databricks documentation.\n\nTables\n\nYou can use the user interface in the Azure Databricks portal to upload delimited data, or import data from a wide range of common data sources. The imported data is stored in files in Databricks File System (DBFS) storage, and a Delta table is defined for it in the Hive metastore.\n\nIf the data files already exist in storage, or you need to define an explicit schema for the table, you can use a CREATE TABLE SQL statement. For example, the following code creates a table named salesorders in the salesdata database, based on the /data/sales/ folder in DBFS storage.\n\nCREATE TABLE salesdata.salesorders\n(\n    orderid INT,\n    orderdate DATE,\n    customerid INT,\n    ordertotal DECIMAL\n)\nUSING DELTA\nLOCATION '/data/sales/';\n\n\n Tip\n\nFor more information, see CREATE TABLE in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create queries and dashboards - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/04-queries-dashboards",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate queries and dashboards\n3 minutes\n\nAzure Databricks SQL is primarily designed for data analytics and visualization workloads. To support these workloads, users can create queries to retrieve and summarize data from tables, and dashboards to share visualizations of the data.\n\nQueries\n\nYou can use the SQL Editor in the Azure Databricks portal to create a query based on any valid SQL SELECT statement, and then save the query with a meaningful name to be retrieved and run later.\n\nAfter saving the query, you can schedule it to be run automatically at regular intervals to refresh the data, or you can open it and run it interactively.\n\nDashboards\n\nDashboards enable you to display the results of queries, either as tables of data or as graphical visualizations.\n\nYou can create multiple visualizations in a dashboard and share it with users in your organization. As with individual queries, you can schedule the dashboard to refresh is data periodically, and notify subscribers by email that new data is available.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get started with SQL Warehouses - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/02-sql-warehouses",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet started with SQL Warehouses\n3 minutes\n\nSQL Warehouses (formerly known as SQL Endpoints) provide a relational database interface for data in Azure Databricks. The data is stored in files that are abstracted by Delta tables in a hive metastore, but from the perspective of the user or client application, the SQL Warehouse behaves like a relational database.\n\nCreating a SQL Warehouse\n\nWhen you create a premium-tier Azure Databricks workspace, it includes a default SQL Warehouse named Starter Warehouse, which you can use to explore sample data and get started with SQL-based data analytics in Azure Databricks. You can modify the configuration of the default SQL Warehouse to suit your needs, or you can create more SQL Warehouses in your workspace.\n\nYou can manage the SQL Warehouses in your Azure Databricks workspace by using the Azure Databricks portal in the SQL persona view.\n\nSQL Warehouse configuration settings\n\nWhen you create or configure a SQL Warehouse, you can specify the following settings:\n\nName: A name used to identify the SQL Warehouse.\nCluster size: Choose from a range of standard sizes to control the number and size of compute resources used to support the SQL Warehouse. Available sizes range from 2X-Small (a single worker node) to 4X-Large (256 worker nodes). For more information, see Cluster size in the Azure Databricks documentation.\nAuto Stop: The amount of time the cluster will remain running when idle before being stopped. Idle clusters continue to incur charges when running.\nScaling: The minimum and maximum number of clusters used to distribute query processing.\nType: You can create a SQL Warehouse that uses serverless compute for fast, cost-effective on-demand provisioning. Alternatively, you can create a Pro or Classic SQL warehouse.\n\n Note\n\nYou can create a SQL Warehouse with any available size, but if you have insufficient quota for the number of cores required to support your choice in the region where Azure Databricks is provisioned, the SQL Warehouse will fail to start.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nData analysts often use SQL to query relational data and create reports and dashboards. Azure Databricks provides support for SQL-based data analytics through SQL Warehouses and the SQL persona.\n\nIn this module, you'll learn how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\n\n Note\n\nSQL Warehouses and the SQL persona are available in premium-tier Azure Databricks workspaces.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use SQL Warehouses in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse SQL Warehouses in Azure Databricks\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nAzure Databricks provides SQL Warehouses that enable data analysts to work with data using familiar relational SQL queries.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nGet started with SQL Warehouses\nmin\nCreate databases and tables\nmin\nCreate queries and dashboards\nmin\nExercise - Use a SQL Warehouse in Azure Databricks\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/08-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nDelta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.\n\nIn this module, you learned how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/07-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich of the following descriptions best fits Delta Lake?\n\n \n\nA Spark API for exporting data from a relational database into CSV files.\n\nA relational storage layer for Spark that supports tables based on Parquet files.\n\nA synchronization solution that replicates data between SQL Server and Spark clusters.\n\n2. \n\nYou've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?\n\n \n\nCSV\n\nPARQUET\n\nDELTA\n\n3. \n\nWhat feature of Delta Lake enables you to retrieve data from previous versions of a table?\n\n \n\nSpark Structured Streaming\n\nTime Travel\n\nCatalog Tables\n\n4. \n\nYou have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?\n\n \n\nThe table metadata and data files will be deleted.\n\nThe table metadata will be removed from the catalog, but the data files will remain intact.\n\nThe table metadata will remain in the catalog, but the data files will be deleted.\n\n5. \n\nWhen using Spark Structured Streaming, a Delta Lake table can be which of the following?\n\n \n\nOnly a source\n\nOnly a sink\n\nEither a source or a sink\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake for streaming data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/05-use-delta-lake-streaming-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake for streaming data\n5 minutes\n\nAll of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n\nSpark Structured Streaming\n\nA typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.\n\nSpark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n\n Tip\n\nFor more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation.\n\nStreaming with Delta Lake tables\n\nYou can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.\n\nUsing a Delta Lake table as a streaming source\n\nIn the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"/delta/internetorders\")\n\n# Now you can process the streaming data in the dataframe\n# for example, show it:\nstream_df.show()\n\n\n Note\n\nWhen using a Delta Lake table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option.\n\nAfter reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n\nUsing a Delta Lake table as a streaming sink\n\nIn the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format {\"device\":\"Dev1\",\"status\":\"ok\"} New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\nstreamFolder = '/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = '/delta/devicetable'\ncheckpoint_path = '/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n\n\n Note\n\nThe checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.\n\nAfter the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:\n\n%sql\n\nCREATE TABLE DeviceTable\nUSING DELTA\nLOCATION '/delta/devicetable';\n\nSELECT device, status\nFROM DeviceTable;\n\n\nTo stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:\n\ndelta_stream.stop()\n\n\n Tip\n\nFor more information about using Delta Lake tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Delta Lake in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/06-exercise-use-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Delta Lake in Azure Databricks\n40 minutes\n\nNow it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark cluster in Azure Databricks to create and query Delta Lake tables.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create and query catalog tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/04-catalog-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate and query catalog tables\n5 minutes\n\nSo far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark cluster, and work with them using SQL.\n\nExternal vs managed tables\n\nTables in a Spark catalog, including Delta Lake tables, can be managed or external; and it's important to understand the distinction between these kinds of table.\n\nA managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.\nAn external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.\nCreating catalog tables\n\nThere are several ways to create catalog tables.\n\nCreating a catalog table from a dataframe\n\nYou can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:\n\n# Save a dataframe as a managed table\ndf.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n\n## specify a path option to save as an external table\ndf.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n\nCreating a catalog table using SQL\n\nYou can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables. You can run the statement using the SparkSQL API, like the following example:\n\nspark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n\n\nAlternatively you can use the native SQL support in Spark to run the statement:\n\n%sql\n\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION '/mydata'\n\n\n Tip\n\nThe CREATE TABLE statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a CREATE TABLE IF NOT EXISTS statement or the CREATE OR REPLACE TABLE statement.\n\nDefining the table schema\n\nIn all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:\n\n%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n\n\nWhen using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.\n\nUsing the DeltaTableBuilder API\n\nYou can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:\n\nfrom delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"default.ManagedProducts\") \\\n  .addColumn(\"Productid\", \"INT\") \\\n  .addColumn(\"ProductName\", \"STRING\") \\\n  .addColumn(\"Category\", \"STRING\") \\\n  .addColumn(\"Price\", \"FLOAT\") \\\n  .execute()\n\n\nSimilarly to the CREATE TABLE SQL statement, the create method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the createIfNotExists or createOrReplace method.\n\nUsing catalog tables\n\nYou can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:\n\n%sql\n\nSELECT orderid, salestotal\nFROM ManagedSalesOrders\n\n\n Tip\n\nFor more information about working with Delta Lake, see Table batch reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create Delta Lake tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate Delta Lake tables\n5 minutes\n\nDelta lake is built on tables, which provide a relational storage abstraction over files in a data lake.\n\nCreating a Delta Lake table from a dataframe\n\nOne of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored.\n\nFor example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:\n\n# Load a file into a dataframe\ndf = spark.read.load('/data/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndelta_table_path = \"/delta/mydata\"\ndf.write.format(\"delta\").save(delta_table_path)\n\n\nAfter saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a _delta_log folder containing the transaction log for the table.\n\n Note\n\nThe transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.\n\nYou can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:\n\nnew_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n\n\nYou can also add rows from a dataframe to an existing table by using the append mode:\n\nnew_rows_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n\nMaking conditional updates\n\nWhile you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations. For example, you could use the following code to update the price column for all rows with a category column value of \"Accessories\":\n\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n\n\nThe data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.\n\n Tip\n\nFor more information about using the Data Lake API, see the Delta Lake API documentation.\n\nQuerying a previous version of a table\n\nDelta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.\n\nYou can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n\n\nAlternatively, you can specify a timestamp by using the timestampAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get Started with Delta Lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/02-understand-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet Started with Delta Lake\n3 minutes\n\nDelta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.\n\nThe benefits of using Delta Lake in Azure Databricks include:\n\nRelational tables that support querying and data modification. With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.\nSupport for ACID transactions. Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\nData versioning and time travel. Because all transactions are logged in the transaction log, you can track multiple versions of each table row, and even use the time travel feature to retrieve a previous version of a row in a query.\nSupport for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.\nStandard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines.\n\n Tip\n\nFor more information about Delta Lake in Azure Databricks, see the Delta Lake guide in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nLinux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\n\n Note\n\nThe version of Delta Lake available in an Azure Databricks cluster depends on the version of the Databricks Runtime being used. The information in this module reflects Delta Lake version 1.2.1, which is installed with Databricks Runtime version 10.5 - 11.0.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Delta Lake in Azure Databricks\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nDelta Lake is an open source relational storage area for Spark that you can use to implement a data lakehouse architecture in Azure Databricks.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nGet Started with Delta Lake\nmin\nCreate Delta Lake tables\nmin\nCreate and query catalog tables\nmin\nUse Delta Lake for streaming data\nmin\nExercise - Use Delta Lake in Azure Databricks\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/09-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark is a key technology used in big data analytics, and the Spark support in Azure Databricks enables you to combine big data processing in Spark with large-scale data analytics.\n\nIn this module, you learned how to:\n\nDescribe key elements of the Apache Spark architecture.\nCreate and configure a Spark cluster.\nDescribe use cases for Spark.\nUse Spark to process and analyze data stored in files.\nUse Spark to visualize data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Spark in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/07-exercise-databricks-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Spark in Azure Databricks\n45 minutes\n\nNow it's your opportunity to use a Spark cluster in Azure Databricks. In this exercise, you'll use a provided script to provision an Azure Databricks workspace in your Azure subscription; and then create a Spark cluster and use a notebook to analyze and visualize data from files in the databricks file system (DBFS).\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/08-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition best describes Apache Spark?\n\n \n\nA highly scalable relational database management system.\n\nA virtual server with a Python runtime.\n\nA distributed platform for parallel data processing using multiple languages.\n\n2. \n\nYou need to use Spark to analyze data in a parquet file. What should you do?\n\n \n\nLoad the parquet file into a dataframe.\n\nImport the data into a table in a serverless SQL pool.\n\nConvert the data to CSV format.\n\n3. \n\nYou want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?\n\n \n\n%spark\n\n%pyspark\n\n%sql\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark to work with data files - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/05-write-spark-code",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark to work with data files\n5 minutes\n\nOne of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Databricks Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java.\n\nExploring data with dataframes\n\nNatively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.\n\n Note\n\nIn addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.\n\nLoading data into a dataframe\n\nLet's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the data folder in your Databricks File System (DBFS) storage:\n\nProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nIn a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:\n\n%pyspark\ndf = spark.read.load('/data/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n\n\nThe %pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. Here's the equivalent Scala code for the products data example:\n\n%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/data/products.csv\")\ndisplay(df.limit(10))\n\n\nThe magic %spark is used to specify Scala.\n\n Tip\n\nYou can also select the language you want to use for each cell in the Notebook interface.\n\nBoth of the examples shown previously would produce output like this:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nSpecifying a dataframe schema\n\nIn the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nThe following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('/data/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n\n\nThe results would once again be similar to:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nFiltering and grouping dataframes\n\nYou can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:\n\npricelist_df = df.select(\"ProductID\", \"ListPrice\")\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductID\tListPrice\n771\t3399.9900\n772\t3399.9900\n773\t3399.9900\n...\t...\n\nIn common with most data manipulation methods, select returns a new dataframe object.\n\n Tip\n\nSelecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n\npricelist_df = df[\"ProductID\", \"ListPrice\"]\n\nYou can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n\nbikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\n\nTo group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:\n\ncounts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nCategory\tcount\nHeadsets\t3\nWheels\t14\nMountain Bikes\t32\n...\t...\nUsing SQL expressions in Spark\n\nThe Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\n\nCreating database objects in the Spark catalog\n\nThe Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\n\nOne of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:\n\ndf.createOrReplaceTempView(\"products\")\n\n\nA view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.\n\n Note\n\nWe won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:\n\nYou can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.\nYou can save a dataframe as a table by using its saveAsTable method.\nYou can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.\nUsing the Spark SQL API to query data\n\nYou can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.\n\nbikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n\n\nThe results from the code example would look similar to the following table:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\nUsing SQL code\n\nThe previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %sql magic to run SQL code that queries objects in the catalog, like this:\n\n%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n\n\nThe SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:\n\nExpand table\nCategory\tProductCount\nBib-Shorts\t3\nBike Racks\t1\nBike Stands\t1\n...\t...\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/06-visualize-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nVisualize data\n6 minutes\n\nOne of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Databricks provide charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.\n\nUsing built-in notebook charts\n\nWhen you display a dataframe or run a SQL query in a Spark notebook in Azure Databricks, the results are displayed under the code cell. By default, results are rendered as a table, but you can also view the results as a visualization and customize how the chart displays the data, as shown here:\n\nThe built-in visualization functionality in notebooks is useful when you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.\n\nUsing graphics packages in code\n\nThere are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n\nFor example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.\n\nfrom matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n\n\nThe Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n\nThe chart produced by the code would look similar to the following image:\n\nYou can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.\n\n Note\n\nThe Matplotlib and Seaborn libraries may already be installed on Databricks clusters, depending on the Databricks Runtime for the cluster. If not, or if you want to use a different library that is not already installed, you can add it to the cluster. See Cluster Libraries in the Azure Databricks documentation for details.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a Spark cluster - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/03-spark-cluster",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use Apache Spark in Azure Databricks \nAdd\nPrevious\nUnit 3 of 9\nNext\nCreate a Spark cluster\nCompleted\n100 XP\n3 minutes\n\nYou can create one or more clusters in your Azure Databricks workspace by using the Azure Databricks portal.\n\nWhen creating the cluster, you can specify configuration settings, including:\n\nA name for the cluster.\nA cluster mode, which can be:\nStandard: Suitable for single-user workloads that require multiple worker nodes.\nHigh Concurrency: Suitable for workloads where multiple users will be using the cluster concurrently.\nSingle Node: Suitable for small workloads or testing, where only a single worker node is required.\nThe version of the Databricks Runtime to be used in the cluster; which dictates the version of Spark and individual components such as Python, Scala, and others that get installed.\nThe type of virtual machine (VM) used for the worker nodes in the cluster.\nThe minimum and maximum number of worker nodes in the cluster.\nThe type of VM used for the driver node in the cluster.\nWhether the cluster supports autoscaling to dynamically resize the cluster.\nHow long the cluster can remain idle before being shut down automatically.\nHow Azure manages cluster resources\n\nWhen you create an Azure Databricks workspace, a Databricks appliance is deployed as an Azure resource in your subscription. When you create a cluster in the workspace, you specify the types and sizes of the virtual machines (VMs) to use for both the driver and worker nodes, and some other configuration options, but Azure Databricks manages all other aspects of the cluster.\n\nThe Databricks appliance is deployed into Azure as a managed resource group within your subscription. This resource group contains the driver and worker VMs for your clusters, along with other required resources, including a virtual network, a security group, and a storage account. All metadata for your cluster, such as scheduled jobs, is stored in an Azure Database with geo-replication for fault tolerance.\n\nInternally, Azure Kubernetes Service (AKS) is used to run the Azure Databricks control-plane and data-planes via containers running on the latest generation of Azure hardware (Dv3 VMs), with NvMe SSDs capable of blazing 100us latency on high-performance Azure virtual machines with accelerated networking. Azure Databricks utilizes these features of Azure to further improve Spark performance. After the services within your managed resource group are ready, you can manage the Databricks cluster through the Azure Databricks UI and through features such as auto-scaling and auto-termination.\n\n Note\n\nYou also have the option of attaching your cluster to a pool of idle nodes to reduce cluster startup time. For more information, see Pools in the Azure Databricks documentation.\n\nNext unit: Use Spark in notebooks\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]