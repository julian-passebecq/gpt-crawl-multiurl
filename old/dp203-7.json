[
  {
    "title": "Load dimension tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/3-load-dimension-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad dimension tables\n5 minutes\n\nAfter staging dimension data, you can load it into dimension tables using SQL.\n\nUsing a CREATE TABLE AS (CTAS) statement\n\nOne of the simplest ways to load data into a new dimension table is to use a CREATE TABLE AS (CTAS) expression. This statement creates a new table based on the results of a SELECT statement.\n\nCREATE TABLE dbo.DimProduct\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\nSELECT ROW_NUMBER() OVER(ORDER BY ProdID) AS ProdKey,\n    ProdID as ProdAltKey,\n    ProductName,\n    ProductCategory,\n    Color,\n    Size,\n    ListPrice,\n    Discontinued\nFROM dbo.StageProduct;\n\n\n Note\n\nYou can't use IDENTITY to generate a unique integer value for the surrogate key when using a CTAS statement, so this example uses the ROW_NUMBER function to generate an incrementing row number for each row in the results ordered by the ProductID business key in the staged data.\n\nYou can also load a combination of new and updated data into a dimension table by using a CREATE TABLE AS (CTAS) statement to create a new table that UNIONs the existing rows from the dimension table with the new and updated records from the staging table. After creating the new table, you can delete or rename the current dimension table, and rename the new table to replace it.\n\nCREATE TABLE dbo.DimProductUpsert\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\n-- New or updated rows\nSELECT  stg.ProductID AS ProductBusinessKey,\n        stg.ProductName,\n        stg.ProductCategory,\n        stg.Color,\n        stg.Size,\n        stg.ListPrice,\n        stg.Discontinued\nFROM    dbo.StageProduct AS stg\nUNION ALL  \n-- Existing rows\nSELECT  dim.ProductBusinessKey,\n        dim.ProductName,\n        dim.ProductCategory,\n        dim.Color,\n        dim.Size,\n        dim.ListPrice,\n        dim.Discontinued\nFROM    dbo.DimProduct AS dim\nWHERE NOT EXISTS\n(   SELECT  *\n    FROM dbo.StageProduct AS stg\n    WHERE stg.ProductId = dim.ProductBusinessKey\n);\n\nRENAME OBJECT dbo.DimProduct TO DimProductArchive;\nRENAME OBJECT dbo.DimProductUpsert TO DimProduct;\n\n\nWhile this technique is effective in merging new and existing dimension data, lack of support for IDENTITY columns means that it's difficult to generate a surrogate key.\n\n Tip\n\nFor more information, see CREATE TABLE AS SELECT (CTAS) in the Azure Synapse Analytics documentation.\n\nUsing an INSERT statement\n\nWhen you need to load staged data into an existing dimension table, you can use an INSERT statement. This approach works if the staged data contains only records for new dimension entities (not updates to existing entities). This approach is much less complicated than the technique in the last section, which required a UNION ALL and then renaming table objects.\n\nINSERT INTO dbo.DimCustomer\nSELECT CustomerNo AS CustAltKey,\n    CustomerName,\n    EmailAddress,\n    Phone,\n    StreetAddress,\n    City,\n    PostalCode,\n    CountryRegion\nFROM dbo.StageCustomers\n\n\n Note\n\nAssuming the DimCustomer dimension table is defined with an IDENTITY CustomerKey column for the surrogate key (as described in the previous unit), the key will be generated automatically and the remaining columns will be populated using the values retrieved from the staging table by the SELECT query.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nMany enterprise analytical solutions include a relational data warehouse. Data engineers are responsible for implementing ingestion solutions that load data into the data warehouse tables, usually on a regular schedule.\n\nAs a data engineer, you need to be familiar with the considerations and techniques that apply to loading a data warehouse. In this module, we'll focus on ways that you can use SQL to load data into tables in a dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load staging tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/2-load-staging-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad staging tables\n5 minutes\n\nOne of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n\nCreating staging tables\n\nMany organized warehouses have standard structures for staging the database and might even use a specific schema for staging the data. The following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n\n Note\n\nThis example creates a staging table in the default dbo schema. You can also create separate schemas for staging tables with a meaningful name, such as stage so architects and users understand the purpose of the schema.\n\nCREATE TABLE dbo.StageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nUsing the COPY command\n\nYou can use the COPY statement to load data from the data lake, as shown in the following example:\n\n Note\n\nThis is generally the recommended approach to load staging tables due to its high performance throughput.\n\nCOPY INTO dbo.StageProduct\n    (ProductID, ProductName, ...)\nFROM 'https://mydatalake.../data/products*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF'\n);\n\n\n Tip\n\nTo learn more about the COPY statement, see COPY (Transact-SQL) in the Transact-SQL documentation.\n\nUsing external tables\n\nIn some cases, if the data to be loaded is stored in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Azure Synapse Analytics workspace:\n\nCREATE EXTERNAL TABLE dbo.ExternalStageProduct\n (\n     ProductID NVARCHAR(10) NOT NULL,\n     ProductName NVARCHAR(10) NOT NULL,\n ...\n )\nWITH\n (\n    DATE_SOURCE = StagedFiles,\n    LOCATION = 'folder_name/*.parquet',\n    FILE_FORMAT = ParquetFormat\n );\nGO\n\n\n Tip\n\nFor more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load data into a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLoad data into a relational data warehouse\nModule\n10 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nA core responsibility for a data engineer is to implement a data ingestion solution that loads new data into a relational data warehouse.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nLoad staging tables in a data warehouse\nLoad dimension tables in a data warehouse\nLoad time dimensions in a data warehouse\nLoad slowly changing dimensions in a data warehouse\nLoad fact tables in a data warehouse\nPerform post-load optimizations in a data warehouse\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with data warehouses in Azure Synapse Analytics. Consider completing the Analyze data in a relational data warehouse module first.\n\nIntroduction\nmin\nLoad staging tables\nmin\nLoad dimension tables\nmin\nLoad time dimension tables\nmin\nLoad slowly changing dimensions\nmin\nLoad fact tables\nmin\nPerform post load optimization\nmin\nExercise - load data into a relational data warehouse\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nRelational data warehousing skills are essential in multiple data professional roles, including data engineers, data analysts, and data scientists.\n\nIn this module, you learned how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nLearn more\n\nTo learn more about using Azure Synapse Analytics for relational data warehousing, refer to Synapse POC playbook: Data warehousing with dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nIn which of the following table types should an insurance company store details of customer attributes by which claims will be aggregated?\n\n \n\nStaging table\n\nDimension table\n\nFact table\n\n2. \n\nYou create a dimension table for product data, assigning a unique numeric key for each row in a column named ProductKey. The ProductKey is only defined in the data warehouse. What kind of key is ProductKey?\n\n \n\nA surrogate key\n\nAn alternate key\n\nA business key\n\n3. \n\nWhat distribution option would be best for a sales fact table that will contain billions of records?\n\n \n\nHASH\n\nROUND_ROBIN\n\nREPLICATE\n\n4. \n\nYou need to write a query to return the total of the UnitsProduced numeric measure in the FactProduction table aggregated by the ProductName attribute in the FactProduct table. Both tables include a ProductKey surrogate key field. What should you do?\n\n \n\nUse two SELECT queries with a UNION ALL clause to combine the rows in the FactProduction table with those in the FactProduct table.\n\nUse a SELECT query against the FactProduction table with a WHERE clause to filter out rows with a ProductKey that doesn't exist in the FactProduct table.\n\nUse a SELECT query with a SUM function to total the UnitsProduced metric, using a JOIN on the ProductKey surrogate key to match the FactProduction records to the FactProduct records and a GROUP BY clause to aggregate by ProductName.\n\n5. \n\nYou use the RANK function in a query to rank customers in order of the number of purchases they have made. Five customers have made the same number of purchases and are all ranked equally as 1. What rank will the customer with the next highest number of purchases be assigned?\n\n \n\ntwo\n\nsix\n\none\n\n6. \n\nYou need to compare approximate production volumes by product while optimizing query response time. Which function should you use?\n\n \n\nCOUNT\n\nNTILE\n\nAPPROX_COUNT_DISTINCT\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/6-exercise-explore-data-warehouse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore a data warehouse\n45 minutes\n\nNow it's your opportunity to explore a relational data warehouse. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then explore a data warehouse that has been created for you.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/5-query-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery a data warehouse\n10 minutes\n\nWhen the dimension and fact tables in a data warehouse have been loaded with data, you can use SQL to query the tables and analyze the data they contain. The Transact-SQL syntax used to query tables in a Synapse dedicated SQL pool is similar to SQL used in SQL Server or Azure SQL Database.\n\nAggregating measures by dimension attributes\n\nMost data analytics with a data warehouse involves aggregating numeric measures in fact tables by attributes in dimension tables. Because of the way a star or snowflake schema is implemented, queries to perform this kind of aggregation rely on JOIN clauses to connect fact tables to dimension tables, and a combination of aggregate functions and GROUP BY clauses to define the aggregation hierarchies.\n\nFor example, the following SQL queries the FactSales and DimDate tables in a hypothetical data warehouse to aggregate sales amounts by year and quarter:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter\nORDER BY dates.CalendarYear, dates.CalendarQuarter;\n\n\nThe results from this query would look similar to the following table:\n\nExpand table\nCalendarYear\tCalendarQuarter\tTotalSales\n2020\t1\t25980.16\n2020\t2\t27453.87\n2020\t3\t28527.15\n2020\t4\t31083.45\n2021\t1\t34562.96\n2021\t2\t36162.27\n...\t...\t...\n\nYou can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the DimCustomer table:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        custs.City,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nJOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City\nORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City;\n\n\nThis time, the results include a quarterly sales total for each city:\n\nExpand table\nCalendarYear\tCalendarQuarter\tCity\tTotalSales\n2020\t1\tAmsterdam\t5982.53\n2020\t1\tBerlin\t2826.98\n2020\t1\tChicago\t5372.72\n...\t...\t...\t..\n2020\t2\tAmsterdam\t7163.93\n2020\t2\tBerlin\t8191.12\n2020\t2\tChicago\t2428.72\n...\t...\t...\t..\n2020\t3\tAmsterdam\t7261.92\n2020\t3\tBerlin\t4202.65\n2020\t3\tChicago\t2287.87\n...\t...\t...\t..\n2020\t4\tAmsterdam\t8262.73\n2020\t4\tBerlin\t5373.61\n2020\t4\tChicago\t7726.23\n...\t...\t...\t..\n2021\t1\tAmsterdam\t7261.28\n2021\t1\tBerlin\t3648.28\n2021\t1\tChicago\t1027.27\n...\t...\t...\t..\nJoins in a snowflake schema\n\nWhen using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a DimProduct dimension table from which the product categories have been normalized into a separate DimCategory table. A query to aggregate items sold by product category might look similar to the following example:\n\nSELECT  cat.ProductCategory,\n        SUM(sales.OrderQuantity) AS ItemsSold\nFROM dbo.FactSales AS sales\nJOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey\nJOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey\nGROUP BY cat.ProductCategory\nORDER BY cat.ProductCategory;\n\n\nThe results from this query include the number of items sold for each product category:\n\nExpand table\nProductCategory\tItemsSold\nAccessories\t28271\nBits and pieces\t5368\n...\t...\n\n Note\n\nJOIN clauses for FactSales and DimProduct and for DimProduct and DimCategory are both required, even though no fields from DimProduct are returned by the query.\n\nUsing ranking functions\n\nAnother common kind of analytical query is to partition the results based on a dimension attribute and rank the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL ranking functions such as ROW_NUMBER, RANK, DENSE_RANK, and NTILE. These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition:\n\nROW_NUMBER returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on.\nRANK returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties.\nDENSE_RANK ranks rows in a partition the same way as RANK, but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties.\nNTILE returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, NTILE(4) returns the quartile in which a store's sales volume places it.\n\nFor example, consider the following query:\n\nSELECT  ProductCategory,\n        ProductName,\n        ListPrice,\n        ROW_NUMBER() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,\n        RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,\n        DENSE_RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,\n        NTILE(4) OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile\nFROM dbo.DimProduct\nORDER BY ProductCategory;\n\n\nThe query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table:\n\nExpand table\nProductCategory\tProductName\tListPrice\tRowNumber\tRank\tDenseRank\tQuartile\nAccessories\tWidget\t8.99\t1\t1\t1\t1\nAccessories\tKnicknak\t8.49\t2\t2\t2\t1\nAccessories\tSprocket\t5.99\t3\t3\t3\t2\nAccessories\tDoodah\t5.99\t4\t3\t3\t2\nAccessories\tSpangle\t2.99\t5\t5\t4\t3\nAccessories\tBadabing\t0.25\t6\t6\t5\t4\nBits and pieces\tFlimflam\t7.49\t1\t1\t1\t1\nBits and pieces\tSnickity wotsit\t6.99\t2\t2\t2\t1\nBits and pieces\tFlange\t4.25\t3\t3\t3\t2\n...\t...\t...\t...\t...\t...\t...\n\n Note\n\nThe sample results demonstrate the difference between RANK and DENSE_RANK. Note that in the Accessories category, the Sprocket and Doodah products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a RANK of 5 (there are four products more expensive than it) and a DENSE_RANK of 4 (there are three higher prices).\n\nTo learn more about ranking functions, see Ranking Functions (Transact-SQL) in the Azure Synapse Analytics documentation.\n\nRetrieving an approximate count\n\nWhile the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data.\n\nFor example, the following query uses the COUNT function to retrieve the number of sales for each year in a hypothetical data warehouse:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    COUNT(DISTINCT sales.OrderNumber) AS Orders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe results of this query might look similar to the following table:\n\nExpand table\nCalendarYear\tOrders\n2019\t239870\n2020\t284741\n2021\t309272\n...\t...\n\nThe volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the APPROX_COUNT_DISTINCT function as shown in the following example:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe APPROX_COUNT_DISTINCT function uses a HyperLogLog algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table:\n\nExpand table\nCalendarYear\tApproxOrders\n2019\t235552\n2020\t290436\n2021\t304633\n...\t...\n\nThe counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the APPROX_COUNT_DISTINCT function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration.\n\n Note\n\nSee the APPROX_COUNT_DISTINCT function documentation for more details.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/4-load-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad data warehouse tables\n10 minutes\n\nAt a basic level, loading a data warehouse is typically achieved by adding new data from files in a data lake into tables in the data warehouse. The COPY statement is an effective way to accomplish this task, as shown in the following example:\n\nCOPY INTO dbo.StageProducts\n    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)\nFROM 'https://mydatalake.blob.core.windows.net/data/stagedfiles/products/*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF'\n);\n\nConsiderations for designing a data warehouse load process\n\nOne of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n\nIn most cases, you should implement a data warehouse load process that performs tasks in the following order:\n\nIngest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.\nLoad the data from files into staging tables in the relational data warehouse.\nLoad the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.\nLoad the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.\nPerform post-load optimization by updating indexes and table distribution statistics.\n\nAfter using the COPY statement to load data into staging tables, you can use a combination of INSERT, UPDATE, MERGE, and CREATE TABLE AS SELECT (CTAS) statements to load the staged data into dimension and fact tables.\n\n Note\n\nImplementing an effective data warehouse loading solution requires careful consideration of how to manage surrogate keys, slowly changing dimensions, and other complexities inherent in a relational data warehouse schema. To learn more about techniques for loading a data warehouse, consider completing the Load data into a relational data warehouse module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-create-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate data warehouse tables\n10 minutes\n\nNow that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse.\n\nCreating a dedicated SQL pool\n\nTo create a relational data warehouse in Azure Synapse Analytics, you must create a dedicated SQL Pool. The simplest way to do this in an existing Azure Synapse Analytics workspace is to use the Manage page in Azure Synapse Studio, as shown here:\n\nWhen provisioning a dedicated SQL pool, you can specify the following configuration settings:\n\nA unique name for the dedicated SQL pool.\nA performance level for the SQL pool, which can range from DW100c to DW30000c and which determines the cost per hour for the pool when it's running.\nWhether to start with an empty pool or restore an existing database from a backup.\nThe collation of the SQL pool, which determines sort order and string comparison rules for the database. (You can't change the collation after creation).\n\nAfter creating a dedicated SQL pool, you can control its running state in the Manage page of Synapse Studio; pausing it when not required to prevent unnecessary costs.\n\nWhen the pool is running, you can explore it on the Data page, and create SQL scripts to run in it.\n\nConsiderations for creating tables\n\nTo create tables in the dedicated SQL pool, you use the CREATE TABLE (or sometimes the CREATE EXTERNAL TABLE) Transact-SQL statement. The specific options used in the statement depend on the type of table you're creating, which can include:\n\nFact tables\nDimension tables\nStaging tables\n\n Note\n\nThe data warehouse is composed of fact and dimension tables as discussed previously. Staging tables are often used as part of the data warehousing loading process to ingest data from source systems.\n\nWhen designing a star schema model for small or medium sized datasets you can use your preferred database, such as Azure SQL. For larger data sets you may benefit from implementing your data warehouse in Azure Synapse Analytics instead of SQL Server. It's important to understand some key differences when creating tables in Synapse Analytics.\n\nData integrity constraints\n\nDedicated SQL pools in Synapse Analytics don't support foreign key and unique constraints as found in other relational database systems like SQL Server. This means that jobs used to load data must maintain uniqueness and referential integrity for keys, without relying on the table definitions in the database to do so.\n\n Tip\n\nFor more information about constraints in Azure Synapse Analytics dedicated SQL pools, see Primary key, foreign key, and unique key using dedicated SQL pool in Azure Synapse Analytics.\n\nIndexes\n\nWhile Synapse Analytics dedicated SQL pools support clustered indexes as found in SQL Server, the default index type is clustered columnstore. This index type offers a significant performance advantage when querying large quantities of data in a typical data warehouse schema and should be used where possible. However, some tables may include data types that can't be included in a clustered columnstore index (for example, VARBINARY(MAX)), in which case a clustered index can be used instead.\n\n Tip\n\nFor more information about indexing in Azure Synapse Analytics dedicated SQL pools, see Indexes on dedicated SQL pool tables in Azure Synapse Analytics.\n\nDistribution\n\nAzure Synapse Analytics dedicated SQL pools use a massively parallel processing (MPP) architecture, as opposed to the symmetric multiprocessing (SMP) architecture used in most OLTP database systems. In an MPP system, the data in a table is distributed for processing across a pool of nodes. Synapse Analytics supports the following kinds of distribution:\n\nHash: A deterministic hash value is calculated for the specified column and used to assign the row to a compute node.\nRound-robin: Rows are distributed evenly across all compute nodes.\nReplicated: A copy of the table is stored on each compute node.\n\nThe table type often determines which option to choose for distributing the table.\n\nExpand table\nTable type\tRecommended distribution option\nDimension\tUse replicated distribution for smaller tables to avoid data shuffling when joining to distributed fact tables. If tables are too large to store on each compute node, use hash distribution.\nFact\tUse hash distribution with clustered columnstore index to distribute fact tables across compute nodes.\nStaging\tUse round-robin distribution for staging tables to evenly distribute data across compute nodes.\n\n Tip\n\nFor more information about distribution strategies for tables in Azure Synapse Analytics, see Guidance for designing distributed tables using dedicated SQL pool in Azure Synapse Analytics.\n\nCreating dimension tables\n\nWhen you create a dimension table, ensure that the table definition includes surrogate and alternate keys as well as columns for the attributes of the dimension that you want to use to group aggregations. It's often easiest to use an IDENTITY column to auto-generate an incrementing surrogate key (otherwise you need to generate unique keys every time you load data). The following example shows a CREATE TABLE statement for a hypothetical DimCustomer dimension table.\n\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Note\n\nIf desired, you can create a specific schema as a namespace for your tables. In this example, the default dbo schema is used.\n\nIf you intend to use a snowflake schema in which dimension tables are related to one another, you should include the key for the parent dimension in the definition of the child dimension table. For example, the following SQL code could be used to move the geographical address details from the DimCustomer table to a separate DimGeography dimension table:\n\nCREATE TABLE dbo.DimGeography\n(\n    GeographyKey INT IDENTITY NOT NULL,\n    GeographyAlternateKey NVARCHAR(10) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    GeographyKey INT NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nTime dimension tables\n\nMost data warehouses include a time dimension table that enables you to aggregate data by multiple hierarchical levels of time interval. For example, the following example creates a DimDate table with attributes that relate to specific dates.\n\nCREATE TABLE dbo.DimDate\n( \n    DateKey INT NOT NULL,\n    DateAltKey DATETIME NOT NULL,\n    DayOfMonth INT NOT NULL,\n    DayOfWeek INT NOT NULL,\n    DayName NVARCHAR(15) NOT NULL,\n    MonthOfYear INT NOT NULL,\n    MonthName NVARCHAR(15) NOT NULL,\n    CalendarQuarter INT  NOT NULL,\n    CalendarYear INT NOT NULL,\n    FiscalQuarter INT NOT NULL,\n    FiscalYear INT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Tip\n\nA common pattern when creating a dimension table for dates is to use the numeric date in DDMMYYYY or YYYYMMDD format as an integer surrogate key, and the date as a DATE or DATETIME datatype as the alternate key.\n\nCreating fact tables\n\nFact tables include the keys for each dimension to which they're related, and the attributes and numeric measures for specific events or observations that you want to analyze.\n\nThe following code example creates a hypothetical fact table named FactSales that is related to multiple dimensions through key columns (date, customer, product, and store)\n\nCREATE TABLE dbo.FactSales\n(\n    OrderDateKey INT NOT NULL,\n    CustomerKey INT NOT NULL,\n    ProductKey INT NOT NULL,\n    StoreKey INT NOT NULL,\n    OrderNumber NVARCHAR(10) NOT NULL,\n    OrderLineItem INT NOT NULL,\n    OrderQuantity SMALLINT NOT NULL,\n    UnitPrice DECIMAL NOT NULL,\n    Discount DECIMAL NOT NULL,\n    Tax DECIMAL NOT NULL,\n    SalesAmount DECIMAL NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(OrderNumber),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCreating staging tables\n\nStaging tables are used as temporary storage for data as it's being loaded into the data warehouse. A typical pattern is to structure the table to make it as efficient as possible to ingest the data from its external source (often files in a data lake) into the relational database, and then use SQL statements to load the data from the staging tables into the dimension and fact tables.\n\nThe following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n\nCREATE TABLE dbo.StageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nUsing external tables\n\nIn some cases, if the data to be loaded is in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Synapse workspace:\n\n\n-- External data source links to data lake location\nCREATE EXTERNAL DATA SOURCE StagedFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/stagedfiles/'\n);\nGO\n\n-- External format specifies file format\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\nGO\n\n-- External table references files in external data source\nCREATE EXTERNAL TABLE dbo.ExternalStageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DATA_SOURCE = StagedFiles,\n    LOCATION = 'products/*.parquet',\n    FILE_FORMAT = ParquetFormat\n);\nGO\n\n\n Note\n\nFor more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Design a data warehouse schema - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/2-design-star-schema",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDesign a data warehouse schema\n7 minutes\n\nLike all relational databases, a data warehouse contains tables in which the data you want to analyze is stored. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling, in which numerical measures associated with events known as facts can be aggregated by the attributes of associated entities across multiple dimensions. For example, measures associated with a sales order (such as the amount paid or the quantity of items ordered) can be aggregated by attributes of the date on which the sale occurred, the customer, the store, and so on.\n\nTables in a data warehouse\n\nA common pattern for relational data warehouses is to define a schema that includes two kinds of table: dimension tables and fact tables.\n\nDimension tables\n\nDimension tables describe business entities, such as products, people, places, and dates. Dimension tables contain columns for attributes of an entity. For example, a customer entity might have a first name, a last name, an email address, and a postal address (which might consist of a street address, a city, a postal code, and a country or region). In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include two key columns:\n\na surrogate key that is specific to the data warehouse and uniquely identifies each row in the dimension table in the data warehouse - usually an incrementing integer number.\nAn alternate key, often a natural or business key that is used to identify a specific instance of an entity in the transactional source system from which the entity record originated - such as a product code or a customer ID.\n\n Note\n\nWhy have two keys? There are a few good reasons:\n\nThe data warehouse may be populated with data from multiple source systems, which can lead to the risk of duplicate or incompatible business keys.\nSimple numeric keys generally perform better in queries that join lots of tables - a common pattern in data warehouses.\nAttributes of entities may change over time - for example, a customer might change their address. Since the data warehouse is used to support historic reporting, you may want to retain a record for each instance of an entity at multiple points in time; so that, for example, sales orders for a specific customer are counted for the city where they lived at the time the order was placed. In this case, multiple customer records would have the same business key associated with the customer, but different surrogate keys for each discrete address where the customer lived at various times.\n\nAn example of a dimension table for customer might contain the following data:\n\nExpand table\nCustomerKey\tCustomerAltKey\tName\tEmail\tStreet\tCity\tPostalCode\tCountryRegion\n123\tI-543\tNavin Jones\tnavin1@contoso.com\t1 Main St.\tSeattle\t90000\tUnited States\n124\tR-589\tMary Smith\tmary2@contoso.com\t234 190th Ave\tBuffalo\t50001\tUnited States\n125\tI-321\tAntoine Dubois\tantoine1@contoso.com\t2 Rue Jolie\tParis\t20098\tFrance\n126\tI-543\tNavin Jones\tnavin1@contoso.com\t24 125th Ave.\tNew York\t50000\tUnited States\n...\t...\t...\t...\t...\t...\t...\t...\n\n Note\n\nObserve that the table contains two records for Navin Jones. Both records use the same alternate key to identify this person (I-543), but each record has a different surrogate key. From this, you can surmise that the customer moved from Seattle to New York. Sales made to the customer while living in Seattle are associated with the key 123, while purchases made after moving to New York are recorded against record 126.\n\nIn addition to dimension tables that represent business entities, it's common for a data warehouse to include a dimension table that represents time. This table enables data analysts to aggregate data over temporal intervals. Depending on the type of data you need to analyze, the lowest granularity (referred to as the grain) of a time dimension could represent times (to the hour, second, millisecond, nanosecond, or even lower), or dates.\n\nAn example of a time dimension table with a grain at the date level might contain the following data:\n\nExpand table\nDateKey\tDateAltKey\tDayOfWeek\tDayOfMonth\tWeekday\tMonth\tMonthName\tQuarter\tYear\n19990101\t01-01-1999\t6\t1\tFriday\t1\tJanuary\t1\t1999\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20220101\t01-01-2022\t7\t1\tSaturday\t1\tJanuary\t1\t2022\n20220102\t02-01-2022\t1\t2\tSunday\t1\tJanuary\t1\t2022\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20301231\t31-12-2030\t3\t31\tTuesday\t12\tDecember\t4\t2030\n\nThe timespan covered by the records in the table must include the earliest and latest points in time for any associated events recorded in a related fact table. Usually there's a record for every interval at the appropriate grain in between.\n\nFact tables\n\nFact tables store details of observations or events; for example, sales orders, stock balances, exchange rates, or recorded temperatures. A fact table contains columns for numeric values that can be aggregated by dimensions. In addition to the numeric columns, a fact table contains key columns that reference unique keys in related dimension tables.\n\nFor example, a fact table containing details of sales orders might contain the following data:\n\nExpand table\nOrderDateKey\tCustomerKey\tStoreKey\tProductKey\tOrderNo\tLineItemNo\tQuantity\tUnitPrice\tTax\tItemTotal\t\n20220101\t123\t5\t701\t1001\t1\t2\t2.50\t0.50\t5.50\t\n20220101\t123\t5\t765\t1001\t2\t1\t2.00\t0.20\t2.20\t\n20220102\t125\t2\t723\t1002\t1\t1\t4.99\t0.49\t5.48\t\n20220103\t126\t1\t823\t1003\t1\t1\t7.99\t0.80\t8.79\t\n...\t...\t...\t...\t...\t...\t...\t...\t\t...\t...\n\nA fact table's dimension key columns determine its grain. For example, the sales orders fact table includes keys for dates, customers, stores, and products. An order might include multiple products, so the grain represents line items for individual products sold in stores to customers on specific days.\n\nData warehouse schema designs\n\nIn most transactional databases that are used in business applications, the data is normalized to reduce duplication. In a data warehouse however, the dimension data is generally de-normalized to reduce the number of joins required to query the data.\n\nOften, a data warehouse is organized as a star schema, in which a fact table is directly related to the dimension tables, as shown in this example:\n\n]\n\nThe attributes of an entity can be used to aggregate measures in fact tables over multiple hierarchical levels - for example, to find total sales revenue by country or region, city, postal code, or individual customer. The attributes for each level can be stored in the same dimension table. However, when an entity has a large number of hierarchical attribute levels, or when some attributes can be shared by multiple dimensions (for example, both customers and stores have a geographical address), it can make sense to apply some normalization to the dimension tables and create a snowflake schema, as shown in the following example:\n\nIn this case, the DimProduct table has been normalized to create separate dimension tables for product categories and suppliers, and a DimGeography table has been added to represent geographical attributes for both customers and stores. Each row in the DimProduct table contains key values for the corresponding rows in the DimCategory and DimSupplier tables; and each row in the DimCustomer and DimStore tables contains a key value for the corresponding row in the DimGeography table.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nRelational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse.\n\nAzure Synapse Analytics includes a highly scalable relational database engine that is optimized for data warehousing workloads. By using dedicated SQL pools in Azure Synapse Analytics, you can create databases that are capable of hosting and querying huge volumes of data in relational tables.\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data in a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAnalyze data in a relational data warehouse\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nRelational data warehouses are a core element of most enterprise Business Intelligence (BI) solutions, and are used as the basis for data models, reports, and analysis.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nAdd\nPrerequisites\n\nBefore taking this module, you should have:\n\nAn understanding of data fundamentals.\nExperience of querying data with Transact-SQL.\nIntroduction\nmin\nDesign a data warehouse schema\nmin\nCreate data warehouse tables\nmin\nLoad data warehouse tables\nmin\nQuery a data warehouse\nmin\nExercise - Explore a data warehouse\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/9-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nDelta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.\n\nIn this module, you learned how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\n\nTo learn more about using Delta Lake in Azure Synapse Analytics, see Linux Foundation Delta Lake overview in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Delta Lake in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/7-exercise-use-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Delta Lake in Azure Synapse Analytics\n40 minutes\n\nNow it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark pool in Azure Synapse Analytics to create and query Delta Lake tables, and query Delta Lake data from a serverless SQL pool.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/8-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich of the following descriptions best fits Delta Lake?\n\n \n\nA Spark API for exporting data from a relational database into CSV files.\n\nA relational storage layer for Spark that supports tables based on Parquet files.\n\nA synchronization solution that replicates data between SQL pools and Spark pools.\n\n2. \n\nYou've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?\n\n \n\nCSV\n\nPARQUET\n\nDELTA\n\n3. \n\nWhat feature of Delta Lake enables you to retrieve data from previous versions of a table?\n\n \n\nSpark Structured Streaming\n\nTime Travel\n\nCatalog Tables\n\n4. \n\nYou have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?\n\n \n\nThe table metadata and data files will be deleted.\n\nThe table metadata will be removed from the catalog, but the data files will remain intact.\n\nThe table metadata will remain in the catalog, but the data files will be deleted.\n\n5. \n\nWhen using Spark Structured Streaming, a Delta Lake table can be which of the following?\n\n \n\nOnly a source\n\nOnly a sink\n\nEither a source or a sink\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in a SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/6-delta-with-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake in a SQL pool\n5 minutes\n\nDelta Lake is designed as a transactional, relational storage layer for Apache Spark; including Spark pools in Azure Synapse Analytics. However, Azure Synapse Analytics also includes a serverless SQL pool runtime that enables data analysts and engineers to run SQL queries against data in a data lake or a relational database.\n\n Note\n\nYou can only query data from Delta Lake tables in a serverless SQL pool; you can't update, insert, or delete data.\n\nQuerying delta formatted files with OPENROWSET\n\nThe serverless SQL pool in Azure Synapse Analytics includes support for reading delta format files; enabling you to use the SQL pool to query Delta Lake tables. This approach can be useful in scenarios where you want to use Spark and Delta tables to process large quantities of data, but use the SQL pool to run queries for reporting and analysis of the processed data.\n\nIn the following example, a SQL SELECT query reads delta format data using the OPENROWSET function.\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK 'https://mystore.dfs.core.windows.net/files/delta/mytable/',\n        FORMAT = 'DELTA'\n    ) AS deltadata\n\n\nYou could run this query in a serverless SQL pool to retrieve the latest data from the Delta Lake table stored in the specified file location.\n\nYou could also create a database and add a data source that encapsulates the location of your Delta Lake data files, as shown in this example:\n\nCREATE DATABASE MyDB\n      COLLATE Latin1_General_100_BIN2_UTF8;\nGO;\n\nUSE MyDB;\nGO\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStore\nWITH\n(\n    LOCATION = 'https://mystore.dfs.core.windows.net/files/delta/'\n);\nGO\n\nSELECT TOP 10 *\nFROM OPENROWSET(\n        BULK 'mytable',\n        DATA_SOURCE = 'DeltaLakeStore',\n        FORMAT = 'DELTA'\n    ) as deltadata;\n\n\n Note\n\nWhen working with Delta Lake data, which is stored in Parquet format, it's generally best to create a database with a UTF-8 based collation in order to ensure string compatibility.\n\nQuerying catalog tables\n\nThe serverless SQL pool in Azure Synapse Analytics has shared access to databases in the Spark metastore, so you can query catalog tables that were created using Spark SQL. In the following example, a SQL query in a serverless SQL pool queries a catalog table that contains Delta Lake data:\n\n-- By default, Spark catalog tables are created in a database named \"default\"\n-- If you created another database using Spark SQL, you can use it here\nUSE default;\n\nSELECT * FROM MyDeltaTable;\n\n\n Tip\n\nFor more information about using Delta Tables from a serverless SQL pool, see Query Delta Lake files using serverless SQL pool in Azure Synapse Analytics in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake with streaming data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/5-use-delta-lake-streaming-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake with streaming data\n6 minutes\n\nAll of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n\nSpark Structured Streaming\n\nA typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.\n\nSpark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n\n Tip\n\nFor more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation.\n\nStreaming with Delta Lake tables\n\nYou can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.\n\nUsing a Delta Lake table as a streaming source\n\nIn the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"/delta/internetorders\")\n\n# Now you can process the streaming data in the dataframe\n# for example, show it:\nstream_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .start()\n\n\n Note\n\nWhen using a Delta Lake table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option.\n\nAfter reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n\nUsing a Delta Lake table as a streaming sink\n\nIn the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format {\"device\":\"Dev1\",\"status\":\"ok\"} New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\ninputPath = '/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = '/delta/devicetable'\ncheckpoint_path = '/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n\n\n Note\n\nThe checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.\n\nAfter the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:\n\n%%sql\n\nCREATE TABLE DeviceTable\nUSING DELTA\nLOCATION '/delta/devicetable';\n\nSELECT device, status\nFROM DeviceTable;\n\n\nTo stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:\n\ndelta_stream.stop()\n\n\n Tip\n\nFor more information about using Delta Lake tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create catalog tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/4-catalog-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate catalog tables\n6 minutes\n\nSo far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark pool, and work with them using SQL.\n\nExternal vs managed tables\n\nTables in a Spark catalog, including Delta Lake tables, can be managed or external; and it's important to understand the distinction between these kinds of table.\n\nA managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.\nAn external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.\nCreating catalog tables\n\nThere are several ways to create catalog tables.\n\nCreating a catalog table from a dataframe\n\nYou can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:\n\n# Save a dataframe as a managed table\ndf.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n\n## specify a path option to save as an external table\ndf.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n\nCreating a catalog table using SQL\n\nYou can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables. You can run the statement using the SparkSQL API, like the following example:\n\nspark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n\n\nAlternatively you can use the native SQL support in Spark to run the statement:\n\n%%sql\n\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION '/mydata'\n\n\n Tip\n\nThe CREATE TABLE statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a CREATE TABLE IF NOT EXISTS statement or the CREATE OR REPLACE TABLE statement.\n\nDefining the table schema\n\nIn all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:\n\n%%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n\n\nWhen using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.\n\nUsing the DeltaTableBuilder API\n\nYou can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:\n\nfrom delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"default.ManagedProducts\") \\\n  .addColumn(\"Productid\", \"INT\") \\\n  .addColumn(\"ProductName\", \"STRING\") \\\n  .addColumn(\"Category\", \"STRING\") \\\n  .addColumn(\"Price\", \"FLOAT\") \\\n  .execute()\n\n\nSimilarly to the CREATE TABLE SQL statement, the create method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the createIfNotExists or createOrReplace method.\n\nUsing catalog tables\n\nYou can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:\n\n%%sql\n\nSELECT orderid, salestotal\nFROM ManagedSalesOrders\n\n\n Tip\n\nFor more information about working with Delta Lake, see Table batch reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create Delta Lake tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/3-create-delta-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate Delta Lake tables\n5 minutes\n\nDelta lake is built on tables, which provide a relational storage abstraction over files in a data lake.\n\nCreating a Delta Lake table from a dataframe\n\nOne of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored.\n\nFor example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:\n\n# Load a file into a dataframe\ndf = spark.read.load('/data/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndelta_table_path = \"/delta/mydata\"\ndf.write.format(\"delta\").save(delta_table_path)\n\n\nAfter saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a _delta_log folder containing the transaction log for the table.\n\n Note\n\nThe transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.\n\nYou can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:\n\nnew_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n\n\nYou can also add rows from a dataframe to an existing table by using the append mode:\n\nnew_rows_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n\nMaking conditional updates\n\nWhile you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations. For example, you could use the following code to update the price column for all rows with a category column value of \"Accessories\":\n\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n\n\nThe data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.\n\n Tip\n\nFor more information about using the Delta Lake API, see the Delta Lake API documentation.\n\nQuerying a previous version of a table\n\nDelta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.\n\nYou can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n\n\nAlternatively, you can specify a timestamp by using the timestampAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nLinux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\n\n Note\n\nThe version of Delta Lake available in an Azure Synapse Analytics pool depends on the version of Spark specified in the pool configuration. The information in this module reflects Delta Lake version 1.0, which is installed with Spark 3.1.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Delta Lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/2-understand-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Delta Lake\n5 minutes\n\nDelta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.\n\nThe benefits of using Delta Lake in a Synapse Analytics Spark pool include:\n\nRelational tables that support querying and data modification. With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.\nSupport for ACID transactions. Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\nData versioning and time travel. Because all transactions are logged in the transaction log, you can track multiple versions of each table row and even use the time travel feature to retrieve a previous version of a row in a query.\nSupport for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.\nStandard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the serverless SQL pool in Azure Synapse Analytics to query Delta Lake tables in SQL.\n\n Tip\n\nFor more information about Delta Lake in Azure Synapse Analytics, see What is Delta Lake in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Delta Lake in Azure Synapse Analytics\nModule\n9 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nDelta Lake is an open source relational storage area for Spark that you can use to implement a data lakehouse architecture in Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\nAdd\nPrerequisites\n\nTo get the best from this module, you will need existing knowledge of working with Spark pools in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first.\n\nIntroduction\nmin\nUnderstand Delta Lake\nmin\nCreate Delta Lake tables\nmin\nCreate catalog tables\nmin\nUse Delta Lake with streaming data\nmin\nUse Delta Lake in a SQL pool\nmin\nExercise - Use Delta Lake in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]