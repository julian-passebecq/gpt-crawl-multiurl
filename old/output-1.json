[
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/11-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nYou have just learned how to build reports using Power BI within Azure Synapse Analytics.\n\nIn this module, you have:\n\nlearned how to integrate a Synapse workspace and Power BI\nlearned how to optimize the integration with Power BI\nexperimented with query performance improvements with materialized views and result-set caching\nvisualized data with SQL serverless and created a Power BI report using serverless SQL Pool.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/10-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhat data source connection method loads all of the data into the Power BI cache, and runs queries against the ingested data?\n\n \n\nDirectQuery.\n\nImport Connection.\n\nHybrid Connection\n\n2. \n\nWhat can be considered a benefit of materialized views?\n\n \n\nA different data distribution compared to the base tables.\n\nDecreased query performance for complex queries with JOINs and aggregate functions, but increased performance for simple queries.\n\nInstant query response times for repetitive query patterns.\n\n3. \n\nWhat is a limitation of result-set caching?\n\n \n\nThe maximum size of the result set cache is 1 TB per database.\n\nUsers cant manually empty the result set cache.\n\nPausing the database will clean the cache.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data with serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/9-visualize-data-serverless-sql-pools",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics \nAdd\nPrevious\nUnit 9 of 11\nNext\nVisualize data with serverless SQL pools\nCompleted\n100 XP\n6 minutes\n\nIn the Azure portal, navigate to your Synapse Workspace. In the Overview (1) tab, copy the Serverless SQL endpoint (2):\n\nSwitch back to Power BI Desktop. Create a new report, then click Get data.\n\nSelect Azure (1) on the left-hand menu, then select Azure Synapse Analytics (SQL DW) (2). Finally, click Connect (3):\n\nPaste the endpoint to the Serverless SQL endpoint identified on the first step into the Server field (1), enter demo for the Database (2), select DirectQuery (3), then paste the query below (4) into the expanded Advanced options section of the SQL Server database dialog. Finally, click OK (5).\n\nSQL\nCopy\nSELECT TOP (100) [Year]\n,[Month]\n,[Day]\n,[TotalAmount]\n,[ProfitAmount]\n,[TransactionsCount]\nFROM [dbo].[2019Q1Sales]\n\n\n(If prompted) Select the Microsoft account (1) option on the left, Sign in (2) (with the same credentials you use for connecting to the Synapse workspace) and click Connect (3).\n\nSelect Load in the preview data window and wait for the connection to be configured.\n\nAfter the data loads, select Line chart from the Visualizations menu.\n\nSelect the line chart visualization and configure it as follows to show Profit, Amount, and Transactions count by day:\n\nAxis: Day\nValues: ProfitAmount, TotalAmount\nSecondary values: TransactionsCount\n\nSelect the line chart visualization and configure it to sort in ascending order by the day of transaction. To do this, select More options next to the chart visualization.\n\nSelect Sort ascending.\n\nSelect More options next to the chart visualization again.\n\nSelect Sort by, then Day.\n\nClick Save in the top-left corner.\n\nSpecify a file name (1), such as synapse-sql-serverless, then click Save (2).\n\nClick Publish above the saved report. Make sure that, in Power BI Desktop, you are signed in with the same account you use in the Power BI portal and in Synapse Studio. You can switch to the proper account from the right topmost corner of the window. In the Publish to Power BI dialog, select the workspace you linked to Synapse (for example, synapse-training), then click Select.\n\nWait until the publish operation successfully completes.\n\nIn Azure Synapse Studio, navigate to the Develop hub.\n\nExpand the Power BI group, expand your Power BI linked service (for example, synapse-training), right-click on Power BI reports and select Refresh (1) to update the list of reports. You should see the two Power BI reports you created in this lab (synapse-lab and synapse-sql-serverless (2)).\n\nSelect the synapse-lab report. You can view and edit the report directly within Synapse Studio!\n\nSelect the synapse-sql-serverless report. You should be able to view and edit this report as well.\n\nNext unit: Knowledge check\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Improve performance with materialized views and result-set caching - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/8-exercise-improve-performance-materialized-views-result-set-caching",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics \nAdd\nPrevious\nUnit 8 of 11\nNext\nExercise - Improve performance with materialized views and result-set caching\nCompleted\n100 XP\n7 minutes\nTask 1: Improve performance with materialized views\n\nIn Azure Synapse Studio, select Develop from the left-hand menu.\n\nSelect +, then SQL script.\n\nConnect to SQLPool01, then execute the following query to get an estimated execution plan and observe the total cost and number of operations:\n\nSQL\nCopy\nEXPLAIN\nSELECT * FROM\n(\n    SELECT\n    FS.CustomerID\n    ,P.Seasonality\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n    ,avg(FS.TotalAmount) as AvgTotalAmount\n    ,avg(FS.ProfitAmount) as AvgProfitAmount\n    ,sum(FS.TotalAmount) as TotalAmount\n    ,sum(FS.ProfitAmount) as ProfitAmount\nFROM\n    wwi.SaleSmall FS\n    JOIN wwi.Product P ON P.ProductId = FS.ProductId\n    JOIN wwi.Date D ON FS.TransactionDateId = D.DateId\nGROUP BY\n    FS.CustomerID\n    ,P.Seasonality\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n) T\n\n\nThe results should look similar to this:\n\nXML\nCopy\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<dsql_query number_nodes=\"1\" number_distributions=\"60\" number_distributions_per_node=\"60\">\n    <sql>SELECT count(*) FROM\n(\n    SELECT\n    FS.CustomerID\n    ,P.Seasonality\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n    ,avg(FS.TotalAmount) as AvgTotalAmount\n    ,avg(FS.ProfitAmount) as AvgProfitAmount\n    ,sum(FS.TotalAmount) as TotalAmount\n    ,sum(FS.ProfitAmount) as ProfitAmount\nFROM\n    wwi.SaleSmall FS\n    JOIN wwi.Product P ON P.ProductId = FS.ProductId\n    JOIN wwi.Date D ON FS.TransactionDateId = D.DateId\nGROUP BY\n    FS.CustomerID\n    ,P.Seasonality\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n) T</sql>\n    <dsql_operations total_cost=\"10.61376\" total_number_operations=\"12\">\n\n\nReplace the query with the following to create a materialized view that can support the above query:\n\nSQL\nCopy\nIF EXISTS(select * FROM sys.views where name = 'mvCustomerSales')\n    DROP VIEW wwi_perf.mvCustomerSales\n    GO\n\nCREATE MATERIALIZED VIEW\n    wwi_perf.mvCustomerSales\nWITH\n(\n    DISTRIBUTION = HASH( CustomerId )\n)\nAS\nSELECT\n    S.CustomerId\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n    ,SUM(S.TotalAmount) as TotalAmount\n    ,SUM(S.ProfitAmount) as TotalProfit\nFROM\n    [wwi_perf].[Sale_Partition02] S\n    join [wwi].[Date] D on\n        S.TransactionDateId = D.DateId\nGROUP BY\n    S.CustomerId\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\nGO\n\n\nThis query will take between 30 and 120 seconds to complete.\n\nWe first drop the view if it exists, since we create it in an earlier lab.\n\nRun the following query to check that it actually hits the created materialized view.\n\nSQL\nCopy\nEXPLAIN\nSELECT * FROM\n(\nSELECT\nFS.CustomerID\n,P.Seasonality\n,D.Year\n,D.Quarter\n,D.Month\n,avg(FS.TotalAmount) as AvgTotalAmount\n,avg(FS.ProfitAmount) as AvgProfitAmount\n,sum(FS.TotalAmount) as TotalAmount\n,sum(FS.ProfitAmount) as ProfitAmount\nFROM\n    wwi_pbi.SaleSmall FS\n    JOIN wwi_pbi.Product P ON P.ProductId = FS.ProductId\n    JOIN wwi_pbi.Date D ON FS.TransactionDateId = D.DateId\nGROUP BY\n    FS.CustomerID\n    ,P.Seasonality\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n) T\n\n\n\nSwitch back to the Power BI Desktop report, then click on Transform data.\n\nIn the Power Query editor, open the settings page of the Source (1) step in the query. Expand the Advanced options (2) section, paste the following query (3) to use the new materialized view, then click OK (4).\n\nSQL\nCopy\nSELECT [CustomerID]\n,[Seasonality]\n,[Year]\n,[Quarter]\n,[Month]\n,[TotalAmount]\n,[ProfitAmount]\n,[cb]\nFROM [wwi].[mvCustomerSales]\n\n\nSelect Close & Apply on the topmost left corner of the editor window to apply the query and fetch the initial schema in the Power BI designer window.\n\nClick the Refresh button above the report to submit the query against the new materialized view.\n\nNotice that the data refresh only takes a few seconds now, compared to before.\n\nCheck the duration of the query again in Synapse Studio, in the monitoring hub (1), under SQL requests (2). Notice that the Power BI queries using the new materialized view run much faster (Duration ~ 10s) (3).\n\nTask 2: Improve performance with result-set caching\n\nIn Azure Synapse Studio, select Develop from the left-hand menu.\n\nSelect +, then SQL script.\n\nConnect to SQLPool01, then execute the following query to check if result set caching is turned on in the current SQL pool:\n\nSQL\nCopy\nSELECT\n    name\n    ,is_result_set_caching_on\nFROM\n    sys.databases\n\n\nIf False is returned for SQLPool01, execute the following query to activate it (you need to run it on the master database):\n\nSQL\nCopy\nALTER DATABASE [SQLPool01]\nSET RESULT_SET_CACHING ON\n\n\nConnect to SQLPool01 and use the master database:\n\n Important\n\nThe operations to create result set cache and retrieve data from the cache happen on the control node of a Synapse SQL pool instance. When result set caching is turned ON, running queries that return large result set (for example, >1GB) can cause high throttling on the control node and slow down the overall query response on the instance. Those queries are commonly used during data exploration or ETL operations. To avoid stressing the control node and cause performance issue, users should turn OFF result set caching on the database before running those types of queries.\n\nNext move back to the Power BI Desktop report and hit the Refresh button to submit the query again.\n\nAfter the data refreshes, hit Refresh once more to ensure we hit the result set cache.\n\nCheck the duration of the query again in Synapse Studio, in the Monitoring hub (1) - SQL Requests (2) page. Notice that now it runs almost instantly (Duration = 0s) (4).\n\nNext unit: Visualize data with serverless SQL pools\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe Power BI optimization options - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/7-describe-optimization-options",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe Power BI optimization options\n7 minutes\n\nAs the amount of data stored and queried continues to rise, it becomes increasingly important to have the most price-performant data warehouse. To improve the performance of applications and reports without adding any more cost, result-set caching and materialized views are second to none.\n\nResult-set caching\n\nResult-set caching helps with predictable and repetitive query patterns and enables instant query response times while reducing time-to-insight for data analysts and reporting users. Result-set caching is an effective solution for interactive dashboard performance improvement.\n\nWith result-set caching enabled, Azure Synapse Analytics automatically caches results from repetitive queries, causing subsequent query executions to return results from the persisted cache that omits complete query execution. In addition to saving compute cycles, queries satisfied by result-set cache do not use any concurrency slots and thus do not count against existing concurrency limits. For security reasons, only users with the appropriate security credentials can access the result sets in cache.\n\nMaterialized views to improve performance\n\nAnother feature that significantly enhances query performance for a broad set of queries is materialized view support. A materialized view improves complex queries' performance (typically queries with joins and aggregations) while offering simple maintenance operations.\n\nWhen materialized views are created, the Azure Synapse Analytics dedicated SQL Pool query optimizer transparently and automatically rewrite user queries to leverage deployed materialized views, leading to improved query performance. Best of all, as the data gets loaded into base tables, the query optimizer automatically maintains and refreshes materialized views, providing easier maintenance and management. As the user queries use materialized views, queries run significantly faster and use fewer system resources. A materialized view pre-computes, stores, and maintains its data in a dedicated SQL pool just like a table. There's no recomputation needed each time a materialized view is used. That's why queries that use all or a subset of the data in a materialized view can get faster performance. The more complex and expensive the query within the view is, the more significant potential for execution-time savings.\n\nA properly designed materialized view provides the following benefits:\n\nReduce the execution time for complex queries with JOINs and aggregate functions. The more complex the query, the higher the potential for execution-time saving. The most benefit is gained when a query's computation cost is high and the resulting data set is small.\nThe optimizer in the dedicated SQL pool can automatically use deployed materialized views to improve query execution plans. This process is transparent to users providing faster query performance and doesn't require queries to make direct reference to the materialized views.\nMaterialized views require low maintenance. All incremental data changes from the base tables are automatically added to the materialized views in a synchronous manner. This design allows querying materialized views to return the same data as directly querying the base tables.\nThe data in a materialized view can be distributed differently from the base tables.\nData in materialized views gets the same high availability and resiliency benefits as data in regular tables.\n\nThe materialized views implemented in a dedicated SQL pool also provide the following additional benefits:\n\nComparing to other data warehouse providers, the materialized views implemented in Azure Synapse Analytics also provide the following additional benefits:\n\nAutomatic and synchronous data refresh with data changes in base tables. No user action is required.\nBroad aggregate function support. See CREATE MATERIALIZED VIEW AS SELECT (Transact-SQL).\nThe support for query-specific materialized view recommendation. See EXPLAIN (Transact-SQL).\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Create a new Power BI report in Synapse Studio - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/6-exercise-create-new-report-synapse-studio",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics \nAdd\nPrevious\nUnit 6 of 11\nNext\nExercise - Create a new Power BI report in Synapse Studio\nCompleted\n100 XP\n6 minutes\n\nIn Azure Synapse Studio, select Develop from the left menu.\n\nSelect + (1), then SQL script (2).\n\nConnect to SQLPool01, then execute the following query to get an approximation of its execution time. This will be the query we'll use to bring data in the Power BI report you'll build later in this exercise.\n\nSQL\nCopy\nSELECT count(*) FROM\n(\n    SELECT\n        FS.CustomerID\n        ,P.Seasonality\n        ,D.Year\n        ,D.Quarter\n        ,D.Month\n        ,avg(FS.TotalAmount) as AvgTotalAmount\n        ,avg(FS.ProfitAmount) as AvgProfitAmount\n        ,sum(FS.TotalAmount) as TotalAmount\n        ,sum(FS.ProfitAmount) as ProfitAmount\n    FROM\n        wwi.SaleSmall FS\n        JOIN wwi.Product P ON P.ProductId = FS.ProductId\n        JOIN wwi.Date D ON FS.TransactionDateId = D.DateId\n    GROUP BY\n        FS.CustomerID\n        ,P.Seasonality\n        ,D.Year\n        ,D.Quarter\n        ,D.Month\n) T\n\n\nYou should see a query result of 194683820.\n\nTo connect to your data source, open the downloaded .pbids file in Power BI Desktop. Select the Microsoft account (1) option on the left, Sign in (2) (with the same credentials you use for connecting to the Synapse workspace) and click Connect (3).\n\nIn the Navigator dialog, right-click on the root database node and select Transform data.\n\nSelect the DirectQuery (1) option in the connection settings dialog since our intention is not to bring a copy of the data into Power BI but to be able to query the data source while working with the report visualizations. Click OK (2) and wait a few seconds while the connection is configured.\n\nIn the Power Query editor, open the settings page of the Source (1) step in the query. Expand the Advanced options (2) section, paste the following query and click OK (3).\n\nSQL\nCopy\nSELECT * FROM\n(\n    SELECT\n        FS.CustomerID\n        ,P.Seasonality\n        ,D.Year\n        ,D.Quarter\n        ,D.Month\n        ,avg(FS.TotalAmount) as AvgTotalAmount\n        ,avg(FS.ProfitAmount) as AvgProfitAmount\n        ,sum(FS.TotalAmount) as TotalAmount\n        ,sum(FS.ProfitAmount) as ProfitAmount\n    FROM\n        wwi.SaleSmall FS\n        JOIN wwi.Product P ON P.ProductId = FS.ProductId\n        JOIN wwi.Date D ON FS.TransactionDateId = D.DateId\n    GROUP BY\n        FS.CustomerID\n        ,P.Seasonality\n        ,D.Year\n        ,D.Quarter\n        ,D.Month\n) T\n\n\n Note\n\nThis step will take at least 40-60 seconds to execute since it submits the query directly on the Synapse SQL Pool connection.\n\nSelect Close & Apply on the topmost left corner of the editor window to apply the query and fetch the initial schema in the Power BI designer window.\n\nBack to the Power BI report editor, expand the Visualizations menu on the right, then select the Line and stacked column chart visualization.\n\nSelect the newly created chart to expand its properties pane. Using the expanded Fields menu, configure the visualization as follows:\n\nShared axis: Year, Quarter\nColumn series: Seasonality\nColumn values: TotalAmount\nLine values: ProfitAmount\n\n Note\n\nIt will take around 40-60 seconds for the visualization to render due to the live query execution on the Synapse dedicated SQL pool.\n\nSwitching back to Azure Synapse Studio, you can check the query executed while configuring the visualization in the Power BI Desktop application. Open the Monitor hub, and under the Activities section, open the SQL requests monitor. Make sure you select SQLPool01 in the Pool filter, as by default, SQL on-demand is selected.\n\nIdentify the query behind your visualization in the topmost requests you see in the log and observe the duration, which is about 30 seconds (1). Use the Request content (2) option to look into the actual query submitted from Power BI Desktop.\n\nSwitch back to the Power BI Desktop application, then click Save in the top-left corner.\n\nSpecify a file name, such as synapse-lab (1), then click Save (2).\n\nClick Publish above the saved report. Make sure that, in Power BI Desktop, you are signed in with the same account you use in the Power BI portal and in Synapse Studio. You can switch to the proper account from the topmost right corner of the window. In the Publish to Power BI dialog, select the workspace you linked to Synapse (for example, synapse-training), then click Select.\n\nWait until the publish operation successfully completes.\n\nAfter the operation successfully completes, you should be able to see this report (2) published in the Power BI portal, as well as in Synapse Studio. To view it in Synapse Studio, navigate to the Develop hub and refresh (1) the Power BI reports node.\n\nNext unit: Describe Power BI optimization options\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Create a new data source to use in Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/5-exercise-create-new-data-source-to-use",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 11\nNext\nExercise - Create a new data source to use in Power BI\nCompleted\n100 XP\n3 minutes\n\nBeneath Power BI, under the linked Power BI workspace, select Power BI datasets (1).\n\nSelect New Power BI dataset (2) from the top actions menu.\n\nSelect Start and make sure you have Power BI Desktop installed on your environment machine.\n\nSelect SQLPool01, then select Continue.\n\nNext, select Download to download the .pbids file.\n\nSelect Continue, then Close and refresh to close the publishing dialog.\n\nNow, you have a new Power BI Dataset.\n\nNext unit: Exercise - Create a new Power BI report in Synapse Studio\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Connect to Power BI from Synapse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/3-exercise-connect-to-from-synapse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics \nAdd\nPrevious\nUnit 3 of 11\nNext\nExercise - Connect to Power BI from Synapse\nCompleted\n100 XP\n4 minutes\nTask 1: Create a Power BI workspace\n\nIn a new browser tab, navigate to https://powerbi.microsoft.com/.\n\nSign in with the same account used to sign in to Azure by selecting the Sign in link on the upper-right corner.\n\nIf this is your first time signing into this account, complete the setup wizard with the default options.\n\nSelect Workspaces, then select Create a workspace.\n\nIf you are prompted to upgrade to Power BI Pro, select Try free.\n\nSelect Got it to confirm the pro subscription.\n\nSet the name to synapse-training, then select Save.\n\nTask 2: Connect to Power BI from Synapse\n\nOpen Synapse Studio (https://web.azuresynapse.net/), and then navigate to the Manage hub.\n\nSelect Linked services on the left-hand menu, then select + New.\n\nSelect Power BI, then select Continue.\n\nIn the dataset properties form, complete the following:\n\nExpand table\nField\tValue\nName (1)\tenter handson_powerbi\nWorkspace name (2)\tselect synapse-training\n\nSelect Create (3).\n\nTask 3: Explore the Power BI linked service in Synapse Studio\n\nIn Azure Synapse Studio and navigate to the Develop hub using the left menu option.\n\nExpand Power BI, expand SynapseDemos (or synapse-training, named after your resource group) and observe that you have access to your Power BI datasets and reports, directly from Synapse Studio.\n\nNew reports can be created by selecting + at the top of the Develop tab. Existing reports can be edited by selecting the report name. Any saved changes will be written back to the Power BI workspace.\n\nNext unit: Understand Power BI data sources\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Power BI data sources - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/4-understand-data-sources",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Power BI data sources\n6 minutes\n\nPower BI reports can connect to a number of data sources. Depending on how data is used, different data sources are available. There is a large number of varied data sources available for Power BI reports, encompassing:\n\nOnline services (Salesforce, Dynamics 365, others)\nDatabases (SQL Server, Access, Amazon Redshift, others)\nSimple files (Excel, JSON, others)\nOther data sources (Spark, Web sites, Microsoft Exchange, others)\n\nYou can connect to all sorts of different data sources when using Power BI Desktop or the Power BI service, and make those data connections in different ways. You can import data to Power BI, which is the most common way to get data, or connect directly to data in the original source repository, which is known as DirectQuery.\n\nImport connections\n\nYou can get data from any of the data sources in Power BI by selecting Get Data in the bottom-left corner of the page.\n\nAfter you select Get Data, you can choose the data you want to access\n\nFor import, when using Get Data in Power BI Desktop to connect to a data source like SQL Server, the behavior of that connection is as follows:\n\nDuring the initial Get Data experience, the set of tables selected each define a query that will return a set of data. Those queries can be edited before loading the data, for example, to apply filters, or aggregate the data, or join different tables.\nUpon load, all of the data defined by those queries will be imported into the Power BI cache.\nUpon building a visual within Power BI Desktop, the imported data will be queried. The Power BI store ensures the query will be fast. All changes to the visual are reflected immediately.\nAny changes to the underlying data aren't reflected in any visuals. It's necessary to Refresh to reimport data.\nUpon publishing the report as a .pbix file to the Power BI service, a dataset is created and uploaded to the Power BI service. The imported data is included with that dataset. It's then possible to schedule refresh of that data, for example, to reimport the data every day. Depending upon the location of the original data source, it might be necessary to configure an on-premises data gateway.\nWhen opening an existing report in the Power BI service, or authoring a new report, the imported data is queried again, ensuring interactivity.\nVisuals, or entire report pages, can be pinned as dashboard tiles. The tiles automatically refresh whenever the underlying dataset refreshes.\nDirectQuery connections\n\nFor DirectQuery, when using Get Data in Power BI Desktop to connect to a data source, the behavior of that connection is as follows:\n\nDuring the initial Get Data experience, the source is selected. For relational sources, a set of tables are selected and each still define a query that logically returns a set of data. For multidimensional sources, like SAP BW, only the source is selected.\nHowever, upon load, no data is imported into the Power BI store. Instead, upon building a visual within Power BI Desktop, queries are sent to the underlying data source to retrieve the necessary data. The time taken to refresh the visual depends on the performance of the underlying data source.\nAny changes to the underlying data aren't immediately reflected in any existing visuals. It's still necessary to refresh. The necessary queries are resent for each visual, and the visual is updated as necessary.\nUpon publishing the report to the Power BI service, it will again result in a dataset in the Power BI service, the same as for import. However, no data is included with that dataset.\nWhen opening an existing report in the Power BI service, or authoring a new one, the underlying data source is again queried to retrieve the necessary data. Depending upon the location of the original data source, it might be necessary to configure an on-premises data gateway, as is needed for import mode if the data is refreshed.\nVisuals, or entire report pages, can be pinned as Dashboard tiles. To ensure that opening a dashboard is fast, the tiles are automatically refreshed on a schedule, for example, every hour. The frequency of this refresh can be controlled, to reflect how frequently the data is changing, and how important it's to see the latest data. When opening a dashboard, the tiles reflect the data at the time of the last refresh, and not necessarily the latest changes made to the underlying source. You can refresh an open dashboard to ensure it's current.\nLive connections\n\nWhen connecting to SQL Server Analysis Services, there's an option to either import data from or connect live to, the selected data model. If you use import, you define a query against that external SQL Server Analysis Services source, and the data is imported as normal. If you use connect live, there's no query defined, and the entire external model is shown in the field list.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe the Power BI and Synapse workspace integration - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/2-describe-synapse-workspace-integration",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe the Power BI and Synapse workspace integration\n5 minutes\n\nThe combination of Azure Synapse Analytics and Power BI opens exciting possibilities for bridging the gap between massive volumes of structured or unstructured data and actionable insights. You can create new reports or consume your existing Power BI datasets and reports, all within Synapse by simply linking your Power BI and Synapse workspaces. You can make any edits to your reports in Synapse and the changes will instantly propagate to all users. This tight integration between Power BI and Azure Synapse greatly reduces time to insights as Data Analysts, Data Scientists, Data Engineers and business users can collaborate easily in a single environment.\n\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data. A Power BI linked service in Azure Synapse Analytics helps your reports access data in a data lake through serverless SQL pools or taps into tables in a dedicated SQL Pool quickly. This capability provides a large selection of data from multiple sources. Moreover, having the authoring tools in Azure Synapse Studio helps to craft the connection between data in the data lake or dedicated SQL pool much more intuitively.\n\nTo move data into a data warehouse, data is periodically extracted from various sources that contain essential business information. As the data is moved, it can be formatted, cleaned, validated, summarized, and reorganized with Azure Synapse Pipelines and Dataflows. Alternatively, the data can be stored in the lowest level of detail, with aggregated materialized views provided in Azure Synapse for reporting. In either case, Azure Synapse becomes a permanent data store for reporting, analysis, and business intelligence (BI), helping ‘align and combine’ data from multiple sources and enforcing consistent data standards when needed.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this lesson, you will learn how to integrate Power BI with your Azure Synapse workspace to build Power BI reports. You will create a new data source and Power BI report in Azure Synapse Studio. Then you will learn how to improve query performance with materialized views and result-set caching. Finally, you will explore the data lake with serverless SQL pools and create visualizations against that data in Power BI.\n\nAfter completing the lesson, you will be able to:\n\nIntegrate an Azure Synapse workspace and Power BI\nOptimize integration with Power BI\nImprove query performance with materialized views and result-set caching\nVisualize data with serverless SQL pools and create a Power BI report\n\nBefore taking this lesson, it is recommended that you can:\n\nLogin to the Azure portal\nCreate a Azure Synapse Analytics Workspace\nCreate and connect to an Azure Synapse Analytics SQL Pool\nCreate a Power BI Workspace\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Build reports using Power BI within Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nBuild reports using Power BI within Azure Synapse Analytics\nModule\n11 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure\n\nIn this module, you will learn how you can build Power BI reports from within Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you'll:\n\nDescribe the Power BI and Synapse workspace integration\nUnderstand Power BI data sources\nDescribe optimization options\nVisualize data with serverless SQL pools\nAdd\nPrerequisites\nIt's recommended that students understand how Azure Synapse Analytics and Power BI work.\nIntroduction\nmin\nDescribe the Power BI and Synapse workspace integration\nmin\nExercise - Connect to Power BI from Synapse\nmin\nUnderstand Power BI data sources\nmin\nExercise - Create a new data source to use in Power BI\nmin\nExercise - Create a new Power BI report in Synapse Studio\nmin\nDescribe Power BI optimization options\nmin\nExercise - Improve performance with materialized views and result-set caching\nmin\nVisualize data with serverless SQL pools\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "settings.png (624×351)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/settings.png#lightbox",
    "html": ""
  },
  {
    "title": "app-template-settings.png (335×336)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/app-template-settings.png#lightbox",
    "html": ""
  },
  {
    "title": "appsource.png (1092×717)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/appsource.png#lightbox",
    "html": ""
  },
  {
    "title": "admin-portal.png (624×384)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/admin-portal.png#lightbox",
    "html": ""
  },
  {
    "title": "dataflow-components.png (1061×654)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/dataflow-components.png#lightbox",
    "html": ""
  },
  {
    "title": "dataflow.png (946×505)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/dataflow.png#lightbox",
    "html": ""
  },
  {
    "title": "developer-settings.png (857×617)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/developer-settings.png#lightbox",
    "html": ""
  },
  {
    "title": "sharepoint-online.png (1025×311)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/sharepoint-online.png#lightbox",
    "html": ""
  },
  {
    "title": "power-bi-teams.png (518×429)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/power-bi-teams.png#lightbox",
    "html": ""
  },
  {
    "title": "share-content.png (975×375)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/share-content.png#lightbox",
    "html": ""
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/13-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\n You learned about the following topics in this module:    \n\nDescribe the various embedding scenarios that allow you to broaden the reach of Power BI\n\nUnderstand the options for developers to customize Power BI solutions\n\nLearn to provision and optimize Power BI embedded capacity and create and deploy dataflows\n\nBuild custom Power BI solutions template apps\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Check your knowledge - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/12-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCheck your knowledge\n4 minutes\nAnswer the following questions to see what you've learned.\n1. \n\nWhich of the following embed scenario allows you to easily embed interactive Power BI content in blog posts, websites, emails, or social media?\n\n \n\nEmbed in PowerApps\n\nSharePoint Online web part\n\nPublish to web\n\nEmbed in Microsoft Teams\n\n2. \n\nIn what resource would you find client tools that provide the ability to produce and consume .NET packages?\n\n \n\n.NET resource library\n\nSharePoint Online gallery\n\nREST API directory\n\nNuGet gallery\n\n3. \n\nWhich of the following is NOT a recognized option to create or build on top of a dataflow?\n\n \n\nUse define new tables\n\nUse linked tables\n\nUse entity tables\n\nUse a computed table\n\n4. \n\nPower BI Template Apps allow Power BI Pro and Power BI Premium users to gain immediate insights through prepackaged dashboards and reports that can be connected to what source?\n\n \n\nSharePoint Online\n\n.NET gallery\n\nO365\n\nLive data sources\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Template app governance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/10-template-app-governance",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTemplate app governance\n1 minute\n\nThe primary governance role regarding template apps is to determine who can install first- and third-party applications from both AppSource and Direct links.\n\nBy default AppSource installation settings are set to on in the Admin portal, and direct links settings by default are set to off.\n\nThe last key governance task entails controlling who can publish template apps outside of the organization. This setting in the admin portal is set to off by default.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Template apps - installed entities - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/9-template-apps-entities",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTemplate apps - installed entities\n1 minute\n\nWhen using a template app, the app will be transferred to your Power BI tenant and you can use it like any other report or Power BI entity. Plus, if there is a newer version of an app you're using, you'll get a notification. You can either install the new version side by side or you can overwrite the previous one.\n\nEvery time you install another version of an app you will be getting more workspace.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Template apps - install packages - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/8-template-apps",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTemplate apps - install packages\n1 minute\n\nThe new Power BI template apps enable Power BI partners to build Power BI apps with little or no coding and deploy them to any Power BI customer. Power BI Template Apps allow Power BI Pro or Power BI Premium users to gain immediate insights through prepackaged dashboards and reports that can be connected to live data sources. \n\nMany Power BI Apps are already available in the Power BI Apps marketplace.\n\nIn addition to providing insights for your organization, there are many apps that have been published for usage monitoring.\n\nThere are other aspects associated with template apps that should be noted. For example:\n\nIn the pro user experience, you can customize everything. Once an app is installed in your tenant, it is similar to any other content you have in Power BI.\n\nYou will get continuous updates to content and you can either install side by side or overwrite it.\n\nAdministrators have authority to define who in the organization can install apps.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Dataflow capabilities on Power BI Premium - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/7-dataflow-power-bi-premium",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDataflow capabilities on Power BI Premium\n1 minute\n\nThere are certain dataflow capabilities that are only available in Power BI Premium. The chart below provides a listing and comparison of capabilities between Power BI Pro and Power BI Premium.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a Dataflow - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/6-create-dataflow",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a Dataflow\n1 minute\n\nTo create a dataflow, launch the Power BI service in a browser then select a workspace (dataflows are not available in my-workspace in the Power BI service) from the navigation pane on the left. You can also create a new workspace in which to create your new dataflow.\n\nThere are multiple ways to create or build on top of a new dataflow:\n\nCreate a dataflow using define new tables - Using the Define new tables option allows you to define a new table and connect to a new data source.\n\nCreate a dataflow using linked tables - Creating a dataflow using linked tables enables you to reference an existing table, defined in another dataflow, in a read-only fashion.\n\nCreate a dataflow using import/export - Creating a dataflow using import/export lets you import a dataflow from a file. This is useful if you want to save a dataflow copy offline or move a dataflow from one workspace to another.\n\nCreate a dataflow by attaching a common data model folder - Creating a dataflow from a CDM folder allows you to reference a table that has been written by another application in the Common Data Model (CDM) format. Using this method has several requirements including:\n\nCreator must possess the storage blob data owner role of the storage account.\n\nCreator must have read access and execute access control lists (ACLs) on both the CDM and any files or folders within it.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Dataflow explained - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/5-dataflow-explained",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDataflow explained\n1 minute\n\nDataflows are created and managed in workspaces by using the Power BI service. Data is stored as entities in the Common Data Model in Azure Data Lake Storage Gen2.\n\nWith dataflows, you can ingest data from both on-premises or cloud data sources, and you have the ability map, edit, and extend standard entities. You can also create custom entities and to create Power BI datasets using a dataflow. Lastly, it's required that a dataflow is refreshed before it can be consumed in a dataset inside Power BI desktop or referenced as a linked or computed table.\n\nFor more information on configuring and consuming dataflows, see this article.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Dataflow introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/4-dataflow",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDataflow introduction\n3 minutes\n\nA dataflow is a collection of tables that are created and managed in workspaces in the Power BI service. A table is a set of columns that are used to store data, much like a table within a database. You can add and edit tables in your dataflow, and manage data refresh schedules, directly from the workspace in which your dataflow was created.\n\nDataflows are designed to support the following scenarios:\n\nCreate reusable transformation logic that can be shared by many datasets and reports inside Power BI. Dataflows promote reusability of the underlying data elements, preventing the need to create separate connections with your cloud or on-premises data sources.\n\nExpose the data in your own Azure Data Lake Gen 2 storage, enabling you to connect other Azure services to the raw underlying data.\n\nCreate a single source of the truth by forcing analysts to connect to the dataflows, rather than connecting to the underlying systems, providing you with control over which data is accessed, and how data is exposed to report creators. You can also map the data to industry standard definitions, enabling you to create tidy curated views, which can work with other services and products in Microsoft Power Platform.\n\nIf you want to work with large data volumes and perform ETL at scale, dataflows with Power BI Premium scales more efficiently and gives you more flexibility. Dataflows supports a wide range of cloud and on-premises sources.\n\nPrevent analysts from having direct access to the underlying data source. Since report creators can build on top of dataflows, it may be more convenient for you to allow access to underlying data sources only to a few individuals, and then provide access to the dataflows for analysts to build on top of. This approach reduces the load to the underlying systems and gives administrators finer control of when the systems get loaded from refreshes.\n\nOnce you've created a dataflow, you can use Power BI Desktop and the Power BI service to create datasets, reports, dashboards, and apps that use the Common Data Model to drive deep insights into your business activities. Dataflow refresh scheduling is managed directly from the workspace in which your dataflow was created, just like your datasets.\n\nTo create a dataflow, launch the Power BI service in a browser then select a workspace (dataflows are not available in my-workspace in the Power BI service) from the navigation pane on the left, as shown in the following screen. You can also create a new workspace in which to create your new dataflow.\n\nThe next two steps are to schedule a refresh and build the dataset using your Power BI desktop.\n\nFor more information or instructions on how to create a dataflow, see the following article.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Provision a Power BI embedded capacity - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/3-embedded-capacity",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nProvision a Power BI embedded capacity\n1 minute\n\nPower BI Embedded simplifies Power BI capabilities by helping users quickly add stunning visuals, reports, and dashboards to apps. A Power BI embedded capacity is created in the Azure portal and assigned in the Power BI service. Requirements to provision Power BI embedded capacity include the need to be signed in as a user in the Power BI tenant and have an active Azure subscription. By default, user is signed in as a capacity administrator and therefore can add other users. Once you've provisioned Power BI embedded capacity, you can add more as your needs grow. You can use any combination of Premium capacity SKUs (P1 through P3) within your organization.\n\nFor additional information on how to provision a Power BI embedded capacity, view the following video.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement and manage an analytics environment  Broaden the reach of Power BI \nAdd\nUnit 1 of 12\nNext\nIntroduction\nCompleted\n100 XP\n3 minutes\n\nThere are various ways to share your Power BI reports beyond your tenant. The following is a list of embedding scenarios you may elect to use. We will examine each option through the remainder this module.\n\nPublish to web\n\nEmbed in Microsoft Teams\n\nSharePoint Online web part\n\nEmbed in PowerApps\n\nThere is also a special version of Power BI that can be initiated through the Azure portal called Power BI Embedded (PBIE). PBIE allows application developers to embed fully interactive reports into their applications. Microsoft developed Power BI Embedded for independent software vendors (ISVs) wanting to embed visuals into their applications to help their customers make analytic decisions and spares ISVs from having to build their own analytics' solutions. \n\nFor more information about PBIE, see this article.\n\nPublish to web\n\nWith the Power BI Publish to web option, you can easily embed interactive Power BI content in blog posts, websites, emails, or social media. You can also easily edit, update, refresh, or stop sharing your published visuals.\n\n Note\n\nPublish to web is disabled by default. Power BI admins can enable this feature in the Admin portal > tenant settings > export and sharing settings.\n\n Warning\n\nProceed with caution. When you use Publish to web, anyone on the Internet can view your published report or visual. Viewing requires no authentication. These factors of Publish to web make it an excellent option for publishing public relations material. It includes viewing detail-level data that your reports aggregate. Before publishing a report, make sure it's okay for you to share the data and visualizations publicly. Don't publish confidential or proprietary information. If in doubt, check your organization's policies before publishing.\n\nAccess the publish to web option through the Power BI admin portal.\n\nFor more information on the publish to web option, see this article.\n\nEmbed in Microsoft Teams\n\nYou can embed Power BI reports in Microsoft Teams. In your Microsoft Teams channel, select the + sign and choose the Power BI option as a tab. Microsoft Teams automatically detects all the reports in your Power BI groups and in My Workspace. You can choose which reports you want to show in your Power BI tab in your channel.\n\nSharePoint Online web part\n\nWith the new Power BI report web part for SharePoint Online, you can easily embed interactive Power BI reports in SharePoint Online pages.\n\nWhen using the new Embed in SharePoint Online option, the embedded reports respect all item permissions and data security through row-level security (RLS), so you can easily create secure internal portals.\n\nEmbedding a report in SharePoint Online requires no coding knowledge, does not work with on-premises SharePoint and requires the user to log into SharePoint to access reports.\n\nFor more information regarding embedding reports in a SharePoint Online web part, see this article.\n\nEmbed in PowerApps\n\nTo embed dashboards and reports in PowerApps, the embed content in apps under developer settings on the portal must be enabled. In this setting, you will also select which users can embed content in apps. You have the option to enable the entire organization or restrict the capability to specific security groups.\n\nNext unit: REST API custom development\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "REST API custom development - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/2-rest-api-custom",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nREST API custom development\n1 minute\n\nThere are several REST APIs developers can use to build solutions. Azure Active Directory (AD) REST APIs and AD libraries for multiple platforms can be used for authentication. Power BI REST APIs provide programmatic access to objects in a Power BI Report Server catalog and the .NET and JavaScript SDKs can be used to embed reports into an application for your organization.\n\nNuGet packages\n\nThe NuGet client tools provide the ability to produce and consume .NET packages. The NuGet Gallery is the central package repository used by all package authors and consumers.\n\nThere are several NuGet packages available to Power BI developers, including:\n\nAzure AD Authentication Library - Microsoft.IdentityModel.Clients.ActiveDirectory: This package includes the AD Authentication Library (ADAL) and provides the authentication functionality for a .NET client.\n\nPower BI .NET SDK - Microsoft.PowerBI.API: This package is a .NET client library for Microsoft Power BI public REST endpoints and provides access to workspaces, content identifiers (GUIDS) for datasets, reports, dashboards, and tiles.\n\nJavaScript SDK - Microsoft.PowerBI.JavaScript: This package is a suite of JavaScript web components for integrating Power BI into apps, including the powerbi.js script, referenced by web pages to enable client-side functionality.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Broaden the reach of Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nBroaden the reach of Power BI\nModule\n12 Units\nFeedback\nIntermediate\nAdministrator\nMicrosoft Power Platform\nPower BI\n\nYou can broaden the reach of Power BI by sharing your reports beyond your Power BI environment. You can publish reports to the public internet, embed reports in Microsoft Teams or in PowerApps, and place BI reports in a SharePoint online web part. There's also a special version of Power BI service called Microsoft Power BI Embedded (PBIE) which allows application developers to embed fully interactive reports into their applications without having to build their own data visualizations and controls from scratch.\n\nLearning objectives\n\nBy the end of this module, you'll be able to:\n\nDescribe the various embedding scenarios that allow you to broaden the reach of Power BI\nUnderstand the options for developers to customize Power BI solutions\nLearn to provision and optimize Power BI embedded capacity and create and deploy dataflows\nBuild custom Power BI solutions template apps\nAdd\nPrerequisites\nFamiliarity with Power BI and governance practices in a BI environment. \nWe recommend completing the Dashboard in a Day learning path before beginning this module. \nIntroduction\nmin\nREST API custom development\nmin\nProvision a Power BI embedded capacity\nmin\nDataflow introduction\nmin\nDataflow explained\nmin\nCreate a Dataflow\nmin\nDataflow capabilities on Power BI Premium\nmin\nTemplate apps - install packages\nmin\nTemplate apps - installed entities\nmin\nTemplate app governance\nmin\nCheck your knowledge\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nYou learned about the following topics in this module:\n\nUsage metrics available through the Power BI admin portal\n\nOptimizing the use of usage metrics for dashboards and reports\n\nThe difference between audit logs and the activity log\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "3-usage.png (990×526)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/3-usage.png#lightbox",
    "html": ""
  },
  {
    "title": "4-log.png (1174×736)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/4-log.png#lightbox",
    "html": ""
  },
  {
    "title": "2-metrics.png (1296×723)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/2-metrics.png#lightbox",
    "html": ""
  },
  {
    "title": "1-portal.png (806×562)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/1-portal.png#lightbox",
    "html": ""
  },
  {
    "title": "Check your knowledge - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/6-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCheck your knowledge\n3 minutes\nAnswer the following questions to see what you've learned.\n1. \n\nWhich one of the following is NOT a Power BI tenant metric available through the O365 admin portal?\n\n \n\nNumber of user dashboards\n\nNumber of user reports\n\nReport peak usage time during the day\n\nNumber of user datasets\n\n2. \n\nWhat's the easiest way to convert a usage metrics report into a full featured editable Power BI report?\n\n \n\nRun the report through the Power BI report converter\n\nUse the \"save as\" command\n\nSet up your report request to render an editable Power BI object\n\nThe usage metrics report is rendered in an editable format\n\n3. \n\nWhich one of the following statements accurately describes searching through a Power BI activity log?\n\n \n\nIt is currently a manual task\n\nYou can use the Microsoft 365 security center to search an audit log\n\nYou can use the Microsoft 365 compliance center to search an audit log\n\nExport the activity log to a searchable file format, for example Excel workbook\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Activity log - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/5-activity-log",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement and manage an analytics environment  Monitor and audit usage \nAdd\nPrevious\nUnit 5 of 7\nNext\nActivity log\nCompleted\n100 XP\n1 minute\n\nThere is a new Power BI API called the activity log, which enables Power BI service admins to track user and admin activities within a Power BI tenant. It is comparable to the unified Office 365 audit log in the sense that both logs contain a complete copy of the Power BI auditing data, but there are also several key differences that are important for Power BI service admins, as the following table reveals.\n\nTable with Unified Audit Log versus Power BI Activity Log. The unified audit log includes events from SharePoint Online, Exchange Online, Dynamics 365, and other services in addition to the Power BI auditing events. Only users with View-Only Audit Logs or Audit Logs permissions have access, such as global admins and auditors. Global admins and auditors can search the unified audit log by using Office 365 Security & Compliance Center, the Microsoft 365 Security Center, and the Microsoft 365 Compliance Center. Global admins and auditors can download audit log entries by using Office 365 Management APIs and cmdlets. By contrast, the Power BI activity log only includes the Power BI auditing events. Global admins and Power BI service admins have access. There's no user interface to search the activity log yet. Global admins and Power BI service admins can download activity log entries by using a Power BI REST API and management cmdlet.\n\nTo distinguish the Power BI-specific log from the unified audit log, Microsoft uses the name activity log for Power BI, but the Power BI auditing data within both logs is identical. In this way, global admins and auditors can continue to use the Security and Compliance Centers for all their auditing needs, while Power BI service admins now have a straightforward way to access and download the data they need to monitor the actions of their users and administrators in their Power BI tenant.\n\nFor more information regarding the new Power BI activity log cmdlet, see Introducing the Power BI Activity Log | Microsoft Power BI Blog | Microsoft Power BI blog post.\n\nNext unit: Check your knowledge\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Usage metrics for dashboards and reports - new version - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/3-metrics-new",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUsage metrics for dashboards and reports - new version\n1 minute\n\nEnhancements have been introduced in the new version of usage metrics reports. These enhancements are only available in v2 workspaces. In addition to the previously available user activity metrics, dashboard and report performance metrics have been added. For example, here is a sampling of new performance metrics available in v2 workspaces:\n\nReport load time\nAverage report load time\nReport views\nUnique viewers\nUnused reports\n\nThe following is an example of one page of the Usage report v2 - new design.\n\nThe new version of the usage metric reports system is built on top of a brand-new infrastructure. That's why the collection and storage of usage information is more reliable and scalable. And it's based on audit events, eliminating previous inconsistencies between the audit log and usage metrics reports.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Audit logs - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/4-logs",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nAudit logs\n2 minutes\n\nKnowing who is taking what action on which item in your Power BI tenant can be critical in helping your organization fulfill its requirements, like meeting regulatory compliance and records management. With Power BI, you have two options to track user activity: The Power BI activity log and the unified audit log. The Power BI activity log will be covered in Unit 5 of this course.\n\nAs the tenant admin, it's likely that at some point you'll be required to answer questions about a specific instance or activity in the tenant. Questions such as who is accessing my report, or when did they access the report will require investigation by the tenant admin. These investigations can be done through the O365 cloud. But these queries produce event activity records on all products in the O365 container, including:\n\nSharePoint Online\nExchange Online\nDynamics 365\nOther services\nPower BI auditing events\n\nAccess to audit logs is restricted to Microsoft 365 global administrators and require an Exchange Online license. You can use a PowerShell cmdlet to download the data for investigations and analysis, but the data is only retained for 90 days. A good practice may be to log regularly scheduled queries in a repository that can be reserved for access beyond the 90-day period.\n\nThe following is an example of an audit log search.\n\nThe Power BI audit log search is primarily a manual task. The Power BI activity log is the set of audit events specific to Power BI.\n\nFor additional information about Power BI audit logs, review Track user activities in Power BI.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Usage metrics for dashboards and reports - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/2-metrics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUsage metrics for dashboards and reports\n2 minutes\n\nUsage metrics let you see the usage of individual dashboards and reports. Some of the metrics identify who the users are and how frequently the users are accessing the dashboard or report.\n\nFollowing is an example of a dashboard usage metrics report.\n\nReported usage metrics may differ from audit log numbers because of the way in which different reporting systems collect the metrics.\n\n Note\n\nSeveral screenshot examples shown in this module include personally identifiable information (PII.) An important privacy feature is that Admins can hide user details from usage metrics via tenant settings. For additional information regarding hiding user details, see Exclude user information from usage metrics reports.\n\nUsage metrics reports are read-only. However, you can personalize a usage metrics report by using \"Save as.\" This creates a brand-new dataset and converts the read-only report to a full-featured Power BI report that can be edited. Not only does the personalized report contain metrics for the selected dashboard or report, removing the default filter allows access to usage metrics for all dashboards or all reports in the selected workspace. Using this practice also allows you to capture the names of your end users. Lastly, the admin settings that control who can access reports allow admins to reflect which users are shown by name. This may be important for compliance, as in some countries/regions such as Germany, restrict by law the ability to show who is doing what as part of their job. For these cases Power BI gives the admin a way to prevent showing a group of users by name.\n\nFor additional information about usage metrics for dashboards and reports, view this article.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nUsage metrics are available through the Power BI admin portal. This view provides a high-level record of what's going on in a tenant. Interestingly, it's sometimes more important to realize what's not happening in a tenant. For example, if a company has many Power BI Pro licenses that aren't used effectively, corrective action can then be taken. These reports tend not to be time bound, or time sensitive. They'll present what objects are being accessed by how many users. However, the report won't reflect the times during the day or reporting period when usage was highest.\n\nNew usage metrics are being regularly added to this report.\n\nFor additional information about Power BI usage metrics, review the Usage Metrics section of this article.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Monitor and audit usage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nMonitor and audit usage\nModule\n7 Units\nFeedback\nIntermediate\nAdministrator\nMicrosoft Power Platform\nPower BI\n\nUsage metrics help you understand the impact of your dashboards and reports. When you run either dashboard usage metrics or report usage metrics, you discover how those dashboards and reports are being used throughout your organization, who's using them, and for what purpose. Knowing who is taking what action on which item in your Power BI tenant can be critical in helping your organization fulfill its requirements, like meeting regulatory compliance and records management. This module outlines what is available in usage metrics reports and audit logs.\n\nLearning objectives\n\nIn this module, you will:\n\nDiscover what usage metrics are available through the Power BI admin portal\nOptimize use of usage metrics for dashboards and reports\nDistinguish between audit logs and the activity logs\nAdd\nPrerequisites\n\nWe recommend completing Dashboard in a Day before beginning this module.\n\nIntroduction\nmin\nUsage metrics for dashboards and reports\nmin\nUsage metrics for dashboards and reports - new version\nmin\nAudit logs\nmin\nActivity log\nmin\nCheck your knowledge\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "6-support.png (752×472)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/6-support.png#lightbox",
    "html": ""
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nYou learned about the following topics in this module:\n\nKey components of an effective BI governance model\n\nKey elements associated with data governance\n\nHow to configure, deploy, and manage elements of a BI governance strategy\n\nHow to set up BI help and support settings\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "6-help.png (657×634)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/6-help.png#lightbox",
    "html": ""
  },
  {
    "title": "5-embed.png (705×288)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/5-embed.png#lightbox",
    "html": ""
  },
  {
    "title": "4-visuals.png (750×315)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/4-visuals.png#lightbox",
    "html": ""
  },
  {
    "title": "3-portal.png (750×407)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/3-portal.png#lightbox",
    "html": ""
  },
  {
    "title": "1-it.png (732×344)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/1-it.png#lightbox",
    "html": ""
  },
  {
    "title": "2-elements.png (752×423)",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/2-elements.png#lightbox",
    "html": ""
  },
  {
    "title": "Check your knowledge - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/7-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCheck your knowledge\n4 minutes\nAnswer the following questions to see what you've learned.\n1. \n\nWhich of the following is NOT a common component of an effective BI governance model?\n\n \n\nData engineering\n\nService and support\n\nTraining and education\n\nEnterprise reporting\n\n2. \n\nWhich element of data governance defines policies for use of data in a Power BI system?\n\n \n\nVisibility\n\nControl\n\nSupport\n\nCompliance\n\n3. \n\nWhich of the following administrator roles is NOT responsible for configuring tenant settings?\n\n \n\nGlobal administrator\n\nPower BI service administrator\n\nMicrosoft Power Platform service administrator\n\nPower BI billing administrator\n\n4. \n\nWhich link is NOT available for customization in the custom help and support menu?\n\n \n\nTraining documentation\n\nEmbed code list\n\nHelp desk\n\nLicensing requests\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Help and support settings - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/6-help",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nHelp and support settings\n1 minute\n\nPower BI allows you to customize help and support links in the help menu to direct your users to specific organizational content.\n\nIt's encouraged that you set up internal Power BI-related sites using Microsoft Teams, or some other collaboration platform. These sites can be used to store training documentation, host discussions, make requests for licenses, or respond to help.\n\nIf you do so, it's recommended that you enable the Publish \"Get Help\" information setting for the entire organization. It's found in the Help and support settings group. You can set URLs for your:\n\nTraining documentation\n\nDiscussion forum\n\nLicensing requests\n\nHelp desk\n\nThese URLs will become available as links in the Power BI help menu.\n\nTo set up your custom help and support menu in your Power BI tenant, access the admin portal, select tenant settings, then select Publish \"Get Help\" information. Toggle the Disabled button to Enabled, and then provide appropriate URLs to your company's sites for training documentation, discussion forums, licensing requests, and help desk.\n\nFor Power BI Support and status use https://support.powerbi.com\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage embed codes - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/5-embed",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage embed codes\n1 minute\n\nWith the Power BI Publish to web option, you can easily embed interactive Power BI content in blog posts, websites, emails, or social media. You can also easily edit, update, refresh, or stop sharing your published visuals.\n\nOnce you create a Publish to web embed code, you can manage your codes from the Settings menu in Power BI. Managing embed codes includes the ability to remove the destination visual or report for a code (rendering the embed code unusable) or getting the embed code.\n\nAs a Power BI admin, you can control all embed codes in the tenant through the Power BI admin portal.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Deploy organizational visuals - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/4-deploy",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement and manage an analytics environment  Provide governance in a Power BI environment \nAdd\nPrevious\nUnit 4 of 8\nNext\nDeploy organizational visuals\nCompleted\n100 XP\n1 minute\n\nAs a Power BI admin for your organization, you can establish the approval process for usage of custom visuals and control which type of Power BI visuals, including custom visuals, users can access across the organization.\n\nTo manage Power BI visuals, you must be a Global Admin in Office 365, or have been assigned the Power BI service administrator role. Organizational visuals are managed and can be distributed through the Power BI admin portal.\n\nFrom the Power BI admin portal, you can manage the list of Power BI visuals available in your organization's organizational store. The Organizational visuals tab in the Admin portal allows you to add and remove visuals, and decide which visuals will automatically display in the visualization pane of your organization's users. You can add to the list any type of visual including uncertified visuals and .pbiviz visuals, even if they contradict the tenant settings of your organization.\n\nOrganizational visuals settings are automatically deployed to Power BI Desktop. For additional information about organizational visuals, you may refer to this article.\n\nNext unit: Manage embed codes\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure tenant settings - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/3-configure",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure tenant settings\n1 minute\n\nTenant settings enable fine-grained control over the features that are made available to your organization and are managed in the Admin portal. Tenant settings can be configured by a global admin, Power BI service administrator, or Microsoft Power Platform service administrator. Many tenant settings can restrict capabilities and features to a limited set of users by assigning one of three states.\n\nDisabled for the entire organization: No one in your organization can use this feature.\n\nEnabled for the entire organization: Everyone in your organization can use this feature.\n\nEnabled for a subset of the organization: Specific security groups in your organization can use this feature.\n\nFor more information about creating, editing, or deleting security groups in Microsoft 365 Admin Center, review this article.\n\nAn example of a commonly enabled tenant setting is - Publish to web. The Publish to web setting gives you options that let users create embed codes to publish reports to the web. This functionality makes the report and its data available to anyone on the web.\n\n Note\n\nFor security reasons, Publish to web is turned off by default.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nA critical role of a BI administrator is to enforce the governance of the BI environment.\n\nThere are three key groups that contribute to building the governance model in a normal BI ecosystem. Each group has assigned tasks and responsibilities. The three groups are the IT/BI team, the business users, and central IT.\n\nThe IT/BI team is responsible for designing models and processes for collecting, cleaning, and cataloging data. The IT/BI team is also responsible for developing rules that determine what reports are created and how they're distributed to the organization.\n\nThe business users' primary responsibilities are to research and document the BI system requirements, including needs for ad hoc analysis and reporting, and how users of the system will work together towards shared or common goals.\n\nThe third group, central IT, contributes to the development of several components of a BI governance model. Starting with training and education, which teaches users, administrators, auditors, and governors of the system how to optimize the output of the environment. Another component is data governance, or the process of governing and managing data to ensure the accuracy, completeness, integrity, and timeliness of data. A third component in our model is the enablement of business process and access controls to define the approval processes for who should be granted access to the system, what they can do in the system, and when they should have access to it. The last component is procurement and administration, which supports the purchasing and procurement of the BI system elements.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Elements of data governance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/2-elements",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nElements of data governance\n1 minute\n\nData governance is the process of governing and managing data and ensures the accuracy, completeness, integrity, and timeliness of data.\n\nThe first element associated with BI data governance is visibility. As an administrator it's important to know what's going on in the Power BI environment.\n\nKey information required by an administrator includes things like, what solutions are being deployed, whether users have access to them, and knowing what users are doing when using the solutions in their BI environment.\n\nAnother element of data governance is control. This includes the enforcement of policies dictating acceptable use of the system by defining who in the organization has authority and control over data assets and how those data assets may be used.\n\nA third component of data governance is compliance. This means that processes are in place to control your data and ensure that all regulations are met in all your organization's data practices. Compliance starts with configuring your system to achieve compliance requirements.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Provide governance in a Power BI environment - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nProvide governance in a Power BI environment\nModule\n8 Units\nFeedback\nIntermediate\nAdministrator\nMicrosoft Power Platform\nPower BI\n\nPower BI governance is a set of rules, regulations, and policies that define and ensure the effective, controlled, and valuable operation of a BI environment. In this module, you'll learn the fundamental components and practices necessary to govern a Power BI tenant.\n\nLearning objectives\n\nIn this module, you will:\n\nDefine the key components of an effective BI governance model\nDescribe the key elements associated with data governance\nConfigure, deploy, and manage elements of a BI governance strategy\nSet up BI help and support settings\nAdd\nPrerequisites\n\nFamiliarity with Power BI and governance practices in a BI environment.\n\nIntroduction\nmin\nElements of data governance\nmin\nConfigure tenant settings\nmin\nDeploy organizational visuals\nmin\nManage embed codes\nmin\nHelp and support settings\nmin\nCheck your knowledge\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "04-save-ss.png (342×373)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-save-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-chart-sections-ss.png (418×380)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-chart-sections-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-added-chart-ss.png (308×210)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-added-chart-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-chart-type-ss.png (571×511)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-chart-type-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-insert-chart-2-ssm.png (360×315)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-chart-2-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-insert-chart-ssm.png (416×282)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-chart-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-dataset-properties-parameter-ss.png (657×645)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-dataset-properties-parameter-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-report-parameter-properties-ssm.png (571×476)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-report-parameter-properties-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-add-parameter-ssm.png (1789×962)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-parameter-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-display-report-ss.png (563×803)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-display-report-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-run-button-ss.png (142×110)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-run-button-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-add-fields-ss.png (1012×316)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-fields-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-insert-tab-ss.png (876×128)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-tab-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-dataset-properties-ssm.png (650×716)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-dataset-properties-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-connection-properties-ssm.png (376×527)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-connection-properties-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nYou have learned about how to create a Power BI paginated report. Now, you can connect to a data source, extract data into a dataset, and populate a table or chart with that data. After you've finished creating the report, you can format your report to make it visually appealing and informative, and then publish it to Power BI.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "04-data-source-properties-build-button-ssm.png (658×489)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-data-source-properties-build-button-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-add-data-source-part-1-ssm.png (1789×961)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-data-source-part-1-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "04-power-bi-premium-ss.png (1084×1289)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-power-bi-premium-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "04-power-bi-report-builder-ssm.png (674×708)",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-power-bi-report-builder-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "Check your knowledge - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/7-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCheck your knowledge\n3 minutes\nAnswer the following questions to see what you've learned.\n1. \n\nWhy are parameters important in Power BI paginated reports?\n\n \n\nThey allow the report developer to control the refresh interval of the report.\n\nThey allow the user to control aspects of how the report is rendered when the report is run.\n\nThey are required so that Power BI can call the paginated report.\n\n2. \n\nPower BI paginated reports are created by using which tool?\n\n \n\nPower BI Desktop\n\nPower BI service\n\nPower BI Report Builder\n\n3. \n\nPower BI paginated reports is an evolved technology that was built from which original tool?\n\n \n\nSQL Server Analysis Services\n\nSQL Server Reporting Services\n\nMicrosoft SharePoint\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Publish the report - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/5-publish",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPublish the report\n2 minutes\n\nTo publish your report, select File > Save as and then select Power BI Service. Your report will now appear in Power BI service. For more information, go to Publish datasets and reports from Power BI Desktop.\n\nBest practices\n\nCreating a report is meant to inform and drive action on the part of the report user. It isn't enough to create a report with sales information on it; the report author should always ask themselves several questions:\n\nWhat purpose is this report for?\n\nWho is using the report?\n\nHow can I help people do a better job?\n\nWhat is the most important information and how can I highlight it?\n\nIs this report readable?\n\nCan people change the elements that they need to if their questions change?\n\nDo I have visuals that are distracting from the core message of the report?\n\nIs this report staying focused in a single topic or only a few topics?\n\nAm I providing all information that the user expects to see in the report?\n\nCreating good headers and footers is an excellent way to help the user interpret the report. You can provide guidance to the user by documenting why this report was created. Adding a report implementation date and time is an excellent practice. Occasionally, reports are run and then saved. People who are looking at a report won't know that they're looking at an older version unless that fact is highlighted in a footer.\n\nTarget the report for your appropriate region. English speakers read top-down, left-to-right. Putting important information, like totals, at the top of the report will highlight that information for English speakers. The way people read dates varies across different countries/regions. For example, Europeans and Americans have different conventions for reading dates. Localize data formats to the appropriate target user.\n\nIn addition to focusing on the visual aspects of the report, a good report author will consider report delivery and data source usage. Good delivery focuses on how the user wants to see the report. Therefore, report authors should ask themselves the following questions to test the appropriate delivery format and ensure that the report is rendering correctly in that format:\n\nDoes the user want the report sent to them in an email message?\n\nDoes the user want the report in a printable format?\n\nDoes the user read the report in a web browser?\n\nPay attention to the height and width of the report page. Verify that the report isn't running off the page when the report renders for the user.\n\nA good report author creates reports that are easy on the data source. If you continue to recall data that you don't need from a data source, you will overburden the data source and affect performance in unpredictable ways. Focusing on only getting pertinent data will help you be a responsible teammate to others who are using the same data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Work with charts on the report - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/4-charts",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Create paginated reports \nAdd\nPrevious\nUnit 4 of 7\nNext\nWork with charts on the report\nCompleted\n100 XP\n1 minute\n\nTwo ways to add a chart to your report are: Select the Chart button, select Insert Chart, and then draw your table on the canvas.\n\nRight-click the report canvas, select Insert, and then select Chart.\n\nNext, choose the type and style of your chart.\n\nAfter you have selected a chart type, the chart will be added to the design surface.\n\nWhen you select the chart, a new window appears to the right. The Chart Data screen allows you to format the chart according to the values and axis properties.\n\nSelect the plus (+) sign beside each section to select the required columns.\n\nFor more information on working with charts, you can search Microsoft documentation regarding SSRS reports. All of the material in the SSRS documentation will apply to Power BI paginated reports.\n\nNext unit: Publish the report\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a paginated report - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/3-paginated-report",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a paginated report\n3 minutes\n\nTo create a report, you must add a visual to the design surface, similar to what you would do in Power BI Desktop. Select the Insert tab from the ribbon at the top to see your options for adding a visual.\n\nFor the purposes of this example, a table visual has been added to the design surface.\n\nWhen you select the Table drop-down menu, you can choose from two options: Insert Table or Table Wizard. Select Insert Table. You can now draw a table on the design surface. From the Report data window, drag fields from the dataset to the table on the design surface.\n\nWhen you have finished, notice that the field is added to the lower portion of the table in the square brackets. The header will also be added. You can rename or apply formatting to the headers, such as bolding or underlining the text.\n\nTo test this simple report, select the Run button from the Home tab in the ribbon.\n\nThe report will run and display actual data in the table.\n\nNotice that some items have changed in the report: a title was entered at the top, table headers were renamed and are in bold font, and a background color was selected on the header. These changes were implemented to help make using the report easier for the report reader.\n\nIf you want to return to the design surface, select the Design button.\n\nAnother aspect of creating a report is to add a parameter. Parameters can be used for different reasons, for example, when you want the user to enter information that displays on a visual on the report. The most popular reason to add a parameter is to affect which data is retrieved from the data source.\n\nConsider the scenario where you are creating a report that retrieves data from a sales database. You only want sales data from between a begin date and an end date. In this case, you would create two parameters and then modify the dataset query to include those parameters in the WHERE clause of the query. Your first step in this situation is to add a parameter.\n\nAdd parameters\n\nTo add a parameter, right-click Parameters and select Add Parameter.\n\nOn the General tab, name the parameter, select the data type, and then choose the prompt that the user will see.\n\nOn the Available Values tab, enter options that the user can choose from. The Default Values tab has the initial value of the parameter when the report loads, but it can be changed by the user.\n\nYou can also get parameter values from a query. For more information, see the Microsoft documentation on parameters.\n\nAfter you have created a parameter, you can use it to interact with the report. If you return to the dataset, you can connect that parameter with the query.\n\nThe parameter reference starts with the at (@) symbol. Add the parameter name to the query text. Now, when the report refreshes, the data will be pulled from the data source according to the WHERE clause and the parameter value.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/2-get-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet data\n3 minutes\n\nThe first step in creating a report is to get data from a data source. Though this process might seem similar to getting data in Power BI, it is different. Power BI paginated reports do not use Power Query when connecting to data sources.\n\nGetting data in a Power BI paginated report does not involve data cleaning steps. In fact, data is not stored in a Power BI paginated report dataset. When data is refreshed on the report, it is retrieved in an unaltered form from the data source, according to the query that was used to retrieve it.\n\nData can be collected from multiple data sources, including Microsoft Excel, Oracle, SQL Server, and many more. However, after the data has been collected, the different data sources cannot be merged into a single semantic model. Each source must be used for a different purpose. For instance, data from an Excel source can be used for a chart, while data from SQL Server can be used for a table on a single report. Paginated reports have an expression language that can be used to look up data in different datasets, but it is nothing like Power Query.\n\nPower BI paginated reports can use a dataset from Power BI service. These datasets have used Power Query to clean and alter the data. The difference is that this work was done in Power BI Desktop or SQL Server Data Tools prior to using Power BI Report Builder, which doesn't have that tool in the user interface.\n\nCreate and configure a data source\n\nTo retrieve data, open Power BI Report Builder. From the Getting Started screen, select New Report. You can choose whether to create a report with a table on it, a chart, or a blank report. For the purposes of this example, a blank report has been selected. These choices create a default visual on your new report, which can be changed at any time. Next, go to the Report Data window, which is typically on the left side of the tool, though it can be moved around.\n\nRight-click the Data Sources folder and select Add Data Source.\n\nOn the General tab, name the data source.\n\nAfter naming the data source, choose the correct connection string by selecting the Build button.\n\nAfter you have selected Build, the Connection Properties screen appears. The properties on this screen will be unique for each data source. The following figure is an example of what you might see in the screen. The figure shows the properties of a SQL Server connection that you, the report author, will enter:\n\nServer name\n\nDatabase name\n\nA button for testing the connection\n\nSelect OK to continue\n\nYou can also enter username and password information on the Connection Properties screen, or you can leave it on the default setting and use your Windows credentials. Select OK again.\n\nYou've now created a data source.\n\nGenerally, authentication is beyond the scope of this course. Typically, you will receive the connection information from your IT department, application specialist, or the software vendor.\n\nCreate and configure a dataset\n\nA data source is the connection information to a particular resource, like SQL Server. A dataset is the saved information of the query against the data source, not the data. The data always resides in its original location.\n\nRight-click Datasets in the Report View window and select Add Dataset. Ensure that the correct data source is selected. This action will run the query against the correct data source.\n\nFrom the window that displays, you can:\n\nName the query.\n\nChoose whether to use a text command or a stored procedure.\n\nEnter a query into the text box.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to paginated reports - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction to paginated reports\n2 minutes\n\nPaginated reports allow report developers to create Power BI artifacts that have tightly controlled rendering requirements. Paginated reports are ideal for creating sales invoices, receipts, purchase orders, and tabular data. This module will teach you how to create reports, add parameters, and work with tables and charts in paginated reports.\n\nPaginated reports defined\n\nPaginated reports give a pixel-perfect view of the data. Pixel perfect means that you have total control of how the report renders. If you want a footer on every sales receipt that you create, a paginated report is the appropriate solution. If you want a certain customer's name to always appear in green font on a report, you can do that in a paginated report.\n\nPower BI paginated reports are descendants of SQL Server Reporting Services (SSRS), which was first introduced in 2004. Power BI paginated reports and SSRS have a lot in common. If you're looking for information on paginated reports and can't find it, searching the internet and Microsoft documentation on SSRS is an excellent idea because you'll find numerous blog posts, videos, and documentation available to you.\n\nWhen paginated reports are the right fit\n\nYou can use paginated reports for operational reports with tables of details and optional headers and footers.\n\nAdditionally, you can use paginated reports when you expect to print the report on paper or when you want an e-receipt, a purchase order, or an invoice. Paginated reports also render tabular data exceedingly well. You can have customized sort orders, clickable-headers, and URLs in results, which allows for simple integration with custom applications.\n\nPower BI paginated reports can also display all of your data in a single report element, such as a table. If you have 25,000 records, and you want the reports to print over 100 pages, you can do that. If you want every third record to be printed with a light pink background, you can do that as well.\n\nPower BI paginated reports are not created in Power BI Desktop; they are built by using Power BI Report Builder. You can share paginated reports with either a Power BI Pro or a Power BI premium license.\n\nIn this module, you will:\n\nGet data.\n\nCreate a paginated report.\n\nWork with charts and tables on the report.\n\nPublish the report.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create paginated reports - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate paginated reports\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\nMicrosoft Power Platform\n\nPaginated reports allow report developers to create Power BI artifacts that have tightly controlled rendering requirements. Paginated reports are ideal for creating sales invoices, receipts, purchase orders, and tabular data. This module will teach you how to create reports, add parameters, and work with tables and charts in paginated reports.\n\nLearning objectives\n\nIn this module, you will:\n\nGet data.\nCreate a paginated report.\nWork with charts and tables on the report.\nPublish the report.\nAdd\nPrerequisites\n\nPower BI paginated reports are a feature of Power BI Premium\n\nIntroduction to paginated reports\nmin\nGet data\nmin\nCreate a paginated report\nmin\nWork with charts on the report\nmin\nPublish the report\nmin\nCheck your knowledge\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Monitor data in real-time with Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5a-exercise-monitor-data-real-time-power-bi?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Monitor data in real-time with Power BI\n45 minutes\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Monitor data in real-time with Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5a-exercise-monitor-data-real-time-power-bi",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI \nAdd\nPrevious\nUnit 6 of 8\nNext\nExercise: Monitor data in real-time with Power BI\nCompleted\n100 XP\n45 minutes\n\nThis unit includes a lab to complete.\n\nUse the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch the lab\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nPower BI offers features to support real-time analytics in Power BI reports, dashboards, and paginated reports.\n\nYou should carefully consider latency tolerances and required functionality before selecting the artifact type. Also, when working with Premium features, you should also determine whether licenses can be purchased.\n\nIn general, you should strive to deliver real-time Power BI visualizations with Power BI reports. They provide the greatest design flexibility, can be filtered, and are highly interactive. Considering creating real-time Power BI dashboards when you can’t achieve the requirements with Power BI reports, like when alerting is a requirement.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nGladys is a developer at Adventure Works who needs to create a real-time Power BI solution to monitor their call center. When there are excessive numbers of callers waiting on hold, Power BI should notify the call center manager. What type of Power BI artifact should Gladys develop?\n\n \n\nDashboard.\n\nPower BI report.\n\nStreaming dataflow.\n\n2. \n\nBenny is a report author at Adventure Works who needs to set up a Power BI report page to show real-time manufacturing data with latency of 5 minutes. The report connects to a dataset that represents a DirectQuery model. Because the DirectQuery source is an operational system, it should be queried only when necessary. What should Benny do?\n\n \n\nSet the AutoRefresh property to a value greater than zero.\n\nEnable automatic page fresh with a change detection measure.\n\nSet up incremental refresh for the dataset and enable the DirectQuery partition.\n\n3. \n\nFiafia is a developer at Adventure Works who needs to deliver a real-time Power BI dashboard. It should present real-time manufacturing data with the lowest possible latency. The dashboard only shows manufacturing events for the past hour. The solution should use the minimum of Power BI resources. What type of dataset should Fiafia develop?\n\n \n\nDirectQuery dataset.\n\nPush dataset.\n\nStreaming dataset.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Set-up auto-refresh paginated reports - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5-set-up-auto-refresh-paginated-reports",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSet-up auto-refresh paginated reports\n2 minutes\n\nPower BI paginated reports support a real-time capability, but only when the report in rendered in HTML.\n\nAt design time, you can set the report’s AutoRefresh property. By default, it’s set to zero, which means that the report won’t automatically refresh. When you set this property to a value greater than zero, it determines the rate in seconds at which the report automatically refreshes.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create real-time dashboards - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/4-create-real-time-dashboards",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI \nAdd\nPrevious\nUnit 4 of 8\nNext\nCreate real-time dashboards\nCompleted\n100 XP\n12 minutes\n\nPower BI dashboards always present the latest data that the Power BI service is aware of. For this reason, dashboards are ideal to support real-time monitoring scenarios.\n\nThere are four special types of Power BI datasets that support real-time dashboards. They are:\n\nStreaming dataset\nPush dataset\nHybrid dataset\nPubNub streaming dataset\n\nThere aren’t any requirements that real-time dashboards or their related datasets be stored in a dedicated capacity.\n\n Tip\n\nRemember that dashboards can refresh as frequently as every 15 minutes when its tiles connect to a regular dataset that represents a DirectQuery model. For more information, see Unit 3.\n\nDashboards offer an interesting capability that reports don’t. You can set dashboard tiles (that display a single numeric value) to alert you when data values reach specific thresholds. At Adventure Works, their manufacturing plant is set up with many IoT devices. A Power BI dashboard presents real-time manufacturing metrics and notifies maintenance staff when devices require attention.\n\nUse streaming datasets\n\nA streaming dataset specifically delivers real-time data with latency of about one second. Real-time dashboard tile updates are smooth and fluid.\n\nThe streaming dataset caches data in a temporary cache for up to 60 minutes. Because there’s no data model, it’s not possible to create Power BI reports that connect to a streaming dataset. Instead, to visualize streaming data, you add streaming tiles to the dashboard. A streaming tile specifically visualizes real-time data sources from a streaming dataset. They’re easy to identify in a dashboard because they’re adorned with a lightning strike icon.\n\nStreaming datasets are simple in their design because they represent a single table. Table columns are either text, number, or date. You can create a streaming dataset in the Power BI service or programmatically by using the Power BI REST API.\n\nDevelopers can programmatically insert rows into the dataset by sending a JSON document to the dataset’s endpoint. The endpoint URL includes an authentication key.\n\nHere’s a methodology to create a real-time dashboard by using a streaming dataset:\n\nCreate a streaming dataset.\n\nAdd a streaming tile to a dashboard.\n\nProgrammatically insert rows into the streaming dataset.\n\nHowever, there are some important limitations to bear in mind:\n\nData retention is 60 minutes, so it’s not possible to monitor on history beyond that time.\n\nThe maximum ingestion rate is 5 requests/second.\n\nThe packet size of a request to add new rows can’t exceed 15 KB.\n\nPower BI reports can’t connect to a streaming dataset.\n\nBecause dashboards don’t support filtering, you can’t filter streaming tiles.\n\nUse push datasets\n\nA push dataset specifically delivers real-time data with latency of between 3-5 seconds. While structurally it’s closely related to a Power BI import dataset, push datasets can’t be created by using Power BI Desktop. A push dataset can include multiple tables, relationships, and measures. However, it can’t include some model objects, like hierarchies and security roles.\n\nAs the dataset type name suggests, data is pushed into dataset tables. When data is pushed, Power BI immediately refreshes related dashboard tiles.\n\nYou can create a push dataset in the Power BI service (or as a hybrid dataset as described later) or programmatically by using the Power BI REST API. Developers must first acquire a Microsoft Entra access token to work with the REST API operations. They can use the API to push rows of data to a specific table. There are also API operations they can use to modify the dataset schema, delete all rows from a table, and delete the dataset.\n\nIt's also possible use Azure Stream Analytics (ASA) to create a push dataset and output rows of data to it. As a complex event-processing engine, ASA can push high volumes of fast streaming data, even from thousands of IoT devices. For more information, see Power BI output from Azure Stream Analytics.\n\n Tip\n\nASA can integrate with Azure Machine Learning (AML), allowing output of machine learning predictions. For example, an ASA job output could predict that an IoT device requires maintenance. You can set up a Power BI dashboard tile to alert you when maintenance is required. For more information, see Integrate Azure Stream Analytics with Azure Machine Learning.\n\nHere’s a methodology to create a real-time dashboard by using a push dataset:\n\nCreate a push dataset.\n\nCreate a Power BI report that connects to the push dataset. Use any type of visual, including custom visuals to visualize the dataset data.\n\nPin report visuals to a dashboard.\n\nOptionally, use the dashboard Q&A experience to add other tiles.\n\nProgrammatically push rows into the push dataset table.\n\nHowever, there are some important limitations to bear in mind:\n\nThe dataset can’t contain more than 75 tables, and tables can’t contain more than 75 columns.\n\nA push dataset table can’t store more than 5 million rows, unless the basicFIFO retention policy is enabled. When enabled, a table will store approximately 200,000 rows of data and Power BI will replace older rows with new rows.\n\nA single request can’t push more than 10,000 rows.\n\nThe ingestion rate is limited to one million rows per hour unless the table stores more than 250,000 rows of data. In this case, the ingestion rate is limited to 120 rows per hour.\n\nUse hybrid datasets\n\nA hybrid dataset is both a streaming and push dataset at the same time. It delivers the benefits of both types of datasets. Use a hybrid dataset to visualize real-time data in streaming tiles and regular tiles, which you pin from Power BI reports or Q&A. Also, hybrid datasets allow your real-time solution to monitor and analyze activities that happened more than 60 minutes ago.\n\nWhen you create a streaming dataset in the Power BI service, you can make it a hybrid dataset by enabling the Historic data analysis option.\n\nUse PubNub streaming datasets\n\nA PubNub streaming dataset is a special type of streaming dataset. It requires you to have an established real-time platform with PubNub. The Power BI web client uses the PubNub SDK to read an existing PubNub data stream. Like with streaming datasets, there’s no underlying data model, so it’s not possible to use Power BI report visuals.\n\nInstead, dashboard streaming tiles connect to a PubNub streaming dataset. These tiles are optimized to quickly display real-time data. Because Power BI connects directly to the PubNub data stream, there’s little latency between when the data is pushed and when the tiles update.\n\nCompare streaming and push datasets\n\nThe following table compares the capabilities of streaming and push datasets.\n\nExpand table\nCapability\tStreaming dataset\tPush dataset\nLatency\t~1 second\t3-5 seconds\nData retention\t60 minutes\t5 million rows per table, or 200,000 rows when basicFIFO retention is set\nMaximum ingestion rates\t5 requests/second, 15 KB per request\t1 request/second, 16 MB per request (maximum 10,000 rows)\nData throughput limits\tNone\t1 million rows/hour, but 120 rows/hours when the table exceeds 250,000 rows\nDataset structure\tSingle table\tRich data model that supports filtering and aggregation\nVisual types\tStreaming tiles only\tReport visuals, including custom visuals\nAnimation updates\tSmooth and fluid\tA bit twitchy\nNext unit: Set-up auto-refresh paginated reports\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Set up automatic page refresh - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/3-set-up-automatic-page-refresh",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI \nAdd\nPrevious\nUnit 3 of 8\nNext\nSet up automatic page refresh\nCompleted\n100 XP\n12 minutes\n\nAutomatic page refresh (APR) is a feature that automatically refreshes a Power BI report. It’s a setting you can enable for a specific report page providing the report connects to a dataset that:\n\nIncludes DirectQuery storage tables.\nIs a live connection to a tabular model in Azure Analysis Services (AAS) or SQL Server Analysis Services (SSAS).\nIs a push (or hybrid) dataset.\n\n Note\n\nPush and hybrid datasets are described in the next unit.\n\nConceptually, the APR feature is simple. According to a refresh interval you set, Power BI automatically refreshes the report page. It simulates the same refresh operation that the report consumer can invoke from the action bar.\n\nWork with DirectQuery storage tables\n\nA Power BI dataset can have DirectQuery storage tables when:\n\nIt represents a DirectQuery model.\nIt represents a composite model.\nIt contains a hybrid table.\nIt consumes a streaming dataflow (and uses DirectQuery storage mode).\n\n Note\n\nFor more information about DirectQuery and composite models, work through the Choose a model framework module.\n\nUse hybrid tables\n\nIf you set up a dataset import table by using the Power BI incremental refresh feature, you can enable the Get the latest data in real-time with DirectQuery option.\n\nBy enabling this option, Power BI automatically creates a table partition that uses DirectQuery storage mode. In this case, the table becomes a hybrid table, meaning it has import partitions to store older data, and a single DirectQuery partition for current data.\n\nWhen Power BI queries a hybrid table, the query may use the cache for older data, or passthrough to the data source to retrieve current data.\n\nThis option is only available with a Premium license.\n\nFor more information, see Configure incremental refresh and real-time data.\n\nUse streaming dataflows\n\nA streaming dataflow allows data modelers to connect to, ingest, mash up, model, and build reports based on streaming, near real-time data directly in the Power BI service.\n\n Note\n\nA streaming dataflow is conceptually different from a regular dataflow.\n\nYou create streaming dataflows in the Power BI service. They ingest streaming input that you can source from Azure Event Hubs, Azure IoT Hubs, or Azure Blob storage. Using a drag-and-drop, no-code designer, you can filter, aggregate, join, group, and union input streams. You can also set up time-window functions, which can use tumbling, hopping, sliding, session, or snapshot windows of time.\n\nStreaming dataflows are consumed by a model that you develop in Power BI Desktop. Use the dataflows connector to connect to the streaming dataflow, and be sure to set the model storage mode to DirectQuery.\n\nFor more information, see Streaming dataflows.\n\nThere are some important restrictions to bear in mind:\n\nStreaming dataflows are only available with a Premium license.\nA Power BI administrator must enable streaming dataflows.\nA capacity administrator must enable the dataflows enhanced compute engine.\nYou can’t store streaming dataflows and regular dataflows in the same workspace.\nSet up automatic page fresh\n\nTo set up APR, in the page settings, you enable the Page refresh setting.\n\n Note\n\nThis setting is only available when the report connects to a supported dataset. It won’t be available, for example, when the report connects to a dataset that represents in import model.\n\nOnce enabled, you can set the Refresh type property to one of two options:\n\nAuto page refresh – Updates all page visuals based on a fixed interval, which can be from every one second to multiple days.\nChange detection – Updates all page visuals providing that source data has changed since the last automatic refresh. It avoids unnecessary refreshes, which can help to reduce resource consumption for the Power BI service and the data source. This option is only supported for reports stored in a workspace that has its license mode set to Premium, Premium per user, or Embedded (known as Premium workspaces).\n\n Important\n\nWhen using a fixed interval, consider the burden it might place on the data source. Factor in that multiple users may open the report page, and that each visual on the page results in at least one query to the data source.\n\nSet up change detection\n\nTo set up change detection, you must create a special type of measure called a change detection measure. Your report can only have one change detection measure. Power BI uses it to query the data source. Each time, Power BI stores the query result so it can compare it with the next result (according to the refresh interval you set). When the results differ, Power BI refreshes the page.\n\nChange detection measures are easy to set up in Power BI Desktop. The Change detection window allows defining a change detection measure that summarizes any column by using an aggregate function (count, count distinct, minimum, maximum, and sum).\n\nAt Adventure Works, they use APR to monitor real-time manufacturing metrics. IoT devices store events that include a timestamp. The change detection measure queries for the maximum timestamp event because the page should only refresh when new events are recorded.\n\nIn Power BI Desktop, you can use Performance Analyzer to monitor when Power BI queries the change detection measure, and when visuals refresh. For more information, see Use Performance Analyzer to examine report element performance.\n\nWork with restrictions\n\nOnce you publish an APR report to the Power BI service, Power BI may enforce restrictions related to APR.\n\nWhen you publish a report to a workspace that has its license mode set to Pro, it means the workspace resides in a shared capacity. A shared capacity is shared with other Microsoft customers. To avoid noisy neighbor situations (where a co-tenant monopolizes resources), an APR refresh has a minimum interval of 30 minutes, even if the refresh interval in your report is less than that value. Change detection measures aren’t supported in shared capacities.\n\nWhen you publish a report to a workspace that has its license mode set to Premium per user, or Embedded (called a dedicated capacity), APR may not be enabled or is constrained. That’s because a capacity admin can enable or disable APR, and enable or disable the use of a change detection measure for a dedicated capacity. They can also set a minimum refresh interval and a minimum execution interval for change detection measures. When your report page settings are lower than the minimum intervals, the minimum intervals will prevail.\n\nFor more information about APR support for different datasets and capacity types, see Restrictions on refresh intervals.\n\nNext unit: Create real-time dashboards\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Demo-Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/2-describe-power-bi-real-time",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI \nAdd\nPrevious\nUnit 2 of 8\nNext\nDescribe Power BI real-time analytics\nCompleted\n100 XP\n6 minutes\n\nPower BI can show real-time in different artifact types. Artifacts include Power BI reports, dashboards, and paginated reports. Whether viewed in the Power BI service, a Power BI mobile app, or an app with embedded Power BI content, Power BI content will refresh automatically to always show current data. Depending on the real-time feature, Power BI can reliably show current data with just seconds of latency.\n\nHere’s an example of a real-time Power BI report. It relies upon a feature known as automatic page refresh, which is described in the next unit.\n\n Tip\n\nIn general, you should strive to deliver real-time Power BI visualizations with Power BI reports. They provide the greatest design flexibility, can be filtered, and are highly interactive. Considering creating real-time Power BI dashboards when you can’t achieve the requirements with Power BI reports, like when alerting is a requirement.\n\nYou should factor into your requirements what degree of latency is tolerable. It often depends on the velocity (or volatility) of the data, and the urgency to keep people informed about the current state of data.\n\nConsider at Adventure Works that there’s a daily finance report. Report consumers expect to see complete financial data up until the previous day. They might describe that requirement as real-time, especially if former reporting solutions took days or weeks to deliver yesterday’s data. From a Power BI perspective, that’s not real-time. A regular Power BI dataset, set up to refresh daily (or even every 30 minutes) could deliver that result.\n\nNow consider a different requirement at Adventure Works. This time, it’s a manufacturing dashboard that allows users to monitor the production processes as they’re happening. Users need to monitor the hundreds of IoT devices to learn about their throughput and anomalies, like excessive waits and delays. This dashboard can be considered real-time because it must show low-latency metrics and refresh the data constantly.\n\nTo be clear, Power BI real-time solutions are concerned with producing up-to-date results with between one second and 15 minutes of latency. Power BI can manage latencies greater than 15 minutes by using conventional data refresh techniques. Power BI isn’t well suited when latency delays can’t exceed one second.\n\nTo create real-time Power BI solutions, you might require advanced data modeling skills or programming skills.\n\nSet automatic dashboard tile refresh\n\n Note\n\nThe focus of this module is on using Power BI real-time features. However, it’s helpful to know that in some circumstances you can achieve real-time results without these features.\n\nPower BI datasets that represent a DirectQuery data model (or live connection to an external-hosted data model, like Azure Analysis Services or SQL Server Analysis Services)) provide an easy way to deliver real-time dashboards. In the dataset settings, you can enable the dataset Automatic dashboard tile refresh property. You can then set a refresh frequency between weekly and every 15 minutes.\n\nFor example, when you set the refresh frequency to 15 minutes, Power BI will refresh dashboard tile caches every 15 minutes. Dashboard consumers will see tiles (that connect to the dataset) update in real-time.\n\nWhile this technique is simple to set up, take care not to place too much burden on the DirectQuery data source(s), especially when datasets enforce dynamic row-level security (RLS). Dynamic RLS applies filters based on the current user.\n\nConsider that a dashboard with 10 tiles, shared with 100 users, connects to a DirectQuery dataset that enforces dynamic RLS, and it’s set to refresh tiles every 15 minutes. It will result in Power BI sending at least 1000 tile refresh queries to the data source every 15 minutes.\n\nNext unit: Set up automatic page refresh\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nReal-time analytics allows organizations to make confident decisions based on up-to-date data. These organizations can expect to be agile and take meaningful actions based on the most current insights. Common scenarios for real-time analytics include Internet of Things (IoT) devices, factory sensors, e-commerce, inventory management, geo-tracking, financial trading, application telemetry, call centers, and others.\n\nWhat’s meant by real-time is that the data is always current, and users aren’t required to interact with visualizations. Data visualizations should refresh automatically to always show current data.\n\n Note\n\nIn this module, the term real-time actually means near real-time. Near real-time means that there’s always a degree of delay (known as latency), due to data processing and network transmission time.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI real-time analytics.\nSet up automatic page refresh.\nCreate real-time dashboards.\nSet up auto-refresh paginated reports.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Monitor data in real-time with Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nMonitor data in real-time with Power BI\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nDescribe real-time analytics in Power BI using automatic page refresh, real-time dashboards, and auto-refresh in paginated reports.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI real-time analytics.\nSet up automatic page refresh.\nCreate real-time dashboards.\nSet up auto-refresh paginated reports.\nAdd\nPrerequisites\nExperience developing Power BI data models and creating reports and dashboards, and familiarity with DirectQuery storage mode.\nIntroduction\nmin\nDescribe Power BI real-time analytics\nmin\nSet up automatic page refresh\nmin\nCreate real-time dashboards\nmin\nSet-up auto-refresh paginated reports\nmin\nExercise: Monitor data in real-time with Power BI\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nEnterprises often provide reports to large groups of users. Building an organization-wide custom theme is an excellent starting point to provide a cohesive user experience across reports. For complex data analysis, custom R and Python visuals allow you to define a clearer story. Enabling personalized visuals then empowers users to interact with the data in the best way that suits their needs. Performance Analyzer helps you develop efficient reports for all. And most importantly, built-in accessibility features allow you to create inclusive reports for everyone.\n\nYou'll surely have an enterprise-worthy report for all to enjoy using and improve overall user satisfaction after implementing some or all of these features.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat's the best way to share a custom theme?\n\n \n\nSave and share custom theme JSON file.\n\nSend the modified Power BI (.PBIX) file.\n\nSet the default theme for the organization.\n\n2. \n\nWhat elements can be changed with personalized visuals?\n\n \n\nCustom theme color.\n\nVisualization type.\n\nBackground images.\n\n3. \n\nWhat's an inclusive benefit of designing a report with accessibility features?\n\n \n\nUsers don't have auto-play videos slowing the performance.\n\nUsers will admire the bright colors and decorative images used for cohesiveness.\n\nUsers can understand the report using a screen reader or keyboard.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Review report performance using Performance Analyzer - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/6-review-report-performance-use-performance-analyzer",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nReview report performance using Performance Analyzer\n3 minutes\n\nA good report designer is always looking for ways to improve the end-user experience, and report speed is a common complaint. In an enterprise with many reports, every millisecond can improve the experience. By using Performance Analyzer, you can identify suboptimal report elements and adjust where possible.\n\nWhat is Performance Analyzer?\n\nPerformance Analyzer is a built-in feature in Power BI Desktop that measures how long report elements take to update and refresh, allowing you to see if certain elements are significantly slower.\n\nMonitor with Performance Analyzer\n\nIn Power BI Desktop, navigate to the View ribbon, then select Performance Analyzer. The Performance Analyzer pane will appear to the right of the canvas. When you're ready to interact with the report, select the Start Recording button, perform report actions like adjusting slicers, highlighting values, changing pages, etc. You can also Refresh visuals, and then Stop when you're done.\n\nSome easy ways to improve performance include:\n\nLimit visuals per page. If the entire page is slow to load, consider spreading visuals across multiple pages instead.\nRemove unnecessary columns and rows. For slower queries, review the data and determine if anything can be removed.\nSet data types. By default, Power BI will assume data types for imported columns, which should be verified, and all new columns must be manually set.\nApply most restrictive filters. More data will take more time to load, and isn't always the best user experience either. Consider limited results first, and allow drilling for more details.\n\n Tip\n\nThe best time to optimize your report is before sharing it with end-users as it allows you to improve performance before soliciting user feedback. However, improvement is a continual process and can and should be done regularly once the report is live as well. Stay informed with the Optimization guide for Power BI.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create custom visuals with R or Python - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/5-create-custom-visuals",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate custom visuals with R or Python\n2 minutes\n\nIf your organization relies on open-source languages like R and Python, you can write scripts to transform data and create visualizations. Using R and Python visuals in Power BI extends the opportunity to collaborate with Data Scientists in your organization, and enables the use of visuals your data team may already be using.\n\n Note\n\nThis module won't go in-depth for creating R and Python visuals, however you can review the Add an R or Python visual Microsoft Learn unit for more detail.\n\nR visuals\n\nIn order to develop R visuals, you'll need a supported version of R installed first. Then add the R visual to the canvas, just like using a table or bar chart visual, and add your R code within the visual.\n\nPython visuals\n\nTo develop Python visuals, you need to have Python installed on your machine. Many Python packages are supported in Power BI, but not all. See Learn which Python packages are supported in Power BI for a complete list of supported packages and Python visual limitations.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enable personalized visuals in a report - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/3-enable-personalized-visuals-report",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEnable personalized visuals in a report\n3 minutes\n\nIn a larger organization, it can be difficult to design a single report that meets everyone's needs, so one solution is using personalized visuals. These allow individual consumers to make minor design changes to best understand the data themselves.\n\n Tip\n\nThis is a powerful feature for more advanced report consumers. Empower your users by sharing the Power BI Consumer Documentation for personalized visuals.\n\nWhat can consumers change\n\nReport consumers can make changes for the following items:\n\nVisualization type\nSwap measure or dimension\nAdd/remove legend\nCompare multiple measures\nChange aggregations\n\nConsumers can create a personal bookmark to save their changes after personalizing a report, and even share changes with others. Best of all, they can always reset to default to restore the report view.\n\nEnable personalized visuals\n\nYou can enable personalized visuals in Power BI Desktop or Power BI service.\n\nIn Power BI Desktop, go to File > Options and settings > Options > Current file > Report settings. Make sure Personalize visuals is turned on.\nIn Power BI service, go to Settings on the specific report, then toggle Personalize Visuals on, and Save.\n\nBy default, when you enable personalized visuals, it's enabled for all visuals in a report. You can modify each visual to allow personalization or not. You'll need to enable this setting for each report.\n\n Tip\n\nUse a Power BI Template (.PBIT) file to enable personalized visuals and add on top of your custom theme for faster report building in the future.\n\nFor more information and limitations, see the Power BI Documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Design and configure Power BI reports for accessibility - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/4-design-configure-power-bi-reports-for-accessibility",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDesign and configure Power BI reports for accessibility\n5 minutes\n\nAccessibility is at the heart of Microsoft's values, and Power BI is committed to making Power BI experiences as accessible to as many people as possible. As an enterprise report creator, it's up to you to use the available tools to create inclusive reporting across the organization.\n\n Note\n\nThis unit highlights some accessible design features, and more details including a checklist, can be found in the Power BI Documentation.\n\nConsider your audience\n\nEasily improve accessibility for most consumers with the following modifications:\n\nAlt text for visuals, shapes, and images\nSet tab order for visuals, shapes, and images\nConsistent font, colors, positioning\nColorblind-friendly color schemes\nUsing text or icons in addition to color\nAvoid jargon and acronyms\nSet sort order for visuals\nDisable auto-start videos and audio\nProvide captions and transcripts for videos and audio\nAvoid excess decorative shapes and images\nAlt text\n\nAlt text is one of the most helpful and easily configurable accessibility features. For all visuals, shapes, and images, you can add alt text - a description for use with screen readers.\n\nScreen readers will automatically read the title and type of visual, so add insight and context for low vision users, such as \"Total sales by category, further broken down by product\". If you want to include specific data points, instead of adding them to the static alt text, you can use DAX measures and conditional formatting to create dynamic alt text in enrich the user experience.\n\nTo set alt text, first select the visual or image, navigate to the Format visual/shape menu, and select Alt text. To add dynamic alt text, select the Fx button.\n\nSet tab order\n\nIn Power BI Desktop, you can set the tab order for how a keyboard user will experience your report. First, navigate to the View tab in the ribbon, then select Selection in the Show Panes section.\n\nYou'll see Layer order and Tab order, with a list of report elements. Layer order allows you to stack elements, whereas tab order dictates which item will be accessed next when the keyboard user tabs to the next item. Use the up/down arrows to set the order. Hide items by clicking the eye icon. This action will move the item to the bottom, remove the numbered position, and put a line through the eye to indicate hidden state.\n\n Tip\n\nSet tab order and turn off tab order (mark the item as hidden) on any decorative items.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create and import a custom report theme - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/2-create-import-custom-report-theme",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate and import a custom report theme\n5 minutes\n\nWhen you create a custom theme, you ensure a cohesive look and feel for reports across your organization. Deploying an enterprise-wide custom theme provide consistency for:\n\nOrganizational branding\nAccessibility requirements\nAccess themes\n\nNavigate to the View tab on the ribbon, then select the drop-down arrow on Themes to:\n\nChoose a built-in theme\nBrowse for themes\nView the Theme gallery\nCustomize current theme\nSave current theme\nCreate your custom theme\n\nCurrently there are almost 20 built-in report themes, including high contrast and color-blind safe options. However, if you have a certain color palette and font family in mind, you can go to Customize current theme. From here, you can make changes in the following categories:\n\nTheme name and color settings include theme colors, sentiment colors, divergent colors, and structural colors (Advanced).\nText settings include font family, size, and color, which sets the primary text class defaults for labels, titles, cards and KPIs, and tab headers.\nVisual settings include background, border, header, and tooltips.\nPage element settings include wallpaper and background.\nFilter pane settings include background color, transparency, font and icon color, size, filter cards.\n\nAfter you make your changes, select Apply to save changes to your theme. Now you can use your theme for your current report and export for future use. Custom themes are an excellent choice for organizations, allowing a cohesive look for reports across entire department or organization.\n\nUsing the Customize current theme option is a quick and easy way to create a custom theme. If you want to make finer adjustments to themes you can also create a custom theme through JSON, which we don’t cover in this module.\n\nExport and import themes\n\nAfter you’ve applied changes to your custom theme, you need to go back to the Themes drop-down menu and select Save current theme to export the theme. A JSON file will be created that can be shared with others and used for any future reports. Now that your custom theme has been exported, you and others will need to import it to apply to other reports. Importing is as easy as exporting is – navigate to the Themes drop-down menu again, select Browse for themes, and choose the JSON file you just created (or that was shared with you). You'll get a notification when the theme successfully imports.\n\nEnterprise considerations\n\nAny changes made to this theme will need to be saved again to either overwrite the existing theme or as a new theme. Themes are local to the file as well, so changes made on someone’s copy on their computer won’t affect your copy. The theme could be saved as a Power BI Template (.PBIT) file, and then shared that way as well. When sharing custom themes, consider a business process to validate that the theme and/or template are being used.\n\n Tip\n\nOnce you’ve published your report, consider creating themes for your dashboard in the Power BI service.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nOften, report creators hear complaints that reports look outdated and take forever to load. You can solve many of these problems with features already available in Power BI. As an enterprise data analyst, implementing these advanced features will make your reports more cohesive, inclusive, and efficient.\n\nIn this module, you’ll learn how to:\n\nCreate and import a custom report theme.\nCreate custom visuals with R or Python.\nEnable personalized visuals in a report.\nReview report performance using Performance Analyzer.\nDesign and configure Power BI reports for accessibility.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand advanced data visualization concepts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUnderstand advanced data visualization concepts\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nCreate cohesive, inclusive, and efficient Power BI reports to effectively communicate results.\n\nLearning objectives\n\nIn this module, you’ll learn how to:\n\nCreate and import a custom report theme.\nCreate custom visuals with R or Python.\nEnable personalized visuals in a report.\nReview report performance using Performance Analyzer.\nDesign and configure Power BI reports for accessibility.\nAdd\nPrerequisites\nAbility to visualize data and create reports in Power BI Desktop.\nUnderstanding of reports in the Power BI Service.\nIntroduction\nmin\nCreate and import a custom report theme\nmin\nEnable personalized visuals in a report\nmin\nDesign and configure Power BI reports for accessibility\nmin\nCreate custom visuals with R or Python\nmin\nReview report performance using Performance Analyzer\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Use tools to optimize Power BI performance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/5-exercise?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Use tools to optimize Power BI performance\n45 minutes\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nNow it's your opportunity to practice optimizing Power BI performance using the tools covered in this module. When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Use tools to optimize Power BI performance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/5-exercise",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use tools to optimize Power BI performance \nAdd\nPrevious\nUnit 5 of 7\nNext\nExercise: Use tools to optimize Power BI performance\nCompleted\n100 XP\n45 minutes\n\nThis unit includes a lab to complete.\n\nUse the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch the lab\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nNow it's your opportunity to practice optimizing Power BI performance using the tools covered in this module. When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nYou've been introduced to three tools that you can use to troubleshoot and improve data models.\n\nTo optimize a slow report, you can run the Performance analyzer to measure how each of the report elements performs when users interact with them. From there, you can dig into DAX query performance in DAX Studio, where you can view, sort, and filter performance data. You can also troubleshoot single measures or queries and/or evaluate the overall performance of your data model. To design data models proactively, you can use the Best Practice Analyzer rules in Tabular Editor to implement data modeling best practices as you go.\n\nLearn more\nOptimize a model for performance in Power BI\nOptimization guide for Power BI\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n4 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nStephanie is an enterprise data analyst, working as a resource to data analysts sitting in the finance department. One of the finance analysts asked Stephanie to help troubleshoot why report visuals take 5 seconds to render after a slicer selection is made. What tool should Stephanie use first to begin troubleshooting?\n\n \n\nVisual Studio.\n\nDAX Studio.\n\nPerformance analyzer.\n\n2. \n\nJames has been asked to troubleshoot a report that contains a matrix visual that renders visibly slower than any other visuals on the page. After James runs the Performance analyzer, he notices the need to dig further into the DAX query for the matrix using DAX Studio to understand what's causing the trouble. That same query is running in less than a second in DAX Studio. Has James solved the problem?\n\n \n\nYes. James doesn't need to make any more changes to the DAX query.\n\nNo. James needs to use Tabular Editor to continue to investigate.\n\nNo. James needs to clear the model cache to ensure that query results aren't cached in the model.\n\n3. \n\nMary-Jo is responsible for managing the maintenance and deployment of Power BI assets for the entire organization. What tool can Mary-Jo use to ensure all data models adhere to data modeling best practices?\n\n \n\nDAX Studio.\n\nBest Practice Analyzer in Tabular Editor.\n\nPerformance analyzer.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Optimize a data model by using Best Practice Analyzer - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/4-optimize-data-model-use-best-practice-analyzer",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use tools to optimize Power BI performance \nAdd\nPrevious\nUnit 4 of 7\nNext\nOptimize a data model by using Best Practice Analyzer\nCompleted\n100 XP\n8 minutes\n\nHow can you be sure to implement data modeling best practices as you design and build data models in Power BI? There's a tool for that! The Best Practice Analyzer (BPA) in Tabular Editor can be used during the development of tabular models in Power BI or Analysis Services models.\n\nDescribe Tabular Editor\n\nTabular Editor is an alternative tool for authoring tabular models for Analysis Services and Power BI. Tabular Editor 2 is an open-source project that can edit a BIM file without accessing any data in the model. Tabular editor enables users to easily manipulate tabular models and can be used in a few different ways. Tabular Editor can be used to:\n\nquickly edit data models\nautomate repetitive tasks using scripting\nincorporate DevOps with tabular models\nrun BPA rules\n\nThis unit focuses on using Tabular Editor to run the BPA to ensure you're implementing data modeling best practices as you build.\n\nDescribe the Best Practice Analyzer (BPA)\n\nBPA is a set of rules run in Tabular Editor that notify you of potential modeling missteps or changes that you can make to improve your model design and performance. It includes recommendations for naming, user experience, and common optimizations that you can apply to improve performance. For more information, see Best practice rules to improve your model's performance.\n\nBPA rules are a set of rules that you can add to your instance of Tabular Editor. When BPA rules are run on your tabular model, you'll get a list of rules that your model violates, and can fix them using Tabular Editor.\n\nBPA includes a set of pre-defined rules, and you can also define your own rules to encourage certain conventions and practices when developing tabular models.\n\nTo use the pre-defined BPA rules, download the BPA rules .json file from the GitHub repository. The rules are divided into categories for easier viewing, for example:\n\nPerformance\nDAX Expressions\nError Prevention\nFormatting\nMaintenance\n\nEach rule has a description, and many of the rules also have a reference article/video. Reading the rule description and article will provide context as to why the rule is important and why you should follow it.\n\n Note\n\nBPA rules can be run against tabular models in Power BI Desktop, SQL Server Analysis Services, Azure Analysis Services, or Power BI Premium.\n\nRun BPA in Tabular Editor\n\nInstall Tabular Editor.\n\nTabular Editor has an open-source and a paid version. This unit refers to use of the open-source version only.\n\nDownload the Best Practice Rules from GitHub.\n\nSave the file in C:\\Users\\username\\AppData\\Local\\TabularEditor and name it BPARules.json.\n\nIn Power BI Desktop, select Tabular Editor on the External Tools tab of the ribbon.\n\nTabular Editor will automatically connect to the data model\n\nOn the Tools tab, select Best Practice Analyzer.\n\nThe Best Practice Analyzer window will open, displaying any violations of the Best Practice Rules.\n\n Note\n\nThe best practice analyzer scans your model for issues whenever a change is made to the model. This is on by default but can be disabled.\n\nReview the objects in violation of the rules and fix them using Tabular Editor. For example, let's fix the columns violating the Don't summarize numeric columns rule, starting with the Weight column in the Product table.\n\nDouble select (or right select) the object to go to the object in Tabular Editor. Change the object properties as necessary. In this case, we're changing Summarize by from Sum to None.\n\nTo save your changes to back to the Model.bim file, select Save, or use the keyboard shortcut ctrl + s.\n\n Important\n\nTabular Editor uses the Tabular Object Model (TOM) to load and save metadata to and from Model.bim files. When fixing Best Practice rule violations, saving changes in Tabular Editor pushes modifications to the connected Power BI desktop data model.\n\nCustomize BPA for your organization\n\nBPA was designed to enable you to create custom rules and best practices for your organization. If the Best Practice Rules don't suit your needs, you can create rules required for data modeling best practices as you deem appropriate.\n\nYou can also edit existing rules, disable and ignore rules, and set severity levels for each rule. From the Best Practice Analyzer window, select Manage Best Practice Rules. This will enable you to turn rules to use in your scan on and off, edit rules, and delete rules. Edits you make to Best Practice Rules in this window will then be saved to the .json file.\n\nIncorporate the use of BPA into your Continuous Integration/Continuous Deployment (CI/CD) process\n\nIf your organization has an established CI/CD process using Git, BPA can be integrated into your Azure Pipelines. If objects violate Best Practice Rules in the build, you can establish a process to either fail the build or proceed with warning based on the severity of the violation.\n\n Note\n\nThe severity level that you set for each rule while managing your best practice rules only comes into play when deploying models using the command line option in Tabular Editor.\n\nNext unit: Exercise: Use tools to optimize Power BI performance\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Troubleshoot DAX performance by using DAX Studio - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/3-troubleshoot-dax-performance-use-dax-studio",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTroubleshoot DAX performance by using DAX Studio\n12 minutes\n\nIn unit 2, we learned that Performance analyzer data can be further analyzed in DAX Studio. Import performance data into DAX Studio, where you can view, sort, and filter all performance data. You can also bring a single query into the editor where you can make adjustments to the query to improve performance.\n\nUnderstand the VertiPaq engine\n\nBefore jumping into optimization, it's important to understand what's happening under the hood of Power BI. By using compression algorithms and a multi-threaded query processor, the Analysis Services VertiPaq engine delivers fast access to tabular model objects and data by Power BI.\n\nPower BI reads the content of your data source and transforms it in the internal VertiPaq columnar data structure, where each column is encoded and compressed. Dictionaries and indexes are created for each column. Finally, data structures are created for relationships and computation and compression of calculated columns occurs.\n\nDAX queries are being processed by two engines, the formula engine and the storage engine. We'll talk more about these engines below.\n\nDescribe DAX Studio\n\nDAX Studio is an open-source tool for executing DAX queries against Power BI and Analysis Services models. DAX Studio is useful for:\n\nReviewing the contents of your data model.\nWriting and optimizing complex DAX formulas and queries.\n\nDownload and install DAX Studio with the default settings, connect to your data model, and begin working on your queries. Once DAX Studio is installed, it can also be launched from the External Tools tab of the ribbon in Power BI desktop.\n\nOptimize the data model\n\nNow that you have an idea of how the VertiPaq engine works, let's discuss how you can use DAX Studio to optimize DAX queries that run in this environment.\n\nOptimize DAX queries\n\nCalculations using DAX, either measures or columns, are a part of a DAX query, which is processed by two engines in VertiPaq. When a query is processed, the formula engine processes the request, asks the storage engine for data, and performs necessary calculations. The storage engine retrieves and aggregates data requested by the formula engine.\n\nIn the diagram below, the DAX query is sent to the tabular model in steps 1 and 2. The request is then processed by the formula engine and sent to the storage engine, represented by step 3. In step 4, the storage engine either retrieves the data from the model and stores it in memory (for import mode), or passes the query on to the data source (for DirectQuery). For import mode, refreshing the data will retrieve the data from the source.\n\nTroubleshooting in DAX Studio enables you to see detailed statistics on the server timings of your query. You're able to view the proportion of time the query takes in each engine, and can then adjust your queries accordingly to improve performance.\n\nLet's talk through a scenario to understand how you can optimize a query using DAX Studio.\n\nYou have a report that contains a matrix visualizing 6 measures. Your CEO informs you that the visual is slow to render, and therefore the report is unusable. You start to dig in and confirmed slow render times using the Performance analyzer in Power BI desktop.\n\nYou then copy the query to look at it in DAX Studio to get more information on what might be causing your problem. In DAX Studio, you clear the cache(1), turn on the server timings (2), and then run the query (3).\n\nFrom top left to bottom right, the statistics tell you how many milliseconds it took to run the query, and the duration the storage engine (SE) CPU took. In this case, the formula engine (FE) took 73.5% of the time, while the SE took the remaining 26.5% of the time. There were 34 individual SE queries and 21 cache hits.\n\nFrom here, you can investigate what in your measures might be causing the issues. This requires deep DAX knowledge and is sometimes a case of trial and error.\n\nYou experiment with the measures and recognize that improving the sales measure by replacing complicated IF statements in DAX with variables and a time intelligence function. A safe way to experiment is to comment out measures and rework them. You can comment out measures by typing two forward slashes at the beginning of a line (//). For multi-row comments, use /* at the beginning of the comment and */ to close the comment.\n\nAfter experimenting, you clear the cache and run the query again with the updates you made to the measure. You find that your updated measure performs much better, with nearly a 50% reduction in query execution time.\n\n Important\n\nThe storage engine caches the results in memory for reuse. Because of this cache, it is critical to clear the cache prior to running queries in DAX Studio.\n\nView model metrics using VertiPaq Analyzer\n\nViewing the VertiPaq Analyzer Metrics in DAX Studio is a great way to get an overall view of what's going on in your data model. VertiPaq Analyzer reports the memory consumption of the data model and can be used to quickly identify where you're spending the most memory. In short - you can use VertiPaq Analyzer to make memory gobbling offenders obvious, rectify them in Power BI, and then rerun VertiPaq Analyzer to see the immediate benefits of your data model updates.\n\n Note\n\nThe VertiPaq engine only stores data in memory in import models. If you're using DirectQuery, the VertiPaq engine simply sends that query to the source. This means that viewing the VertiPaq Analyzer Metrics will only be helpful for import or composite models.\n\nYou can look at the size of the table, columns, etc., in bytes. The .pbix file further compresses these sizes - the displayed sizes in bytes are evaluated prior to compression.\n\nTo view model metrics, launch DAX Studio from the external tools tab of the Power BI ribbon and select View Metrics from the Advanced tab in DAX Studio.\n\nVertiPaq analyzer displays a number of important metrics about your model. We're going to focus specifically on memory consumption and cardinality. For a complete list of what each of these columns mean, consult the DAX Studio documentation.\n\nViewing metrics in DAX Studio helps you immediately find and fix problems. In this case, you can see that the problem is a column with high cardinality. You can then fix that issue back in Power BI, refresh the metrics, and immediately see the effects of your changes on the model.\n\nFor example, notice that the model in the image below contains a table that consumes 99.6% of the database memory. By drilling into the table, you can see that two columns, End date and Start date are gobbling up the most memory.\n\nTake a look at those two columns back in Power BI desktop and notice that they're Date/time columns. Date/time columns inherently have high cardinality due to all of the possible combinations of dates and times.\n\nUsing the VertiPaq Analyzer in DAX Studio can help you easily identify and eliminate columns with high cardinality (including auto Date/time and floating-point decimal data types), and identify and remove columns that aren't used for anything.\n\n Note\n\nRefer to the Power BI optimization guide for more detailed information on optimizing the data model.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Performance analyzer - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/2-use-performance-analyzer",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Performance analyzer\n10 minutes\n\nThe first tool we're looking at is the Performance analyzer, which is built in to Power BI Desktop. The Performance analyzer helps you understand how report elements like visuals and DAX queries are performing.\n\nThe Performance analyzer helps you optimize at two of the four architecture levels, the data model and report visuals. The Performance analyzer is a great place to start when you're optimizing reports.\n\nUnderstand performance using Performance analyzer\n\nThe Performance analyzer displays and records logs that measure how each of your report elements performs when users interact with them. You can also see which aspects of their performance are most (or least) resource intensive. You can immediately see results, make changes, run the analyzer again, and see the impact of changes you've made.\n\nThe Performance Analyzer captures operations that occur in several major subsystems involved in executing a Power BI Report:\n\nReport Canvas provides the user interface for Power BI reports including hosting visuals and filters, managing user interactions for consuming and authoring reports, and retrieving data for display. The Report Canvas is written using web technologies and runs in web browsers or web browser components. The Report Canvas retrieves data using a high-level, internal, Power BI query language known as Semantic Query.\nData Shape Engine (DSE) evaluates Semantic Queries by generating and running one, or more DAX queries against a data model hosted inside Power BI, Power BI Desktop, Azure Analysis Services, or SQL Server Analysis Services.\nData Model Engine (AS) stores the data model and provides services to reports, such as DAX query evaluation. The model may be hosted in Power BI, Power BI Desktop, Azure Analysis Services, or SQL Server Analysis Services. Depending on the data model host, a model may be tabular or multidimensional. Tabular models may contain in-memory tables, Direct Query tables, or a mix of such tables. DAX queries against tables in Direct Query mode will trigger queries to the Direct Query data source. For example, a DAX query against a Direct Query table backed by a SQL Server database will trigger one, or more, SQL queries.\nUse Performance analyzer\n\nTo use the Performance analyzer, enable the Performance analyzer on the View tab of the ribbon, and select Start recording.\n\nAfter you start recording, take actions in the report. For example, move from one report tab to the next, select a slicer item, or interact with any of your visuals. Any actions you take in the report are displayed and logged in real time in the Performance Analyzer pane, in the order that the visual is loaded by Power BI.\n\nThe Performance analyzer is looking at the time it takes for each visual to query the data model and render results. This is the time from when a user does something on the page to when the visual is rendered, in milliseconds.\n\nIn the example below, three actions were taken after recording started.\n\nNavigated to a new report page\nChanged a slicer value\nCross highlighted a table\n\nEach visual's log information includes the time spent (duration) to complete the following categories of tasks:\n\nDAX query - if a DAX query was required, this is the time between the visual sending the query, and for Analysis Services to return the results.\nEvaluated parameters (preview) - time spent evaluating the field parameters within a visual. Learn more about field parameters (preview).\nVisual display - time required for the visual to appear on the screen, including time required to retrieve any web images or geocoding.\nOther - time required by the visual for preparing queries, waiting for other visuals to complete, or performing other background processing.\n\nAfter you've interacted with elements of the report you want to measure with Performance Analyzer, you can select the Stop button. The performance information remains in the pane after you select Stop for you to analyze.\n\nYou can select Refresh visuals in the Performance Analyzer pane to refresh all visuals on the current page of the report, which will gather information about all visuals.\n\nIf one particular visual appears to be performing slow, you can also refresh individual visuals. When Performance Analyzer is recording, you can select Analyze this visual found in the top-right corner of each visual, to refresh that visual, and capture its performance information.\n\n Important\n\nAs users interact with visuals in Power BI reports, DAX queries are submitted to the dataset and cached into memory. Because of this, you may need to clear the DAX query cache to get accurate results in the Performance analyzer. You can clear the cache by either closing and re-opening your report, or using DAX Studio.\n\nEvaluate performance data further\n\nThere are many different ways you can dig deeper into information recorded by the Performance analyzer.\n\nYou can use DAX Studio to investigate your queries in more detail by copying your query from the performance analyzer. After analyzing the query in DAX Studio, you can use your own knowledge and experience to identify where the performance issues are.\n\nIn the screenshot below, you can see that the DAX query in this table visual took 14 seconds. Users waited 14 seconds before seeing the results of the action that resulted in that query running, which in this case was a cross-highlight action. It's clear that this DAX needs to be optimized.\n\nTo analyze this query further, copy and paste the query into DAX Studio to repeat the execution of the query. In DAX Studio, you can activate more diagnostic tools using the Query Plan and Server Timings tracing options.\n\nFor DAX queries with long duration times, it's likely that a measure is written poorly or an issue has occurred with the data model. The issue might be caused by the relationships, columns, or metadata in your model, or it could be the status of the Auto date/time option.\n\nYou can also save Performance analyzer results by selecting the Export button, which creates a .json file containing results. Each event in the .json file contains timestamps, correlation information, and other metadata about the operation.\n\nYou can then load that data into DAX Studio to navigate through the performance metrics in more detail.\n\n Tip\n\nFor more information about the contents of the .json export file, see the Performance analyzer export file documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nYou've built Power BI reports and they're running slow. How do you identify the source of the problem? The report could be slow because of issues at the source, the structure of the data model, the visuals on the report page, or the environment.\n\nOptimizing performance in the context of this module means making changes to the data model so that it runs more efficiently. The tools we're discussing in this module will help you troubleshoot and improve the data model, which ultimately improves user experience.\n\nIdeally, you should assess performance as you move through the stages of the development lifecycle; development, testing, production, and optimization of your solutions.\n\nIn this module you'll learn how to use tools to optimize a data model, and which tools are useful in which scenarios.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nOptimize queries using Performance analyzer.\nTroubleshoot DAX performance using DAX Studio.\nOptimize a data model using Tabular Editor.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use tools to optimize Power BI performance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse tools to optimize Power BI performance\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nUse tools to develop, manage, and optimize Power BI data model and DAX query performance.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nOptimize queries using performance analyzer.\nTroubleshoot DAX performance using DAX Studio.\nOptimize a data model using Tabular Editor.\nAdd\nPrerequisites\nExperience designing and building Power BI data models.\nIntroduction\nmin\nUse Performance analyzer\nmin\nTroubleshoot DAX performance by using DAX Studio\nmin\nOptimize a data model by using Best Practice Analyzer\nmin\nExercise: Use tools to optimize Power BI performance\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Enforce model security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4a-exercise-enforce-model-security?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Enforce model security\n45 minutes\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nWhen report consumers can see all model data there’s no need to take special steps. However, when they should see subsets of model data or be restricted from certain tables or columns, it’s critical that your model enforces appropriate security.\n\nYou typically restrict access to model data with RLS rules. However, when you develop a DirectQuery model for data sources that support SSO, Power BI can leverage the data source RLS. In this case, as a data modeler, you don’t need to create model roles.\n\nThere are many good development practices you should apply to ensure data permissions are enforced accurately and efficiently.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nJoshua is a data modeler at Adventure Works who is developing a model for a large data warehouse. The model must enforce RLS, and the Power BI reports that connect to the model should deliver the fastest possible performance. What should Joshua do?\n\n \n\nApply rules to dimension tables.\n\nApply rules to hierarchies.\n\nApply rules to fact tables.\n\n2. \n\nRupali is a data modeler at Adventure Works who is developing an import model to analyze employee timesheet data. The employee table stores the employee social security number (SSN) in a column. While the model will be available for all company managers, it will also be available to employees in the Payroll department. However, reports must only reveal employee SSNs to payroll employees. What feature should Rupali use to restrict access to the SSN column?\n\n \n\nRLS.\n\nSSO.\n\nOLS.\n\n3. \n\nKasper is a data modeler at Adventure Works who is developing a model that must enforce RLS. It must restrict access to only the sales regions assigned to the report consumer. The source database includes a table that stores employee usernames and their assigned region(s). What should Kasper do?\n\n \n\nCreate an OLS role and use a dynamic rule.\n\nCreate an RLS role and use a dynamic rule.\n\nCreate an RLS role and use a static rule.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Enforce model security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4a-exercise-enforce-model-security",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Enforce Power BI model security \nAdd\nPrevious\nUnit 5 of 7\nNext\nExercise: Enforce model security\nCompleted\n100 XP\n45 minutes\n\nThis unit requires a VM to complete.\n\nVM Mode provides a free, web-based virtual machine environment to complete the steps in this unit.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch VM mode\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Apply good modeling practices - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4-apply-good-modeling-practices",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nApply good modeling practices\n3 minutes\n\nIt’s critical that your model enforces data permissions correctly and efficiently. The following list provides you with good development practices to apply.\n\nStrive to define fewer datasets (data models) with well-designed roles.\n\nStrive to create fewer roles by using dynamic rules. A data-driven solution is easier to maintain because you don’t need to add new roles.\n\nWhen possible, create rules that filter dimension tables instead of fact tables. It will help to deliver faster query performance.\n\nValidate that the model design, including its relationships and relationship properties, are correctly set up.\n\nUse the USERPRINCIPALNAME function instead of USERNAME function. It provides consistency when validating the roles in Power BI Desktop and the Power BI service.\n\nRigorously validate RLS and OLS by testing all roles.\n\nEnsure that the Power BI Desktop data source connection uses the same credentials that will be applied when set up in the Power BI service.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Restrict access to Power BI model objects - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/3-restrict-access-to-power-bi-model-objects",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nRestrict access to Power BI model objects\n8 minutes\n\nAs a data modeler, you can consider restricting user access to Power BI model objects. Object-level security (OLS) can restrict access to specific tables and columns, and their metadata. Typically, you apply OLS to secure objects that store sensitive data, like employee personal data.\n\nWhen Power BI enforces OLS, not only does it restrict access to tables and columns, but it can also secure metadata. When you secure metadata, it’s not possible to retrieve information about secured tables and columns by using Dynamic Management Views (DMVs).\n\n Important\n\nTabular models can hide tables and columns (and other objects) by using a perspective. A perspective defines viewable subsets of model objects to help provide a specific focus for report authors. Perspectives are intended to reduce the complexity of a model, helping report authors find resources of interest. However, perspectives aren’t a security feature because they don’t secure objects. A user can still query a table or column even when it’s not visible to them.\n\nConsider an example at Adventure Works. This organization has a data warehouse dimension table named DimEmployee. The table includes columns that store employee name, phone, email address, and salary. While general report consumers can see employee name and contact details, they must not be able to see salary values. Only senior Human Resources staff are permitted to see salary values. So, the data modeler used OLS to grant access to the salary column only to specific Human Resources staff.\n\nOLS is a feature inherited from Azure Analysis Services (AAS) and SQL Server Analysis Services (SSAS). The feature is available in Power BI Premium to provide backward compatibility for models migrated to Power BI. For this reason, it’s not possible to completely set up OLS in Power BI Desktop.\n\nSet up OLS\n\nTo set up OLS, you start by creating roles. You can create roles in Power BI Desktop in the same way you do when setting up RLS. Next, you need to add OLS rules to the roles. This capability isn’t supported by Power BI Desktop, so you’ll need to take a different approach.\n\nYou add OLS rules to a Power BI Desktop model by using an XML for Analysis (XMLA) endpoint. XMLA endpoints are available with Power BI Premium, and they provide access to the Analysis Services engine in the Power BI service. The read/write endpoint supports dataset management, application lifecycle management, advanced data modeling, and more. You can use XMLA endpoint-enabled APIs for scripting, such as Tabular Model Scripting Language (TMSL) or the PowerShell SqlServer module. Or you can use a client tool, like SSMS. There are third-party tool options too, like Tabular Editor, which is an open-source tool for creating, maintaining, and managing models.\n\nBy default, all model tables and columns aren’t restricted. You can set them to None or Read. When set to None, users associated with the role can’t access the object. When set to Read, users associated with the role can access the object. When you’re restricting specific columns, ensure the table isn’t set to None.\n\nOnce you’ve added the OLS rules, you can publish the model to the Power BI service. Use the same process for RLS to map accounts and security groups to the roles.\n\nConsiderations\n\nIn a Power BI report, when a user doesn’t have permission to access a table or column, they'll receive an error message. The message will inform them that the object doesn’t exist.\n\nConsider carefully whether OLS is the right solution for your project. When a user opens a Power BI report that queries a restricted object (for them), the error message could be confusing and will result in a negative experience. To them, it looks like the report is broken. A better approach might be to create a separate set of models or reports for the different report consumer requirements.\n\nRestrictions\n\nThere are restrictions to be aware of when implementing OLS.\n\nYou can’t mix RLS and OLS in the same role. If you need to apply RLS and OLS in the same model, create separate roles dedicated to each type. Also, you can’t set table-level security if it breaks a relationship chain. For example, if there are relationships between tables A and B, and B and C, you can't secure table B. If table B is secured, a query on table A can't transit the relationships between table A and B, and B and C. In this case, you could set up a separate relationship between tables A and C.\n\nHowever, model relationships that reference a secured column will work, providing that the column’s table isn’t secured.\n\nLastly, while it isn’t possible to secure measures, a measure that references secured objects is automatically restricted.\n\nFor more information, see Object-level security.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Restrict access to Power BI model data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/2-restrict-access-to-power-bi-model-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Enforce Power BI model security \nAdd\nPrevious\nUnit 2 of 7\nNext\nRestrict access to Power BI model data\nCompleted\n100 XP\n14 minutes\n\nAs a data modeler, you set up RLS by creating one or more roles. A role has a unique name in the model, and it usually includes one or more rules. Rules enforce filters on model tables by using Data Analysis Expressions (DAX) filter expressions.\n\n Note\n\nBy default, a data model has no roles. A data model without roles means that users (who have permission to query the data model) have access to all model data.\n\n Tip\n\nIt's possible to define a role that includes no rules. In this case, the role provides access to all rows of all model tables. This role set up would be suitable for an admin user who is allowed to view all data.\n\nYou can create, validate, and manage roles in Power BI Desktop. For Azure Analysis Services or SQL Server Analysis Services models, you can create, validate, and manage roles by using SQL Server Data Tools (SSDT).\n\nYou can also create and manage roles by using SQL Server Management Studio (SSMS), or by using a third-party tool, like Tabular Editor.\n\nTo gain a better understanding of how RLS restricts access to data, watch the following animated image.\n\nApply star schema design principles\n\nWe recommend you apply star schema design principles to produce a model comprising dimension and fact tables. It’s common to set up Power BI to enforce rules that filter dimension tables, allowing model relationships to efficiently propagate those filters to fact tables.\n\nThe following image is the model diagram of the Adventure Works sales analysis data model. It shows a star schema design comprising a single fact table named Sales. The other tables are dimension tables that support the analysis of sales measures by date, sales territory, customer, reseller, order, product, and salesperson. Notice the model relationships connecting all tables. These relationships propagate filters (directly or indirectly) to the Sales table.\n\nThis model design supports examples presented in this unit.\n\nDefine rules\n\nRule expressions are evaluated within row context. Row context means the expression is evaluated for each row using the column values of that row. When the expression returns TRUE, the user can “see” the row.\n\n Tip\n\nTo learn more about row context, work through the Add calculated tables and columns to Power BI Desktop models module. While this module describes adding model calculations, it includes a unit that introduces and describes row context.\n\nYou can define rules that are either static or dynamic.\n\nStatic rules\n\nStatic rules use DAX expressions that refer to constants.\n\nConsider the following rule applied to the Region table that restricts data access to Midwest sales.\n\nDAX\nCopy\n\n'Region'[Region] = \"Midwest\"\n\n\n\nThe following steps explain how Power BI enforces the rule. It:\n\nFilters the Region table, resulting in one visible row (for Midwest).\n\nUses the model relationship to propagate the Region table filter to the State table, resulting in 14 visible rows (the Midwest region comprise 14 states).\n\nUses the model relationship to propagate the State table filter to the Sales table, resulting in thousands of visible rows (the sales facts for the states that belong to the Midwest region).\n\nThe simplest static rule that you can create restricts access to all table rows. Consider the following rule applied to the Sales Details table (not depicted in the model diagram).\n\nDAX\nCopy\n\nFALSE()\n\n\n\nThis rule ensures users can't access any table data. It could be useful when salespeople are allowed to see aggregated sales results (from the Sales table) but not order-level details.\n\nDefining static rules is simple and effective. Consider using them when you need to create only a few of them, as might be the case at Adventure Works where there are only six US regions. However, be aware of disadvantages: setting up static rules can involve significant effort to create and set up. It would also require you to update and republish the dataset when new regions are onboarded.\n\nIf there are many rules to set up and you anticipate adding new rules in the future, consider creating dynamic rules instead.\n\nDynamic rules\n\nDynamic rules use specific DAX functions that return environmental values (as opposed to constants). Environmental values are returned from three specific DAX functions:\n\nUSERNAME or USERPRINCIPALNAME – Returns the Power BI authenticated user as a text value.\n\nCUSTOMDATA - Returns the CustomData property passed in the connection string. Non-Power BI reporting tools that connect to the dataset by using a connection string can set this property, like Microsoft Excel.\n\n Note\n\nBe aware that the USERNAME function returns the user in the format of DOMAIN\\username when used in Power BI Desktop. However, when used in the Power BI service, it returns the format of the user's User Principal Name (UPN), like username@adventureworks.com. Alternatively, you can use the USERPRINCIPALNAME function, which always returns the user in the user principal name format.\n\nConsider a revised model design that now includes the (hidden) AppUser table. Each row of the AppUser table describes a username and region. A model relationship to the Region table propagates filters from the AppUser table.\n\nThe following rule applied to the AppUser table restricts data access to the region(s) of the authenticated user.\n\nDAX\nCopy\n\n'AppUser'[UserName] = USERPRINCIPALNAME()\n\n\n\nDefining dynamic rules is simple and effective when a model table stores username values. They allow you to enforce a data-driven RLS design. For example, when salespeople are added to, or removed from, the AppUser table (or are assigned to different regions), this design approach just works.\n\nValidate roles\n\nWhen you create roles, it’s important to test them to ensure they apply the correct filters. For data models created in Power BI Desktop, there’s the View as function that allows you to see the report when different roles are enforced, and different username values are passed.\n\nSet up role mappings\n\nRole mappings must be set up in advance of users accessing Power BI content. Role mapping involves assigning Microsoft Entra security objects to roles. Security objects can be user accounts or security groups.\n\n Tip\n\nWhen possible, it’s a good practice to map roles to security groups. That way, there will be fewer mappings, and you can delegate the group membership management to the network administrators.\n\nFor Power BI Desktop developed models, role mapping is typically done in the Power BI service. For Azure Analysis Services or SQL Server Analysis Services models, role mapping is typically done in SSMS.\n\nFor more information about setting up RLS, see:\n\nRow-level security (RLS) with Power BI\n\nRow-level security (RLS) guidance in Power BI Desktop\n\nUse single sign-on (SSO) for DirectQuery sources\n\nWhen your data model has DirectQuery tables and their data source supports SSO, the data source can enforce data permissions. This way, the database enforces RLS, and Power BI datasets and reports honor the data source security.\n\nConsider that Adventure Works has an Azure SQL Database for their sales operations that resides in the same tenant as Power BI. The database enforces RLS to control access to rows in various database tables. You can create a DirectQuery model that connects to this database without roles and publish it to the Power BI service. When you set the data source credentials in the Power BI service, you enable SSO. When report consumers open Power BI reports, Power BI passes their identity to the data source. The data source then enforces RLS based on the identity of the report consumer.\n\nFor information about Azure SQL Database RLS, see Row-level security.\n\n Note\n\nCalculated tables and calculated columns that reference a DirectQuery table from a data source with SSO authentication aren’t supported in the Power BI service.\n\nFor more information about data sources that support SSO, see Single sign-on (SSO) for DirectQuery sources.\n\nNext unit: Restrict access to Power BI model objects\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nYou can enforce model security by restricting access to subset of data, and by restricting access to specific model tables and columns. You might restrict access because some report consumers aren’t permitted to view specific data, like sales results of other sales regions. Achieving this requirement commonly involves setting up row-level security (RLS), which involves defining roles and rules in that filter data model. You can also set up object-level security (OLS), to restrict access to entire tables or columns.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nRestrict access to Power BI model data with RLS.\nRestrict access to Power BI model objects with OLS.\nApply good development practices to enforce Power BI model security.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enforce Power BI model security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nEnforce Power BI model security\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nEnforce model security in Power BI using row-level security and object-level security.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nRestrict access to Power BI model data with RLS.\nRestrict access to Power BI model objects with OLS.\nApply good development practices to enforce Power BI model security.\nAdd\nPrerequisites\nExperience developing Power BI data models by using Power BI Desktop.\nIntroduction\nmin\nRestrict access to Power BI model data\nmin\nRestrict access to Power BI model objects\nmin\nApply good modeling practices\nmin\nExercise: Enforce model security\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Create calculation groups - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/5-lab?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Create calculation groups\n45 minutes\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Create calculation groups - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/5-lab",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Design and build tabular models  Create calculation groups \nAdd\nPrevious\nUnit 5 of 7\nNext\nExercise: Create calculation groups\nCompleted\n100 XP\n45 minutes\n\nThis unit includes a lab to complete.\n\nUse the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch the lab\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nCalculation groups can help you analyze existing measures in multiple ways. You can easily analyze a measure for several time slices, such as month to date, quarter to date, year to date, and previous year.\n\nCalculation groups can significantly reduce the number of redundant measures by grouping common measure expressions as calculation items.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat calculation item property should a report creator define to allow conditional application of format strings to measures?\n\n \n\nDynamic format strings.\n\nPrecedence.\n\nOrdering.\n\n2. \n\nWhat limitation of calculation groups should Power BI report authors be aware of?\n\n \n\nThere are no limitations of calculation groups.\n\nApplying calculation groups to a model disables the use of implicit measures in a Power BI report.\n\nApplying calculation groups disables the ability to refresh data models in the Power BI service.\n\n3. \n\nAuthors working with data models containing multiple calculation groups must be sure to specify what to control the order of application of calculation groups?\n\n \n\nOrdering.\n\nPrecedence.\n\nCalculation group name.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore calculation groups features and usage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/3-explore-usage-features",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Design and build tabular models  Create calculation groups \nAdd\nPrevious\nUnit 3 of 7\nNext\nExplore calculation groups features and usage\nCompleted\n100 XP\n3 minutes\n\nBefore creating calculation groups, let's explore their properties. Calculation groups are made up of calculation items, which are simply DAX statements containing a substitute or placeholder for existing explicit measures in your model. For example, a calculation group, Time Intelligence, may contain the calculation items Year to Date, Quarter to Date, and Month to Date.\n\nCalculation group properties\n\nAnyone using calculation groups needs to be aware of the precedence property of a calculation group.\n\nPrecedence\n\nPrecedence is a property defined for a calculation group. When a data model contains more than one calculation group it's essential to define the precedence, or the order of evaluation.\n\nDefining the precedence ensures that the different calculation groups are executed in the proper order. A higher number indicates greater precedence, meaning it will be applied before calculation groups with lower precedence.\n\nAll calculation items within a single calculation group share the same precedence.\n\nUse Tabular Editor to set the precedence property for the calculation group.\n\nCalculation item properties\n\nCalculation items also contain properties that are important to report developers, including ordering and dynamic format strings.\n\nOrdering\n\nThe ordinal value is the sort order of the calculation item. The order in which calculation items appear in a report can be changed by specifying the Ordinal property. Specifying calculation item order with the Ordinal property doesn't change precedence, the order in which calculation items are evaluated.\n\nIf the ordinal value isn't specified, the default behavior is that calculation items are ordered alphabetically by name.\n\nDynamic format strings\n\nCalculation groups can also be used to define conditional format strings to a measure.\n\nA simple example of using dynamic format strings may be having a different format for totals compared to other values.\n\nDynamic format strings are particularly useful for currency conversion. For example, report consumers may want to see sales by country/region, with the correct currency formatting applied for each. This is accomplished by adding a format string column to the currency dimension table and then creating a currency conversion calculation group and item.\n\nNext unit: Create calculation groups in a model\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create calculation groups in a model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/4-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate calculation groups in a model\n3 minutes\n\nUsing calculation groups requires a baseline knowledge of DAX and filter context. This unit describes the application of calculation groups in a Power BI data model using Tabular Editor.\n\nInstall Tabular Editor\n\nApplication of calculation groups can't be accomplished in Power BI desktop. Download the open-source Tabular Editor 2.x tool to apply calculation groups.\n\nWith Tabular Editor installed, you can access it from the external tools tab of the ribbon in Power BI.\n\nLaunching Tabular Editor from the External Tools tab of the ribbon will open the Tabular Editor interface, with a connection to your Power BI data model.\n\nIn the screenshot below, Tabular Editor was launched from the Adventure Works DW 2020 Power BI report.\n\n Note\n\nLearn more about external tools in Power BI Desktop.\n\nCreate calculation group and items\n\nUse the Tabular Editor interface to create calculation groups and calculation items within those groups.\n\nApply a calculation group\n\nSaving the calculation group in Tabular Editor will save changes to the connected data model. To apply those changes, you must refresh the calculation group query in Power BI desktop.\n\n Note\n\nCalculation groups do not work with implicit measures. Measures to be evaluated by the calculation group must be explicitly defined.\n\nUse your calculation group in a visual\n\nYou can now add your calculation group to a visual. A simple way to visualize your calculation group is to drop the calculation group into the columns field of a matrix visual. This will apply the calculation group to the measure you've placed in the values field.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand calculation groups - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/2-understand",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand calculation groups\n3 minutes\n\nCalculation groups are a simple way to reduce the number of measures in a model by grouping common measure expressions. Calculation groups work with existing explicit DAX measures by automating repetitive patterns.\n\nCalculation groups can be created for tabular models in SQL Server 2019 and later, Analysis Services, Azure Analysis Services, and Power BI.\n\nThe matrix below contains three measures: Orders, Sales, and Profit. Using a calculation group enables you to create two calculation items, Previous Year and Year over Year, which are then applied to each of our three existing measures.\n\nHow to create calculation groups\n\nThe recommended tool for creating calculation groups in Power BI is the free, open-source Tabular Editor 2.x tool. Tabular Editor 2.x lets you manipulate and manage measures, calculated columns, display folders, perspectives, and translations in Analysis Services Tabular and Power BI XMLA Models (from Compatibility Level 1200 and onwards). Power BI Desktop doesn't have the user interface to create calculation groups.\n\nCalculation groups are also supported in Visual Studio with Analysis Services Projects VSIX update 2.9.2 and later.\n\n Note\n\nTo understand more see Calculation groups in analysis services models.\n\nBenefits of using a calculation group\n\nThe main benefit of using calculation groups is a reduction in the overall number of measures you need to create and maintain.\n\nCalculation groups also enable the creation of creative report features, such as switching measures using a slicer, dynamic formatting, and even turning display labels on and off.\n\nLimitations of using a calculation group\n\nThe main limitation of using calculation groups is that implicit measures are no longer supported in your report. If you're used to using implicit measures to quickly cross-check calculations, note that implementing calculation groups will require the creation of explicit measures.\n\n Tip\n\nImplicit measures are automatically generated calculations, achieved by configuring a Power BI visual to summarize column values. Explicit measures are calculations added to a tabular model using a DAX formula.\n\nRefer to calculation groups documentation to read more about limitations.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nDesigning Power BI reports to meet the needs of users can be a challenge, requiring many different calculations and the ability to evaluate each of those calculations in multiple time periods.\n\nFor example, the sales manager at Contoso wants to understand sales trends for the year. They've asked you to create multiple measures (orders, sales, and profit) for several time slices, such as month to date, quarter to date, year to date, and previous year. After jumping in and starting to create measures for each metric for each time slice, you realize what a large amount of work this is going to be! Calculation groups are the best way to achieve this kind of a task without increasing the model’s complexity.\n\nLearning objectives\n\nIn this module, you will:\n\nExplore how calculation groups work.\nMaintain calculation groups in a model.\nUse calculation groups in a Power BI report.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create calculation groups - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate calculation groups\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nPower BI\n\nIn this module you’ll learn what calculation groups are, explore key features and usage scenarios, and learn to create calculation groups.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nExplore how calculation groups work.\nMaintain calculation groups in a model.\nUse calculation groups in a Power BI report.\nAdd\nPrerequisites\nExperience creating reports using Power BI desktop.\nBasic understanding of Tabular Editor 2.\nProficient with Data Analysis Expressions (DAX) in tabular models, at a basic level.\nIntroduction\nmin\nUnderstand calculation groups\nmin\nExplore calculation groups features and usage\nmin\nCreate calculation groups in a model\nmin\nExercise: Create calculation groups\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Create Advanced DAX Calculations in Power BI Desktop - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3b-lab?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Create Advanced DAX Calculations in Power BI Desktop\n45 minutes\nAccess your environment\n\nBefore you start this lab (unless you are continuing from a previous lab), select Launch lab above.\n\nYou are automatically logged in to your lab environment as data-ai\\student.\n\nYou can now begin your work on this lab.\n\n Tip\n\nTo dock the lab environment so that it fills the window, select the PC icon at the top and then select Fit Window to Machine.\n\nExercise story\n\nIn this exercise, you’ll create measures with DAX expressions involving filter context manipulation.\n\nIn this exercise you learn how to:\n\nUse the CALCULATE() function to manipulate filter context\nUse Time Intelligence functions\n\nThis exercise should take approximately 45 minutes.\n\n Note\n\nA limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nUse your own device\n\nAlternatively, configure your environment with the setup instructions.\n\nThen follow these exercise instructions to complete the exercise.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "dax-matrix-mountain-200-bike-stock-2020-june-ssm.png (314×242)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-2020-june-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-mountain-200-bike-stock-2-ss.png (1096×240)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-2-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-mountain-200-bike-stock-1-ss.png (1098×244)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-1-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-model-diagram-inventory-ss.png (532×462)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-model-diagram-inventory-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-new-customers-ssm.png (568×313)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-new-customers-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-customers-ltd-ssm.png (557×312)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-customers-ltd-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-revenue-ytd-ss.png (373×387)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-ytd-ss.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-revenue-yoy-ssm.png (460×566)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-yoy-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-revenue-py-ssm.png (480×567)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-py-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "dax-matrix-revenue-ytd-activity-ssm.png (367×313)",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-ytd-activity-ssm.png#lightbox",
    "html": ""
  },
  {
    "title": "Exercise - Create Advanced DAX Calculations in Power BI Desktop - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3b-lab",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use DAX time intelligence functions in Power BI Desktop models \nAdd\nPrevious\nUnit 4 of 6\nNext\nExercise - Create Advanced DAX Calculations in Power BI Desktop\nCompleted\n100 XP\n45 minutes\n\nThis unit includes a lab to complete.\n\nUse the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch the lab\n\nAccess your environment\n\nBefore you start this lab (unless you are continuing from a previous lab), select Launch lab above.\n\nYou are automatically logged in to your lab environment as data-ai\\student.\n\nYou can now begin your work on this lab.\n\n Tip\n\nTo dock the lab environment so that it fills the window, select the PC icon at the top and then select Fit Window to Machine.\n\nExercise story\n\nIn this exercise, you’ll create measures with DAX expressions involving filter context manipulation.\n\nIn this exercise you learn how to:\n\nUse the CALCULATE() function to manipulate filter context\nUse Time Intelligence functions\n\nThis exercise should take approximately 45 minutes.\n\n Note\n\nA limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nUse your own device\n\nAlternatively, configure your environment with the setup instructions.\n\nThen follow these exercise instructions to complete the exercise.\n\nNext unit: Check your knowledge\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/5-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n5 minutes\n\nIn this module, you learned that time intelligence calculations are concerned with modifying the filter context for date filters. You were introduced to many DAX time intelligence functions, which support the creation of calculations, such as year-to-date (YTD) or year-over-year (YoY). You also learned that life-to-date (LTD) calculations can help you count new occurrences over your fact data, and that snapshot data can be filtered in a way to help guarantee that only a single snapshot value is returned.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Check your knowledge - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/4-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCheck your knowledge\n3 minutes\nAnswer the following questions to see what you've learned.\n1. \n\nIn the context of semantic model calculations, which statement best describes time intelligence?\n\n \n\nSnapshot balance reporting\n\nFilter context modifications involving a date table\n\nComplex calculations involving time\n\nCalculations involving hours, minutes, or seconds\n\n2. \n\nYou're developing a semantic model in Power BI Desktop. You've just added a date table by using the CALENDARAUTO function. You've extended it with calculated columns, and you've related it to other model tables. What else should you do to ensure that DAX time intelligence calculations work correctly?\n\n \n\nAdd time intelligence measures to the date table.\n\nMark as a Date table.\n\nAdd a fiscal hierarchy.\n\nAdd a date column.\n\n3. \n\nYou have a table that stores account balance snapshots for each date, excluding weekends. You need to ensure that your measure formula only filters by a single date. Also, if no record is on the last date of a time period, it should use the latest account balance. Which DAX time intelligence function should you use?\n\n \n\nFIRST\n\nFIRSTNONBLANK\n\nLAST\n\nLASTNONBLANK\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Additional time intelligence calculations - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3-calculations",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nAdditional time intelligence calculations\n5 minutes\n\nOther DAX time intelligence functions exist that are concerned with returning a single date. You'll learn about these functions by applying them in two different scenarios.\n\nThe FIRSTDATE and the LASTDATE DAX functions return the first and last date in the current filter context for the specified column of dates.\n\nCalculate new occurrences\n\nAnother use of time intelligence functions is to count new occurrences. The following example shows how you can calculate the number of new customers for a time period. A new customer is counted in the time period in which they made their first purchase.\n\nYour first task is to add the following measure to the Sales table that counts the number of distinct customers life-to-date (LTD). Life-to-date means from the beginning of time until the last date in filter context. Format the measure as a whole number by using the thousands separator.\n\nCustomers LTD =\nVAR CustomersLTD =\n    CALCULATE(\n        DISTINCTCOUNT(Sales[CustomerKey]),\n        DATESBETWEEN(\n            'Date'[Date],\n            BLANK(),\n            MAX('Date'[Date])\n        ),\n        'Sales Order'[Channel] = \"Internet\"\n    )\nRETURN\n    CustomersLTD\n\n\nAdd the Customers LTD measure to the matrix visual. Notice that it produces a result of distinct customers LTD until the end of each month.\n\nThe DATESBETWEEN function returns a table that contains a column of dates that begins with a given start date and continues until a given end date. When the start date is BLANK, it will use the first date in the date column. (Conversely, when the end date is BLANK, it will use the last date in the date column.) In this case, the end date is determined by the MAX function, which returns the last date in filter context. Therefore, if the month of August 2017 is in filter context, then the MAX function will return August 31, 2017 and the DATESBETWEEN function will return all dates through to August 31, 2017.\n\nNext, you will modify the measure by renaming it to New Customers and by adding a second variable to store the count of distinct customers before the time period in filter context. The RETURN clause now subtracts this value from LTD customers to produce a result, which is the number of new customers in the time period.\n\nNew Customers =\nVAR CustomersLTD =\n    CALCULATE(\n        DISTINCTCOUNT(Sales[CustomerKey]),\n        DATESBETWEEN(\n            'Date'[Date],\n            BLANK(),\n            MAX('Date'[Date])\n        ),\n    'Sales Order'[Channel] = \"Internet\"\n    )\nVAR CustomersPrior =\n    CALCULATE(\n        DISTINCTCOUNT(Sales[CustomerKey]),\n        DATESBETWEEN(\n            'Date'[Date],\n            BLANK(),\n            MIN('Date'[Date]) - 1\n        ),\n        'Sales Order'[Channel] = \"Internet\"\n    )\nRETURN\n    CustomersLTD - CustomersPrior\n\n\nFor the CustomersPrior variable, notice that the DATESBETWEEN function includes dates until the first date in filter context minus one. Because Microsoft Power BI internally stores dates as numbers, you can add or subtract numbers to shift a date.\n\nSnapshot calculations\n\nOccasionally, fact data is stored as snapshots in time. Common examples include inventory stock levels or account balances. A snapshot of values is loaded into the table on a periodic basis.\n\nWhen summarizing snapshot values (like inventory stock levels), you can summarize values across any dimension except date. Adding stock level counts across product categories produces a meaningful summary, but adding stock level counts across dates does not. Adding yesterday's stock level to today's stock level isn't a useful operation to perform (unless you want to average that result).\n\nWhen you are summarizing snapshot tables, measure formulas can rely on DAX time intelligence functions to enforce a single date filter.\n\nIn the following example, you will explore a scenario for the Adventure Works company. Switch to model view and select the Inventory model diagram.\n\nNotice that the diagram shows three tables: Product, Date, and Inventory. The Inventory table stores snapshots of unit balances for each date and product. Importantly, the table contains no missing dates and no duplicate entries for any product on the same date. Also, the last snapshot record is stored for the date of June 15, 2020.\n\nNow, switch to report view and select Page 2 of the report. Add the UnitsBalance column of the Inventory table to the matrix visual. Its default summarization is set to sum values.\n\nThis visual configuration is an example of how not to summarize a snapshot value. Adding daily snapshot balances together doesn't produce a meaningful result. Therefore, remove the UnitsBalance field from the matrix visual.\n\nNow, you'll add a measure to the Inventory table that sums the UnitsBalance value for a single date. The date will be the last date of each time period. It's achieved by using the LASTDATE function. Format the measure as a whole number with the thousands separator.\n\nStock on Hand =\nCALCULATE(\n    SUM(Inventory[UnitsBalance]),\n    LASTDATE('Date'[Date])\n)\n\n\n Note\n\nNotice that the measure formula uses the SUM function. An aggregate function must be used (measures don't allow direct references to columns), but given that only one row exists for each product for each date, the SUM function will only operate over a single row.\n\nAdd the Stock on Hand measure to the matrix visual. The value for each product is now based on the last recorded units balance for each month.\n\nThe measure returns BLANKs for June 2020 because no record exists for the last date in June. According to the data, it hasn't happened yet.\n\nFiltering by the last date in filter context has inherent problems: A recorded date might not exist because it hasn't yet happened, or perhaps because stock balances aren't recorded on weekends.\n\nYour next step is to adjust the measure formula to determine the last date that has a non-BLANK result and then filter by that date. You can achieve this task by using the LASTNONBLANK DAX function.\n\nUse the following measure definition to modify the Stock on Hand measure.\n\nStock on Hand =\nCALCULATE(\n    SUM(Inventory[UnitsBalance]),\n    LASTNONBLANK(\n        'Date'[Date],\n        CALCULATE(SUM(Inventory[UnitsBalance]))\n    )\n)\n\n\nIn the matrix visual, notice the values for June 2020 and the total (representing the entire year).\n\nThe LASTNONBLANK function is an iterator function. It returns the last date that produces a non-BLANK result. It achieves this result by iterating through all dates in filter context in descending chronological order. (Conversely, the FIRSTNONBLANK iterates in ascending chronological order.) For each date, it evaluates the passed in expression. When it encounters a non-BLANK result, the function returns the date. That date is then used to filter the CALCULATE function.\n\n Note\n\nThe LASTNONBLANK function evaluates its expression in row context. The CALCULATE function must be used to transition the row context to filter context to correctly evaluate the expression.\n\nYou should now hide the Inventory table UnitsBalance column. It will prevent report authors from inappropriately summarizing snapshot unit balances.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nTime intelligence relates to calculations over time. Specifically, it relates to calculations over dates, months, quarters, or years, and possibly time. Rarely would you need to calculate over time in the sense of hours, minutes, or seconds.\n\nIn Data Analysis Expressions (DAX) calculations, time intelligence means modifying the filter context for date filters.\n\nFor example, at the Adventure Works company, their financial year begins on July 1 and ends on June 30 of the following year. They produce a table visual that displays monthly revenue and year-to-date (YTD) revenue.\n\nThe filter context for 2017 August contains each of the 31 dates of August, which are stored in the Date table. However, the calculated year-to-date revenue for 2017 August applies a different filter context. It's the first date of the year through to the last date in filter context. In this example, that's July 1, 2017 through to August 31, 2017.\n\nTime intelligence calculations modify date filter contexts. They can help you answer these time-related questions:\n\nWhat's the accumulation of revenue for the year, quarter, or month?\nWhat revenue was produced for the same period last year?\nWhat growth in revenue has been achieved over the same period last year?\nHow many new customers made their first order in each month?\nWhat's the inventory stock on-hand value for the company's products?\n\nThis module describes how to create time intelligence measures to answer these questions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use DAX time intelligence functions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/2-functions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse DAX time intelligence functions\n5 minutes\n\nDAX includes several time intelligence functions to simplify the task of modifying date filter context. You could write many of these intelligence formulas by using a CALCULATE function that modifies date filters, but that would create more work.\n\n Note\n\nMany DAX time intelligence functions are concerned with standard date periods, specifically years, quarters, and months. If you have irregular time periods (for example, financial months that begin mid-way through the calendar month), or you need to work with weeks or time periods (hours, minutes, and so on), the DAX time intelligence functions won't be helpful. Instead, you'll need to use the CALCULATE function and pass in hand-crafted date or time filters.\n\nDate table requirement\n\nTo work with time intelligence DAX functions, you need to meet the prerequisite model requirement of having at least one date table in your model. A date table is a table that meets the following requirements:\n\nIt must have a column of data type Date (or date/time), known as the date column.\nThe date column must contain unique values.\nThe date column must not contain BLANKs.\nThe date column must not have any missing dates.\nThe date column must span full years. A year isn't necessarily a calendar year (January-December).\nThe date table must be indicated as a date table.\n\nFor more information, see Create date tables in Power BI Desktop.\n\nSummarizations over time\n\nOne group of DAX time intelligence functions is concerned with summarizations over time:\n\nDATESYTD - Returns a single-column table that contains dates for the year-to-date (YTD) in the current filter context. This group also includes the DATESMTD and DATESQTD DAX functions for month-to-date (MTD) and quarter-to-date (QTD). You can pass these functions as filters into the CALCULATE DAX function.\nTOTALYTD - Evaluates an expression for YTD in the current filter context. The equivalent QTD and MTD DAX functions of TOTALQTD and TOTALMTD are also included.\nDATESBETWEEN - Returns a table that contains a column of dates that begins with a given start date and continues until a given end date.\nDATESINPERIOD - Returns a table that contains a column of dates that begins with a given start date and continues for the specified number of intervals.\n\n Note\n\nWhile the TOTALYTD function is simple to use, you are limited to passing in one filter expression. If you need to apply multiple filter expressions, use the CALCULATE function and then pass the DATESYTD function in as one of the filter expressions.\n\nIn the following example, you will create your first time intelligence calculation that will use the TOTALYTD function. The syntax is as follows:\n\nTOTALYTD(<expression>, <dates>, [, <filter>][, <year_end_date>])\n\n\nThe function requires an expression and, as is common to all time intelligence functions, a reference to the date column of a marked date table. Optionally, a single filter expression or the year-end date can be passed in (required only when the year doesn't finish on December 31).\n\nDownload and open the Adventure Works DW 2020 M07.pbix file. Then, add the following measure definition to the Sales table that calculates YTD revenue. Format the measure as currency with two decimal places.\n\nRevenue YTD =\nTOTALYTD([Revenue], 'Date'[Date], \"6-30\")\n\n\nThe year-end date value of \"6-30\" represents June 30.\n\nOn Page 1 of the report, add the Revenue YTD measure to the matrix visual. Notice that it produces a summarization of the revenue amounts from the beginning of the year through to the filtered month.\n\nComparisons over time\n\nAnother group of DAX time intelligence functions is concerned with shifting time periods:\n\nDATEADD - Returns a table that contains a column of dates, shifted either forward or backward in time by the specified number of intervals from the dates in the current filter context.\nPARALLELPERIOD - Returns a table that contains a column of dates that represents a period that is parallel to the dates in the specified dates column, in the current filter context, with the dates shifted a number of intervals either forward in time or back in time.\nSAMEPERIODLASTYEAR - Returns a table that contains a column of dates that are shifted one year back in time from the dates in the specified dates column, in the current filter context.\nMany helper DAX functions for navigating backward or forward for specific time periods, all of which returns a table of dates. These helper functions include NEXTDAY, NEXTMONTH, NEXTQUARTER, NEXTYEAR, and PREVIOUSDAY, PREVIOUSMONTH, PREVIOUSQUARTER, and PREVIOUSYEAR.\n\nNow, you will add a measure to the Sales table that calculates revenue for the prior year by using the SAMEPERIODLASTYEAR function. Format the measure as currency with two decimal places.\n\nRevenue PY =\nVAR RevenuePriorYear = CALCULATE([Revenue], SAMEPERIODLASTYEAR('Date'[Date]))\nRETURN\n    RevenuePriorYear\n\n\nAdd the Revenue PY measure to the matrix visual. Notice that it produces results that are similar to the previous year's revenue amounts.\n\nNext, you will modify the measure by renaming it to Revenue YoY % and then updating the RETURN clause to calculate the change ratio. Be sure to change the format to a percentage with two decimals places.\n\nRevenue YoY % =\nVAR RevenuePriorYear = CALCULATE([Revenue], SAMEPERIODLASTYEAR('Date'[Date]))\nRETURN\n    DIVIDE(\n        [Revenue] - RevenuePriorYear,\n        RevenuePriorYear\n    )\n\n\nNotice that the Revenue YoY % measure produces a ratio of change factor over the previous year's monthly revenue. For example, July 2018 represents a 106.53 percent increase over the previous year's monthly revenue, and November 2018 represents a 24.22 percent decrease over the previous year's monthly revenue.\n\n Note\n\nThe Revenue YoY % measure demonstrates a good use of DAX variables. The measure improves the readability of the formula and allows you to unit test part of the measure logic (by returning the RevenuePriorYear variable value). Additionally, the measure is an optimal formula because it doesn't need to retrieve the prior year's revenue value twice. Having stored it once in a variable, the RETURN clause uses to the variable value twice.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use DAX time intelligence functions in Power BI Desktop models - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse DAX time intelligence functions in Power BI Desktop models\nModule\n6 Units\nFeedback\nIntermediate\nData Analyst\nApp Maker\nPower BI\nMicrosoft Power Platform\n\nBy the end of this module, you'll learn the meaning of time intelligence and how to add time intelligence DAX calculations to your model.\n\nLearning objectives\n\nBy the end of this module, you'll be able to:\n\nDefine time intelligence.\nUse common DAX time intelligence functions.\nCreate useful intelligence calculations.\nAdd\nPrerequisites\n\nYou should have experience creating Microsoft Power BI Desktop models and designing Power BI report layouts. You should also understand how to create Data Analysis Expressions (DAX) measures and how to work with iterator functions and filter context.\n\nIntroduction\nmin\nUse DAX time intelligence functions\nmin\nAdditional time intelligence calculations\nmin\nExercise - Create Advanced DAX Calculations in Power BI Desktop\nmin\nCheck your knowledge\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Work with model relationships - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5a-exercise-work-model-relationships?launch-lab=true",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Work with model relationships\n45 minutes\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nBy far, the most common model relationship is the one-to-many relationship. It fits with well with star schema design principals. Dimension tables are the “one” side, and your fact tables are the “many” side.\n\nOnce you gain mastery of model relationships and how to set them up, you’re on your way to develop more complex and efficient models. You can develop models with many-to-many relationships, and work with role-playing dimensions by activating inactive relationships.\n\nWhile you add relationships at model design time, model calculations can navigate relationships, modify relationship behavior, and even create virtual relationships.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Work with model relationships - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5a-exercise-work-model-relationships",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Create Power BI model relationships \nAdd\nPrevious\nUnit 6 of 8\nNext\nExercise: Work with model relationships\nCompleted\n100 XP\n45 minutes\n\nThis unit includes a lab to complete.\n\nUse the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.\n\nMicrosoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.\n\nSign in to launch the lab\n\n Note\n\nA virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.\n\nWhen you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nBelle is a data modeler at Adventure Works who is developing a model for a data warehouse. The model includes a table that stores products and another table that stores sales of those products. Belle adds a relationship to relate the two tables.\n\nWhich cardinality type should Belle set to achieve an optimal model?\n\n \n\nOne-to-one.\n\nOne-to-many.\n\nMany-to-many.\n\n2. \n\nAkira is a data modeler at Adventure Works who is developing a model for a data warehouse. The model includes a table that stores products and another table that stores sales monthly targets of product categories. Each product can belong to multiple categories and each category can have multiple products. Akira adds a relationship to relate the two tables.\n\nWhich cardinality type should Akira set to achieve an optimal model?\n\n \n\nOne-to-one.\n\nOne-to-many.\n\nMany-to-many.\n\n3. \n\nMargaret is a data modeler at Adventure Works who is adding a measure to sales model. When evaluated, the measure must filter by filters already applied to an unrelated table.\n\nWhich DAX function should Margaret use to create a virtual relationship?\n\n \n\nRELATEDTABLE.\n\nTREATAS.\n\nUSERELATIONSHIP\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand relationship evaluation - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5-understand-relationship-evaluation",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Create Power BI model relationships \nAdd\nPrevious\nUnit 5 of 8\nNext\nUnderstand relationship evaluation\nCompleted\n100 XP\n12 minutes\n\nModel relationships, from an evaluation perspective, are classified as either regular or limited. It's not a configurable relationship property. It’s inferred from the cardinality type and the data source of the two related tables. It's important to understand the evaluation type because there may be performance implications or consequences should data integrity be compromised. These implications and integrity consequences are described in this unit.\n\nFirst, some modeling theory is required to fully understand relationship evaluations.\n\nAn import or DirectQuery model sources all of its data from either the VertiPaq cache or the source database. In both instances, Power BI is able to determine that a \"one\" side of a relationship exists.\n\nA composite model, however, can comprise tables using different storage modes (import, DirectQuery, or dual), or multiple DirectQuery sources. Each source, including the VertiPaq cache of imported data, is considered to be a source group. Model relationships can then be classified as intra source group or inter/cross source group. An intra source group relationship is one that relates two tables within a source group, while an inter/cross source group relationship relates tables across two source groups. Relationships in import or DirectQuery models are always intra source group.\n\nHere's an example of a composite model.\n\nIn this example, the composite model consists of two source groups: a VertiPaq source group and a DirectQuery source group. The VertiPaq source group contains three tables, and the DirectQuery source group contains two tables. One cross source group relationship exists to relate a table in the VertiPaq source group to a table in the DirectQuery source group.\n\nRegular relationships\n\nA model relationship is regular when the query engine can determine the \"one\" side of relationship. It has confirmation that the \"one\" side column contains unique values. All one-to-many intra source group relationships are regular relationships.\n\nIn the following example, there are two regular relationships, both marked as R. Relationships include the one-to-many relationship contained within the VertiPaq source group, and the one-to-many relationship contained within the DirectQuery source group.\n\nFor import models, where all data is stored in the VertiPaq cache, Power BI creates a data structure for each regular relationship at data refresh time. The data structures consist of indexed mappings of all column-to-column values, and their purpose is to accelerate joining tables at query time.\n\nAt query time, regular relationships permit table expansion to happen. Table expansion results in the creation of a virtual table by including the native columns of the base table and then expanding into related tables. For import tables, table expansion is done in the query engine; for DirectQuery tables it’s done in the native query that’s sent to the source database (as long as the Assume referential integrity property isn't enabled). The query engine then acts upon the expanded table, applying filters and grouping by the values in the expanded table columns.\n\n Note\n\nInactive relationships are expanded also, even when the relationship isn't used by a calculation. Bi-directional relationships have no impact on table expansion.\n\nFor one-to-many relationships, table expansion takes place from the \"many\" to the \"one\" sides by using LEFT OUTER JOIN semantics. When a matching value from the \"many\" to the \"one\" side doesn't exist, a blank virtual row is added to the \"one\" side table. This behavior applies only to regular relationships, not to limited relationships.\n\nTable expansion also occurs for one-to-one intra source group relationships, but by using FULL OUTER JOIN semantics. This join type ensures that blank virtual rows are added on either side, when necessary.\n\nBlank virtual rows are effectively unknown members. Unknown members represent referential integrity violations where the \"many\" side value has no corresponding \"one\" side value. Ideally these blanks shouldn’t exist. They can be eliminated by cleansing or repairing the source data.\n\nHere’s how table expansion works with an animated example.\n\nIn this example, the model consists of three tables: Category, Product, and Sales. The Category table relates to the Product table with a one-to-many relationship, and the Product table relates to the Sales table with a one-to-many relationship. The Category table contains two rows, the Product table contains three rows, and the Sales tables contains five rows.\n\nThere are matching values on both sides of all relationships meaning that there aren’t any referential integrity violations. A query-time expanded table is revealed. The table consists of the columns from all three tables. It's effectively a denormalized perspective of the data contained in the three tables. A new row is added to the Sales table, and it has a production identifier value (9) that has no matching value in the Product table. It's a referential integrity violation. In the expanded table, the new row has (Blank) values for the Category and Product table columns.\n\nLimited relationships\n\nA model relationship is limited when there's no guaranteed \"one\" side. A limited relationship can happen for two reasons:\n\nThe relationship uses a many-to-many cardinality type (even if one or both columns contain unique values).\n\nThe relationship is cross source group (which can only ever be the case for composite models).\n\nIn the following example, there are two limited relationships, both marked as L. The two relationships include the many-to-many relationship contained within the VertiPaq source group, and the one-to-many cross source group relationship.\n\nFor import models, data structures are never created for limited relationships. In that case, Power BI resolves table joins at query time.\n\nTable expansion never occurs for limited relationships. Table joins are achieved by using INNER JOIN semantics, and for this reason, blank virtual rows aren’t added to compensate for referential integrity violations.\n\nThere are additional restrictions related to limited relationships:\n\nThe RELATED DAX function can't be used to retrieve the \"one\" side column values.\n\nEnforcing RLS has topology restrictions.\n\n Note\n\nIn Power BI Desktop model view, it's not always possible to determine whether a model relationship is regular or limited. A many-to-many relationship will always be limited, as will be a one-to-many relationship when it's a cross source group relationship. To determine whether it's a cross source group relationship, you'll need to inspect the table storage modes and data sources to arrive at the correct determination.\n\nPrecedence rules\n\nBi-directional relationships can introduce multiple, and therefore ambiguous, filter propagation paths between model tables. The following list presents precedence rules that Power BI uses for ambiguity detection and path resolution:\n\nMany-to-one and one-to-one relationships, including limited relationships\n\nMany-to-many relationships\n\nBi-directional relationships, in the reverse direction (that is, from the \"many\" side)\n\nPerformance preference\n\nThe following list orders filter propagation performance, from fastest to slowest performance:\n\nOne-to-many intra source group relationships\n\nMany-to-many model relationships achieved with an intermediary table and that involve at least one bi-directional relationship\n\nMany-to-many cardinality relationships\n\nCross source group relationships\n\nNext unit: Exercise: Work with model relationships\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use DAX relationship functions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/4-use-dax-relationship-functions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse DAX relationship functions\n3 minutes\n\nThere are several DAX functions that are relevant to model relationships.\n\nRELATED\n\nThe RELATED function retrieves the value from \"one\" side of a relationship. It’s useful when involving calculations from different tables that are evaluated in row context.\n\n Tip\n\nTo learn more about row context, work through the Add calculated tables and columns to Power BI Desktop models module. While this module describes adding model calculations, it includes a unit that introduces and describes row context.\n\nRELATEDTABLE\n\nThe RELATEDTABLE function retrieves a table of rows from the \"many\" side of a relationship.\n\nUSERELATIONSHIP\n\nThe USERELATIONSHIP function forces the use of a specific inactive model relationship. It’s useful when your model includes a role-playing dimension table, and you choose to create inactive relationships from this table.\n\nFor more information, see Active vs inactive relationship guidance.\n\nCROSSFILTER\n\nThe CROSSFILTER function either modifies the relationship cross filter direction (to one or both), or it disables filter propagation (none). It’s useful when you need to change or ignore model relationships during the evaluation of a specific calculation.\n\nCOMBINEVALUES\n\nThe COMBINEVALUES function joins two or more text strings into one text string. The purpose of this function is to support multi-column relationships in DirectQuery models when tables belong to the same source group.\n\nTREATAS\n\nThe TREATAS function applies the result of a table expression as filters to columns from an unrelated table. It’s helpful in advanced scenarios when you want to create a virtual relationship during the evaluation of a specific calculation.\n\nParent and child functions\n\nThe Parent and child functions are a family of related functions that you can use to generate calculated columns to naturalize a parent-child hierarchy. You can then use these columns to create a fixed-level hierarchy.\n\nFor more information, see Understanding functions for parent-child hierarchies in DAX.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Set up relationships - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/3-set-up-relationships",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSet up relationships\n14 minutes\n\nA model relationship relates one column in a table to one column in a different table. (There's one specialized case where this requirement isn't true, and it applies only to multi-column relationships in DirectQuery models. This topic is described in the next unit.)\n\n Note\n\nIt's not possible to relate a column to a different column in the same table. This concept is sometimes confused with the ability to define a relational database foreign key constraint that’s table self-referencing. You can use this relational database concept to store parent-child relationships (for example, each employee record is related to a \"reports to\" employee). However, you can’t use model relationships to generate a model hierarchy based on this type of relationship. To create a parent-child hierarchy, see Parent and Child functions.\n\nSet relationship cardinality\n\nEach model relationship is defined by a cardinality type. There are four cardinality type options, representing the data characteristics of the \"from\" and \"to\" related columns. The \"one\" side means the column contains unique values; the \"many\" side means the column can contain duplicate values.\n\n Note\n\nIf a data refresh operation attempts to load duplicate values into a \"one\" side column, the entire data refresh will fail.\n\nThe four options, together with their shorthand notations, are described in the following bulleted list:\n\nOne-to-many (1:*)\n\nMany-to-one (*:1)\n\nOne-to-one (1:1)\n\nMany-to-many (:)\n\nWhen you create a relationship in Power BI Desktop, the designer automatically detects and sets the cardinality type. Power BI Desktop queries the model to know which columns contain unique values. For import models, it uses internal storage statistics; for DirectQuery models, it sends profiling queries to the data source.\n\nSometimes, however, Power BI Desktop can get it wrong. It can get it wrong when tables are yet to be loaded with data, or because columns that you expect to contain duplicate values currently contain unique values. In either case, you can update the cardinality type as long as any \"one\" side columns contain unique values (or the table is yet to be loaded with rows of data).\n\nOne-to-many (and many-to-one) cardinality\n\nThe one-to-many and many-to-one cardinality options are the same, and they're also the most common cardinality types.\n\nWhen configuring a one-to-many or many-to-one relationship, you'll choose the one that matches the order in which you related the columns. Consider how you would configure the relationship from the Product table to the Sales table by using the ProductID column found in each table. The cardinality type would be one-to-many, as the ProductID column in the Product table contains unique values. If you related the tables in the reverse direction, Sales to Product, then the cardinality would be many-to-one.\n\nOne-to-one cardinality\n\nA one-to-one relationship means both columns contain unique values. This cardinality type isn't common, and it likely represents a suboptimal model design because of the storage of redundant data. It’s often a better idea to use Power Query to consolidate the two tables into one.\n\nFor more information on using this cardinality type, see One-to-one relationship guidance.\n\nMany-to-many cardinality\n\nA many-to-many relationship means both columns can contain duplicate values. This cardinality type is infrequently used. It's typically useful when designing complex model requirements. You can use it to relate many-to-many facts or to relate higher grain facts. For example, when sales target facts are stored at product category level and the product dimension table is stored at product level.\n\nFor guidance on using this cardinality type, see Many-to-many relationship guidance.\n\n Tip\n\nIn Power BI Desktop model view, you can interpret a relationship's cardinality type by looking at the indicators (1 or *) on either side of the relationship line. To determine which columns are related, you'll need to select, or hover the cursor over, the relationship line to highlight the related columns.\n\nSet cross filter direction\n\nEach model relationship is defined with a cross filter direction. Your setting determines the direction(s) that filters will propagate. The possible cross filter options are dependent on the cardinality type.\n\nExpand table\nCardinality type\tCross filter options\nOne-to-many (or many-to-one)\tSingle\nBoth\nOne-to-one\tBoth\nMany-to-many\tSingle (Table1 to Table2)\nSingle (Table2 to Table1)\nBoth\n\nSingle cross filter direction means \"single direction\", and Both means \"both directions\". A relationship that filters in both directions is commonly described as bi-directional.\n\nFor one-to-many relationships, the cross filter direction is always from the \"one\" side, and optionally from the \"many\" side (bi-directional). For one-to-one relationships, the cross filter direction is always from both tables. Lastly, for many-to-many relationships, cross filter direction can be from either one of the tables, or from both tables. Notice that when the cardinality type includes a \"one\" side, that filters will always propagate from that side.\n\nWhen the cross filter direction is set to Both, an additional property becomes available. It can apply bi-directional filtering when Power BI enforces row-level security (RLS) rules. For more information about RLS, see the Row-level security (RLS) with Power BI Desktop article.\n\nYou can modify the relationship cross filter direction, including the disabling of filter propagation, by using a model calculation. It's achieved by using the CROSSFILTER DAX function, which is described in Unit 3.\n\nBear in mind that bi-directional relationships can impact negatively on performance. Further, attempting to configure a bi-directional relationship could result in ambiguous filter propagation paths. In this case, Power BI Desktop may fail to commit the relationship change and will alert you with an error message. Sometimes, however, Power BI Desktop may allow you to define ambiguous relationship paths between tables. Precedence rules that affect ambiguity detection and path resolution are described in Unit 4.\n\nWe recommend using bi-directional filtering only as needed. For more information, see Bi-directional relationship guidance.\n\n Tip\n\nIn Power BI Desktop model view, you can interpret a relationship's cross filter direction by noticing the arrowhead(s) along the relationship line. A single arrowhead represents a single-direction filter in the direction of the arrowhead; a double arrowhead represents a bi-directional relationship.\n\nSet active vs inactive relationships\n\nThere can only be one active filter propagation path between two model tables. However, it's possible to introduce additional relationship paths, though you must set these relationships as inactive. Inactive relationships can only be made active during the evaluation of a model calculation. It’s achieved by using the USERELATIONSHIP DAX function, which is described in Unit 3.\n\nGenerally, we recommend defining active relationships whenever possible. They widen the scope and potential of how report authors can use your model. Using only active relationships means that role-playing dimension tables should be duplicated in your model.\n\nIn specific circumstances, however, you can define one or more inactive relationships for a role-playing dimension table. You can consider this design when:\n\nThere's no requirement for report visuals to simultaneously filter by different roles.\n\nYou use the USERELATIONSHIP DAX function to activate a specific relationship for relevant model calculations.\n\nFor more information, see Active vs inactive relationship guidance.\n\n Tip\n\nIn Power BI Desktop model view, you can interpret a relationship's active vs inactive status. An active relationship is represented by a solid line; an inactive relationship is represented as a dashed line.\n\nSet assume referential integrity\n\nThe Assume referential integrity property is available only for one-to-many and one-to-one relationships between two DirectQuery storage mode tables that belong to the same source group. You can only enable this property when the “many” side column doesn’t contain NULLs.\n\nWhen enabled, native queries sent to the data source will join the two tables together by using an INNER JOIN rather than an OUTER JOIN. Generally, enabling this property improves query performance, though it does depend on the specifics of the data source.\n\nAlways enable this property when a single-column database foreign key constraint exists between the two tables. Even when a foreign key constraint doesn't exist, consider enabling the property as long as you're certain data integrity exists.\n\n Important\n\nIf data integrity should become compromised, the inner join will eliminate unmatched rows between the tables. For example, consider a model Sales table with a ProductID column value that didn’t exist in the related Product table. Filter propagation from the Product table to the Sales table will eliminate sales rows for unknown products. This would result in an understatement of the sales results.\n\nFor more information, see Assume referential integrity settings in Power BI Desktop.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand model relationships - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/2-understand-model-relationships",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Create Power BI model relationships \nAdd\nPrevious\nUnit 2 of 8\nNext\nUnderstand model relationships\nCompleted\n100 XP\n6 minutes\n\nA model relationship propagates filters applied on the column of one model table to a different model table. Filters will propagate so long as there's a relationship path to follow, which can involve propagation to multiple tables.\n\nRelationship paths are deterministic, meaning that filters are always propagated in the same way and without random variation. Relationships can, however, be disabled, or have filter context modified by model calculations that use particular DAX functions. Use DAX relationship functions are described in Unit 2.\n\n Important\n\nModel relationships don’t enforce data integrity. Unit 4, which describes relationship evaluation, explains how model relationships behave when there are data integrity issues with your data.\n\nHere's how relationships propagate filters with an animated example.\n\nIn this example, the model consists of four tables: Category, Product, Year, and Sales. The Category table relates to the Product table, and the Product table relates to the Sales table. The Year table also relates to the Sales table. All relationships are one-to-many (the details of which are described in the next unit).\n\nA query, possibly generated by a Power BI card visual, requests the total sales quantity for sales orders made for a single category, Cat-A, and for a single year, CY2018. It's why you can see filters applied on the Category and Year tables. The filter on the Category table propagates to the Product table to isolate two products that are assigned to the category Cat-A. Then the Product table filters propagate to the Sales table to isolate just two sales rows for these products. These two sales rows represent the sales of products assigned to category Cat-A. Their combined quantity is 14 units. At the same time, the Year table filter propagates to further filter the Sales table, resulting in just the one sales row that is for products assigned to category Cat-A and that was ordered in year CY2018. The quantity value returned by the query is 11 units. Note that when multiple filters are applied to a table (like the Sales table in this example), it's always an AND operation, requiring that all conditions must be true.\n\nApply star schema design principles\n\nWe recommend you apply star schema design principles to produce a model comprising dimension and fact tables. It’s common to set up Power BI to enforce rules that filter dimension tables, allowing model relationships to efficiently propagate those filters to fact tables.\n\nThe following image is the model diagram of the Adventure Works sales analysis data model. It shows a star schema design comprising a single fact table named Sales. The other four tables are dimension tables that support the analysis of sales measures by date, state, region, and product. Notice the model relationships connecting all tables. These relationships propagate filters (directly or indirectly) to the Sales table.\n\nUse disconnected tables\n\nIt's unusual that a model table isn't related to another model table. Such a table in a valid model design is described as a disconnected table. A disconnected table isn't intended to propagate filters to other model tables. Instead, it accepts \"user input\" (perhaps with a slicer visual), allowing model calculations to use the input value in a meaningful way. For example, consider a disconnected table that’s loaded with a range of currency exchange rate values. As long as a filter is applied to filter by a single rate value, a measure expression can use that value to convert sales values.\n\nThe Power BI Desktop what-if parameter is a feature that creates a disconnected table. For more information, see Create and use a What if parameter to visualize variables in Power BI Desktop.\n\nNext unit: Set up relationships\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nTogether with model tables, model relationships form the basis of a tabular model. Relationships are responsible for the propagation of filters to other model tables. When correctly set up, they silently work to provide a natural and intuitive querying experience.\n\nThere’s a lot to learn about relationships, and it can take new developers time to understand how to set them up correctly, especially for complex data relationships. There are also Data Analysis Expressions (DAX) functions that work with model relationships. These functions allow your model calculation to navigate relationships, modify relationship behavior, and even create virtual relationships.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nUnderstand how model relationship work.\nSet up relationships.\nUse DAX relationship functions.\nUnderstand relationship evaluation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create Power BI model relationships - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate Power BI model relationships\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nPower BI model relationships form the basis of a tabular model. Define Power BI model relationships, set up relationships, recognize DAX relationship functions, and describe relationship evaluation.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nUnderstand how model relationship work.\nSet up relationships.\nUse DAX relationship functions.\nUnderstand relationship evaluation.\nAdd\nPrerequisites\nExperience developing Power BI data models by using Power BI Desktop.\nIntroduction\nmin\nUnderstand model relationships\nmin\nSet up relationships\nmin\nUse DAX relationship functions\nmin\nUnderstand relationship evaluation\nmin\nExercise: Work with model relationships\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nPower BI dataflows are an enterprise-focused data prep solution, enabling an ecosystem of data that's ready for consumption, reuse, and integration. Power BI dataflows work well for analysts who want to cleanse and transform data one time in Power Query online for reuse in other reports and by other analysts. They're also a great solution to reduce the load placed on source systems to extract data.\n\nLearn more\nAzure Data Factory wrangling overview\nUnderstanding the differences between dataflow types\nDeveloping dataflows solutions\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nIvor is working on an analytics solution that requires ETL logic to be applied to a table that sits outside of the data warehouse. Multiple analysts need to use this supplemental cleaned data in their reports, and it's important that all reports reflect the exact version of the cleansed data. Analysts don't have access to the data source. What should Ivor do to provide analysts with this data at the lowest access level necessary?\n\n \n\nCreate a Power BI dataset in a shared workspace and give all analysts contributor access.\n\nCreate a Power BI dataflow in a shared workspace and give all analysts viewer access.\n\nCreate a Power BI dataflow in a shared workspace and give all analysts admin access.\n\n2. \n\nSherry is using a dataflow in Power BI Premium to refresh data from a web API. The volume of data is large and Sherry is using incremental refresh. Sherry notices that the dataflow refresh is suddenly taking much longer than it used to. Where can she check dataflow performance and investigate what might be going on?\n\n \n\nThe refresh history in the dataflow settings.\n\nThe performance analyzer in Power BI desktop.\n\nThe performance profiler in Visual Studio.\n\n3. \n\nJuliane is creating a dataset to be used in multiple reports. To avoid duplicating logic, she wants to create a dataflow that references an existing dataflow. What requirement does Juliane need to confirm prior to creating her dataflow that contains the linked entity?\n\n \n\nJuliane must have access to both of the underlying data sources.\n\nThe workspace where the dataflow resides must be in Premium capacity.\n\nBoth dataflows must reside in the same workspace.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Create a dataflow - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/5-exercise-create-dataflow",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Create a dataflow\n45 minutes\n\nNow it's your opportunity to create a dataflow yourself.\n\nIn this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription. You'll then create a dataflow to deliver date dimension data sourced from the Azure Synapse Adventure Works data warehouse. The dataflow will provide a consistent definition of date-related data for use by the organization's business analysts.\n\n Note\n\nTo complete this lab, you will need both an Azure subscription in which you have administrative access and a Power BI account. If you need a free trial Power BI account, sign up and follow the steps to create an account before continuing with this lab.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create reusable assets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/3-create-reusable-assets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Prepare data for tabular models in Power BI  Create and manage scalable Power BI dataflows \nAdd\nPrevious\nUnit 3 of 7\nNext\nCreate reusable assets\nCompleted\n100 XP\n5 minutes\n\nDataflows are created and refreshed in the Power BI service, and can be consumed in either Power BI desktop or the Power BI service.\n\nCreate a dataflow\n\nThere are multiple methods of creating dataflows in the Power BI service. We're going to cover creating a dataflow using the option to define new tables.\n\nYou can also create dataflows using linked tables, a computed table, or using import/export. Refer to the Power BI documentation for step-by-step instructions of each creation option.\n\nFrom within a shared workspace, select New Dataflow.\n\nHere you'll have the option to define new tables, link tables from other dataflows, import model, or attach a Common Data Model folder. Select Add new tables.\n\nSelecting Add new tables will direct you to Power Query online, where you'll choose a data source.\n\nDepending on the data source you select, you'll need to enter connection settings and specify connection credentials. Specifying connection credentials will look similar to data connections using Power Query in Power BI desktop.\n\nEnter the appropriate connection settings and credentials and select Sign In.\n\nAfter signing in, you'll get a preview of the assets in the source data system. Here you can select which tables to use. Dataflows contain tables, but don't contain relationships.\n\nOnce you select the data you'd like to use, you can use the dataflow editor to shape and transform that data. The dataflow editor looks and behaves similar to Power Query in Power BI desktop.\n\n Important\n\nDataflows can only be created in shared workspaces.\n\nRefresh a dataflow\n\nWhen you create a dataflow, you're prompted to refresh the data for the dataflow. Refreshing a dataflow is required before it can be consumed in a dataset inside Power BI Desktop, or referenced as a linked or computed table.\n\nTo configure a dataflow refresh from the shared workspace, navigate to Settings via the More options menu.\n\nHere you can take ownership of a dataflow, edit the data source credentials, schedule a refresh, configure enhanced compute engine settings, and endorse content.\n\nConnect to a dataflow\n\nDataflows can be consumed in three ways. Report builders can:\n\nCreate a linked table from the dataflow.\nCreate a dataset from the dataflow.\nCreate a connection from external tools that can read from the common data model format.\n\nYou can connect to a dataflow in Power BI desktop using the Power BI dataflows connector in the Get Data window.\n\n Note\n\nLearn more about configuring and consuming dataflows.\n\nRefining dataflow settings\n\nFor dataflows in workspaces using Power BI Premium capacity, you can use the Admin portal to change, or refine, how dataflows are created and how they use resources in your Power BI Premium subscription. See refining dataflow settings to learn more.\n\nNext unit: Implement best practices\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement best practices - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/4-implement-best-practices",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nImplement best practices\n3 minutes\n\nDataflows can solve problems, but they can also create problems when implemented suboptimally.\n\nBest practices\n\nKeep the following best practices in mind when planning your dataflow implementation:\n\nBring in only data you need.\nLeverage query folding.\nEndorse your dataflows as either promoted or certified to encourage use.\nUse incremental refresh to control partition processing.\nUse Power Automate for trigger-based dataflow and dataset refresh.\nReview and optimize dataflow refresh using refresh history and the CSV log.\nUse dataflows in Power BI Premium to take advantage of:\nEnhanced compute engine.\nDirectQuery.\nComputed entities.\nLinked entities.\nIncremental refresh.\nSplit things into multiple dataflows and reuse dataflows cross multiple workspaces.\n\n Tip\n\nFor more detailed information, see Dataflow best practices.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Define use cases for dataflows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/2-define-use-cases-for-dataflows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDefine use cases for dataflows\n9 minutes\n\nPower BI dataflows enable you to build reusable data tables in a workspace using Power Query Online, and share them for use in other reports and with other users for reuse in other workspaces. Dataflows are objects in a workspace alongside datasets, dashboards, reports, and workbooks. When a Power BI dataflow is refreshed, behind the scenes it loads its data into files located in a data lake, Azure Data Lake Storage Gen 2 (ADLS Gen 2).\n\nPower BI dataflows should be used in Premium capacity for enterprise solutions, to take advantage of features like advanced compute, incremental refresh, and linked and computed entities.\n\n Note\n\nDataflows are supported for Power BI Pro, Premium Per User (PPU), and Power BI Premium users. Learn more about Premium-only features of dataflows.\n\nData used with Power BI is stored in internal storage provided by Power BI by default. With the integration of dataflows and Azure Data Lake Storage Gen 2 (ADLS Gen2), you can store your dataflows in your organization's Azure Data Lake Storage Gen2 account. This essentially allows you to \"bring your own storage\" to Power BI dataflows, and establish a connection at the tenant or workspace level.\n\nWhy use dataflows?\n\nDataflows were designed to promote reusable ETL logic that prevents the need to create additional connections to your data source.\n\nDataflows are a great choice for you if:\n\nThere's no data warehouse in your organization.\nYou want to extend a core dataset or data in the data warehouse with consistent data.\nSelf service users need frequent access to an up-to-date subset of data from the data warehouse without having access to the data warehouse itself.\nYou have slower data sources.\nDataflows extract data once and reuse it multiple times, which can reduce the overall data refresh time for slower data sources.\nComputed entities may be faster than referencing queries with the enhanced compute engine.\nYou have chargeable data sources.\nDataflows can reduce costs associated with data refresh if you're getting data from chargeable data sources.\nDataflows increase control and reduce the number of calls to the source system.\nDatasets refresh against dataflows without affecting source systems.\nYou have different versions of datasets floating around your organization. Dataflows increase consistency between datasets.\nIncreased structural consistency by reducing the chance that users will prepare data differently\nIncreased temporal consistency by having a single set of data extracted from source systems at a single point in time\nShared tables that have no source, such as a standard date dimension, can be standardized across your organization.\nYou want to reduce or hide the complexity of data sources.\nYou can expose common data entities for larger groups of analysts that have already been transformed and simplified.\nYou can also partition data horizontally, using multiple data flows. For example, upstream dataflows contain all data and are available only to a small group of users. Downstream dataflows then contain curated subsets of data, and can be made available to members of appropriate security groups.\nBenefits and limitations\n\nWhile there are notable benefits to using dataflows in your dataset design, there are also a few limitations that users should keep in mind.\n\nBenefits:\n\nReduced load on database queries.\nReduced number of users accessing source data.\nProvides single version of properly structured data for analysts to build reports from.\n\nLimitations:\n\nNot a replacement for a data warehouse.\nRow-level security isn't supported.\nIf not using dataflows in Premium capacity, performance can be an issue.\n\n Important\n\nSee Dataflow considerations and limitations for a complete list of considerations and limitations.\n\nDataflows in Power BI Premium\n\nPower BI Premium was designed for enterprise deployments. Dataflow features available in premium offer substantial performance benefits and include the use of:\n\nEnhanced compute engine\nDirectQuery\nComputed entities\nLinked entities\nIncremental refresh\nOptimize dataflows using the enhanced compute engine\n\nThe enhanced compute engine in Power BI dataflows enables you to optimize the use of dataflows by:\n\nSpeeding up refresh operations when computed entities or linked entities are involved (for example, performing joins, distinct, filters, and group by).\nEnabling DirectQuery connectivity over dataflows using the compute engine.\nAchieve improved performance in the transformation steps of dataflows when entities are cached within the compute engine.\n\n Tip\n\nLearn more about Power BI Premium features of dataflows.\n\nDistinction between dataflows\n\nPerhaps you've also heard of Azure Data Factory dataflows and you're wondering what the best type of dataflow is to use in your scenario.\n\nPower BI dataflows and Azure Data Factory (ADF) wrangling dataflows are often considered to do the same thing: extract data from source systems, transform data, and load the transformed data into a destination. They're both powered by Power Query online, but there are differences in these two types of dataflows. You can implement a solution that works with a combination of the two.\n\nWhen to use ADF wrangling dataflows or Power BI dataflows\n\nData transformation should always be done as close to the source as possible. If your analytics solution includes Azure Data Factory and you have the skills to implement transformations upstream of Power BI, you should.\n\nExpand table\nFeatures\tPower BI dataflows\tData Factory wrangling dataflows\nDestinations\tDataverse or Azure Data Lake Storage\tMany destinations\nPower Query transformation\tAll Power Query functions are supported\tA limited set of functions is supported\nSources\tMany sources are supported\tOnly a few sources\nScalability\tDepends on the Premium capacity and the use of the enhanced compute engine\tHighly scalable\n\n Tip\n\nLearn more about how Microsoft Power Platform dataflows and Azure Data Factory wrangling dataflows relate to each other.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nDataflows in Power BI enable standardized data preparation at a level in between the data source and Power BI reports. They're a great option in both self-service and enterprise analytics, enabling analysts to build reports on a single version of transformed data.\n\nThis module introduces Power BI dataflows, their use cases, and best practices in dataflow implementation. You'll also practice building a dataflow for use in a Power BI report.\n\nLearning objectives\n\nIn this module, you will:\n\nDescribe Power BI dataflows and use cases.\nDescribe best practices for implementing Power BI dataflows.\nCreate and consume Power BI dataflows.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create and manage scalable Power BI dataflows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate and manage scalable Power BI dataflows\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nCreate Power BI transformation logic for reuse across your organization with Power BI dataflows. Learn how to combine Power BI dataflows with Power BI Premium for scalable ETL, and practice creating and consuming dataflows.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI dataflows and use cases.\nDescribe best practices for implementing Power BI dataflows.\nCreate and consume Power BI dataflows.\nAdd\nPrerequisites\nYou'll need knowledge of Power BI data model design including star schema design basics.\nConsider completing the Model data in Power BI learning path.\nIntroduction\nmin\nDefine use cases for dataflows\nmin\nCreate reusable assets\nmin\nImplement best practices\nmin\nExercise: Create a dataflow\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n2 minutes\n\nYou've learned that scalability and working with large data is achievable in Power BI. The correct model framework, a proper data model, and using Power BI Premium features like large dataset storage enables performant enterprise reporting in Power BI.\n\nLearn more\nPower BI enterprise documentation\nPower BI implementation planning: Advanced data model management\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat is the most efficient approach for data transformation when designing data models for scalability?\n\n \n\nData transformation should always be done using Power Query in Power BI.\n\nData transformation should be done as close to the source as possible, before reaching Power BI.\n\nData transformation should be done using DAX, after data is ingested and loaded to the model.\n\n2. \n\nWhich of the following best practices for Power BI data modeling is relevant only to DirectQuery models?\n\n \n\nSet relationships to enforce integrity using the assume referential integrity property on relationships.\n\nUse a star schema as opposed to wide tables.\n\nDisable auto date/time in Power BI Desktop.\n\n3. \n\nWhat are the data model size limits on a dataset with large dataset storage format determined by?\n\n \n\nPower BI Premium 10 GB size limit.\n\nPower BI Premium capacity size or the maximum size set by the administrator.\n\nThere are no size limits if large dataset storage format is enabled.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure large datasets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4-configure-large-datasets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure large datasets\n4 minutes\n\nPower BI datasets store data in a highly compressed, in-memory cache for optimized query performance. Enterprise deployment of an analytics solution using Power BI will likely require Power BI Premium. With the large dataset storage format enabled, dataset sizes are limited only by the capacity size, or a maximum size set by the administrator. This differs from datasets in Power BI Premium, which are limited to 10 GB after compression if large dataset storage format isn't enabled.\n\nLarge datasets can be enabled for all Premium P SKUs, Embedded A SKUs, and with Premium Per User (PPU). The large dataset size limit in Premium is comparable to Azure Analysis Services, in terms of data model size limitations.\n\nThe large dataset feature brings the Power BI dataset cache sizes to parity with Azure Analysis Services model sizes. The large dataset feature enables consolidation of tabular models from SQL Server Analysis Services and Azure Analysis Services on one common platform based on Power BI Premium.\n\n Note\n\nTo use large dataset storage format, the dataset must be stored in a workspace that allocated to Premium capacity.\n\nEnabling the large dataset format enables fast user interactivity and allows data to grow beyond the 10-GB limit. Additionally, the large dataset format can also improve xmla write operation performance, even for datasets that may not be large.\n\n Important\n\nDatasets enabled for large models can't be downloaded as a Power BI Desktop (.pbix) file from the Power BI service. Read more about .pbix download limitations.\n\nEnable large dataset storage format\n\nTo take advantage of the large dataset storage format option, it must be enabled in the Power BI service. Here you can enable large dataset storage format for a single dataset, or for all datasets created in a workspace.\n\nEnable large dataset storage format for a single dataset\n\nIn the dataset settings in the Power BI service, toggle the slider to on and select Apply.\n\nEnable large dataset storage format for all datasets created in a workspace\n\nYou can set the default storage format for all datasets created in a workspace in the workspace settings. In the settings, select Premium, and select Large dataset storage format as the Default storage format.\n\nLarge dataset storage format for a workspace can also be enabled using PowerShell.\n\n Note\n\nSee Configure large datasets to learn more about large models in Power BI Premium including information on checking dataset size, dataset eviction, considerations, and limitations.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Create a star schema model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4a-exercise-create-star-schema-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Create a star schema model\n45 minutes\n\nNow it's your opportunity to try creating a star schema in Power BI yourself, connecting to an Azure Synapse Analytics dedicated SQL pool. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription, develop a dataset, and create a star schema model in Power BI desktop.\n\n Note\n\nTo complete this lab, you will need both an Azure subscription in which you have administrative access and a Power BI account. If you need a free trial Power BI account, sign up and follow the steps to create an account before continuing with this lab. Launch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement Power BI data modeling best practices - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/3-implement-data-modeling-best-practices",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nImplement Power BI data modeling best practices\n7 minutes\n\nImplementing data modeling best practices is key to performant, scalable data models.\n\nChoose the correct Power BI model framework\n\nChoosing the correct Power BI model framework is at the heart of building any scalable solution.\n\nThe first place to start with your Power BI data model is import mode. Import mode offers you the most options, design flexibility, and delivers fast performance.\n\nUse DirectQuery when your data source stores large volumes of data and/or your report needs to deliver near real-time data.\n\nFinally, use a composite model when you need to:\n\nBoost the query performance of a DirectQuery model.\nDeliver near real-time query results from an import model.\nExtend a Power BI dataset (or Azure Analysis Services model) with other data.\n\nComposite models combine data from more than one DirectQuery source or combine DirectQuery with import data.\n\n Important\n\nReview the Choose a Power BI model framework module for more information on using import, DirectQuery, or composite models.\n\nImplement data modeling best practices\n\nThere are some basic principles to abide by when building any data model. These principles become even more important as data begins to grow.\n\nMost importantly, you want to do as much data preparation work as possible before data reaches Power BI, as far upstream as possible. For example, if you have the opportunity to transform data in the data warehouse, that's where it should be done. Transformation at the source produces consistency for any other solutions built on that data and ensures that your Power BI model doesn't need to do any extra processing. This may require working with your data engineer or other members of the data team and is critically important.\n\nBest practices for import mode:\nAlways start with import mode if you can.\nOnly bring in data you need.\nRemove unnecessary rows and columns.\nOnly process what is absolutely necessary (tables/partitions) given the business requirements.\nAvoid wide tables.\nUse a star schema in Power BI.\nIf your source is a beautifully modeled data warehouse, you're a step ahead.\nBig data is often in wide flat tables. Take advantage of dimensional models for their performance benefits.\nPower BI supports multiple fact tables with different dimensionality and different granularities – you don’t have to put everything into one large table.\nPre-aggregate data before loading it to the model where possible.\nReduce the usage of calculated columns.\nData transformations requiring additional columns should be done as close to the source as possible.\nAvoid high cardinality columns.\nConsider breaking a datetime column into two columns, one for date and one for time.\nUse appropriate data types.\nUse integers instead of strings for ID columns.\nUse surrogate keys for ID columns if necessary.\nLimit the use of bi-directional filters on relationships.\nDisable auto date/time.\nConnect to a date table at the source or create your own date table.\nDisable attribute hierarchies for non-attribute columns.\nIf querying a relational database, query database views rather than tables.\nA view provides an abstraction layer to manage columns, and relates back to the first consideration, pushing transformations as close to the source as possible.\nViews shouldn't contain logic. They should only contain a SELECT statement from a table.\nConsider partitioning and incremental refresh to avoid loading data you don’t need to.\nCheck to ensure query folding is achieved.\nIf query folding isn't possible, you have another opportunity to work with the data engineer to move transformation upstream.\n\n Tip\n\nLearn more about techniques to help reduce the data loaded into import models.\n\nBest practices specific to DirectQuery mode:\nSet relationships to enforce integrity using the Assume referential integrity property on relationships.\nThe Assume Referential Integrity setting on relationships enables queries to use INNER JOIN statements rather than OUTER JOIN.\nLimit the use of bi-directional filters on relationships.\nUse only when necessary.\nLimit the complexity of DAX calculations.\nBecause query folding occurs by default in DirectQuery, more complex DAX measures means added complexity at the source, leading to slow queries.\nThe need for complex DAX also leads back to the key principle of applying transformations as far upstream as possible. You may need to work with the data engineer to apply transformations at the source.\nAvoid the use of calculated columns.\nTransformations requiring additional columns should be done as far upstream as possible, particularly when using DirectQuery.\nAvoid relationships on calculated columns\nAvoid relationships on Unique Identifier columns\nUse dual storage mode for dimensions related to fact tables that are in DirectQuery.\n\n Note\n\nRefer to the DirectQuery model guidance for a complete list of considerations in developing DirectQuery models.\n\nThere's also a tool you can use as you're developing tabular models that will alert you of modeling missteps or changes that would improve model design and performance. The Best Practice Analyzer within Tabular Editor was designed to help you design models that adhere to modeling best practices.\n\nIn the next unit, you'll learn how to configure the large dataset storage format using Power BI Premium.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nPerhaps you've heard that Power BI can seamlessly handle trillions of rows of data, but you're not able to accomplish that in your own Power BI tenant. Working with high volume and large-scale data can be done in Power BI with the right groundwork in place.\n\nThis module introduces considerations for building enterprise scale, IT-driven solutions. You'll review data modeling best practices and Power BI Premium features for working with large data.\n\nLearning objectives\n\nIn this module, you will:\n\nDescribe the importance of building scalable data models\nImplement Power BI data modeling best practices\nUse the Power BI large dataset storage format\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe the significance of scalable models - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/2-describe-significance-of-scalable-models",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe the significance of scalable models\n4 minutes\n\nOne of the keys to manageable, performant solutions with large data is good model design. We'll discuss model scalability, why it's important, and what tools exist in Power BI to help you and your team accomplish your goals.\n\nWhat is enterprise or large-scale data?\n\nBefore we talk through scalability, let's define what we're talking about. You'll see throughout the module that we refer to enterprise-scale or large-scale data rather than big data. In this module, enterprise-scale or large-scale data refers to tables with a large number of records or rows. Power BI, used with tools like Azure Synapse Analytics, can analyze massive datasets, in the range of trillions of rows or petabytes of data.\n\nIf you're familiar with working with enterprise data, it may be helpful to understand that Power BI is the next generation of Analysis Services. It's the same technology under the hood of Analysis Services and Power BI datasets, the VertiPaq engine.\n\n Tip\n\nTake a look at the Model, query, and explore data in Azure Synapse learning path for more information on data analytics in Azure.\n\nWhat is scalability and why is it important?\n\nScalability in this context refers to building data models that can handle growth in the volume of data. A data model that ingests thousands of rows of data may grow to millions of rows over time, and the model must be designed to accommodate such growth. It's important to consider that your data will grow and/or change, which increases complexity.\n\nScalability must be at the forefront in enterprise solutions to ensure:\n\nFlexibility - models need to be able to accommodate change\nData growth - models must be able to handle an increase in data volume with acceptable report performance\nReduced complexity - models built with scalability in mind will be less complex and easier to manage\nHow do I design for scalability?\n\nThe best approach to building scalable Power BI data models will always be building with data modeling best practices in mind.\n\nBeyond the data model, Power BI Premium was designed specifically for enterprise deployments. Premium capacity offers greater storage capacity and allows for larger individual datasets depending on the SKU. Implementing the premium only large dataset storage feature enables data to grow beyond the Power BI desktop (.pbix) file size limitations.\n\n Tip\n\nAre you planning a Power BI enterprise deployment? Read the Power BI enterprise deployment whitepaper for a full list of enterprise deployment considerations.\n\nAnother important consideration in designing for scalability using Power BI Premium is choosing the right capacity. You'll need to work with your Power BI administrator to determine which Power BI Premium licensing SKU is available to you. If you're having performance issues in Premium capacity, work first to optimize your model, and then work with your Power BI administrator to monitor Power BI Premium capacities.\n\nAt the most basic level, it's important to understand that Premium capacities require sufficient memory for processing. You'll need to double the amount of RAM to process your data model refresh. For example, if you have a 40-GB dataset, you'll need at least 80-GB of memory available. A 40-GB dataset would be best supported by a P3/A6 capacity, which contains 100-GB of memory.\n\n Tip\n\nReview Power BI license types and capabilities. If you're not sure which license type your organization has, check with the Power BI administrator.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand scalability in Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUnderstand scalability in Power BI\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nAzure Synapse Analytics\nPower BI\n\nScalable data models enable enterprise-scale analytics in Power BI. Implement data modeling best practices, use large dataset storage format, and practice building a star schema to design analytics solutions that can scale.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe the importance of building scalable data models\nImplement Power BI data modeling best practices\nUse the Power BI large dataset storage format\nAdd\nPrerequisites\n\nConsider completing the Model data in Power BI learning path. You will need knowledge of:\n\nPower BI data model design including star schema design basics\nIntroduction\nmin\nDescribe the significance of scalable models\nmin\nImplement Power BI data modeling best practices\nmin\nConfigure large datasets\nmin\nExercise: Create a star schema model\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nYou can develop your Power BI model based on three different frameworks: import, DirectQuery, and composite. Each framework has its own benefits and limitations, and features to help you optimize your model.\n\nUltimately, you should strive to develop a model that efficiently delivers fast performance with low latency, even for high volume data sources.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nGeoffrey is a data modeler at Adventure Works who developed a DirectQuery model that connects to the data warehouse. To improve the query performance of higher-grain sales queries, Geoffrey added an import aggregation table. What else should Geoffrey do to improve query performance of the higher-grain queries?\n\n \n\nSet related dimension tables as aggregation tables.\n\nSet related dimension tables to dual storage mode.\n\nSet related dimension tables to import storage mode.\n\n2. \n\nBreana is a data modeler at Adventure Works who developed a manufacturing model, which is an import model. Breana needs to ensure that manufacturing reports deliver real-time results. Which type of table should Breana create?\n\n \n\nAggregation table.\n\nHybrid table.\n\nPartitioned table.\n\n3. \n\nMousef is a business analyst at Adventure Works who wants to create a new model by extending the sales dataset, which is delivered by IT. Mousef wants to add a new table of census population data sourced from a web page. Which model framework should Mousef use?\n\n \n\nComposite.\n\nDirectQuery.\n\nLive connection.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop a composite model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/5-determine-when-to-develop-composite-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop a composite model\n7 minutes\n\nA composite model comprises more than one source group. Typically, there’s always the import source group and a DirectQuery source group.\n\n Note\n\nGenerally, the benefits and limitations associated with import and DirectQuery storage modes apply to composite models.\n\nComposite model benefits\n\nThere are several benefits to developing a composite model.\n\nComposite models provide you with design flexibility. You can choose to integrate data using different storage modes, striking the right balance between imported data and pass-through data. Commonly, enterprise models benefit from using DirectQuery tables on large data sources and by boosting query performance with imported tables. Power BI features that support this scenario are described later in this unit.\n\nComposite models can also boost the performance of a DirectQuery model by providing Power BI with opportunity to satisfy some analytic queries from imported data. Querying cached data almost always performs better than pass-through queries.\n\nLastly, when your model includes DirectQuery tables to a remote model, like a Power BI dataset, you can extend your model with new calculated columns and tables. It results in a specialized model based on a core model. For more information, see Power BI usage scenarios: Customizable managed self-service BI.\n\nComposite model limitations\n\nThere are several limitations related to composite models.\n\nImport (or dual, as described later) storage mode tables still require periodic refresh. Imported data can become out of sync with DirectQuery sourced data, so it’s important to refresh it periodically.\n\nWhen an analytic query must combine imported and DirectQuery data, Power BI must consolidate source group query results, which can impact performance. To help avoid this situation for higher-grain queries, you can add import aggregation tables to your model (or enable automatic aggregations) and set related dimension tables to use dual storage mode. This scenario is described later in this unit.\n\nWhen chaining models (DirectQuery to Power BI datasets), modifications made to upstream models can break downstream models. Be sure to assess the impact of modifications by performing dataset impact analysis first.\n\nRelationships between tables from different source groups are known as limited relationships. A model relationship is limited when the Power BI can’t determine a “one” side of a relationship. Limited relationships may result in different evaluations of model queries and calculations. For more information, see Relationship evaluation.\n\nBoost DirectQuery model performance with import data\n\nWhen there’s a justification to develop a DirectQuery model, you can mitigate some limitations by using specific Power BI features that involve import tables.\n\nImport aggregation tables\n\nYou can add import storage mode user-defined aggregation tables or enable automatic aggregations. This way, Power BI directs higher-grain fact queries to a cached aggregation. To boost query performance further, ensure that related dimension tables are set to use dual storage mode.\n\nAutomatic aggregations are a Premium feature. For more information, see Automatic aggregations.\n\nDual storage mode\n\nA dual storage mode table is set to use both import and DirectQuery storage modes. At query time, Power BI determines the most efficient mode to use. Whenever possible, Power BI attempts to satisfy analytic queries by using cached data.\n\nDual storage mode tables work well with import aggregation tables. They allow Power BI to satisfy higher-grain queries entirely from cached data.\n\nSlicer visuals and filter card lists, which are often based on dimension table columns, render more quickly because they’re queried from cached data.\n\nDeliver real-time data from an import model\n\nWhen you set up an import table with incremental refresh, you can enable the Get the latest data in real-time with DirectQuery option.\n\nBy enabling this option, Power BI automatically creates a table partition that uses DirectQuery storage mode. In this case, the table becomes a hybrid table, meaning it has import partitions to store older data, and a single DirectQuery partition for current data.\n\nWhen Power BI queries a hybrid table, the query uses the cache for older data, and passes through to the data source to retrieve current data.\n\nThis option is only available with a Premium license.\n\nFor more information, see Configure incremental refresh and real-time data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose a model framework - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/6-choose-model-framework",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nChoose a model framework\n3 minutes\n\nHere’s some general guidance about choosing the appropriate model framework for your project. It especially applies to enterprise solutions, where data volumes are large, query throughput is high, and fast responsiveness is important.\n\nMost importantly, choose the import model framework whenever possible. This framework offers you the most options, design flexibility, and delivers fast performance. Be sure to apply data reduction techniques to ensure that Power BI loads the least amount of data possible.\n\nChoose the DirectQuery model framework when your data source stores large volumes of data and/or your report needs to deliver near real-time data.\n\nChoose the composite model framework to:\n\nBoost the query performance of a DirectQuery model.\nDeliver near real-time query results from an import model.\nExtend a Power BI dataset (or AAS model) with additional data.\n\nYou can boost the query performance of a DirectQuery model by using aggregation tables, which can use import or DirectQuery storage mode. When using import aggregation tables, be sure to set related dimension tables to use dual storage mode. That way, Power BI can satisfy higher-grain queries entirely from cache.\n\nYou can deliver near real-time query results from in import model by creating a hybrid table. In this case, Power BI adds a DirectQuery partition for the current period.\n\nLastly, you can create specialized models by chaining to a core model by using DirectQuery. This type of development is typically done by a business analyst who extends core models, which IT delivers and supports.\n\n Important\n\nPlan carefully. In Power BI Desktop, it’s always possible to convert a DirectQuery table to an import table. But it’s not possible to convert an import table to a DirectQuery table.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop an import model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/3-determine-when-to-develop-import-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop an import model\n7 minutes\n\nAn import model comprises tables that have their storage mode property set to Import. It includes calculated tables, which you define with a DAX formula, too.\n\nImport model benefits\n\nImport models are the most frequently developed model framework because there are many benefits. Import models:\n\nSupport all Power BI data source types, including databases, files, feeds, web pages, dataflows, and more.\nCan integrate source data. For example, one table sources its data from a relational database while a related table sources its data from a web page.\nSupport all DAX and Power Query (M) functionality.\nSupport calculated tables.\nDeliver the best query performance. That’s because the data cached in the model is optimized for analytic queries (filter, group, and summarize) and the model is stored entirely in memory.\n\nIn short, import models offer you the most options and design flexibility, and they deliver fast performance. For this reason, Power BI Desktop defaults to use import storage mode when you “Get data.”\n\nImport model limitations\n\nDespite the many compelling benefits, there are limitations of import models that you must bear in mind. Limitations are related to model size and data refresh.\n\nModel size\n\nPower BI imposes dataset size restrictions, which limit the size of a model. When you publish the model to a shared capacity, there’s a 1-GB limit per dataset. When this size limit is exceeded, the dataset will fail to refresh. When you publish the model to a dedicated capacity (also known as Premium capacities), it can grow beyond 10-GB, providing you enable the Large dataset storage format setting for the capacity.\n\nYou should always strive to reduce the amount of data stored in tables. This strategy helps to reduce the duration of model refreshes and speed up model queries. There are numerous data reduction techniques that you can apply, including:\n\nRemove unnecessary columns\nRemove unnecessary rows\nGroup by and summarize to raise the grain of fact tables\nOptimize column data types with a preference for numeric data\nPreference for custom columns in Power Query instead of calculated columns in the model\nDisable Power Query query load\nDisable auto date/time\nUse DirectQuery table storage, as described in later units of this module.\n\nFor more information, see Data reduction techniques for Import modeling.\n\n Note\n\nThe 1-GB per dataset limit refers to the compressed size of the Power BI model, not the volume of data being collected from the source system.\n\nData refresh\n\nImported data must be periodically refreshed. Dataset data is only as current as the last successful data refresh. To keep data current, you set up scheduled data refresh, or report consumers can perform an on-demand refresh.\n\nPower BI imposes limits on how often scheduled refresh operations can occur. It’s up to eight times per day in a shared capacity, and up to 48 times per day in a dedicated capacity.\n\nYou should determine whether this degree of latency is tolerable. It often depends on the velocity (or volatility) of the data, and the urgency to keep users informed about the current state of data. When scheduled refresh limits aren’t acceptable, consider using DirectQuery storage tables, or creating a hybrid table. Or take a different approach, and create a real-time dataset instead.\n\n Tip\n\nHybrid tables are described in unit 4. For information about real-time datasets, work through the Monitor data in real-time with Power BI module.\n\nYou must also consider refresh workload and duration. By default, to refresh a table, Power BI removes all data and reloads it again. These operations can place an unacceptable burden on source systems, especially for large fact tables. To reduce this burden, you can set up the incremental refresh feature. Incremental refresh automates the creation and management of time-period partitions, and intelligently update only those partitions that require refresh.\n\nWhen your data source supports incremental refresh, it can result in faster and more reliable refreshes, and reduced resource consumption of Power BI and source systems.\n\nAdvanced data modelers can customize their own partitioning strategy. Automation scripts can create, manage, and refresh table partitions. For more information, see Power BI usage scenarios: Advanced data model management. This usage scenario describes using the XMLA endpoint available with Power BI Premium.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Determine when to develop a DirectQuery model - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/4-determine-when-to-develop-directquery-model",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDetermine when to develop a DirectQuery model\n9 minutes\n\nA DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group.\n\nA source group is a set of model tables that relate to a data source. There are two types:\n\nImport – Represents all import storage mode tables including calculated tables. There can only be one import source group in a model.\nDirectQuery – Represents all DirectQuery storage mode tables that relate to a specific data source.\n\n Note\n\nAn import model and a DirectQuery model only comprise a single source group. When there’s more than one source group, the model framework is known as a composite model. Composite models are described in Unit 5.\n\nDirectQuery model benefits\n\nThere are several benefits to developing a DirectQuery model.\n\nModel large or fast-changing data sources\n\nA DirectQuery model is a great framework choice when your source data exhibits volume and/or velocity characteristics. Because DirectQuery tables don’t require refresh, they’re well suited to large data stores, like a data warehouse. It’s impractical and inefficient, if not impossible, to import an entire data warehouse into a model. When the source data changes rapidly and users need to see current data, a DirectQuery model can deliver near real-time query results.\n\nWhen a report queries a DirectQuery model, Power BI passes those queries to the underlying data source.\n\nEnforce source RLS\n\nDirectQuery is also useful when the source database enforces row-level security (RLS). Instead of replicating RLS rules in your Power BI model, the source data base can enforce its rules. This approach works only for some relational databases, and it involves setting up single sign-on for the dataset data source. For more information, see Azure SQL Database with DirectQuery.\n\nData sovereignty restrictions\n\nIf your organization has security policies that restrict data leaving their premises, then it isn’t possible to import data. A DirectQuery model that connects to an on-premises data source may be appropriate. (You can also consider installing Power BI Report Server for on-premises reporting.)\n\nCreate specialized datasets\n\nTypically, DirectQuery mode supports relational database sources. That’s because Power BI must translate analytic queries to native queries understood by the data source.\n\nHowever, there’s one powerful exception. You can connect to a Power BI dataset (or Azure Analysis Services model) and convert it to a DirectQuery local model. A local model is a relative term that describes a model’s relationship to another model. In this case, the original dataset is a remote model, and the new dataset is the local model. These models are chained, which is term used to describe related models. You can chain up to three models in this way.\n\nThis capability to chain models supports the potential to personalize and/or extend a remote model. The simplest thing you can do is rename objects, like tables or columns, or add measures to the local model. You can also extend the model with calculated columns or calculated tables, or add new import or DirectQuery tables. However, these extensions result in the creation of new source groups, which means the model becomes a composite model. That scenario is described in Unit 5.\n\nFor more information, see Using DirectQuery for Power BI datasets and Azure Analysis Services.\n\nDirectQuery model limitations\n\nThere are many limitations related to DirectQuery models that you must bear in mind. Here are the main limitations:\n\nNot all data sources are supported. Typically, only major relational database systems are supported. Power BI datasets and Azure Analysis Services models are supported too.\n\nAll Power Query (M) transformations are not possible, because these queries must translate to native queries that are understood by source systems. So, for example, it’s not possible to use pivot or unpivot transformations.\n\nAnalytic query performance can be slow, especially if source systems aren’t optimized (with indexes or materialized views), or there are insufficient resources for the analytic workload.\n\nAnalytic queries can impact on source system performance. It could result in a slower experience for all workloads, including OLTP operations.\n\nBoost DirectQuery model performance\n\nWhen there’s a justification to develop a DirectQuery model, you can mitigate some limitations in two ways.\n\nData source optimizations\n\nYou can optimize the source database to ensure the expected analytic query workload performs well. Specifically, you can create indexes and materialized views, and ensure the database has sufficient resources for all workloads.\n\n Tip\n\nWe recommend that you always work in collaboration with the database owner. It’s important that they understand the additional workload a DirectQuery model can place on their database.\n\nDirectQuery user-defined aggregation tables\n\nYou can add user-defined aggregation tables to a DirectQuery model. User-defined aggregation tables are special model tables that are hidden (from users, calculations, and RLS). They work best when they satisfy higher-grain analytic queries over large fact tables. When you set the aggregation table to use DirectQuery storage mode, it can query a materialized view in the data source. You can also set an aggregation table to use import storage mode or enable automatic aggregations, and these options are described in Unit 4.\n\nFor more information, see DirectQuery model guidance in Power BI Desktop.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe Power BI model fundamentals - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/2-describe-power-bi-model-fundamentals",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe Power BI model fundamentals\n9 minutes\n\nThis unit introduces Power BI model terms. It’s important that you understand these terms in order to choose the appropriate model framework for your project. This unit describes the following terms:\n\nData model\nPower BI dataset\nAnalytic query\nTabular model\nStar schema design\nTable storage mode\nModel framework\nData model\n\nA Power BI data model is a query-able data resource that’s optimized for analytics. Reports can query data models by using one of two analytic languages: Data Analysis Expressions (DAX) or Multidimensional Expressions (MDX). Power BI uses DAX, while paginated reports can use either DAX or MDX. The Analyze in Excel features uses MDX.\n\n Tip\n\nA data model is also described as semantic model, especially in enterprise scenarios. Commonly, in the context of a data discussion and in this module, a data model is simply referred to as a model.\n\nPower BI dataset\n\nYou develop a Power BI model in Power BI Desktop, and once published to a workspace in the Power BI service, it’s then known as a dataset. A dataset is a Power BI artifact that’s a source of data for visualizations in Power BI reports and dashboards.\n\n Note\n\nNot all datasets originate from models developed in Power BI Desktop. Some datasets represent connections to external-hosted models in AAS or SSAS. Others can represent real-time data structures, including push datasets, streaming datasets, or hybrid datasets. This module is concerned only with models developed in Power BI Desktop.\n\nAnalytic query\n\nPower BI reports and dashboards must query a dataset. When Power BI visualizes dataset data, it prepares and sends an analytic query. An analytic query produces a query result from a model that’s easy for a person to understand, especially when visualized.\n\nAn analytic query has three phases that are executed in this order:\n\nFilter\nGroup\nSummarize\n\nFiltering (sometimes known as slicing) narrows down on a subset of the model data. Filter values aren’t visible in the query result. Most analytic queries apply filters because it’s common to filter by a time period, and usually other attributes. Filtering happens in different ways. In a Power BI report, you can set filters at report, page, or visual level. Report layouts often include slicer visuals to filter visuals on the report page. When the model enforces row-level security (RLS), it applies filters to model tables to restrict access to specific data. Measures, which summarize model data, can also apply filters.\n\nGrouping (sometimes known as dicing) divides query result into groups. Each group is also a filter, but unlike the filtering phase, filter values are visible in the query result. For example, grouping by customer filters each group by customer.\n\nSummarization produces a single value result. Typically, a report visual summarizes a numeric field by using an aggregate function. Aggregate functions include sum, count, minimum, maximum, and others. You can achieve simple summarization by aggregating a column, or you can achieve complex summarization by creating a measure using a DAX formula.\n\nConsider an example: A Power BI report page includes a slicer to filter by a single year. There’s also a column chart visual that shows quarterly sales for the filtered year.\n\nIn this example, the slicer filters the visual by calendar year 2021. The column chart groups by quarters (of the filtered year). Each column is a group that represents a visible filter. The column heights represent the summarized sales values for each quarter of the filtered year.\n\nTabular model\n\nA Power BI model is a tabular model. A tabular model comprises one or more tables of columns. It can also include relationships, hierarchies, and calculations.\n\nStar schema design\n\nTo produce an optimized and easy-to-use tabular model, we recommend you produce a star schema design. Star schema design is a mature modeling approach widely adopted by relational data warehouses. It requires you to classify model tables as either dimension or fact.\n\nDimension tables describe business entities; the things you model. Entities can include products, people, places, and concepts including time itself. Fact tables store observations or events, and can be, for example, sales orders, stock balances, exchange rates, or temperature readings. A fact table contains dimension key columns that relate to dimension tables, and numeric measure columns. A fact table forms the center of a star, and the related dimension tables form the points of the star.\n\nIn an analytic query, dimensions table columns filter or group. Fact table columns are summarized.\n\nFor more information, see Understand star schema and the importance for Power BI.\n\nTable storage mode\n\nEach Power BI model table (except calculated tables) has a storage mode property. The storage mode property can be either Import, DirectQuery, or Dual, and it determines whether table data is stored in the model.\n\nImport – Queries retrieve data that’s stored, or cached, in the model.\nDirectQuery – Queries pass through to the data source.\nDual – Queries retrieve stored data or pass through to the data source. Power BI determines the most efficient plan, striving to use cached data whenever possible.\nModel framework\n\nTable storage mode settings determine the model framework, which can be either import, DirectQuery, or composite. The following units in this module describe each of these frameworks and provides guidance on their use.\n\nAn import model comprises tables that have their storage mode property set to Import.\nA DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group. Source group is described later in this module.\nA composite model comprises more than one source group.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nFor over two decades, Microsoft continues to make deep investments in enterprise business intelligence (BI). Azure Analysis Services (AAS) and SQL Server Analysis Services (SSAS) are based on mature BI data modeling technology used by countless enterprises. The same technology is also at the heart of Power BI data models.\n\nPower BI offers you a choice when designing your model. You can use Power BI Desktop to develop your model, and you can develop it by using different frameworks. These frameworks help to deliver fast performance, near real-time results, or both.\n\nThis module introduces the frameworks, their benefits and limitations, and features to help optimize your models. Lastly, it provides you with guidance to help you choose the right framework and features for your project.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI model fundamentals.\nDetermine when to develop an import model.\nDetermine when to develop a DirectQuery model.\nDetermine when to develop a composite model.\nChoose an appropriate Power BI model framework.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose a Power BI model framework - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nChoose a Power BI model framework\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nPower BI\n\nDescribe model frameworks, their benefits and limitations, and features to help optimize your Power BI data models.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe Power BI model fundamentals.\nDetermine when to develop an import model.\nDetermine when to develop a DirectQuery model.\nDetermine when to develop a composite model.\nChoose an appropriate Power BI model framework.\nAdd\nPrerequisites\nExperience developing Power BI data models, reports, and dashboards.\nIntroduction\nmin\nDescribe Power BI model fundamentals\nmin\nDetermine when to develop an import model\nmin\nDetermine when to develop a DirectQuery model\nmin\nDetermine when to develop a composite model\nmin\nChoose a model framework\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n2 minutes\n\nUnderstanding the rapidly changing data landscape and how roles and technologies are evolving will enable your organization to achieve analytics at scale. Solutions for scaling analytics are technology, people, and process based, and will set your organization up for data-driven decision making.\n\nIn this module you've learned how to:\n\nDescribe job roles in analytics\nUnderstand tools for scaling analytics solutions\nLearn more\nGet started with Azure Synapse Analytics\nPower BI adoption roadmap\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhich of the following tasks are completed by a data analyst?\n\n \n\nBuild monthly sales reports and gather feedback from the business on how reports are being used.\n\nDesign, implement, and maintain operational aspects of on-premises and cloud-based database systems.\n\nManage and secure the flow of structured and unstructured data from multiple sources into analytical data storage.\n\n2. \n\nWhat Microsoft tools do data analysts use for enterprise-scale analytics solutions?\n\n \n\nAzure Machine Learning and Azure Cosmos DB.\n\nMicrosoft Excel.\n\nAzure Synapse Analytics and Microsoft Power BI\n\n3. \n\nWhat people focused strategies can help an organization achieve analytics at scale?\n\n \n\nAzure Synapse Analytics and Microsoft Power BI can help achieve analytics at scale in an organization.\n\nBuilding a data culture, establishing a Center of Excellence, and creating a community of practice.\n\nAchieving analytics at scale should be technology focused rather than people focused.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Strategies to scale analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/5-scale-analytics-center-excellence",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nStrategies to scale analytics\n6 minutes\n\nIn the previous unit, we focused on the technology used to scale analytics. In this unit we'll focus on other equally important aspects, people and process.\n\nScaling analytics and data use across an enterprise isn't possible with just great tools. This requires a cultural shift in which the technology becomes the easy piece to implement. Building a data culture, creating a community of practice, and implementing a data governance strategy are tangible ways to focus on people and process.\n\nJust because there's been a massive increase in data generation over the last decade doesn't inherently mean we've equally increased our use of that data. Building a data culture, building a center of excellence, and supporting analytics practitioners with a community of practice are a few strategies to nurture data use.\n\nBuild a data culture\n\nBuilding a data culture goes well beyond analytics solutions. Building and maintaining a data culture is a key aspect in an organization's digital transformation, and encourages a culture that makes data-informed decisions. Data culture can be defined in different ways, but generally data culture means a set of behaviors and norms in the organization that encourage data informed decision making.\n\nIt's important that you have a well-understood definition of what a healthy data culture means to your organization. This vision of a healthy data culture should:\n\nOriginate from the executive level.\nAlign with organizational objectives.\nDirectly influence your adoption strategy.\nServe as the high-level guiding principles for enacting governance policies and guidelines.\n\nAs an analyst working in a mid-size company, you may have more influence on the data culture than you recognize. Producing useful insights for decision makers often leaves them wanting more, which drives data use. Building a data culture may ideally originate from the executive level but data teams can act as advocates and evangelists.\n\n Tip\n\nLearn more about building a data culture in the Power BI adoption roadmap: data culture\n\nEstablish a Center of Excellence\n\nAn analytics Center of Excellence (COE) is an internal team in your organization whose role is to maximize the potential of analytics in the organization. An analytics COE is a focused initiative to achieve analytics at scale.\n\nAn analytics COE is traditionally made up of both technical and business experts who evangelize the data-driven culture and guide others in the uptake of analytics solutions. COE members provide expert advice to others and serve as a guiding force promoting the uptake of analytics solutions.\n\n Tip\n\nLearn more about how to establish your own Center of Excellence\n\nCommunity of practice\n\nA community of practice is a group of people with a common interest that interacts with, and helps, each other on a voluntary basis. Mastering analytics tools like Power BI creates common interests and encourages individuals across departments and teams to work towards a common goal.\n\nBuilding a community of practice at your organization encourages knowledge sharing and can catalyze the building of a healthy data culture from the ground-up.\n\n Tip\n\nLearn more about building a community of practice in the Power BI adoption roadmap: Community of Practice\n\nBuilding a data culture, building a center of excellence, and supporting analytics practitioners with a community of practice are only a few strategies to ensure that analytics will scale throughout your organization. These, combined with the right technologies will help your organization scale analytics solutions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Review tasks and tools for data analysts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/3-review-tasks-tools-for-data-analysts",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Introduction to data analytics on Azure  Explore data analytics at scale \nAdd\nPrevious\nUnit 3 of 7\nNext\nReview tasks and tools for data analysts\nCompleted\n100 XP\n6 minutes\n\nData analysts discover and present insights in data. In an enterprise, analysts work with data that has been prepared and stored by the data engineer. A data team working together can surface insights from large volumes of data and enable the business to regularly make data-informed decisions.\n\nThe data analyst role can be broad. Analysts may sit in the business, with domain expertise, and focus on producing actionable reports for decision makers quickly. This individual may be referred to as a Power BI data analyst.\n\nAlternatively, an analyst may sit in the IT department, and be responsible for more complex data modeling, with a focus on creating scalable, reusable assets. This individual may be referred to as an enterprise data analyst. The enterprise data analyst may provide technical assistance to the Power BI data analyst.\n\n Note\n\nDepending on the size and makeup of your organization, data analyst roles may look different. There may be distinct roles for data visualization, data modeling, development, and/or administration and governance of the analytics platform. In a different organization, there might be a single person managing all analyst responsbilities.\n\nData analyst tasks\n\nData analysts collect and transform data to generate insights. In addition to digging into and analyzing data, analysts must be able to present data to stakeholders in a way that enables decision making.\n\nAnalyst tasks throughout the analytics process include:\n\nCollect and clean data for analysis\nIngest, transform, and model data in a reporting tool\nGenerate data products, dashboards and reports\nCommunicate findings with relevant stakeholders\nMonitor usage of analytics solution\nGather and incorporate feedback and additional requirements\n\n Note\n\nSee Understand concepts of data analytics for more information on the analytics process.\n\nData analyst tools\n\nThere are hundreds of tools that analysts may use to clean and analyze data. Here, we'll focus on Microsoft tools used in an enterprise analytics solution.\n\nData analysts often use tools like Microsoft Excel, Power BI, and Azure Synapse Analytics to build analytics solutions. Excel may be used for one-off analysis, but when it comes to enterprise analytics solutions analysts are most often querying data from sources like Azure SQL databases or Azure Synapse Analytics.\n\nThe Power BI data analyst and the enterprise analyst will likely use similar tools and have similar skill sets, with one major distinction. An enterprise data analyst will be working with data at scale, and is more likely to be working with tools that can handle larger data.\n\nFor example, it isn't uncommon for companies today to have petabytes of data across the organization. Analyzing and extracting insights from such massive amounts of data used to take hours, if not days. By using tools like Azure Synapse analytics, this processing time can be brought down to minutes. Massive amounts of data are likely to be handled by the enterprise data analyst.\n\nIn addition to technical knowledge of analytics tools, it's critical that analysts have foundational knowledge of relational databases, basic statistics, and data visualization.\n\n Note\n\nLearn more about foundational knowledge at Explore core data concepts.\n\nThe final skill set that makes for a great analyst are soft skills that enable decision making including:\n\nCommunication\nProcess management\nProblem solving\nCollaboration and team work\nCreativity\n\nSuccessful data analysts need a combination of technical and soft skills to solve problems and deliver actionable insights.\n\nNext unit: Scale analytics with Azure Synapse Analytics and Power BI\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Scale analytics with Azure Synapse Analytics and Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/4-scale-analytics-azure-synapse-analytics-power-bi",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nScale analytics with Azure Synapse Analytics and Power BI\n3 minutes\n\nYou've likely noticed the shift in terminology from business intelligence to modern analytics. This shift is due in part to the sheer volume of data generated that needs to be processed quickly, which wouldn't be possible with a traditional ETL, data warehousing, and reporting approach.\n\nModern analytics takes advantage of advances in computing both on premises and in the cloud. Large in-memory and specialized data stores have become increasingly affordable. These data stores use massively parallel processing, meaning that computations are completed in parallel, distributed across many processors or machines. Distributed processing leads to dramatic improvements in processing time. Additionally, the move to the cloud made it possible to acquire large amounts of computing power that can be turned on and off.\n\nModern analytics at scale\n\nAzure Synapse Analytics is a unified, end-to-end solution for large scale data analytics. It brings together multiple technologies and capabilities, enabling you to combine the data integrity and reliability of a scalable, high-performance SQL Server based relational data warehouse with the flexibility of a data lake and open-source Apache Spark. Azure Synapse Analytics also includes native support for log and telemetry analytics with Azure Synapse Data Explorer pools, as well as built in data pipelines for data ingestion and transformation.\n\nAll Azure Synapse Analytics services can be managed through a single, interactive user interface called Azure Synapse Studio, which includes the ability to create interactive notebooks in which Spark code and markdown content can be combined. Synapse Analytics is a great choice when you want to create a single, unified analytics solution on Azure.\n\nLooking beyond data exploration and into reporting and data use, Power BI and Azure Synapse are natively integrated. Using Power BI with Synapse enables analysts to process large-scale data quickly.\n\n Note\n\nAzure Synapse Analytics and Power BI are not Microsoft's only tools for scaling analytics. They are however the main tools used by data analysts in scalable modern analytics solutions. Learn more about large-scale data analytics solutions in Explore fundamentals of modern data warehousing.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore data team roles and responsibilities - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/2-explore-data-team-roles-responsibilities",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExplore data team roles and responsibilities\n7 minutes\n\nLarge data projects can be complex. The projects often involve hundreds of decisions. Multiple people are typically involved, and each person helps take the project from design to production.\n\nYour organization may define roles differently, or give them different names. The roles described in this unit represent the most common division of tasks and responsibilities.\n\nThese roles are common in modern data projects:\n\nData analyst\nDatabase administrator\nData engineer\nData scientist\nData architect\nData analyst\n\nA data analyst enables businesses to maximize the value of their data assets. They're responsible for exploring data to identify trends, designing and building analytical models, and enabling advanced analytics capabilities through reports and visualizations.\n\nA data analyst processes raw data into relevant insights based on identified business requirements.\n\nDatabase administrator\n\nA database administrator (DBA) is responsible for the design, implementation, maintenance, and operational aspects of on-premises and cloud-based database systems. DBAs are responsible for the overall availability and consistent performance and optimizations of databases. DBAs work with stakeholders to implement policies, tools, and processes for backup and recovery plans to recover following a natural disaster or human-made error.\n\nThe database administrator is also responsible for managing the security of the data in the database, granting privileges over the data, granting or denying access to users as appropriate.\n\nData engineer\n\nData engineers configure data platform technologies that are on-premises and in the cloud. They manage and secure the flow of structured and unstructured data from multiple sources. The data platforms they use can include relational databases, nonrelational databases, data streams, and file stores. Data engineers also ensure that data services securely integrate with other data platform technologies or application services such as Azure Cognitive Services, Azure Search, or even bots.\n\nThe role of data engineer is different from the role of a database administrator. A data engineer's scope of work goes beyond looking after a database and the server where it's hosted. Data engineers must also get, ingest, transform, validate, and clean up data to meet business requirements. This process is called data wrangling.\n\nData scientist\n\nData scientists perform advanced analytics to extract value from data. Their work can vary from descriptive analytics to predictive analytics. Predictive analytics are used in machine learning to apply modeling techniques that can detect anomalies or patterns.\n\nPredictive analytics is just one aspect of data scientists' work. Some data scientists might even work in the realms of deep learning, iteratively experimenting to solve a complex data problem by using customized algorithms.\n\nData architect\n\nData architects are typically responsible for planning and executing an overall data management strategy, including defining standards of data quality and security. They collaborate with other members of the data team to execute the high-level strategy.\n\nData architects must have deep technical knowledge and strong soft skills like effective communication and leadership.\n\nRole differences\n\nThe roles of the data analyst, database administrator, data engineer, data scientist, and data architect differ. Each role solves a different problem and contributes an important part to digital transformation projects.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nOver the last decade, the amount of data that systems and devices generate has increased significantly. Because of this increase, new technologies, roles, and approaches to working with data are affecting data professionals. In many industries, data professionals want to understand better how these changes affect both their careers and their daily working lives.\n\nTo generate value, anyone working with data needs to understand the rapidly changing data landscape and how roles and technologies are evolving.\n\nLearning objectives\n\nIn this module, you will:\n\nExplore job roles in analytics\nUnderstand tools for scaling analytics solutions\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore data analytics at scale - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-data-analytics-scale/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nExplore data analytics at scale\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nAzure\nAzure Synapse Analytics\nPower BI\n\nDescribe data analytics at scale and understand the roles of a data team. Learn about the responsibilities of an enterprise data analyst and what tools are available to build scalable solutions.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nExplore data job roles in analytics\nUnderstand tools for scaling analytics solutions\nAdd\nPrerequisites\nYou should be familiar with basic data concepts and terminology.\nIntroduction\nmin\nExplore data team roles and responsibilities\nmin\nReview tasks and tools for data analysts\nmin\nScale analytics with Azure Synapse Analytics and Power BI\nmin\nStrategies to scale analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nUnderstanding data analytics concepts enables you to plan for a successful data analytics project. Knowing that the sales manager is asking for descriptive, diagnostic, and prescriptive analytics helps the data team understand who and what technologies need to be involved.\n\nIn this module you've learned how to:\n\nDefine the five types of data analytics\nDescribe the data analytics process\nIdentify data types and storage\nLearn more\nExplore Azure database and analytics services\nExplore fundamentals of modern data warehousing\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhich of the following is an example of descriptive analytics?\n\n \n\nA monthly sales report looking at sales data over the last year\n\nA social media algorithm that recommends curated content\n\nAn annual HR report that forecasts predicted attrition for the next year\n\n2. \n\nWhat are the first three steps in the data analytics process?\n\n \n\nData exploration, data analysis, and deploy analytics solution\n\nData analysis, deploy analytics solution, and request feedback\n\nRequirements gathering, data ingestion and processing, and data exploration\n\n3. \n\nWhy is it important to understand the difference between structured and unstructured data?\n\n \n\nThere is no difference between structured and unstructured data\n\nUnderstanding the difference can determine where data should be stored and what kind of analysis is most appropriate\n\nUnderstanding the difference will determine if the business requirements have been met\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand types of data and data storage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/4-understand-types-of-data-data-storage",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand types of data and data storage\n3 minutes\n\nUnderstanding how data is structured and stored is a critical step that occurs at the beginning of every analytics project, during requirements gathering. Both structured and unstructured data are suitable for analysis, but the tools the data team will use to ingest, transform, and store data will differ according to the data type.\n\nStructured data\n\nStructured data is familiar to most of us. Letters and numbers are organized into columns and rows for simplified search and processing. Structured data is typically quantitative in nature and stored in relational databases and data warehouses. Structured data may reside in something familiar, a Microsoft Excel table. Structured data storage on a larger scale may be stored in a relational database, like an Azure SQL database.\n\nStructured data lends well to all types of analytics and is the most accessible. Structured Query Language (SQL) is used to query relational databases and is commonly used by data analysts, data engineers, and data scientists alike.\n\nPresentation of annual financial data is a common example of using structured data, whether that data is stored in Excel spreadsheets or a relational database like Azure SQL database.\n\nUnstructured data\n\nUnstructured data is information that isn't organized in any discernable manner. Unstructured data is often more suitable for qualitative analysis and is stored in non-relational databases and data lakes.\n\nThe formats of unstructured data vary widely, from Word documents, .csv files, json files, images, and PDFs, to audio and video files. These files would be stored in an Azure Data Lake.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore the data analytics process - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/3-explore-data-analytics-process",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExplore the data analytics process\n10 minutes\n\nData analytics is the process of collecting, transforming, and presenting data to inform decision making. Developing an analytics solution begins before any technology is involved, with a requirements gathering exercise. From there the process continues to ingesting, processing, and exploring data. Analysis and solution deployment are followed by requesting feedback from the business. Finally, the analytics solution is optimized and the process begins again. The analytics process is never done.\n\nHere, you'll learn what steps are included in the data analytics process.\n\nRequirements gathering\n\nData teams work with the business to understand business needs and intended outcomes of an analytics project. Requirements gathering includes identification of:\n\nWhat are the key business questions?\nWhat data are available? Will available data respond to business needs or does more data need to be collected?\nWhat are the essential dimensions - how will stakeholders want to slice and dice the data?\nWhat are the key performance indicators or performance metrics?\nHow will users consume the analysis?\nWhat is the frequency of data ingestion?\nWhat is the frequency of reporting?\n\nIt's a common misunderstanding that a data team will be able to extract insights from volumes of data without having discussed any of the questions above. A data team won't be able to determine the appropriate type of analysis and/or the correct solution without having followed a structured requirements gathering process.\n\nRequirements gathering may take many forms depending on team structure, data volume and velocity, and the type of analysis required.\n\nData ingestion and processing\n\nUsing the requirements gathered from the business, a data team will begin to ingest and transform data.\n\nAzure data services available for ingestion and transformation include, but aren't limited to Azure Cosmos DB, Azure SQL Database, Azure Synapse Analytics, Azure Databricks, Azure Data Lake, Azure Event Hubs, and Azure Stream Analytics.\n\nA data engineer is often responsible for the initial ingestion and transformation of data. Data is then surfaced to other members of the data team for exploration and analysis. Azure data services commonly used by enterprise data analysts and data scientists may be limited to specific databases or data lakes.\n\nThe terms Extract, Transform, and Load (ETL) or Extract, Load, and Transform (ELT) refer to the process of ingesting and processing data.\n\n Note\n\nLearn more about the ETL process.\n\nData exploration\n\nData exploration is the effort to understand what you're working with, and how that data can respond to the needs of the business. Data exploration can be done in many different tools. At a basic level, the data team might use Excel to look at the contents of a .csv to view the number of records and/or the specific variables they have to explore. Each member of the data team may conduct data profiling in a different tool. An analyst may profile data using Power Query in Power BI, while a data scientist may use Apache Spark in Azure Synapse.\n\nData exploration helps inform required data transformation and cleaning steps, which can be communicated back upstream to the data engineer to build into the analytics solution.\n\nThe analyst may also begin dashboard or report prototyping in the data exploration phase. Understanding how the business wants to see and use the results of the analysis will inform the prototype, along with trends and or insights uncovered during data exploration.\n\nData analysis\n\nAfter data have been explored, data analysis can begin. Analysis can be descriptive, predictive, prescriptive, or even cognitive and can be conducted in many different tools. Results should respond to identified business needs and upon initial review, will likely lead to more questions and analysis.\n\nThere is a difference between a one-off analysis and an analytics solution. Both have their place, and the need for one or the other will be determined during the requirements gathering process.\n\nDeploy analytics solution\n\nResults will be presented to stakeholders in a reporting or data visualization tool like Microsoft Power BI, where people can interact with and use the results of the analysis for decision making.\n\nKey considerations in the deployment of an analytics solution will help determine the right tools, licensing, and permissions needed to get data into the hands of everyone that needs it. Access to timely insights will ultimately lead to data-informed decisions.\n\nRequest and process feedback\n\nDeployment of an analytics solution may feel like a finish line, but it's important to understand the answers to a few key questions.\n\nIs the data product being used?\nDoes the analysis truly respond to the business needs?\nAre there any unforeseen technical issues with the solution?\nIs the data product accessible?\nWhat new business questions does this analysis raise?\n\nThe individuals using your analytics solution are your customers, and if the product you have built doesn't adequately respond to their needs, there's work to be done.\n\nThere are multiple mediums of soliciting feedback. The first launch of a solution may require regular review meetings, whereas monitoring usage metrics of an ongoing project will help you understand usage over time and even areas of your solution that are and aren't useful.\n\nOptimize solution\n\nImplementing the feedback of your users is a logical first step to optimize your analytics solution. There may also be opportunities to remove latency in the process, for example, ensuring the data refresh occurs in the allotted time. Optimization could also mean more accurately reflecting user needs by tweaking visual design or ensuring report visuals render quickly.\n\nBegin again\n\nThe analytics process is cyclical by nature. Exposing data and insights often leads to requests for more analysis, which leads to more feedback, and so on. On a large data team, the analytics process may occur in short sprints, where different team members work simultaneously to achieve small goals before moving onto the next step in the process. On smaller teams, one person may be acting in multiple roles, which would make the process look different.\n\nRegardless of what the process looks like for you, communication is a critical component throughout. The data team must communicate with each other and be in dialogue with the business, to ensure solution development is responding to business needs and needs that may appear in the data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand data analytics types - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/2-understand-data-analytics-types",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand data analytics types\n7 minutes\n\nData analytics is used for exploring data, extracting insights, and acting on those insights.\n\nData analytics covers a range of activities, each with its own focus and goals. These activities are categorized as descriptive, diagnostic, predictive, prescriptive, and cognitive analytics.\n\nIn this unit, you'll learn about these five categories of data analytics and what they are used for.\n\nDescriptive analytics\nWhat happened?\n\nDescriptive analytics answers questions about what happened, based on historical data, to inform decisions about the future. Descriptive analytics techniques summarize large datasets to present insights to stakeholders. Descriptive analytics is the most common type of analytics and is often performed by a data analyst.\n\nThe development of key performance indicators (KPIs) and other performance measures helps track the success or failure of business objectives. KPIs and performance metrics are often set by the business to track key initiatives. The presentation of data related to those KPIs is descriptive analytics.\n\nDescriptive analytics outputs can take many forms, including reports and dashboards. The Sales and Marketing report below displays sales and marketing data for a year by product, channel, and over time.\n\nDiagnostic analytics\nWhy did it happen?\n\nDiagnostic analytics helps answer questions about why things happened and is often the next step in data analytics after descriptive analytics. Analysts take findings from descriptive analytics and dig deeper to find the cause. Metrics and indicators of interest are further investigated to discover why they got better or worse. Diagnostic analytics is often performed by data analysts and data scientists.\n\nDiagnostic analytics generally occurs in three steps:\n\nIdentify anomalies in the data. Anomalies may be unexpected changes in a metric or a particular market.\nCollect data that's related to these anomalies.\nUse statistical techniques to discover relationships and trends that explain these anomalies.\n\nIn Contoso's sales report below, we want to understand why Contoso is or isn't winning bids for new business. Diagnostic analytics help decision makers see that applying discounts of 2% increases the likelihood of winning a bid.\n\nPredictive analytics\nWhat will happen in the future?\n\nPredictive analytics helps answer questions about what will happen in the future. Predictive analytics techniques use historical data to identify trends and determine if they're likely to recur, providing insight into what may happen in the future. Techniques include statistical and machine learning techniques such as forecasting, neural networks, decision trees, and regression. Predictive analytics is often performed by data scientists.\n\nThe line chart below looks at revenue won by year and month, which shows historical decline. Forecasting predicts that revenue won will continue to decrease. Decision makers may use this forecast to change course in an effort to increase the amount of revenue won.\n\nPrescriptive analytics\nWhat actions should be taken?\n\nPrescriptive analytics takes predictive analytics one step further and helps answer questions about what actions should be taken to achieve a goal or target. This technique allows businesses to make data-informed decisions in the face of uncertainty. Prescriptive analytics techniques rely on machine learning strategies to find patterns in large datasets. By analyzing past decisions and events, the likelihood of different outcomes can be estimated. Prescriptive analytics is often performed by data scientists. Microsoft also provides low-code tools that can be used by analysts to conduct prescriptive analytics, like using machine learning in Power BI.\n\nAlgorithmic content recommendations are a common implementation of prescriptive analytics. Using the recommendation algorithm in Azure Machine Learning studio, data scientists can recommend the best actions Contoso should take based on a customer's past habit and characteristics. The screenshot below displays the recommendation algorithm in Azure Machine Learning designer, in which customer data is being used to prescribe a specific recommendation rating.\n\n Note\n\nTo learn more about prescriptive analytics using Azure Machine Learning, review Build a content-based recommendation system.\n\nCognitive analytics\nHow can the problem be solved best?\n\nCognitive analytics combines artificial intelligence, machine learning, and data analytics approaches to guide decision making. Cognitive analytics draws inferences from existing data and patterns, derives conclusions based on existing knowledge bases, and adds findings back into the knowledge base for future inferences--a self-learning feedback loop. This feedback loop enables cognitive applications to become more precise over time.\n\nBy tapping the benefits of massive parallel/distributed computing and the falling costs of data storage and computing power, there's no limit to the cognitive development that these systems can achieve. Microsoft's Azure AI Services enables users to take advantage of cognitive analytics by extracting insights from various types of data, including things like text and images.\n\n Note\n\nTo learn more about data cognitive analytics using Azure AI Services, review Get started with Azure AI Services.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nUnderstanding key concepts of data analytics will help you and your team begin to explore and make the best use of your data.\n\nSuppose that the sales manager at Contoso wants to understand sales and marketing trends for the year. They've asked you to analyze data from the last two years to guide their next steps. What are the data showing? Why are sales trending upward? What should the sales team do to continue to increase sales? Before you can begin your analysis, your team reviews data analytics concepts to be sure you're using the right type of analytics for the job.\n\nBy the end of this module, you'll understand how different types of analytics may respond to the sales manager's questions. You'll also be able to describe the process of exploring and using data, along with types of data and how they're stored.\n\nLearning objectives\n\nIn this module, you will:\n\nDefine the five types of data analytics\nDescribe the data analytics process\nIdentify data types and storage\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand concepts of data analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-concepts-of-data-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUnderstand concepts of data analytics\nModule\n6 Units\nFeedback\nIntermediate\nData Analyst\nAzure\nAzure SQL Database\nAzure Synapse Analytics\nPower BI\n\nExplore key concepts of data analytics, including types of analytics, data, and storage. Explore the analytics process and tools used to discover insights.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nDescribe types of data analytics\nUnderstand the data analytics process\nAdd\nPrerequisites\nYou should be familiar with basic data concepts and terminology.\nIntroduction\nmin\nUnderstand data analytics types\nmin\nExplore the data analytics process\nmin\nUnderstand types of data and data storage\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/5-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n2 minutes\n\nYou've now been introduced to the Azure data ecosystem and modern enterprise analytics solutions.\n\nAs an analyst on the Relecloud team, you have a solid understanding of data analytics in Azure for data ingestion, processing, and modern analytics. You're confident that you'll be able to make appropriate recommendations as the data team begins to build its analytics solution.\n\nIn this module you've learned how to:\n\nDescribe the Azure data ecosystem for analytics\nLearn more\nExplore fundamentals of modern data warehousing\nAzure data architecture guide\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/4-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat type of data is best suited for batch processing?\n\n \n\nIoT gas sensor data that monitors and detects the presence of toxic or hazardous gasses.\n\nReal-time personalized advertising data.\n\nDaily sales report data.\n\n2. \n\nWhat is one advantage of using Power BI over a Synapse notebook for data visualization?\n\n \n\nThere is no advantage, Power BI and Azure Synapse Analytics are comparable for data visualization\n\nPower BI was designed for distribution of data products outside of the data team.\n\nPower BI displays simple visualizations compared to Synapse notebooks.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand the Azure data ecosystem - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/2-understand-azure-data-ecosystem",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand the Azure data ecosystem\n6 minutes\n\nModern analytics requires tools that can store and transform data from many sources. In this unit, you'll learn about Azure data storage solutions, data ingestion, and data processing.\n\nBefore presenting an analytics solution to Relecloud's CEO, the data team needs a clear understanding of where data will be coming from, what forms data will be in, and the anticipated scale and frequency of incoming data. Before conducting structured requirements gathering, you sit down with the team to review key data concepts.\n\nAzure data storage solutions\n\nAzure Storage accounts are the base storage type within Azure. Azure Storage offers a scalable object store for data objects and file system services in the cloud.\n\nIn an analytics solution, data from different sources are combined and prepared for use. Data can be stored as files in a data lake store or in a database. Understanding base storage types within Azure is important for the data engineer, while the data analyst needs to be familiar with an analytical data store that serves processed data in a format that can be queried using analytical tools.\n\nAreas outlined in red in the image above highlight the pieces of the analytics solution that data analysts use to make sense of the data.\n\n Note\n\nLearn more about data storage in Azure and technology choices for analytical data stores.\n\nData ingestion and processing\n\nData ingestion is the process of obtaining and importing data for immediate use or storage in an analytical data store.\n\nData processing is simply the conversion of raw data to meaningful information through a process. Depending on how data is ingested into your system, you could process each data item as it arrives, or buffer the raw data and process it in groups. Processing data as it arrives is called streaming. Buffering and processing the data in groups is called batch processing.\n\nIn batch processing, newly arriving data elements are collected into a group. The whole group is then processed at a future time as a batch. Exactly when each group is processed can be determined in many ways. For example, you can process data based on a scheduled time interval (for example, every hour), or it could be triggered when a certain amount of data has arrived. Relecloud's monthly billing process is a good example of batch processing, as account transactions are processed and billed on a monthly basis.\n\n Note\n\nBatch processing is the most common type of data processing, best suited for large datasets or data coming from legacy data systems. Batch processing is not suited for rapid analysis and decision making.\n\nIn stream processing, each new piece of data is processed when it arrives. For example, data ingestion is inherently a streaming process.\n\nStreaming handles data in real time. Unlike batch processing, there's no waiting until the next batch processing interval, and data is processed as individual pieces rather than being processed a batch at a time. Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis.\n\nA fraud department would use stream processing to handle real-time fraud and anomaly detection.\n\n Note\n\nStream processing is ideal for projects that require real-time analysis, and is less suited for projects requiring complex analytics.\n\nWhile data processing typically occurs upstream of the analytical data store, it's critical that analysts understand how data are ingested and at what frequency, to build the appropriate analytics solution.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore modern analytics solution architecture - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/3-explore-modern-analytics-solution-architecture",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExplore modern analytics solution architecture\n5 minutes\n\nAfter data ingestion and processing, data are now in a format that can be used for analysis and presentation to decision makers. Presentation of insights for decision making is the end goal of a larger analytics project. Data analysts present findings to decision makers in the form of a data product, like a dashboard or report.\n\nAzure data analytics and reporting\n\nThere are many options for analytics and reporting in Azure, depending on the needs of the business. Analysts can explore and visualize data directly in the Azure ecosystem, using tools like Synapse notebooks in Azure Synapse Analytics. Analysts can also build and deploy solutions for use by others using robust reporting tools like Microsoft Power BI.\n\nExplore and visualize data in Azure Synapse Analytics\n\nAzure Synapse Analytics provides a suite of tools to process and analyze an organization's data. It incorporates SQL technologies, Transact-SQL query capabilities, and open-source Spark tools to enable you to quickly process very large amounts of data. Data exploration in Azure Synapse Studio may be the first step in the analytics process, where you can profile and examine data.\n\nData can be explored and visualized directly in Azure Synapse Studio using the Azure Synapse SQL results pane and using native visuals in Spark notebooks. Simple visualizations of your data make it easier to detect patterns, trends, and outliers, and may help you understand what your next steps in analysis will be.\n\nSynapse Studio provides a SQL script web interface for you to author SQL queries. You can also visualize your SQL script results in a chart by selecting the Chart button.\n\nSynapse notebooks enable you to analyze data across raw formats (CSV, txt, JSON, etc.), processed file formats (parquet, Delta Lake, ORC, etc.), and SQL tabular data files against Spark and SQL.\n\n Note\n\nLearn more about large-scale data analytics in Azure Synapse Analytics.\n\nVisualize data and create reporting solutions using Microsoft Power BI\n\nPower BI is an enterprise analytics tool that can help you discover and distribute insights from data stored in Azure and beyond. Azure and Power BI can be used together to connect, combine, and analyze your entire data estate. With Power BI, you can create reports with interactive visualizations to drive decision making.\n\nData visualization in Power BI helps you turn large amounts of granular data into easily understood, visually compelling, and useful business information. The use of Power BI takes the data exploration you may have done in Azure Synapse Analytics a step further. In addition to the enhanced visualization capability, Power BI also provides a platform for secure distribution of dashboards and reports.\n\nPower BI also has native connectors to many Azure data services. Using Power BI, you can connect to data in Azure Synapse Analytics, Azure Databricks, Azure HDInsight, and more.\n\n Note\n\nLearn more about analytics and reporting with Power BI.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nIn the era of data as currency, companies are looking to do more with their data. Distilling large volumes of data into useful insights is key. There are many ways to store, ingest and process, and analyze data in the Azure ecosystem.\n\nYou've been hired as an analyst on the data team at a Relecloud, a new communications company. Relecloud's CEO has emphasized that they'll only make data-informed decisions; you're going to be busy! Because the company is new, the existing volume of data feels manageable, but you know from experience it will grow quickly. The CEO has tasked the data team with implementing a modern analytics solution, as soon as possible.\n\nThis module introduces the Azure data ecosystem and modern enterprise analytics solutions. You'll learn about data storage, data ingestion and processing, and data analytics in Azure.\n\nLearning objectives\n\nIn this module, you will:\n\nExplore the Azure data ecosystem for analytics\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore Azure data services for modern analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-data-services-for-modern-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nExplore Azure data services for modern analytics\nModule\n5 Units\nFeedback\nIntermediate\nData Analyst\nAzure\nAzure Synapse Analytics\nPower BI\n\nUnderstand analytics solutions in the Azure data ecosystem. Explore the architecture of a scalable analytics solution to meet business needs.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nDescribe the Azure data ecosystem for analytics\nAdd\nPrerequisites\nYou should be familiar with basic data concepts and terminology.\nIntroduction\nmin\nUnderstand the Azure data ecosystem\nmin\nExplore modern analytics solution architecture\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/08-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nNotebooks are one of the most common ways that data engineers and data analysts implement data ingestion and processing logic in Azure Databricks. Using Azure Data Factory to run notebooks in a pipeline enables you to create data processing solutions that can be run on-demand, at scheduled intervals, or in response to a specific event.\n\nIn this module, you learned how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/07-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou want to connect to an Azure Databricks workspace from Azure Data Factory. What must you define in Azure Data Factory?\n\n \n\nA global parameter\n\nA linked service\n\nA customer managed key\n\n2. \n\nYou need to run a notebook in the Azure Databricks workspace referenced by a linked service. What type of activity should you add to a pipeline?\n\n \n\nNotebook\n\nPython\n\nJar\n\n3. \n\nYou need to use a parameter in a notebook. Which library should you use to define parameters with default values and get parameter values that are passed to the notebook?\n\n \n\nnotebook\n\nargparse\n\ndbutils.widget\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Run an Azure Databricks Notebook with Azure Data Factory - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/06-exercise-databricks-factory",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Data engineering with Azure Databricks  Run Azure Databricks Notebooks with Azure Data Factory \nAdd\nPrevious\nUnit 6 of 8\nNext\nExercise - Run an Azure Databricks Notebook with Azure Data Factory\nCompleted\n100 XP\n40 minutes\n\nNow it's your chance to explore how to use Azure Data Factory to run a notebook in Azure Databricks for yourself.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use parameters in a notebook - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/05-notebook-parameters",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse parameters in a notebook\n3 minutes\n\nYou can use parameters to pass variable values to a notebook from the pipeline. Parameterization enables greater flexibility than using hard-coded values in the notebook code.\n\nUsing parameters in a notebook\n\nTo define and use parameters in a notebook, use the dbutils.widgets library in your notebook code.\n\nFor example, the following Python code defines a variable named folder and assigns a default value of data:\n\nPython\nCopy\ndbutils.widgets.text(\"folder\", \"data\")\n\n\nTo retrieve a parameter value, use the get function, like this:\n\nPython\nCopy\nfolder = dbutils.widgets.get(\"folder\")\n\n\nThe get function will retrieve the value for the specific parameter that was passed to the notebook. If no such parameter was passed, it will get the default value of the variable you declared previously.\n\nPassing output values\n\nIn addition to using parameters that can be passed in to a notebook, you can pass values out to the calling application by using the notebook.exit function, as shown here:\n\nPython\nCopy\npath = \"dbfs:/{0}/products.csv\".format(folder)\ndbutils.notebook.exit(path)\n\nSetting parameter values in a pipeline\n\nTo pass parameter values to a Notebook activity, add each parameter to the activity's Base parameters, as shown here:\n\nIn this example, the parameter value is explicitly specified as a property of the Notebook activity. You could also define a pipeline parameter and assign its value dynamically to the Notebook activity's base parameter; adding a further level of abstraction.\n\n Tip\n\nFor more information about using parameters in Azure Data Factory, see How to use parameters, expressions and functions in Azure Data Factory in the Azure Data Factory documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a linked service for Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/03-linked-service",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Data engineering with Azure Databricks  Run Azure Databricks Notebooks with Azure Data Factory \nAdd\nPrevious\nUnit 3 of 8\nNext\nCreate a linked service for Azure Databricks\nCompleted\n100 XP\n5 minutes\n\nTo run notebooks in an Azure Databricks workspace, the Azure Data Factory pipeline must be able to connect to the workspace; which requires authentication. To enable this authenticated connection, you must perform two configuration tasks:\n\nGenerate an access token for your Azure Databricks workspace.\nCreate a linked service in your Azure Data Factory resource that uses the access token to connect to Azure Databricks.\nGenerating an access token\n\nAn access token provides an authentication method for Azure Databricks as an alternative to credentials on the form of a user name and password. You can generate access tokens for applications, specifying an expiration period after which the token must be regenerated and updated in the client applications.\n\nTo create an Access token, use the Generate new token option on the Developer tab of the User Settings page in Azure Databricks portal.\n\nCreating a linked service\n\nTo connect to Azure Databricks from Azure Data Factory, you need to create a linked service for Azure Databricks compute. You can create a linked service in the Linked services page in the Manage section of Azure Data Factory Studio.\n\nWhen you create an Azure Databricks linked service, you must specify the following configuration settings:\n\nExpand table\nSetting\tDescription\nName\tA unique name for the linked service\nDescription\tA meaningful description\nIntegration runtime\tThe integration runtime used to run activities in this linked service. See Integration runtime in Azure Data Factory for more details.\nAzure subscription\tThe Azure subscription in which Azure Databricks is provisioned\nDatabricks workspace\tThe Azure Databricks workspace\nCluster\tThe Spark cluster on which activity code will be run. You can have Azure Databricks dynamically provision a job cluster on-demand or you can specify an existing cluster in the workspace.\nAuthentication type\tHow the linked connection will be authenticated by Azure Databricks. For example, using an access token (in which case, you need to specify the access token you generated for your workspace).\nCluster configuration\tThe Databricks runtime version, Python version, worker node type, and number of worker nodes for your cluster.\nNext unit: Use a Notebook activity in a pipeline\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a Notebook activity in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/04-notebook-activity",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a Notebook activity in a pipeline\n5 minutes\n\nAfter you've created a linked service in Azure Data Factory for your Azure Databricks workspace, you can use it to define the connection for a Notebook activity in a pipeline.\n\nTo use a Notebook activity, create a pipeline and from the Databricks category, add a Notebook activity to the pipeline designer surface.\n\nUse the following properties of the Notebook activity to configure it:\n\nExpand table\nCategory\tSetting\tDescriptions\nGeneral\tName\tA unique name for the activity.\n\tDescription\tA meaningful description.\n\tTimeout\tHow long the activity should run before automatically canceling.\n\tRetries\tHow many times should Azure Data Factory try before failing.\n\tRetry interval\tHow long to wait before retrying.\n\tSecure input and output\tDetermines if input and output values are logged.\nAzure Databricks\tAzure Databricks linked service\tThe linked service for the Azure Databricks workspace containing the notebook.\nSettings\tNotebook path\tThe path to the notebook file in the Workspace.\n\tBase parameters\tUsed to pass parameters to the notebook.\n\tAppend libraries\tRequired code libraries that aren't installed by default.\nUser properties\t\tCustom user-defined properties.\nRunning a pipeline\n\nWhen the pipeline containing the Notebook activity is published, you can run it by defining a trigger. You can then monitor pipeline runs in the Monitor section of Azure Data Factory Studio.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Databricks notebooks and pipelines - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/02-databricks-notebooks",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Databricks notebooks and pipelines\n3 minutes\n\nIn Azure Databricks, you can use notebooks to run code written in Python, Scala, SQL, and other languages to ingest and process data. Notebooks provide an interactive interface in which you can run individual code cells and use Markdown to include notes and annotations.\n\nIn many data engineering solutions, code that is written and tested interactively can later be incorporated into an automated data processing workload. On Azure, such workloads are often implemented as pipelines in Azure Data Factory, in which one or more activities are used to orchestrate a series of tasks that can be run on-demand, at scheduled intervals, or in response to an event (such as new data being loaded into a folder in a data lake). Azure Data Factory supports a Notebook activity that can be used to automate the unattended execution of a notebook in an Azure Databricks workspace.\n\n Note\n\nThe same Notebook activity is available in pipelines built in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Databricks enables data engineers to use code in notebooks to ingest and process data. While notebooks are designed to be used interactively, you can use them to encapsulate activities in a data ingestion or processing pipeline that is orchestrated using Azure Data Factory.\n\nIn this module, you'll learn how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\n\n Note\n\nWhile this module focuses on using Azure Databricks notebooks in an Azure Data Factory pipeline, the same principles and techniques apply when using an Azure Synapse Analytics pipeline.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Run Azure Databricks Notebooks with Azure Data Factory - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nRun Azure Databricks Notebooks with Azure Data Factory\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Data Factory\nAzure Databricks\n\nUsing pipelines in Azure Data Factory to run notebooks in Azure Databricks enables you to automate data engineering processes at cloud scale.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe how Azure Databricks notebooks can be run in a pipeline.\nCreate an Azure Data Factory linked service for Azure Databricks.\nUse a Notebook activity in a pipeline.\nPass parameters to a notebook.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nUnderstand Azure Databricks notebooks and pipelines\nmin\nCreate a linked service for Azure Databricks\nmin\nUse a Notebook activity in a pipeline\nmin\nUse parameters in a notebook\nmin\nExercise - Run an Azure Databricks Notebook with Azure Data Factory\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/07-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Databricks SQL enables you to create a scalable relational SQL Warehouse in Azure Databricks. Data analysts can use the SQL Warehouse to query tables and create dashboards of data visualizations.\n\nIn this module, you learned how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/06-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich of the following workloads is best suited for Azure Databricks SQL?\n\n \n\nRunning Scala code in notebooks to transform data.\n\nQuerying and visualizing data in relational tables.\n\nTraining and deploying machine learning models.\n\n2. \n\nWhich statement should you use to create a database in a SQL warehouse?\n\n \n\nCREATE VIEW\n\nCREATE SCHEMA\n\nCREATE GROUP\n\n3. \n\nYou need to share data visualizations, including charts and tables of data, with users in your organization. What should you create?\n\n \n\nA table\n\nA query\n\nA dashboard\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use a SQL Warehouse in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/05-exercise-databricks-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use a SQL Warehouse in Azure Databricks\n30 minutes\n\nNow it's your chance to explore Azure Databricks SQL for yourself. In this exercise, you'll use a SQL Warehouse in Azure Databricks to query tables and create a dashboard.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create queries and dashboards - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/04-queries-dashboards",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate queries and dashboards\n3 minutes\n\nAzure Databricks SQL is primarily designed for data analytics and visualization workloads. To support these workloads, users can create queries to retrieve and summarize data from tables, and dashboards to share visualizations of the data.\n\nQueries\n\nYou can use the SQL Editor in the Azure Databricks portal to create a query based on any valid SQL SELECT statement, and then save the query with a meaningful name to be retrieved and run later.\n\nAfter saving the query, you can schedule it to be run automatically at regular intervals to refresh the data, or you can open it and run it interactively.\n\nDashboards\n\nDashboards enable you to display the results of queries, either as tables of data or as graphical visualizations.\n\nYou can create multiple visualizations in a dashboard and share it with users in your organization. As with individual queries, you can schedule the dashboard to refresh is data periodically, and notify subscribers by email that new data is available.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create databases and tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/03-databases-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate databases and tables\n3 minutes\n\nAfter creating and starting a SQL Warehouse, you can start to work with data in tables.\n\nDatabase schema\n\nAll SQL Warehouses contain a default database schema named default. You can use create tables in this schema in order to analyze data. However, if you need to work with multiple tables in a relational schema, or you have multiple analytical workloads where you want to manage the data (and access to it) separately, you can create custom database schema. To create a database, use the SQL editor to run a CREATE DATABASE or CREATE SCHEMA SQL statement. These statements are equivalent, but CREATE SCHEMA is preferred, as shown in this example:\n\nCREATE SCHEMA salesdata;\n\n\n Tip\n\nFor more information, see CREATE SCHEMA in the Azure Databricks documentation.\n\nTables\n\nYou can use the user interface in the Azure Databricks portal to upload delimited data, or import data from a wide range of common data sources. The imported data is stored in files in Databricks File System (DBFS) storage, and a Delta table is defined for it in the Hive metastore.\n\nIf the data files already exist in storage, or you need to define an explicit schema for the table, you can use a CREATE TABLE SQL statement. For example, the following code creates a table named salesorders in the salesdata database, based on the /data/sales/ folder in DBFS storage.\n\nCREATE TABLE salesdata.salesorders\n(\n    orderid INT,\n    orderdate DATE,\n    customerid INT,\n    ordertotal DECIMAL\n)\nUSING DELTA\nLOCATION '/data/sales/';\n\n\n Tip\n\nFor more information, see CREATE TABLE in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nData analysts often use SQL to query relational data and create reports and dashboards. Azure Databricks provides support for SQL-based data analytics through SQL Warehouses and the SQL persona.\n\nIn this module, you'll learn how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\n\n Note\n\nSQL Warehouses and the SQL persona are available in premium-tier Azure Databricks workspaces.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get started with SQL Warehouses - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/02-sql-warehouses",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet started with SQL Warehouses\n3 minutes\n\nSQL Warehouses (formerly known as SQL Endpoints) provide a relational database interface for data in Azure Databricks. The data is stored in files that are abstracted by Delta tables in a hive metastore, but from the perspective of the user or client application, the SQL Warehouse behaves like a relational database.\n\nCreating a SQL Warehouse\n\nWhen you create a premium-tier Azure Databricks workspace, it includes a default SQL Warehouse named Starter Warehouse, which you can use to explore sample data and get started with SQL-based data analytics in Azure Databricks. You can modify the configuration of the default SQL Warehouse to suit your needs, or you can create more SQL Warehouses in your workspace.\n\nYou can manage the SQL Warehouses in your Azure Databricks workspace by using the Azure Databricks portal in the SQL persona view.\n\nSQL Warehouse configuration settings\n\nWhen you create or configure a SQL Warehouse, you can specify the following settings:\n\nName: A name used to identify the SQL Warehouse.\nCluster size: Choose from a range of standard sizes to control the number and size of compute resources used to support the SQL Warehouse. Available sizes range from 2X-Small (a single worker node) to 4X-Large (256 worker nodes). For more information, see Cluster size in the Azure Databricks documentation.\nAuto Stop: The amount of time the cluster will remain running when idle before being stopped. Idle clusters continue to incur charges when running.\nScaling: The minimum and maximum number of clusters used to distribute query processing.\nType: You can create a SQL Warehouse that uses serverless compute for fast, cost-effective on-demand provisioning. Alternatively, you can create a Pro or Classic SQL warehouse.\n\n Note\n\nYou can create a SQL Warehouse with any available size, but if you have insufficient quota for the number of cores required to support your choice in the region where Azure Databricks is provisioned, the SQL Warehouse will fail to start.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use SQL Warehouses in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Data engineering with Azure Databricks \n800 XP\nUse SQL Warehouses in Azure Databricks\n44 min\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nAzure Databricks provides SQL Warehouses that enable data analysts to work with data using familiar relational SQL queries.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nCreate and configure SQL Warehouses in Azure Databricks.\nCreate databases and tables.\nCreate queries and dashboards.\nStart\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nThis module is part of these learning paths\nData engineering with Azure Databricks\nIntroduction\n1 min\nGet started with SQL Warehouses\n3 min\nCreate databases and tables\n3 min\nCreate queries and dashboards\n3 min\nExercise - Use a SQL Warehouse in Azure Databricks\n30 min\nKnowledge check\n3 min\nSummary\n1 min\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/07-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich of the following descriptions best fits Delta Lake?\n\n \n\nA Spark API for exporting data from a relational database into CSV files.\n\nA relational storage layer for Spark that supports tables based on Parquet files.\n\nA synchronization solution that replicates data between SQL Server and Spark clusters.\n\n2. \n\nYou've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?\n\n \n\nCSV\n\nPARQUET\n\nDELTA\n\n3. \n\nWhat feature of Delta Lake enables you to retrieve data from previous versions of a table?\n\n \n\nSpark Structured Streaming\n\nTime Travel\n\nCatalog Tables\n\n4. \n\nYou have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?\n\n \n\nThe table metadata and data files will be deleted.\n\nThe table metadata will be removed from the catalog, but the data files will remain intact.\n\nThe table metadata will remain in the catalog, but the data files will be deleted.\n\n5. \n\nWhen using Spark Structured Streaming, a Delta Lake table can be which of the following?\n\n \n\nOnly a source\n\nOnly a sink\n\nEither a source or a sink\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/08-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nDelta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.\n\nIn this module, you learned how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Delta Lake in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/06-exercise-use-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Delta Lake in Azure Databricks\n40 minutes\n\nNow it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark cluster in Azure Databricks to create and query Delta Lake tables.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake for streaming data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/05-use-delta-lake-streaming-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake for streaming data\n5 minutes\n\nAll of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n\nSpark Structured Streaming\n\nA typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.\n\nSpark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n\n Tip\n\nFor more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation.\n\nStreaming with Delta Lake tables\n\nYou can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.\n\nUsing a Delta Lake table as a streaming source\n\nIn the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"/delta/internetorders\")\n\n# Now you can process the streaming data in the dataframe\n# for example, show it:\nstream_df.show()\n\n\n Note\n\nWhen using a Delta Lake table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option.\n\nAfter reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n\nUsing a Delta Lake table as a streaming sink\n\nIn the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format {\"device\":\"Dev1\",\"status\":\"ok\"} New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\nstreamFolder = '/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = '/delta/devicetable'\ncheckpoint_path = '/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n\n\n Note\n\nThe checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.\n\nAfter the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:\n\n%sql\n\nCREATE TABLE DeviceTable\nUSING DELTA\nLOCATION '/delta/devicetable';\n\nSELECT device, status\nFROM DeviceTable;\n\n\nTo stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:\n\ndelta_stream.stop()\n\n\n Tip\n\nFor more information about using Delta Lake tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create and query catalog tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/04-catalog-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate and query catalog tables\n5 minutes\n\nSo far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark cluster, and work with them using SQL.\n\nExternal vs managed tables\n\nTables in a Spark catalog, including Delta Lake tables, can be managed or external; and it's important to understand the distinction between these kinds of table.\n\nA managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.\nAn external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.\nCreating catalog tables\n\nThere are several ways to create catalog tables.\n\nCreating a catalog table from a dataframe\n\nYou can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:\n\n# Save a dataframe as a managed table\ndf.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n\n## specify a path option to save as an external table\ndf.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n\nCreating a catalog table using SQL\n\nYou can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables. You can run the statement using the SparkSQL API, like the following example:\n\nspark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n\n\nAlternatively you can use the native SQL support in Spark to run the statement:\n\n%sql\n\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION '/mydata'\n\n\n Tip\n\nThe CREATE TABLE statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a CREATE TABLE IF NOT EXISTS statement or the CREATE OR REPLACE TABLE statement.\n\nDefining the table schema\n\nIn all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:\n\n%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n\n\nWhen using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.\n\nUsing the DeltaTableBuilder API\n\nYou can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:\n\nfrom delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"default.ManagedProducts\") \\\n  .addColumn(\"Productid\", \"INT\") \\\n  .addColumn(\"ProductName\", \"STRING\") \\\n  .addColumn(\"Category\", \"STRING\") \\\n  .addColumn(\"Price\", \"FLOAT\") \\\n  .execute()\n\n\nSimilarly to the CREATE TABLE SQL statement, the create method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the createIfNotExists or createOrReplace method.\n\nUsing catalog tables\n\nYou can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:\n\n%sql\n\nSELECT orderid, salestotal\nFROM ManagedSalesOrders\n\n\n Tip\n\nFor more information about working with Delta Lake, see Table batch reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create Delta Lake tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate Delta Lake tables\n5 minutes\n\nDelta lake is built on tables, which provide a relational storage abstraction over files in a data lake.\n\nCreating a Delta Lake table from a dataframe\n\nOne of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored.\n\nFor example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:\n\n# Load a file into a dataframe\ndf = spark.read.load('/data/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndelta_table_path = \"/delta/mydata\"\ndf.write.format(\"delta\").save(delta_table_path)\n\n\nAfter saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a _delta_log folder containing the transaction log for the table.\n\n Note\n\nThe transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.\n\nYou can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:\n\nnew_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n\n\nYou can also add rows from a dataframe to an existing table by using the append mode:\n\nnew_rows_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n\nMaking conditional updates\n\nWhile you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations. For example, you could use the following code to update the price column for all rows with a category column value of \"Accessories\":\n\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n\n\nThe data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.\n\n Tip\n\nFor more information about using the Data Lake API, see the Delta Lake API documentation.\n\nQuerying a previous version of a table\n\nDelta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.\n\nYou can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n\n\nAlternatively, you can specify a timestamp by using the timestampAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get Started with Delta Lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/02-understand-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet Started with Delta Lake\n3 minutes\n\nDelta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.\n\nThe benefits of using Delta Lake in Azure Databricks include:\n\nRelational tables that support querying and data modification. With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.\nSupport for ACID transactions. Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\nData versioning and time travel. Because all transactions are logged in the transaction log, you can track multiple versions of each table row, and even use the time travel feature to retrieve a previous version of a row in a query.\nSupport for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.\nStandard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines.\n\n Tip\n\nFor more information about Delta Lake in Azure Databricks, see the Delta Lake guide in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nLinux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\n\n Note\n\nThe version of Delta Lake available in an Azure Databricks cluster depends on the version of the Databricks Runtime being used. The information in this module reflects Delta Lake version 1.2.1, which is installed with Databricks Runtime version 10.5 - 11.0.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Delta Lake in Azure Databricks\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nDelta Lake is an open source relational storage area for Spark that you can use to implement a data lakehouse architecture in Azure Databricks.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in Azure Databricks.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nGet Started with Delta Lake\nmin\nCreate Delta Lake tables\nmin\nCreate and query catalog tables\nmin\nUse Delta Lake for streaming data\nmin\nExercise - Use Delta Lake in Azure Databricks\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/09-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark is a key technology used in big data analytics, and the Spark support in Azure Databricks enables you to combine big data processing in Spark with large-scale data analytics.\n\nIn this module, you learned how to:\n\nDescribe key elements of the Apache Spark architecture.\nCreate and configure a Spark cluster.\nDescribe use cases for Spark.\nUse Spark to process and analyze data stored in files.\nUse Spark to visualize data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/08-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition best describes Apache Spark?\n\n \n\nA highly scalable relational database management system.\n\nA virtual server with a Python runtime.\n\nA distributed platform for parallel data processing using multiple languages.\n\n2. \n\nYou need to use Spark to analyze data in a parquet file. What should you do?\n\n \n\nLoad the parquet file into a dataframe.\n\nImport the data into a table in a serverless SQL pool.\n\nConvert the data to CSV format.\n\n3. \n\nYou want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?\n\n \n\n%spark\n\n%pyspark\n\n%sql\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Spark in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/07-exercise-databricks-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Spark in Azure Databricks\n45 minutes\n\nNow it's your opportunity to use a Spark cluster in Azure Databricks. In this exercise, you'll use a provided script to provision an Azure Databricks workspace in your Azure subscription; and then create a Spark cluster and use a notebook to analyze and visualize data from files in the databricks file system (DBFS).\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/06-visualize-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use Apache Spark in Azure Databricks \nAdd\nPrevious\nUnit 6 of 9\nNext\nVisualize data\nCompleted\n100 XP\n6 minutes\n\nOne of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Databricks provide charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.\n\nUsing built-in notebook charts\n\nWhen you display a dataframe or run a SQL query in a Spark notebook in Azure Databricks, the results are displayed under the code cell. By default, results are rendered as a table, but you can also view the results as a visualization and customize how the chart displays the data, as shown here:\n\nThe built-in visualization functionality in notebooks is useful when you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.\n\nUsing graphics packages in code\n\nThere are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n\nFor example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.\n\nPython\nCopy\nfrom matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n\n\nThe Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n\nThe chart produced by the code would look similar to the following image:\n\nYou can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.\n\n Note\n\nThe Matplotlib and Seaborn libraries may already be installed on Databricks clusters, depending on the Databricks Runtime for the cluster. If not, or if you want to use a different library that is not already installed, you can add it to the cluster. See Cluster Libraries in the Azure Databricks documentation for details.\n\nNext unit: Exercise - Use Spark in Azure Databricks\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark to work with data files - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/05-write-spark-code",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark to work with data files\n5 minutes\n\nOne of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Databricks Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java.\n\nExploring data with dataframes\n\nNatively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.\n\n Note\n\nIn addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.\n\nLoading data into a dataframe\n\nLet's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the data folder in your Databricks File System (DBFS) storage:\n\nProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nIn a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:\n\n%pyspark\ndf = spark.read.load('/data/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n\n\nThe %pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. Here's the equivalent Scala code for the products data example:\n\n%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/data/products.csv\")\ndisplay(df.limit(10))\n\n\nThe magic %spark is used to specify Scala.\n\n Tip\n\nYou can also select the language you want to use for each cell in the Notebook interface.\n\nBoth of the examples shown previously would produce output like this:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nSpecifying a dataframe schema\n\nIn the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nThe following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('/data/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n\n\nThe results would once again be similar to:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nFiltering and grouping dataframes\n\nYou can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:\n\npricelist_df = df.select(\"ProductID\", \"ListPrice\")\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductID\tListPrice\n771\t3399.9900\n772\t3399.9900\n773\t3399.9900\n...\t...\n\nIn common with most data manipulation methods, select returns a new dataframe object.\n\n Tip\n\nSelecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n\npricelist_df = df[\"ProductID\", \"ListPrice\"]\n\nYou can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n\nbikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\n\nTo group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:\n\ncounts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nCategory\tcount\nHeadsets\t3\nWheels\t14\nMountain Bikes\t32\n...\t...\nUsing SQL expressions in Spark\n\nThe Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\n\nCreating database objects in the Spark catalog\n\nThe Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\n\nOne of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:\n\ndf.createOrReplaceTempView(\"products\")\n\n\nA view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.\n\n Note\n\nWe won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:\n\nYou can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.\nYou can save a dataframe as a table by using its saveAsTable method.\nYou can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.\nUsing the Spark SQL API to query data\n\nYou can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.\n\nbikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n\n\nThe results from the code example would look similar to the following table:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\nUsing SQL code\n\nThe previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %sql magic to run SQL code that queries objects in the catalog, like this:\n\n%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n\n\nThe SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:\n\nExpand table\nCategory\tProductCount\nBib-Shorts\t3\nBike Racks\t1\nBike Stands\t1\n...\t...\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a Spark cluster - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/03-spark-cluster",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Use Apache Spark in Azure Databricks \nAdd\nPrevious\nUnit 3 of 9\nNext\nCreate a Spark cluster\nCompleted\n100 XP\n3 minutes\n\nYou can create one or more clusters in your Azure Databricks workspace by using the Azure Databricks portal.\n\nWhen creating the cluster, you can specify configuration settings, including:\n\nA name for the cluster.\nA cluster mode, which can be:\nStandard: Suitable for single-user workloads that require multiple worker nodes.\nHigh Concurrency: Suitable for workloads where multiple users will be using the cluster concurrently.\nSingle Node: Suitable for small workloads or testing, where only a single worker node is required.\nThe version of the Databricks Runtime to be used in the cluster; which dictates the version of Spark and individual components such as Python, Scala, and others that get installed.\nThe type of virtual machine (VM) used for the worker nodes in the cluster.\nThe minimum and maximum number of worker nodes in the cluster.\nThe type of VM used for the driver node in the cluster.\nWhether the cluster supports autoscaling to dynamically resize the cluster.\nHow long the cluster can remain idle before being shut down automatically.\nHow Azure manages cluster resources\n\nWhen you create an Azure Databricks workspace, a Databricks appliance is deployed as an Azure resource in your subscription. When you create a cluster in the workspace, you specify the types and sizes of the virtual machines (VMs) to use for both the driver and worker nodes, and some other configuration options, but Azure Databricks manages all other aspects of the cluster.\n\nThe Databricks appliance is deployed into Azure as a managed resource group within your subscription. This resource group contains the driver and worker VMs for your clusters, along with other required resources, including a virtual network, a security group, and a storage account. All metadata for your cluster, such as scheduled jobs, is stored in an Azure Database with geo-replication for fault tolerance.\n\nInternally, Azure Kubernetes Service (AKS) is used to run the Azure Databricks control-plane and data-planes via containers running on the latest generation of Azure hardware (Dv3 VMs), with NvMe SSDs capable of blazing 100us latency on high-performance Azure virtual machines with accelerated networking. Azure Databricks utilizes these features of Azure to further improve Spark performance. After the services within your managed resource group are ready, you can manage the Databricks cluster through the Azure Databricks UI and through features such as auto-scaling and auto-termination.\n\n Note\n\nYou also have the option of attaching your cluster to a pool of idle nodes to reduce cluster startup time. For more information, see Pools in the Azure Databricks documentation.\n\nNext unit: Use Spark in notebooks\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark in notebooks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/04-use-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark in notebooks\n6 minutes\n\nYou can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:\n\nBatch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.\nInteractive analytics sessions to explore, analyze, and visualize data.\nRunning Spark code in notebooks\n\nAzure Databricks includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Databricks is similar to that of Jupyter notebooks - a popular open source notebook platform.\n\nNotebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:\n\nSyntax highlighting and error support.\nCode auto-completion​.\nInteractive data visualizations.\nThe ability to export results.\n\n Tip\n\nTo learn more about working with notebooks in Azure Databricks, see the Notebooks article in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get to know Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/02-understand-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet to know Spark\n4 minutes\n\nTo gain a better understanding of how to process and analyze data with Apache Spark in Azure Databricks, it's important to understand the underlying architecture.\n\nHigh-level overview\n\nFrom a high level, the Azure Databricks service launches and manages Apache Spark clusters within your Azure subscription. Apache Spark clusters are groups of computers that are treated as a single computer and handle the execution of commands issued from notebooks. Clusters enable processing of data to be parallelized across many computers to improve scale and performance. They consist of a Spark driver and worker nodes. The driver node sends work to the worker nodes and instructs them to pull data from a specified data source.\n\nIn Databricks, the notebook interface is typically the driver program. This driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations to those datasets. Driver programs access Apache Spark through a SparkSession object regardless of deployment location.\n\nMicrosoft Azure manages the cluster, and auto-scales it as needed based on your usage and the setting used when configuring the cluster. Auto-termination can also be enabled, which allows Azure to terminate the cluster after a specified number of minutes of inactivity.\n\nSpark jobs in detail\n\nWork submitted to the cluster is split into as many independent jobs as needed. This is how work is distributed across the Cluster's nodes. Jobs are further subdivided into tasks. The input to a job is partitioned into one or more partitions. These partitions are the unit of work for each slot. In between tasks, partitions may need to be reorganized and shared over the network.\n\nThe secret to Spark's high performance is parallelism. Scaling vertically (by adding resources to a single computer) is limited to a finite amount of RAM, Threads and CPU speeds; but clusters scale horizontally, adding new nodes to the cluster as needed.\n\nSpark parallelizes jobs at two levels:\n\nThe first level of parallelization is the executor - a Java virtual machine (JVM) running on a worker node, typically, one instance per node.\nThe second level of parallelization is the slot - the number of which is determined by the number of cores and CPUs of each node.\nEach executor has multiple slots to which parallelized tasks can be assigned.\n\nThe JVM is naturally multi-threaded, but a single JVM, such as the one coordinating the work on the driver, has a finite upper limit. By splitting the work into tasks, the driver can assign units of work to *slots in the executors on worker nodes for parallel execution. Additionally, the driver determines how to partition the data so that it can be distributed for parallel processing. So, the driver assigns a partition of data to each task so that each task knows which piece of data it is to process. Once started, each task will fetch the partition of data assigned to it.\n\nJobs and stages\n\nDepending on the work being performed, multiple parallelized jobs may be required. Each job is broken down into stages. A useful analogy is to imagine that the job is to build a house:\n\nThe first stage would be to lay the foundation.\nThe second stage would be to erect the walls.\nThe third stage would be to add the roof.\n\nAttempting to do any of these steps out of order just doesn't make sense, and may in fact be impossible. Similarly, Spark breaks each job into stages to ensure everything is done in the right order.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Databricks offers a highly scalable platform for data analytics and processing using Apache Spark.\n\nSpark is a flexible platform that supports many different programming languages and APIs. Most data processing and analytics tasks can be accomplished using the Dataframe API, which is what we'll focus on in this module.\n\nIn this module, you'll learn how to:\n\nDescribe key elements of the Apache Spark architecture.\nCreate and configure a Spark cluster.\nDescribe use cases for Spark.\nUse Spark to process and analyze data stored in files.\nUse Spark to visualize data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Apache Spark in Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Apache Spark in Azure Databricks\nModule\n9 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nAzure Databricks is built on Apache Spark and enables data engineers and analysts to run Spark jobs to transform, analyze and visualize data at scale.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe key elements of the Apache Spark architecture.\nCreate and configure a Spark cluster.\nDescribe use cases for Spark.\nUse Spark to process and analyze data stored in files.\nUse Spark to visualize data.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.\n\nIntroduction\nmin\nGet to know Spark\nmin\nCreate a Spark cluster\nmin\nUse Spark in notebooks\nmin\nUse Spark to work with data files\nmin\nVisualize data\nmin\nExercise - Use Spark in Azure Databricks\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/07-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Databricks is a scalable platform for data analytics in Microsoft Azure. You can use Azure Databricks to build highly scalable solutions for data science and engineering, machine learning, and SQL-based data analytics.\n\nIn this module, you learned how to:\n\nProvision an Azure Databricks workspace.\nIdentify core workloads and personas for Azure Databricks.\nDescribe key concepts of an Azure Databricks solution.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/06-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou plan to create an Azure Databricks workspace and use it to work with a SQL Warehouse. Which of the following pricing tiers can you select?\n\n \n\nEnterprise\n\nStandard\n\nPremium\n\n2. \n\nYou've created an Azure Databricks workspace in which you plan to use code to process data files. What must you create in the workspace?\n\n \n\nA SQL Warehouse\n\nA Spark cluster\n\nA Windows Server virtual machine\n\n3. \n\nYou want to use Python code to interactively explore data in a text file that you've uploaded to your Azure Databricks workspace. What should you create?\n\n \n\nA SQL query\n\nAn Azure function\n\nA Notebook\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/05-exercise-explore-databricks",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore Azure Databricks\n30 minutes\n\nNow it's your chance to explore Azure Databricks for yourself. In this exercise, you'll use a provided script to provision an Azure Databricks workspace in your Azure subscription; and then use the Azure Databricks portal to create a Spark cluster and perform some common data analytics tasks.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Identify Azure Databricks workloads - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/03-workloads",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIdentify Azure Databricks workloads\n3 minutes\n\nAzure Databricks is a comprehensive platform that offers many data processing capabilities. While you can use the service to support any workload that requires scalable data processing, Azure Databricks is optimized for three specific types of data workload and associated user personas:\n\nData Science and Engineering\nMachine Learning\nSQL*\n\n*SQL workloads are only available in premium tier workspaces.\n\nData Science and Engineering\n\nAzure Databricks provides Apache Spark based processing and analysis of large volumes of data in a data lake. Data engineers, data scientists, and data analysts can use interactive notebooks to run code in Python, Scala, SparkSQL, or other languages to cleanse, transform, aggregate, and analyze data.\n\nMachine Learning\n\nAzure Databricks supports machine learning workloads that involve data exploration and preparation, training and evaluating machine learning models, and serving models to generate predictions for applications and analyses. Data scientists and ML engineers can use AutoML to quickly train predictive models, or apply their skills with common machine learning frameworks such as SparkML, Scikit-Learn, PyTorch, and Tensorflow. They can also manage the end-to-end machine learning lifecycle with MLFlow.\n\nSQL\n\nAzure Databricks supports SQL-based querying for data stored in tables in a SQL Warehouse. This capability enables data analysts to query, aggregate, summarize, and visualize data using familiar SQL syntax and a wide range of SQL-based data analytical tools.\n\n Note\n\nSQL Warehouses are only available in premium Azure Databricks workspaces.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand key concepts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/04-key-concepts",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand key concepts\n3 minutes\n\nAzure Databricks is an amalgamation of multiple technologies that enable you to work with data at scale. Before using Azure Databricks, there are some key concepts that you should understand.\n\nApache Spark clusters - Spark is a distributed data processing solution that makes use of clusters to scale processing across multiple compute nodes. Each Spark cluster has a driver node to coordinate processing jobs, and one or more worker nodes on which the processing occurs. This distributed model enables each node to operate on a subset of the job in parallel; reducing the overall time for the job to complete. To learn more about clusters in Azure Databricks, see Clusters in the Azure Databricks documentation.\nDatabricks File System (DBFS) - While each cluster node has its own local file system (on which operating system and other node-specific files are stored), the nodes in a cluster have access to a shared, distributed file system in which they can access and operate on data files. The Databricks File System (DBFS) enables you to mount cloud storage and use it to work with and persist file-based data. To learn more about DBFS, see Databricks File System (DBFS) in the Azure Databricks documentation.\nNotebooks - One of the most common ways for data analysts, data scientists, data engineers, and developers to work with Spark is to write code in notebooks. Notebooks provide an interactive environment in which you can combine text and graphics in Markdown format with cells containing code that you run interactively in the notebook session. To learn more about notebooks, see Notebooks in the Azure Databricks documentation.\nHive metastore - Hive is an open source technology used to define a relational abstraction layer of tables over file-based data. The tables can then be queried using SQL syntax. The table definitions and details of the file system locations on which they're based is stored in the metastore for a Spark cluster. A Hive metastore is created for each cluster when it's created, but you can configure a cluster to use an existing external metastore if necessary - see Metastores in the Azure Databricks documentation for more details.\nDelta Lake - Delta Lake builds on the relational table schema abstraction over files in the data lake to add support for SQL semantics commonly found in relational database systems. Capabilities provided by Delta Lake include transaction logging, data type constraints, and the ability to incorporate streaming data into a relational table. To learn more about Delta Lake, see Delta Lake Guide in the Azure Databricks documentation.\nSQL Warehouses - SQL Warehouses are relational compute resources with endpoints that enable client applications to connect to an Azure Databricks workspace and use SQL to work with data in tables. The results of SQL queries can be used to create data visualizations and dashboards to support business analytics and decision making. SQL Warehouses are only available in premium tier Azure Databricks workspaces. To learn more about SQL Warehouses, see SQL Warehouses in the Azure Databricks documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get started with Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/02-azure-databricks",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet started with Azure Databricks\n3 minutes\n\nAzure Databricks is a cloud-based distributed platform for data processing built on Apache Spark. Databricks was designed to unify data science, data engineering, and business data analytics on Spark by creating an easy to use environment that enables users to spend more time working effectively with data, and less time focused on managing clusters and infrastructure. As the platform has evolved, it has kept up to date with the latest advances in the Spark runtime and added usability features to support common data workloads in a single, centrally managed interface.\n\nAzure Databricks is hosted on the Microsoft Azure cloud platform, and integrated with Azure services such as Microsoft Entra ID, Azure Storage, Azure Synapse Analytics, and Azure Machine Learning. Organizations can apply their existing capabilities with the Databricks platform, and build fully integrated data analytics solutions that work with cloud infrastructure used by other enterprise applications.\n\nCreating an Azure Databricks workspace\n\nTo use Azure Databricks, you must create an Azure Databricks workspace in your Azure subscription. You can accomplish this by:\n\nUsing the Azure portal user interface.\nUsing an Azure Resource Manager (ARM) or Bicep template.\nUsing the New-AzDatabricksWorkspace Azure PowerShell cmdlet\nUsing the az databricks workspace create Azure command line interface (CLI) command.\n\nWhen you create a workspace, you must specify one of the following pricing tiers:\n\nStandard - Core Apache Spark capabilities with Microsoft Entra integration.\nPremium - Role-based access controls and other enterprise-level features.\nTrial - A 14-day free trial of a premium-level workspace\n\nUsing the Azure Databricks portal\n\nAfter you've provisioned an Azure Databricks workspace, you can use the Azure Databricks portal to work with data and compute resources. The Azure Databricks portal is a web-based user interface through which you can create and manage workspace resources (such as Spark clusters) and use notebooks and queries to work with data in files and tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Databricks is a fully managed, cloud-based data analytics platform, which empowers developers to accelerate AI and innovation by simplifying the process of building enterprise-grade data applications. Built as a joint effort by Microsoft and the team that started Apache Spark, Azure Databricks provides data science, engineering, and analytical teams with a single platform for big data processing and machine learning.\n\nBy combining the power of Databricks, an end-to-end, managed Apache Spark platform optimized for the cloud, with the enterprise scale and security of Microsoft's Azure platform, Azure Databricks makes it simple to run large-scale Spark workloads.\n\nIn this module, you'll learn how to:\n\nProvision an Azure Databricks workspace.\nIdentify core workloads and personas for Azure Databricks.\nDescribe key concepts of an Azure Databricks solution.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore Azure Databricks - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nExplore Azure Databricks\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Databricks\n\nAzure Databricks is a cloud service that provides a scalable platform for data analytics using Apache Spark.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nProvision an Azure Databricks workspace.\nIdentify core workloads and personas for Azure Databricks.\nDescribe key concepts of an Azure Databricks solution.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a fundamental knowledge of data analytics concepts. Consider completing Azure Data Fundamentals certification before starting this module.\n\nIntroduction\nmin\nGet started with Azure Databricks\nmin\nIdentify Azure Databricks workloads\nmin\nUnderstand key concepts\nmin\nExercise - Explore Azure Databricks\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/08-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nComprehensive data governance is an important element of an enterprise data analytics solution. By combining Azure Synapse Analytics and Microsoft Purview, you can improve data discoverability while addressing data trustworthiness and compliance requirements across your data estate.\n\nIn this module you learned how to:\n\nCatalog Azure Synapse Analytics database assets in Microsoft Purview.\nConfigure Microsoft Purview integration in Azure Synapse Analytics.\nSearch the Microsoft Purview catalog from Synapse Studio.\nTrack data lineage in Azure Synapse Analytics pipelines activities.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/07-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n4 minutes\n1. \n\nYou want to scan data assets in a dedicated SQL pool in your Azure Synapse Analytics workspace. What kind of source should you register in Microsoft Purview?\n\n \n\nAzure Synapse Analytics\n\nAzure Data Lake Storage Gen2\n\nAzure SQL Database\n\n2. \n\nYou want to scan data assets in the default data lake used by your Azure Synapse Analytics workspace. What kind of source should you register in Microsoft Purview?\n\n \n\nAzure Synapse Analytics\n\nAzure Data Lake Storage Gen2\n\nAzure Cosmos DB\n\n3. \n\nYou want data analysts using Synapse Studio to be able to find data assets that are registered in a Microsoft Purview collection. What should you do?\n\n \n\nRegister an Azure Synapse Analytics source in the Purview account\n\nAdd a Data Explorer pool to the Synapse Workspace\n\nConnect the Purview account to the Synapse analytics workspace\n\n4. \n\nWhich of the following pipeline activities records data lineage data in a connected Purview account?\n\n \n\nGet Metadata\n\nCopy Data\n\nLookup\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Integrate Azure Synapse Analytics and Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/06-exercise-synapse-purview",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Integrate Azure Synapse Analytics and Microsoft Purview\n40 minutes\n\nNow it's your chance to explore integration between Microsoft Purview and Azure Synapse Analytics for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and a Microsoft Purview account in your Azure subscription; and then you'll catalog data assets in your Azure Synapse Analytics workspace and data lake before connecting the Purview account to the workspace to support data discovery and lineage tracking.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Track data lineage in pipelines - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/05-track-data-lineage",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTrack data lineage in pipelines\n4 minutes\n\nIn a typical large-scale analytics solution, data is transferred and transformed across multiple systems until it's loaded into an analytical data store for reporting and analysis. Tracking the lineage of data as moves across the enterprise is an important factor in determining the provenance, trustworthiness, and recency of data assets used to inform analysis and decision making.\n\nGenerate and view data lineage information\n\nIn Azure Synapse Analytics, data movement and transformation is managed by using pipelines, which consist of an orchestrated set of activities that operate on data. The design and implementation of pipelines is too large a subject to cover in depth in this module, but a key point to be aware of is that there are two activity types available in Synapse Analytics pipelines that automatically generate data lineage information in a connected Purview catalog:\n\nThe Copy Data activity\nThe Data Flow activity\n\nRunning a pipeline that includes either of these activities in a workspace with a connected Purview account will result in the creation or update of data assets with lineage information. The assets recorded include:\n\nThe source from which the data is extracted.\nThe activity used to transfer the data.\nThe destination where the data is stored.\n\nIn the Microsoft Purview Governance Portal, you can open the assets in the Purview catalog, and view the lineage information as shown here:\n\nYou can also view the lineage for a pipeline activity in Synapse Studio.\n\n Tip\n\nFor more information about tracking data lineage for Azure Synapse Analytics pipelines in Microsoft Purview, see How to get lineage from Azure Synapse Analytics into Microsoft Purview.\n\nYou'll get a chance to generate and view data lineage from a Synapse Analytics pipeline in the exercise later in this module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Search a Purview catalog in Synapse Studio - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/04-search-purview",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSearch a Purview catalog in Synapse Studio\n4 minutes\n\nAfter connecting an Azure Synapse Analytics workspace to a Microsoft Purview account, you can search the Purview catalog from Synapse Studio. This ability to discover and examine data assets from across the enterprise can greatly assist data engineers, data analysts, and other consumers of data by providing a curated catalog of documented data sources for analysis and reporting.\n\nSearch the Purview catalog in Synapse Studio\n\nYou can search the catalog from a connected Purview account by using the Search bar in the Data, Develop, or Integrate pages in Synapse Studio, as shown here:\n\nThe search results interface, and the details for each asset found reflect the user interface in the Microsoft Purview Governance Portal, ensuring that the data discovery and examination experience in Synapse Studio is consistent for users of Microsoft Purview in its own portal.\n\n Tip\n\nFor more information about searching the Purview catalog in Synapse Studio, see Discover, connect, and explore data in Synapse using Microsoft Purview.\n\nYou'll get a chance to try searching a connected Purview account for yourself in the exercise later in this module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Connect Microsoft Purview to an Azure Synapse Analytics workspace - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/03-configure-purview-integration",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConnect Microsoft Purview to an Azure Synapse Analytics workspace\n5 minutes\n\nSo far, you've learned how you can use Azure Synapse Analytics data stores as sources for a Microsoft Purview catalog; which is similar in most respects to using any other data source.\n\nWhat sets Azure Synapse Analytics apart from many other data sources is the ability to configure direct integration between an Azure Synapse Analytics workspace and a Microsoft Purview account. By linking your workspace to a Purview account, you can:\n\nSearch the Purview catalog in the Synapse Studio user interface.\nPush details of data pipeline activities to Purview in order to track data lineage information.\nConnect a Purview account to a Synapse Analytics workspace\n\nYou connect a Microsoft Purview account to an Azure Synapse Analytics workspace on the Manage page of Synapse Studio, as shown here:\n\nSecurity considerations\n\nTo connect a Purview account by using the Synapse Studio interface, you require Collection Administrator access to the Purview account's root collection. After successfully connecting the account, the managed identity used by your Azure Synapse Analytics workspace will be added to the collection's Data Curator role.\n\nIf your Microsoft Purview account is behind a firewall, you need to create a managed endpoint, and configure the connection to access Purview using that endpoint. For more information, see Access a secured Microsoft Purview account from Azure Synapse Analytics.\n\n Tip\n\nTo learn more about connecting Azure Synapse Analytics to Microsoft Purview, see QuickStart: Connect a Synapse workspace to a Microsoft Purview account.\n\nYou'll get a chance to connect an Azure Synapse Analytics workspace to a Microsoft Purview account in the exercise later in this module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Catalog Azure Synapse Analytics data assets in Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/02-catalog-azure-synapse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCatalog Azure Synapse Analytics data assets in Microsoft Purview\n8 minutes\n\nAzure Synapse Analytics is a platform for cloud-scale analytics workloads that process data in multiple sources; including:\n\nRelational databases in serverless and dedicated SQL pools\nFiles in Azure Data Lake Storage Gen2\n\nA comprehensive data analytics solution can include many folders and files in a data lake, and multiple databases that each contain many tables, each with multiple fields. For a data analyst, finding and understanding the data assets associated with a Synapse Analytics workspace can present a significant challenge before any analysis or reporting can even begin.\n\nMicrosoft Purview can help in this scenario by cataloging the data assets in a data map, and enabling data stewards to add metadata, categorization, subject matter contact details, and other information that helps data analysts identify and understand data.\n\nConfigure data access for Microsoft Purview\n\nIn order to scan the data assets in the data lake storage and databases used in your Azure Synapse Workspace, Microsoft Purview must have appropriate permissions to read the data. In practice, this means that the account used by your Microsoft Purview account (usually a system-assigned managed identity that is created when Microsoft Purview is provisioned) needs to be a member of the appropriate role-based access control (RBAC) and database roles.\n\nThe diagram shows that Microsoft Purview requires role membership that permits the following access:\n\nRead access to the Azure Synapse workspace (achieved through membership of the Reader role for the Azure Synapse Workspace resource in the Azure subscription).\nRead access to each SQL database that will be scanned (achieved through membership of the db_datareader fixed database role in each database).\nRead access to data lake storage (achieved through membership of the Storage Blob Data Reader role for the Azure Storage account hosting the Azure Data Lake Storage Gen2 container for the data lake).\n\n Tip\n\nLearn more:\n\nFor more information about RBAC in Microsoft Azure, see What is Azure role-based access control (Azure RBAC)?\nFor more information about database-level roles in Azure Synapse Analytics SQL pools, see Database-level roles.\n\nYou'll get a chance to assign RBAC and SQL database role membership to support Microsoft Purview data access for yourself in the exercise later in this module.\n\nRegister and scan data sources\n\nMicrosoft Purview supports the creation of a data map that catalogs data assets in collections by scanning registered sources. Collections form a hierarchy of logical groupings of related data assets, under a root collection that is created when you provision a Microsoft Purview account. You can use the Microsoft Purview Governance Portal to create and manage collections in your account.\n\nTo include assets from a particular data source, you need to register the source in a collection. Microsoft Purview supports many kinds of source, including:\n\nAzure Synapse Analytics - One or more SQL databases in a Synapse Analytics workspace.\nAzure Data Lake Storage Gen2 - Blob containers used to host folders and files in a data lake.\n\nTo catalog assets used in an Azure Synapse Analytics workspace, you can register one or both of these sources in a collection, as shown here:\n\nAfter registering the sources where your data assets are stored, you can scan each source to catalog the assets it contains. You can scan each source interactively, and you can schedule period scans to keep the data map up to date.\n\n Tip\n\nTo learn more about registering and scanning sources, see Scans and ingestion in Microsoft Purview.\n\nYou'll get a chance to register and scan sources for an Azure Synapse Analytics workspace in the exercise later in this module.\n\nView and manage cataloged data assets\n\nAs each scan finds data assets in the registered sources, they're added to the associated collection in the data catalog. You can query the data catalog in the Microsoft Purview Governance Portal to view and filter the data assets, as shown here:\n\nData assets include items in the registered data stores at multiple levels. For example, assets from an Azure Synapse Analytics source include databases, schemas, tables, and individual fields; and assets from an Azure Data Lake Storage Gen 2 source include containers, folders, and files.\n\nYou can view and edit the properties of each asset to add contextual information such as descriptions, contacts for expert help, and other useful metadata. Data assets can also be classified using built-in or custom classifications that match specific patterns of data field to common types of data - for example, passport numbers, credit card numbers, and others.\n\n Tip\n\nTo learn more about data asset classification, see Data classification in the Microsoft Purview governance portal.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/01-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nMicrosoft Purview is a cloud service that provides the basis of a data governance solution in which you can catalog, classify, and track data assets across a large-scale data estate.\n\nAzure Synapse Analytics is a cloud-scale data analytics suite that supports data ingestion and transformation, distributed big data processing and exploration with SQL and Spark, and enterprise data warehousing.\n\nWhen combined, Microsoft Purview and Azure Synapse Analytics can be used to create a comprehensive solution for reliable, massively scalable data analytics with rich data asset discovery and lineage tracking capabilities.\n\nIn this module you'll learn how to:\n\nCatalog Azure Synapse Analytics database assets in Microsoft Purview.\nConfigure Microsoft Purview integration in Azure Synapse Analytics.\nSearch the Microsoft Purview catalog from Synapse Studio.\nTrack data lineage in Azure Synapse Analytics pipelines activities.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Integrate Microsoft Purview and Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntegrate Microsoft Purview and Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\nMicrosoft Purview\n\nLearn how to integrate Microsoft Purview with Azure Synapse Analytics to improve data discoverability and lineage tracking.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nCatalog Azure Synapse Analytics database assets in Microsoft Purview.\nConfigure Microsoft Purview integration in Azure Synapse Analytics.\nSearch the Microsoft Purview catalog from Synapse Studio.\nTrack data lineage in Azure Synapse Analytics pipelines activities.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with both Azure Synapse Analytics and Microsoft Purview. Consider completing the following modules before starting this one:\n\nIntroduction to Azure Synapse Analytics\nIntroduction to Microsoft Purview\nIntroduction\nmin\nCatalog Azure Synapse Analytics data assets in Microsoft Purview\nmin\nConnect Microsoft Purview to an Azure Synapse Analytics workspace\nmin\nSearch a Purview catalog in Synapse Studio\nmin\nTrack data lineage in pipelines\nmin\nExercise - Integrate Azure Synapse Analytics and Microsoft Purview\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n2 minutes\n\nAs data grows in your organization, tracking its origins and evolution becomes more and more difficult. Microsoft Purview makes it possible to see end-to-end lineage of data sources both in Power BI and extending to the data platform.\n\nFrom an enterprise perspective, the ability to scan and view data across your entire Power BI tenant is critical. Microsoft Purview enables users to find trusted data and also to troubleshoot and understand dependencies across the analytics landscape.\n\nLearn more\nRegister and scan a Power BI tenant in Microsoft Purview\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat are the prerequisite steps to register and scan a Power BI tenant in Microsoft Purview?\n\n \n\nSet up authentication between Purview and Power BI, and configure the Power BI tenant.\n\nSet up and deploy a Power BI data gateway.\n\nConfigure the Power BI tenant only.\n\n2. \n\nWhat steps are required in the Power BI admin portal to enable the scanning and display of enhanced metadata?\n\n \n\nThere are no other steps required. Enhanced metadata displays by default.\n\nEnable enhanced metadata scanning in the Azure portal.\n\nEnable an admin API setting in the Power BI admin portal.\n\n3. \n\nWhat details of an asset would be helpful in performing an impact analysis?\n\n \n\nLineage.\n\nProperties.\n\nSchema.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "View Power BI metadata and lineage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/4-view-lineage",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Manage Power BI assets by using Microsoft Purview \nAdd\nPrevious\nUnit 4 of 6\nNext\nView Power BI metadata and lineage\nCompleted\n100 XP\n4 minutes\n\nPurview and Power BI together are powerful, enhancing the ability of the search and browse features to see both the schema and lineage of Power BI assets.\n\nExtend your search with enhanced metadata\n\nMetadata scanning facilitates governance by making it possible to catalog and report on the metadata of your organization's Power BI artifacts. The results of metadata scanning are displayed on the schema tab of the asset.\n\n Note\n\nMetadata scanning must be enabled in the Power BI admin portal. See Set up metadata scanning in your organization to learn more.\n\nAfter performing a search in the Purview Governance Portal, select a Power BI asset from your search result to see the sensitivity labels and endorsement metadata. Additional business metadata includes the dataset user configuration, create datetime, and description.\n\nUnder the Schema tab, you can see the list of all the tables, columns, and measures created inside the Power BI dataset.\n\nFor more detail, selecting a particular field within the schema tab will take you to the details for that field. You can then view the overview, properties, lineage, contacts, and related assets for that particular field.\n\nMetadata scanning requires no special license. It works for all of your tenant metadata, including items that are located in non-Premium workspaces.\n\nIf you'd like more information about assets, you also have the option open the Power BI dataset in the Power BI service for further analytics, root-cause investigation, impact analysis, management tasks, and dataset enrichment.\n\nExtend your search with lineage\n\nIf you're using the search and browse features in Microsoft Purview to find assets for reporting or to troubleshoot existing assets, you likely need more information on where data actually comes from, and what transformation steps it has undergone. The lineage view displays the flow of data from the source through to Power BI assets, including dataflows, datasets, reports, and dashboards.\n\nAlthough you can track data lineage in Power BI, this information is limited to the items in a single workspace. Lineage in Purview enables you to view the movement of data across more than one workspace, in a single view.\n\nLineage enables easy troubleshooting and deeper analysis of analytics projects. You're able to look both up and down-stream, to perform either root cause or impact analysis.\n\nFor example, you can detect the Azure Synapse Analytics pipeline that is responsible for the transformation of the data upstream of Power BI.\n\nIn the Purview Governance Portal, lineage is displayed from the asset you're currently viewing.\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Search and browse Power BI assets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/3-search-browse-assets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSearch and browse Power BI assets\n4 minutes\n\nAfter data is registered and scanned, analysts and data consumers need to be able to find data, view enhanced metadata, and track data lineage. Search and browse in the Purview Data Catalog enables you to quickly find trustworthy data.\n\nAfter scanning your Power BI tenant, you'll see those assets appear in the search results, including underlying data sources.\n\nSearch the Microsoft Purview Data Catalog\n\nFrom the Microsoft Purview Governance Portal, you can type relevant keywords to start discovering assets. In this scenario, you're looking for \"sales.\"\n\nThe screenshot below displays the search result, with all assets corresponding to the keywords entered in the search engine. Notice the appearance of Power BI assets.\n\nYou can fine-tune your search using the filters on the left side of the page. Filters available include source type, keyword, object type, collection, classification, contact, label, and glossary term.\n\nBrowse the Microsoft Purview Data Catalog\n\nSearching for specific assets is great if you know what you're looking for, but analysts and data consumers may not know exactly how their data estate is structured. The browse experience enables you to explore what data is available, either by collection or through traversing the hierarchy of each data source in the catalog.\n\nTo access the browse experience, select Browse assets from the governance portal home page.\n\nYou can browse the data catalog either by collection or by source type, depending on your needs. Browsing by either collection or source type allows you to see assets you have access to. Once you find the asset you're looking for, you can select it to see details on schema, lineage, and a detailed classification list.\n\nUniquely, browsing by source type allows you to see the hierarchies of data sources using an explorer view. This is a helpful and familiar way to navigate to see lists of scanned assets.\n\n Note\n\nAssets in Purview are organized by collection and permissions are granted at collection level. Both searching and browsing require data reader permissions. See Access control in the Microsoft Purview Data Map for details on permissions.\n\nSelect an asset to see details about the properties, schema, lineage, contacts, and related assets.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Register and scan a Power BI tenant - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/2-register-scan-tenant",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Manage Power BI assets by using Microsoft Purview \nAdd\nPrevious\nUnit 2 of 6\nNext\nRegister and scan a Power BI tenant\nCompleted\n100 XP\n4 minutes\n\nTo get an understanding of what is going on in your Power BI tenant, you can perform a full scan in Microsoft Purview to view the schema and lineage of assets across all workspaces. After, you can schedule incremental scans on workspaces that have changed since the previous scan.\n\nThere are a few pre-requisite steps required to scan your Power BI tenant in Microsoft Purview.\n\n Tip\n\nIf you need to create a Microsoft Purview account, see the quickstart guide to create a Microsoft Purview account in the Azure Portal.\n\nEstablish a connection between Microsoft Purview and Power BI\n\nMicrosoft Purview can connect to and scan Power BI either in the same tenant or across tenants. You'll need to set up authentication either by using a Managed Identity or a Delegated Authentication.\n\n Note\n\nSee Register and scan a Power BI tenant to learn more about the set-up and authentication of Power BI connections in same and cross-tenant scenarios.\n\nAuthenticate to Power BI tenant\n\nGive Microsoft Purview permissions to access your Power BI tenant.\n\nIf you're using Managed Identity to authenticate to Power BI, you'll need to create a security group in Microsoft Entra ID, and add your Microsoft Purview managed identity to this security group.\n\nIf a security group containing the Purview managed identity already exists, you can proceed to configuring the Power BI tenant.\n\nConfigure Power BI tenant\n\nNext you need to enable access to Power BI by Microsoft Purview in Power BI itself. This is done by enabling Allow service principals to use read-only Power BI admin APIs in the Power BI admin portal.\n\nRegister and scan Power BI\n\nNow that you've got access set up in both Microsoft Purview and Power BI, you can register and scan your Power BI tenant.\n\nAfter registering the Power BI tenant, initiate the scan by selecting New scan. Give your scan a name and step through the interface, where you'll be able to to exclude personal workspaces, confirm integration runtime and credentials, and select a collection. Test the connection to ensure authentication is set up properly.\n\n Note\n\nIf you're performing the scan, you must be both a Data Source Administrator and a Data Reader. See Access control in the Microsoft Purview Data Map for details on permissions.\n\nYou're able to track the progress of the scan in the data map, and once the scan is complete, you'll be able to search and browse the contents of your entire Power BI tenant!\n\nIf you're having any issues with scanning your Power BI tenant, see Troubleshoot Power BI tenant scans in Microsoft Purview for details and helpful hints.\n\nNext unit: Search and browse Power BI assets\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nAs the landscape of enterprise data continues to grow, it's critical to get an accurate view of your organization's data. Microsoft Purview and Power BI integration enables you to scan your entire Power BI tenant to search and browse Power BI assets, explore enhanced dataset metadata, trace end-to-end data lineage, and drill-down into datasets in Power BI for further analysis.\n\nLearning objectives\n\nIn this module, you will:\n\nRegister and scan a Power BI tenant.\nUse the search and browse functions to find data assets.\nDescribe the schema details and data lineage tracing of Power BI data assets.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage Power BI assets by using Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nManage Power BI assets by using Microsoft Purview\nModule\n6 Units\nFeedback\nBeginner\nData Analyst\nData Engineer\nPower BI\nMicrosoft Purview\n\nImprove data governance and asset discovery using Power BI and Microsoft Purview integration.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nRegister and scan a Power BI tenant.\nUse the search and browse functions to find data assets.\nDescribe the schema details and data lineage tracing of Power BI data assets.\nAdd\nPrerequisites\nFamiliarity with the Azure data ecosystem.\nIntroduction\nmin\nRegister and scan a Power BI tenant\nmin\nSearch and browse Power BI assets\nmin\nView Power BI metadata and lineage\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nData classification in Microsoft Purview is similar to subject tagging, and is used to mark and identify data of a specific type that's found within your data estate during scanning. Classification is based on the business context of the data.\n\nPrior to data classification and labeling, data assets must be registered and scanned in Microsoft Purview. After assets are registered, scanned, classified, and labeled, analysts and other data consumers can easily identify and use data assets.\n\nLearn more\nQuickstart: Create an account in the Microsoft Purview governance portal\nMicrosoft Purview how-to guides\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat level are user permissions set at in Microsoft Purview?\n\n \n\nTenant.\n\nData catalog.\n\nCollection.\n\n2. \n\nWhat are the two types of classification in Microsoft Purview?\n\n \n\nSystem classifications and custom classifications.\n\nMicrosoft Information Protection Sensitivity Labels and system classifications.\n\nCustom classifications and user-defined classifications.\n\n3. \n\nIf a data analyst is looking for a specific resource for reporting, what should they use?\n\n \n\nPurview Data Catalog to search.\n\nThe business glossary.\n\nImport into Power BI and create a custom report.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Search the data catalog - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/4-search-data-catalog",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSearch the data catalog\n3 minutes\n\nA data catalog search can empower business and data analysts to find and interpret data. The data catalog provides intelligent recommendations based on data relationships, business context, and search history. The Purview data catalog can assist data teams by adding business context to assets to drive analytics, AI and ML initiatives.\n\nThe data catalog can be searched by keyword, object type, collection, classification, contact, label, or assigned term. Results can then be sorted by relevance or name.\n\nFor more information about searching for trusted assets for reporting, see Discover trusted data using Microsoft Purview.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Classify and label data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/3-classify-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nClassify and label data\n6 minutes\n\nGlossary terms, classifications and labels are all annotations to a data asset. Each of them have a different meaning in the context of the data catalog.\n\nWhat is data classification?\n\nClassifications are annotations that can be assigned to entities. The flexibility of classifications enables you to use them for multiple scenarios such as:\n\nunderstanding the nature of data stored in the data assets\ndefining access control policies\n\nClassification is based on the business context of the data. For example, you might classify assets by Passport Number, Driver's License Number, Credit Card Number, SWIFT Code, Person’s Name, and so on. Microsoft Purview has more than 200 system classifiers today. Users can also define their own classifiers in the data catalog. As part of the scanning process, classifications are automatically detected and applied as metadata within the Purview Data Catalog.\n\nClassification rules\n\nIn Microsoft Purview, you can apply system or custom classifications on a file, table, or column asset. Microsoft Purview makes use of Regex patterns and bloom filters to classify data. These classifications are then associated with the metadata discovered in the Azure Purview Data Catalog.\n\nMetadata is used to help describe the data that is being scanned and made available in the catalog. During the configuration of a scan set, you can specify classification rules to apply during the scan that will also serve as metadata. The existing classification rules fall under five major categories:\n\nGovernment - covers attributes such as government identity cards, driver license numbers, passport numbers, etc.\nFinancial - covers attributes such as bank account numbers or credit card numbers.\nPersonal - personal information such as a person's age, date of birth, email address, phone number, etc.\nSecurity - attributes like passwords that may be stored.\nMiscellaneous - attributes not covered in the other categories.\nWhy classify data?\n\nA good data governance strategy includes a process to classify data to understand its level of confidentiality, determine if the data source is compliant with various regulations, or how long to retain it for. Classification in Microsoft Purview makes data assets easier to understand, search, and govern. Classification can also help you implement measures to protect sensitive data.\n\nOnce a classification is tagged to a data source after a scan, you can generate reports and insights to gain a stronger understanding of your data estate. Because classification is based on the business context of the data, it can help bridge the gap between the business and the data team.\n\nData classification: system vs. custom classification\n\nMicrosoft Purview supports both system and custom classifications. There are over +200 system classifications available in Microsoft Purview today. Data teams need to know that if necessary classifications aren't available out of the box, they can work with the data stewards to create custom classifications, to meet their own organizational data governance requirements.\n\n Important\n\nFor the entire list of available system classifications, see Supported classifications in Microsoft Purview.\n\nWho creates custom classifications?\n\nPurview Data Curators can create, update, and delete custom classifiers and classification rules. Purview Data Readers can only view classifiers and classification rules.\n\nIn practical terms, Data Curators may not be members of the data team. It is however critical that data team members understand classification to be able to successfully work together and govern data across an organization.\n\nWhat are data labels?\n\nThe Microsoft Purview Data Map supports labeling structured and unstructured data stored across various data sources. This may sound familiar to you from other Microsoft technologies - and may be known as sensitivity labels. The data map extends the use of sensitivity labels from Microsoft Purview Information Protection to assets stored in infrastructure cloud locations and structured data sources.\n\nLabels are defined in Microsoft Purview Information Protection, and you can extend the application to Microsoft Purview Data Catalog.\n\nThe screenshot below shows both data classification and label in the Microsoft Purview Data Catalog. You can see that this Azure SQL table has a column called “CreditCard”:\n\nClassified as “Credit Card Number” because scan detected numbers corresponding to credit card pattern rules.\nLabeled as “Confidential – Finance” because credit card number was defined in your organization as confidential information (and this label brings encryption).\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Register and scan data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/2-register-scan-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Catalog data artifacts by using Microsoft Purview \nAdd\nPrevious\nUnit 2 of 6\nNext\nRegister and scan data\nCompleted\n100 XP\n10 minutes\n\nRegistration and scanning of data enables discoverability of data across an estate.\n\nBefore you can register and scan data, it’s important to understand the concept of collections. In Microsoft Purview Data Catalog, collections are key concept because they drive permissions and asset protection. Collections are also used to understand data estate health and catalog usage and adoption, as featured in the data stewardship section of your Data Estate Insights.\n\nCollections\n\nThe data map is at the core of Microsoft Purview, which keeps an up-to-date map of assets and their metadata across your data estate. To hydrate the data map, you need to register and scan your data sources, which is done at the collection level. Collections support organizational mapping of metadata. By using collections, you can manage and maintain data sources, scans, and assets in a hierarchy instead of a flat structure. Collections allow you to build a custom hierarchical model of your data landscape based on how your organization plans to use Microsoft Purview to govern your landscape.\n\nCollections also provide a security boundary for your metadata in the data map. Access to collections, data sources, and metadata is set up and maintained based on the collection’s hierarchy in Microsoft Purview, following a least-privilege model:\n\nUsers have the minimum amount of access they need to do their jobs.\nUsers don't have access to sensitive data that they don't need.\n\nData sources are registered at the collection level. Scan results can then be sent to this collection or a sub collection. The image below displays the structure of a collection.\n\n Tip\n\nLearn more about Microsoft Purview collections architectures and best practices.\n\nRegister and scan data sources\n\nData governance use begins at collection level, with the registration of data sources in Microsoft Purview governance portal. Microsoft Purview supports an array of data sources. Data teams (analysts, engineers, and scientists) may not be actively registering and scanning data in Microsoft Purview, but it's critical that data consumers understand governance efforts. Registering and scanning assets requires Data Curator permissions.\n\n Important\n\nData registered and scanned in Microsoft Purview only collects metadata information. Data remains in its location and isn't migrated to any other platform.\n\nRegister a data source\n\nRegistering a data source is done from within the Azure portal. Once you have a Microsoft Purview service configured in Azure, you use the Microsoft Purview governance portal to register your data sources.\n\nTo register a data source, you'll select the icon to register a data source as displayed in the image below. Selecting this icon will give you access to all data source connectors.\n\nBelow is a small sample of available connectors in Microsoft Purview Data Catalog. See supported data sources and file types for an up-to-date list of supported data sources and connectors.\n\nRegistering a data source is straightforward, you need to complete the required fields. Authentication will be done during the scanning phase.\n\nEach type of data source you choose will require specific information to complete the registration. For example, if your data sources reside in your Azure subscription, you'll choose the necessary subscription and storage account name.\n\nScan a data source\n\nOnce you have data sources registered in the Microsoft Purview governance portal and displayed in the data map, you can set up scanning. The scanning process can be triggered to run immediately or can be scheduled to run on a periodic basis to keep your Microsoft Purview account up to date.\n\nScanning assets is as simple as selecting New scan from the resource as displayed in the data map.\n\nYou'll now need to configure your scan and assign the following details:\n\nAssign a friendly name.\nDefine which integration runtime to use to perform the scan.\nCreate credentials to authenticate to your registered data sources.\nChoose a collection to send scan results.\n\nAfter the basic configuration, you'll scope your scan, which allows you to choose just a specific zone of your data source. For instance, if you have a collection called “Raw” in your data map, you can define the scope to scan only the raw container of your data lake.\n\nAfter configuring and scoping your scan, you'll define the scan rule set. A scan rule set is a container for grouping a set of scan rules together so that you can easily associate them with a scan. For example, you might create a default scan rule set for each of your data source types, and then use these scan rule sets by default for all scans within your company. You might also want users with the right permissions to create other scan rule sets with different configurations based on business need.\n\nOnce a scan is complete, you can refer to the scan details to view information about the number of scans completed, assets detected, assets classified, Scan information. It’s a good place to monitor scan progress, including success or failure.\n\n Tip\n\nRefer to Scanning best practices for more information on scanning assets.\n\nRoles and permissions\n\nPermissions in Microsoft Purview are assigned at collection level. Collections are used to organize assets and sources and can be thought of as a logical grouping of data assets.\n\nData teams looking to discover and use data need to be assigned the Data Reader role in a collection in Microsoft Purview. The Data Reader role enables users to find assets, but doesn't enable users to edit anything. The Data Curator role is required to edit information about assets, assign classifications, and associate assets with glossary entries. To set up scans via the Microsoft Purview Governance Portal, individuals need to be either a data curator on the collection or data curator and data source administrator where the source is registered.\n\nWhen a Microsoft Purview account is created, it starts with a root collection that has the same name as the Microsoft Purview account itself. The creator of the Microsoft Purview account is automatically added as a Collection Admin, who can then assign Data Source Admin, Data Curator, and Data Reader on this root collection, and can edit and manage this collection.\n\n Tip\n\nLearn more about Microsoft Purview permissions and access.\n\nNext unit: Classify and label data\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nThe Microsoft Purview Data Catalog offers a browse experience that enables users to explore available data. Users can explore the data catalog either by collection or through traversing the hierarchy of each data source. The first step in understanding the contents of your data map is registering and scanning data, after which you can classify data for easy identification of assets to use for reporting.\n\nLearning objectives\n\nIn this module, you will:\n\nDescribe asset classification in Microsoft Purview.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Catalog data artifacts by using Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCatalog data artifacts by using Microsoft Purview\nModule\n6 Units\nFeedback\nBeginner\nData Analyst\nData Engineer\nMicrosoft Purview\n\nRegister, scan, catalog, and view data assets and their relevant details in Microsoft Purview.\n\nLearning objectives\n\nBy the end of this module, you’ll be able to:\n\nDescribe asset classification in Microsoft Purview.\nAdd\nPrerequisites\nExperience using the Azure data ecosystem.\nIntroduction\nmin\nRegister and scan data\nmin\nClassify and label data\nmin\nSearch the data catalog\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nUsing search in Azure Purview data catalog enabled you to find the correct asset to use for the sales report. You were able to locate the asset, verify that it is certified for use, contact experts and dataset owners, and open a Power BI desktop report containing a connection to the asset. You also integrated Microsoft Purview into Azure Synapse studio to enrich the Azure Synapse experience.\n\nMicrosoft Purview empowers data analysts and other data consumers to find valuable, trustworthy data by searching and browsing data assets across an organization's data estate.\n\nLearn more\nMicrosoft Purview Permissions\nMicrosoft Source Readiness at Scale\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Demo-Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/5-integrate",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Discover trusted data using Microsoft Purview \nAdd\nPrevious\nUnit 5 of 7\nNext\nIntegrate with Azure Synapse Analytics\nCompleted\n100 XP\n4 minutes\n\nMicrosoft Purview can be integrated directly into Azure Synapse. If Azure Synapse Studio is massively deployed in your organization, you can get the data catalog experience directly in Azure Synapse Studio.\n\nThis integrated experience allows you to discover Microsoft Purview assets, interact with them through Synapse capabilities, and push lineage information to Microsoft Purview.\n\n Note\n\nTo connect an Microsoft Purview Account to a Synapse workspace, you need 2 types of permissions. You need a contributor role in Synapse workspace from Azure portal identity and access management (IAM). You also need access to that Microsoft Purview Account. For more information, see Microsoft Purview permissions.\n\nLet’s imagine you need to find and understand some assets before working with them in pipelines or notebooks. From Azure Synapse Studio, you can easily query your Microsoft Purview data catalog.\n\nIn Azure Synapse Studio, from the Data blade on the left, select Purview in the dropdown next to the search bar.\n\nSearch for the asset that exists in Purview. Imagine you're looking for movie files. Enter the keyword movie in the search bar, and fine tune your search by selecting Files as the object type and Raw as the collection.\n\nSelect the first asset “Movies.csv” to get asset details. Because you are in Azure Synapse Studio, you can also leverage Azure Synapse capabilities.\n\nFor instance, you can use Azure Synapse serverless to query your assets. Select Develop, New SQL Script and Select top 100.\n\nDouble check you're connected to your serveless instance and select Run to execute the script and get an overview of your data.\n\nAfter reviewing data, you can use the asset, for example, adding to a new dataflow in Azure Synapse.\n\n Note\n\nSee Connect an Microsoft Purview Account for detailed information about integrating Microsoft Purview into Azure Synapse Analytics.\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions below. Then select Check your answers.\n\nCheck your knowledge\n1. \n\nWhat feature of Microsoft Purview can analysts and other data consumers use to find trustworthy data for reports?\n\n \n\nData map.\n\nData catalog.\n\nData policies.\n\n2. \n\nIf an analyst is looking for a specific asset by name and type, what is the most efficient way to find that asset in the data catalog?\n\n \n\nBrowse assets.\n\nManage glossary.\n\nSearch catalog.\n\n3. \n\nHow can users download Power BI data source files that contain connections to assets discovered in Microsoft Purview?\n\n \n\nIn the Microsoft Purview data catalog, in the asset view.\n\nIn the Microsoft Purview insights report.\n\nUsers can't download Power BI data source files from Microsoft Purview.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Browse assets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/3-browse-assets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Discover trusted data using Microsoft Purview \nAdd\nPrevious\nUnit 3 of 7\nNext\nBrowse assets\nCompleted\n100 XP\n4 minutes\n\nSearching a data catalog is a great tool for data discovery you know what you're looking for. Often, you may not know how your data estate is structured. The Microsoft Purview data catalog offers a browse experience that enables exploration of available data, either by collection or by exploring the hierarchy of each data source in the catalog.\n\nBrowse by collection or source type\n\nIf you're new to an organization or department, you may want to familiarize yourself with the contents of the data estate. From the Microsoft Purview Studio home page, select the “Browse assets” tile to browse either by collection or by source type.\n\nHere you can specify whether you'd like to browse by collection or by source type.\n\nBrowse by collection allows you to explore the different collections you're a data reader or curator for. You'll only see collections you have access to. Select a collection to get a list of assets in that collection with the facets and filters available in search.\n\n Tip\n\nCollections are a tool to manage ownership and access control across assets and data sources. They also organize assets and sources into categories that are customized to match the business flow. See Create and manage collections in Microsoft Purview to learn more.\n\nBrowse by source type allows you to explore the hierarchies of data sources using an explorer view.\n\nAfter selecting a tile associated with a data source type, you'll see a list of assets belonging to that type. From there, you'll be able to use the explorer view to see parent and child assets.\n\nThe Microsoft Purview data catalog browse experience enables analysts or data consumers to explore what data is available in many different ways. Microsoft Purview has the ability to enable users access to data you may not have known about before. The possibilities are endless so long as your organization's data stewards have scanned and classified data across the estate.\n\nNext unit: Use assets with Power BI\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use assets with Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/4-use-asset-power-bi",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse assets with Power BI\n3 minutes\n\nThe integration of Microsoft Purview and Power BI makes it possible to gain a more complete understanding of the data across your estate.\n\nRequest access to assets\n\nIn your search or browsing session, you may come across assets that you don't have access to. Microsoft Purview makes it simple to request access directly from the Data Catalog by using the “Request access” button. Requesting access will kick off a workflow that manages requests and approvals.\n\nBuild a Power BI report using data discovered in Purview\n\nWorking as a new analyst, you've taken the time to search and browse assets and now you'd like to use those trusted assets in a Power BI report. Purview makes it simple, with the ability to open the asset in Power BI desktop.\n\nSelecting Open in Power BI Desktop initiates the download of a Power BI Data Source file (PBIDS) you can open with Power BI Desktop. PBIDS files contain a connection to the data source, so all you need to do is enter credentials upon opening the file and you're ready to start building.\n\nScan a Power BI tenant\n\nIn addition to using Purview to find trusted data sources in the data estate to build reports, you can also scan your Power BI tenant to manage and catalog assets. The metadata of Power BI assets, and information about their lineage across Power BI workspaces and their connections to data sources, are then available in Microsoft Purview.\n\n Note\n\nSee Connect to and manage a Power BI tenant in Microsoft Purview for more details.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Search for assets - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/2-search-browse-assets",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Discover trusted data using Microsoft Purview \nAdd\nPrevious\nUnit 2 of 7\nNext\nSearch for assets\nCompleted\n100 XP\n10 minutes\n\nMicrosoft Purview offers a central place to discover and understand assets to use in your day-to-day activities. This central place, Microsoft Purview data catalog, provides advanced search capabilities to quickly find the right assets and information. Using keywords, business terms and Microsoft Purview data catalog functionalities, you can find the assets needed to build and design reports.\n\nAs a data analyst looking for assets, you'll be searching the Microsoft Purview data catalog. This assumes that the Microsoft Purview data Map has been created by your organization. The data map provides the foundation for data discovery. The data map captures metadata about enterprise data in analytics and operations systems on-premises and in the cloud and must be established before the data catalog can be searched.\n\n Note\n\nLearn more about the Microsoft Purview data map components.\n\nSearch the Microsoft Purview data catalog\n\nFrom the Microsoft Purview Studio home page, users can type relevant keywords to start discovering assets. In this scenario, you're looking for “product sales.”\n\nThe screenshot below displays the search result, with all assets corresponding to the keywords entered in the search engine.\n\nYou can fine-tune your search using the filters on the left side of the page.\n\nYou can filter by:\n\nSource type (and instance if needed)\nObject type\nClassification\nGlossary term\nIf needed, more options are available like Collection, Contact and Label\n\nYou've been instructed to connect to sources like Azure SQL tables. In the result displayed below, two assets are displayed. To use the correct asset, it’s possible to browse each asset to dig for more detailed information. Alternatively, you can rely on the work done by the data stewards who have labeled certified assets for the organization.\n\nBefore using this asset to create your report, you need to verify more details and validate where data comes from to populate this asset. Select the asset to access more information.\n\nUnderstand a single asset\nAsset overview\n\nSelect an asset to see the overview. The overview displays information at a glance, including a description, asset classification, schema classification, collection path, asset hierarchy, and glossary terms.\n\nThe asset description provides a brief explanation of the purpose of an asset. Data stewards have made data analysts lives easier in the screenshot below, by noting that this is the correct resource to use for sales reporting.\n\nBeneath the description, you'll see the asset classification and schema classification.\n\nData classification, in the context of Microsoft Purview, is a way of categorizing data assets by assigning unique logical labels or classes. Classification is based on the business context of the data. For example, you might classify assets by Passport Number, Driver's License Number, Credit Card Number, SWIFT Code, Person’s Name, and so on. Asset classifications can be automatically applied during a scan or applied manually.\n\n Note\n\nMicrosoft Purview comes with more than 200 classifications out of the box. For a full list of classifications, see System classifications in Microsoft Purview.\n\nThe overview tab reflects both asset level classifications and column level classifications that have been applied, which you can also view as part of the schema.\n\n Important\n\nYou may notice that the classifications displayed above are sensitive or contain personally identifiable information (PII). data encryption is done at the source level, and Microsoft Purview stores only the metadata. It does not preview data.\n\nYou can also view the collection path, hierarchy and glossary terms on the right side of the overview tab.\n\nThe collection path refers to the location of the asset inside Microsoft Purview. You have the option to move an asset to another collection.\n\nYou can view the full asset hierarchy within the overview tab. As an example: if you navigate to a SQL table, then you can see the schema, database, and the server the table belongs to.\n\nGlossary terms are a managed vocabulary for business terms that can be used to categorize and relate assets across your environment. For example, terms like 'customer,' 'buyer, 'cost center,' or any terms that give your data context for your users. You can view the glossary terms for an asset in the overview section, and you can add a glossary term on an asset by editing the asset.\n\n Note\n\nFor more information, see the business glossary page.\n\nAsset schema\n\nThe schema view of the asset includes more granular details about the asset, such as column names, data types, column level classifications, terms, and descriptions.\n\nAsset lineage\n\nAsset lineage gives you a clear view of how the asset is populated and where data comes from. Data lineage is broadly understood as the lifecycle that spans the data's origin, and where it moves over time across the data estate. Data lineage is important to analysts because it enables understanding of where data is coming from, what upstream changes may have occurred, and how it flows through the enterprise data systems.\n\nA single view on the asset lineage tab displays the data flow to and from the asset. Asset lineage can also help you understand how the asset was built and how the asset is used inside the organization.\n\nThe columns pane on the left side of the lineage tab allows users to select and track columns as they flow through the lineage. For example, if you select the column Full Name, you can see how the Full Name field was created and where the information comes from.\n\n Note\n\nThe lineage view is a powerful way to understand the transformation process an asset has undergone. Learn more about the lineage experience in Microsoft Purview data catalog\n\nAsset contacts and related assets\n\nAsset contacts provide you contact details of experts or dataset owners with any questions. As a new analyst searching for the right data sources for your report, you may find these individuals helpful.\n\nIf needed, you can also navigate through the technical hierarchy of assets that are related to the current asset you're viewing.\n\nThe ability to search the Microsoft Purview data catalog has the potential to break down data silos and enable the next level of enterprise analytics.\n\nNext unit: Browse assets\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nMicrosoft Purview is a unified data governance service that helps you manage and govern your on-premises, multi-cloud, and software-as-a-service (SaaS) data. Data professionals can easily create a holistic, up-to-date map of the entire data landscape. Microsoft Purview includes automated data discovery, sensitive data classification, and end-to-end data lineage. Microsoft Purview can empower data analysts and other data consumers to find valuable, trustworthy data.\n\nImagine that you're a new Data Analyst at Contoso. In your second week, the sales manager desperately asks for the latest inventory and sales data for an impromptu review. After clarifying the requirements, you know you need to quickly find accurate assets to create a report. With the help of Microsoft Purview data catalog, you'll be able to search, browse, and discover assets. More importantly, you'll be able to validate that you’re using the right data sources for your reports.\n\nLearning objectives\n\nIn this module, you will:\n\nBrowse and search data catalog assets.\nUse data catalog assets with Power BI.\nUse Microsoft Purview in Azure Synapse Studio.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Discover trusted data using Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nDiscover trusted data using Microsoft Purview\nModule\n7 Units\nFeedback\nBeginner\nData Analyst\nData Engineer\nPower BI\nMicrosoft Purview\n\nUse Microsoft Purview Studio to discover trusted organizational assets for reporting.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nBrowse, search, and manage data catalog assets.\nUse data catalog assets with Power BI.\nUse Microsoft Purview in Azure Synapse Studio.\nAdd\nPrerequisites\nExperience using the Azure data ecosystem.\nIntroduction\nmin\nSearch for assets\nmin\nBrowse assets\nmin\nUse assets with Power BI\nmin\nIntegrate with Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "lineage-end-end-expanded.png (1908×817)",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/lineage-end-end-expanded.png#lightbox",
    "html": ""
  },
  {
    "title": "where-is-data-expanded.png (936×526)",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/where-is-data-expanded.png#lightbox",
    "html": ""
  },
  {
    "title": "scan-rule-sets-expanded.png (1733×607)",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/scan-rule-sets-expanded.png#lightbox",
    "html": ""
  },
  {
    "title": "summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nOur goal was to help you evaluate whether Microsoft Purview is the right choice to help you manage your enterprise data environment and its various data sources. We looked at how to:\n\nRegister data sources.\nMap data sources.\nScan data in your sources.\nExplore metadata and classification of the data.\n\nWe covered how you can use the Microsoft Purview governance portal to register data sources and create a data map. Setting up scanning causes Microsoft Purview to scan through the selected data types in the sources and list metadata associated with those sources. This metadata documents the expected usage to help users discover what's contained in the data sources. We also showed how the metadata includes classification information to help identify sensitive data.\n\nThese criteria help you evaluate how Microsoft Purview can help your business catalog data for users and data producers. We also showed how Microsoft Purview can help your company meet its data governance needs by using the metadata and classification features.\n\nReferences\n\nFor more information, see:\n\nIntroduction to Microsoft Purview\nMap your data estate with Microsoft Purview\nMicrosoft Purview for unified data governance\nEnable unified data governance with Microsoft Purview (video)\nDeploy Microsoft Purview and scan an Azure Data Lake resource using the Azure portal\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n15 minutes\n\nChoose the best response for each of the following questions, and then select Check your answers.\n\n1. \n\nWhat does Microsoft Purview do with the data it discovers from your registered sources?\n\n \n\nIt catalogs and classifies the data that's scanned.\n\nIt moves the data to your Azure subscription, automatically creating the necessary storage accounts.\n\nIt performs data transformations to match your on-premises schemas.\n\n2. \n\nWhere would you register your data sources for use in Microsoft Purview?\n\n \n\nOn the Overview tab of the Microsoft Purview account page.\n\nOn the Managed Resources tab of the Microsoft Purview account page.\n\nIn the Microsoft Purview governance portal.\n\n3. \n\nWhat aspect of Microsoft Purview is used to configure the data discovery for your data sources?\n\n \n\nScan rules\n\nCollections\n\nClassifications\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "how-azure-purview-works - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/3-how-microsoft-purview-works",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Introduction to Microsoft Purview \nAdd\nPrevious\nUnit 3 of 6\nNext\nHow Microsoft Purview works\nCompleted\n100 XP\n5 minutes\n\nHere's where we take a look at how Microsoft Purview works. In this unit, you learn the core operational theory behind the functioning of Microsoft Purview for mapping and scanning your data sources. The key areas we focus on include how to:\n\nLoad data in the data map.\nBrowse and search information in the data catalog.\nLoad data in the data map\n\nThe Microsoft Purview Data Map is a unified map of your data assets and their relationships. As one cohesive map, it's easier for you and your users to visualize and govern. It also houses the metadata that underpins the Microsoft Purview Data Catalog and Data Estate Insights. The Data Map scales up and down to meet your enterprise compliance requirements. You can use it to govern your data estate in a way that makes the most sense for your business.\n\nSource data\n\nSourcing your data starts with a process where you register data sources. Microsoft Purview supports an array of data sources that span on-premises, multicloud, and software-as-a-service (SaaS) options. You register the various data sources so that Microsoft Purview is aware of them. The data remains in its location and isn't migrated to any other platform.\n\nAfter you have a Microsoft Purview service configured in Azure, you use the Microsoft Purview governance portal to register your data sources.\n\nEach type of data source you choose requires specific information to complete the registration. For example, if your data sources reside in your Azure subscription, you choose the necessary subscription and storage account name. The following image is an example of choosing an Azure Blob Storage source.\n\nAfter registration, you scan the data source. Scanning ingests metadata about your data source into the Microsoft Purview Data Map. Each data source has specific requirements for authenticating and configuration to permit scanning of the assets in that data source.\n\nFor example, if you have data stored in an Amazon S3 standard bucket, you'll need to provide a configuration for the connection. For this service, you use Microsoft Purview to provide a Microsoft account with secure access to AWS, where the Microsoft Purview scanner will run. The Microsoft Purview scanner uses this access to your Amazon S3 buckets to read your data. The scanner then reports the results (including only the metadata and classification) back to Azure. You can use the Microsoft Purview classification and labeling reports to analyze and review your data scan results.\n\n Note\n\nCheck the Microsoft Purview connector for Amazon S3 documentation for region support related to AWS S3 sources.\n\nIn Microsoft Purview, there are a few options to use for authentication when the service needs to scan data sources. Some of these options are:\n\nMicrosoft Purview managed identity\nAccount key (using Azure Key Vault)\nSQL authentication (using Key Vault)\nService principal (using Key Vault)\nMap data\n\nThe data map is the foundational platform for Microsoft Purview. The data map consists of:\n\nData assets.\nData lineage.\nData classifications.\nBusiness context.\n\nCustomers create a knowledge graph of data that comes in from a range of sources. Microsoft Purview makes it easy to register and automatically scan and classify data at scale. Within a data map, you can identify the type of data source, along with other details around security and scanning.\n\nThe data map uses collections to organize these details. Collections are a way of grouping data assets into logical categories to simplify management and discovery of assets within the catalog. You also use collections to manage access to the metadata that's available in the data map.\n\nSelect Map view in the Microsoft Purview governance portal to display the data sources in a graphical view, along with the collections you created for them.\n\nScan data\n\nAfter you register your data sources, you'll need to run a scan to access their metadata and browse the asset information. Before you can scan the data sources, you're required to enter the credentials for these sources. You can use Azure Key Vault to store the credentials for security and ease of access by your scan rules. The Microsoft Purview governance portal comes with existing system scan rule sets that you can select when you create a new scan rule. You can also specify a custom scan rule set.\n\nA scan rule set is a container for grouping scan rules together to use the same rules repeatedly. A scan rule set lets you select file types for schema extraction and classification. It also lets you define new custom file types. You might create a default scan rule set for each of your data source types. Then you can use these scan rule sets by default for all scans within your company.\n\nFor example, you might want to scan only the .csv files in an Azure Data Lake Storage account. Or you might want to check your data only for credit card numbers rather than all the possible classifications. You might also want users with the right permissions to create other scan rule sets with different configurations based on business needs.\n\nClassification\n\nMetadata is used to help describe the data that's being scanned and made available in the catalog. During the configuration of a scan set, you can specify classification rules to apply during the scan that also serve as metadata. The classification rules fall under five major categories:\n\nGovernment: Attributes such as government identity cards, driver license numbers, and passport numbers.\nFinancial: Attributes such as bank account numbers or credit card numbers.\nPersonal: Personal information such as a person's age, date of birth, email address, and phone number.\nSecurity: Attributes like passwords that can be stored.\nMiscellaneous: Attributes not included in the other categories.\n\nYou can use several system classifications to classify your data. These classifications align with the sensitive information types in the Microsoft Purview compliance portal. You can also create custom classifications to identify other important or sensitive information types in your data estate.\n\nAfter you register a data source, you can enrich its metadata. With proper access, you can annotate a data source by providing descriptions, ratings, tags, glossary terms, identifying experts, or other metadata for requesting data-source access. This descriptive metadata supplements the structural metadata, such as column names and data types, that's registered from the data source.\n\nDiscovering and understanding data sources and their use is the primary purpose of registering the sources. If you're an enterprise user, you might need data for business intelligence, application development, data science, or any other task where the right data is required. You can use the data catalog discovery experience to quickly find data that matches your needs. You can evaluate the data for its fitness for the purpose and then open the data source in your tool of choice.\n\nAt the same time, you can contribute to the catalog by tagging, documenting, and annotating data sources that have already been registered. You can also register new data sources, which are then discovered, evaluated, and used by the community of catalog users.\n\nBrowse and search\n\nMicrosoft Purview allows you to search information from the data map by using the Microsoft Purview Data Catalog. You can perform text-based search and browse through results by using filters like data source type, tags, ratings, or collection.\n\nYou can use business context to search information from the Microsoft Purview catalog. You can define business glossaries and bulk import existing ones, too. You can also apply business context onto assets in the data map. By using a metamodel, you can define business processes in your environment and associate your data sources with those processes. Users can then apply these business contexts to browse and search for information in the data catalog.\n\nDiscovery enables you to use:\n\nSemantic search and browse.\nBusiness glossary and workflows.\nData lineage with sources, owners, transformations, and lifecycle​.\n\nData lineage\n\nThe concept of data lineage focuses on the lifecycle of data. The lifecycle concerns itself with the various stages data might go through. Data is sourced, moved, and stored throughout its lifecycle. Data might also undergo transformations in the extract, load, and transform/extract, transform, and load (ELT/ETL) operations.\n\nData lineage can offer insights into the data lifecycle by looking at the data pipeline. You can use the lineage to identify the root cause of data issues, perform data quality analysis, and verify compliance.\n\nMicrosoft Purview represents this data lineage in a visual form by showing data movement from source to destination.\n\nNext unit: When to use Microsoft Purview\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "when-to-use-microsoft-purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/4-when-to-use-microsoft-purview",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Govern data across an enterprise  Introduction to Microsoft Purview \nAdd\nPrevious\nUnit 4 of 6\nNext\nWhen to use Microsoft Purview\nCompleted\n100 XP\n5 minutes\n\nIn this unit, we discuss how you can decide whether Microsoft Purview is the right choice for your data governance and discovery needs. The criteria that indicate whether Microsoft Purview will meet your requirements are:\n\nDiscovery\nGovernance\n\nLet's take a look at these criteria and see how Microsoft Purview can help address their needs.\n\nDiscovery\n\nWithout a central location to register data sources, you might be unaware of a data source unless you come into contact with it as part of another process.\n\nUnless you know the location of a data source, you can't connect to the data by using a client application. You're required to know the connection string or path.\n\nThe intended use of the data is hidden to you unless you know the location of a data source's documentation. Data sources and documentation might live in several places and be utilized through different kinds of experiences.\n\nGovernance\n\nAs the data in your organization grows, the task of discovering, protecting, and governing that data becomes more difficult. Data is stored in different locations, which might be required for compliance reasons. The data might contain sensitive information such as credit card numbers, social security numbers, or other personal information.\n\nCompliance with company security policies, government regulations, and customer needs are critical considerations for data governance. Understanding which data sources contain sensitive information is key to knowing where protections are needed and how to guard against access to this sensitive data.\n\nApply the criteria\n\nLet's take a look at how Microsoft Purview can address the data discovery and governance criteria.\n\nDoes Microsoft Purview help with data discovery?\n\nDo you require a solution or centralized location to register data sources? Often, users might be unaware of a data source unless they come into contact with it as part of another process. Microsoft Purview can help provide a solution.\n\nAfter you've registered data sources in the Microsoft Purview governance portal and displayed them in the data map, you can set up scanning of those data sources. The metadata that's returned catalogs the data in those sources. In this way, it's easier for users to discover what the data sources contain. The metadata is indexed to make each data source easy to discover via search. It's also more understandable to the users who discover it.\n\nUsers can contribute to the catalog by tagging, documenting, and annotating data sources that have already been registered. They can register new data sources so that other catalog users can discover, understand, and utilize them.\n\nDoes Microsoft Purview help with data governance?\n\nMicrosoft Purview can scan and automatically classify data in files and tables. Microsoft Purview classifies data by Bloom Filter and RegEx. Bloom Filter classifications include attributes for city, country/region, place, and person information. RegEx classifications cover attributes that include categories like bank information (ABA routing numbers or country/region-specific banking account numbers), passport numbers, and country/region-specific identification numbers. You can find the full list of supported classifications in the documentation for Microsoft Purview.\n\nMicrosoft Purview also uses predefined Data Plane roles to help control who has access to the information in Microsoft Purview. For access, users can use the Microsoft Purview governance portal only if they're placed in at least one of the three supported roles. When a Microsoft Purview account is created, no one but the creator can access the account or use its APIs. New users must be put in one or more of the following roles:\n\nPurview Data Reader role: Has access to the Microsoft Purview governance portal and can read all content in Microsoft Purview except for scan bindings.\nPurview Data Curator role: Has access to the Microsoft Purview governance portal and can read all content in Microsoft Purview except for scan bindings. Can edit information about assets, classification definitions, and glossary terms. Can also apply classifications and glossary terms to assets.\nPurview Data Source Administrator role: Doesn't have access to the Microsoft Purview governance portal (unless the user is also in the Data Reader or Data Curator roles). Can manage all aspects of scanning data into Microsoft Purview. Doesn't have read or write access to content in Microsoft Purview beyond those tasks related to scanning.\n\nThese roles are assigned by using the collections where your data sources are registered. You can grant users access to the data they might need without granting them access to the entire data estate. By assigning roles, you can promote resource discoverability while still protecting sensitive information.\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "what-is-Microsoft-Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/2-what-is-microsoft-purview",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhat is Microsoft Purview?\n3 minutes\n\nLet's start with a few definitions and a quick tour of the core features of Microsoft Purview.\n\nWhat's Microsoft Purview?\n\nMicrosoft Purview is a unified data-governance service that helps you manage and govern your on-premises, multicloud, and software-as-a-service (SaaS) data. You can easily create a broad, up-to-date map of your data landscape with:\n\nAutomated data discovery.\nSensitive data classification.\nEnd-to-end data lineage.\n\nYou can also empower data users to find valuable, trustworthy data.\n\nMicrosoft Purview is designed to help enterprises get the most value from their existing information assets. With this cloud-based service, you can register your data sources to help you discover and manage them. Your data sources remain in place, but a copy of the metadata for the source is added to Microsoft Purview.\n\nYou can register a wide range of sources in Azure and across your multicloud data estate in Microsoft Purview. These sources include Azure Data Lake Storage, AWS, Azure SQL Database on-premises and in the cloud, and many more.\n\nMicrosoft Purview has three main elements:\n\nMicrosoft Purview Data Map: The data map provides a structure for your data estate in Microsoft Purview, where you can map your existing data stores into groups and hierarchies. In the data map, you can grant users and teams access to these groups so that they have access to find relevant data stores. The data map can then scan your data stores and gather metadata such as schemas and data types. It can also identify sensitive data types so that you can keep track of them in your data estate.\n\nMicrosoft Purview Data Catalog: The data catalog allows your users to browse the metadata stored in the data map so that they can find reliable data and understand its context. For example, users can see where the data comes from and who are the experts they can contact about that data source. The data catalog also integrates with other Azure products, like the Azure Synapse Analytics workspace, so that users can search for the data they need from the applications they need it in.\n\nMicrosoft Purview Data Estate Insights: Insights offer a high-level view into your data catalog, covering these key facets:\n\nData stewardship: A report on how curated your data assets are so that you can track your governance progress.\nCatalog adoption: A report on the number of active users in your data catalog, their top searches, and your most viewed assets.\nAsset insights: A report on the data estate and source-type distribution. You can view by source type, classification, and file size. View the insights as a graph or as key performance indicators.\nScan insights: A report that provides information on the health of your scans (successes, failures, or canceled).\nGlossary insights: A status report on the glossary to help users understand the distribution of glossary terms by status, and view how the terms are attached to assets.\nClassification insights: A report that shows where classified data is located. It allows security administrators to understand the types of information found in their organization's data estate.\nSensitivity insights: A report that focuses on sensitivity labels found during scans. Security administrators can make use of this information to ensure security is appropriate for the data estate.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nAs the volume and variety of data increases, the challenges of good data governance are likely to become more difficult. Digital transformation technologies have resulted in new data sources. How do users know what data is available? How do administrators manage data when they might not know what type of data exists and where it's stored? Does the data contain sensitive or personal information?\n\nAll these questions aren't easy to answer without insights into the data and the source of storage. Before you can develop data-governance plans for usage and storage, you need to understand the data your organization uses.\n\nExample scenario\n\nAs a user or producer of data, you might be a business or technical data analyst, data scientist, or data engineer. You probably spend significant time on manual processes to annotate, catalog, and find trusted data sources.\n\nWithout a central location to register data sources, you might be unaware of a data source unless you come into contact with it as part of another process.\n\nWriting metadata descriptions for data sources is often a wasted effort. Client applications typically ignore descriptions that are stored in the data source. Creating documentation for data sources is difficult because you must keep documentation in sync with data sources. Users also might not trust documentation that they think is out of date.\n\nWithout the ability to track data from end to end, you must spend time tracing problems created by data pipelines that other teams own. If you make changes to your datasets, you can accidentally affect related reports that are business or mission critical.\n\nMicrosoft Purview is designed to address these issues and help enterprises get the most value from their existing information assets. Its catalog makes data sources easy to discover and understand by the users who manage the data.\n\nWhat will we be doing?\n\nThis high-level overview of Microsoft Purview helps you discover the key aspects that make it the tool of choice for mapping out your enterprise data. You learn how it can help you:\n\nManage and govern your data across various platforms and locations.\nMap out your data landscape.\nClassify sensitive data.\nEmpower customers to find trustworthy data.\nWhat's the main goal?\n\nBy the end of this session, you'll be able to decide whether Microsoft Purview is the right choice to help you manage your enterprise data environment and your various data sources.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to Microsoft Purview - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntroduction to Microsoft Purview\nModule\n6 Units\nFeedback\nBeginner\nDeveloper\nData Analyst\nData Analyst\nData Scientist\nDatabase Administrator\nAdministrator\nSolution Architect\nAI Engineer\nMicrosoft Purview\n\nIn this module, you'll evaluate whether Microsoft Purview is the right choice for your data discovery and governance needs.\n\nLearning objectives\n\nBy the end of this module, you'll be able to:\n\nEvaluate whether Microsoft Purview is appropriate for your data discovery and governance needs.\nDescribe how the features of Microsoft Purview work to provide data discovery and governance.\nAdd\nPrerequisites\nKnowledge of Azure accounts and services\nKnowledge of various data sources such as SQL Server and Azure Cosmos DB\nKnowledge of the concepts around data governance\nIntroduction\nmin\nWhat is Microsoft Purview?\nmin\nHow Microsoft Purview works\nmin\nWhen to use Microsoft Purview\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nIn this module, you've learned how to integrate Azure Stream Analytics and Microsoft Power BI to generate real-time data visualizations.\n\nBy using a Power BI output in Azure Stream Analytics, you can send the results of a stream processing query to a dataset in Power BI, from which you can create real-time dashboard visualizations. You can also use the dataset to support reports that reflect the latest data when rendered.\n\n Note\n\nTo learn more about real-time visualization in Power BI, see Real-time streaming in Power BI in the Power BI documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Create a real-time data visualization - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/5-exercise-create-real-time-data-visualization",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Create a real-time data visualization\n45 minutes\n\nNow it's your opportunity to use Azure Stream Analytics and Power BI to create a real-time data visualization.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access. You'll also need access to the Microsoft Power BI service. Your school or organization may already provide this, or you can sign up for the Power BI service as an individual.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich type of Azure Stream Analytics output should you use to support real-time visualizations in Microsoft Power BI?\n\n \n\nAzure Synapse Analytics\n\nAzure Event Hubs\n\nPower BI\n\n2. \n\nYou want to use an output to write the results of a Stream Analytics query to a table named device-events in a dataset named realtime-data in a Power BI workspace named analytics workspace. What should you do?\n\n \n\nCreate only the workspace. The dataset and table will be created automatically.\n\nCreate the workspace and dataset. The table will be created automatically.\n\nCreate the workspace, dataset, and table before creating the output.\n\n3. \n\nYou want to create a visualization that updates dynamically based on a table in a streaming dataset in Power BI. What should you do?\n\n \n\nCreate a report from the dataset.\n\nCreate a dashboard with a tile based on the streaming dataset.\n\nExport the streaming dataset to Excel and create a report from the Excel workbook.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create real-time data visualizations in Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/4-create-real-time-data-visualizations-power-bi",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate real-time data visualizations in Power BI\n5 minutes\n\nWhen you successfully run an Azure Stream Analytics job that sends results to a Power BI output, a streaming dataset containing a single table is created in the Power BI workspace specified for the output. The table contains the data produced by the Stream Analytics query.\n\nCreating real-time visualizations in a dashboard\n\nTo visualize data in real-time, you can create a dashboard with a real-time visualization tile. Real-time visualizations on a dashboard show data from a streaming dataset, and are updated dynamically as new data flows into the dataset.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a query for real-time visualization - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/3-realtime-query",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a query for real-time visualization\n6 minutes\n\nTo send streaming data to Power BI, your Azure Stream Analytics job uses a query that writes its results to a Power BI output. A simple query that forwards event data from an event hub directly to Power BI might look something like this:\n\nSELECT\n    EventEnqueuedUtcTime AS ReadingTime,\n    SensorID,\n    ReadingValue\nINTO\n    [powerbi-output]\nFROM\n    [eventhub-input] TIMESTAMP BY EventEnqueuedUtcTime\n\n\nThe results of the query determine the schema of the table in the output dataset in Power BI.\n\nAlternatively, you might use your query to filter and/or aggregate the data, sending only relevant or summarized data to the Power BI dataset. For example, the following query calculates the maximum reading for each sensor other than sensor 0 for each consecutive minute in which an event occurs.\n\nSELECT\n    DateAdd(second, -60, System.TimeStamp) AS StartTime,\n    System.TimeStamp AS EndTime,\n    SensorID,\n    MAX(ReadingValue) AS MaxReading\nINTO\n    [powerbi-output]\nFROM\n    [eventhub-input] TIMESTAMP BY EventEnqueuedUtcTime\nWHERE SensorID <> 0\nGROUP BY SensorID, TumblingWindow(second, 60)\nHAVING COUNT(*) > 1\n\n\nWhen working with window functions (such as the TumblingWindow function in the previous example), consider that Power BI is capable of handling a call every second. Additionally, streaming visualizations support packets with a maximum size of 15 KB. As a general rule, use window functions to ensure data is sent to Power BI no more frequently than every second, and minimize the fields included in the results to optimize the size of the data load.\n\n Note\n\nFor more information about Power BI output limitations, see Power BI output from Azure Stream Analytics in the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a Power BI output in Azure Stream Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/2-power-bi-output",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a Power BI output in Azure Stream Analytics\n6 minutes\n\nAll Azure Stream Analytics jobs include at least one input and output. In most cases, inputs reference sources of streaming data (though you can also define inputs for static reference data to augment the streamed event data). Outputs determine where the results of the stream processing query will be sent. To support real-time data visualization, you can use a Power BI output.\n\nStreaming data inputs\n\nInputs for streaming data consumed by Azure Stream Analytics can include:\n\nAzure Event Hubs\nAzure IoT Hubs\nAzure Blob or Data Lake Gen 2 Storage\n\nDepending on the specific input type, the data for each streamed event includes the event's data fields and input-specific metadata fields. For example, data consumed from an Azure Event Hubs input includes an EventEnqueuedUtcTime field indicating the time when the event was received in the event hub.\n\n Note\n\nFor more information about streaming inputs, see Stream data as input into Stream Analytics in the Azure Stream Analytics documentation.\n\nPower BI outputs\n\nYou can use a Power BI output to write the results of a Stream Analytics query to a table in a Power BI streaming dataset, from where it can be visualized in a dashboard. When adding a Power BI output to a Stream Analytics job, you need to specify the following properties:\n\nOutput alias: A name for the output that can be used in a query.\nGroup workspace: The Power BI workspace in which you want to create the resulting dataset.\nDataset name: The name of the dataset to be generated by the output. You shouldn't pre-create this dataset as it will be created automatically (replacing any existing dataset with the same name).\nTable name: The name of the table to be created in the dataset.\nAuthorize connection: You must authenticate the connection to your Power BI tenant so that the Stream Analytics job can write data to the workspace.\n\n Note\n\nFor more information about Power BI outputs, see Power BI output from Azure Stream Analytics in the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n2 minutes\n\nMicrosoft Power BI is used by organizations all around the world to create dynamic, interactive data visualizations that reveal insights on which important business decisions are based. Access to timely data can be the difference between failure and success, so the ability to capture and visualize data in real-time, or as near as possible, is critical in many scenarios.\n\nAzure Stream Analytics provides a way to process a stream of real-time data from an input such as Azure Event Hubs, and direct the results to an output. One possible output is a Power BI dataset, from which dashboards can consume data for real-time visualization.\n\nIn this module, we'll examine how to use Azure Stream Analytics to process a stream of real-time data, and send the results to a Power BI dataset for visualization.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize real-time data with Azure Stream Analytics and Power BI - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nVisualize real-time data with Azure Stream Analytics and Power BI\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure\nAzure Stream Analytics\nPower BI\n\nBy combining the stream processing capabilities of Azure Stream Analytics and the data visualization capabilities of Microsoft Power BI, you can create real-time data dashboards.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nConfigure a Stream Analytics output for Power BI.\nUse a Stream Analytics query to write data to Power BI.\nCreate a real-time data visualization in Power BI.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Stream Analytics and Microsoft Power BI. Consider completing the following modules first:\n\nExplore fundamentals of data visualization.\nGet started with Azure Stream Analytics.\nIntroduction\nmin\nUse a Power BI output in Azure Stream Analytics\nmin\nCreate a query for real-time visualization\nmin\nCreate real-time data visualizations in Power BI\nmin\nExercise - Create a real-time data visualization\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this module, you explored multiple ways in which you can use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.\n\nThe ability to include real-time data ingestion in an analytics solution enables organizations to build reports and interactive data models that reflect the most up-to-date information available. Azure Synapse Analytics offers a wide range of analytical capabilities that you can use with both streaming and batch data to create highly scalable solutions for massive volumes of data. To learn more, see the Introduction to Azure Synapse Analytics module on Microsoft Learn.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich type of output should you use to ingest the results of an Azure Stream Analytics job into a dedicated SQL pool table in Azure Synapse Analytics?\n\n \n\nAzure Synapse Analytics\n\nBlob storage/ADLS Gen2\n\nAzure Event Hubs\n\n2. \n\nWhich type of output should be used to ingest the results of an Azure Stream Analytics job into files in a data lake for analysis in Azure Synapse Analytics?\n\n \n\nAzure Synapse Analytics\n\nBlob storage/ADLS Gen2\n\nAzure Event Hubs\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Ingest streaming data into Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/6-exercise-ingest-streaming-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Ingest streaming data into Azure Synapse Analytics\n45 minutes\n\nNow it's your opportunity to use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Run a job to ingest data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/5-run-job-ingest",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nRun a job to ingest data\n3 minutes\n\nWhen you've created and saved your query, you can run the Azure Stream Analytics job to process events in the input(s) and write the results to output(s). Once started, the query will run perpetually until stopped; constantly ingesting new event data into your Azure Synapse Analytics workspace (into a table in relational data warehouse or files in a data lake, depending on the output type).\n\nWorking with ingested data\n\nYou can work with the ingested streaming data like any other data in Azure Synapse Analytics, combining it with data ingested using batch processing techniques or synchronized from operational data sources by using Azure Synapse Link.\n\nQuerying data in a relational data warehouse\n\nIf you used an Azure Synapse Analytics output to ingest the results of your stream processing job into a table in a dedicated SQL pool, you can query the table using a SQL query, just like any other table. The results of the query will always include the latest data to be ingested at the time the query is run. Your data warehouse can include tables for streaming data as well as tables for batch ingested data, enabling you to join real-time and batch data for historical analytics.\n\nFor example, the following SQL code could be used to query a table named factSensorReadings that contains the results of stream processing, and combine it with a dimDate table containing detailed data about the dates on which readings were captured.\n\nSELECT d.Weekday, s.SensorID, AVG(s.SensorReading) AS AverageReading\nFROM factSensorReadings AS s\nJOIN dimDate AS d\n    ON CAST(s.ReadingTime AS DATE) = d.DateKey\nGROUP BY d.Weekday, s.SensorID\n\n\n Tip\n\nTo Learn more about using a dedicated SQL pool to analyze data in a data warehouse, see the Analyze data in a relational data warehouse module on Microsoft Learn.\n\nQuerying data in a data lake\n\nAs streaming data is ingested into files in a data lake, you can query those files by using a serverless SQL pool in Azure Synapse Analytics. For example, the following query reads all fields from all Parquet files under the sensors folder in the data file system container.\n\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/sensors/*',\n    FORMAT = 'parquet') AS rows\n\n\n Tip\n\nTo Learn more about using serverless SQL pools to query files in a data lake, see the Use Azure Synapse serverless SQL pool to query files in a data lake module on Microsoft Learn.\n\nYou can also query the data lake by using code running in an Apache Spark pool, as shown in this example:\n\n%%pyspark\ndf = spark.read.load('abfss://data@datalake.dfs.core.windows.net/sensors/*', format='parquet'\n)\ndisplay(df)\n\n\n Tip\n\nTo Learn more about using Apache Spark pools to query files in a data lake, see the Analyze data with Apache Spark in Azure Synapse Analytics module on Microsoft Learn.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Define a query to select, filter, and aggregate data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/4-define-query",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDefine a query to select, filter, and aggregate data\n5 minutes\n\nAfter defining the input(s) and output(s) for your Azure Stream Analytics job, you can define a query to process the incoming data from an input and write the results to an output.\n\nSelecting input fields\n\nThe simplest approach to ingesting streaming data into Azure Synapse Analytics is to capture the required field values for every event using a SELECT...INTO query, as shown here:\n\nSELECT\n    EventEnqueuedUtcTime AS ReadingTime,\n    SensorID,\n    ReadingValue\nINTO\n    [synapse-output]\nFROM\n    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime\n\n\n Tip\n\nWhen using an Azure Synapse Analytics output to write the results to a table in a dedicated SQL pool, the schema of the results produced by the query must match the table into which the data is to be loaded. You can use AS clauses to rename fields, and cast them to alternative (compatible) data types as necessary.\n\nFiltering event data\n\nIn some cases, you might want to filter the data to include only specific events by adding a WHERE clause. For example, the following query writes data only for events with a negative ReadingValue field value.\n\nSELECT\n    EventEnqueuedUtcTime AS ReadingTime,\n    SensorID,\n    ReadingValue\nINTO\n    [synapse-output]\nFROM\n    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime\nWHERE ReadingValue < 0\n\nAggregating events over temporal windows\n\nA common pattern for streaming queries is to aggregate event data over temporal (time-based) intervals, or windows. To accomplish this, you can use a GROUP BY clause that includes a Window function defining the kind of window you want to define (for example, tumbling, hopping, or sliding).\n\n Tip\n\nFor more information about window functions, see Introduction to Stream Analytics windowing functions in the Azure Stream Analytics documentation.\n\nThe following example groups streaming sensor readings into 1 minute tumbling (serial, non-overlapping) windows, recording the start and end time of each window and the maximum reading for each sensor. The HAVING clause filters the results to include only windows where at least one event occurred.\n\nSELECT\n    DateAdd(second, -60, System.TimeStamp) AS StartTime,\n    System.TimeStamp AS EndTime,\n    SensorID,\n    MAX(ReadingValue) AS MaxReading\nINTO\n    [synapse-output]\nFROM\n    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime\nGROUP BY SensorID, TumblingWindow(second, 60)\nHAVING COUNT(*) >= 1\n\n\n Tip\n\nFor more information about common patters for queries, see Common query patterns in Azure Stream Analytics in the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure inputs and outputs - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/3-configure-inputs-outputs",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure inputs and outputs\n9 minutes\n\nAll Azure Stream Analytics jobs include at least one input and output. In most cases, inputs reference sources of streaming data (though you can also define inputs for static reference data to augment the streamed event data). Outputs determine where the results of the stream processing query will be sent. In the case of data ingestion into Azure Synapse Analytics, the output usually references an Azure Data Lake Storage Gen2 container or a table in a dedicated SQL pool database.\n\nStreaming data inputs\n\nInputs for streaming data consumed by Azure Stream Analytics can include:\n\nAzure Event Hubs\nAzure IoT Hubs\nAzure Blob or Data Lake Gen 2 Storage\n\nDepending on the specific input type, the data for each streamed event includes the event's data fields as well as input-specific metadata fields. For example, data consumed from an Azure Event Hubs input includes an EventEnqueuedUtcTime field indicating the time when the event was received in the event hub.\n\n Note\n\nFor more information about streaming inputs, see Stream data as input into Stream Analytics in the Azure Stream Analytics documentation.\n\nAzure Synapse Analytics outputs\n\nIf you need to load the results of your stream processing into a table in a dedicated SQL pool, use an Azure Synapse Analytics output. The output configuration includes the identity of the dedicated SQL pool in an Azure Synapse Analytics workspace, details of how the Azure Stream Analytics job should establish an authenticated connection to it, and the existing table into which the data should be loaded.\n\nAuthentication to Azure Synapse Analytics is usually accomplished through SQL Server authentication, which requires a username and password. Alternatively, you can use a managed identity to authenticate. When using an Azure Synapse Analytics output, your Azure Stream Analytics job configuration must include an Azure Storage account in which authentication metadata for the job is stored securely.\n\n Note\n\nFor more information about using an Azure Synapse Analytics output, see Azure Synapse Analytics output from Azure Stream Analytics in the Azure Stream Analytics documentation.\n\nAzure Data Lake Storage Gen2 outputs\n\nIf you need to write the results of stream processing to an Azure Data Lake Storage Gen2 container that hosts a data lake in an Azure Synapse Analytics workspace, use a Blob storage/ADLS Gen2 output. The output configuration includes details of the storage account in which the container is defined, authentication settings to connect to it, and details of the files to be created. You can specify the file format, including CSV, JSON, Parquet, and Delta formats. You can also specify custom patterns to define the folder hierarchy in which the files are saved - for example using a pattern such as YYYY/MM/DD to generate a folder hierarchy based on the current year, month, and day.\n\nYou can specify minimum and maximum row counts for each batch, which determines the number of output files generated (each batch creates a new file). You can also configure the write mode to control when the data is written for a time window - appending each row as it arrives or writing all rows once (which ensures \"exactly once\" delivery).\n\n Note\n\nFor more information about using a Blob storage/ADLS Gen2 output, see Blob storage and Azure Data Lake Gen2 output from Azure Stream Analytics in the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Stream ingestion scenarios - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/2-stream-ingestion-scenarios",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nStream ingestion scenarios\n5 minutes\n\nAzure Synapse Analytics provides multiple ways to analyze large volumes of data. Two of the most common approaches to large-scale data analytics are:\n\nData warehouses - relational databases, optimized for distributed storage and query processing. Data is stored in tables and queried using SQL.\nData lakes - distributed file storage in which data is stored as files that can be processed and queried using multiple runtimes, including Apache Spark and SQL.\nData warehouses in Azure Synapse Analytics\n\nAzure Synapse Analytics provides dedicated SQL pools that you can use to implement enterprise-scale relational data warehouses. Dedicated SQL pools are based on a massively parallel processing (MPP) instance of the Microsoft SQL Server relational database engine in which data is stored and queried in tables.\n\nTo ingest real-time data into a relational data warehouse, your Azure Stream Analytics query must write its results to an output that references the table into which you want to load the data.\n\nData lakes in Azure Synapse Analytics\n\nAn Azure Synapse Analytics workspace typically includes at least one storage service that is used as a data lake. Most commonly, the data lake is hosted in an Azure Storage account using a container configured to support Azure Data Lake Storage Gen2. Files in the data lake are organized hierarchically in directories (folders), and can be stored in multiple file formats, including delimited text (such as comma-separated values, or CSV), Parquet, and JSON.\n\nWhen ingesting real-time data into a data lake, your Azure Stream Analytics query must write its results to an output that references the location in the Azure Data Lake Gen2 storage container where you want to save the data files. Data analysts, engineers, and scientists can then process and query the files in the data lake by running code in an Apache Spark pool, or by running SQL queries using a serverless SQL pool.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nSuppose a retail company captures real-time sales transaction data from an e-commerce website, and wants to analyze this data along with more static data related to products, customers, and employees. A common way to approach this problem is to ingest the stream of real-time data into a data lake or data warehouse, where it can be queried together with data that is loaded using batch processing techniques.\n\nMicrosoft Azure Synapse Analytics provides a comprehensive enterprise data analytics platform, into which real-time data captured in Azure Event Hubs or Azure IoT Hub, and processed by Azure Stream Analytics can be loaded.\n\nA typical pattern for real-time data ingestion in Azure consists of the following sequence of service integrations:\n\nA real-time source of data is captured in an event ingestor, such as Azure Event Hubs or Azure IoT Hub.\nThe captured data is perpetually filtered and aggregated by an Azure Stream Analytics query.\nThe results of the query are loaded into a data lake or data warehouse in Azure Synapse Analytics for subsequent analysis.\n\nIn this module, you'll explore multiple ways in which you can use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Ingest streaming data using Azure Stream Analytics and Azure Synapse Analytic - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIngest streaming data using Azure Stream Analytics and Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Stream Analytics\nAzure Synapse Analytics\n\nAzure Stream Analytics provides a real-time data processing engine that you can use to ingest streaming event data into Azure Synapse Analytics for further analysis and reporting.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nDescribe common stream ingestion scenarios for Azure Synapse Analytics.\nConfigure inputs and outputs for an Azure Stream Analytics job.\nDefine a query to ingest real-time data into Azure Synapse Analytics.\nRun a job to ingest real-time data, and consume that data in Azure Synapse Analytics.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Stream Analytics and Azure Synapse Analytics. Consider completing the following modules first:\n\nGet started with Azure Stream Analytics\nIntroduction to Azure Synapse Analytics\nIntroduction\nmin\nStream ingestion scenarios\nmin\nConfigure inputs and outputs\nmin\nDefine a query to select, filter, and aggregate data\nmin\nRun a job to ingest data\nmin\nExercise - Ingest streaming data into Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n5 minutes\n\nAzure Stream Analytics is a platform-as-a-service (PaaS) solution that you can use to process a perpetual stream of data for real-time reporting, automated action, or integration into an enterprise analytical solution.\n\nIn this module, you learned how to:\n\nUnderstand data streams.\nUnderstand event processing.\nGet started with Azure Stream Analytics.\n\nTo learn more about the capabilities of Azure Stream Analytics, see the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition of stream processing is correct?\n\n \n\nData is processed continually as new data records arrive.\n\nData is collected in a temporary store, and all records are processed together as a batch.\n\nData that is incomplete or contains errors is redirected to separate storage for correction by a human operator.\n\n2. \n\nYou need to process a stream of sensor data, aggregating values over one minute windows and storing the results in a data lake. Which service should you use?\n\n \n\nAzure SQL Database\n\nAzure Cosmos DB\n\nAzure Stream Analytics\n\n3. \n\nYou want to aggregate event data by contiguous, fixed-length, non-overlapping temporal intervals. What kind of window should you use?\n\n \n\nSliding\n\nSession\n\nTumbling\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Get started with Azure Stream Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/4-process-events-azure-stream-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Get started with Azure Stream Analytics\n15 minutes\n\nNow it's your opportunity to explore Azure Stream Analytics in a sample solution that aggregates streaming data events.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand window functions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3b-understand-windows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement a Data Streaming Solution with Azure Stream Analytics  Get started with Azure Stream Analytics \nAdd\nPrevious\nUnit 4 of 7\nNext\nUnderstand window functions\nCompleted\n100 XP\n6 minutes\n\nA common goal of stream processing is to aggregate events into temporal intervals, or windows. For example, to count the number of social media posts per minute or to calculate the average rainfall per hour.\n\nAzure Stream Analytics includes native support for five kinds of temporal windowing functions. These functions enable you to define temporal intervals into which data is aggregated in a query. The supported windowing functions are Tumbling, Hopping, Sliding, Session, and Snapshot.\n\nTumbling\n\nTumbling window functions segment a data stream into a contiguous series of fixed-size, non-overlapping time segments and operate against them. Events can't belong to more than one tumbling window.\n\nThe Tumbling window example, represented by the following query, finds the maximum reading value in each one-minute window. Windowing functions are applied in Stream Analytics jobs using the GROUP BY clause of the query syntax. The GROUP BY clause in the following query contains the TumblingWindow() function, which specifies a one-minute window size.\n\nSQL\nCopy\nSELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY TumblingWindow(minute, 1)\n\nHopping\n\nHopping window functions model scheduled overlapping windows, jumping forward in time by a fixed period. It's easiest to think of them as Tumbling windows that can overlap and be emitted more frequently than the window size. In fact, tumbling windows are simply a hopping window whose hop is equal to its size. When you use Hopping windows, events can belong to more than one window result set.\n\nTo create a hopping window, you must specify three parameters. The first parameter indicates the time unit, such as second, minute, or hour. The following parameter sets the window size, which designates how long each window lasts. The final required parameter is the hop size, which specifies how much each window moves forward relative to the previous one. An optional fourth parameter denoting the offset size may also be used.\n\nThe following query demonstrates using a HoppingWindow() where the timeunit is set to second. The windowsize is 60 seconds, and the hopsize is 30 seconds. This query outputs an event every 30 seconds containing the maximum reading value that occurred over the last 60 seconds.\n\nSQL\nCopy\nSELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY HoppingWindow(second, 60, 30)\n\n\nSliding\n\nSliding windows generate events for points in time when the content of the window actually changes. This function model limits the number of windows that need to be considered. Azure Stream Analytics outputs events for only those points in time when an event entered or exited the window. As such, every window contains a minimum of one event. Events in Sliding windows can belong to more than one sliding window, similar to Hopping windows.\n\nThe following query uses the SlidingWindow() function to find the maximum reading value in each one-minute window in which an event occurred.\n\nSQL\nCopy\nSELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY SlidingWindow(minute, 1)\n\nSession\n\nSession window functions cluster together events that arrive at similar times, filtering out periods of time where there's no data. It has three primary parameters: timeout, maximum duration, and partitioning key (optional).\n\nThe occurrence of the first event starts a session window. Suppose another event occurs within the specified timeout from the last ingested event. In that case, the window will be extended to incorporate the new event. However, if no other events occur within the specified timeout period, the window will be closed at the timeout. If events keep happening within the specified timeout, the session window will extend until the maximum duration is reached.\n\nThe following query measures user session length by creating a SessionWindow over clickstream data with a timeoutsize of 20 seconds and a maximumdurationsize of 60 seconds.\n\nSQL\nCopy\nSELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY SessionWindow(second, 20, 60)\n\nSnapshot\n\nSnapshot windows groups events by identical timestamp values. Unlike other windowing types, a specific window function isn't required. You can employ a snapshot window by specifying the System.Timestamp() function to your query's GROUP BY clause.\n\nFor example, the following query finds the maximum reading value for events that occur at precisely the same time.\n\nSQL\nCopy\nSELECT System.TimeStamp() AS WindowTime,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY System.Timestamp()\n\n\nSystem.Timestamp() is considered in the GROUP BY clause as a snapshot window definition because it groups events into a window based on the equality of timestamps.\n\nNext unit: Exercise - Get started with Azure Stream Analytics\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand event processing - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3-understand-event-processing",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand event processing\n5 minutes\n\nAzure Stream Analytics is a service for complex event processing and analysis of streaming data. Stream Analytics is used to:\n\nIngest data from an input, such as an Azure event hub, Azure IoT Hub, or Azure Storage blob container.\nProcess the data by using a query to select, project, and aggregate data values.\nWrite the results to an output, such as Azure Data Lake Gen 2, Azure SQL Database, Azure Synapse Analytics, Azure Functions, Azure event hub, Microsoft Power BI, or others.\n\nOnce started, a Stream Analytics query will run perpetually, processing new data as it arrives in the input and storing results in the output.\n\nStream Analytics guarantees exactly once event processing and at-least-once event delivery, so events are never lost. It has built-in recovery capabilities in case the delivery of an event fails. Also, Stream Analytics provides built-in checkpointing to maintain the state of your job and produces repeatable results. Because Azure Stream Analytics is a platform-as-a-service (PaaS) solution, it's fully managed and highly reliable. Its built-in integration with various sources and destinations and provides a flexible programmability model. The Stream Analytics engine enables in-memory compute, so it offers high performance.\n\nAzure Stream Analytics jobs and clusters\n\nThe easiest way to use Azure Stream Analytics is to create a Stream Analytics job in an Azure subscription, configure its input(s) and output(s), and define the query that the job will use to process the data. The query is expressed using structured query language (SQL) syntax, and can incorporate static reference data from multiple data sources to supply lookup values that can be combined with the streaming data ingested from an input.\n\nIf your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.\n\nInputs\n\nAzure Stream Analytics can ingest data from the following kinds of input:\n\nAzure Event Hubs\nAzure IoT Hub\nAzure Blob storage\nAzure Data Lake Storage Gen2\n\nInputs are generally used to reference a source of streaming data, which is processed as new event records are added. Additionally, you can define reference inputs that are used to ingest static data to augment the real-time event stream data. For example, you could ingest a stream of real-time weather observation data that includes a unique ID for each weather station, and augment that data with a static reference input that matches the weather station ID to a more meaningful name.\n\nOutputs\n\nOutputs are destinations to which the results of stream processing are sent. Azure Stream Analytics supports a wide range of outputs, which can be used to:\n\nPersist the results of stream processing for further analysis; for example by loading them into a data lake or data warehouse.\nDisplay a real-time visualization of the data stream; for example by appending data to a dataset in Microsoft Power BI.\nGenerate filtered or summarized events for downstream processing; for example by writing the results of stream processing to an event hub.\nQueries\n\nThe stream processing logic is encapsulated in a query. Queries are defined using SQL statements that SELECT data fields FROM one or more inputs, filter or aggregate the data, and write the results INTO an output. For example, the following query filters the events from the weather-events input to include only data from events with a temperature value less than 0, and writes the results to the cold-temps output:\n\nSELECT observation_time, weather_station, temperature\nINTO cold-temps\nFROM weather-events TIMESTAMP BY observation_time\nWHERE temperature < 0\n\n\nA field named EventProcessedUtcTime is automatically created to define the time when the event is processed by your Azure Stream Analytics query. You can use this field to determine the timestamp of the event, or you can explicitly specify another DateTime field by using the TIMESTAMP BY clause, as shown in this example. Depending on the input from which the streaming data is read, one or more potential timestamp fields may be created automatically; for example, when using an Event Hubs input, a field named EventQueuedUtcTime is generated to record the time when the event was received in the event hub queue.\n\nThe field used as a timestamp is important when aggregating data over temporal windows, which is discussed next.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nToday, massive amounts of real-time data are generated by connected applications, Internet of Things (IoT) devices and sensors, and various other sources. The proliferation of streaming data sources has made the ability to consume and make informed decisions from these data in near-real-time an operational necessity for many organizations.\n\nSome typical examples of streaming data workloads include:\n\nOnline stores analyzing real-time clickstream data to provide product recommendations to consumers as they browse the website.\nManufacturing facilities using telemetry data from IoT sensors to remotely monitor high-value assets.\nCredit card transactions from point-of-sale systems being scrutinized in real-time to detect and prevent potentially fraudulent activities.\n\nAzure Stream Analytics provides a cloud-based stream processing engine that you can use to filter, aggregate, and otherwise process a real-time stream of data from various sources. The results of this processing can then be used to trigger automated activity by a service or application, generate real-time visualizations, or integrate streaming data into an enterprise analytics solution.\n\nIn this module, you'll learn how to get started with Azure Stream Analytics, and use it to process a stream of event data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand data streams - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/2-understand-data-streams",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand data streams\n5 minutes\n\nA data stream consists of a perpetual series of data, typically related to specific point-in-time events. For example, a stream of data might contain details of messages submitted to a social media micro-blogging site, or a series of environmental measurements recorded by an internet-connected weather sensor. Streaming data analytics is most often used to better understand change over time. For example, a marketing organization may perform sentiment analysis on social media messages to see if an advertising campaign results in more positive comments about the company or its products, or an agricultural business might monitor trends in temperature and rainfall to optimize irrigation and crop harvesting.\n\nCommon goals for stream analytics include\n\nContinuously analyzing data to report issues or trends.\nUnderstanding component or system behavior under various conditions to help plan future enhancements.\nTriggering specific actions or alerts when certain events occur or thresholds are exceeded.\nCharacteristics of stream processing solutions\n\nStream processing solutions typically exhibit the following characteristics:\n\nThe source data stream is unbounded - data is added to the stream perpetually.\nEach data record in the stream includes temporal (time-based) data indicating when the event to which the record relates occurred (or was recorded).\nAggregation of streaming data is performed over temporal windows - for example, recording the number of social media posts per minute or the average rainfall per hour.\nThe results of streaming data processing can be used to support real-time (or near real-time) automation or visualization, or persisted in an analytical store to be combined with other data for historical analysis. Many solutions combine these approaches to support both real-time and historical analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get started with Azure Stream Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nGet started with Azure Stream Analytics\nModule\n7 Units\nFeedback\nBeginner\nData Engineer\nAzure\nAzure Stream Analytics\n\nAzure Stream Analytics enables you to process real-time data streams and integrate the data they contain into applications and analytical solutions.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nUnderstand data streams.\nUnderstand event processing.\nUnderstand window functions.\nGet started with Azure Stream Analytics.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Microsoft Azure and have a basic knowledge of data storage and querying using SQL.\n\nIntroduction\nmin\nUnderstand data streams\nmin\nUnderstand event processing\nmin\nUnderstand window functions\nmin\nExercise - Get started with Azure Stream Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Link for SQL makes it possible to replicate data from SQL Server 2022 or Azure SQL Database to a dedicated pool in Azure Synapse Analytics with low latency. This replication enables you to analyze operational data in near-real-time without incurring a large resource utilization overhead on your transactional data store.\n\nIn this module, you learned how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Implement Azure Synapse Link for SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/5-exercise-synapse-link-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Implement Azure Synapse Link for SQL\n35 minutes\n\nNow it's your chance to explore Azure Synapse Link for SQL for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure SQL Database resource in your Azure subscription; and then you'll enable Azure Synapse Link for Azure SQL Database and use it to synchronize data.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nFrom which of the following data sources can you use Azure Synapse Link for SQL to replicate data to Azure Synapse Analytics?\n\n \n\nAzure Cosmos DB\n\nSQL Server 2022\n\nAzure SQL Managed Instance\n\n2. \n\nWhat must you create in your Azure Synapse Analytics workspace as a target database for Azure Synapse Link for Azure SQL Database?\n\n \n\nA serverless SQL pool\n\nAn Apache Spark pool\n\nA dedicated SQL pool\n\n3. \n\nYou plan to use Azure Synapse Link for SQL to replicate tables from SQL Server 2022 to Azure Synapse Analytics. What additional Azure resource must you create?\n\n \n\nAn Azure Storage account with an Azure Data Lake Storage Gen2 container\n\nAn Azure Key Vault containing the SQL Server admin password\n\nAn Azure Application Insights resource\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Azure Synapse Link for SQL Server 2022 - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/4-synapse-link-sql-server",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure Azure Synapse Link for SQL Server 2022\n5 minutes\n\nMicrosoft SQL Server is one of the world's most commonly used relational database systems. SQL Server 2022 is the latest release, and includes many enhancements and new features; including the ability to be used as a source for Azure Synapse Link.\n\nAzure Synapse Link for SQL Server uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a .parquet file for each source table to a landing zone in Azure Data Lake Storage Gen2; from where the data is imported into tables in the dedicated SQL pool. Subsequently, the change feed process copies all changes as .csv files to the landing zone where they're applied to the target tables.\n\nSynchronization between SQL Server (which can be on-premises or in a private network) and Azure Synapse Analytics is achieved through a self-hosted integration runtime. An integration runtime is a software agent that handles secure connectivity when using Azure Data Factory or Azure Synapse Analytics to transfer data across networks. It must be installed on a Microsoft Windows computer with direct access to your SQL Server instance.\n\n Tip\n\nFor more information about using a self-hosted integration runtime to work with Azure Synapse Analytics, see Create and configure a self-hosted integration runtime.\n\nImplementing Azure Synapse Link for SQL Server 2022\n\nTo use Azure Synapse Link for SQL Server 2022, you need to create storage for the landing zone in Azure and configure your SQL Server instance before creating a link connection in Azure Synapse Analytics.\n\nCreate landing zone storage\n\nYou need to create an Azure Data Lake Storage Gen2 account in your Azure subscription to use as a landing zone. You can't use the default storage for your Azure Synapse Analytics workspace.\n\n Tip\n\nFor more information about provisioning an Azure Data Lake Storage Gen2 account, see Create a storage account to use with Azure Data Lake Storage Gen2.\n\nCreate a master key in the SQL Server database\n\nTo support Azure Synapse Link, your SQL Server database must contain a master key. You can use a CREATE MASTER KEY SQL statement like the following example to create one:\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'my$ecretPa$$w0rd';\n\nCreate a dedicated SQL pool in Azure Synapse Analytics\n\nIn your Azure Synapse Analytics workspace, you need to create a dedicated SQL pool where the target tables will be created. You also need to create master key in this database by using the following SQL statement:\n\nCREATE MASTER KEY\n\nCreate a linked service for the SQL Server source database\n\nNext, in Azure Synapse Analytics, create a linked service for your SQL Server database. When you do this, you need to specify the self-hosted integration runtime to be used for connectivity between SQL Server and Azure Synapse Analytics. If you haven't already configured a self-hosted integration runtime, you can create one now, and then download and install the agent onto a Windows machine in the network where your SQL Server instance is located.\n\nCreate a linked service for your Data Lake Storage Gen2 account\n\nIn addition to the linked service for SQL Server, you need a linked service for the Data Lake Storage Gen2 account that will be used as a landing zone. To support this, you need to add the managed identity of your Azure Synapse Analytics Workspace to the Storage Blob Data Contributor role for your storage account and configure the linked service to use the managed identity for authentication.\n\nCreate a link connection for Azure Synapse Link\n\nFinally, you're ready to create a link connection for Azure Synapse Link data synchronization. As you do so, you'll specify the service link for the SQL Server source database, the individual tables to be replicated, the number of CPU cores to be used for the synchronization process, and the Azure Data Lake Storage Gen2 linked service and folder location for the landing zone.\n\nAfter the link connection is created, you can start it to initialize synchronization. After a short time, the tables will be available to query in the dedicated SQL pool, and will be kept in sync with modifications in the source database by the change feed process.\n\n Tip\n\nLearn more:\n\nFor more information about Synapse Link for SQL Server 2022, see Azure Synapse Link for SQL Server 2022.\nTo learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.\nFor a step-by-step guide to setting up Synapse Link for SQL Server 2022, see Get started with Azure Synapse Link for SQL Server 2022.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Azure Synapse Link for Azure SQL Database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/3-synapse-link-azure-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure Azure Synapse Link for Azure SQL Database\n5 minutes\n\nAzure SQL Database is a platform-as-a-service (PaaS) relational database service based on the SQL Server database engine. It's commonly used in cloud-native applications as a scalable, secure, and easy to manage relational database store for operational data.\n\nAzure Synapse Link for Azure SQL Database uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a snapshot of the source tables to the target tables. Subsequently, the change feed process applies all modifications made in the source tables to the target tables.\n\nImplementing Azure Synapse Link for Azure SQL Database\n\nTo use Azure Synapse Link for Azure SQL Database, you need to configure some settings in your Azure SQL Database server, before creating a link connection in Azure Synapse Analytics.\n\nConfigure Azure SQL Database\n\nBefore you can use Azure SQL Database as a source for a linked connection in Azure Synapse Analytics, you must ensure the following settings are configured in the Azure SQL Database server that hosts the database you want to synchronize:\n\nSystem assigned managed identity - enable this option so that your Azure SQL Database server uses a system assigned managed identity.\nFirewall rules - ensure that Azure services can access your Azure SQL Database server.\n\nIn addition to these server-level settings, if you plan to configure the link connection from Azure Synapse Analytics to use a managed identity when connecting to Azure SQL Database, you must create a user for the workspace identity in the database and add it to the db_owner role, as shown in the following code example:\n\nCREATE USER my_synapse_workspace FROM EXTERNAL PROVIDER;\nALTER ROLE [db_owner] ADD MEMBER my_synapse_workspace;\n\n\n Tip\n\nIf you intend to use SQL authentication, you can omit this step.\n\nPrepare the target SQL pool\n\nAzure Synapse Link for Azure SQL Database synchronizes the source data to tables in a dedicated SQL pool in Azure Synapse Analytics. You therefore need to create and start a dedicated SQL pool in your Azure Synapse Analytics workspace before you can create the link connection.\n\nThe database associated with the dedicated SQL pool must include the appropriate schema for the target table. If source tables are defined in a schema other than the default dbo schema, you must create a schema of the same name in the dedicated SQL pool database:\n\nCREATE SCHEMA myschema;\n\nCreate a link connection\n\nTo create a linked connection, add a linked connection on the Integrate page in Azure Synapse Studio. You'll need to:\n\nSelect or create a linked service for your Azure SQL Database. You can create this separately ahead of time, or as part of the process of creating a linked connection for Azure Synapse Link. You can use a managed identity or SQL authentication to connect the linked service to Azure SQL Database.\nSelect the tables in the source database that you want to include in the linked connection.\nSelect the target dedicated SQL pool in which the target tables should be created.\nSpecify the number of CPU cores you want to use to process synchronization. Four driver cores will be used in addition to the number of cores you specify.\n\nAfter creating the linked connection, you can configure the mappings between the source and target tables. In particular, you can specify the table structure (index) type and distribution configuration for the target tables.\n\n Note\n\nSome data types in your source tables may not be supported by specific dedicated SQL pool index types. For example, you cannot use a clustered columnstore index for tables that include VARBINARY(MAX) columns. You can map such tables to a heap (an unindexed table) in the dedicated SQL pool.\n\nWhen the linked connection is configured appropriately, you can start it to initialize synchronization. The source tables are initially copied to the target database as snapshots, and then subsequent data modifications are replicated.\n\n Tip\n\nLearn more:\n\nFor more information about Synapse Link for Azure SQL Database, see Azure Synapse Link for Azure SQL Database.\nTo learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.\nFor a step-by-step guide to setting up Synapse Link for Azure SQL Database, see Get started with Azure Synapse Link for Azure SQL Database. You'll also get a chance to try configuring Synapse Link for Azure SQL Database in the exercise, later in this module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "What is Azure Synapse Link for SQL? - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/2-understand-synapse-link-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhat is Azure Synapse Link for SQL?\n5 minutes\n\nMany organizations use a relational database in Azure SQL Database or Microsoft SQL Server to support business applications. These databases are optimized for transactional workloads that store and manipulate operational data. Performing analytical queries on the data in these databases to support reporting and data analysis incurs resource contention that can be detrimental to application performance.\n\nA traditional approach to resolving this problem is to implement an extract, transform, and load (ETL) solution that loads data from the operational data store into an analytical store as a batch operation at regular intervals. While this solution supports the analytical workloads required for reporting and data analysis, it suffers from the following limitations:\n\nThe ETL process can be complex to implement and operate.\nThe analytical store is only updated at periodic intervals, so reporting doesn't reflect the most up-to-date operational data.\nAzure Synapse Link for SQL\n\nAzure Synapse Link for SQL addresses the limitations of a traditional ETL process by automatically replicating changes made to tables in the operational database to corresponding tables in an analytical database. After the initial synchronization process, the changes are replicated in near real-time without the need for a complex ETL batch process.\n\nIn the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:\n\nAn Azure SQL Database or SQL Server 2022 instance contains a relational database in which transactional data is stored in tables.\nAzure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.\nThe replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.\nSource and target databases\n\nAzure Synapse Link for SQL supports the following source databases (used as operational data stores):\n\nAzure SQL Database\nMicrosoft SQL Server 2022\n\n Note\n\nAzure Synapse link for SQL is not supported for Azure SQL Managed Instance.\n\nThe target database (used as an analytical data store) must be a dedicated SQL pool in an Azure Synapse Analytics workspace.\n\nThe implementation details for Azure Synapse Link vary between the two types of data source, but the high-level principle is the same - changes made to tables in the source database are synchronized to the target database.\n\nChange feed\n\nAzure Synapse Link for SQL uses the change feed feature in Azure SQL Database and Microsoft SQL Server 2022 to capture changes to the source tables. All data modifications are recorded in the transaction log for the source database. The change feed feature monitors the log and applies the same data modifications in the target database. In the case of Azure SQL Database, the modifications are made directly to the target database. When using Azure Synapse Link for SQL Server, the changes are recorded in files and saved to a landing zone in Azure Data Lake Gen2 storage before being applied to the target database.\n\n Note\n\nChange feed is similar to the change data capture (CDC) feature in SQL Server. The key difference is that CDC is used to reproduce data modifications in a table in the same database as the modified table. Change feed caches the data modification in memory and forwards it to Azure Synapse Analytics.\n\nAfter implementing Azure Synapse Link for SQL, you can use system views and stored procedures in your Azure SQL Database or SQL Server database to monitor and manage change feed activity.\n\n Tip\n\nLearn more:\n\nFor more information about change feed, see Azure Synapse Link for SQL change feed.\nTo learn more about monitoring and managing change feed, see Manage Azure Synapse Link for SQL Server and Azure SQL Database.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Link for SQL is a hybrid transactional / analytical processing (HTAP) capability in Azure Synapse Analytics that you can use to synchronize transactional data in Azure SQL Database or Microsoft SQL Server with a dedicated SQL pool in Azure Synapse Analytics. This synchronization enables you to perform near real-time analytical workloads on operational data with minimal impact on the transactional store used by business applications.\n\nIn this module, you'll learn how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement Azure Synapse Link for SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nImplement Azure Synapse Link for SQL\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure SQL Database\nAzure Synapse Analytics\nSQL Server\n\nAzure Synapse Link for SQL enables low-latency synchronization of operational data in a relational database to Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Synapse Analytics, Azure SQL Database, and SQL Server. Consider completing Explore relational database services in Azure and Introduction to Azure Synapse Analytics first.\n\nIntroduction\nmin\nWhat is Azure Synapse Link for SQL?\nmin\nConfigure Azure Synapse Link for Azure SQL Database\nmin\nConfigure Azure Synapse Link for SQL Server 2022\nmin\nExercise - Implement Azure Synapse Link for SQL\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/9-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nHTAP integration between Azure Cosmos DB and Azure Synapse Analytics enables a range of analytical workloads, including:\n\nSupply chain analytics, forecasting & reporting.\nReal-time personalization.\nIOT predictive maintenance.\n\nIn this module, you learned how to:\n\nConfigure an Azure Cosmos DB Account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\n\nTo learn more about using Azure Synapse Link to enable analytics scenarios, see Azure Synapse Link for Azure Cosmos DB: Near real-time analytics use cases.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/8-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nYou have an Azure Cosmos DB for NoSQL account and an Azure Synapse Analytics workspace. What must you do first to enable HTAP integration with Azure Synapse Analytics?\n\n \n\nConfigure global replication in Azure Cosmos DB.\n\nCreate a dedicated SQL pool in Azure Synapse Analytics.\n\nEnable Azure Synapse Link in Azure Cosmos DB.\n\n2. \n\nYou have an existing container in a Cosmos DB core (SQL) database. What must you do to enable analytical queries over Azure Synapse Link from Azure Synapse Analytics?\n\n \n\nDelete and recreate the container.\n\nEnable Azure Synapse Link in the container to create an analytical store.\n\nAdd an item to the container.\n\n3. \n\nYou plan to use a Spark pool in Azure Synapse Analytics to query an existing analytical store in Azure Cosmos DB. What must you do?\n\n \n\nCreate a linked service for the Azure Cosmos DB database where the analytical store enabled container is defined.\n\nDisable automatic pausing for the Spark pool in Azure Synapse Analytics.\n\nInstall the Azure Cosmos DB SDK for Python package in the Spark pool.\n\n4. \n\nYou're writing PySpark code to load data from an Azure Cosmos DB analytical store into a dataframe. What format should you specify?\n\n \n\ncosmos.json\n\ncosmos.olap\n\ncosmos.sql\n\n5. \n\nYou're writing a SQL code in a serverless SQL pool to query an analytical store in Azure Cosmos DB. What function should you use?\n\n \n\nOPENDATASET\n\nROW\n\nOPENROWSET\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Implement Azure Synapse Link for Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/7-exercise-synapse-link-cosmos-db",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Implement Azure Synapse Link for Cosmos DB\n35 minutes\n\nNow it's your chance to explore Azure Synapse Link for Azure Cosmos DB for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure Cosmos DB account in your Azure subscription; and then you'll enable Azure Synapse Link for Azure Cosmos DB and use it to analyze data with Spark and SQL.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query Cosmos DB with Synapse SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/6-query-with-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery Cosmos DB with Synapse SQL\n5 minutes\n\nIn addition to using a Spark pool, you can also query an Azure Cosmos DB analytical container by using a built-in serverless SQL pool in Azure Synapse Analytics. To do this, you can use the OPENROWSET SQL function to connect to the linked service for your Azure Cosmos DB database.\n\nUsing OPENROWSET with an authentication key\n\nBy default, access to an Azure Cosmos DB account is authenticated by an authentication key. You can use this key as part of a connection string in an OPENROWSET statement to connect through a linked service from a SQL pool, as shown in the following example:\n\nSELECT *\nFROM OPENROWSET(​\n    'CosmosDB',\n    'Account=my-cosmos-db;Database=my-db;Key=abcd1234....==',\n    [my-container]) AS products_data\n\n\n Tip\n\nYou can find a primary and secondary key for your Cosmos DB account on its Keys page in the Azure portal.\n\nThe results of this query might look something like the following, including metadata and application-defined fields from the items in the Azure Cosmos DB container:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\n\nThe data is retrieved from the analytical store, and the query doesn't impact the operational store.\n\nUsing OPENROWSET with a credential\n\nInstead of including the authentication key in each call to OPENROWSET, you can define a credential that encapsulates the authentication information for your Cosmos DB account, and use the credential in subsequent queries. To create a credential, use the CREATE CREDENTIAL statement as shown in this example:\n\n CREATE CREDENTIAL my_credential\n WITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n SECRET = 'abcd1234....==';\n\n\nWith the credential in place, you can use it in an OPENROWSET function like this:\n\nSELECT *\nFROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                OBJECT = 'my-container',\n                SERVER_CREDENTIAL = 'my_credential'\n) AS products_data\n\n\nOnce again, the results include metadata and application-defined fields from the analytical store:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\nSpecifying a schema\n\nThe OPENROWSET syntax includes a WITH clause that you can use to define a schema for the resulting rowset. You can use this to specify individual fields and assign data types as shown in the following example:\n\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    productID INT,\n    productName VARCHAR(20)\n ) AS products_data\n\n\nIn this case, assuming the fields in the analytical store include productID and productName, the resulting rowset will resemble the following table:\n\nExpand table\nproductID\tproductName\n123\tWidget\n124\tWotsit\n125\tThingumy\n...\t...\n\nYou can of course specify individual column names in the SELECT clause (for example, SELECT productID, productName ...), so this ability to specify individual columns may seem of limited use. However, consider cases where the source JSON documents stored in the operational store include multiple levels of fields, as show in the following example:\n\n{\n    \"productID\": 126,\n    \"productName\": \"Sprocket\",\n    \"supplier\": {\n        \"supplierName\": \"Contoso\",\n        \"supplierPhone\": \"555-123-4567\"\n    }\n    \"id\": \"62588f072-11c3-42b1-a738-...\",\n    \"_rid\": \"mjMaAL...==\",\n    ...\n}\n\n\nThe WITH clause supports the inclusion of explicit JSON paths, enabling you to handle nested fields and to assign aliases to field names; as shown in this example:\n\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    ProductNo INT '$.productID',\n    ProductName VARCHAR(20) '$.productName',\n    Supplier VARCHAR(20) '$.supplier.supplierName',\n    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'\n ) AS products_data\n\n\nThe results of this query would include the following row for product 126:\n\nExpand table\nProductNo\tProductName\tSupplier\tSupplierPhoneNo\n126\tSprocket\tContoso\t555-123-4567\nCreating a view in a database\n\nIf you need to query the same data frequently, or you need to use reporting and visualization tools that rely on SELECT statements that don't include the OPENROWSET function, you can use a view to abstract the data. To create a view, you should create a new database in which to define it (user-defined views in the master database aren't supported), as shown in the following example:\n\nCREATE DATABASE sales_db\n   COLLATE Latin1_General_100_BIN2_UTF8;\n GO;\n\n USE sales_db;\n GO;\n\n CREATE VIEW products\n AS\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    ProductNo INT '$.productID',\n    ProductName VARCHAR(20) '$.productName',\n    Supplier VARCHAR(20) '$.supplier.supplierName',\n    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'\n ) AS products_data\n GO\n\n\n Tip\n\nWhen creating a database that will access data in Cosmos DB, it's best to use a UTF-8 based collation to ensure compatibility with strings in Cosmos DB.\n\nAfter the view has been created, users and client applications can query it like any other SQL view or table:\n\nSELECT * FROM products;\n\nConsiderations for Serverless SQL pools and Azure Cosmos DB\n\nWhen planning to use a serverless SQL pool to query data in an Azure Cosmos DB analytical store, consider the following best practices:\n\nProvision your Azure Cosmos DB analytical storage and any client applications (for example Microsoft Power BI) in the same region as serverless SQL pool.\n\nAzure Cosmos DB containers can be replicated to multiple regions. If you have a multi-region container, you can specify a region parameter in the OPENROWSET connection string to ensure queries are sent to a specific regional replica of the container.\n\nWhen working with string columns, use the OPENROWSET function with the explicit WITH clause and specify an appropriate data length for the string data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query Cosmos DB data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/5-query-with-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery Cosmos DB data with Spark\n5 minutes\n\nAfter you've added a linked service for your analytical store enabled Azure Cosmos DB database, you can use it to query the data using a Spark pool in your Azure Synapse Analytics workspace.\n\nLoading Azure Cosmos DB analytical data into a dataframe\n\nFor initial exploration or quick analysis of data from an Azure Cosmos DB linked service, it's often easiest to load data from a container into a dataframe using a Spark-supported language like PySpark (A Spark-specific implementation of Python) or Scala (a Java-based language often used on Spark).\n\nFor example, the following PySpark code could be used to load a dataframe named df from the data in the my-container container connected to using the my_linked_service linked service, and display the first 10 rows of data:\n\n df = spark.read\n     .format(\"cosmos.olap\")\\\n     .option(\"spark.synapse.linkedService\", \"my_linked_service\")\\\n     .option(\"spark.cosmos.container\", \"my-container\")\\\n     .load()\n\ndisplay(df.limit(10))\n\n\nLet's suppose the my-container container is used to store items similar to the following example:\n\n{\n    \"productID\": 123,\n    \"productName\": \"Widget\",\n    \"id\": \"7248f072-11c3-42b1-a368-...\",\n    \"_rid\": \"mjMaAL...==\",\n    \"_self\": \"dbs/mjM...==/colls/mjMaAL...=/docs/mjMaAL...==/\",\n    \"_etag\": \"\\\"54004b09-0000-2300-...\\\"\",\n    \"_attachments\": \"attachments/\",\n    \"_ts\": 1655414791\n}\n\n\nThe output from the PySpark code would be similar to the following table:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\n\nThe data is loaded from the analytical store in the container, not from the operational store; ensuring that there's no querying overhead on the operational store. The fields in the analytical data store include the application-defined fields (in this case productID and productName) and automatically created metadata fields.\n\nAfter loading the dataframe, you can use its native methods to explore the data. For example, the following code creates a new dataframe containing only the productID and productName columns, ordered by the productName:\n\nproducts_df = df.select(\"productID\", \"productName\").orderBy(\"productName\")\n\ndisplay(products_df.limit(10))\n\n\nThe output of this code would look similar this table:\n\nExpand table\nproductID\tproductName\n125\tThingumy\n123\tWidget\n124\tWotsit\n...\t...\nWriting a dataframe to a Cosmos DB container\n\nIn most HTAP scenarios, you should use the linked service to read data into Spark from the analytical store. However you can write the contents of a dataframe to the container as shown in the following example:\n\nmydf.write.format(\"cosmos.oltp\")\\\n    .option(\"spark.synapse.linkedService\", \"my_linked_service\")\\\n    .option(\"spark.cosmos.container\", \"my-container\")\\\n    .mode('append')\\\n    .save()\n\n\n Note\n\nWriting a dataframe to a container updates the operational store and can have an impact on its performance. The changes are then synchronized to the analytical store.\n\nUsing Spark SQL to query Azure Cosmos DB analytical data\n\nSpark SQL is a Spark API that provides SQL language syntax and relational database semantics in a Spark pool. You can use Spark SQL to define metadata for tables that can be queried using SQL.\n\nFor example, the following code creates a table named Products based on the hypothetical container used in the previous examples:\n\n%%sql\n\n-- Create a logical database in the Spark metastore\nCREATE DATABASE mydb;\n\nUSE mydb;\n\n-- Create a table from the Cosmos DB container\nCREATE TABLE products using cosmos.olap options (\n    spark.synapse.linkedService 'my_linked_service',\n    spark.cosmos.container 'my-container'\n);\n\n-- Query the table\nSELECT productID, productName\nFROM products;\n\n\n Tip\n\nThe %%sql keyword at the beginning of the code is a magic that instructs the Spark pool to run the code as SQL rather than the default language (which is usually set to PySpark).\n\nBy using this approach, you can create a logical database in your Spark pool that you can then use to query the analytical data in Azure Cosmos DB to support data analysis and reporting workloads without impacting the operational store in your Azure Cosmos DB account.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a linked service for Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/4-implement-synapse-link",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a linked service for Cosmos DB\n5 minutes\n\nWhen you have an Azure Cosmos DB container with analytical store support, you can create a linked service in an Azure Synapse Analytics workspace to connect to it.\n\nTo create a linked service to an Azure Cosmos DB analytical data store, use Azure Synapse Studio, and add a linked service on the Data page by selecting the Connect to external data option, as shown here:\n\nAs you complete the steps to create your linked service, select the type of Azure Cosmos DB account and then assign your linked service a meaningful name and provide the necessary information to connect to your Azure Cosmos DB database.\n\nTo connect to the Azure Cosmos DB database, you can use any of the following authentication options:\n\nAccount key: Specify an authentication key for your Cosmos DB account.\nService Principal: Use the identity of the Azure Synapse Analytics service.\nSystem Assigned Managed Identity: Use system-assigned managed identity.\nUser Managed Identity: Use a user-defined managed identity.\n\n Tip\n\nFor more information about using managed identities in Microsoft Entra ID, see What are managed identities for Azure resources?\n\nAfter creating a linked service, the Azure Cosmos DB database and its containers will be shown in the Data page of Azure Synapse Studio, as shown here:\n\n Note\n\nThe user interface differentiates between containers with analytical store support and those without by using the following icons:\n\nExpand table\nAnalytical store enabled\tAnalytical store not enabled\n\t\n\nYou can query a container without an analytical store, but you won't benefit from the advantages of an HTAP solution that offloads analytical query overhead from the operational data store.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create an analytical store enabled container - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/3-create-analytical-store-enabled-container",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate an analytical store enabled container\n5 minutes\n\nAfter enabling Azure Synapse Link in an Azure Cosmos DB account, you can create or update a container with support for an analytical store.\n\nAn analytical store is a column-based store within the same container as a row-based operational store. An auto-sync process synchronizes changes in the operational store to the analytical store; from where it can be queried without incurring processing overhead in the operational store.\n\nAnalytical store schema types\n\nAs the data from the operational store is synchronized to the analytical store, the schema is updated dynamically to reflect the structure of the documents being synchronized. The specific behavior of this dynamic schema maintenance depends on the analytical store schema type configured for the Azure Cosmos DB account. Two types of schema representation are supported:\n\nWell-defined: The default schema type for an Azure Cosmos DB for NoSQL account.\nFull fidelity: The default (and only supported) schema type for an Azure Cosmos DB for MongoDB account.\n\nThe analytical store receives JSON data from the operational store and organizes it into a column-based structure. In a well-defined schema, the first non-null occurrence of a JSON field determines the data type for that field. Subsequent occurrences of the field that aren't compatible with the assigned data type aren't ingested into the analytical store.\n\nFor example, consider the following two JSON documents:\n\n{\"productID\": 123, \"productName\": \"Widget\"}\n{\"productID\": \"124\", \"productName\": \"Wotsit\"}\n\n\nThe first document determines that the productID field is a numeric (integer) value. When the second document is encountered, its productID field has a string value, and so isn't imported into the analytical store. The document and the rest of its field is imported, but the incompatible field is dropped. The following columns represent the data in the analytical store:\n\nExpand table\nproductID\tproductName\n123\tWidget\n\tWotsit\n\nIn a full fidelity schema, the data type is appended to each instance of the field, with new columns created as necessary; enabling the analytical store to contain multiple occurrences of a field, each with a different data type, as shown in the following table:\n\nExpand table\nproductID.int32\tproductName.string\tproductID.string\n123\tWidget\t\n\tWotsit\t124\n\n Note\n\nFor more information, see What is Azure Cosmos DB analytical store?.\n\nEnabling analytical store support in a container\n\nYou can enable analytical store support when creating a new container or for an existing container. To enable analytical store support, you can use the Azure portal, or you can use the Azure CLI or Azure PowerShell from a command line or in a script.\n\nUsing the Azure portal\n\nTo enable analytical store support when creating a new container in the Azure portal, select the On option for Analytical Store, as shown here:\n\nAlternatively, you can enable analytical store support for an existing container in the Azure Synapse Link page in the Integrations section of the page for your Cosmos DB account, as shown here:\n\nUsing the Azure CLI\n\nTo use the Azure CLI to enable analytical store support in an Azure Cosmos DB for NoSQL container, run the az cosmosdb sql container create command (to create a new container) or az cosmosdb sql container update command (to configure an existing container) with the --analytical-storage-ttl parameter, assigning a retention time for analytical data. Specifying an -analytical-storage-ttl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.\n\naz cosmosdb sql container create --resource-group my-rg --account-name my-cosmos-db --database-name my-db --name my-container --partition-key-path \"/productID\" --analytical-storage-ttl -1\n\n\nFor an Azure Cosmos DB for MongoDB account, you can use the az cosmosdb mongodb collection create or az cosmosdb mongodb collection update command with the --analytical-storage-ttl parameter. For an Azure Cosmos DB for Apache Gremlin account, use the az cosmosdb gremlin graph create or az cosmosdb gremlin graph update command with the --analytical-storage-ttl parameter.\n\nUsing Azure PowerShell\n\nTo use Azure PowerShell to enable analytical store support in n Azure Cosmos DB for NoSQL container, run the New-AzCosmosDBSqlContainer cmdlet (to create a new container) or Update-AzCosmosDBSqlContainer cmdlet (to configure an existing container) with the -AnalyticalStorageTtl parameter, assigning a retention time for analytical data. Specifying an -AnalyticalStorageTtl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.\n\nNew-AzCosmosDBSqlContainer -ResourceGroupName \"my-rg\" -AccountName \"my-cosmos-db\" -DatabaseName \"my-db\" -Name \"my-container\" -PartitionKeyKind \"hash\" -PartitionKeyPath \"/productID\" -AnalyticalStorageTtl -1\n\n\nFor an Azure Cosmos DB for MongoDB API account, use the New-AzCosmosDBMongoDBCollection or Update-AzCosmosDBMongoDBCollection cmdlet with the -AnalyticalStorageTtl parameter.\n\nConsiderations for enabling analytical store support\n\nAnalytical store support can't be disabled without deleting the container. Setting the analytical store TTL value to 0 or null effectively disables the analytical store by no longer synchronizing new items to it from the operational store and deleting items already synchronized from the analytical store. After setting this value to 0, you can't re-enable analytical store support in the container.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enable Cosmos DB account to use Azure Synapse Link - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/2-enable-cosmos-db-account-to-use",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEnable Cosmos DB account to use Azure Synapse Link\n5 minutes\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables integration between Azure Cosmos DB and Azure Synapse Analytics.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:\n\nAn Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.\nThe container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.\nAzure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.\nAzure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.\nEnabling Azure Synapse Link in Azure Cosmos DB\n\nThe first step in using Azure Synapse Link for Cosmos DB is to enable it in an Azure Cosmos DB account. Azure Synapse Link is supported in the following types of Azure Cosmos DB account:\n\nAzure Cosmos DB for NoSQL\nAzure Cosmos DB for MongoDB\nAzure Cosmos DB for Apache Gremlin (preview)\n\nYou can enable Azure Synapse Link in the Azure portal page for your Cosmos DB account, or by using the Azure CLI or Azure PowerShell from a command line or in a script.\n\nUsing the Azure portal\n\nIn the Azure portal, you can enable Azure Synapse Link for a Cosmos DB account on the Azure Synapse Link page in the Integrations section, as shown below.\n\n Tip\n\nFor Azure Cosmos DB for NoSQL accounts, there's also a link on the Data Explorer page.\n\nUsing the Azure CLI\n\nTo enable Azure Synapse Link using the Azure CLI, run the az cosmosdb create command (to create a new Cosmos DB account) or az cosmosdb update command (to configure an existing Cosmos DB account) with the --enable-analytical-storage true parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.\n\naz cosmosdb update --name my-cosmos-db --resource-group my-rg --enable-analytical-storage true\n\n\nTo enable Azure Synapse Link for an Azure Cosmos DB for Apache Gremlin account, include the --capabilities EnableGremlin parameter.\n\nUsing Azure PowerShell\n\nTo enable Azure Synapse Link using Azure PowerShell, run the New-AzCosmosDBAccount cmdlet (to create a new Cosmos DB account) or Update-AzCosmosDBAccount cmdlet (to configure an existing Cosmos DB account) with the -EnableAnalyticalStorage 1 parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.\n\nUpdate-AzCosmosDBAccount -Name \"my-cosmos-db\" -ResourceGroupName \"my-rg\" -EnableAnalyticalStorage 1\n\nConsiderations for enabling Azure Synapse Link\n\nWhen planning to enable Azure Synapse Link for a Cosmos DB account, consider the following facts:\n\nAfter enabling Azure Synapse Link for an account, you can't disable it.\n\nEnabling Azure Synapse Link doesn't start synchronization of operational data to an analytical store - you must also create or update a container with support for an analytical store.\n\nWhen enabling Azure Synapse Link for a Cosmos DB for NoSQL account using the Azure CLI or PowerShell, you can use the --analytical-storage-schema-type (Azure CLI) or -AnalyticalStorageSchemaType (PowerShell) parameter to specify the schema type as WellDefined (default) or FullFidelity. For a Cosmos DB for MongoDB account, the default (and only supported) schema type is FullFidelity.\n\nAfter a schema type has been assigned, you can't change it.\n\n Note\n\nYou'll learn more about the analytical store and its schema types in the next unit.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Analytics Link for Cosmos DB enables hybrid transactional/analytical processing (HTAP) integration between Azure Cosmos DB and Azure Synapse Analytics. By using this HTAP solution, organizations can make operational data in Azure Cosmos DB available for analysis and reporting in Azure Synapse Analytics in near-real time without the need to develop a complex ETL pipeline.\n\nIn this module, you'll learn how to:\n\nConfigure an Azure Cosmos DB account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement Azure Synapse Link with Azure Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nImplement Azure Synapse Link with Azure Cosmos DB\nModule\n9 Units\nFeedback\nIntermediate\nData Engineer\nAzure Cosmos DB\nAzure Synapse Analytics\n\nAzure Synapse Link for Azure Cosmos DB enables HTAP integration between operational data in Azure Cosmos DB and Azure Synapse Analytics runtimes for Spark and SQL.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nConfigure an Azure Cosmos DB Account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of Azure Cosmos DB and Azure Synapse Analytics. Consider completing the following modules first:\n\nExplore fundamentals of Azure Cosmos DB\nExplore fundamentals of large-scale data warehousing\nIntroduction\nmin\nEnable Cosmos DB account to use Azure Synapse Link\nmin\nCreate an analytical store enabled container\nmin\nCreate a linked service for Cosmos DB\nmin\nQuery Cosmos DB data with Spark\nmin\nQuery Cosmos DB with Synapse SQL\nmin\nExercise - Implement Azure Synapse Link for Cosmos DB\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/5-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nHTAP solutions enable extensive, near-realtime data analytics without impacting the performance of operational data stores. Azure Synapse Link offers multiple ways to create HTAP solutions that integrate operational data in commonly used data stores with Azure Synapse Analytics.\n\nIn this module, you learned to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\n\nFor more information about Azure Synapse Link, see the following articles in the Azure Synapse Analytics documentation:\n\nWhat is Azure Synapse Link for Azure Cosmos DB?\nWhat is Azure Synapse Link for SQL?\nWhat is Azure Synapse Link for Dataverse?\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/4-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich of the following descriptions matches a hybrid transactional/analytical processing (HTAP) architecture.\n\n \n\nBusiness applications store data in an operational data store, which is also used to support analytical queries for reporting.\n\nBusiness applications store data in an operational data store, which is synchronized with low latency to a separate analytical store for reporting and analysis.\n\nBusiness applications store operational data in an analytical data store that is optimized for queries to support reporting and analysis.\n\n2. \n\nYou want to use Azure Synapse Analytics to analyze operational data stored in a Cosmos DB for NoSQL container. Which Azure Synapse Link service should you use?\n\n \n\nAzure Synapse Link for SQL\n\nAzure Synapse Link for Dataverse\n\nAzure Synapse Link for Azure Cosmos DB\n\n3. \n\nYou plan to use Azure Synapse Link for Dataverse to analyze business data in your Azure Synapse Analytics workspace. Where is the replicated data from Dataverse stored?\n\n \n\nIn an Azure Synapse dedicated SQL pool\n\nIn an Azure Data Lake Gen2 storage container.\n\nIn an Azure Cosmos DB container.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Describe Azure Synapse Link - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/3-azure-synapse-link",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDescribe Azure Synapse Link\n6 minutes\n\nHTAP solutions are supported in Azure Synapse Analytics through Azure Synapse Link; a general term for a set of linked services that support HTAP data synchronization into your Azure Synapse Analytics workspace.\n\nAzure Synapse Link for Cosmos DB\n\nAzure Cosmos DB is a global-scale NoSQL data service in Microsoft Azure that enables applications to store and access operational data by using a choice of application programming interfaces (APIs).\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables you to run near-real-time analytics over operational data stored in a Cosmos DB container. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:\n\nAn Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.\nThe container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.\nAzure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.\nAzure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.\nAzure Synapse Link for SQL\n\nMicrosoft SQL Server is a popular relational database system that powers business applications in some of the world's largest organizations. Azure SQL Database is a cloud-based platform-as-a-service database solution based on SQL Server. Both of these relational database solutions are commonly used as operational data stores.\n\nAzure Synapse Link for SQL enables HTAP integration between data in SQL Server or Azure SQL Database and an Azure Synapse Analytics workspace.\n\nIn the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:\n\nAn Azure SQL Database or SQL Server instance contains a relational database in which transactional data is stored in tables.\nAzure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.\nThe replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.\nAzure Synapse Link for Dataverse\n\nMicrosoft Dataverse is data storage service within the Microsoft Power Platform. You can use Dataverse to store business data in tables that are accessed by Power Apps, Power BI, Power Virtual Agents, and other applications and services across Microsoft 365, Dynamics 365, and Azure.\n\nAzure Synapse Link for Dataverse enables HTAP integration by replicating table data to Azure Data Lake storage, where it can be accessed by runtimes in Azure Synapse Analytics - either directly from the data lake or through a Lake Database defined in a serverless SQL pool.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Dataverse architecture are illustrated:\n\nBusiness applications store data in Microsoft Dataverse tables.\nAzure Synapse Link for Dataverse replicates the table data to an Azure Data Lake Gen2 storage account associated with an Azure Synapse workspace.\nThe data in the data lake can be used to define tables in a lake database and queried using a serverless SQL pool, or read directly from storage using SQL or Spark.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand hybrid transactional and analytical processing patterns - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/2-understand-patterns",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand hybrid transactional and analytical processing patterns\n6 minutes\n\nMany business application architectures separate transactional and analytical processing into separate systems with data stored and processed on separate infrastructures. These infrastructures are commonly referred to as OLTP (online transaction processing) systems working with operational data, and OLAP (online analytical processing) systems working with historical data, with each system is optimized for their specific task.\n\nOLTP systems are optimized for dealing with discrete system or user requests immediately and responding as quickly as possible.\n\nOLAP systems are optimized for the analytical processing, ingesting, synthesizing, and managing large sets of historical data. The data processed by OLAP systems largely originates from OLTP systems and needs to be loaded into the OLAP systems by ETL (Extract, Transform, and Load) batch processes.\n\nDue to their complexity and the need to physically copy large amounts of data, this approach creates a delay in data being available to analyze in OLAP systems.\n\nHybrid Transactional / Analytical Processing (HTAP)\n\nAs more businesses move to digital processes, they increasingly recognize the value of being able to respond to opportunities by making faster and well-informed decisions. HTAP (Hybrid Transactional/Analytical processing) enables business to run advanced analytics in near-real-time on data stored and processed by OLTP systems.\n\nThe following diagram illustrates the generalized pattern of an HTAP architecture:\n\nA business application processes user input and stores data in a transactional database that is optimized for a mix of data reads and writes based on the application's expected usage profile.\nThe application data is automatically replicated to an analytical store with low latency.\nThe analytical store supports data modeling, analytics, and reporting without impacting the transactional system.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nHybrid Transactional / Analytical Processing (HTAP) is a style of data processing that combines transactional data processing, such as is typically found in a business application, with analytical processing, such as is used in a business intelligence (BI) or reporting solution. The data access patterns and storage optimizations used in these two kinds of workload are very different, so usually a complex extract, transform, and load (ETL) process is required to copy data out of transactional systems and into analytical systems; adding complexity and latency to data analysis. In an HTAP solution, the transactional data is replicated automatically, with low-latency, to an analytical store, where it can be queried without impacting the performance of the transactional system.\n\nIn Azure Synapse Analytics, HTAP capabilities are provided by multiple Azure Synapse Link services, each connecting a commonly used transactional data store to your Azure Synapse Analytics workspace and making the data available for processing using Spark or SQL.\n\nAfter completing this module, you'll be able to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Plan hybrid transactional and analytical processing using Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nPlan hybrid transactional and analytical processing using Azure Synapse Analytics\nModule\n5 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how hybrid transactional / analytical processing (HTAP) can help you perform operational analytics with Azure Synapse Analytics.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nDescribe Hybrid Transactional / Analytical Processing patterns.\nIdentify Azure Synapse Link services for HTAP.\nAdd\nPrerequisites\n\nBefore starting this module, you should have a basic knowledge of data analytics and Azure services for data. Consider completing the Azure Data Fundamentals certification first.\n\nIntroduction\nmin\nUnderstand hybrid transactional and analytical processing patterns\nmin\nDescribe Azure Synapse Link\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark offers data engineers a powerful platform for transforming and processing data. The ability to include Spark notebooks in a pipeline enables you to automate Spark processing and integrate it into a data integration workflow.\n\n Tip\n\nTo learn more about using Spark notebooks in an Azure Synapse Analytics pipeline, see Transform data by running a Synapse notebook in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use an Apache Spark notebook in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/5-exercise-use-spark-notebooks-pipeline",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use an Apache Spark notebook in a pipeline\n30 minutes\n\nNow it's your chance to integrate spark into an Azure Synapse Analytics pipeline. In this exercise, you'll create a pipeline that includes a notebook activity, and configure parameters for the notebook.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhat kind of pool is required to run a Synapse notebook in a pipeline?\n\n \n\nA Dedicated SQL pool\n\nA Data Explorer pool\n\nAn Apache Spark pool\n\n2. \n\nWhat kind of pipeline activity encapsulates a Synapse notebook?\n\n \n\nNotebook activity\n\nHDInsight Spark activity\n\nScript activity\n\n3. \n\nA notebook cell contains variable declarations. How can you use them as parameters?\n\n \n\nAdd a %%Spark magic at the beginning of the cell\n\nToggle the Parameters cell setting for the cell\n\nUse the var keyword for each variable declaration\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use parameters in a notebook - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/4-notebook-parameters",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse parameters in a notebook\n5 minutes\n\nParameters enable you to dynamically pass values for variables in the notebook each time it's run. This approach provides flexibility, enabling you to adjust the logic encapsulated in the notebook for each run.\n\nCreate a parameters cell in the notebook\n\nTo define the parameters for a notebook, you declare and initialize variables in a cell, which you then configure as a Parameters cell by using the toggle option in the notebook editor interface.\n\nInitializing a variable ensures that it has a default value, which will be used if the parameter isn't set in the notebook activity.\n\nSet base parameters for the notebook activity\n\nAfter defining a parameters cell in the notebook, you can set values to be used when the notebook is run by a notebook activity in a pipeline. To set parameter values, expand and edit the Base parameters section of the settings for the activity.\n\nYou can assign explicit parameter values, or use an expression to assign a dynamic value. For example, the expression @pipeline().RunId returns the unique identifier for the current run of the pipeline.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a Synapse notebook activity in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/3-use-notebook-activity",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a Synapse notebook activity in a pipeline\n3 minutes\n\nTo run a Spark notebook in a pipeline, you must add a notebook activity and configure it appropriately. You'll find the Notebook activity in the Synapse section of the activities pane in the Azure Synapse Analytics pipeline designer.\n\n Tip\n\nYou can also add a notebook to a pipeline from within the notebook editor.\n\nTo configure the notebook activity, edit the settings in the properties pane beneath the pipeline designer canvas. Notebook activity specific settings include:\n\nNotebook: The notebook you want to run. You can select an existing notebook in your Azure Synapse Analytics workspace, or create a new one.\nSpark pool: The Apache Spark pool on which the notebook should be run.\nExecutor size: The node size for the worker nodes in the pool, which determines the number of processor cores and the amount of memory allocated to worker nodes.\nDynamically allocate executors: Configures Spark dynamic allocation, enabling the pool to automatically scale up and down to support the workload.\nMin executors: The minimum number of executors to be allocated.\nMax executors: The maximum number of executors to be allocated.\nDriver size: The node size for the driver node.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Synapse Notebooks and Pipelines - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/2-understand-notebooks-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Synapse Notebooks and Pipelines\n5 minutes\n\nAzure Synapse Pipelines enable you to create, run, and manage data integration and data flow activities. While many of these activities are built-into the Azure Synapse Pipeline platform and run natively in the integration runtime for your pipeline, you can also use external processing resources to perform specific tasks. One such external resource is an Apache Spark pool in your Azure Synapse Analytics workspace on which you can run code in a notebook.\n\nIt's common in big data analytics solutions for data engineers to use Spark notebooks for initial data exploration and interactive experimentation when designing data transformation processes. When the transformation logic has been completed, you can perform some final code optimization and refactoring for maintainability, and then include the notebook in a pipeline. The pipeline can then be run on a schedule or in response to an event (such as new data files being loaded into the data lake).\n\nThe notebook is run on a Spark pool, which you can configure with the appropriate compute resources and Spark runtime for your specific workload. The pipeline itself is run in an integration runtime that orchestrates the activities in the pipeline, coordinating the external services needed to run them.\n\n Tip\n\nThere are several best practices that can help make working with Spark notebooks more efficient and effective. Some of these include:\n\nKeep your code organized: Use clear and descriptive variable and function names, and organize your code into small, reusable chunks.\nCache intermediate results: Spark allows you to cache intermediate results, which can significantly speed up the performance of your notebook.\nAvoid unnecessary computations: Be mindful of the computations you are performing and try to avoid unnecessary steps. For example, if you only need a subset of your data, filter it out before running any further computations.\nAvoid using collect() unless necessary: When working with large datasets, it is often better to perform operations on the entire dataset rather than bringing the data into the driver node using the collect() method.\nUse Spark UI for monitoring and debugging: Spark's web-based user interface (UI) provides detailed information about the performance of your Spark jobs, including task execution times, input and output data sizes, and more.\nKeep your dependencies version-consistent and updated: when working with Spark, it is important to keep dependencies version-consistent across your cluster and to use the latest version of Spark and other dependencies if possible.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWith Azure Synapse Analytics pipelines, you can orchestrate data transfer and transformation activities and build data integration solutions across multiple systems. When you're working with analytical data in a data lake, Apache Spark provides a scalable, distributed processing platform that you can use to process huge volumes of data efficiently.\n\nThe Synapse Notebook activity enables you to run data processing code in Spark notebooks as a task in a pipeline; making it possible to automate big data processing and integrate it into extract, transform, and load (ETL) workloads.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark Notebooks in an Azure Synapse Pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Spark Notebooks in an Azure Synapse Pipeline\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nApache Spark provides data engineers with a scalable, distributed data processing platform, which can be integrated into an Azure Synapse Analytics pipeline.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nDescribe notebook and pipeline integration.\nUse a Synapse notebook activity in a pipeline.\nUse parameters with a notebook activity.\nAdd\nPrerequisites\n\nBefore starting this module, you should have experience of using Apache Spark in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first:\n\nIntroduction\nmin\nUnderstand Synapse Notebooks and Pipelines\nmin\nUse a Synapse notebook activity in a pipeline\nmin\nUse parameters in a notebook\nmin\nExercise - Use an Apache Spark notebook in a pipeline\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhat does a pipeline use to access external data source and processing resources?\n\n \n\nData Explorer pools\n\nLinked services\n\nExternal tables\n\n2. \n\nWhat kind of object should you add to a data flow to define a target to which data is loaded?\n\n \n\nSource\n\nTransformation\n\nSink\n\n3. \n\nWhat must you create to run a pipeline at scheduled intervals?\n\n \n\nA control flow\n\nA trigger\n\nAn activity\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Analytics provides data integration services through the creation of pipelines. By using pipelines, you can implement complex extract, transform, and load (ETL) solutions that support enterprise data analytics.\n\n Tip\n\nTo learn more about developing and debugging pipelines, see Iterative development and debugging with Azure Data Factory and Synapse Analytics pipelines\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Run a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/5-run-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nRun a pipeline\n5 minutes\n\nWhen you’re ready, you can publish a pipeline and use a trigger to run it. Triggers can be defined to run the pipeline:\n\nImmediately\nAt explicitly scheduled intervals\nIn response to an event, such as new data files being added to a folder in a data lake.\n\nYou can monitor each individual run of a pipeline in the Monitor page in Azure Synapse Studio.\n\nThe ability to monitor past and ongoing pipeline runs is useful for troubleshooting purposes. Additionally, when combined with the ability to integrate Azure Synapse Analytics and Microsoft Purview, you can use pipeline run history to track data lineage data flows.\n\n Tip\n\nTo learn more about integration between Azure Synapse Analytics and Microsoft Purview, consider completing the Integrate Microsoft Purview and Azure Synapse Analytics module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/6-exercise-build-data-pipeline-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Build a data pipeline in Azure Synapse Analytics\n45 minutes\n\nNow it's your chance to build an Azure Synapse Analytics pipeline. In this exercise, you'll implement a run an Azure Synapse Analytics pipeline that transfers and transforms data.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Define data flows - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/4-define-data-flows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDefine data flows\n5 minutes\n\nA Data Flow is a commonly used activity type to define data flow and transformation. Data flows consist of:\n\nSources - The input data to be transferred.\nTransformations – Various operations that you can apply to data as it streams through the data flow.\nSinks – Targets into which the data will be loaded.\n\nWhen you add a Data Flow activity to a pipeline, you can open it in a separate graphical design interface in which to create and configure the required data flow elements.\n\nAn important part of creating a data flow is to define mappings for the columns as the data flows through the various stages, ensuring column names and data types are defined appropriately. While developing a data flow, you can enable the Data flow debug option to pass a subset of data through the flow, which can be useful to test that your columns are mapped correctly.\n\n Tip\n\nTo learn more about implementing a Data Flow activity, see Data Flow activity in Azure Data Factory and Azure Synapse Analytics in the Azure documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a pipeline in Azure Synapse Studio - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/3-create-pipeline-azure-synapse-studio",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a pipeline in Azure Synapse Studio\n5 minutes\n\nYou can create a pipeline in Azure Synapse Studio by using shortcuts on the Home page, but the primary place where pipelines are created and managed is the Integrate page.\n\nWhen you create a pipeline in Azure Synapse Studio, you can use the graphical design interface.\n\n.\n\nThe pipeline designer includes a set of activities, organized into categories, which you can drag onto a visual design canvas. You can select each activity on the canvas and use the properties pane beneath the canvas to configure the settings for that activity.\n\nTo define the logical sequence of activities, you can connect them by using the Succeeded, Failed, and Completed dependency conditions, which are shown as small icons on the right-hand edge of each activity.\n\nDefining a pipeline with JSON\n\nWhile the graphical development environment is the preferred way to create a pipeline, you can also create or edit the underlying JSON definition of a pipeline. The following code example shows the JSON definition of a pipeline that includes a Copy Data activity:\n\n{\n  \"name\": \"CopyPipeline\",\n  \"properties\": {\n    \"description\": \"Copy data from a blob to Azure SQL table\",\n    \"activities\": [\n      {\n        \"name\": \"CopyFromBlobToSQL\",\n        \"type\": \"Copy\",\n        \"inputs\": [\n          {\n            \"name\": \"InputDataset\"\n          }\n        ],\n        \"outputs\": [\n          {\n            \"name\": \"OutputDataset\"\n          }\n        ],\n        \"typeProperties\": {\n          \"source\": {\n            \"type\": \"BlobSource\"\n          },\n          \"sink\": {\n            \"type\": \"SqlSink\",\n            \"writeBatchSize\": 10000,\n            \"writeBatchTimeout\": \"60:00:00\"\n          }\n        },\n        \"policy\": {\n          \"retry\": 2,\n          \"timeout\": \"01:00:00\"\n        }\n      }\n    ]\n  }\n}\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWith the wide range of data stores available in Azure, there's the need to manage and orchestrate the movement data between them. In fact, you'll usually want to automate extract, transform, and load (ETL) workloads as a regular process in a wider enterprise analytical solution. Pipelines are a mechanism for defining and orchestrating data movement activities. In this module, you'll be introduced to Azure Synapse Analytics pipelines, their component parts, and how to implement and run a pipeline in Azure Synapse Studio.\n\n Note\n\nAzure Synapse Analytics pipelines are built on the same technology as Azure Data Factory, and offer a similar authoring experience. The authoring processes described in this module are also applicable to Azure Data Factory. For a detailed discussion of the differences between Azure Synapse Analytics pipelines and Azure Data Factory, see Data integration in Azure Synapse Analytics versus Azure Data Factory.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand pipelines in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/2-understand-pipelines-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand pipelines in Azure Synapse Analytics\n6 minutes\n\nPipelines in Azure Synapse Analytics encapsulate a sequence of activities that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical design tools in Azure Synapse Studio enable you to build complex pipelines with minimal or no coding required.\n\nCore pipeline concepts\n\nBefore building pipelines in Azure Synapse Analytics, you should understand a few core concepts.\n\nActivities\n\nActivities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence.\n\nActivities can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows that apply transformations to the data as part of an extract, transfer, and load (ETL) operation. Additionally, there are activities that encapsulate processing tasks on specific systems, such as running a Spark notebook or calling an Azure function. Finally, there are control flow activities that you can use to implement loops, conditional branching, or manage variable and parameter values.\n\nIntegration runtime\n\nThe pipeline requires compute resources and an execution context in which to run. The pipeline's integration runtime provides this context, and is used to initiate and coordinate the activities in the pipeline.\n\nLinked services\n\nWhile many of the activities are run directly in the integration runtime for the pipeline, some activities depend on external services. For example, a pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database. To enable secure connections to the external services used by your pipelines, you must define linked services for them.\n\n Note\n\nLinked services are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.\n\nDatasets\n\nMost pipelines process data, and the specific data that is consumed and produced by activities in a pipeline is defined using datasets. A dataset defines the schema for each data object that will be used in the pipeline, and has an associated linked service to connect to its source. Activities can have datasets as inputs or outputs.\n\n Note\n\nSimilarly to linked services, datasets are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nBuild a data pipeline in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Data Factory\nAzure Synapse Analytics\n\nPipelines are the lifeblood of a data analytics solution. Learn how to use Azure Synapse Analytics pipelines to build integrated data solutions that extract, transform, and load data across diverse systems.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nDescribe core concepts for Azure Synapse Analytics pipelines.\nCreate a pipeline in Azure Synapse Studio.\nImplement a data flow activity in a pipeline.\nInitiate and monitor pipeline runs.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Synapse Analytics and data analytics solutions in general. Consider completing the Introduction to Azure Synapse Analytics module first.\n\nIntroduction\nmin\nUnderstand pipelines in Azure Synapse Analytics\nmin\nCreate a pipeline in Azure Synapse Studio\nmin\nDefine data flows\nmin\nRun a pipeline\nmin\nExercise - Build a data pipeline in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/9-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou want to configure a private endpoint. You open up Azure Synapse Studio, go to the manage hub, and see that the private endpoints are greyed out. Why is the option not available?\n\n \n\nAzure Synapse Studio doesn't support the creation of private endpoints.\n\nA Conditional Access policy has to be defined first.\n\nA managed virtual network hasn't been created.\n\n2. \n\nYou require an Azure Synapse Analytics Workspace to access an Azure Data Lake Store using the benefits of the security provided by Microsoft Entra ID. What is the best authentication method to use?\n\n \n\nStorage account keys.\n\nShared access signatures.\n\nManaged identities.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/10-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this module, you have learned how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nIn this module, you have:\n\nUnderstood network security options for Azure Synapse Analytics\nConfigured Conditional Access\nConfigured Authentication\nManaged authorization through column and row level security\nManaged sensitive data with Dynamic Data masking\nImplemented encryption in Azure Synapse Analytics\nUnderstood advanced data security options for Azure Synapse Analytics\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement encryption in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/8-implement-encryption",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nImplement encryption in Azure Synapse Analytics\n3 minutes\n\nIn this section, we will go through Transparent Data Encryption and TokenLibrary for Apache Spark.\n\nWhat is transparent data encryption\n\nTransparent data encryption (TDE) is an encryption mechanism to help you protect Azure Synapse Analytics. It will protect Azure Synapse Analytics against threats of malicious offline activity. The way TDE will do so is by encrypting data at rest. TDE performs real-time encryption as well as decryption of the database, associated backups, and transaction log files at rest without you having to make changes to the application. In order to use TDE for Azure Synapse Analytics, you will have to manually enable it.\n\nWhat TDE does is performing I/O encryption and decryption of data at the page level in real time. When a page is read into memory, it is decrypted. It is encrypted before writing it to disk. TDE encrypts the entire database storage using a symmetric key called a Database Encryption Key (DEK). When you start up a database, the encrypted Database Encryption Key is decrypted. The DEK will then be used for decryption and re-encryption of the database files in the SQL Server database engine. The DEK is protected by the Transparent Data Encryption Protector. This protector can be either a service-managed certificated, which is referred to as service-managed transparent data encryption, or an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption).\n\nWhat is important to understand is that for Azure Synapse Analytics, this TDE protector is set on the server level. There it is inherited by all the databases that are attached or aligned to that server. The term server refers both to server and instance.\n\nService-managed transparent data encryption\n\nAs stated above, the DEK that is protected by the Transparent Encryption protector can be service-managed certificates which we call service-managed TDE. When you look in Azure, that default setting means that the DEK is protected by a built-in certificate unique for each server with encryption algorithm AES256. When a database is in a geo-replicated relationship then primary and the geo-secondary database are protected by the primary database's parent server key. If the databases are connected to the same server, they will also have the same built-in AES 256 certificate. As Microsoft we automatically rotate the certificates in compliance with the internal security policy. The root key is protected by a Microsoft internal secret store. Microsoft also seamlessly moves and manages the keys as needed for geo-replication and restores.\n\nTransparent data encryption with bring your own key for customer-managed transparent data encryption\n\nAs stated above, the DEK that is protected by the Transparent Data Encryption Protector can also be customer managed by bringing an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption). This is also referred to as Bring Your Own Key (BYOK) support for TDE. When this is the scenario that is applicable to you, the TDE Protector that encrypts the DEK is a customer-managed asymmetric key. It is stored in your own and managed Azure Key Vault. Azure Key Vault is Azure's cloud-based external key management system. This managed key never leaves the key vault. The TDE Protector can be generated by the key vault. Another option is to transfer the TDE Protector to the key vault from, for example, an on-premise hardware security module (HSM) device. Azure Synapse Analytics needs to be granted permissions to the customer-owned key vault in order to decrypt and encrypt the DEK. If permissions of the server to the key vault are revoked, a database will be inaccessible, and all data is encrypted.\n\nBy using Azure Key Vault integration for TDE, you have control over the key management tasks such as key rotations, key backups, and key permissions. It also enables you to audit and report on all the TDE protectors when using the Azure Key Vault functionality. The reason for using Key Vault is that it provides you with a central key management system where tightly monitored HSMs are leveraged. It also enables you to separate duties of management of keys and data in order to meet compliance with security policies.\n\nManage transparent data encryption in the Azure portal.\n\nFor Azure Synapse Analytics, you can manage TDE for the database in the Azure portal after you've signed in with the Azure Administrator or Contributor account. The TDE settings can be found under your user database.\n\nIt is by default that the service-managed TDE is used and therefore a TDE certificate is automatically generated for the server that contains that database.\n\nMoving a transparent data encryption protected database\n\nIn some use cases you need to move a database that is protected with TDE. Within Azure, there is no need to decrypt the databases. The TDE settings on the source database or primary database, will be inherited on the target. Some of the operations within Azure that inherited the TDE are:\n\nGeo-restore\nSelf-service point-in-time restore\nRestoration of a deleted database\nActive geo-replication\nCreation of a database copy\nRestore of backup file to Azure SQL Managed Instance\n\nIf you export a TDE-protected database, the exported content is not encrypted. This will be stored in an unencrypted BACPAC file. You need to make sure that you protect this BACPAC file and enable TDE as soon as the import of the bacpac file in the new database is finished.\n\nSecuring your credentials through linked services with TokenLibrary for Apache Spark\n\nIt is quite a common pattern to access data from external sources. Unless the external data source allows anonymous access, it is highly likely that you need to secure your connection with a credential, secret, or connection string.\n\nWithin Azure Synapse Analytics, the integration process is simplified by providing linked services. Doing so, the connection details can be stored in the linked service or an Azure Key Vault. If the Linked Service is created, Apache spark can reference the linked service to apply the connection information in your code. When you want to access files from the Azure Data Lake Storage Gen 2 within your Azure Synapse Analytics Workspace, it uses AAD passthrough for the authentication. Therefore, there is no need to use TokenLibrary. However, to connect to other linked services, you are enabled to make a direct call to the TokenLibrary.\n\nAn example can be found below: In order to connect to other linked services, you are enabled to make a direct call to TokenLibrary by retrieving the connection string. In order to retrieve the connection string, use the getConnectionString function and pass in the linked service name.\n\n// Scala\n// retrieve connectionstring from TokenLibrary\n\nimport com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n\nval connectionString: String = TokenLibrary.getConnectionString(\"<LINKED SERVICE NAME>\")\nprintln(connectionString)\n\n# Python\n# retrieve connectionstring from TokenLibrary\n\nfrom pyspark.sql import SparkSession\n\nsc = SparkSession.builder.getOrCreate()\ntoken_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\nconnection_string = token_library.getConnectionString(\"<LINKED SERVICE NAME>\")\nprint(connection_string)\n\n\nIf you want to Get the connection string as map and parse specific values from a key in the connection string, you can find an example below:\n\nTo parse specific values from a key=value pair in the connection string such as\n\nDefaultEndpointsProtocol=https;AccountName=<AccountName>;AccountKey=<AccountKey>\n\nuse the getConnectionStringAsMap function and pass the key to return the value.\n\n// Linked services can be used for storing and retreiving credentials (e.g, account key)\n// Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\nimport com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n\nval accountKey: String = TokenLibrary.getConnectionStringAsMap(\"<LINKED SERVICE NAME\">).get(\"<KEY NAME>\")\nprintln(accountKey)\n\n# Linked services can be used for storing and retreiving credentials (e.g, account key)\n# Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\nfrom pyspark.sql import SparkSession\n\nsc = SparkSession.builder.getOrCreate()\ntoken_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\naccountKey = token_library.getConnectionStringAsMap(\"<LINKED SERVICE NAME>\").get(\"<KEY NAME>\")\nprint(accountKey)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage sensitive data with Dynamic Data Masking - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/7-manage-sensitive-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage sensitive data with Dynamic Data Masking\n8 minutes\n\nAzure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics support Dynamic Data Masking. Dynamic Data Masking ensures limited data exposure to nonprivileged users, such that they can't see the data that is being masked. It also helps you in preventing unauthorized access to sensitive information that has minimal impact on the application layer. Dynamic Data Masking is a policy-based security feature. It will hide the sensitive data in a result set of a query that runs over designated database fields.\n\nLet's give you an example how it works. Let's say you work at a bank as a service representative in a call center. Due to compliance, any caller must identify themselves by providing several digits of their credit card number. In this scenario, the full credit card number shouldn't be fully exposed to the service representative in the call center. You can define a masking rule that masks all but the last four digits of a credit card number so that you would get a query that only gives as a result the last four digits of the credit card number. This is just one example that could be equally applied to a variety of personal data such that compliance isn't violated. For Azure Synapse Analytics, the way to set up a Dynamic Data Masking policy is using PowerShell or the REST API. The configuration of the Dynamic Data Masking policy can be done by the Azure SQL Database admin, server admin, or SQL Security Manager roles.\n\nIn Azure Synapse Analytics, you can find Dynamic Data Masking here;\n\nLooking into Dynamic Data Masking Policies:\n\nSQL users excluded from Dynamic Data Masking Policies\n\nThe following SQL users or Microsoft Entra identities can get unmasked data in the SQL query results. Users with administrator privileges are always excluded from masking, and will see the original data without any mask.\n\nMasking rules - Masking rules are a set of rules that define the designated fields to be masked including the masking function that is used. The designated fields can be defined using a database schema name, table name, and column name.\n\nMasking functions - Masking functions are a set of methods that control the exposure of data for different scenarios.\n\nSet up Dynamic Data Masking for your database in Azure Synapse Analytics using PowerShell cmdlets\n\nIn this part, we're going to look into Dynamic Data Masking for a database in Azure Synapse Analytics using PowerShell cmdlets.\n\nData masking policies\nGet-AzSqlDatabaseDataMaskingPolicy\n\nThe Get-AzSqlDatabaseDataMaskingPolicy gets the data masking policy for a database.\n\nThe syntax for the Get-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:\n\nGet-AzSqlDatabaseDataMaskingPolicy [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the Get-AzSqlDatabaseDataMaskingPolicy cmdlet does, is getting the data masking policy of an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group you deployed the database in\nServerName: sql server name\nDatabaseName : name of the database\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet-AzSqlDatabaseDataMaskingPolicy\n\nThe Set-AzSqlDatabaseDataMaskingPolicy sets data masking for a database.\n\nThe syntax for the Set-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:\n\nSet-AzSqlDatabaseDataMaskingPolicy [-PassThru] [-PrivilegedUsers <String>] [-DataMaskingState <String>]\n [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Set-AzSqlDatabaseDataMaskingPolicy cmdlet does is setting the data masking policy for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nIn addition, you'll need to set the DataMaskingState parameter to specify whether data masking operations are enabled or disabled.\n\nIf the cmdlet succeeds and the PassThru parameter is used, it will return an object describing the current data masking policy in addition to the database identifiers.\n\nDatabase identifiers can include, ResourceGroupName, ServerName, and DatabaseName.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nData masking rules\nGet-AzSqlDatabaseDataMaskingRule\n\nThe Get-AzSqlDatabaseDataMaskingRule Gets the data masking rules from a database.\n\nThe syntax for the Get-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nGet-AzSqlDatabaseDataMaskingRule [-SchemaName <String>] [-TableName <String>] [-ColumnName <String>]\n [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Get-AzSqlDatabaseDataMaskingRule cmdlet does it getting either a specific data masking rule or all of the data masking rules for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nYou'd also have to specify the RuleId parameter to specify which rule this cmdlet returns.\n\nIf you don't provide RuleId, all the data masking rules for that Azure SQL database are returned.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nNew-AzSqlDatabaseDataMaskingRule\n\nThe New-AzSqlDatabaseDataMaskingRule creates a data masking rule for a database.\n\nThe syntax for the New-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nNew-AzSqlDatabaseDataMaskingRule -MaskingFunction <String> [-PrefixSize <UInt32>] [-ReplacementString <String>]\n [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru] -SchemaName <String>\n -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the New-AzSqlDatabaseDataMaskingRule cmdlet does is creating a data masking rule for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\n\nProviding the TableName and ColumnName is necessary in order to specify the target of the rule.\n\nThe MaskingFunction parameter is necessary to define how the data is masked.\n\nIf MaskingFunction has a value of Number or Text, you can specify the NumberFrom and NumberTo parameters, for number masking, or the PrefixSize, ReplacementString, and SuffixSize for text masking.\n\nIf the command succeeds and the PassThru parameter is used, the cmdlet returns an object describing the data masking rule properties in addition to the rule identifiers.\n\nRule identifiers can be, for example, ResourceGroupName, ServerName, DatabaseName, and RuleID.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nRemove-AzSqlDatabaseDataMaskingRule\n\nThe Remove-AzSqlDatabaseDataMaskingRule removes a data masking rule from a database.\n\nThe syntax for the Remove-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nRemove-AzSqlDatabaseDataMaskingRule [-PassThru] [-Force] -SchemaName <String> -TableName <String>\n -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>\n [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n\nWhat the Remove-AzSqlDatabaseDataMaskingRule cmdlet does, is it removes a specific data masking rule from an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule that needs to be removed:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\nRuleId : identifier of the rule\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet-AzSqlDatabaseDataMaskingRule\n\nThe Set-AzSqlDatabaseDataMaskingRule Sets the properties of a data masking rule for a database.\n\nThe syntax for the Set-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:\n\nSet-AzSqlDatabaseDataMaskingRule [-MaskingFunction <String>] [-PrefixSize <UInt32>]\n [-ReplacementString <String>] [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru]\n -SchemaName <String> -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>\n [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]\n [<CommonParameters>]\n\n\nWhat the Set-AzSqlDatabaseDataMaskingRule cmdlet does is setting a data masking rule for an Azure SQL database.\n\nTo use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:\n\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\nRuleId : identifier of the rule\n\nYou can provide any of the parameters of SchemaName, TableName, and ColumnName to retarget the rule.\n\nSpecify the MaskingFunction parameter to modify how the data is masked.\n\nIf you specify a value of Number or Text for MaskingFunction, you can specify the NumberFrom and NumberTo parameters for number masking or the PrefixSize, ReplacementString, and SuffixSize parameters for text masking.\n\nIf the command succeeds, and if you specify the PassThru parameter, the cmdlet returns an object that describes the data masking rule properties and the rule identifiers.\n\nRule identifiers can be, ResourceGroupName, ServerName, DatabaseName, and RuleId.\n\nThis cmdlet is also supported by the SQL Server Stretch Database service on Azure.\n\nSet up Dynamic Data Masking for your database in Azure Synapse Analytics using the REST API\n\nFor setting up Dynamic Data Masking in Azure Synapse Analytics, you can also make use of the REST API. It will enable you to programmatically manage data masking policy and rules.\n\nThe REST API will support the following operations:\n\nData masking policies\nCreate Or Update\n\nThe Create Or Update masking policy using the REST API will create or update a database data masking policy.\n\nIn HTTP the following request can be made: > Note: The date of the API will change over time and the version you use will be determined by your needs and the funtionality requred.\n\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\nGet\n\nThe Get policy, gets a database data masking policy.\n\nIn HTTP the following request can be made:\n\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\n\nData masking rules\n\nCreate Or Update\n\nThe Create or Update masking rule creates or updates a database data masking rule.\n\nIn HTTP the following request can be made:\n\nPUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules/{dataMaskingRuleName}?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\ndataMaskingRuleName: the name of the rule for data masking\napi version: version of the api that is used.\nList By Database\n\nThe List By Database request gets a list of database data masking rules.\n\nIn HTTP the following request can be made:\n\nGET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules?api-version=2021-06-01\n\n\nThe following parameters need to be passed through:\n\nSubscriptionID: the ID of the subscription\nResourceGroupName: name of the resource group that you deployed the database in\nServerName : sql server name\nDatabaseName : name of the database\ndataMaskingPolicyName: the name of the data masking policy\napi version: version of the api that is used.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Manage authorization through column and row level security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/6-exercise-manage-authorization-through-column-row-level-security",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Manage authorization through column and row level security\n9 minutes\n\nIn this exercise, examples are shown how you can manage authorization through column and row level security.\n\nAn example of column level security\n\nThe following example shows how to restrict TestUser from accessing the SSN column of the Membership table:\n\nCreate Membership table with SSN column used to store social security numbers:\n\nCREATE TABLE Membership\n  (MemberID int IDENTITY,\n   FirstName varchar(100) NULL,\n   SSN char(9) NOT NULL,\n   LastName varchar(100) NOT NULL,\n   Phone varchar(12) NULL,\n   Email varchar(100) NULL);\n\n\nAllow TestUser to access all columns except for the SSN column, which has the sensitive data:\n\nGRANT SELECT ON Membership(MemberID, FirstName, LastName, Phone, Email) TO TestUser;\n\n\nQueries executed as TestUser will fail if they include the SSN column:\n\nSELECT * FROM Membership;\n\n-- Msg 230, Level 14, State 1, Line 12\n-- The SELECT permission was denied on the column 'SSN' of the object 'Membership', database 'CLS_TestDW', schema 'dbo'.\n\nAn example of row level security\n\nThis scenario gives you an example for row level security on an Azure Synapse external table.\n\nThis short example creates three users and an external table with six rows. It then creates an inline table-valued function and a security policy for the external table. The example shows how select statements are filtered for the various users.\n\nPrerequisites\nYou must have a SQL pool. See Create a Synapse SQL pool\nThe server hosting your SQL pool must be registered with AAD and you must have an Azure storage account with Storage Blog Contributor permissions. Follow the steps here.\nCreate a file system for your Azure Storage account. Use Storage Explorer to view your storage account. Right click on containers and select Create file system.\n\nOnce you have the prerequisites in place, create three user accounts that will demonstrate different access capabilities.\n\n--run in master\nCREATE LOGIN Manager WITH PASSWORD = '<user_password>'\nGO\nCREATE LOGIN Sales1 WITH PASSWORD = '<user_password>'\nGO\nCREATE LOGIN Sales2 WITH PASSWORD = '<user_password>'\nGO\n\n--run in master and your SQL pool database\nCREATE USER Manager FOR LOGIN Manager;  \nCREATE USER Sales1  FOR LOGIN Sales1;  \nCREATE USER Sales2  FOR LOGIN Sales2 ;\n\n\nCreate a table to hold data.\n\nCREATE TABLE Sales  \n    (  \n    OrderID int,  \n    SalesRep sysname,  \n    Product varchar(10),  \n    Qty int  \n    );  \n\n\nPopulate the table with six rows of data, showing three orders for each sales representative.\n\nINSERT INTO Sales VALUES (1, 'Sales1', 'Valve', 5);\nINSERT INTO Sales VALUES (2, 'Sales1', 'Wheel', 2);\nINSERT INTO Sales VALUES (3, 'Sales1', 'Valve', 4);\nINSERT INTO Sales VALUES (4, 'Sales2', 'Bracket', 2);\nINSERT INTO Sales VALUES (5, 'Sales2', 'Wheel', 5);\nINSERT INTO Sales VALUES (6, 'Sales2', 'Seat', 5);\n-- View the 6 rows in the table  \nSELECT * FROM Sales;\n\n\nCreate an Azure Synapse external table from the Sales table you just created.\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = '<user_password>';\n\nCREATE DATABASE SCOPED CREDENTIAL msi_cred WITH IDENTITY = 'Managed Service Identity';\n\nCREATE EXTERNAL DATA SOURCE ext_datasource_with_abfss WITH (TYPE = hadoop, LOCATION = 'abfss://<file_system_name@storage_account>.dfs.core.windows.net', CREDENTIAL = msi_cred);\n\nCREATE EXTERNAL FILE FORMAT MSIFormat  WITH (FORMAT_TYPE=DELIMITEDTEXT);\n  \nCREATE EXTERNAL TABLE Sales_ext WITH (LOCATION='<your_table_name>', DATA_SOURCE=ext_datasource_with_abfss, FILE_FORMAT=MSIFormat, REJECT_TYPE=Percentage, REJECT_SAMPLE_VALUE=100, REJECT_VALUE=100)\nAS SELECT * FROM sales;\n\n\nGrant SELECT for the three users on the external table Sales_ext that you created.\n\nGRANT SELECT ON Sales_ext TO Sales1;  \nGRANT SELECT ON Sales_ext TO Sales2;  \nGRANT SELECT ON Sales_ext TO Manager;\n\n\nCreate a new schema, and an inline table-valued function, you may have completed this in example A. The function returns 1 when a row in the SalesRep column is the same as the user executing the query (@SalesRep = USER_NAME()) or if the user executing the query is the Manager user (USER_NAME() = 'Manager').\n\nCREATE SCHEMA Security;  \nGO  \n  \nCREATE FUNCTION Security.fn_securitypredicate(@SalesRep AS sysname)  \n    RETURNS TABLE  \nWITH SCHEMABINDING  \nAS  \n    RETURN SELECT 1 AS fn_securitypredicate_result\nWHERE @SalesRep = USER_NAME() OR USER_NAME() = 'Manager';  \n\n\nCreate a security policy on your external table using the inline table-valued function as a filter predicate. The state must be set to ON to enable the policy.\n\nCREATE SECURITY POLICY SalesFilter_ext\nADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)\nON dbo.Sales_ext  \nWITH (STATE = ON);\n\n\nNow test the filtering predicate, by selecting from the Sales_ext external table. Sign in as each user, Sales1, Sales2, and manager. Run the following command as each user.\n\nSELECT * FROM Sales_ext;\n\n\nThe Manager should see all six rows. The Sales1 and Sales2 users should only see their sales.\n\nAlter the security policy to disable the policy.\n\nALTER SECURITY POLICY SalesFilter_ext  \nWITH (STATE = OFF);  \n\n\nNow the Sales1 and Sales2 users can see all six rows.\n\nConnect to the Azure Synapse database to clean up resources\n\nDROP USER Sales1;\nDROP USER Sales2;\nDROP USER Manager;\n\nDROP SECURITY POLICY SalesFilter_ext;\nDROP TABLE Sales;\nDROP EXTERNAL TABLE Sales_ext;\nDROP EXTERNAL DATA SOURCE ext_datasource_with_abfss ;\nDROP EXTERNAL FILE FORMAT MSIFormat;\nDROP DATABASE SCOPED CREDENTIAL msi_cred; \nDROP MASTER KEY;\n\n\nConnect to logical master to clean up resources.\n\nDROP LOGIN Sales1;\nDROP LOGIN Sales2;\nDROP LOGIN Manager;\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage authorization through column and row level security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/5-manage-authorization-through-column-row-level-security",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 10\nNext\nManage authorization through column and row level security\nCompleted\n100 XP\n6 minutes\n\nIn this topic, we'll go through how you can manage authorization through column and row level security within Azure Synapse Analytics. We'll start off by talking about column level security in Azure Synapse Analytics, and finish with row level security.\n\nColumn level security in Azure Synapse Analytics\n\nGenerally speaking, column level security is simplifying a design and coding for the security in your application. It allows you to restrict column access in order to protect sensitive data. For example, if you want to ensure that a specific user 'Leo' can only access certain columns of a table because he's in a specific department. The logic for 'Leo' only to access the columns specified for the department he works in, is a logic that is located in the database tier, rather than on the application level data tier. If he needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system. Column level security will also eliminate the necessity for the introduction of view, where you would filter out columns, to impose access restrictions on 'Leo'\n\nThe way to implement column level security is by using the GRANT T-SQL statement. Using this statement, SQL and Microsoft Entra ID support the authentication.\n\nSyntax\n\nThe syntax to use for implementing column level security looks as follows:\n\nsyntaxsql\nCopy\nGRANT <permission> [ ,...n ] ON\n    [ OBJECT :: ][ schema_name ]. object_name [ ( column [ ,...n ] ) ] // specifying the column access\n    TO <database_principal> [ ,...n ]\n    [ WITH GRANT OPTION ]\n    [ AS <database_principal> ]\n<permission> ::=\n    SELECT\n  | UPDATE\n<database_principal> ::=\n      Database_user // specifying the database user\n    | Database_role // specifying the database role \n    | Database_user_mapped_to_Windows_User\n    | Database_user_mapped_to_Windows_Group\n\n\nSo when would you use column-level security? Let's say that you are a financial services firm, and can only have an account manager allowed to have access to a customer's social security number, phone number, or other personally identifiable information. It is imperative to distinguish the role of an account manager versus the manager of the account managers.\n\nAnother use case might be related to the Healthcare Industry. Let's say you have a specific health care provider. This healthcare provider only wants doctors and nurses to be able to access medical records. The billing department should not have access to view this data. Column-level security might be the option to use.\n\nSo how does column level security distinguishes from row-level security? Let's look into that.\n\nRow level security in Azure Synapse Analytics\n\nRow-level security (RLS) can help you to create a group membership or execution context in order to control not just columns in a database table, but actually, the rows. RLS, just like column-level security, can simply help and enable your design and coding of your application security. However, compared to column-level security where it's focused on the columns (parameters), RLS helps you implement restrictions on data row access. Let's say that your employee can only access rows of data that are important to the department, you should implement RLS. If you want to restrict, for example, customer data access that is only relevant to the company, you can implement RLS. The restriction on the access of the rows is a logic that is located in the database tier, rather than on the application level data tier. If 'Leo' needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system.\n\nThe way to implement RLS is by using the CREATE SECURITY POLICY[!INCLUDEtsql] statement. The predicates are created as inline table-valued functions. It is imperative to understand that within Azure Synapse, only supports filter predicates. If you need to use a block predicate, you won't be able to find support at this moment within Azure synapse.\n\nDescription of row level security in relation to filter predicates\n\nRLS within Azure Synapse supports one type of security predicates, which are Filter predicates, not block predicates.\nWhat filter predicates do, is silently filtering the rows that are available for reading operations such as SELECT, UPDATE, DELETE.\n\nThe access to row-level data in a table is restricted as an inline table-valued function, which is a security predicate. This table-valued function will then be invoked and enforced by the security policy that you need. An application is not aware of rows that are filtered from the result set for filter predicates. So what will happen is that if all rows are filtered, a null set is returned.\n\nWhen you are using filter predicates, it will be applied when data is read from the base table. The filter predicate affects all get operations such as SELECT, DELETE, UPDATE. You are unable to select or delete rows that have been filtered. It is not possible for you to update a row that has been filtered. What you can do, is update rows in a way that they will be filtered afterward.\n\nUse cases\n\nWe've already mentioned some use cases for RLS. Another use case might where you have created a multi-tenant application where you create a policy where logical separations of a tenant's data rows from another tenant's data rows are enforced. In order to implement this efficiently, it is highly recommended to store data for many tenants in a single table.\n\nWhen we look at RLS filter predicates, they are functionally equivalent to appending a WHERE clause. The predicate can be as sophisticated as business practices dictate, or the clause can be as simple as WHERE TenantId = 42.\n\nWhen we look at RLS more formally, RLS introduces predicate based access control. The reason why RLS can be used for predicate access control is that it is a flexible, centralized, predicate-based evaluation. The filter predicate can be based on metadata or any other criteria you would determine as appropriate. The predicate is used as a criterion to determine if the user has the appropriate access to the data based on user attributes. Label-based access control can be implemented by using predicate-based access control.\n\nPermissions\n\nIf you want to create, alter or drop the security policies, you would have to use the ALTER ANY SECURITY POLICY permission. The reason for that is when you are creating or dropping a security policy it requires ALTER permissions on the schema.\n\nIn addition to that, there are other permissions required for each predicate that you would add:\n\nSELECT and REFERENCES permissions on the inline table-valued function being used as a predicate.\n\nREFERENCES permission on the table that you target to be bound to the policy.\n\nREFERENCES permission on every column from the target table used as arguments.\n\nOnce you've set up the security policies, they will apply to all the users (including dbo users in the database) Even though DBO users can alter or drop security policies, their changes to the security policies can be audited. If you have special circumstances where highly privileged users, like a sysadmin or db_owner, need to see all rows to troubleshoot or validate data, you would still have to write the security policy in order to allow that.\n\nIf you have created a security policy where SCHEMABINDING = OFF, in order to query the target table, the user must have the SELECT or EXECUTE permission on the predicate function. They also need permissions to any additional tables, views, or functions used within the predicate function. If a security policy is created with SCHEMABINDING = ON (the default), then these permission checks are bypassed when users query the target table.\n\nBest practices\n\nThere are some best practices to take in mind when you want to implement RLS. We recommended creating a separate schema for the RLS objects. RLS objects in this context would be the predicate functions, and security policies. Why is that a best practice? It helps to separate the permissions that are required on these special objects from the target tables. In addition to that, separation for different policies and predicate functions may be needed in multi-tenant-databases. However, it is not a standard for every case.\n\nAnother best practice to bear in mind is that the ALTER ANY SECURITY POLICY permission should only be intended for highly privileged users (such as a security policy manager). The security policy manager should not require SELECT permission on the tables they protect.\n\nIn order to avoid potential runtime errors, you should take into mind type conversions in predicate functions that you write. Also, you should try to avoid recursion in predicate functions. The reason for this is to avoid performance degradation. Even though the query optimizer will try to detect the direct recursions, there is no guarantee to find the indirect recursions. With indirect recursion, we mean where a second function calls the predicate function.\n\nIt would also be recommended to avoid the use of excessive table joins in predicate functions. This would maximize performance.\n\nGenerally speaking when it comes to the logic of predicates, you should try to avoid logic that depends on session-specific SET options. Even though this is highly unlikely to be used in practical applications, predicate functions whose logic depends on certain session-specific SET options can leak information if users are able to execute arbitrary queries. For example, a predicate function that implicitly converts a string to datetime could filter different rows based on the SET DATEFORMAT option for the current session.\n\nNext unit: Exercise - Manage authorization through column and row level security\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Conditional Access - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/3-configure-conditional-access",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 3 of 10\nNext\nConfigure Conditional Access\nCompleted\n100 XP\n7 minutes\n\nConditional Access is a feature that enables you to define the conditions under which a user can connect to your Azure subscription and access services. Conditional Access provides an additional layer of security that can be used in combination with authentication to strengthen the security access to your network.\n\nConditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. As an example, if a Data Engineer wishes to access services in Azure Synapse Analytics, they may be requested by the Conditional Access policy to perform an additional step of multifactor authentication (MFA) to complete the authentication to get onto the service\n\nConditional Access policies use signals as a basis to determine if Conditional Access should first be applied. Common signals include:\n\nUser or group membership names\nIP address information\nDevice platforms or type\nApplication access requests\nReal-time and calculated risk detection\nMicrosoft Cloud App Security (MCAS)\n\nBased on these signals, you can then choose to block access. The alternative is you can grant access, and at the same time request that the user perform an additional action including:\n\nPerform multifactor authentication\nUse a specific device to connect\n\nGiven the amount of data that could potentially be stored, Azure Synapse Analytics dedicated SQL pools supports Conditional Access to provide protection for your data. It does require that Azure Synapse Analytics is configured to support Microsoft Entra ID, and that if you chose multifactor authentication, that the tool you are using support it.\n\nTo configure Conditional Access, you can perform the following steps:\n\nSign in to the Azure portal, select Microsoft Entra ID, and then select Conditional Access.\n\nIn the Conditional Access-Policies blade, click New policy, provide a name, and then click Configure rules.\n\nUnder Assignments, select Users and groups, check Select users and groups, and then select the user or group for Conditional Access. Click Select, and then click Done to accept your selection.\n\nSelect Cloud apps, click Select apps. You see all apps available for Conditional Access. Select Azure SQL Database, at the bottom click Select, and then click Done.\n\nIf you can't find Azure SQL Database listed in the following third screenshot, complete the following steps:\n\nConnect to your database in Azure SQL Database by using SSMS with a Microsoft Entra admin account.\nExecute CREATE USER [user@yourtenant.com] FROM EXTERNAL PROVIDER.\nSign into Microsoft Entra ID and verify that Azure SQL Database, SQL Managed Instance, or Azure Synapse are listed in the applications in your Microsoft Entra instance.\n\nSelect Access controls, select Grant, and then check the policy you want to apply. For this example, we select Require multifactor authentication.\n\nNext unit: Configure authentication\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure authentication - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/4-configure-authentication",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 4 of 10\nNext\nConfigure authentication\nCompleted\n100 XP\n8 minutes\n\nAuthentication is the process of validating credentials as you access resources in a digital infrastructure. This ensures that you can validate that an individual, or a service that wants to access a service in your environment can prove who they are. Azure Synapse Analytics provides several different methods for authentication.\n\nWhat needs to be authenticated\n\nThere are a variety of scenarios that means that authentication must take place to protect the data that is stored in your Azure Synapse Analytics estate.\n\nThe common form of authentication is that of individuals who want to access the data in the service. This is typically seen as an individual providing a username and password to authenticate against a service. However, this is also becoming more sophisticated with authentication requests working in combination with Conditional Access policies to further secure the authentication process with additional security steps.\n\nWhat is less obvious is the fact that services must authenticate with other services so that they can operate seamlessly. An example of this is using an Azure Synapse Spark or serverless SQL pool to access data in an Azure Data Lake store. An authentication mechanism must take place in the background to ensure that Azure Synapse Analytics can access the data in the data lake in an authenticated manner.\n\nFinally, there are situations where users and services operate together at the same time. Here you have a combination of both user and service authentication taking place under the hood to ensure that the user is getting access to the data seamlessly. An example of this is using Power BI to view reports in a dashboard that is being serviced by a dedicated SQL pool. Here you have multiple levels of authentication taking place that needs to be managed.\n\nTypes of security\n\nThe following are the types of authentication that you should be aware of when working with Azure Synapse Analytics.\n\nMicrosoft Entra ID\n\nMicrosoft Entra ID is a directory service that allows you to centrally maintain objects that can be secured. The objects can include user accounts and computer accounts. An employee of an organization will typically have a user account that represents them in the organizations Microsoft Entra tenant, and they then use the user account with a password to authenticate against other resources that are stored within the directory using a process known as single sign-on.\n\nThe power of Microsoft Entra ID is that they only have to log in once, and Microsoft Entra ID will manage access to other resources based on the information held within it using pass through authentication. If a user and an instance of Azure Synapse Analytics are part of the same Microsoft Entra ID, it is possible for the user to access Azure Synapse Analytics without an apparent login. If managed correctly, this process is seamless as the administrator would have given the user authorization to access Azure Synapse Analytics dedicated SQL pool as an example.\n\nIn this situation, it is normal for an Azure Administrator to create the user accounts and assign them to the appropriate roles and groups in Microsoft Entra ID. The Data Engineer will then add the user, or a group to which the user belongs to access a dedicated SQL pool.\n\nManaged identities\n\nManaged identity for Azure resources is a feature of Microsoft Entra ID. The feature provides Azure services with an automatically managed identity in Microsoft Entra ID. You can use the Managed Identity capability to authenticate to any service that supports Microsoft Entra authentication.\n\nManaged identities for Azure resources are the new name for the service formerly known as Managed Service Identity (MSI). A system-assigned managed identity is created for your Azure Synapse workspace when you create the workspace.\n\nAzure Synapse also uses the managed identity to integrate pipelines. The managed identity lifecycle is directly tied to the Azure Synapse workspace. If you delete the Azure Synapse workspace, then the managed identity is also cleaned up.\n\nThe workspace managed identity needs permissions to perform operations in the pipelines. You can use the object ID or your Azure Synapse workspace name to find the managed identity when granting permissions.\n\nYou can retrieve the managed identity in the Azure portal. Open your Azure Synapse workspace in Azure portal and select Overview from the left navigation. The managed identity's object ID is displayed to in the main screen.\n\nThe managed identity information will also show up when you create a linked service that supports managed identity authentication from Azure Synapse Studio.\n\nLaunch Azure Synapse Studio and select the Manage tab from the left navigation. Then select Linked services and choose the + New option to create a new linked service.\n\nIn the New linked service window, type Azure Data Lake Storage Gen2. Select the Azure Data Lake Storage Gen2 resource type from the list below and choose Continue.\n\nIn the next window, choose Managed Identity for Authentication method. You'll see the managed identity's Name and Object ID.\n\nSQL Authentication\n\nFor user accounts that are not part of a Microsoft Entra ID, then using SQL Authentication will be an alternative. In this instance, a user is created in the instance of a dedicated SQL pool. If the user in question requires administrator access, then the details of the user are held in the master database. If administrator access is not required, you can create a user in a specific database. A user then connects directly to the Azure Synapse Analytics dedicated SQL pool where they are prompted to use a username and password to access the service.\n\nThis approach is typically useful for external users who need to access the data, or if you are using third party or legacy applications against the Azure Synapse Analytics dedicated SQL pool\n\nMultifactor authentication\n\nSynapse SQL support connections from SQL Server Management Studio (SSMS) using Active Directory Universal Authentication.\n\nThis enables you to operate in environments that use Conditional Access policies that enforce multifactor authentication as part of the policy.\n\nKeys\n\nIf you are unable to use a managed identity to access resources such as Azure Data Lake then you can use storage account keys and shared access signatures.\n\nWith a storage account key. Azure creates two of these keys (primary and secondary) for each storage account you create. The keys give access to everything in the account. You'll find the storage account keys in the Azure portal view of the storage account. Just select Settings, and then click Access keys.\n\nAs a best practice, you shouldn't share storage account keys, and you can use Azure Key Vault to manage and secure the keys.\n\nAzure Key Vault is a secret store: a centralized cloud service for storing app secrets - configuration values like passwords and connection strings that must remain secure at all times. Key Vault helps you control your apps' secrets by keeping them in a single central location and providing secure access, permissions control, and access logging.\n\nThe main benefits of using Key Vault are:\n\nSeparation of sensitive app information from other configuration and code, reducing risk of accidental leaks\nRestricted secret access with access policies tailored to the apps and individuals that need them\nCentralized secret storage, allowing required changes to happen in only one place\nAccess logging and monitoring to help you understand how and when secrets are accessed\n\nSecrets are stored in individual vaults, which are Azure resources used to group secrets together. Secret access and vault management is accomplished via a REST API, which is also supported by all of the Azure management tools as well as client libraries available for many popular languages. Every vault has a unique URL where its API is hosted.\n\nShared access signatures\n\nIf an external third-party application needs access to your data, you'll need to secure their connections without using storage account keys. For untrusted clients, use a shared access signature (SAS). A shared access signature is a string that contains a security token that can be attached to a URI. Use a shared access signature to delegate access to storage objects and specify constraints, such as the permissions and the time range of access. You can give a customer a shared access signature token.\n\nTypes of shared access signatures\n\nYou can use a service-level shared access signature to allow access to specific resources in a storage account. You'd use this type of shared access signature, for example, to allow an app to retrieve a list of files in a file system or to download a file.\n\nUse an account-level shared access signature to allow access to anything that a service-level shared access signature can allow, plus additional resources and abilities. For example, you can use an account-level shared access signature to allow the ability to create file systems.\n\nNext unit: Manage authorization through column and row level security\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand network security options for Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand network security options for Azure Synapse Analytics\n5 minutes\n\nThere are a range of network security steps that you should consider to secure Azure Synapse Analytics. One of the first aspects that you will consider is securing access to the service itself. This can be achieved by creating the following network objects including:\n\nFirewall rules\nVirtual networks\nPrivate endpoints\nFirewall rules\n\nFirewall rules enable you to define the type of traffic that is allowed or denied access to an Azure Synapse workspace using the originating IP address of the client that is trying to access the Azure Synapse Workspace. IP firewall rules configured at the workspace level apply to all public endpoints of the workspace including dedicated SQL pools, serverless SQL pool, and the development endpoint.\n\nYou can choose to allow connections from all IP addresses as you are creating the Azure Synapse Workspaces, although this is not recommended as it does not allow for control access to the workspace. Instead, within the Azure portal, you can configure specific IP address ranges and associate them with a rule name so that you have greater control.\n\nMake sure that the firewall on your network and local computer allows outgoing communication on TCP ports 80, 443 and 1443 for Synapse Studio.\n\nAlso, you need to allow outgoing communication on UDP port 53 for Synapse Studio. To connect using tools such as SSMS and Power BI, you must allow outgoing communication on TCP port 1433.\n\nVirtual networks\n\nAzure Virtual Network (VNet) enables private networks in Azure. VNet enables many types of Azure resources, such as Azure Synapse Analytics, to securely communicate with other virtual networks, the internet, and on-premises networks. When you create your Azure Synapse workspace, you can choose to associate it to a Microsoft Azure Virtual Network. The Virtual Network associated with your workspace is managed by Azure Synapse. This Virtual Network is called a Managed workspace Virtual Network.\n\nUsing a managed workspace virtual network provides the following benefits:\n\nWith a Managed workspace Virtual Network, you can offload the burden of managing the Virtual Network to Azure Synapse.\nYou don't have to configure inbound NSG rules on your own Virtual Networks to allow Azure Synapse management traffic to enter your Virtual Network. Misconfiguration of these NSG rules causes service disruption for customers.\nYou don't need to create a subnet for your Spark clusters based on peak load.\nManaged workspace Virtual Network along with Managed private endpoints protects against data exfiltration. You can only create Managed private endpoints in a workspace that has a Managed workspace Virtual Network associated with it.\nit ensures that your workspace is network isolated from other workspaces.\n\nIf your workspace has a Managed workspace Virtual Network, Data integration and Spark resources are deployed in it. A Managed workspace Virtual Network also provides user-level isolation for Spark activities because each Spark cluster is in its own subnet.\n\nDedicated SQL pool and serverless SQL pool are multi-tenant capabilities and therefore reside outside of the Managed workspace Virtual Network. Intra-workspace communication to dedicated SQL pool and serverless SQL pool use Azure private links. These private links are automatically created for you when you create a workspace with a Managed workspace Virtual Network associated to it.\n\nYou can only choose to enable managed virtual networks as you are creating the Azure Synapse Workspaces.\n\nPrivate endpoints\n\nAzure Synapse Analytics enables you to connect up its various components through endpoints. You can set up managed private endpoints to access these components in a secure manner known as private links. This can only be achieved in an Azure Synapse workspace with a Managed workspace Virtual Network. Private link enables you to access Azure services (such as Azure Storage and Azure Cosmos DB) and Azure hosted customer/partner services from your Azure Virtual Network securely.\n\nWhen you use a private link, traffic between your Virtual Network and workspace traverses entirely over the Microsoft backbone network. Private Link protects against data exfiltration risks. You establish a private link to a resource by creating a private endpoint.\n\nPrivate endpoint uses a private IP address from your Virtual Network to effectively bring the service into your Virtual Network. Private endpoints are mapped to a specific resource in Azure and not the entire service. Customers can limit connectivity to a specific resource approved by their organization. You can manage the private endpoints in the Azure Synapse Studio manage hub.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this module, you will learn how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nIn this module, you will:\n\nUnderstand network security options for Azure Synapse Analytics\nConfigure Conditional Access\nConfigure Authentication\nManage authorization through column and row level security\nManage sensitive data with Dynamic Data masking\nImplement encryption in Azure Synapse Analytics\nUnderstand advanced data security options for Azure Synapse Analytics\nPrerequisites\n\nBefore taking this module, it is recommended that the student is able to:\n\nLog into the Azure portal\nCreate a Synapse Analytics Workspace\nCreate an Azure Synapse Analytics SQL Pool\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Secure a data warehouse in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nSecure a data warehouse in Azure Synapse Analytics\nModule\n10 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you will:\n\nUnderstand network security options for Azure Synapse Analytics\nConfigure Conditional Access\nConfigure Authentication\nManage authorization through column and row level security\nManage sensitive data with Dynamic Data masking\nImplement encryption in Azure Synapse Analytics\nAdd\nPrerequisites\nBefore taking this module, it is recommended that you complete Data Fundamentals.\nIntroduction\nmin\nUnderstand network security options for Azure Synapse Analytics\nmin\nConfigure Conditional Access\nmin\nConfigure authentication\nmin\nManage authorization through column and row level security\nmin\nExercise - Manage authorization through column and row level security\nmin\nManage sensitive data with Dynamic Data Masking\nmin\nImplement encryption in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this module, you have learned some of the features you can use to manage and monitor Azure Synapse Analytics, including:\n\nHow to Scale compute resources in Azure Synapse Analytics\nPausing the compute in Azure Synapse Analytics\nHow to manage workloads in Azure Synapse Analytics\nUsing the Azure Advisor to review recommendations\nUsing Dynamic Management Views to identify and troubleshoot query performance\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich ALTER DATABASE statement parameter allows a dedicated SQL pool to scale?\n\n \n\nSCALE.\n\nMODIFY\n\nCHANGE.\n\n2. \n\nWhich workload management feature influences the order in which a request gets access to resources?\n\n \n\nWorkload classification.\n\nWorkload importance.\n\nWorkload isolation.\n\n3. \n\nWhich Dynamic Management View enables the view of the active connections against a dedicated SQL pool?\n\n \n\nsys.dm_pdw_exec_requests.\n\nsys.dm_pdw_dms_workers.\n\nDBCC PDW_SHOWEXECUTIONPLAN.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use dynamic management views to identify and troubleshoot query performance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/6-use-dynamic-management-views-to-identify-troubleshoot-query-performance",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse dynamic management views to identify and troubleshoot query performance\n8 minutes\n\nDynamic Management Views provide a programmatic experience for monitoring the Azure Synapse Analytics SQL pool activity by using the Transact-SQL language. The views that are provided, not only enable you to troubleshoot and identify performance bottlenecks with the workloads working on your system, but they are also used by other services such as Azure Advisor to provide recommendations about Azure Synapse Analytics.\n\nThere are over 90 Dynamic Management Views that can queried against dedicated SQL pools to retrieve information about the following areas of the service:\n\nConnection information and activity\nSQL execution requests and queries\nIndex and statistics information\nResource blocking and locking activity\nData movement service activity\nErrors\n\nThe following is an example of monitoring query execution of the Azure Synapse Analytics SQL pools. The first step involves checking the connections against the server first, before checking the query execution activity. \n\nMonitoring connections\n\nAll logins to your data warehouse are logged to sys.dm_pdw_exec_sessions. The session_id is the primary key and is assigned sequentially for each new logon.\n\n-- Other Active Connections\nSELECT * FROM sys.dm_pdw_exec_sessions where status <> 'Closed' and session_id <> session_id();\n\nMonitor query execution\n\nAll queries executed on SQL pool are logged to sys.dm_pdw_exec_requests. The request_id uniquely identifies each query and is the primary key for this DMV. The request_id is assigned sequentially for each new query and is prefixed with QID, which stands for query ID. Querying this DMV for a given session_id shows all queries for a given logon.\n\nStep 1\n\nThe first step is to identify the query you want to investigate\n\n-- Monitor active queries\nSELECT *\nFROM sys.dm_pdw_exec_requests\nWHERE status not in ('Completed','Failed','Cancelled')\n  AND session_id <> session_id()\nORDER BY submit_time DESC;\n\n-- Find top 10 queries longest running queries\nSELECT TOP 10 *\nFROM sys.dm_pdw_exec_requests\nORDER BY total_elapsed_time DESC;\n\n\nFrom the preceding query results, note the Request ID of the query that you would like to investigate.\n\nQueries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceType. For information on concurrency limits, see Memory and concurrency limits or Resource classes for workload management. Queries can also wait for other reasons such as for object locks. If your query is waiting for a resource, see Investigating queries waiting for resources further down in this article.\n\nTo simplify the lookup of a query in the sys.dm_pdw_exec_requests table, use LABEL to assign a comment to your query, which can be looked up in the sys.dm_pdw_exec_requests view.\n\n-- Query with Label\nSELECT *\nFROM sys.tables\nOPTION (LABEL = 'My Query')\n;\n\n-- Find a query with the Label 'My Query'\n-- Use brackets when querying the label column, as it it a key word\nSELECT  *\nFROM    sys.dm_pdw_exec_requests\nWHERE   [label] = 'My Query';\n\nStep 2\n\nUse the Request ID to retrieve the queries distributed SQL (DSQL) plan from sys.dm_pdw_request_steps\n\n-- Find the distributed query plan steps for a specific query.\n-- Replace request_id with value from Step 1.\n\nSELECT * FROM sys.dm_pdw_request_steps\nWHERE request_id = 'QID####'\nORDER BY step_index;\n\n\nWhen a DSQL plan is taking longer than expected, the cause can be a complex plan with many DSQL steps or just one step taking a long time. If the plan is many steps with several move operations, consider optimizing your table distributions to reduce data movement.\n\nThe Table distribution article explains why data must be moved to solve a query. The article also explains some distribution strategies to minimize data movement.\n\nTo investigate further details about a single step, the operation_type column of the long-running query step and note the Step Index:\n\nProceed with Step 3 for SQL operations: OnOperation, RemoteOperation, ReturnOperation.\nProceed with Step 4 for Data Movement operations: ShuffleMoveOperation, BroadcastMoveOperation, TrimMoveOperation, PartitionMoveOperation, MoveOperation, CopyOperation.\nStep 3\n\nUse the Request ID and the Step Index to retrieve details from sys.dm_pdw_sql_requests, which contains execution information of the query step on all of the distributed databases.\n\n-- Find the distribution run times for a SQL step.\n-- Replace request_id and step_index with values from Step 1 and 3.\n\nSELECT * FROM sys.dm_pdw_sql_requests\nWHERE request_id = 'QID####' AND step_index = 2;\n\n\nWhen the query step is running, DBCC PDW_SHOWEXECUTIONPLAN can be used to retrieve the SQL Server estimated plan from the SQL Server plan cache for the step running on a particular distribution.\n\n-- Find the SQL Server execution plan for a query running on a specific SQL pool or control node.\n-- Replace distribution_id and spid with values from previous query.\n\nDBCC PDW_SHOWEXECUTIONPLAN(1, 78);\n\nStep 4\n\nUse the Request ID and the Step Index to retrieve information about a data movement step running on each distribution from sys.dm_pdw_dms_workers.\n\n-- Find information about all the workers completing a Data Movement Step.\n-- Replace request_id and step_index with values from Step 1 and 3.\n\nSELECT * FROM sys.dm_pdw_dms_workers\nWHERE request_id = 'QID####' AND step_index = 2;\n\nCheck the total_elapsed_time column to see if a particular distribution is taking longer than others for data movement.\nFor the long-running distribution, check the rows_processed column to see if the number of rows being moved from that distribution is larger than others. If so, this finding might indicate skew of your underlying data. One cause for data skew is distributing on a column with many NULL values (whose rows will all land in the same distribution). Prevent slow queries by avoiding distribution on these types of columns or filtering your query to eliminate NULLs when possible.\n\nIf the query is running, you can use DBCC PDW_SHOWEXECUTIONPLAN to retrieve the SQL Server estimated plan from the SQL Server plan cache for the currently running SQL Step within a particular distribution.\n\n-- Find the SQL Server estimated plan for a query running on a specific SQL pool Compute or control node.\n-- Replace distribution_id and spid with values from previous query.\n\nDBCC PDW_SHOWEXECUTIONPLAN(55, 238);\n\n\nDynamic Management Views (DMV) only contains 10,000 rows of data. On heavily utilized systems this means that data held in this table may be lost with hours, or even minutes as data is managed in a first in, first out system. As a result you can potentially lose meaningful information that can help you diagnose query performance issues on your system. In this situation, you should use the Query Store.\n\nYou can also monitor additional aspects of Azure Synapse SQL pools including:\n\nMonitoring waits\nMonitoring tempdb\nMonitoring memory\nMonitoring transaction log\nMonitoring PolyBase\n\nYou can view information about monitoring these areas here\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage workloads in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/4-manage-workloads",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 4 of 8\nNext\nManage workloads in Azure Synapse Analytics\nCompleted\n100 XP\n10 minutes\n\nAzure Synapse Analytics allows you to create, control and manage resource availability when workloads are competing. This allows you to manage the relative importance of each workload when waiting for available resources.\n\nTo facilitate faster load times, you can create a workload classifier for the load user with the “importance” set to above_normal or High. Workload importance ensures that the load takes precedence over other waiting tasks of a lower importance rating. Use this in conjunction with your own workload group definitions for workload isolation to manage minimum and maximum resource allocations during peak and quiet periods.\n\nDedicated SQL pool workload management in Azure Synapse consists of three high-level concepts:\n\nWorkload Classification\nWorkload Importance\nWorkload Isolation\n\nThese capabilities give you more control over how your workload utilizes system resources.\n\nWorkload classification\n\nWorkload management classification allows workload policies to be applied to requests through assigning resource classes and importance.\n\nWhile there are many ways to classify data warehousing workloads, the simplest and most common classification is load and query. You load data with insert, update, and delete statements. You query the data using selects. A data warehousing solution will often have a workload policy for load activity, such as assigning a higher resource class with more resources. A different workload policy could apply to queries, such as lower importance compared to load activities.\n\nYou can also subclassify your load and query workloads. Subclassification gives you more control of your workloads. For example, query workloads can consist of cube refreshes, dashboard queries or ad-hoc queries. You can classify each of these query workloads with different resource classes or importance settings. Load can also benefit from subclassification. Large transformations can be assigned to larger resource classes. Higher importance can be used to ensure key sales data is loaded before weather data or a social data feed.\n\nNot all statements are classified as they do not require resources or need importance to influence execution. DBCC commands, BEGIN, COMMIT, and ROLLBACK TRANSACTION statements are not classified.\n\nWorkload importance\n\nWorkload importance influences the order in which a request gets access to resources. On a busy system, a request with higher importance has first access to resources. Importance can also ensure ordered access to locks. There are five levels of importance: low, below_normal, normal, above_normal, and high. Requests that don't set importance are assigned the default level of normal. Requests that have the same importance level have the same scheduling behavior that exists today.\n\nWorkload isolation\n\nWorkload isolation reserves resources for a workload group. Resources reserved in a workload group are held exclusively for that workload group to ensure execution. Workload groups also allow you to define the amount of resources that are assigned per request, much like resource classes do. Workload groups give you the ability to reserve or cap the amount of resources a set of requests can consume. Finally, workload groups are a mechanism to apply rules, such as query timeout, to requests.\n\nYou can perform the following steps to implement workload management\n\nCreate a workload classifier to add importance to certain queries\n\nYour organization has asked you if there is a way to mark queries executed by the CEO as more important than others, so they don't appear slow due to heavy data loading or other workloads in the queue. You decide to create a workload classifier and add importance to prioritize the CEO's queries.\n\nSelect the Develop hub.\n\nFrom the Develop menu, select the + button (1) and choose SQL Script (2) from the context menu.\n\nIn the toolbar menu, connect to the SQL Pool database to execute the query.\n\nIn the query window, replace the script with the following to confirm that there are no queries currently being run by users logged in as asa.sql.workload01, representing the CEO of the organization or asa.sql.workload02 representing the data analyst working on the project:\n\nSQL\nCopy\n--First, let's confirm that there are no queries currently being run by users logged in workload01 or workload02\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, \nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') \n--and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,s.login_name\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nNow that we have confirmed that there are no running queries, we need to flood the system with queries and see what happens for asa.sql.workload01 and asa.sql.workload02. To do this, we'll run a Azure Synapse Pipeline which triggers queries.\n\nSelect the Integrate hub.\n\nSelect the Lab 08 - Execute Data Analyst and CEO Queries Pipeline (1), which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.\n\nLet's see what happened to all the queries we just triggered as they flood the system. In the query window, replace the script with the following:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following:\n\nNotice that the Importance level for all queries is set to normal.\n\nWe will give our asa.sql.workload01 user queries priority by implementing the Workload Importance feature. In the query window, replace the script with the following:\n\nSQL\nCopy\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n\nWe are executing this script to create a new Workload Classifier named CEO that uses the largerc Workload Group and sets the Importance level of the queries to High.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nLet's flood the system again with queries and see what happens this time for asa.sql.workload01 and asa.sql.workload02 queries. To do this, we'll run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab, run the Lab 08 - Execute Data Analyst and CEO Queries Pipeline, which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries.\n\nIn the query window, replace the script with the following to see what happens to the asa.sql.workload01 queries this time:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status desc\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following:\n\nNotice that the queries executed by the asa.sql.workload01 user have a high importance.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nReserve resources for specific workloads through workload isolation\n\nWorkload isolation means resources are reserved, exclusively, for a workload group. Workload groups are containers for a set of requests and are the basis for how workload management, including workload isolation, is configured on a system. A simple workload management configuration can manage data loads and user queries.\n\nIn the absence of workload isolation, requests operate in the shared pool of resources. Access to resources in the shared pool is not guaranteed and is assigned on an importance basis.\n\nGiven the workload requirements provided by Tailwind Traders, you decide to create a new workload group called CEODemo to reserve resources for queries executed by the CEO.\n\nLet's start by experimenting with different parameters.\n\nIn the query window, replace the script with the following:\n\nSQL\nCopy\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_groups where name = 'CEODemo')\nBEGIN\n    Create WORKLOAD GROUP CEODemo WITH  \n    ( MIN_PERCENTAGE_RESOURCE = 50        -- integer value\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25 --  \n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\nEND\n\n\nThe script creates a workload group called CEODemo to reserve resources exclusively for the workload group. In this example, a workload group with a MIN_PERCENTAGE_RESOURCE set to 50% and REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 25% is guaranteed 2 concurrency.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nIn the query window, replace the script with the following to create a Workload Classifier called CEODreamDemo that assigns a workload group and importance to incoming requests:\n\nSQL\nCopy\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where  name = 'CEODreamDemo')\nBEGIN\n    Create Workload Classifier CEODreamDemo with\n    ( Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n\nThis script sets the Importance to BELOW_NORMAL for the asa.sql.workload02 user, through the new CEODreamDemo Workload Classifier.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nIn the query window, replace the script with the following to confirm that there are no active queries being run by asa.sql.workload02 (suspended queries are OK):\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nSelect the Integrate hub.\n\nSelect the Lab 08 - Execute Business Analyst Queries Pipeline (1), which will run / trigger asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.\n\nIn the query window, replace the script with the following to see what happened to all the asa.sql.workload02 queries we just triggered as they flood the system:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following that shows the importance for each session set to below_normal:\n\nNotice that the running scripts are executed by the asa.sql.workload02 user (1) with an Importance level of below_normal (2). We have successfully configured the business analyst queries to execute at a lower importance than the CEO queries. We can also see that the CEODreamDemo Workload Classifier works as expected.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nReturn to the query window under the Develop hub. In the query window, replace the script with the following to set 3.25% minimum resources per request:\n\nSQL\nCopy\nIF  EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where group_name = 'CEODemo')\nBEGIN\n    Drop Workload Classifier CEODreamDemo\n    DROP WORKLOAD GROUP CEODemo\n    --- Creates a workload group 'CEODemo'.\n        Create  WORKLOAD GROUP CEODemo WITH  \n    (MIN_PERCENTAGE_RESOURCE = 26 -- integer value\n        ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 3.25 -- factor of 26 (guaranteed more than 4 concurrencies)\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\n    --- Creates a workload Classifier 'CEODreamDemo'.\n    Create Workload Classifier CEODreamDemo with\n    (Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n\n Note\n\nConfiguring workload containment implicitly defines a maximum level of concurrency. With a CAP_PERCENTAGE_RESOURCE set to 60% and a REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 1%, up to a 60-concurrency level is allowed for the workload group. Consider the method included below for determining the maximum concurrency: [Max Concurrency] = [CAP_PERCENTAGE_RESOURCE] / [REQUEST_MIN_RESOURCE_GRANT_PERCENT]\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nLet's flood the system again and see what happens for asa.sql.workload02. To do this, we will run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab. Run the Lab 08 - Execute Business Analyst Queries Pipeline, which will run / trigger asa.sql.workload02 queries.\n\nIn the query window, replace the script with the following to see what happened to all of the asa.sql.workload02 queries we just triggered as they flood the system:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis  not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nAfter several moments (up to a minute), we should see several concurrent executions by the asa.sql.workload02 user running at below_normal importance. We have validated that the modified Workload Group and Workload Classifier works as expected.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nNext unit: Use Azure Advisor to review recommendations\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Advisor to review recommendations - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/5-use-azure-advisor-to-review-recommendations",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 8\nNext\nUse Azure Advisor to review recommendations\nCompleted\n100 XP\n9 minutes\n\nAzure Advisor provides you with personalized messages that provide information on best practices to optimize the setup of your Azure services. It analyzes your resource configuration and usage telemetry and then recommends solutions that can help you improve the cost effectiveness, performance, Reliability (formerly called High availability), and security of your Azure resources.\n\nThe Advisor may appear when you log into the Azure portal, but you can also access the Advisor by selecting Advisor in the navigation menu.\n\nOn accessing Advisor, a dashboard is presented that provides recommendations in the following areas:\n\nCost\nSecurity\nReliability\nOperational excellence\nPerformance\n\nYou can click on any of the dashboard items for more information. In the following example, the performance dashboard item is showing more information on two high impact items in Azure Synapse Analytics.\n\nYou can also click on each item to get even more information that can help you resolve the issue. In the following example, this is the information that is shown when clicking on the Create statistics on table columns recommendation.\n\nIn this screen, you can click on the view impacted tables to see which tables are being impacted specifically, and there are also links to the help in the Azure documentation that you can use to get more understanding of the issue.\n\nHow Azure Synapse Analytics works with Azure Advisor\n\nAzure Advisor recommendations are free, and the recommendations are based on telemetry data that is generated by Azure Synapse Analytics. The telemetry data that is captured by Azure Synapse Analytics include\n\nData Skew and replicated table information.\nColumn statistics data.\nTempDB utilization data.\nAdaptive Cache.\n\nAzure Advisor recommendations are checked every 24 hours, as the recommendation API is queried against the telemetry generated from with Azure Synapse Analytics, and the recommendation dashboards are then updated to reflect the information that the telemetry has generated. This can then be viewed in the Azure Advisor dashboard.\n\nNext unit: Use dynamic management views to identify and troubleshoot query performance\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Pause compute in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/3-pause-compute",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPause compute in Azure Synapse Analytics\n3 minutes\n\nWhen performing the batch movement of data to populate a data warehouse, it is typical for the data engineer to understand the schedule on which the data loads take place. In these circumstances, you may be able to predict the periods of downtime in the data loading and querying process and take advantage of the pause operations to minimize your costs.\n\nIn the Azure portal you can use the Pause command within the dedicated SQL pool\n\nAnd this can also be used within Azure Synapse Studio for Apache Spark pools too, in the Manage hub.\n\nWhich allows you to enable it, and set the number of minutes idle\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Scale compute resources in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/2-scale-compute-resources",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nScale compute resources in Azure Synapse Analytics\n8 minutes\n\nOne of the key management features that you have at your disposal within Azure Synapse Analytics, is the ability to scale the compute resources for SQL or Spark pools to meet the demands of processing your data. In SQL pools, the unit of scale is an abstraction of compute power that is known as a data warehouse unit. Compute is separate from storage, which enables you to scale compute independently of the data in your system. This means you can scale up and scale down the compute power to meet your needs.\n\nYou can scale a Synapse SQL pool either through the Azure portal, Azure Synapse Studio or programmatically using TSQL or PowerShell.\n\nIn the Azure portal, you can click on scale icon\n\nAnd then you can adjust the slider to scale the SQL Pool\n\nAnother option to scale is within Azure Synapse Studio, click on the scale icon:\n\nAnd then move the slider as follows:\n\nYou can also make the modification using Transact-SQL\n\nSQL\nCopy\nALTER DATABASE mySampleDataWarehouse\nMODIFY (SERVICE_OBJECTIVE = 'DW300c');\n\n\nOr by using PowerShell\n\nPowerShell\nCopy\nSet-AzSqlDatabase -ResourceGroupName \"resourcegroupname\" -DatabaseName \"mySampleDataWarehouse\" -ServerName \"sqlpoolservername\" -RequestedServiceObjectiveName \"DW300c\"\n\nScaling Apache Spark pools in Azure Synapse Analytics\n\nApache Spark pools for Azure Synapse Analytics uses an Autoscale feature that automatically scales the number of nodes in a cluster instance up and down. During the creation of a new Spark pool, a minimum and maximum number of nodes can be set when Autoscale is selected. Autoscale then monitors the resource requirements of the load and scales the number of nodes up or down. To enable the Autoscale feature, complete the following steps as part of the normal pool creation process:\n\nOn the Basics tab, select the Enable autoscale checkbox.\nEnter the desired values for the following properties:\nMin number of nodes.\nMax number of nodes.\n\nThe initial number of nodes will be the minimum. This value defines the initial size of the instance when it's created. The minimum number of nodes can't be fewer than three.\n\nYou can also modify this in the Azure portal, you can click on auto-scale settings icon\n\nChoose the node size and the number of nodes\n\nand for Azure Synapse Studio as follows\n\nAnd Choose the node size and the number of nodes\n\nAutoscale continuously monitors the Spark instance and collects the following metrics:\n\nExpand table\nMetric\tDescription\nTotal Pending CPU\tThe total number of cores required to start execution of all pending nodes.\nTotal Pending Memory\tThe total memory (in MB) required to start execution of all pending nodes.\nTotal Free CPU\tThe sum of all unused cores on the active nodes.\nTotal Free Memory\tThe sum of unused memory (in MB) on the active nodes.\nUsed Memory per Node\tThe load on a node. A node on which 10 GB of memory is used, is considered under more load than a worker with 2 GB of used memory.\n\nThe following conditions will then autoscale the memory or CPU\n\nExpand table\nScale-up\tScale-down\nTotal pending CPU is greater than total free CPU for more than 1 minute.\tTotal pending CPU is less than total free CPU for more than 2 minutes.\nTotal pending memory is greater than total free memory for more than 1 minute.\tTotal pending memory is less than total free memory for more than 2 minutes.\n\nThe scaling operation can take between 1 -5 minutes. During an instance where there is a scale down process, Autoscale will put the nodes in decommissioning state so that no new executors can launch on that node.\n\nThe running jobs will continue to run and finish. The pending jobs will wait to be scheduled as normal with fewer available nodes.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this module, you will learn some of the features you can use to manage and monitor Azure Synapse Analytics.\n\nAt the end of this module, you will\n\nScale compute resources in Azure Synapse Analytics\nPause compute in Azure Synapse Analytics\nManage workloads in Azure Synapse Analytics\nUse Azure Advisor to review recommendations\nUse Dynamic Management Views to identify and troubleshoot query performance\nPrerequisites\n\nBefore taking this module, it is recommended that the student is able to:\n\nLog into the Azure portal\nCreate a Synapse Analytics Workspace\nCreate an Azure Synapse Analytics SQL Pool\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage and monitor data warehouse activities in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nManage and monitor data warehouse activities in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nBeginner\nData Engineer\nAzure Synapse Analytics\n\nLearn how to manage and monitor Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you will:\n\nScale compute resources in Azure Synapse Analytics\nPause compute in Azure Synapse Analytics\nManage workloads in Azure Synapse Analytics\nUse Azure Advisor to review recommendations\nUse Dynamic Management Views to identify and troubleshoot query performance\nAdd\nPrerequisites\nBefore taking this module, it is recommended that you complete Data Fundamentals.\nIntroduction\nmin\nScale compute resources in Azure Synapse Analytics\nmin\nPause compute in Azure Synapse Analytics\nmin\nManage workloads in Azure Synapse Analytics\nmin\nUse Azure Advisor to review recommendations\nmin\nUse dynamic management views to identify and troubleshoot query performance\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/10-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nLoading data warehouse tables is a core task for data engineers. This module introduced some common SQL-based techniques that you can use to stage and load data in a relational data warehouse that's hosted in a dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - load data into a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/8-exercise-load-data-into-relational-data-warehouse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - load data into a relational data warehouse\n40 minutes\n\nNow it's your chance to explore loading and updating data into a relational data warehouse for yourself. In this exercise, you'll create and update fact and dimension tables into a dedicated SQL pool using the various techniques described in this module.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/9-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions, then select Check your answers.\n\n1. \n\nIn which order should you load tables in the data warehouse?\n\n \n\nStaging tables, then dimension tables, then fact tables\n\nStaging tables, then fact tables, then dimension tables\n\nDimension tables, then staging tables, then fact tables\n\n2. \n\nWhich command should you use to load a staging table with data from files in the data lake?\n\n \n\nCOPY\n\nLOAD\n\nINSERT\n\n3. \n\nWhen a customer changes their phone number, the change should be made in the existing row for that customer in the dimension table. What type of slowly changing dimension does this scenario require?\n\n \n\nType 0\n\nType 1\n\nType 2\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Perform post load optimization - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/7-perform-post-load-optimization",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPerform post load optimization\n3 minutes\n\nAfter loading new data into the data warehouse, it's a good idea to rebuild the table indexes and update statistics on commonly queried columns.\n\nRebuild indexes\n\nThe following example rebuilds all indexes on the DimProduct table.\n\nALTER INDEX ALL ON dbo.DimProduct REBUILD\n\n\n Tip\n\nFor more information about rebuilding indexes, see the Indexes on dedicated SQL pool tables in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nUpdate statistics\n\nThe following example creates statistics on the ProductCategory column of the DimProduct table:\n\nCREATE STATISTICS productcategory_stats\nON dbo.DimProduct(ProductCategory);\n\n\n Tip\n\nFor more information about updating statistics, see the Table statistics for dedicated SQL pool in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load fact tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/6-load-fact-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad fact tables\n3 minutes\n\nTypically, a regular data warehouse load operation loads fact tables after dimension tables. This approach ensures that the dimensions to which the facts will be related are already present in the data warehouse.\n\nThe staged fact data usually includes the business (alternate) keys for the related dimensions, so your logic to load the data must look up the corresponding surrogate keys. When the data warehouse slowly changing dimensions, the appropriate version of the dimension record must be identified to ensure the correct surrogate key is used to match the event recorded in the fact table with the state of the dimension at the time the fact occurred.\n\nIn many cases, you can retrieve the latest \"current\" version of the dimension; but in some cases you might need to find the right dimension record based on DateTime columns that indicate the period of validity for each version of the dimension.\n\nThe following example assumes that the dimension records have an incrementing surrogate key, and that the most recently added version of a specific dimension instance (which will have the highest key value) should be used.\n\nINSERT INTO dbo.FactSales\nSELECT  (SELECT MAX(DateKey)\n         FROM dbo.DimDate\n         WHERE FullDateAlternateKey = stg.OrderDate) AS OrderDateKey,\n        (SELECT MAX(CustomerKey)\n         FROM dbo.DimCustomer\n         WHERE CustomerAlternateKey = stg.CustNo) AS CustomerKey,\n        (SELECT MAX(ProductKey)\n         FROM dbo.DimProduct\n         WHERE ProductAlternateKey = stg.ProductID) AS ProductKey,\n        (SELECT MAX(StoreKey)\n         FROM dbo.DimStore\n         WHERE StoreAlternateKey = stg.StoreID) AS StoreKey,\n        OrderNumber,\n        OrderLineItem,\n        OrderQuantity,\n        UnitPrice,\n        Discount,\n        Tax,\n        SalesAmount\nFROM dbo.StageSales AS stg\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load slowly changing dimensions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/5-load-slowly-changing-dimensions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad slowly changing dimensions\n5 minutes\n\nIn most relational data warehouses, you need to handle updates to dimension data and support what are commonly referred to as slowly changing dimensions (SCDs).\n\nTypes of slowly changing dimension\n\nThere are multiple kinds of slowly changing dimension, of which three are commonly implemented:\n\nType 0\n\nType 0 dimension data can't be changed. Any attempted changes fail.\n\nExpand table\nDateKey\tDateAltKey\tDay\tMonth\tYear\n20230101\t01-01-2023\tSunday\tJanuary\t2023\nType 1\n\nIn type 1 dimensions, the dimension record is updated in-place. Changes made to an existing dimension row apply to all previously loaded facts related to the dimension.\n\nExpand table\nStoreKey\tStoreAltKey\tStoreName\n123\tEH199J\tHigh Street Store Town Central Store\nType 2\n\nIn a type 2 dimension, a change to a dimension results in a new dimension row. Existing rows for previous versions of the dimension are retained for historical fact analysis and the new row is applied to future fact table entries.\n\nExpand table\nCustomerKey\tCustomerAltKey\tName\tAddress\tCity\tDateFrom\tDateTo\tIsCurrent\n1211\tjo@contoso.com\tJo Smith\t999 Main St\tSeattle\t20190101\t20230105\tFalse\n2996\tjo@contoso.com\tJo Smith\t1234 9th Ave\tBoston\t20230106\t\tTrue\n\n Note\n\nType 2 dimensions often include columns to track the effective time periods for each version of an entity, and/or a flag to indicate which row represents the current version of the entity. If you’re using an incrementing surrogate key and you only need to track the most recently added version of an entity, then you may not need these columns; but before making that decision, consider how you’ll look up the appropriate version of an entity when a new fact is entered based on the time at which the event the fact relates to occurred.\n\nCombining INSERT and UPDATE statements\n\nLogic to implement Type 1 and Type 2 updates can be complex, and there are various techniques you can use. For example, you could use a combination of UPDATE and INSERT statements.\n\n-- New Customers\nINSERT INTO dbo.DimCustomer\nSELECT stg.*\nFROM dbo.StageCustomers AS stg\nWHERE NOT EXISTS\n    (SELECT * FROM dbo.DimCustomer AS dim\n    WHERE dim.CustomerAltKey = stg.CustNo)\n\n-- Type 1 updates (name)\nUPDATE dbo.DimCustomer\nSET CustomerName = stg.CustomerName\nFROM dbo.StageCustomers AS stg\nWHERE dbo.DimCustomer.CustomerAltKey = stg.CustomerNo;\n\n-- Type 2 updates (StreetAddress)\nINSERT INTO dbo.DimCustomer\nSELECT stg.*\nFROM dbo.StageCustomers AS stg\nJOIN dbo.DimCustomer AS dim\nON stg.CustNo = dim.CustomerAltKey\nAND stg.StreetAddress <> dim.StreetAddress;\n\n\n\nIn the previous example, it's assumed that an incrementing surrogate key based on an IDENTITY column identifies each row, and that the highest value surrogate key for a given alternate key indicates the most recent or \"current\" instance of the dimension entity associated with that alternate key. In practice, many data warehouse designers include a Boolean column to indicate the current active instance of a changing dimension or use DateTime fields to indicate the active time periods for each version of the dimension instance. With these approaches, the logic for a type 2 change must include an INSERT of the new dimension row and an UPDATE to mark the current row as inactive.\n\nUsing a MERGE statement\n\nAs an alternative to using multiple INSERT and UPDATE statements, you can use a single MERGE statement to perform an \"upsert\" operation to insert new records and update existing ones.\n\nMERGE dbo.DimProduct AS tgt\n    USING (SELECT * FROM dbo.StageProducts) AS src\n    ON src.ProductID = tgt.ProductBusinessKey\nWHEN MATCHED THEN\n    -- Type 1 updates\n    UPDATE SET\n        tgt.ProductName = src.ProductName,\n        tgt.ProductCategory = src.ProductCategory,\n        tgt.Color = src.Color,\n        tgt.Size = src.Size,\n        tgt.ListPrice = src.ListPrice,\n        tgt.Discontinued = src.Discontinued\nWHEN NOT MATCHED THEN\n    -- New products\n    INSERT VALUES\n        (src.ProductID,\n        src.ProductName,\n        src.ProductCategory,\n        src.Color,\n        src.Size,\n        src.ListPrice,\n        src.Discontinued);\n\n\n Note\n\nFor more information about the MERGE statement, see the MERGE documentation for Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load time dimension tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/4-load-time-dimension-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad time dimension tables\n3 minutes\n\nTime dimension tables store a record for each time interval based on the grain by which you want to aggregate data over time. For example, a time dimension table at the date grain contains a record for each date between the earliest and latest dates referenced by the data in related fact tables.\n\nThe following code example shows how you can generate a sequence of time dimension values based on a date grain.\n\n-- Create a temporary table for the dates we need\nCREATE TABLE #TmpStageDate (DateVal DATE NOT NULL)\n\n-- Populate the temp table with a range of dates\nDECLARE @StartDate DATE\nDECLARE @EndDate DATE\nSET @StartDate = '2019-01-01'\nSET @EndDate = '2023-12-31'\nDECLARE @LoopDate = @StartDate\nWHILE @LoopDate <= @EndDate\nBEGIN\n    INSERT INTO #TmpStageDate VALUES\n    (\n        @LoopDate\n    )\n    SET @LoopDate = DATEADD(dd, 1, @LoopDate)\nEND\n\n-- Insert the dates and calculated attributes into the dimension table\nINSERT INTO dbo.DimDate\nSELECT CAST(CONVERT(VARCHAR(8), DateVal, 112) as INT), -- date key\n    DateVal, --date alt key\n    Day(DateVal) -- day number of month\n    --,  other derived temporal fields as required\nFROM #TmpStageDate\nGO\n\n--Drop temporary table\nDROP TABLE #TmpStageDate\n\n\n Tip\n\nScripting this in SQL may be time-consuming in a dedicated SQL pool – it may be more efficient to prepare the data in Microsoft Excel or an external script and import it using the COPY statement.\n\nAs the data warehouse is populated in the future with new fact data, you periodically need to extend the range of dates related time dimension tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load dimension tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/3-load-dimension-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad dimension tables\n5 minutes\n\nAfter staging dimension data, you can load it into dimension tables using SQL.\n\nUsing a CREATE TABLE AS (CTAS) statement\n\nOne of the simplest ways to load data into a new dimension table is to use a CREATE TABLE AS (CTAS) expression. This statement creates a new table based on the results of a SELECT statement.\n\nCREATE TABLE dbo.DimProduct\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\nSELECT ROW_NUMBER() OVER(ORDER BY ProdID) AS ProdKey,\n    ProdID as ProdAltKey,\n    ProductName,\n    ProductCategory,\n    Color,\n    Size,\n    ListPrice,\n    Discontinued\nFROM dbo.StageProduct;\n\n\n Note\n\nYou can't use IDENTITY to generate a unique integer value for the surrogate key when using a CTAS statement, so this example uses the ROW_NUMBER function to generate an incrementing row number for each row in the results ordered by the ProductID business key in the staged data.\n\nYou can also load a combination of new and updated data into a dimension table by using a CREATE TABLE AS (CTAS) statement to create a new table that UNIONs the existing rows from the dimension table with the new and updated records from the staging table. After creating the new table, you can delete or rename the current dimension table, and rename the new table to replace it.\n\nCREATE TABLE dbo.DimProductUpsert\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\n-- New or updated rows\nSELECT  stg.ProductID AS ProductBusinessKey,\n        stg.ProductName,\n        stg.ProductCategory,\n        stg.Color,\n        stg.Size,\n        stg.ListPrice,\n        stg.Discontinued\nFROM    dbo.StageProduct AS stg\nUNION ALL  \n-- Existing rows\nSELECT  dim.ProductBusinessKey,\n        dim.ProductName,\n        dim.ProductCategory,\n        dim.Color,\n        dim.Size,\n        dim.ListPrice,\n        dim.Discontinued\nFROM    dbo.DimProduct AS dim\nWHERE NOT EXISTS\n(   SELECT  *\n    FROM dbo.StageProduct AS stg\n    WHERE stg.ProductId = dim.ProductBusinessKey\n);\n\nRENAME OBJECT dbo.DimProduct TO DimProductArchive;\nRENAME OBJECT dbo.DimProductUpsert TO DimProduct;\n\n\nWhile this technique is effective in merging new and existing dimension data, lack of support for IDENTITY columns means that it's difficult to generate a surrogate key.\n\n Tip\n\nFor more information, see CREATE TABLE AS SELECT (CTAS) in the Azure Synapse Analytics documentation.\n\nUsing an INSERT statement\n\nWhen you need to load staged data into an existing dimension table, you can use an INSERT statement. This approach works if the staged data contains only records for new dimension entities (not updates to existing entities). This approach is much less complicated than the technique in the last section, which required a UNION ALL and then renaming table objects.\n\nINSERT INTO dbo.DimCustomer\nSELECT CustomerNo AS CustAltKey,\n    CustomerName,\n    EmailAddress,\n    Phone,\n    StreetAddress,\n    City,\n    PostalCode,\n    CountryRegion\nFROM dbo.StageCustomers\n\n\n Note\n\nAssuming the DimCustomer dimension table is defined with an IDENTITY CustomerKey column for the surrogate key (as described in the previous unit), the key will be generated automatically and the remaining columns will be populated using the values retrieved from the staging table by the SELECT query.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load staging tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/2-load-staging-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad staging tables\n5 minutes\n\nOne of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n\nCreating staging tables\n\nMany organized warehouses have standard structures for staging the database and might even use a specific schema for staging the data. The following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n\n Note\n\nThis example creates a staging table in the default dbo schema. You can also create separate schemas for staging tables with a meaningful name, such as stage so architects and users understand the purpose of the schema.\n\nCREATE TABLE dbo.StageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nUsing the COPY command\n\nYou can use the COPY statement to load data from the data lake, as shown in the following example:\n\n Note\n\nThis is generally the recommended approach to load staging tables due to its high performance throughput.\n\nCOPY INTO dbo.StageProduct\n    (ProductID, ProductName, ...)\nFROM 'https://mydatalake.../data/products*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF'\n);\n\n\n Tip\n\nTo learn more about the COPY statement, see COPY (Transact-SQL) in the Transact-SQL documentation.\n\nUsing external tables\n\nIn some cases, if the data to be loaded is stored in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Azure Synapse Analytics workspace:\n\nCREATE EXTERNAL TABLE dbo.ExternalStageProduct\n (\n     ProductID NVARCHAR(10) NOT NULL,\n     ProductName NVARCHAR(10) NOT NULL,\n ...\n )\nWITH\n (\n    DATE_SOURCE = StagedFiles,\n    LOCATION = 'folder_name/*.parquet',\n    FILE_FORMAT = ParquetFormat\n );\nGO\n\n\n Tip\n\nFor more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nMany enterprise analytical solutions include a relational data warehouse. Data engineers are responsible for implementing ingestion solutions that load data into the data warehouse tables, usually on a regular schedule.\n\nAs a data engineer, you need to be familiar with the considerations and techniques that apply to loading a data warehouse. In this module, we'll focus on ways that you can use SQL to load data into tables in a dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load data into a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLoad data into a relational data warehouse\nModule\n10 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nA core responsibility for a data engineer is to implement a data ingestion solution that loads new data into a relational data warehouse.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nLoad staging tables in a data warehouse\nLoad dimension tables in a data warehouse\nLoad time dimensions in a data warehouse\nLoad slowly changing dimensions in a data warehouse\nLoad fact tables in a data warehouse\nPerform post-load optimizations in a data warehouse\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with data warehouses in Azure Synapse Analytics. Consider completing the Analyze data in a relational data warehouse module first.\n\nIntroduction\nmin\nLoad staging tables\nmin\nLoad dimension tables\nmin\nLoad time dimension tables\nmin\nLoad slowly changing dimensions\nmin\nLoad fact tables\nmin\nPerform post load optimization\nmin\nExercise - load data into a relational data warehouse\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nIn which of the following table types should an insurance company store details of customer attributes by which claims will be aggregated?\n\n \n\nStaging table\n\nDimension table\n\nFact table\n\n2. \n\nYou create a dimension table for product data, assigning a unique numeric key for each row in a column named ProductKey. The ProductKey is only defined in the data warehouse. What kind of key is ProductKey?\n\n \n\nA surrogate key\n\nAn alternate key\n\nA business key\n\n3. \n\nWhat distribution option would be best for a sales fact table that will contain billions of records?\n\n \n\nHASH\n\nROUND_ROBIN\n\nREPLICATE\n\n4. \n\nYou need to write a query to return the total of the UnitsProduced numeric measure in the FactProduction table aggregated by the ProductName attribute in the FactProduct table. Both tables include a ProductKey surrogate key field. What should you do?\n\n \n\nUse two SELECT queries with a UNION ALL clause to combine the rows in the FactProduction table with those in the FactProduct table.\n\nUse a SELECT query against the FactProduction table with a WHERE clause to filter out rows with a ProductKey that doesn't exist in the FactProduct table.\n\nUse a SELECT query with a SUM function to total the UnitsProduced metric, using a JOIN on the ProductKey surrogate key to match the FactProduction records to the FactProduct records and a GROUP BY clause to aggregate by ProductName.\n\n5. \n\nYou use the RANK function in a query to rank customers in order of the number of purchases they have made. Five customers have made the same number of purchases and are all ranked equally as 1. What rank will the customer with the next highest number of purchases be assigned?\n\n \n\ntwo\n\nsix\n\none\n\n6. \n\nYou need to compare approximate production volumes by product while optimizing query response time. Which function should you use?\n\n \n\nCOUNT\n\nNTILE\n\nAPPROX_COUNT_DISTINCT\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nRelational data warehousing skills are essential in multiple data professional roles, including data engineers, data analysts, and data scientists.\n\nIn this module, you learned how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nLearn more\n\nTo learn more about using Azure Synapse Analytics for relational data warehousing, refer to Synapse POC playbook: Data warehousing with dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/6-exercise-explore-data-warehouse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore a data warehouse\n45 minutes\n\nNow it's your opportunity to explore a relational data warehouse. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then explore a data warehouse that has been created for you.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query a data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/5-query-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery a data warehouse\n10 minutes\n\nWhen the dimension and fact tables in a data warehouse have been loaded with data, you can use SQL to query the tables and analyze the data they contain. The Transact-SQL syntax used to query tables in a Synapse dedicated SQL pool is similar to SQL used in SQL Server or Azure SQL Database.\n\nAggregating measures by dimension attributes\n\nMost data analytics with a data warehouse involves aggregating numeric measures in fact tables by attributes in dimension tables. Because of the way a star or snowflake schema is implemented, queries to perform this kind of aggregation rely on JOIN clauses to connect fact tables to dimension tables, and a combination of aggregate functions and GROUP BY clauses to define the aggregation hierarchies.\n\nFor example, the following SQL queries the FactSales and DimDate tables in a hypothetical data warehouse to aggregate sales amounts by year and quarter:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter\nORDER BY dates.CalendarYear, dates.CalendarQuarter;\n\n\nThe results from this query would look similar to the following table:\n\nExpand table\nCalendarYear\tCalendarQuarter\tTotalSales\n2020\t1\t25980.16\n2020\t2\t27453.87\n2020\t3\t28527.15\n2020\t4\t31083.45\n2021\t1\t34562.96\n2021\t2\t36162.27\n...\t...\t...\n\nYou can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the DimCustomer table:\n\nSELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        custs.City,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nJOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City\nORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City;\n\n\nThis time, the results include a quarterly sales total for each city:\n\nExpand table\nCalendarYear\tCalendarQuarter\tCity\tTotalSales\n2020\t1\tAmsterdam\t5982.53\n2020\t1\tBerlin\t2826.98\n2020\t1\tChicago\t5372.72\n...\t...\t...\t..\n2020\t2\tAmsterdam\t7163.93\n2020\t2\tBerlin\t8191.12\n2020\t2\tChicago\t2428.72\n...\t...\t...\t..\n2020\t3\tAmsterdam\t7261.92\n2020\t3\tBerlin\t4202.65\n2020\t3\tChicago\t2287.87\n...\t...\t...\t..\n2020\t4\tAmsterdam\t8262.73\n2020\t4\tBerlin\t5373.61\n2020\t4\tChicago\t7726.23\n...\t...\t...\t..\n2021\t1\tAmsterdam\t7261.28\n2021\t1\tBerlin\t3648.28\n2021\t1\tChicago\t1027.27\n...\t...\t...\t..\nJoins in a snowflake schema\n\nWhen using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a DimProduct dimension table from which the product categories have been normalized into a separate DimCategory table. A query to aggregate items sold by product category might look similar to the following example:\n\nSELECT  cat.ProductCategory,\n        SUM(sales.OrderQuantity) AS ItemsSold\nFROM dbo.FactSales AS sales\nJOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey\nJOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey\nGROUP BY cat.ProductCategory\nORDER BY cat.ProductCategory;\n\n\nThe results from this query include the number of items sold for each product category:\n\nExpand table\nProductCategory\tItemsSold\nAccessories\t28271\nBits and pieces\t5368\n...\t...\n\n Note\n\nJOIN clauses for FactSales and DimProduct and for DimProduct and DimCategory are both required, even though no fields from DimProduct are returned by the query.\n\nUsing ranking functions\n\nAnother common kind of analytical query is to partition the results based on a dimension attribute and rank the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL ranking functions such as ROW_NUMBER, RANK, DENSE_RANK, and NTILE. These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition:\n\nROW_NUMBER returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on.\nRANK returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties.\nDENSE_RANK ranks rows in a partition the same way as RANK, but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties.\nNTILE returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, NTILE(4) returns the quartile in which a store's sales volume places it.\n\nFor example, consider the following query:\n\nSELECT  ProductCategory,\n        ProductName,\n        ListPrice,\n        ROW_NUMBER() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,\n        RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,\n        DENSE_RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,\n        NTILE(4) OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile\nFROM dbo.DimProduct\nORDER BY ProductCategory;\n\n\nThe query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table:\n\nExpand table\nProductCategory\tProductName\tListPrice\tRowNumber\tRank\tDenseRank\tQuartile\nAccessories\tWidget\t8.99\t1\t1\t1\t1\nAccessories\tKnicknak\t8.49\t2\t2\t2\t1\nAccessories\tSprocket\t5.99\t3\t3\t3\t2\nAccessories\tDoodah\t5.99\t4\t3\t3\t2\nAccessories\tSpangle\t2.99\t5\t5\t4\t3\nAccessories\tBadabing\t0.25\t6\t6\t5\t4\nBits and pieces\tFlimflam\t7.49\t1\t1\t1\t1\nBits and pieces\tSnickity wotsit\t6.99\t2\t2\t2\t1\nBits and pieces\tFlange\t4.25\t3\t3\t3\t2\n...\t...\t...\t...\t...\t...\t...\n\n Note\n\nThe sample results demonstrate the difference between RANK and DENSE_RANK. Note that in the Accessories category, the Sprocket and Doodah products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a RANK of 5 (there are four products more expensive than it) and a DENSE_RANK of 4 (there are three higher prices).\n\nTo learn more about ranking functions, see Ranking Functions (Transact-SQL) in the Azure Synapse Analytics documentation.\n\nRetrieving an approximate count\n\nWhile the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data.\n\nFor example, the following query uses the COUNT function to retrieve the number of sales for each year in a hypothetical data warehouse:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    COUNT(DISTINCT sales.OrderNumber) AS Orders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe results of this query might look similar to the following table:\n\nExpand table\nCalendarYear\tOrders\n2019\t239870\n2020\t284741\n2021\t309272\n...\t...\n\nThe volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the APPROX_COUNT_DISTINCT function as shown in the following example:\n\nSELECT dates.CalendarYear AS CalendarYear,\n    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear;\n\n\nThe APPROX_COUNT_DISTINCT function uses a HyperLogLog algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table:\n\nExpand table\nCalendarYear\tApproxOrders\n2019\t235552\n2020\t290436\n2021\t304633\n...\t...\n\nThe counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the APPROX_COUNT_DISTINCT function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration.\n\n Note\n\nSee the APPROX_COUNT_DISTINCT function documentation for more details.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/4-load-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad data warehouse tables\n10 minutes\n\nAt a basic level, loading a data warehouse is typically achieved by adding new data from files in a data lake into tables in the data warehouse. The COPY statement is an effective way to accomplish this task, as shown in the following example:\n\nCOPY INTO dbo.StageProducts\n    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)\nFROM 'https://mydatalake.blob.core.windows.net/data/stagedfiles/products/*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF'\n);\n\nConsiderations for designing a data warehouse load process\n\nOne of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n\nIn most cases, you should implement a data warehouse load process that performs tasks in the following order:\n\nIngest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.\nLoad the data from files into staging tables in the relational data warehouse.\nLoad the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.\nLoad the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.\nPerform post-load optimization by updating indexes and table distribution statistics.\n\nAfter using the COPY statement to load data into staging tables, you can use a combination of INSERT, UPDATE, MERGE, and CREATE TABLE AS SELECT (CTAS) statements to load the staged data into dimension and fact tables.\n\n Note\n\nImplementing an effective data warehouse loading solution requires careful consideration of how to manage surrogate keys, slowly changing dimensions, and other complexities inherent in a relational data warehouse schema. To learn more about techniques for loading a data warehouse, consider completing the Load data into a relational data warehouse module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-create-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate data warehouse tables\n10 minutes\n\nNow that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse.\n\nCreating a dedicated SQL pool\n\nTo create a relational data warehouse in Azure Synapse Analytics, you must create a dedicated SQL Pool. The simplest way to do this in an existing Azure Synapse Analytics workspace is to use the Manage page in Azure Synapse Studio, as shown here:\n\nWhen provisioning a dedicated SQL pool, you can specify the following configuration settings:\n\nA unique name for the dedicated SQL pool.\nA performance level for the SQL pool, which can range from DW100c to DW30000c and which determines the cost per hour for the pool when it's running.\nWhether to start with an empty pool or restore an existing database from a backup.\nThe collation of the SQL pool, which determines sort order and string comparison rules for the database. (You can't change the collation after creation).\n\nAfter creating a dedicated SQL pool, you can control its running state in the Manage page of Synapse Studio; pausing it when not required to prevent unnecessary costs.\n\nWhen the pool is running, you can explore it on the Data page, and create SQL scripts to run in it.\n\nConsiderations for creating tables\n\nTo create tables in the dedicated SQL pool, you use the CREATE TABLE (or sometimes the CREATE EXTERNAL TABLE) Transact-SQL statement. The specific options used in the statement depend on the type of table you're creating, which can include:\n\nFact tables\nDimension tables\nStaging tables\n\n Note\n\nThe data warehouse is composed of fact and dimension tables as discussed previously. Staging tables are often used as part of the data warehousing loading process to ingest data from source systems.\n\nWhen designing a star schema model for small or medium sized datasets you can use your preferred database, such as Azure SQL. For larger data sets you may benefit from implementing your data warehouse in Azure Synapse Analytics instead of SQL Server. It's important to understand some key differences when creating tables in Synapse Analytics.\n\nData integrity constraints\n\nDedicated SQL pools in Synapse Analytics don't support foreign key and unique constraints as found in other relational database systems like SQL Server. This means that jobs used to load data must maintain uniqueness and referential integrity for keys, without relying on the table definitions in the database to do so.\n\n Tip\n\nFor more information about constraints in Azure Synapse Analytics dedicated SQL pools, see Primary key, foreign key, and unique key using dedicated SQL pool in Azure Synapse Analytics.\n\nIndexes\n\nWhile Synapse Analytics dedicated SQL pools support clustered indexes as found in SQL Server, the default index type is clustered columnstore. This index type offers a significant performance advantage when querying large quantities of data in a typical data warehouse schema and should be used where possible. However, some tables may include data types that can't be included in a clustered columnstore index (for example, VARBINARY(MAX)), in which case a clustered index can be used instead.\n\n Tip\n\nFor more information about indexing in Azure Synapse Analytics dedicated SQL pools, see Indexes on dedicated SQL pool tables in Azure Synapse Analytics.\n\nDistribution\n\nAzure Synapse Analytics dedicated SQL pools use a massively parallel processing (MPP) architecture, as opposed to the symmetric multiprocessing (SMP) architecture used in most OLTP database systems. In an MPP system, the data in a table is distributed for processing across a pool of nodes. Synapse Analytics supports the following kinds of distribution:\n\nHash: A deterministic hash value is calculated for the specified column and used to assign the row to a compute node.\nRound-robin: Rows are distributed evenly across all compute nodes.\nReplicated: A copy of the table is stored on each compute node.\n\nThe table type often determines which option to choose for distributing the table.\n\nExpand table\nTable type\tRecommended distribution option\nDimension\tUse replicated distribution for smaller tables to avoid data shuffling when joining to distributed fact tables. If tables are too large to store on each compute node, use hash distribution.\nFact\tUse hash distribution with clustered columnstore index to distribute fact tables across compute nodes.\nStaging\tUse round-robin distribution for staging tables to evenly distribute data across compute nodes.\n\n Tip\n\nFor more information about distribution strategies for tables in Azure Synapse Analytics, see Guidance for designing distributed tables using dedicated SQL pool in Azure Synapse Analytics.\n\nCreating dimension tables\n\nWhen you create a dimension table, ensure that the table definition includes surrogate and alternate keys as well as columns for the attributes of the dimension that you want to use to group aggregations. It's often easiest to use an IDENTITY column to auto-generate an incrementing surrogate key (otherwise you need to generate unique keys every time you load data). The following example shows a CREATE TABLE statement for a hypothetical DimCustomer dimension table.\n\nSQL\nCopy\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Note\n\nIf desired, you can create a specific schema as a namespace for your tables. In this example, the default dbo schema is used.\n\nIf you intend to use a snowflake schema in which dimension tables are related to one another, you should include the key for the parent dimension in the definition of the child dimension table. For example, the following SQL code could be used to move the geographical address details from the DimCustomer table to a separate DimGeography dimension table:\n\nSQL\nCopy\nCREATE TABLE dbo.DimGeography\n(\n    GeographyKey INT IDENTITY NOT NULL,\n    GeographyAlternateKey NVARCHAR(10) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    GeographyKey INT NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nTime dimension tables\n\nMost data warehouses include a time dimension table that enables you to aggregate data by multiple hierarchical levels of time interval. For example, the following example creates a DimDate table with attributes that relate to specific dates.\n\nSQL\nCopy\nCREATE TABLE dbo.DimDate\n( \n    DateKey INT NOT NULL,\n    DateAltKey DATETIME NOT NULL,\n    DayOfMonth INT NOT NULL,\n    DayOfWeek INT NOT NULL,\n    DayName NVARCHAR(15) NOT NULL,\n    MonthOfYear INT NOT NULL,\n    MonthName NVARCHAR(15) NOT NULL,\n    CalendarQuarter INT  NOT NULL,\n    CalendarYear INT NOT NULL,\n    FiscalQuarter INT NOT NULL,\n    FiscalYear INT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Tip\n\nA common pattern when creating a dimension table for dates is to use the numeric date in DDMMYYYY or YYYYMMDD format as an integer surrogate key, and the date as a DATE or DATETIME datatype as the alternate key.\n\nCreating fact tables\n\nFact tables include the keys for each dimension to which they're related, and the attributes and numeric measures for specific events or observations that you want to analyze.\n\nThe following code example creates a hypothetical fact table named FactSales that is related to multiple dimensions through key columns (date, customer, product, and store)\n\nSQL\nCopy\nCREATE TABLE dbo.FactSales\n(\n    OrderDateKey INT NOT NULL,\n    CustomerKey INT NOT NULL,\n    ProductKey INT NOT NULL,\n    StoreKey INT NOT NULL,\n    OrderNumber NVARCHAR(10) NOT NULL,\n    OrderLineItem INT NOT NULL,\n    OrderQuantity SMALLINT NOT NULL,\n    UnitPrice DECIMAL NOT NULL,\n    Discount DECIMAL NOT NULL,\n    Tax DECIMAL NOT NULL,\n    SalesAmount DECIMAL NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(OrderNumber),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCreating staging tables\n\nStaging tables are used as temporary storage for data as it's being loaded into the data warehouse. A typical pattern is to structure the table to make it as efficient as possible to ingest the data from its external source (often files in a data lake) into the relational database, and then use SQL statements to load the data from the staging tables into the dimension and fact tables.\n\nThe following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n\nSQL\nCopy\nCREATE TABLE dbo.StageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nUsing external tables\n\nIn some cases, if the data to be loaded is in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Synapse workspace:\n\nSQL\nCopy\n\n-- External data source links to data lake location\nCREATE EXTERNAL DATA SOURCE StagedFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/stagedfiles/'\n);\nGO\n\n-- External format specifies file format\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\nGO\n\n-- External table references files in external data source\nCREATE EXTERNAL TABLE dbo.ExternalStageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DATA_SOURCE = StagedFiles,\n    LOCATION = 'products/*.parquet',\n    FILE_FORMAT = ParquetFormat\n);\nGO\n\n\n Note\n\nFor more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Design a data warehouse schema - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/2-design-star-schema",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDesign a data warehouse schema\n7 minutes\n\nLike all relational databases, a data warehouse contains tables in which the data you want to analyze is stored. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling, in which numerical measures associated with events known as facts can be aggregated by the attributes of associated entities across multiple dimensions. For example, measures associated with a sales order (such as the amount paid or the quantity of items ordered) can be aggregated by attributes of the date on which the sale occurred, the customer, the store, and so on.\n\nTables in a data warehouse\n\nA common pattern for relational data warehouses is to define a schema that includes two kinds of table: dimension tables and fact tables.\n\nDimension tables\n\nDimension tables describe business entities, such as products, people, places, and dates. Dimension tables contain columns for attributes of an entity. For example, a customer entity might have a first name, a last name, an email address, and a postal address (which might consist of a street address, a city, a postal code, and a country or region). In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include two key columns:\n\na surrogate key that is specific to the data warehouse and uniquely identifies each row in the dimension table in the data warehouse - usually an incrementing integer number.\nAn alternate key, often a natural or business key that is used to identify a specific instance of an entity in the transactional source system from which the entity record originated - such as a product code or a customer ID.\n\n Note\n\nWhy have two keys? There are a few good reasons:\n\nThe data warehouse may be populated with data from multiple source systems, which can lead to the risk of duplicate or incompatible business keys.\nSimple numeric keys generally perform better in queries that join lots of tables - a common pattern in data warehouses.\nAttributes of entities may change over time - for example, a customer might change their address. Since the data warehouse is used to support historic reporting, you may want to retain a record for each instance of an entity at multiple points in time; so that, for example, sales orders for a specific customer are counted for the city where they lived at the time the order was placed. In this case, multiple customer records would have the same business key associated with the customer, but different surrogate keys for each discrete address where the customer lived at various times.\n\nAn example of a dimension table for customer might contain the following data:\n\nExpand table\nCustomerKey\tCustomerAltKey\tName\tEmail\tStreet\tCity\tPostalCode\tCountryRegion\n123\tI-543\tNavin Jones\tnavin1@contoso.com\t1 Main St.\tSeattle\t90000\tUnited States\n124\tR-589\tMary Smith\tmary2@contoso.com\t234 190th Ave\tBuffalo\t50001\tUnited States\n125\tI-321\tAntoine Dubois\tantoine1@contoso.com\t2 Rue Jolie\tParis\t20098\tFrance\n126\tI-543\tNavin Jones\tnavin1@contoso.com\t24 125th Ave.\tNew York\t50000\tUnited States\n...\t...\t...\t...\t...\t...\t...\t...\n\n Note\n\nObserve that the table contains two records for Navin Jones. Both records use the same alternate key to identify this person (I-543), but each record has a different surrogate key. From this, you can surmise that the customer moved from Seattle to New York. Sales made to the customer while living in Seattle are associated with the key 123, while purchases made after moving to New York are recorded against record 126.\n\nIn addition to dimension tables that represent business entities, it's common for a data warehouse to include a dimension table that represents time. This table enables data analysts to aggregate data over temporal intervals. Depending on the type of data you need to analyze, the lowest granularity (referred to as the grain) of a time dimension could represent times (to the hour, second, millisecond, nanosecond, or even lower), or dates.\n\nAn example of a time dimension table with a grain at the date level might contain the following data:\n\nExpand table\nDateKey\tDateAltKey\tDayOfWeek\tDayOfMonth\tWeekday\tMonth\tMonthName\tQuarter\tYear\n19990101\t01-01-1999\t6\t1\tFriday\t1\tJanuary\t1\t1999\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20220101\t01-01-2022\t7\t1\tSaturday\t1\tJanuary\t1\t2022\n20220102\t02-01-2022\t1\t2\tSunday\t1\tJanuary\t1\t2022\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20301231\t31-12-2030\t3\t31\tTuesday\t12\tDecember\t4\t2030\n\nThe timespan covered by the records in the table must include the earliest and latest points in time for any associated events recorded in a related fact table. Usually there's a record for every interval at the appropriate grain in between.\n\nFact tables\n\nFact tables store details of observations or events; for example, sales orders, stock balances, exchange rates, or recorded temperatures. A fact table contains columns for numeric values that can be aggregated by dimensions. In addition to the numeric columns, a fact table contains key columns that reference unique keys in related dimension tables.\n\nFor example, a fact table containing details of sales orders might contain the following data:\n\nExpand table\nOrderDateKey\tCustomerKey\tStoreKey\tProductKey\tOrderNo\tLineItemNo\tQuantity\tUnitPrice\tTax\tItemTotal\t\n20220101\t123\t5\t701\t1001\t1\t2\t2.50\t0.50\t5.50\t\n20220101\t123\t5\t765\t1001\t2\t1\t2.00\t0.20\t2.20\t\n20220102\t125\t2\t723\t1002\t1\t1\t4.99\t0.49\t5.48\t\n20220103\t126\t1\t823\t1003\t1\t1\t7.99\t0.80\t8.79\t\n...\t...\t...\t...\t...\t...\t...\t...\t\t...\t...\n\nA fact table's dimension key columns determine its grain. For example, the sales orders fact table includes keys for dates, customers, stores, and products. An order might include multiple products, so the grain represents line items for individual products sold in stores to customers on specific days.\n\nData warehouse schema designs\n\nIn most transactional databases that are used in business applications, the data is normalized to reduce duplication. In a data warehouse however, the dimension data is generally de-normalized to reduce the number of joins required to query the data.\n\nOften, a data warehouse is organized as a star schema, in which a fact table is directly related to the dimension tables, as shown in this example:\n\n]\n\nThe attributes of an entity can be used to aggregate measures in fact tables over multiple hierarchical levels - for example, to find total sales revenue by country or region, city, postal code, or individual customer. The attributes for each level can be stored in the same dimension table. However, when an entity has a large number of hierarchical attribute levels, or when some attributes can be shared by multiple dimensions (for example, both customers and stores have a geographical address), it can make sense to apply some normalization to the dimension tables and create a snowflake schema, as shown in the following example:\n\nIn this case, the DimProduct table has been normalized to create separate dimension tables for product categories and suppliers, and a DimGeography table has been added to represent geographical attributes for both customers and stores. Each row in the DimProduct table contains key values for the corresponding rows in the DimCategory and DimSupplier tables; and each row in the DimCustomer and DimStore tables contains a key value for the corresponding row in the DimGeography table.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nRelational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse.\n\nAzure Synapse Analytics includes a highly scalable relational database engine that is optimized for data warehousing workloads. By using dedicated SQL pools in Azure Synapse Analytics, you can create databases that are capable of hosting and querying huge volumes of data in relational tables.\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data in a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAnalyze data in a relational data warehouse\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nRelational data warehouses are a core element of most enterprise Business Intelligence (BI) solutions, and are used as the basis for data models, reports, and analysis.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nAdd\nPrerequisites\n\nBefore taking this module, you should have:\n\nAn understanding of data fundamentals.\nExperience of querying data with Transact-SQL.\nIntroduction\nmin\nDesign a data warehouse schema\nmin\nCreate data warehouse tables\nmin\nLoad data warehouse tables\nmin\nQuery a data warehouse\nmin\nExercise - Explore a data warehouse\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/9-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nDelta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.\n\nIn this module, you learned how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\n\nTo learn more about using Delta Lake in Azure Synapse Analytics, see Linux Foundation Delta Lake overview in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/8-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich of the following descriptions best fits Delta Lake?\n\n \n\nA Spark API for exporting data from a relational database into CSV files.\n\nA relational storage layer for Spark that supports tables based on Parquet files.\n\nA synchronization solution that replicates data between SQL pools and Spark pools.\n\n2. \n\nYou've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?\n\n \n\nCSV\n\nPARQUET\n\nDELTA\n\n3. \n\nWhat feature of Delta Lake enables you to retrieve data from previous versions of a table?\n\n \n\nSpark Structured Streaming\n\nTime Travel\n\nCatalog Tables\n\n4. \n\nYou have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?\n\n \n\nThe table metadata and data files will be deleted.\n\nThe table metadata will be removed from the catalog, but the data files will remain intact.\n\nThe table metadata will remain in the catalog, but the data files will be deleted.\n\n5. \n\nWhen using Spark Structured Streaming, a Delta Lake table can be which of the following?\n\n \n\nOnly a source\n\nOnly a sink\n\nEither a source or a sink\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Use Delta Lake in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/7-exercise-use-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Use Delta Lake in Azure Synapse Analytics\n40 minutes\n\nNow it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark pool in Azure Synapse Analytics to create and query Delta Lake tables, and query Delta Lake data from a serverless SQL pool.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake with streaming data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/5-use-delta-lake-streaming-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake with streaming data\n6 minutes\n\nAll of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n\nSpark Structured Streaming\n\nA typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.\n\nSpark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n\n Tip\n\nFor more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation.\n\nStreaming with Delta Lake tables\n\nYou can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.\n\nUsing a Delta Lake table as a streaming source\n\nIn the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"/delta/internetorders\")\n\n# Now you can process the streaming data in the dataframe\n# for example, show it:\nstream_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .start()\n\n\n Note\n\nWhen using a Delta Lake table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option.\n\nAfter reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n\nUsing a Delta Lake table as a streaming sink\n\nIn the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format {\"device\":\"Dev1\",\"status\":\"ok\"} New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\ninputPath = '/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = '/delta/devicetable'\ncheckpoint_path = '/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n\n\n Note\n\nThe checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.\n\nAfter the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:\n\n%%sql\n\nCREATE TABLE DeviceTable\nUSING DELTA\nLOCATION '/delta/devicetable';\n\nSELECT device, status\nFROM DeviceTable;\n\n\nTo stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:\n\ndelta_stream.stop()\n\n\n Tip\n\nFor more information about using Delta Lake tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in a SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/6-delta-with-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Delta Lake in a SQL pool\n5 minutes\n\nDelta Lake is designed as a transactional, relational storage layer for Apache Spark; including Spark pools in Azure Synapse Analytics. However, Azure Synapse Analytics also includes a serverless SQL pool runtime that enables data analysts and engineers to run SQL queries against data in a data lake or a relational database.\n\n Note\n\nYou can only query data from Delta Lake tables in a serverless SQL pool; you can't update, insert, or delete data.\n\nQuerying delta formatted files with OPENROWSET\n\nThe serverless SQL pool in Azure Synapse Analytics includes support for reading delta format files; enabling you to use the SQL pool to query Delta Lake tables. This approach can be useful in scenarios where you want to use Spark and Delta tables to process large quantities of data, but use the SQL pool to run queries for reporting and analysis of the processed data.\n\nIn the following example, a SQL SELECT query reads delta format data using the OPENROWSET function.\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK 'https://mystore.dfs.core.windows.net/files/delta/mytable/',\n        FORMAT = 'DELTA'\n    ) AS deltadata\n\n\nYou could run this query in a serverless SQL pool to retrieve the latest data from the Delta Lake table stored in the specified file location.\n\nYou could also create a database and add a data source that encapsulates the location of your Delta Lake data files, as shown in this example:\n\nCREATE DATABASE MyDB\n      COLLATE Latin1_General_100_BIN2_UTF8;\nGO;\n\nUSE MyDB;\nGO\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStore\nWITH\n(\n    LOCATION = 'https://mystore.dfs.core.windows.net/files/delta/'\n);\nGO\n\nSELECT TOP 10 *\nFROM OPENROWSET(\n        BULK 'mytable',\n        DATA_SOURCE = 'DeltaLakeStore',\n        FORMAT = 'DELTA'\n    ) as deltadata;\n\n\n Note\n\nWhen working with Delta Lake data, which is stored in Parquet format, it's generally best to create a database with a UTF-8 based collation in order to ensure string compatibility.\n\nQuerying catalog tables\n\nThe serverless SQL pool in Azure Synapse Analytics has shared access to databases in the Spark metastore, so you can query catalog tables that were created using Spark SQL. In the following example, a SQL query in a serverless SQL pool queries a catalog table that contains Delta Lake data:\n\n-- By default, Spark catalog tables are created in a database named \"default\"\n-- If you created another database using Spark SQL, you can use it here\nUSE default;\n\nSELECT * FROM MyDeltaTable;\n\n\n Tip\n\nFor more information about using Delta Tables from a serverless SQL pool, see Query Delta Lake files using serverless SQL pool in Azure Synapse Analytics in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create catalog tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/4-catalog-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate catalog tables\n6 minutes\n\nSo far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark pool, and work with them using SQL.\n\nExternal vs managed tables\n\nTables in a Spark catalog, including Delta Lake tables, can be managed or external; and it's important to understand the distinction between these kinds of table.\n\nA managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.\nAn external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.\nCreating catalog tables\n\nThere are several ways to create catalog tables.\n\nCreating a catalog table from a dataframe\n\nYou can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:\n\n# Save a dataframe as a managed table\ndf.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n\n## specify a path option to save as an external table\ndf.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n\nCreating a catalog table using SQL\n\nYou can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables. You can run the statement using the SparkSQL API, like the following example:\n\nspark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n\n\nAlternatively you can use the native SQL support in Spark to run the statement:\n\n%%sql\n\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION '/mydata'\n\n\n Tip\n\nThe CREATE TABLE statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a CREATE TABLE IF NOT EXISTS statement or the CREATE OR REPLACE TABLE statement.\n\nDefining the table schema\n\nIn all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:\n\n%%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n\n\nWhen using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.\n\nUsing the DeltaTableBuilder API\n\nYou can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:\n\nfrom delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"default.ManagedProducts\") \\\n  .addColumn(\"Productid\", \"INT\") \\\n  .addColumn(\"ProductName\", \"STRING\") \\\n  .addColumn(\"Category\", \"STRING\") \\\n  .addColumn(\"Price\", \"FLOAT\") \\\n  .execute()\n\n\nSimilarly to the CREATE TABLE SQL statement, the create method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the createIfNotExists or createOrReplace method.\n\nUsing catalog tables\n\nYou can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:\n\n%%sql\n\nSELECT orderid, salestotal\nFROM ManagedSalesOrders\n\n\n Tip\n\nFor more information about working with Delta Lake, see Table batch reads and writes in the Delta Lake documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create Delta Lake tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/3-create-delta-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate Delta Lake tables\n5 minutes\n\nDelta lake is built on tables, which provide a relational storage abstraction over files in a data lake.\n\nCreating a Delta Lake table from a dataframe\n\nOne of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored.\n\nFor example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:\n\n# Load a file into a dataframe\ndf = spark.read.load('/data/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndelta_table_path = \"/delta/mydata\"\ndf.write.format(\"delta\").save(delta_table_path)\n\n\nAfter saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a _delta_log folder containing the transaction log for the table.\n\n Note\n\nThe transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.\n\nYou can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:\n\nnew_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n\n\nYou can also add rows from a dataframe to an existing table by using the append mode:\n\nnew_rows_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n\nMaking conditional updates\n\nWhile you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations. For example, you could use the following code to update the price column for all rows with a category column value of \"Accessories\":\n\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n\n\nThe data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.\n\n Tip\n\nFor more information about using the Delta Lake API, see the Delta Lake API documentation.\n\nQuerying a previous version of a table\n\nDelta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.\n\nYou can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n\n\nAlternatively, you can specify a timestamp by using the timestampAsOf option:\n\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nLinux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\n\n Note\n\nThe version of Delta Lake available in an Azure Synapse Analytics pool depends on the version of Spark specified in the pool configuration. The information in this module reflects Delta Lake version 1.0, which is installed with Spark 3.1.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Delta Lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/2-understand-delta-lake",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Delta Lake\n5 minutes\n\nDelta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.\n\nThe benefits of using Delta Lake in a Synapse Analytics Spark pool include:\n\nRelational tables that support querying and data modification. With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.\nSupport for ACID transactions. Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\nData versioning and time travel. Because all transactions are logged in the transaction log, you can track multiple versions of each table row and even use the time travel feature to retrieve a previous version of a row in a query.\nSupport for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.\nStandard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the serverless SQL pool in Azure Synapse Analytics to query Delta Lake tables in SQL.\n\n Tip\n\nFor more information about Delta Lake in Azure Synapse Analytics, see What is Delta Lake in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Delta Lake in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Delta Lake in Azure Synapse Analytics\nModule\n9 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nDelta Lake is an open source relational storage area for Spark that you can use to implement a data lakehouse architecture in Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDescribe core features and capabilities of Delta Lake.\nCreate and use Delta Lake tables in a Synapse Analytics Spark pool.\nCreate Spark catalog tables for Delta Lake data.\nUse Delta Lake tables for streaming data.\nQuery Delta Lake tables from a Synapse Analytics SQL pool.\nAdd\nPrerequisites\n\nTo get the best from this module, you will need existing knowledge of working with Spark pools in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first.\n\nIntroduction\nmin\nUnderstand Delta Lake\nmin\nCreate Delta Lake tables\nmin\nCreate catalog tables\nmin\nUse Delta Lake with streaming data\nmin\nUse Delta Lake in a SQL pool\nmin\nExercise - Use Delta Lake in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nIn this module, you've learned how to use Apache Spark to transform data in Azure Synapse Analytics. Using Spark for complex data transformations is a common technique because of the inherent scalability of the Spark platform. You can use code in notebooks to experiment with data transformations, and then include those notebooks in automated pipelines as part of a data integration solution.\n\n Tip\n\nLearn more about using the Spark SQL and DataFrames Guide.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich method of the Dataframe object is used to save a dataframe as a file?\n\n \n\ntoFile()\n\nwrite()\n\nsave()\n\n2. \n\nWhich method is used to split the data across folders when saving a dataframe?\n\n \n\nsplitBy()\n\ndistributeBy()\n\npartitionBy()\n\n3. \n\nWhat happens if you drop an external table that is based on existing files?\n\n \n\nAn error – you must delete the files first\n\nThe table is dropped from the metastore but the files remain unaffected\n\nThe table is dropped from the metastore and the files are deleted\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise: Transform data with Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/6-exercise-transform-data-spark-azure-synapse-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise: Transform data with Spark in Azure Synapse Analytics\n30 minutes\n\nNow it's your chance to use Spark to transform data for yourself. In this exercise, you’ll use a Spark notebook in Azure Synapse Analytics to transform data in files.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data with SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/4-tramsform-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTransform data with SQL\n5 minutes\n\nThe SparkSQL library, which provides the dataframe structure also enables you to use SQL as a way of working with data. With this approach, You can query and transform data in dataframes by using SQL queries, and persist the results as tables.\n\n Note\n\nTables are metadata abstractions over files. The data is not stored in a relational table, but the table provides a relational layer over files in the data lake.\n\nDefine tables and views\n\nTable definitions in Spark are stored in the metastore, a metadata layer that encapsulates relational abstractions over files. External tables are relational tables in the metastore that reference files in a data lake location that you specify. You can access this data by querying the table or by reading the files directly from the data lake.\n\n Note\n\nExternal tables are \"loosely bound\" to the underlying files and deleting the table does not delete the files. This allows you to use Spark to do the heavy lifting of transformation then persist the data in the lake. After this is done you can drop the table and downstream processes can access these optimized structures. You can also define managed tables, for which the underlying data files are stored in an internally managed storage location associated with the metastore. Managed tables are \"tightly-bound\" to the files, and dropping a managed table deletes the associated files.\n\nThe following code example saves a dataframe (loaded from CSV files) as an external table name sales_orders. The files are stored in the /sales_orders_table folder in the data lake.\n\norder_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')\n\nUse SQL to query and transform the data\n\nAfter defining a table, you can use of SQL to query and transform its data. The following code creates two new derived columns named Year and Month and then creates a new table transformed_orders with the new derived columns added.\n\n# Create derived columns\nsql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\n\n# Save the results\nsql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')\n\n\nThe data files for the new table are stored in a hierarchy of folders with the format of Year=*NNNN* / Month=*N*, with each folder containing a parquet file for the corresponding orders by year and month.\n\nQuery the metastore\n\nBecause this new table was created in the metastore, you can use SQL to query it directly with the %%sql magic key in the first line to indicate that the SQL syntax will be used as shown in the following script:\n\n%%sql\n\nSELECT * FROM transformed_orders\nWHERE Year = 2021\n    AND Month = 1\n\nDrop tables\n\nWhen working with external tables, you can use the DROP command to delete the table definitions from the metastore without affecting the files in the data lake. This approach enables you to clean up the metastore after using SQL to transform the data, while making the transformed data files available to downstream data analysis and ingestion processes.\n\n%%sql\n\nDROP TABLE transformed_orders;\nDROP TABLE sales_orders;\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Partition data files - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/3-partition-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPartition data files\n5 minutes\n\nPartitioning is an optimization technique that enables spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO.\n\nPartition the output file\n\nTo save a dataframe as a partitioned set of files, use the partitionBy method when writing the data.\n\nThe following example creates a derived Year field. Then uses it to partition the data.\n\nfrom pyspark.sql.functions import year, col\n\n# Load source data\ndf = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n\n# Add Year column\ndated_df = df.withColumn(\"Year\", year(col(\"OrderDate\")))\n\n# Partition by year\ndated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\")\n\n\nThe folder names generated when partitioning a dataframe include the partitioning column name and value in a column=value format, as shown here:\n\n Note\n\nYou can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you could partition the order in the example by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value.\n\nFilter parquet files in a query\n\nWhen reading data from parquet files into a dataframe, you have the ability to pull data from any folder within the hierarchical folders. This filtering process is done with the use of explicit values and wildcards against the partitioned fields.\n\nIn the following example, the following code will pull the sales orders, which were placed in 2020.\n\norders_2020 = spark.read.parquet('/partitioned_data/Year=2020')\ndisplay(orders_2020.limit(5))\n\n\n Note\n\nThe partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a Year column - all rows would be from 2020.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nApache Spark provides a powerful platform for performing data cleansing and transformation tasks on large volumes of data. By using the Spark dataframe object, you can easily load data from files in a data lake and perform complex modifications. You can then save the transformed data back to the data lake for downstream processing or ingestion into a data warehouse.\n\nAzure Synapse Analytics provides Apache Spark pools that you can use to run Spark workloads to transform data as part of a data ingestion and preparation workload. You can use natively supported notebooks to write and run code on a Spark pool to prepare data for analysis. You can then use other Azure Synapse Analytics capabilities such as SQL pools to work with the transformed data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Modify and save dataframes - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/2-transform-dataframe",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nModify and save dataframes\n5 minutes\n\nApache Spark provides the dataframe object as the primary structure for working with data. You can use dataframes to query and transform data, and persist the results in a data lake. To load data into a dataframe, you use the spark.read function, specifying the file format, path, and optionally the schema of the data to be read. For example, the following code loads data from all .csv files in the orders folder into a dataframe named order_details and then displays the first five records.\n\norder_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\ndisplay(order_details.limit(5))\n\nTransform the data structure\n\nAfter loading the source data into a dataframe, you can use the dataframe object's methods and Spark functions to transform it. Typical operations on a dataframe include:\n\nFiltering rows and columns\nRenaming columns\nCreating new columns, often derived from existing ones\nReplacing null or other values\n\nIn the following example, the code uses the split function to separate the values in the CustomerName column into two new columns named FirstName and LastName. Then it uses the drop method to delete the original CustomerName column.\n\nfrom pyspark.sql.functions import split, col\n\n# Create the new FirstName and LastName fields\ntransformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Remove the CustomerName field\ntransformed_df = transformed_df.drop(\"CustomerName\")\n\ndisplay(transformed_df.limit(5))\n\n\nYou can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\n\nSave the transformed data\n\nAfter your dataFrame is in the required structure, you can save the results to a supported format in your data lake.\n\nThe following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name.\n\ntransformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\nprint (\"Transformed data saved!\")\n\n\n\n Note\n\nThe Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data with Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/transform-data-spark-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nTransform data with Spark in Azure Synapse Analytics\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nData engineers commonly need to transform large volumes of data. Apache Spark pools in Azure Synapse Analytics provide a distributed processing platform that they can use to accomplish this goal.\n\nLearning objectives\n\nIn this module, you will learn how to:\n\nUse Apache Spark to modify and save dataframes\nPartition data files for improved performance and scalability.\nTransform data with SQL\nAdd\nPrerequisites\n\nBefore taking this module, you should be familiar with Apache Spark pools in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first.\n\nIntroduction\nmin\nModify and save dataframes\nmin\nPartition data files\nmin\nTransform data with SQL\nmin\nExercise: Transform data with Spark in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark is a key technology used in big data analytics, and the Spark pool support in Azure Synapse Analytics enables you to combine big data processing in Spark with large-scale data warehousing in SQL.\n\nIn this module, you learned how to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition best describes Apache Spark?\n\n \n\nA highly scalable relational database management system.\n\nA virtual server with a Python runtime.\n\nA distributed platform for parallel data processing using multiple languages.\n\n2. \n\nYou need to use Spark to analyze data in a parquet file. What should you do?\n\n \n\nLoad the parquet file into a dataframe.\n\nImport the data into a table in a serverless SQL pool.\n\nConvert the data to CSV format.\n\n3. \n\nYou want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?\n\n \n\n%%spark\n\n%%pyspark\n\n%%sql\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/6-exercise-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Analyze data with Apache Spark in Azure Synapse Analytics \nAdd\nPrevious\nUnit 6 of 8\nNext\nExercise - Analyze data with Spark\nCompleted\n100 XP\n45 minutes\n\nNow it's your opportunity to use a Spark pool in Azure Synapse Analytics. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a Spark pool to analyze and visualize data from files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNext unit: Knowledge check\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/5-visualize-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Analyze data with Apache Spark in Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 8\nNext\nVisualize data with Spark\nCompleted\n100 XP\n5 minutes\n\nOne of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Synapse Analytics provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.\n\nUsing built-in notebook charts\n\nWhen you display a dataframe or run a SQL query in a Spark notebook in Azure Synapse Analytics, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here:\n\nThe built-in charting functionality in notebooks is useful when you're working with results of a query that don't include any existing groupings or aggregations, and you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.\n\nUsing graphics packages in code\n\nThere are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n\nFor example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.\n\nPython\nCopy\nfrom matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n\n\nThe Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n\nThe chart produced by the code would look similar to the following image:\n\nYou can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.\n\nNext unit: Exercise - Analyze data with Spark\n\nContinue\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/4-write-spark-code",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nAnalyze data with Spark\n5 minutes\n\nOne of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Synapse Analytics Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java and Microsoft .NET.\n\nExploring data with dataframes\n\nNatively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.\n\n Note\n\nIn addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.\n\nLoading data into a dataframe\n\nLet's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the primary storage account for an Azure Synapse Analytics workspace:\n\nProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nIn a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:\n\n%%pyspark\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n\n\nThe %%pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example:\n\n%%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://container@store.dfs.core.windows.net/products.csv\")\ndisplay(df.limit(10))\n\n\nThe magic %%spark is used to specify Scala.\n\nBoth of these code samples would produce output like this:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nSpecifying a dataframe schema\n\nIn the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nThe following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n\n\nThe results would once again be similar to:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nFiltering and grouping dataframes\n\nYou can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:\n\npricelist_df = df.select(\"ProductID\", \"ListPrice\")\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductID\tListPrice\n771\t3399.9900\n772\t3399.9900\n773\t3399.9900\n...\t...\n\nIn common with most data manipulation methods, select returns a new dataframe object.\n\n Tip\n\nSelecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n\npricelist_df = df[\"ProductID\", \"ListPrice\"]\n\nYou can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n\nbikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\n\nTo group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:\n\ncounts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nCategory\tcount\nHeadsets\t3\nWheels\t14\nMountain Bikes\t32\n...\t...\nUsing SQL expressions in Spark\n\nThe Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\n\nCreating database objects in the Spark catalog\n\nThe Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\n\nOne of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:\n\ndf.createOrReplaceTempView(\"products\")\n\n\nA view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.\n\n Note\n\nWe won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:\n\nYou can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.\nYou can save a dataframe as a table by using its saveAsTable method.\nYou can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.\nUsing the Spark SQL API to query data\n\nYou can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.\n\nbikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n\n\nThe results from the code example would look similar to the following table:\n\nExpand table\nProductID\tProductName\tListPrice\n38\tMountain-100 Silver, 38\t3399.9900\n52\tRoad-750 Black, 52\t539.9900\n...\t...\t...\nUsing SQL code\n\nThe previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this:\n\n%%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n\n\nThe SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:\n\nExpand table\nCategory\tProductCount\nBib-Shorts\t3\nBike Racks\t1\nBike Stands\t1\n...\t...\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/3-use-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark in Azure Synapse Analytics\n3 minutes\n\nYou can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:\n\nBatch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.\nInteractive analytics sessions to explore, analyze, and visualize data.\nRunning Spark code in notebooks\n\nAzure Synapse Studio includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Synapse Studio is similar to that of Jupyter notebooks - a popular open source notebook platform.\n\n Note\n\nWhile usually used interactively, notebooks can be included in automated pipelines and run as an unattended script.\n\nNotebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:\n\nSyntax highlighting and error support.\nCode auto-completion​.\nInteractive data visualizations.\nThe ability to export results.\n\n Tip\n\nTo learn more about working with notebooks in Azure Synapse Analytics, see the Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nAccessing data from a Synapse Spark pool\n\nYou can use Spark in Azure Synapse Analytics to work with data from various sources, including:\n\nA data lake based on the primary storage account for the Azure Synapse Analytics workspace.\nA data lake based on storage defined as a linked service in the workspace.\nA dedicated or serverless SQL pool in the workspace.\nAn Azure SQL or SQL Server database (using the Spark connector for SQL Server)\nAn Azure Cosmos DB analytical database defined as a linked service and configured using Azure Synapse Link for Cosmos DB.\nAn Azure Data Explorer Kusto database defined as a linked service in the workspace.\nAn external Hive metastore defined as a linked service in the workspace.\n\nOne of the most common uses of Spark is to work with data in a data lake, where you can read and write files in multiple commonly used formats, including delimited text, Parquet, Avro, and others.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get to know Apache Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/2-get-to-know-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet to know Apache Spark\n3 minutes\n\nApache Spark is distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster.\n\nHow Spark works\n\nApache Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). The SparkContext connects to the cluster manager, which allocates resources across applications using an implementation of Apache Hadoop YARN. Once connected, Spark acquires executors on nodes in the cluster to run your application code.\n\nThe SparkContext runs the main function and parallel operations on the cluster nodes, and then collects the results of the operations. The nodes read and write data from and to the file system and cache transformed data in-memory as Resilient Distributed Datasets (RDDs).\n\nThe SparkContext is responsible for converting an application to a directed acyclic graph (DAG). The graph consists of individual tasks that get executed within an executor process on the nodes. Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.\n\nSpark pools in Azure Synapse Analytics\n\nIn Azure Synapse Analytics, a cluster is implemented as a Spark pool, which provides a runtime for Spark operations. You can create one or more Spark pools in an Azure Synapse Analytics workspace by using the Azure portal, or in Azure Synapse Studio. When defining a Spark pool, you can specify configuration options for the pool, including:\n\nA name for the spark pool.\nThe size of virtual machine (VM) used for the nodes in the pool, including the option to use hardware accelerated GPU-enabled nodes.\nThe number of nodes in the pool, and whether the pool size is fixed or individual nodes can be brought online dynamically to auto-scale the cluster; in which case, you can specify the minimum and maximum number of active nodes.\nThe version of the Spark Runtime to be used in the pool; which dictates the versions of individual components such as Python, Java, and others that get installed.\n\n Tip\n\nFor more information about Spark pool configuration options, see Apache Spark pool configurations in Azure Synapse Analytics in the Azure Synapse Analytics documentation.\n\nSpark pools in an Azure Synapse Analytics Workspace are serverless - they start on-demand and stop when idle.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nApache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become extremely popular in \"big data\" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Databricks, and Azure Synapse Analytics.\n\nThis module explores how you can use Spark in Azure Synapse Analytics to ingest, process, and analyze data from a data lake. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other Synapse analytical runtimes are specific to Azure Synapse Analytics.\n\nAfter completing this module, you'll be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Apache Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAnalyze data with Apache Spark in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nApache Spark is a core technology for large-scale data analytics. Learn how to use Spark in Azure Synapse Analytics to analyze and visualize data in a data lake.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\nAdd\nPrerequisites\n\nIf you are not already familiar with Azure Synapse Analytics, consider completing the Introduction to Azure Synapse Analytics module before starting this module.\n\nIntroduction\nmin\nGet to know Apache Spark\nmin\nUse Spark in Azure Synapse Analytics\nmin\nAnalyze data with Spark\nmin\nVisualize data with Spark\nmin\nExercise - Analyze data with Spark\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this lesson, you will learn how you can set up security when using Azure Synapse serverless SQL pools by:\n\nChoosing an authentication method in Azure Synapse serverless SQL pools\nManaging users in Azure Synapse serverless SQL pools\nManaging user permissions in Azure Synapse serverless SQL pools\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich authentication method would be the likeliest choice to use for an individual who needs to access your serverless SQL pool who works for an external organization?\n\n \n\nLocal authentication.\n\nSQL Authentication.\n\nMicrosoft Entra ID.\n\n2. \n\nWhich Azure Synapse Studio hub is where you assign administrator privileges to an Azure Synapse workspace?\n\n \n\nManage.\n\nData.\n\nDevelop.\n\n3. \n\nWhich role enables a user to create external table as select (CETAS) against an Azure Data Lake Gen2 data store?\n\n \n\nStorage Blob Data Reader.\n\nStorage Blob Data Contributor.\n\nExecutor.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage users in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/3-manage-users",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage users in Azure Synapse serverless SQL pools\n3 minutes\n\nYou can give administrator privileges to a user to Azure Synapse serverless SQL pool. To do this you should open the Azure Synapse workspace and do the following steps:\n\nGo to Manage menu\n\nGo to Access control\n\nClick on Add\n\nChoose Synapse Administrator\n\nSelect a User or Security group (a security group is the recommended option here)\n\nClick Apply\n\nNow this user or group is the administrator of the Azure Synapse workspace and serverless SQL pool.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage user permissions in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/4-manage-user-permissions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage user permissions in Azure Synapse serverless SQL pools\n3 minutes\n\nTo secure data, Azure Storage implements an access control model that supports both Azure role-based access control (Azure RBAC) and access control lists (ACLs) like Portable Operating System Interface for Unix (POSIX)\n\nYou can associate a security principal with an access level for files and directories. These associations are captured in an access control list (ACL). Each file and directory in your storage account has an access control list. When a security principal attempts an operation on a file or directory, an ACL check determines whether that security principal (user, group, service principal, or managed identity) has the correct permission level to perform the operation.\n\nThere are two kinds of access control lists:\n\nAccess ACLs\n\nControls access to an object. Files and directories both have access ACLs.\n\nDefault ACLs\n\nAre templates of ACLs associated with a directory that determine the access ACLs for any child items that are created under that directory. Files do not have default ACLs.\n\nBoth access ACLs and default ACLs have the same structure.\n\nThe permissions on a container object are Read, Write, and Execute, and they can be used on files and directories as shown in the following table:\n\nLevels of permissions\n\nExpand table\nPermission\tFile\tDirectory\nRead (R)\tCan read the contents of a file\tRequires Read and Execute to list the contents of the directory\nWrite (W)\tCan write or append to a file\tRequires Write and Execute to create child items in a directory\nExecute (X)\tDoes not mean anything in the context of Data Lake Storage Gen2\tRequired to traverse the child items of a directory\nGuidelines in setting up ACLs\n\nAlways use Microsoft Entra security groups as the assigned principal in an ACL entry. Resist the opportunity to directly assign individual users or service principals. Using this structure will allow you to add and remove users or service principals without the need to reapply ACLs to an entire directory structure. Instead, you can just add or remove users and service principals from the appropriate Microsoft Entra security group.\n\nThere are many ways to set up groups. For example, imagine that you have a directory named /LogData which holds log data that is generated by your server. Azure Data Factory (ADF) ingests data into that folder. Specific users from the service engineering team will upload logs and manage other users of this folder, and various Databricks clusters will analyze logs from that folder.\n\nTo enable these activities, you could create a LogsWriter group and a LogsReader group. Then, you could assign permissions as follows:\n\nAdd the LogsWriter group to the ACL of the /LogData directory with rwx permissions.\nAdd the LogsReader group to the ACL of the /LogData directory with r-x permissions.\nAdd the service principal object or Managed Service Identity (MSI) for ADF to the LogsWriters group.\nAdd users in the service engineering team to the LogsWriter group.\nAdd the service principal object or MSI for Databricks to the LogsReader group.\n\nIf a user in the service engineering team leaves the company, you could just remove them from the LogsWriter group. If you did not add that user to a group, but instead, you added a dedicated ACL entry for that user, you would have to remove that ACL entry from the /LogData directory. You would also have to remove the entry from all subdirectories and files in the entire directory hierarchy of the /LogData directory.\n\nRoles necessary for serverless SQL pool users\n\nFor users which need read only access you should assign role named Storage Blob Data Reader.\n\nFor users which need read/write access you should assign role named Storage Blob Data Contributor. Read/Write access is needed if user should have access to create external table as select (CETAS).\n\n Note\n\nIf user has a role Owner or Contributor, that role is not enough. Azure Data Lake Storage gen 2 has super-roles which should be assigned.\n\nDatabase level permission\n\nTo provide more granular access to the user, you should use Transact-SQL syntax to create logins and users.\n\nTo grant access to a user to a single serverless SQL pool database, follow the steps in this example:\n\nCreate LOGIN\n\nuse master\nCREATE LOGIN [alias@domain.com] FROM EXTERNAL PROVIDER;\n\n\nCreate USER\n\nuse yourdb -- Use your DB name\nCREATE USER alias FROM LOGIN [alias@domain.com];\n\n\nAdd USER to members of the specified role\n\nuse yourdb -- Use your DB name\nalter role db_datareader \nAdd member alias -- Type USER name from step 2\n-- You can use any Database Role which exists \n-- (examples: db_owner, db_datareader, db_datawriter)\n-- Replace alias with alias of the user you would like to give access and domain with the company domain you are using.\n\nServer level permission\n\nTo grant full access to a user to all serverless SQL pool databases, follow the step in this example:\n\nCREATE LOGIN [alias@domain.com] FROM EXTERNAL PROVIDER;\nALTER SERVER ROLE sysadmin ADD MEMBER [alias@domain.com];\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Choose an authentication method in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/2-choose-authentication-method",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nChoose an authentication method in Azure Synapse serverless SQL pools\n3 minutes\n\nServerless SQL pool authentication refers to how users prove their identity when connecting to the endpoint. Two types of authentication are supported:\n\nSQL Authentication\n\nThis authentication method uses a username and password.\n\nMicrosoft Entra authentication\n\nThis authentication method uses identities managed by Microsoft Entra ID. For Microsoft Entra users, multi-factor authentication can be enabled. Use Active Directory authentication (integrated security) whenever possible.\n\nAuthorization\n\nAuthorization refers to what a user can do within a serverless SQL pool database and is controlled by your user account's database role memberships and object-level permissions.\n\nIf SQL Authentication is used, the SQL user exists only in the serverless SQL pool and permissions are scoped to the objects in the serverless SQL pool. Access to securable objects in other services (such as Azure Storage) can't be granted to a SQL user directly since it only exists in scope of serverless SQL pool. The SQL user needs get authorization to access the files in the storage account.\n\nIf Microsoft Entra authentication is used, a user can sign in to a serverless SQL pool and other services, like Azure Storage, and can grant permissions to the Microsoft Entra user.\n\nAccess to storage accounts\n\nA user that is logged into the serverless SQL pool service must be authorized to access and query the files in Azure Storage. Serverless SQL pool supports the following authorization types:\n\nAnonymous access\n\nTo access publicly available files placed on Azure storage accounts that allow anonymous access.\n\nShared access signature (SAS)\n\nProvides delegated access to resources in storage account. With a SAS, you can grant clients access to resources in storage account, without sharing account keys. A SAS gives you granular control over the type of access you grant to clients who have the SAS: validity interval, granted permissions, acceptable IP address range, acceptable protocol (https/http).\n\nManaged Identity.\n\nIs a feature of Microsoft Entra ID that provides Azure services for serverless SQL pool. Also, it deploys an automatically managed identity in Microsoft Entra ID. This identity can be used to authorize the request for data access in Azure Storage. Before accessing the data, the Azure Storage administrator must grant permissions to Managed Identity for accessing the data. Granting permissions to Managed Identity is done the same way as granting permission to any other Microsoft Entra user.\n\nUser Identity\n\nAlso known as \"pass-through\", is an authorization type where the identity of the Microsoft Entra user that logged into serverless SQL pool is used to authorize access to the data. Before accessing the data, Azure Storage administrator must grant permissions to Microsoft Entra user for accessing the data. This authorization type uses the Microsoft Entra user that logged into serverless SQL pool, therefore it's not supported for SQL user types.\n\nSupported authorization types for database users can be found in the table below:\n\nExpand table\nAuthorization type\tSQL user\tMicrosoft Entra user\nUser Identity\tNot supported\tSupported\nSAS\tSupported\tSupported\nManaged Identity\tNot supported\tSupported\n\nSupported storage and authorization types can be found in the table below:\n\nExpand table\nAuthorization type\tBlob Storage\tADLS Gen1\tADLS Gen2\nUser Identity\tSupported - SAS token can be used to access storage that is not protected with firewall\tNot supported\tSupported - SAS token can be used to access storage that is not protected with firewall\nSAS\tSupported\tSupported\tSupported\nManaged Identity\tSupported\tSupported\tSupported\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this lesson, you will learn how you can set up security when using Azure Synapse serverless SQL pools\n\nAfter the completion of this lesson, you will be able to:\n\nChoose an authentication method in Azure Synapse serverless SQL pools\nManage users in Azure Synapse serverless SQL pools\nManage user permissions in Azure Synapse serverless SQL pools\nPrerequisites\n\nBefore taking this lesson, it is recommended that the student is able to:\n\nLog into the Azure portal\nExplain the different components of Azure Synapse Analytics\nUse Azure Synapse Studio\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Secure data and manage users in Azure Synapse serverless SQL pools - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nSecure data and manage users in Azure Synapse serverless SQL pools\nModule\n6 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how you can set up security when using Azure Synapse serverless SQL pools\n\nLearning objectives\n\nAfter the completion of this module, you will be able to:\n\nChoose an authentication method in Azure Synapse serverless SQL pools\nManage users in Azure Synapse serverless SQL pools\nManage user permissions in Azure Synapse serverless SQL pools\nAdd\nPrerequisites\nIt is recommended that students have completed Data Fundamentals before starting this learning path.\nIntroduction\nmin\nChoose an authentication method in Azure Synapse serverless SQL pools\nmin\nManage users in Azure Synapse serverless SQL pools\nmin\nManage user permissions in Azure Synapse serverless SQL pools\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nA lake database can provide the benefits of a relational schema and query interface with the flexibility of file storage in a data lake.\n\nIn this module, you learned how to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nLearn more\n\nTo learn more about lake databases, refer to the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhich if the following statements is true of a lake database?\n\n \n\nData is stored in a relational database store and cannot be directly accessed in the data lake files.\n\nData is stored in files that cannot be queried using SQL.\n\nA relational schema is overlaid on the underlying files, and can be queried using a serverless SQL pool or a Spark pool.\n\n2. \n\nYou need to create a new lake database for a retail solution. What's the most efficient way to do this?\n\n \n\nCreate a sample database in Azure SQL Database and export the SQL scripts to create the schema for the lake database.\n\nStart with the Retail database template in Azure Synapse Studio, and adapt it as necessary.\n\nStart with an empty database and create a normalized schema.\n\n3. \n\nYou have Parquet files in an existing data lake folder for which you want to create a table in a lake database. What should you do?\n\n \n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) query to create the table.\n\nConvert the files in the folder to CSV format.\n\nUse the database designer to create a table based on the existing folder.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Analyze data in a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/6-exercise-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Analyze data in a lake database\n45 minutes\n\nNow it's your opportunity to create and use a lake database. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then create a lake database in Azure Synapse Studio.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/5-use-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse a lake database\n3 minutes\n\nAfter creating a lake database, you can store data files that match the table schemas in the appropriate folders in the data lake, and query them using SQL.\n\nUsing a serverless SQL pool\n\nYou can query a lake database in a SQL script by using a serverless SQL pool.\n\nFor example, suppose a lake database named RetailDB contains an Customer table. You could query it using a standard SELECT statement like this:\n\nUSE RetailDB;\nGO\n\nSELECT CustomerID, FirstName, LastName\nFROM Customer\nORDER BY LastName;\n\n\nThere is no need to use an OPENROWSET function or include any additional code to access the data from the underlying file storage. The serverless SQL pool handles the mapping to the files for you.\n\nUsing an Apache Spark pool\n\nIn addition to using a serverless SQL pool, you can work with lake database tables using Spark SQL in an Apache Spark pool.\n\nFor example, you could use the following code to insert a new customer record into the Customer table.\n\n%%sql\nINSERT INTO `RetailDB`.`Customer` VALUES (123, 'John', 'Yang')\n\n\nYou could then use the following code to query the table:\n\n%%sql\nSELECT * FROM `RetailDB`.`Customer` WHERE CustomerID = 123\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a lake database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/4-create-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate a lake database\n3 minutes\n\nYou can create a lake database using the lake database designer in Azure Synapse Studio. Start by adding a new lake database on the Data page, selecting a template from the gallery or starting with a blank lake database; and then add and customize tables using the visual database designer interface.\n\nAs you create each table, you can specify the type and location of the files you want to use to store the underlying data, or you can create a table from existing files that are already in the data lake. In most cases, it's advisable to store all of the database files in a consistent format within the same root folder in the data lake.\n\nDatabase designer\n\nThe database designer interface in Azure Synapse Studio provides a drag-and-drop surface on which you can edit the tables in your database and the relationships between them.\n\nUsing the database designer, you can define the schema for your database by adding or removing tables and:\n\nSpecifying the name and storage settings for each table.\nSpecifying the names, key usage, nullability, and data types for each column.\nDefining relationships between key columns in tables.\n\nWhen your database schema is ready for use, you can publish the database and start using it.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Explore database templates - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/3-database-templates",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExplore database templates\n3 minutes\n\nYou can create a Lake database from an empty schema, to which you add definitions for tables and the relationships between them. However, Azure Synapse Analytics provides a comprehensive collection of database templates that reflect common schemas found in multiple business scenarios; including:\n\nAgriculture\nAutomotive\nBanking\nConsumer goods\nEnergy and commodity trading\nFreight and logistics\nFund management\nHealthcare insurance\nHealthcare provider\nManufacturing\nRetail\nand many others...\n\nYou can use one of the enterprise database templates as the starting point for creating your lake database, or you can start with a blank schema and add and modify tables from the templates as required.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand lake database concepts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/2-lake-database",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand lake database concepts\n3 minutes\n\nIn a traditional relational database, the database schema is composed of tables, views, and other objects. Tables in a relational database define the entities for which data is stored - for example, a retail database might include tables for products, customers, and orders. Each entity consists of a set of attributes that are defined as columns in the table, and each column has a name and a data type. The data for the tables is stored in the database, and is tightly coupled to the table definition; which enforces data types, nullability, key uniqueness, and referential integrity between related keys. All queries and data manipulations must be performed through the database system.\n\nIn a data lake, there is no fixed schema. Data is stored in files, which may be structured, semi-structured, or unstructured. Applications and data analysts can work directly with the files in the data lake using the tools of their choice; without the constraints of a relational database system.\n\nA lake database provides a relational metadata layer over one or more files in a data lake. You can create a lake database that includes definitions for tables, including column names and data types as well as relationships between primary and foreign key columns. The tables reference files in the data lake, enabling you to apply relational semantics to working with the data and querying it using SQL. However, the storage of the data files is decoupled from the database schema; enabling more flexibility than a relational database system typically offers.\n\nLake database schema\n\nYou can create a lake database in Azure Synapse Analytics, and define the tables that represent the entities for which you need to store data. You can apply proven data modeling principles to create relationships between tables and use appropriate naming conventions for tables, columns, and other database objects.\n\nAzure Synapse Analytics includes a graphical database design interface that you can use to model complex database schema, using many of the same best practices for database design that you would apply to a traditional database.\n\nLake database storage\n\nThe data for the tables in your lake database is stored in the data lake as Parquet or CSV files. The files can be managed independently of the database tables, making it easier to manage data ingestion and manipulation with a wide variety of data processing tools and technologies.\n\nLake database compute\n\nTo query and manipulate the data through the tables you have defined, you can use an Azure Synapse serverless SQL pool to run SQL queries or an Azure Synapse Apache Spark pool to work with the tables using the Spark SQL API.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nData analysts and engineers often find themselves forced to choose between the flexibility of storing data files in a data lake, with the advantages of a structured schema in a relational database. Lake databases in Azure Synapse Analytics provide a way to combine these two approaches and benefit from an explicit relational schema of tables, views, and relationships that is decoupled from file-based storage.\n\nIn this module, you'll learn how to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with the Microsoft Azure portal\nFamiliarity with data lake and data warehouse concepts\nExperience of using SQL to query database tables\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a lake database in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nCreate a lake database in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nWhy choose between working with files in a data lake or a relational database schema? With lake databases in Azure Synapse Analytics, you can combine the benefits of both.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nUnderstand lake database concepts and components\nDescribe database templates in Azure Synapse Analytics\nCreate a lake database\nAdd\nPrerequisites\n\nConsider completing the Explore data analytics in Azure and Get started querying with Transact-SQL learning paths before starting this module. You will need knowledge of:\n\nAnalytical data workloads in Microsoft Azure\nQuerying data with Transact-SQL\nIntroduction\nmin\nUnderstand lake database concepts\nmin\nExplore database templates\nmin\nCreate a lake database\nmin\nUse a lake database\nmin\nExercise - Analyze data in a lake database\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nBy using the CREATE EXTERNAL TABLE AS statement, you can use Azure Synapse serverless SQL pool to transform data as part of a data ingestion pipeline or an extract, transform, and load (ETL) process. The transformed data is persisted in files in the data lake with a relational table based on the file location; enabling you to work with the transformed data using SQL in the serverless SQL database, or directly in the file data lake.\n\nIn this lesson, you learned how to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\n\n Tip\n\nFor more information about using the CETAS statement, see CETAS with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou need to store the results of a query in a serverless SQL pool as files in a data lake. Which SQL statement should you use?\n\n \n\nBULK INSERT\n\nCREATE EXTERNAL TABLE AS SELECT\n\nCOPY\n\n2. \n\nWhich of the following file formats can you use to persist the results of a query?\n\n \n\nCSV only\n\nParquet only.\n\nCSV and Parquet.\n\n3. \n\nYou drop an existing external table from a database in a serverless SQL pool. What else must you do before recreating an external table with the same location?\n\n \n\nDelete the folder containing the data files for dropped table.\n\nDrop and recreate the database.\n\nCreate an Apache Spark pool.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Transform files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4a-exercise-transform-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Transform files using a serverless SQL pool\n30 minutes\n\nNow it's your opportunity to use the CREATE EXTERNAL TABLE AS SELECT statement to transform data. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to transform data from files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Encapsulate data transformations in a stored procedure - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/3-operationalize-data-transformation-using-stored-procedures",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEncapsulate data transformations in a stored procedure\n4 minutes\n\nWhile you can run a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a script whenever you need to transform data, it's good practice to encapsulate the transformation operation in stored procedure. This approach can make it easier to operationalize data transformations by enabling you to supply parameters, retrieve outputs, and include additional logic in a single procedure call.\n\nFor example, the following code creates a stored procedure that drops the external table if it already exists before recreating it with order data for the specified year:\n\nCREATE PROCEDURE usp_special_orders_by_year @order_year INT\nAS\nBEGIN\n\n\t-- Drop the table if it already exists\n\tIF EXISTS (\n                SELECT * FROM sys.external_tables\n                WHERE name = 'SpecialOrders'\n            )\n        DROP EXTERNAL TABLE SpecialOrders\n\n\t-- Create external table with special orders\n\t-- from the specified year\n\tCREATE EXTERNAL TABLE SpecialOrders\n\t\tWITH (\n\t\t\tLOCATION = 'special_orders/',\n\t\t\tDATA_SOURCE = files,\n\t\t\tFILE_FORMAT = ParquetFormat\n\t\t)\n\tAS\n\tSELECT OrderID, CustomerName, OrderTotal\n\tFROM\n\t\tOPENROWSET(\n\t\t\tBULK 'sales_orders/*.csv',\n\t\t\tDATA_SOURCE = 'files',\n\t\t\tFORMAT = 'CSV',\n\t\t\tPARSER_VERSION = '2.0',\n\t\t\tHEADER_ROW = TRUE\n\t\t) AS source_data\n\tWHERE OrderType = 'Special Order'\n\tAND YEAR(OrderDate) = @order_year\nEND\n\n\n Note\n\nAs discussed previously, dropping an existing external table does not delete the folder containing its data files. You must explicitly delete the target folder if it exists before running the stored procedure, or an error will occur.\n\nIn addition to encapsulating Transact-SQL logic, stored procedures also provide the following benefits:\n\nReduces client to server network traffic\n\nThe commands in a procedure are executed as a single batch of code; which can significantly reduce network traffic between the server and client because only the call to execute the procedure is sent across the network.\n\nProvides a security boundary\n\nMultiple users and client programs can perform operations on underlying database objects through a procedure, even if the users and programs don't have direct permissions on those underlying objects. The procedure controls what processes and activities are performed and protects the underlying database objects; eliminating the requirement to grant permissions at the individual object level and simplifies the security layers.\n\nEases maintenance\n\nAny changes in the logic or file system locations involved in the data transformation can be applied only to the stored procedure; without requiring updates to client applications or other calling functions.\n\nImproved performance\n\nStored procedures are compiled the first time they're executed, and the resulting execution plan is held in the cache and reused on subsequent runs of the same stored procedure. As a result, it takes less time to process the procedure.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Include a data transformation stored procedure in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4-pool-stored-procedures-synapse-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nInclude a data transformation stored procedure in a pipeline\n3 minutes\n\nEncapsulating a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a stored procedure makes it easier for you to operationalize data transformations that you may need to perform repeatedly. In Azure Synapse Analytics and Azure Data Factory, you can create pipelines that connect to linked services, including Azure Data Lake Store Gen2 storage accounts that host data lake files, and serverless SQL pools; enabling you to call your stored procedures as part of an overall data extract, transform, and load (ETL) pipeline.\n\nFor example, you can create a pipeline that includes the following activities:\n\nA Delete activity that deletes the target folder for the transformed data in the data lake if it already exists.\nA Stored procedure activity that connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation.\n\nCreating a pipeline for the data transformation enables you to schedule the operation to run at specific times or based on specific events (such as new files being added to the source storage location).\n\n Tip\n\nFor more information about using the Stored procedure activity in a pipeline, see Transform data by using the SQL Server Stored Procedure activity in Azure Data Factory or Synapse Analytics in the Azure Data Factory documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data files with the CREATE EXTERNAL TABLE AS SELECT statement - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTransform data files with the CREATE EXTERNAL TABLE AS SELECT statement\n5 minutes\n\nThe SQL language includes many features and functions that enable you to manipulate data. For example, you can use SQL to:\n\nFilter rows and columns in a dataset.\nRename data fields and convert between data types.\nCalculate derived data fields.\nManipulate string values.\nGroup and aggregate data.\n\nAzure Synapse serverless SQL pools can be used to run SQL statements that transform data and persist the results as a file in a data lake for further processing or querying. If you're familiar with Transact-SQL syntax, you can craft a SELECT statement that applies the specific transformation you're interested in, and store the results of the SELECT statement in a selected file format with a metadata table schema that can be queried using SQL.\n\nYou can use a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a dedicated SQL pool or serverless SQL pool to persist the results of a query in an external table, which stores its data in a file in the data lake.\n\nThe CETAS statement includes a SELECT statement that queries and manipulates data from any valid data source (which could be an existing table or view in a database, or an OPENROWSET function that reads file-based data from the data lake). The results of the SELECT statement are then persisted in an external table, which is a metadata object in a database that provides a relational abstraction over data stored in files. The following diagram illustrates the concept visually:\n\nBy applying this technique, you can use SQL to extract and transform data from files or tables, and store the transformed results for downstream processing or analysis. Subsequent operations on the transformed data can be performed against the relational table in the SQL pool database or directly against the underlying data files.\n\nCreating external database objects to support CETAS\n\nTo use CETAS expressions, you must create the following types of object in a database for either a serverless or dedicated SQL pool. When using a serverless SQL pool, create these objects in a custom database (created using the CREATE DATABASE statement), not the built-in database.\n\nExternal data source\n\nAn external data source encapsulates a connection to a file system location in a data lake. You can then use this connection to specify a relative path in which the data files for the external table created by the CETAS statement are saved.\n\nIf the source data for the CETAS statement is in files in the same data lake path, you can use the same external data source in the OPENROWSET function used to query it. Alternatively, you can create a separate external data source for the source files or use a fully qualified file path in the OPENROWSET function.\n\nTo create an external data source, use the CREATE EXTERNAL DATA SOURCE statement, as shown in this example:\n\n-- Create an external data source for the Azure storage account\nCREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/',\n    TYPE = HADOOP, -- For dedicated SQL pool\n    -- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n    CREDENTIAL = storageCred\n);\n\n\nThe previous example assumes that users running queries that use the external data source will have sufficient permissions to access the files. An alternative approach is to encapsulate a credential in the external data source so that it can be used to access file data without granting all users permissions to read it directly:\n\nCREATE DATABASE SCOPED CREDENTIAL storagekeycred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = storagekeycred\n);\n\n\n Tip\n\nIn addition to SAS authentication, you can define credentials that use managed identity (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the Control storage account access for serverless SQL pool in Azure Synapse Analytics article in Azure Synapse Analytics documentation.\n\nExternal file format\n\nThe CETAS statement creates a table with its data stored in files. You must specify the format of the files you want to create as an external file format.\n\nTo create an external file format, use the CREATE EXTERNAL FILE FORMAT statement, as shown in this example:\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n        FORMAT_TYPE = PARQUET,\n        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n    );\n\n\n Tip\n\nIn this example, the files will be saved in Parquet format. You can also create external file formats for other types of file. See CREATE EXTERNAL FILE FORMAT (Transact-SQL) for details.\n\nUsing the CETAS statement\n\nAfter creating an external data source and external file format, you can use the CETAS statement to transform data and stored the results in an external table.\n\nFor example, suppose the source data you want to transform consists of sales orders in comma-delimited text files that are stored in a folder in a data lake. You want to filter the data to include only orders that are marked as \"special order\", and save the transformed data as Parquet files in a different folder in the same data lake. You could use the same external data source for both the source and destination folders as shown in this example:\n\nCREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        -- details for storing results\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        -- details for reading source files\n        BULK 'sales_orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n\n\nThe LOCATION and BULK parameters in the previous example are relative paths for the results and source files respectively. The paths are relative to the file system location referenced by the files external data source.\n\nAn important point to understand is that you must use an external data source to specify the location where the transformed data for the external table is to be saved. When file-based source data is stored in the same folder hierarchy, you can use the same external data source. Otherwise, you can use a second data source to define a connection to the source data or use the fully qualified path, as shown in this example:\n\nCREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        -- details for storing results\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        -- details for reading source files\n        BULK 'https://mystorage.blob.core.windows.net/data/sales_orders/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n\nDropping external tables\n\nIf you no longer need the external table containing the transformed data, you can drop it from the database by using the DROP EXTERNAL TABLE statement, as shown here:\n\nDROP EXTERNAL TABLE SpecialOrders;\n\n\nHowever, it's important to understand that external tables are a metadata abstraction over the files that contain the actual data. Dropping an external table does not delete the underlying files.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWhile SQL is commonly used by data analysts to query data and support analytical and reporting workloads, data engineers often need to use SQL to transform data; often as part of a data ingestion pipeline or extract, transform, and load (ETL) process.\n\nIn this module, you'll learn how to use CREATE EXTERNAL TABLE AS SELECT (CETAS) statements to transform data, and store the results in files in a data lake that can be queried through a relational table in a serverless SQL database or processed directly from the file system.\n\nAfter completing this module, you'll be able to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with Azure Synapse Analytics.\nExperience using Transact-SQL to query and manipulate data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Synapse serverless SQL pools to transform data in a data lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Azure Synapse serverless SQL pools to transform data in a data lake\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nBy using a serverless SQL pool in Azure Synapse Analytics, you can use the ubiquitous SQL language to transform data in files in a data lake.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\nAdd\nPrerequisites\n\nConsider completing Use Azure Synapse serverless SQL pool to query files in a data lake before starting this module. You will need the following knowledge or experience:\n\nFamiliarity with Azure Synapse Analytics.\nExperience using Transact-SQL to query and manipulate data.\nIntroduction\nmin\nTransform data files with the CREATE EXTERNAL TABLE AS SELECT statement\nmin\nEncapsulate data transformations in a stored procedure\nmin\nInclude a data transformation stored procedure in a pipeline\nmin\nExercise - Transform files using a serverless SQL pool\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nServerless SQL pools enable you to easily query files in data lake. You can query various file formats CSV, JSON, Parquet, and create external database objects to provide a relational abstraction layer over the raw files.\n\nIn this module, you've learned how to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nLearn more\n\nTo learn more about using serverless SQL pools to query files, refer to the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhat function is used to read the data in files stored in a data lake?\n\n \n\nFORMAT\n\nROWSET\n\nOPENROWSET\n\n2. \n\nWhat character in file path can be used to select all the file/folders that match rest of the path?\n\n \n\n&\n\n*\n\n/\n\n3. \n\nWhich external database object encapsulates the connection information to a file location in a data lake store?\n\n \n\nFILE FORMAT\n\nDATA SOURCE\n\nEXTERNAL TABLE\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/5-exercise-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Query files using a serverless SQL pool\n40 minutes\n\nNow it's your opportunity to try using a serverless SQL pool for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to query data files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create external database objects - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/4-external-objects",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate external database objects\n6 minutes\n\nYou can use the OPENROWSET function in SQL queries that run in the default master database of the built-in serverless SQL pool to explore data in the data lake. However, sometimes you may want to create a custom database that contains some objects that make it easier to work with external data in the data lake that you need to query frequently.\n\nCreating a database\n\nYou can create a database in a serverless SQL pool just as you would in a SQL Server instance. You can use the graphical interface in Synapse Studio, or a CREATE DATABASE statement. One consideration is to set the collation of your database so that it supports conversion of text data in files to appropriate Transact-SQL data types.\n\nThe following example code creates a database named salesDB with a collation that makes it easier to import UTF-8 encoded text data into VARCHAR columns.\n\nCREATE DATABASE SalesDB\n    COLLATE Latin1_General_100_BIN2_UTF8\n\nCreating an external data source\n\nYou can use the OPENROWSET function with a BULK path to query file data from your own database, just as you can in the master database; but if you plan to query data in the same location frequently, it's more efficient to define an external data source that references that location. For example, the following code creates a data source named files for the hypothetical https://mydatalake.blob.core.windows.net/data/files/ folder:\n\nCREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/'\n)\n\n\nOne benefit of an external data source, is that you can simplify an OPENROWSET query to use the combination of the data source and the relative path to the folders or files you want to query:\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK 'orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'csv',\n        PARSER_VERSION = '2.0'\n    ) AS orders\n\n\nIn this example, the BULK parameter is used to specify the relative path for all .csv files in the orders folder, which is a subfolder of the files folder referenced by the data source.\n\nAnother benefit of using a data source is that you can assign a credential for the data source to use when accessing the underlying storage, enabling you to provide access to data through SQL without permitting users to access the data directly in the storage account. For example, the following code creates a credential that uses a shared access signature (SAS) to authenticate against the underlying Azure storage account hosting the data lake.\n\nCREATE DATABASE SCOPED CREDENTIAL sqlcred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\nGO\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = sqlcred\n);\nGO\n\n\n Tip\n\nIn addition to SAS authentication, you can define credentials that use managed identity (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the Control storage account access for serverless SQL pool in Azure Synapse Analytics article in Azure Synapse Analytics documentation.\n\nCreating an external file format\n\nWhile an external data source simplifies the code needed to access files with the OPENROWSET function, you still need to provide format details for the file being access; which may include multiple settings for delimited text files. You can encapsulate these settings in an external file format, like this:\n\nCREATE EXTERNAL FILE FORMAT CsvFormat\n    WITH (\n        FORMAT_TYPE = DELIMITEDTEXT,\n        FORMAT_OPTIONS(\n            FIELD_TERMINATOR = ',',\n            STRING_DELIMITER = '\"'\n        )\n    );\nGO\n\n\nAfter creating file formats for the specific data files you need to work with, you can use the file format to create external tables, as discussed next.\n\nCreating an external table\n\nWhen you need to perform a lot of analysis or reporting from files in the data lake, using the OPENROWSET function can result in complex code that includes data sources and file paths. To simplify access to the data, you can encapsulate the files in an external table; which users and reporting applications can query using a standard SQL SELECT statement just like any other database table. To create an external table, use the CREATE EXTERNAL TABLE statement, specifying the column schema as for a standard table, and including a WITH clause specifying the external data source, relative path, and external file format for your data.\n\nCREATE EXTERNAL TABLE dbo.products\n(\n    product_id INT,\n    product_name VARCHAR(20),\n    list_price DECIMAL(5,2)\n)\nWITH\n(\n    DATA_SOURCE = files,\n    LOCATION = 'products/*.csv',\n    FILE_FORMAT = CsvFormat\n);\nGO\n\n-- query the table\nSELECT * FROM dbo.products;\n\n\nBy creating a database that contains the external objects discussed in this unit, you can provide a relational database layer over files in a data lake, making it easier for many data analysts and reporting tools to access the data by using standard SQL query semantics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery files using a serverless SQL pool\n10 minutes\n\nYou can use a serverless SQL pool to query data files in various common file formats, including:\n\nDelimited text, such as comma-separated values (CSV) files.\nJavaScript object notation (JSON) files.\nParquet files.\n\nThe basic syntax for querying is the same for all of these types of file, and is built on the OPENROWSET SQL function; which generates a tabular rowset from data in one or more files. For example, the following query could be used to extract data from CSV files.\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv') AS rows\n\n\nThe OPENROWSET function includes more parameters that determine factors such as:\n\nThe schema of the resulting rowset\nAdditional formatting options for delimited text files.\n\n Tip\n\nYou'll find the full syntax for the OPENROWSET function in the Azure Synapse Analytics documentation.\n\nThe output from OPENROWSET is a rowset to which an alias must be assigned. In the previous example, the alias rows is used to name the resulting rowset.\n\nThe BULK parameter includes the full URL to the location in the data lake containing the data files. This can be an individual file, or a folder with a wildcard expression to filter the file types that should be included. The FORMAT parameter specifies the type of data being queried. The example above reads delimited text from all .csv files in the files folder.\n\n Note\n\nThis example assumes that the user has access to the files in the underlying store, If the files are protected with a SAS key or custom identity, you would need to create a server-scoped credential.\n\nAs seen in the previous example, you can use wildcards in the BULK parameter to include or exclude files in the query. The following list shows a few examples of how this can be used:\n\nhttps://mydatalake.blob.core.windows.net/data/files/file1.csv: Only include file1.csv in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/file*.csv: All .csv files in the files folder with names that start with \"file\".\nhttps://mydatalake.blob.core.windows.net/data/files/*: All files in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/**: All files in the files folder, and recursively its subfolders.\n\nYou can also specify multiple file paths in the BULK parameter, separating each path with a comma.\n\nQuerying delimited text files\n\nDelimited text files are a common file format within many businesses. The specific formatting used in delimited files can vary, for example:\n\nWith and without a header row.\nComma and tab-delimited values.\nWindows and Unix style line endings.\nNon-quoted and quoted values, and escaping characters.\n\nRegardless of the type of delimited file you're using, you can read data from them by using the OPENROWSET function with the csv FORMAT parameter, and other parameters as required to handle the specific formatting details for your data. For example:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    FIRSTROW = 2) AS rows\n\n\nThe PARSER_VERSION is used to determine how the query interprets the text encoding used in the files. Version 1.0 is the default and supports a wide range of file encodings, while version 2.0 supports fewer encodings but offers better performance. The FIRSTROW parameter is used to skip rows in the text file, to eliminate any unstructured preamble text or to ignore a row containing column headings.\n\nAdditional parameters you might require when working with delimited text files include:\n\nFIELDTERMINATOR - the character used to separate field values in each row. For example, a tab-delimited file separates fields with a TAB (\\t) character. The default field terminator is a comma (,).\nROWTERMINATOR - the character used to signify the end of a row of data. For example, a standard Windows text file uses a combination of a carriage return (CR) and line feed (LF), which is indicated by the code \\n; while UNIX-style text files use a single line feed character, which can be indicated using the code 0x0a.\nFIELDQUOTE - the character used to enclose quoted string values. For example, to ensure that the comma in the address field value 126 Main St, apt 2 isn't interpreted as a field delimiter, you might enclose the entire field value in quotation marks like this: \"126 Main St, apt 2\". The double-quote (\") is the default field quote character.\n\n Tip\n\nFor details of additional parameters when working with delimited text files, refer to the Azure Synapse Analytics documentation.\n\nSpecifying the rowset schema\n\nIt's common for delimited text files to include the column names in the first row. The OPENROWSET function can use this to define the schema for the resulting rowset, and automatically infer the data types of the columns based on the values they contain. For example, consider the following delimited text:\n\nproduct_id,product_name,list_price\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThe data consists of the following three columns:\n\nproduct_id (integer number)\nproduct_name (string)\nlist_price (decimal number)\n\nYou could use the following query to extract the data with the correct column names and appropriately inferred SQL Server data types (in this case INT, NVARCHAR, and DECIMAL)\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE) AS rows\n\n\nThe HEADER_ROW parameter (which is only available when using parser version 2.0) instructs the query engine to use the first row of data in each file as the column names, like this:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nNow consider the following data:\n\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThis time, the file doesn't contain the column names in a header row; so while the data types can still be inferred, the column names will be set to C1, C2, C3, and so on.\n\nExpand table\nC1\tC2\tC3\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nTo specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a WITH clause, like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0')\nWITH (\n    product_id INT,\n    product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8,\n    list_price DECIMAL(5,2)\n) AS rows\n\n\nThis query produces the expected results:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.99\n124\tGadget\t3.99\n\n Tip\n\nWhen working with text files, you may encounter some incompatibility with UTF-8 encoded data and the collation used in the master database for the serverless SQL pool. To overcome this, you can specify a compatible collation for individual VARCHAR columns in the schema. See the troubleshooting guidance for more details.\n\nQuerying JSON files\n\nJSON is a popular format for web applications that exchange data through REST interfaces or use NoSQL data stores such as Azure Cosmos DB. So, it's not uncommon to persist data as JSON documents in files in a data lake for analysis.\n\nFor example, a JSON file that defines an individual product might look like this:\n\n{\n    \"product_id\": 123,\n    \"product_name\": \"Widget\",\n    \"list_price\": 12.99\n}\n\n\nTo return product data from a folder containing multiple JSON files in this format, you could use the following SQL query:\n\nSELECT doc\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nOPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:\n\nExpand table\ndoc\n{\"product_id\":123,\"product_name\":\"Widget\",\"list_price\": 12.99}\n{\"product_id\":124,\"product_name\":\"Gadget\",\"list_price\": 3.99}\n\nTo extract individual values from the JSON, you can use the JSON_VALUE function in the SELECT statement, as shown here:\n\nSELECT JSON_VALUE(doc, '$.product_name') AS product,\n           JSON_VALUE(doc, '$.list_price') AS price\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nThis query would return a rowset similar to the following results:\n\nExpand table\nproduct\tprice\nWidget\t12.99\nGadget\t3.99\nQuerying Parquet files\n\nParquet is a commonly used format for big data processing on distributed file storage. It's an efficient data format that is optimized for compression and analytical querying.\n\nIn most cases, the schema of the data is embedded within the Parquet file, so you only need to specify the BULK parameter with a path to the file(s) you want to read, and a FORMAT parameter of parquet; like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.*',\n    FORMAT = 'parquet') AS rows\n\nQuery partitioned data\n\nIt's common in a data lake to partition data by splitting across multiple files in subfolders that reflect partitioning criteria. This enables distributed processing systems to work in parallel on multiple partitions of the data, or to easily eliminate data reads from specific folders based on filtering criteria. For example, suppose you need to efficiently process sales order data, and often need to filter based on the year and month in which orders were placed. You could partition the data using folders, like this:\n\n/orders\n/year=2020\n/month=1\n/01012020.parquet\n/02012020.parquet\n...\n/month=2\n/01022020.parquet\n/02022020.parquet\n...\n...\n/year=2021\n/month=1\n/01012021.parquet\n/02012021.parquet\n...\n...\n\nTo create a query that filters the results to include only the orders for January and February 2020, you could use the following code:\n\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*',\n    FORMAT = 'parquet') AS orders\nWHERE orders.filepath(1) = '2020'\n    AND orders.filepath(2) IN ('1','2');\n\n\nThe numbered filepath parameters in the WHERE clause reference the wildcards in the folder names in the BULK path -so the parameter 1 is the * in the year=* folder name, and parameter 2 is the * in the month=* folder name.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Analytics includes serverless SQL pools, which are tailored for querying data in a data lake. With a serverless SQL pool you can use SQL code to query data in files of various common formats without needing to load the file data into database storage. This capability helps data analysts and data engineers analyze and process file data in the data lake using a familiar data processing language, without the need to create or maintain a relational database store.\n\nAfter completing this module, you'll be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with the Microsoft Azure portal\nFamiliarity with data lake and data warehouse concepts\nExperience of using SQL to query database tables\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Synapse serverless SQL pool capabilities and use cases - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/2-understand-serverless-pools",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\n5 minutes\n\nAzure Synapse Analytics is an integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale. One of the most prevalent technologies used in data solutions is SQL - an industry standard language for querying and manipulating data.\n\nServerless SQL pools in Azure Synapse Analytics\n\nAzure Synapse SQL is a distributed query system in Azure Synapse Analytics that offers two kinds of runtime environments:\n\nServerless SQL pool: on-demand SQL query processing, primarily used to work with data in a data lake.\nDedicated SQL pool: Enterprise-scale relational database instances used to host data warehouses in which data is stored in relational tables.\n\nIn this module, we'll focus on serverless SQL pool, which provides a pay-per-query endpoint to query the data in your data lake. The benefits of using serverless SQL pool include:\n\nA familiar Transact-SQL syntax to query data in place without the need to copy or load data into a specialized store.\nIntegrated connectivity from a wide range of business intelligence and ad-hoc querying tools, including the most popular drivers.\nDistributed query processing that is built for large-scale data, and computational functions - resulting in fast query performance.\nBuilt-in query execution fault-tolerance, resulting in high reliability and success rates even for long-running queries involving large data sets.\nNo infrastructure to setup or clusters to maintain. A built-in endpoint for this service is provided within every Azure Synapse workspace, so you can start querying data as soon as the workspace is created.\nNo charge for resources reserved, you're only charged for the data processed by queries you run.\nWhen to use serverless SQL pools\n\nServerless SQL pool is tailored for querying the data residing in the data lake, so in addition to eliminating management burden, it eliminates a need to worry about ingesting the data into the system. You just point the query to the data that is already in the lake and run it.\n\nSynapse SQL serverless resource model is great for unplanned or \"bursty\" workloads that can be processed using the always-on serverless SQL endpoint in your Azure Synapse Analytics workspace. Using the serverless pool helps when you need to know exact cost for each query executed to monitor and attribute costs.\n\n Note\n\nServerless SQL pool is an analytics system and is not recommended for OLTP workloads such as databases used by applications to store transactional data. Workloads that require millisecond response times and are looking to pinpoint a single row in a data set are not good fit for serverless SQL pool.\n\nCommon use cases for serverless SQL pools include:\n\nData exploration: Data exploration involves browsing the data lake to get initial insights about the data, and is easily achievable with Azure Synapse Studio. You can browse through the files in your linked data lake storage, and use the built-in serverless SQL pool to automatically generate a SQL script to select TOP 100 rows from a file or folder just as you would do with a table in SQL Server. From there, you can apply projections, filtering, grouping, and most of the operation over the data as if the data were in a regular SQL Server table.\nData transformation: While Azure Synapse Analytics provides great data transformations capabilities with Synapse Spark, some data engineers might find data transformation easier to achieve using SQL. Serverless SQL pool enables you to perform SQL-based data transformations; either interactively or as part of an automated data pipeline.\nLogical data warehouse: After your initial exploration of the data in the data lake, you can define external objects such as tables and views in a serverless SQL database. The data remains stored in the data lake files, but are abstracted by a relational schema that can be used by client applications and analytical tools to query the data as they would in a relational database hosted in SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Synapse serverless SQL pool to query files in a data lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Azure Synapse serverless SQL pool to query files in a data lake\nModule\n7 Units\nFeedback\nBeginner\nData Engineer\nAzure Synapse Analytics\n\nWith Azure Synapse serverless SQL pool, you can leverage your SQL skills to explore and analyze data in files, without the need to load the data into a relational database.\n\nLearning objectives\n\nAfter the completion of this module, you will be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nAdd\nPrerequisites\n\nConsider completing the Explore data analytics in Azure and Get started querying with Transact-SQL learning paths before starting this module. You will need knowledge of:\n\nAnalytical data workloads in Microsoft Azure\nQuerying data with Transact-SQL\nIntroduction\nmin\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\nmin\nQuery files using a serverless SQL pool\nmin\nCreate external database objects\nmin\nExercise - Query files using a serverless SQL pool\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Analytics provides an integrated cloud-based platform for big data processing and analysis. You can use it to build descriptive, diagnostic, predictive, and prescriptive analytics solutions.\n\nIn this module, you learned how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\n\n Tip\n\nTo learn more about the capabilities of Azure Synapse Analytics, see What is Azure Synapse Analytics? in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich feature of Azure Synapse Analytics enables you to transfer data from one store to another and apply transformations to the data at scheduled intervals?\n\n \n\nServerless SQL pool\n\nApache Spark pool\n\nPipelines\n\n2. \n\nYou want to create a data warehouse in Azure Synapse Analytics in which the data is stored and queried in a relational data store. What kind of pool should you create?\n\n \n\nServerless SQL pool\n\nDedicated SQL pool\n\nApache Spark pool\n\n3. \n\nA data analyst wants to analyze data by using Python code combined with text descriptions of the insights gained from the analysis. What should they use to perform the analysis?\n\n \n\nA notebook connected to an Apache Spark pool.\n\nA SQL script connected to a serverless SQL pool.\n\nA KQL script connected to a Data Explorer pool.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/4a-exercise-explore-synapse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore Azure Synapse Analytics\n60 minutes\n\nNow it's your chance to explore the capabilities of Azure Synapse Analytics for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use Azure Synapse Studio to perform core data analytics tasks.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "When to use Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/4-when-use",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhen to use Azure Synapse Analytics\n4 minutes\n\nAcross all organizations and industries, the common use cases for Azure Synapse Analytics are identified by the need for:\n\nLarge-scale data warehousing\n\nData warehousing includes the need to integrate all data, including big data, to reason over data for analytics and reporting purposes from a descriptive analytics perspective, independent of its location or structure.\n\nAdvanced analytics\n\nEnables organizations to perform predictive analytics using both the native features of Azure Synapse Analytics, and integrating with other technologies such as Azure Machine Learning.\n\nData exploration and discovery\n\nThe serverless SQL pool functionality provided by Azure Synapse Analytics enables Data Analysts, Data Engineers and Data Scientist alike to explore the data within your data estate. This capability supports data discovery, diagnostic analytics, and exploratory data analysis.\n\nReal time analytics\n\nAzure Synapse Analytics can capture, store and analyze data in real-time or near-real time with features such as Azure Synapse Link, or through the integration of services such as Azure Stream Analytics and Azure Data Explorer.\n\nData integration\n\nAzure Synapse Pipelines enables you to ingest, prepare, model and serve the data to be used by downstream systems. This can be used by components of Azure Synapse Analytics exclusively.\n\nIntegrated analytics\n\nWith the variety of analytics that can be performed on the data at your disposal, putting together the services in a cohesive solution can be a complex operation. Azure Synapse Analytics removes this complexity by integrating the analytics landscape into one service. That way you can spend more time working with the data to bring business benefit, than spending much of your time provisioning and maintaining multiple systems to achieve the same outcomes.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "How Azure Synapse Analytics works - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/3-how-works",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nHow Azure Synapse Analytics works\n8 minutes\n\nTo support the analytics needs of today's organizations, Azure Synapse Analytics combines a centralized service for data storage and processing with an extensible architecture through which linked services enable you to integrate commonly used data stores, processing platforms, and visualization tools.\n\nCreating and using an Azure Synapse Analytics workspace\n\nA Synapse Analytics workspace defines an instance of the Synapse Analytics service in which you can manage the services and data resources needed for your analytics solution. You can create a Synapse Analytics workspace in an Azure subscription interactively by using the Azure portal, or you can automate deployment by using Azure PowerShell, the Azure command-line interface (CLI), or with an Azure Resource Manager or Bicep template.\n\nAfter creating a Synapse Analytics workspace, you can manage the services in it and perform data analytics tasks with them by using Synapse Studio; a web-based portal for Azure Synapse Analytics.\n\nWorking with files in a data lake\n\nOne of the core resources in a Synapse Analytics workspace is a data lake, in which data files can be stored and processed at scale. A workspace typically has a default data lake, which is implemented as a linked service to an Azure Data Lake Storage Gen2 container. You can add linked services for multiple data lakes that are based on different storage platforms as required.\n\nIngesting and transforming data with pipelines\n\nIn most enterprise data analytics solutions, data is extracted from multiple operational sources and transferred to a central data lake or data warehouse for analysis. Azure Synapse Analytics includes built-in support for creating, running, and managing pipelines that orchestrate the activities necessary to retrieve data from a range of sources, transform the data as required, and load the resulting transformed data into an analytical store.\n\n Note\n\nPipelines in Azure Synapse Analytics are based on the same underlying technology as Azure Data Factory. If you are already familiar with Azure Data Factory, you can leverage your existing skills to build data ingestion and transformation solutions in Azure Synapse Analytics.\n\nQuerying and manipulating data with SQL\n\nStructured Query Language (SQL) is a ubiquitous language for querying and manipulating data, and is the foundation for relational databases, including the popular Microsoft SQL Server database platform. Azure Synapse Analytics supports SQL-based data querying and manipulation through two kinds of SQL pool that are based on the SQL Server relational database engine:\n\nA built-in serverless pool that is optimized for using relational SQL semantics to query file-based data in a data lake.\nCustom dedicated SQL pools that host relational data warehouses.\n\nThe Azure Synapse SQL system uses a distributed query processing model to parallelize SQL operations, resulting in a highly scalable solution for relational data processing. You can use the built-in serverless pool for cost-effective analysis and processing of file data in the data lake, and use dedicated SQL pools to create relational data warehouses for enterprise data modeling and reporting.\n\nProcessing and analyzing data with Apache Spark\n\nApache Spark is an open source platform for big data analytics. Spark performs distributed processing of files in a data lake by running jobs that can be implemented using any of a range of supported programming languages. Languages supported in Spark include Python, Scala, Java, SQL, and C#.\n\nIn Azure Synapse Analytics, you can create one or more Spark pools and use interactive notebooks to combine code and notes as you build solutions for data analytics, machine learning, and data visualization.\n\nExploring data with Data Explorer\n\nAzure Synapse Data Explorer is a data processing engine in Azure Synapse Analytics that is based on the Azure Data Explorer service. Data Explorer uses an intuitive query syntax named Kusto Query Language (KQL) to enable high performance, low-latency analysis of batch and streaming data.\n\nIntegrating with other Azure data services\n\nAzure Synapse Analytics can be integrated with other Azure data services for end-to-end analytics solutions. Integrated solutions include:\n\nAzure Synapse Link enables near-realtime synchronization between operational data in Azure Cosmos DB, Azure SQL Database, SQL Server, and Microsoft Power Platform Dataverse and analytical data storage that can be queried in Azure Synapse Analytics.\nMicrosoft Power BI integration enables data analysts to integrate a Power BI workspace into a Synapse workspace, and perform interactive data visualization in Azure Synapse Studio.\nMicrosoft Purview integration enables organizations to catalog data assets in Azure Synapse Analytics, and makes it easier for data engineers to find data assets and track data lineage when implementing data pipelines that ingest data into Azure Synapse Analytics.\nAzure Machine Learning integration enables data analysts and data scientists to integrate predictive model training and consumption into analytical solutions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nThe volume of data generated by individuals and organizations is growing at a phenomenal rate. This data powers businesses and other organizations by providing a basis for descriptive, diagnostic, predictive, and prescriptive analytical solutions that support decision making and autonomous systems by providing real-time insights into established and emerging patterns.\n\nOrganizations have a choice of many tools and techniques for data analytics, often requiring expertise across multiple systems and complex integration of infrastructure and administrative operations. Azure Synapse Analytics provides a single, cloud-scale platform that supports multiple analytical technologies; enabling a consolidated and integrated experience for data engineers, data analysts, data scientists, and other professionals who need to work with data.\n\nIn this module, you'll learn how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\nPrerequisites\n\nBefore completing this module, you should have the following prerequisite knowledge and experience:\n\nFamiliarity with cloud computing concepts and Microsoft Azure.\nFamiliarity with fundamental data concepts.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "What is Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/2-what-happening-business",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhat is Azure Synapse Analytics\n5 minutes\n\nThe technological research and consulting firm Gartner defines four common types of analytical technique that organizations commonly use:\n\nDescriptive analytics, which answers the question “What is happening in my business?”. The data to answer this question is typically answered through the creation of a data warehouse in which historical data is persisted in relational tables for multidimensional modeling and reporting.\n\nDiagnostic analytics, which deals with answering the question “Why is it happening?”. This may involve exploring information that already exists in a data warehouse, but typically involves a wider search of your data estate to find more data to support this type of analysis.\n\nPredictive analytics, which enables you to answer the question “What is likely to happen in the future based on previous trends and patterns?”\n\nPrescriptive analytics, which enables autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics.\n\nAzure Synapse Analytics provides a cloud platform for all of these analytical workloads through support for multiple data storage, processing, and analysis technologies in a single, integrated solution. The integrated design of Azure Synapse Analytics enables organizations to leverage investments and skills in multiple commonly used data technologies, including SQL, Apache Spark, and others; while providing a centrally managed service and a single, consistent user interface.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntroduction to Azure Synapse Analytics\nModule\n7 Units\nFeedback\nBeginner\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nLearn about the features and capabilities of Azure Synapse Analytics - a cloud-based platform for big data processing and analysis.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\nAdd\nPrerequisites\n\nBefore completing this module, you should have the following prerequisite knowledge and experience:\n\nFamiliarity with cloud computing concepts and Microsoft Azure.\nFamiliarity with fundamental data concepts.\nIntroduction\nmin\nWhat is Azure Synapse Analytics\nmin\nHow Azure Synapse Analytics works\nmin\nWhen to use Azure Synapse Analytics\nmin\nExercise - Explore Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Data Lake Storage Gen2 provides a cloud storage service that is available, secure, durable, scalable, and redundant. It's a comprehensive data lake solution.\n\nAzure Data Lake Storage brings efficiencies to process big data analytics workloads and can provide data to many compute technologies including Azure Synapse Analytics, Azure HDInsight, and Azure Databricks without needing to move the data around. Creating an Azure Data Lake Storage Gen2 data store can be an important tool in building a big data analytics solution.\n\n Tip\n\nTo learn more about Azure Data Lake Storage Gen2, see Introduction to Azure Data Lake Storage Gen2 in the Microsoft Azure documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nAzure Data Lake Storage Gen2 stores data in…\n\n \n\nA document database hosted in Azure Cosmos DB.\n\nAn HDFS-compatible file system hosted in Azure Storage.\n\nA relational data warehouse hosted in Azure Synapse Analytics.\n\n2. \n\nWhat option must you enable to use Azure Data Lake Storage Gen2?\n\n \n\nGlobal replication\n\nData encryption\n\nHierarchical namespace\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Data Lake Storage Gen2 in data analytics workloads - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/6-use-cases",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Azure Data Lake Storage Gen2 in data analytics workloads\n5 minutes\n\nAzure Data Lake Store Gen2 is an enabling technology for multiple data analytics use cases. Let's explore a few common types of analytical workload, and identify how Azure Data Lake Storage Gen2 works with other Azure services to support them.\n\nBig data processing and analytics\n\nBig data scenarios usually refer to analytical workloads that involve massive volumes of data in a variety of formats that needs to be processed at a fast velocity - the so-called \"three v's\". Azure Data Lake Storage Gen 2 provides a scalable and secure distributed data store on which big data services such as Azure Synapse Analytics, Azure Databricks, and Azure HDInsight can apply data processing frameworks such as Apache Spark, Hive, and Hadoop. The distributed nature of the storage and the processing compute enables tasks to be performed in parallel, resulting in high-performance and scalability even when processing huge amounts of data.\n\nData warehousing\n\nData warehousing has evolved in recent years to integrate large volumes of data stored as files in a data lake with relational tables in a data warehouse. In a typical example of a data warehousing solution, data is extracted from operational data stores, such as Azure SQL database or Azure Cosmos DB, and transformed into structures more suitable for analytical workloads. Often, the data is staged in a data lake in order to facilitate distributed processing before being loaded into a relational data warehouse. In some cases, the data warehouse uses external tables to define a relational metadata layer over files in the data lake and create a hybrid \"data lakehouse\" or \"lake database\" architecture. The data warehouse can then support analytical queries for reporting and visualization.\n\nThere are multiple ways to implement this kind of data warehousing architecture. The diagram shows a solution in which Azure Synapse Analytics hosts pipelines to perform extract, transform, and load (ETL) processes using Azure Data Factory technology. These processes extract data from operational data sources and load it into a data lake hosted in an Azure Data Lake Storage Gen2 container. The data is then processed and loaded into a relational data warehouse in an Azure Synapse Analytics dedicated SQL pool, from where it can support data visualization and reporting using Microsoft Power BI.\n\nReal-time data analytics\n\nIncreasingly, businesses and other organizations need to capture and analyze perpetual streams of data, and analyze it in real-time (or as near to real-time as possible). These streams of data can be generated from connected devices (often referred to as internet-of-things or IoT devices) or from data generated by users in social media platforms or other applications. Unlike traditional batch processing workloads, streaming data requires a solution that can capture and process a boundless stream of data events as they occur.\n\nStreaming events are often captured in a queue for processing. There are multiple technologies you can use to perform this task, including Azure Event Hubs as shown in the image. From here, the data is processed, often to aggregate data over temporal windows (for example to count the number of social media messages with a given tag every five minutes, or to calculate the average reading of an Internet connected sensor per minute). Azure Stream Analytics enables you to create jobs that query and aggregate event data as it arrives, and write the results in an output sink. One such sink is Azure Data Lake Storage Gen2; from where the captured real-time data can be analyzed and visualized.\n\nData science and machine learning\n\nData science involves the statistical analysis of large volumes of data, often using tools such as Apache Spark and scripting languages such as Python. Azure Data Lake Storage Gen 2 provides a highly scalable cloud-based data store for the volumes of data required in data science workloads.\n\nMachine learning is a subarea of data science that deals with training predictive models. Model training requires huge amounts of data, and the ability to process that data efficiently. Azure Machine Learning is a cloud service in which data scientists can run Python code in notebooks using dynamically allocated distributed compute resources. The compute processes data in Azure Data Lake Storage Gen2 containers to train models, which can then be deployed as production web services to support predictive analytical workloads.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand the stages for processing big data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/5-stages-for-processing-big-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand the stages for processing big data\n5 minutes\n\nData lakes have a fundamental role in a wide range of big data architectures. These architectures can involve the creation of:\n\nAn enterprise data warehouse.\nAdvanced analytics against big data.\nA real-time analytical solution.\n\nThere are four stages for processing big data solutions that are common to all architectures:\n\nIngest - The ingestion phase identifies the technology and processes that are used to acquire the source data. This data can come from files, logs, and other types of unstructured data that must be put into the data lake. The technology that is used will vary depending on the frequency that the data is transferred. For example, for batch movement of data, pipelines in Azure Synapse Analytics or Azure Data Factory may be the most appropriate technology to use. For real-time ingestion of data, Apache Kafka for HDInsight or Stream Analytics may be an appropriate choice.\nStore - The store phase identifies where the ingested data should be placed. Azure Data Lake Storage Gen2 provides a secure and scalable storage solution that is compatible with commonly used big data processing technologies.\nPrep and train - The prep and train phase identifies the technologies that are used to perform data preparation and model training and scoring for machine learning solutions. Common technologies that are used in this phase are Azure Synapse Analytics, Azure Databricks, Azure HDInsight, and Azure Machine Learning.\nModel and serve - Finally, the model and serve phase involves the technologies that will present the data to users. These technologies can include visualization tools such as Microsoft Power BI, or analytical data stores such as Azure Synapse Analytics. Often, a combination of multiple technologies will be used depending on the business requirements.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Compare Azure Data Lake Store to Azure Blob storage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/4-azure-data-lake-and-blob-storage",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCompare Azure Data Lake Store to Azure Blob storage\n5 minutes\n\nIn Azure Blob storage, you can store large amounts of unstructured (\"object\") data in a flat namespace within a blob container. Blob names can include \"/\" characters to organize blobs into virtual \"folders\", but in terms of blob manageability the blobs are stored as a single-level hierarchy in a flat namespace.\n\nYou can access this data by using HTTP or HTTPs\n\nAzure Data Lake Storage Gen2 builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace that organizes blob data into directories, and stores metadata about each directory and the files within it. This structure allows operations, such as directory renames and deletes, to be performed in a single atomic operation. Flat namespaces, by contrast, require several operations proportionate to the number of objects in the structure. Hierarchical namespaces keep the data organized, which yields better storage and retrieval performance for an analytical use case and lowers the cost of analysis.\n\n Tip\n\nIf you want to store data without performing analysis on the data, set the Hierarchical Namespace option to Disabled to set up the storage account as an Azure Blob storage account. You can also use blob storage to archive rarely used data or to store website assets such as images and media.\n\nIf you are performing analytics on the data, set up the storage account as an Azure Data Lake Storage Gen2 account by setting the Hierarchical Namespace option to Enabled. Because Azure Data Lake Storage Gen2 is integrated into the Azure Storage platform, applications can use either the Blob APIs or the Azure Data Lake Storage Gen2 file system APIs to access data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enable Azure Data Lake Storage Gen2 in Azure Storage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/3-create-data-lake-account",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEnable Azure Data Lake Storage Gen2 in Azure Storage\n5 minutes\n\nAzure Data Lake Storage Gen2 isn't a standalone Azure service, but rather a configurable capability of a StorageV2 (General Purpose V2) Azure Storage.\n\nTo enable Azure Data Lake Storage Gen2 in an Azure Storage account, you can select the option to Enable hierarchical namespace in the Advanced page when creating the storage account in the Azure portal:\n\nAlternatively, if you already have an Azure Storage account and want to enable the Azure data Lake Storage Gen2 capability, you can use the Data Lake Gen2 upgrade wizard in the Azure portal page for your storage account resource.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nMany organizations have spent the last two decades building data warehouses and business intelligence (BI) solutions based on relational database systems. Many BI solutions have lost out on opportunities to store unstructured data due to cost and complexity in these types of data in databases.\n\nData lakes have become a common solution to this problem. A data lake provides file-based storage, usually in a distributed file system that supports high scalability for massive volumes of data. Organizations can store structured, semi-structured, and unstructured files in the data lake and then consume them from there in big data processing technologies, such as Apache Spark.\n\nAzure Data Lake Storage Gen2 provides a cloud-based solution for data lake storage in Microsoft Azure, and underpins many large-scale analytics solutions built on Azure.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Data Lake Storage Gen2 - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/2-azure-data-lake-gen2",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Data Lake Storage Gen2\n5 minutes\n\nA data lake is a repository of data that is stored in its natural format, usually as blobs or files. Azure Data Lake Storage is a comprehensive, massively scalable, secure, and cost-effective data lake solution for high performance analytics built into Azure.\n\nAzure Data Lake Storage combines a file system with a storage platform to help you quickly identify insights into your data. Data Lake Storage builds on Azure Blob storage capabilities to optimize it specifically for analytics workloads. This integration enables analytics performance, the tiering and data lifecycle management capabilities of Blob storage, and the high-availability, security, and durability capabilities of Azure Storage.\n\nBenefits\n\nData Lake Storage is designed to deal with this variety and volume of data at exabyte scale while securely handling hundreds of gigabytes of throughput. With this, you can use Data Lake Storage Gen2 as the basis for both real-time and batch solutions.\n\nHadoop compatible access\n\nA benefit of Data Lake Storage is that you can treat the data as if it's stored in a Hadoop Distributed File System (HDFS). With this feature, you can store the data in one place and access it through compute technologies including Azure Databricks, Azure HDInsight, and Azure Synapse Analytics without moving the data between environments. The data engineer also has the ability to use storage mechanisms such as the parquet format, which is highly compressed and performs well across multiple platforms using an internal columnar storage.\n\nSecurity\n\nData Lake Storage supports access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions that don't inherit the permissions of the parent directory. In fact, you can set permissions at a directory level or file level for the data stored within the data lake, providing a much more secure storage system. This security is configurable through technologies such as Hive and Spark or utilities such as Azure Storage Explorer, which runs on Windows, macOS, and Linux. All data that is stored is encrypted at rest by using either Microsoft or customer-managed keys.\n\nPerformance\n\nAzure Data Lake Storage organizes the stored data into a hierarchy of directories and subdirectories, much like a file system, for easier navigation. As a result, data processing requires less computational resources, reducing both the time and cost.\n\nData redundancy\n\nData Lake Storage takes advantage of the Azure Blob replication models that provide data redundancy in a single data center with locally redundant storage (LRS), or to a secondary region by using the Geo-redundant storage (GRS) option. This feature ensures that your data is always available and protected if catastrophe strikes.\n\n Tip\n\nWhenever planning for a data lake, a data engineer should give thoughtful consideration to structure, data governance, and security. This should include consideration of factors that can influence lake structure and organization, such as:\n\nTypes of data to be stored\nHow the data will be transformed\nWho should access the data\nWhat are the typical access patterns\n\nThis approach will help determine how to plan for access control governance across your lake. Data engineers should be proactive in ensuring that the lake doesn't become the proverbial data swamp which becomes inaccessible and non-useful to users due to the lack of data governance and data quality measures. Establishing a baseline and following best practices for Azure Data Lake will help ensure a proper and robust implementation that will allow the organization to grow and gain insight to achieve more.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to Azure Data Lake Storage Gen2 - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntroduction to Azure Data Lake Storage Gen2\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure\nAzure Data Lake\n\nData lakes are a core element of data analytics architectures. Azure Data Lake Storage Gen2 provides a scalable, secure, cloud-based solution for data lake storage.\n\nLearning objectives\n\nIn this module you will learn how to:\n\nDescribe the key features and benefits of Azure Data Lake Storage Gen2\nEnable Azure Data Lake Storage Gen2 in an Azure Storage account\nCompare Azure Data Lake Storage Gen2 and Azure Blob storage\nDescribe where Azure Data Lake Storage Gen2 fits in the stages of analytical processing\nDescribe how Azure data Lake Storage Gen2 is used in common analytical workloads\nAdd\nPrerequisites\n\nBefore starting this module, you should have completed the Microsoft Azure Data Fundamentals certification or have equivalent knowledge and experience.\n\nIntroduction\nmin\nUnderstand Azure Data Lake Storage Gen2\nmin\nEnable Azure Data Lake Storage Gen2 in Azure Storage\nmin\nCompare Azure Data Lake Store to Azure Blob storage\nmin\nUnderstand the stages for processing big data\nmin\nUse Azure Data Lake Storage Gen2 in data analytics workloads\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nData engineering is a well established industry role that is supported by multiple services in Microsoft Azure. Data engineers can use these services to work with different types of data, building integration, transformation, and consolidation solutions to support enterprise analytics.\n\n Tip\n\nTo learn more about Azure architecture solutions for data analytics, see Analytics architecture design.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions, then select Check your answers.\n\n1. \n\nData in a relational database table is…\n\n \n\nStructured\n\nSemi-structured\n\nUnstructured\n\n2. \n\nIn a data lake, data is stored in?\n\n \n\nRelational tables\n\nFiles\n\nA single JSON document\n\n3. \n\nWhich of the following Azure services provides capabilities for running data pipelines AND managing analytical data in a data lake or relational data warehouse?\n\n \n\nAzure Stream Analytics\n\nAzure Synapse Analytics\n\nAzure Databricks\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Data engineering in Microsoft Azure - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/5-common-tooling-azure-data-engineering",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nData engineering in Microsoft Azure\n6 minutes\n\nMicrosoft Azure includes many services that can be used to implement and manage data engineering workloads.\n\nThe diagram displays the flow from left to right of a typical enterprise data analytics solution, including some of the key Azure services that may be used. Operational data is generated by applications and devices and stored in Azure data storage services such as Azure SQL Database, Azure Cosmos DB, and Microsoft Dataverse. Streaming data is captured in event broker services such as Azure Event Hubs.\n\nThis operational data must be captured, ingested, and consolidated into analytical stores; from where it can be modeled and visualized in reports and dashboards. These tasks represent the core area of responsibility for the data engineer. The core Azure technologies used to implement data engineering workloads include:\n\nAzure Synapse Analytics\nAzure Data Lake Storage Gen2\nAzure Stream Analytics\nAzure Data Factory\nAzure Databricks\n\nThe analytical data stores that are populated with data produced by data engineering workloads support data modeling and visualization for reporting and analysis, often using sophisticated visualization tools such as Microsoft Power BI.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Important data engineering concepts - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/4-common-patterns-azure-data-engineering",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Get started with data engineering on Azure  Introduction to data engineering on Azure \nAdd\nPrevious\nUnit 3 of 6\nNext\nImportant data engineering concepts\nCompleted\n100 XP\n6 minutes\n\nThere are some core concepts with which data engineers should be familiar. These concepts underpin many of the workloads that data engineers must implement and support.\n\nOperational and analytical data\n\nOperational data is usually transactional data that is generated and stored by applications, often in a relational or non-relational database. Analytical data is data that has been optimized for analysis and reporting, often in a data warehouse.\n\nOne of the core responsibilities of a data engineer is to design, implement, and manage solutions that integrate operational and analytical data sources or extract operational data from multiple systems, transform it into appropriate structures for analytics, and load it into an analytical data store (usually referred to as ETL solutions).\n\nStreaming data\n\nStreaming data refers to perpetual sources of data that generate data values in real-time, often relating to specific events. Common sources of streaming data include internet-of-things (IoT) devices and social media feeds.\n\nData engineers often need to implement solutions that capture real-time stream of data and ingest them into analytical data systems, often combining the real-time data with other application data that is processed in batches.\n\nData pipelines\n\nData pipelines are used to orchestrate activities that transfer and transform data. Pipelines are the primary way in which data engineers implement repeatable extract, transform, and load (ETL) solutions that can be triggered based on a schedule or in response to events.\n\nData lakes\n\nA data lake is a storage repository that holds large amounts of data in native, raw formats. Data lake stores are optimized for scaling to massive volumes (terabytes or petabytes) of data. The data typically comes from multiple heterogeneous sources, and may be structured, semi-structured, or unstructured.\n\nThe idea with a data lake is to store everything in its original, untransformed state. This approach differs from a traditional data warehouse, which transforms and processes the data at the time of ingestion.\n\nData warehouses\n\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data in relational tables that are organized into a schema that optimizes performance for analytical queries.\n\nData engineers are responsible for designing and implementing relational data warehouses, and managing regular data loads into tables.\n\nApache Spark\n\nApache Spark is a parallel processing framework that takes advantage of in-memory processing and a distributed file storage. It's a common open-source software (OSS) tool for big data scenarios.\n\nData engineers need to be proficient with Spark, using notebooks and other code artifacts to process data in a data lake and prepare it for modeling and analysis.\n\nNext unit: Data engineering in Microsoft Azure\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "What is data engineering - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/2-what-data-engineering",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Get started with data engineering on Azure  Introduction to data engineering on Azure \nAdd\nPrevious\nUnit 2 of 6\nNext\nWhat is data engineering\nCompleted\n100 XP\n5 minutes\n\nThe data engineer will often work with multiple types of data to perform many operations using many scripting or coding languages that are appropriate to their individual organization.\n\nTypes of data\n\nThere are three primary types of data that a data engineer will work with.\n\nExpand table\nStructured\tSemi-structured\tUnstructured\n\t\t\nStructured data primarily comes from table-based source systems such as a relational database or from a flat file such as a comma separated (CSV) file. The primary element of a structured file is that the rows and columns are aligned consistently throughout the file.\tSemi-structured data is data such as JavaScript object notation (JSON) files, which may require flattening prior to loading into your source system. When flattened, this data doesn't have to fit neatly into a table structure.\tUnstructured data includes data stored as key-value pairs that don't adhere to standard relational models and Other types of unstructured data that are commonly used include portable data format (PDF), word processor documents, and images.\nData operations\n\nAs a data engineer some of the main tasks that you'll perform in Azure include data integration, data transformation, and data consolidation.\n\nData integration\n\nData Integration involves establishing links between operational and analytical services and data sources to enable secure, reliable access to data across multiple systems. For example, a business process might rely on data that is spread across multiple systems, and a data engineer is required to establish links so that the required data can be extracted from all of these systems.\n\nData transformation\n\nOperational data usually needs to be transformed into suitable structure and format for analysis, often as part of an extract, transform, and load (ETL) process; though increasingly a variation in which you extract, load, and transform (ELT) the data is used to quickly ingest the data into a data lake and then apply \"big data\" processing techniques to transform it. Regardless of the approach used, the data is prepared to support downstream analytical needs.\n\nData consolidation\n\nData consolidation is the process of combining data that has been extracted from multiple data sources into a consistent structure - usually to support analytics and reporting. Commonly, data from operational systems is extracted, transformed, and loaded into analytical stores such as a data lake or data warehouse.\n\nCommon languages\n\nData Engineers must be proficient with a range of tools and scripting languages - in particular SQL and Python, and potentially others.\n\nSQL - One of the most common languages data engineers use is SQL, or Structured Query Language, which is a relatively easy language to learn. SQL uses queries that include SELECT, INSERT, UPDATE, and DELETE statements to directly work with the data stored in tables.\n\nPython - Python is one of the most popular and fastest growing programming languages in the world. It's used for all sorts of tasks, including web programming and data analysis. It has emerged as the language to learn for machine learning, and is increasing in popularity in data engineering with the use of notebooks.\n\nOthers - Depending upon the needs of the organization and your individual skill set, you may also use other popular languages within or outside of notebooks including R, Java, Scala, .NET, and more. The use of notebooks is growing in popularity, and allows collaboration using different languages within the same notebook.\n\nNext unit: Important data engineering concepts\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn most organizations, a data engineer is the primary role responsible for integrating, transforming, and consolidating data from various structured and unstructured data systems into structures that are suitable for building analytics solutions. An Azure data engineer also helps ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to data engineering on Azure - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-engineering-azure/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntroduction to data engineering on Azure\nModule\n6 Units\nFeedback\nIntermediate\nData Engineer\nAzure\n\nMicrosoft Azure provides a comprehensive platform for data engineering; but what is data engineering? Complete this module to find out.\n\nLearning objectives\n\nIn this module you will learn how to:\n\nIdentify common data engineering tasks\nDescribe common data engineering concepts\nIdentify Azure services for data engineering\nAdd\nPrerequisites\n\nBefore starting this module, you should have completed the Microsoft Azure Data Fundamentals certification or have equivalent knowledge and experience.\n\nIntroduction\nmin\nWhat is data engineering\nmin\nImportant data engineering concepts\nmin\nData engineering in Microsoft Azure\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]