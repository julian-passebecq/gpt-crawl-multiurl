[
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nYou need to store the results of a query in a serverless SQL pool as files in a data lake. Which SQL statement should you use?\n\n \n\nBULK INSERT\n\nCREATE EXTERNAL TABLE AS SELECT\n\nCOPY\n\n2. \n\nWhich of the following file formats can you use to persist the results of a query?\n\n \n\nCSV only\n\nParquet only.\n\nCSV and Parquet.\n\n3. \n\nYou drop an existing external table from a database in a serverless SQL pool. What else must you do before recreating an external table with the same location?\n\n \n\nDelete the folder containing the data files for dropped table.\n\nDrop and recreate the database.\n\nCreate an Apache Spark pool.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Transform files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4a-exercise-transform-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Transform files using a serverless SQL pool\n30 minutes\n\nNow it's your opportunity to use the CREATE EXTERNAL TABLE AS SELECT statement to transform data. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to transform data from files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Include a data transformation stored procedure in a pipeline - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4-pool-stored-procedures-synapse-pipelines",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nInclude a data transformation stored procedure in a pipeline\n3 minutes\n\nEncapsulating a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a stored procedure makes it easier for you to operationalize data transformations that you may need to perform repeatedly. In Azure Synapse Analytics and Azure Data Factory, you can create pipelines that connect to linked services, including Azure Data Lake Store Gen2 storage accounts that host data lake files, and serverless SQL pools; enabling you to call your stored procedures as part of an overall data extract, transform, and load (ETL) pipeline.\n\nFor example, you can create a pipeline that includes the following activities:\n\nA Delete activity that deletes the target folder for the transformed data in the data lake if it already exists.\nA Stored procedure activity that connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation.\n\nCreating a pipeline for the data transformation enables you to schedule the operation to run at specific times or based on specific events (such as new files being added to the source storage location).\n\n Tip\n\nFor more information about using the Stored procedure activity in a pipeline, see Transform data by using the SQL Server Stored Procedure activity in Azure Data Factory or Synapse Analytics in the Azure Data Factory documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Encapsulate data transformations in a stored procedure - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/3-operationalize-data-transformation-using-stored-procedures",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEncapsulate data transformations in a stored procedure\n4 minutes\n\nWhile you can run a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a script whenever you need to transform data, it's good practice to encapsulate the transformation operation in stored procedure. This approach can make it easier to operationalize data transformations by enabling you to supply parameters, retrieve outputs, and include additional logic in a single procedure call.\n\nFor example, the following code creates a stored procedure that drops the external table if it already exists before recreating it with order data for the specified year:\n\nCREATE PROCEDURE usp_special_orders_by_year @order_year INT\nAS\nBEGIN\n\n\t-- Drop the table if it already exists\n\tIF EXISTS (\n                SELECT * FROM sys.external_tables\n                WHERE name = 'SpecialOrders'\n            )\n        DROP EXTERNAL TABLE SpecialOrders\n\n\t-- Create external table with special orders\n\t-- from the specified year\n\tCREATE EXTERNAL TABLE SpecialOrders\n\t\tWITH (\n\t\t\tLOCATION = 'special_orders/',\n\t\t\tDATA_SOURCE = files,\n\t\t\tFILE_FORMAT = ParquetFormat\n\t\t)\n\tAS\n\tSELECT OrderID, CustomerName, OrderTotal\n\tFROM\n\t\tOPENROWSET(\n\t\t\tBULK 'sales_orders/*.csv',\n\t\t\tDATA_SOURCE = 'files',\n\t\t\tFORMAT = 'CSV',\n\t\t\tPARSER_VERSION = '2.0',\n\t\t\tHEADER_ROW = TRUE\n\t\t) AS source_data\n\tWHERE OrderType = 'Special Order'\n\tAND YEAR(OrderDate) = @order_year\nEND\n\n\n Note\n\nAs discussed previously, dropping an existing external table does not delete the folder containing its data files. You must explicitly delete the target folder if it exists before running the stored procedure, or an error will occur.\n\nIn addition to encapsulating Transact-SQL logic, stored procedures also provide the following benefits:\n\nReduces client to server network traffic\n\nThe commands in a procedure are executed as a single batch of code; which can significantly reduce network traffic between the server and client because only the call to execute the procedure is sent across the network.\n\nProvides a security boundary\n\nMultiple users and client programs can perform operations on underlying database objects through a procedure, even if the users and programs don't have direct permissions on those underlying objects. The procedure controls what processes and activities are performed and protects the underlying database objects; eliminating the requirement to grant permissions at the individual object level and simplifies the security layers.\n\nEases maintenance\n\nAny changes in the logic or file system locations involved in the data transformation can be applied only to the stored procedure; without requiring updates to client applications or other calling functions.\n\nImproved performance\n\nStored procedures are compiled the first time they're executed, and the resulting execution plan is held in the cache and reused on subsequent runs of the same stored procedure. As a result, it takes less time to process the procedure.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Transform data files with the CREATE EXTERNAL TABLE AS SELECT statement - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nTransform data files with the CREATE EXTERNAL TABLE AS SELECT statement\n5 minutes\n\nThe SQL language includes many features and functions that enable you to manipulate data. For example, you can use SQL to:\n\nFilter rows and columns in a dataset.\nRename data fields and convert between data types.\nCalculate derived data fields.\nManipulate string values.\nGroup and aggregate data.\n\nAzure Synapse serverless SQL pools can be used to run SQL statements that transform data and persist the results as a file in a data lake for further processing or querying. If you're familiar with Transact-SQL syntax, you can craft a SELECT statement that applies the specific transformation you're interested in, and store the results of the SELECT statement in a selected file format with a metadata table schema that can be queried using SQL.\n\nYou can use a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a dedicated SQL pool or serverless SQL pool to persist the results of a query in an external table, which stores its data in a file in the data lake.\n\nThe CETAS statement includes a SELECT statement that queries and manipulates data from any valid data source (which could be an existing table or view in a database, or an OPENROWSET function that reads file-based data from the data lake). The results of the SELECT statement are then persisted in an external table, which is a metadata object in a database that provides a relational abstraction over data stored in files. The following diagram illustrates the concept visually:\n\nBy applying this technique, you can use SQL to extract and transform data from files or tables, and store the transformed results for downstream processing or analysis. Subsequent operations on the transformed data can be performed against the relational table in the SQL pool database or directly against the underlying data files.\n\nCreating external database objects to support CETAS\n\nTo use CETAS expressions, you must create the following types of object in a database for either a serverless or dedicated SQL pool. When using a serverless SQL pool, create these objects in a custom database (created using the CREATE DATABASE statement), not the built-in database.\n\nExternal data source\n\nAn external data source encapsulates a connection to a file system location in a data lake. You can then use this connection to specify a relative path in which the data files for the external table created by the CETAS statement are saved.\n\nIf the source data for the CETAS statement is in files in the same data lake path, you can use the same external data source in the OPENROWSET function used to query it. Alternatively, you can create a separate external data source for the source files or use a fully qualified file path in the OPENROWSET function.\n\nTo create an external data source, use the CREATE EXTERNAL DATA SOURCE statement, as shown in this example:\n\n-- Create an external data source for the Azure storage account\nCREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/',\n    TYPE = HADOOP, -- For dedicated SQL pool\n    -- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n    CREDENTIAL = storageCred\n);\n\n\nThe previous example assumes that users running queries that use the external data source will have sufficient permissions to access the files. An alternative approach is to encapsulate a credential in the external data source so that it can be used to access file data without granting all users permissions to read it directly:\n\nCREATE DATABASE SCOPED CREDENTIAL storagekeycred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = storagekeycred\n);\n\n\n Tip\n\nIn addition to SAS authentication, you can define credentials that use managed identity (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the Control storage account access for serverless SQL pool in Azure Synapse Analytics article in Azure Synapse Analytics documentation.\n\nExternal file format\n\nThe CETAS statement creates a table with its data stored in files. You must specify the format of the files you want to create as an external file format.\n\nTo create an external file format, use the CREATE EXTERNAL FILE FORMAT statement, as shown in this example:\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n        FORMAT_TYPE = PARQUET,\n        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n    );\n\n\n Tip\n\nIn this example, the files will be saved in Parquet format. You can also create external file formats for other types of file. See CREATE EXTERNAL FILE FORMAT (Transact-SQL) for details.\n\nUsing the CETAS statement\n\nAfter creating an external data source and external file format, you can use the CETAS statement to transform data and stored the results in an external table.\n\nFor example, suppose the source data you want to transform consists of sales orders in comma-delimited text files that are stored in a folder in a data lake. You want to filter the data to include only orders that are marked as \"special order\", and save the transformed data as Parquet files in a different folder in the same data lake. You could use the same external data source for both the source and destination folders as shown in this example:\n\nCREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        -- details for storing results\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        -- details for reading source files\n        BULK 'sales_orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n\n\nThe LOCATION and BULK parameters in the previous example are relative paths for the results and source files respectively. The paths are relative to the file system location referenced by the files external data source.\n\nAn important point to understand is that you must use an external data source to specify the location where the transformed data for the external table is to be saved. When file-based source data is stored in the same folder hierarchy, you can use the same external data source. Otherwise, you can use a second data source to define a connection to the source data or use the fully qualified path, as shown in this example:\n\nCREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        -- details for storing results\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        -- details for reading source files\n        BULK 'https://mystorage.blob.core.windows.net/data/sales_orders/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n\nDropping external tables\n\nIf you no longer need the external table containing the transformed data, you can drop it from the database by using the DROP EXTERNAL TABLE statement, as shown here:\n\nDROP EXTERNAL TABLE SpecialOrders;\n\n\nHowever, it's important to understand that external tables are a metadata abstraction over the files that contain the actual data. Dropping an external table does not delete the underlying files.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nWhile SQL is commonly used by data analysts to query data and support analytical and reporting workloads, data engineers often need to use SQL to transform data; often as part of a data ingestion pipeline or extract, transform, and load (ETL) process.\n\nIn this module, you'll learn how to use CREATE EXTERNAL TABLE AS SELECT (CETAS) statements to transform data, and store the results in files in a data lake that can be queried through a relational table in a serverless SQL database or processed directly from the file system.\n\nAfter completing this module, you'll be able to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with Azure Synapse Analytics.\nExperience using Transact-SQL to query and manipulate data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Synapse serverless SQL pools to transform data in a data lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Azure Synapse serverless SQL pools to transform data in a data lake\nModule\n7 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nBy using a serverless SQL pool in Azure Synapse Analytics, you can use the ubiquitous SQL language to transform data in files in a data lake.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nUse a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\nEncapsulate a CETAS statement in a stored procedure.\nInclude a data transformation stored procedure in a pipeline.\nAdd\nPrerequisites\n\nConsider completing Use Azure Synapse serverless SQL pool to query files in a data lake before starting this module. You will need the following knowledge or experience:\n\nFamiliarity with Azure Synapse Analytics.\nExperience using Transact-SQL to query and manipulate data.\nIntroduction\nmin\nTransform data files with the CREATE EXTERNAL TABLE AS SELECT statement\nmin\nEncapsulate data transformations in a stored procedure\nmin\nInclude a data transformation stored procedure in a pipeline\nmin\nExercise - Transform files using a serverless SQL pool\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nServerless SQL pools enable you to easily query files in data lake. You can query various file formats CSV, JSON, Parquet, and create external database objects to provide a relational abstraction layer over the raw files.\n\nIn this module, you've learned how to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nLearn more\n\nTo learn more about using serverless SQL pools to query files, refer to the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhat function is used to read the data in files stored in a data lake?\n\n \n\nFORMAT\n\nROWSET\n\nOPENROWSET\n\n2. \n\nWhat character in file path can be used to select all the file/folders that match rest of the path?\n\n \n\n&\n\n*\n\n/\n\n3. \n\nWhich external database object encapsulates the connection information to a file location in a data lake store?\n\n \n\nFILE FORMAT\n\nDATA SOURCE\n\nEXTERNAL TABLE\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/5-exercise-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Query files using a serverless SQL pool\n40 minutes\n\nNow it's your opportunity to try using a serverless SQL pool for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to query data files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create external database objects - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/4-external-objects",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate external database objects\n6 minutes\n\nYou can use the OPENROWSET function in SQL queries that run in the default master database of the built-in serverless SQL pool to explore data in the data lake. However, sometimes you may want to create a custom database that contains some objects that make it easier to work with external data in the data lake that you need to query frequently.\n\nCreating a database\n\nYou can create a database in a serverless SQL pool just as you would in a SQL Server instance. You can use the graphical interface in Synapse Studio, or a CREATE DATABASE statement. One consideration is to set the collation of your database so that it supports conversion of text data in files to appropriate Transact-SQL data types.\n\nThe following example code creates a database named salesDB with a collation that makes it easier to import UTF-8 encoded text data into VARCHAR columns.\n\nCREATE DATABASE SalesDB\n    COLLATE Latin1_General_100_BIN2_UTF8\n\nCreating an external data source\n\nYou can use the OPENROWSET function with a BULK path to query file data from your own database, just as you can in the master database; but if you plan to query data in the same location frequently, it's more efficient to define an external data source that references that location. For example, the following code creates a data source named files for the hypothetical https://mydatalake.blob.core.windows.net/data/files/ folder:\n\nCREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/'\n)\n\n\nOne benefit of an external data source, is that you can simplify an OPENROWSET query to use the combination of the data source and the relative path to the folders or files you want to query:\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK 'orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'csv',\n        PARSER_VERSION = '2.0'\n    ) AS orders\n\n\nIn this example, the BULK parameter is used to specify the relative path for all .csv files in the orders folder, which is a subfolder of the files folder referenced by the data source.\n\nAnother benefit of using a data source is that you can assign a credential for the data source to use when accessing the underlying storage, enabling you to provide access to data through SQL without permitting users to access the data directly in the storage account. For example, the following code creates a credential that uses a shared access signature (SAS) to authenticate against the underlying Azure storage account hosting the data lake.\n\nCREATE DATABASE SCOPED CREDENTIAL sqlcred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\nGO\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = sqlcred\n);\nGO\n\n\n Tip\n\nIn addition to SAS authentication, you can define credentials that use managed identity (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the Control storage account access for serverless SQL pool in Azure Synapse Analytics article in Azure Synapse Analytics documentation.\n\nCreating an external file format\n\nWhile an external data source simplifies the code needed to access files with the OPENROWSET function, you still need to provide format details for the file being access; which may include multiple settings for delimited text files. You can encapsulate these settings in an external file format, like this:\n\nCREATE EXTERNAL FILE FORMAT CsvFormat\n    WITH (\n        FORMAT_TYPE = DELIMITEDTEXT,\n        FORMAT_OPTIONS(\n            FIELD_TERMINATOR = ',',\n            STRING_DELIMITER = '\"'\n        )\n    );\nGO\n\n\nAfter creating file formats for the specific data files you need to work with, you can use the file format to create external tables, as discussed next.\n\nCreating an external table\n\nWhen you need to perform a lot of analysis or reporting from files in the data lake, using the OPENROWSET function can result in complex code that includes data sources and file paths. To simplify access to the data, you can encapsulate the files in an external table; which users and reporting applications can query using a standard SQL SELECT statement just like any other database table. To create an external table, use the CREATE EXTERNAL TABLE statement, specifying the column schema as for a standard table, and including a WITH clause specifying the external data source, relative path, and external file format for your data.\n\nCREATE EXTERNAL TABLE dbo.products\n(\n    product_id INT,\n    product_name VARCHAR(20),\n    list_price DECIMAL(5,2)\n)\nWITH\n(\n    DATA_SOURCE = files,\n    LOCATION = 'products/*.csv',\n    FILE_FORMAT = CsvFormat\n);\nGO\n\n-- query the table\nSELECT * FROM dbo.products;\n\n\nBy creating a database that contains the external objects discussed in this unit, you can provide a relational database layer over files in a data lake, making it easier for many data analysts and reporting tools to access the data by using standard SQL query semantics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery files using a serverless SQL pool\n10 minutes\n\nYou can use a serverless SQL pool to query data files in various common file formats, including:\n\nDelimited text, such as comma-separated values (CSV) files.\nJavaScript object notation (JSON) files.\nParquet files.\n\nThe basic syntax for querying is the same for all of these types of file, and is built on the OPENROWSET SQL function; which generates a tabular rowset from data in one or more files. For example, the following query could be used to extract data from CSV files.\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv') AS rows\n\n\nThe OPENROWSET function includes more parameters that determine factors such as:\n\nThe schema of the resulting rowset\nAdditional formatting options for delimited text files.\n\n Tip\n\nYou'll find the full syntax for the OPENROWSET function in the Azure Synapse Analytics documentation.\n\nThe output from OPENROWSET is a rowset to which an alias must be assigned. In the previous example, the alias rows is used to name the resulting rowset.\n\nThe BULK parameter includes the full URL to the location in the data lake containing the data files. This can be an individual file, or a folder with a wildcard expression to filter the file types that should be included. The FORMAT parameter specifies the type of data being queried. The example above reads delimited text from all .csv files in the files folder.\n\n Note\n\nThis example assumes that the user has access to the files in the underlying store, If the files are protected with a SAS key or custom identity, you would need to create a server-scoped credential.\n\nAs seen in the previous example, you can use wildcards in the BULK parameter to include or exclude files in the query. The following list shows a few examples of how this can be used:\n\nhttps://mydatalake.blob.core.windows.net/data/files/file1.csv: Only include file1.csv in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/file*.csv: All .csv files in the files folder with names that start with \"file\".\nhttps://mydatalake.blob.core.windows.net/data/files/*: All files in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/**: All files in the files folder, and recursively its subfolders.\n\nYou can also specify multiple file paths in the BULK parameter, separating each path with a comma.\n\nQuerying delimited text files\n\nDelimited text files are a common file format within many businesses. The specific formatting used in delimited files can vary, for example:\n\nWith and without a header row.\nComma and tab-delimited values.\nWindows and Unix style line endings.\nNon-quoted and quoted values, and escaping characters.\n\nRegardless of the type of delimited file you're using, you can read data from them by using the OPENROWSET function with the csv FORMAT parameter, and other parameters as required to handle the specific formatting details for your data. For example:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    FIRSTROW = 2) AS rows\n\n\nThe PARSER_VERSION is used to determine how the query interprets the text encoding used in the files. Version 1.0 is the default and supports a wide range of file encodings, while version 2.0 supports fewer encodings but offers better performance. The FIRSTROW parameter is used to skip rows in the text file, to eliminate any unstructured preamble text or to ignore a row containing column headings.\n\nAdditional parameters you might require when working with delimited text files include:\n\nFIELDTERMINATOR - the character used to separate field values in each row. For example, a tab-delimited file separates fields with a TAB (\\t) character. The default field terminator is a comma (,).\nROWTERMINATOR - the character used to signify the end of a row of data. For example, a standard Windows text file uses a combination of a carriage return (CR) and line feed (LF), which is indicated by the code \\n; while UNIX-style text files use a single line feed character, which can be indicated using the code 0x0a.\nFIELDQUOTE - the character used to enclose quoted string values. For example, to ensure that the comma in the address field value 126 Main St, apt 2 isn't interpreted as a field delimiter, you might enclose the entire field value in quotation marks like this: \"126 Main St, apt 2\". The double-quote (\") is the default field quote character.\n\n Tip\n\nFor details of additional parameters when working with delimited text files, refer to the Azure Synapse Analytics documentation.\n\nSpecifying the rowset schema\n\nIt's common for delimited text files to include the column names in the first row. The OPENROWSET function can use this to define the schema for the resulting rowset, and automatically infer the data types of the columns based on the values they contain. For example, consider the following delimited text:\n\nproduct_id,product_name,list_price\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThe data consists of the following three columns:\n\nproduct_id (integer number)\nproduct_name (string)\nlist_price (decimal number)\n\nYou could use the following query to extract the data with the correct column names and appropriately inferred SQL Server data types (in this case INT, NVARCHAR, and DECIMAL)\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE) AS rows\n\n\nThe HEADER_ROW parameter (which is only available when using parser version 2.0) instructs the query engine to use the first row of data in each file as the column names, like this:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nNow consider the following data:\n\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThis time, the file doesn't contain the column names in a header row; so while the data types can still be inferred, the column names will be set to C1, C2, C3, and so on.\n\nExpand table\nC1\tC2\tC3\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nTo specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a WITH clause, like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0')\nWITH (\n    product_id INT,\n    product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8,\n    list_price DECIMAL(5,2)\n) AS rows\n\n\nThis query produces the expected results:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.99\n124\tGadget\t3.99\n\n Tip\n\nWhen working with text files, you may encounter some incompatibility with UTF-8 encoded data and the collation used in the master database for the serverless SQL pool. To overcome this, you can specify a compatible collation for individual VARCHAR columns in the schema. See the troubleshooting guidance for more details.\n\nQuerying JSON files\n\nJSON is a popular format for web applications that exchange data through REST interfaces or use NoSQL data stores such as Azure Cosmos DB. So, it's not uncommon to persist data as JSON documents in files in a data lake for analysis.\n\nFor example, a JSON file that defines an individual product might look like this:\n\n{\n    \"product_id\": 123,\n    \"product_name\": \"Widget\",\n    \"list_price\": 12.99\n}\n\n\nTo return product data from a folder containing multiple JSON files in this format, you could use the following SQL query:\n\nSELECT doc\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nOPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:\n\nExpand table\ndoc\n{\"product_id\":123,\"product_name\":\"Widget\",\"list_price\": 12.99}\n{\"product_id\":124,\"product_name\":\"Gadget\",\"list_price\": 3.99}\n\nTo extract individual values from the JSON, you can use the JSON_VALUE function in the SELECT statement, as shown here:\n\nSELECT JSON_VALUE(doc, '$.product_name') AS product,\n           JSON_VALUE(doc, '$.list_price') AS price\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nThis query would return a rowset similar to the following results:\n\nExpand table\nproduct\tprice\nWidget\t12.99\nGadget\t3.99\nQuerying Parquet files\n\nParquet is a commonly used format for big data processing on distributed file storage. It's an efficient data format that is optimized for compression and analytical querying.\n\nIn most cases, the schema of the data is embedded within the Parquet file, so you only need to specify the BULK parameter with a path to the file(s) you want to read, and a FORMAT parameter of parquet; like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.*',\n    FORMAT = 'parquet') AS rows\n\nQuery partitioned data\n\nIt's common in a data lake to partition data by splitting across multiple files in subfolders that reflect partitioning criteria. This enables distributed processing systems to work in parallel on multiple partitions of the data, or to easily eliminate data reads from specific folders based on filtering criteria. For example, suppose you need to efficiently process sales order data, and often need to filter based on the year and month in which orders were placed. You could partition the data using folders, like this:\n\n/orders\n/year=2020\n/month=1\n/01012020.parquet\n/02012020.parquet\n...\n/month=2\n/01022020.parquet\n/02022020.parquet\n...\n...\n/year=2021\n/month=1\n/01012021.parquet\n/02012021.parquet\n...\n...\n\nTo create a query that filters the results to include only the orders for January and February 2020, you could use the following code:\n\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*',\n    FORMAT = 'parquet') AS orders\nWHERE orders.filepath(1) = '2020'\n    AND orders.filepath(2) IN ('1','2');\n\n\nThe numbered filepath parameters in the WHERE clause reference the wildcards in the folder names in the BULK path -so the parameter 1 is the * in the year=* folder name, and parameter 2 is the * in the month=* folder name.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Synapse serverless SQL pool capabilities and use cases - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/2-understand-serverless-pools",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\n5 minutes\n\nAzure Synapse Analytics is an integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale. One of the most prevalent technologies used in data solutions is SQL - an industry standard language for querying and manipulating data.\n\nServerless SQL pools in Azure Synapse Analytics\n\nAzure Synapse SQL is a distributed query system in Azure Synapse Analytics that offers two kinds of runtime environments:\n\nServerless SQL pool: on-demand SQL query processing, primarily used to work with data in a data lake.\nDedicated SQL pool: Enterprise-scale relational database instances used to host data warehouses in which data is stored in relational tables.\n\nIn this module, we'll focus on serverless SQL pool, which provides a pay-per-query endpoint to query the data in your data lake. The benefits of using serverless SQL pool include:\n\nA familiar Transact-SQL syntax to query data in place without the need to copy or load data into a specialized store.\nIntegrated connectivity from a wide range of business intelligence and ad-hoc querying tools, including the most popular drivers.\nDistributed query processing that is built for large-scale data, and computational functions - resulting in fast query performance.\nBuilt-in query execution fault-tolerance, resulting in high reliability and success rates even for long-running queries involving large data sets.\nNo infrastructure to setup or clusters to maintain. A built-in endpoint for this service is provided within every Azure Synapse workspace, so you can start querying data as soon as the workspace is created.\nNo charge for resources reserved, you're only charged for the data processed by queries you run.\nWhen to use serverless SQL pools\n\nServerless SQL pool is tailored for querying the data residing in the data lake, so in addition to eliminating management burden, it eliminates a need to worry about ingesting the data into the system. You just point the query to the data that is already in the lake and run it.\n\nSynapse SQL serverless resource model is great for unplanned or \"bursty\" workloads that can be processed using the always-on serverless SQL endpoint in your Azure Synapse Analytics workspace. Using the serverless pool helps when you need to know exact cost for each query executed to monitor and attribute costs.\n\n Note\n\nServerless SQL pool is an analytics system and is not recommended for OLTP workloads such as databases used by applications to store transactional data. Workloads that require millisecond response times and are looking to pinpoint a single row in a data set are not good fit for serverless SQL pool.\n\nCommon use cases for serverless SQL pools include:\n\nData exploration: Data exploration involves browsing the data lake to get initial insights about the data, and is easily achievable with Azure Synapse Studio. You can browse through the files in your linked data lake storage, and use the built-in serverless SQL pool to automatically generate a SQL script to select TOP 100 rows from a file or folder just as you would do with a table in SQL Server. From there, you can apply projections, filtering, grouping, and most of the operation over the data as if the data were in a regular SQL Server table.\nData transformation: While Azure Synapse Analytics provides great data transformations capabilities with Synapse Spark, some data engineers might find data transformation easier to achieve using SQL. Serverless SQL pool enables you to perform SQL-based data transformations; either interactively or as part of an automated data pipeline.\nLogical data warehouse: After your initial exploration of the data in the data lake, you can define external objects such as tables and views in a serverless SQL database. The data remains stored in the data lake files, but are abstracted by a relational schema that can be used by client applications and analytical tools to query the data as they would in a relational database hosted in SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Analytics includes serverless SQL pools, which are tailored for querying data in a data lake. With a serverless SQL pool you can use SQL code to query data in files of various common formats without needing to load the file data into database storage. This capability helps data analysts and data engineers analyze and process file data in the data lake using a familiar data processing language, without the need to create or maintain a relational database store.\n\nAfter completing this module, you'll be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with the Microsoft Azure portal\nFamiliarity with data lake and data warehouse concepts\nExperience of using SQL to query database tables\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Synapse serverless SQL pool to query files in a data lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Azure Synapse serverless SQL pool to query files in a data lake\nModule\n7 Units\nFeedback\nBeginner\nData Engineer\nAzure Synapse Analytics\n\nWith Azure Synapse serverless SQL pool, you can leverage your SQL skills to explore and analyze data in files, without the need to load the data into a relational database.\n\nLearning objectives\n\nAfter the completion of this module, you will be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nAdd\nPrerequisites\n\nConsider completing the Explore data analytics in Azure and Get started querying with Transact-SQL learning paths before starting this module. You will need knowledge of:\n\nAnalytical data workloads in Microsoft Azure\nQuerying data with Transact-SQL\nIntroduction\nmin\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\nmin\nQuery files using a serverless SQL pool\nmin\nCreate external database objects\nmin\nExercise - Query files using a serverless SQL pool\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Analytics provides an integrated cloud-based platform for big data processing and analysis. You can use it to build descriptive, diagnostic, predictive, and prescriptive analytics solutions.\n\nIn this module, you learned how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\n\n Tip\n\nTo learn more about the capabilities of Azure Synapse Analytics, see What is Azure Synapse Analytics? in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/4a-exercise-explore-synapse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore Azure Synapse Analytics\n60 minutes\n\nNow it's your chance to explore the capabilities of Azure Synapse Analytics for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use Azure Synapse Studio to perform core data analytics tasks.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich feature of Azure Synapse Analytics enables you to transfer data from one store to another and apply transformations to the data at scheduled intervals?\n\n \n\nServerless SQL pool\n\nApache Spark pool\n\nPipelines\n\n2. \n\nYou want to create a data warehouse in Azure Synapse Analytics in which the data is stored and queried in a relational data store. What kind of pool should you create?\n\n \n\nServerless SQL pool\n\nDedicated SQL pool\n\nApache Spark pool\n\n3. \n\nA data analyst wants to analyze data by using Python code combined with text descriptions of the insights gained from the analysis. What should they use to perform the analysis?\n\n \n\nA notebook connected to an Apache Spark pool.\n\nA SQL script connected to a serverless SQL pool.\n\nA KQL script connected to a Data Explorer pool.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "When to use Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/4-when-use",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhen to use Azure Synapse Analytics\n4 minutes\n\nAcross all organizations and industries, the common use cases for Azure Synapse Analytics are identified by the need for:\n\nLarge-scale data warehousing\n\nData warehousing includes the need to integrate all data, including big data, to reason over data for analytics and reporting purposes from a descriptive analytics perspective, independent of its location or structure.\n\nAdvanced analytics\n\nEnables organizations to perform predictive analytics using both the native features of Azure Synapse Analytics, and integrating with other technologies such as Azure Machine Learning.\n\nData exploration and discovery\n\nThe serverless SQL pool functionality provided by Azure Synapse Analytics enables Data Analysts, Data Engineers and Data Scientist alike to explore the data within your data estate. This capability supports data discovery, diagnostic analytics, and exploratory data analysis.\n\nReal time analytics\n\nAzure Synapse Analytics can capture, store and analyze data in real-time or near-real time with features such as Azure Synapse Link, or through the integration of services such as Azure Stream Analytics and Azure Data Explorer.\n\nData integration\n\nAzure Synapse Pipelines enables you to ingest, prepare, model and serve the data to be used by downstream systems. This can be used by components of Azure Synapse Analytics exclusively.\n\nIntegrated analytics\n\nWith the variety of analytics that can be performed on the data at your disposal, putting together the services in a cohesive solution can be a complex operation. Azure Synapse Analytics removes this complexity by integrating the analytics landscape into one service. That way you can spend more time working with the data to bring business benefit, than spending much of your time provisioning and maintaining multiple systems to achieve the same outcomes.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "How Azure Synapse Analytics works - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/3-how-works",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nHow Azure Synapse Analytics works\n8 minutes\n\nTo support the analytics needs of today's organizations, Azure Synapse Analytics combines a centralized service for data storage and processing with an extensible architecture through which linked services enable you to integrate commonly used data stores, processing platforms, and visualization tools.\n\nCreating and using an Azure Synapse Analytics workspace\n\nA Synapse Analytics workspace defines an instance of the Synapse Analytics service in which you can manage the services and data resources needed for your analytics solution. You can create a Synapse Analytics workspace in an Azure subscription interactively by using the Azure portal, or you can automate deployment by using Azure PowerShell, the Azure command-line interface (CLI), or with an Azure Resource Manager or Bicep template.\n\nAfter creating a Synapse Analytics workspace, you can manage the services in it and perform data analytics tasks with them by using Synapse Studio; a web-based portal for Azure Synapse Analytics.\n\nWorking with files in a data lake\n\nOne of the core resources in a Synapse Analytics workspace is a data lake, in which data files can be stored and processed at scale. A workspace typically has a default data lake, which is implemented as a linked service to an Azure Data Lake Storage Gen2 container. You can add linked services for multiple data lakes that are based on different storage platforms as required.\n\nIngesting and transforming data with pipelines\n\nIn most enterprise data analytics solutions, data is extracted from multiple operational sources and transferred to a central data lake or data warehouse for analysis. Azure Synapse Analytics includes built-in support for creating, running, and managing pipelines that orchestrate the activities necessary to retrieve data from a range of sources, transform the data as required, and load the resulting transformed data into an analytical store.\n\n Note\n\nPipelines in Azure Synapse Analytics are based on the same underlying technology as Azure Data Factory. If you are already familiar with Azure Data Factory, you can leverage your existing skills to build data ingestion and transformation solutions in Azure Synapse Analytics.\n\nQuerying and manipulating data with SQL\n\nStructured Query Language (SQL) is a ubiquitous language for querying and manipulating data, and is the foundation for relational databases, including the popular Microsoft SQL Server database platform. Azure Synapse Analytics supports SQL-based data querying and manipulation through two kinds of SQL pool that are based on the SQL Server relational database engine:\n\nA built-in serverless pool that is optimized for using relational SQL semantics to query file-based data in a data lake.\nCustom dedicated SQL pools that host relational data warehouses.\n\nThe Azure Synapse SQL system uses a distributed query processing model to parallelize SQL operations, resulting in a highly scalable solution for relational data processing. You can use the built-in serverless pool for cost-effective analysis and processing of file data in the data lake, and use dedicated SQL pools to create relational data warehouses for enterprise data modeling and reporting.\n\nProcessing and analyzing data with Apache Spark\n\nApache Spark is an open source platform for big data analytics. Spark performs distributed processing of files in a data lake by running jobs that can be implemented using any of a range of supported programming languages. Languages supported in Spark include Python, Scala, Java, SQL, and C#.\n\nIn Azure Synapse Analytics, you can create one or more Spark pools and use interactive notebooks to combine code and notes as you build solutions for data analytics, machine learning, and data visualization.\n\nExploring data with Data Explorer\n\nAzure Synapse Data Explorer is a data processing engine in Azure Synapse Analytics that is based on the Azure Data Explorer service. Data Explorer uses an intuitive query syntax named Kusto Query Language (KQL) to enable high performance, low-latency analysis of batch and streaming data.\n\nIntegrating with other Azure data services\n\nAzure Synapse Analytics can be integrated with other Azure data services for end-to-end analytics solutions. Integrated solutions include:\n\nAzure Synapse Link enables near-realtime synchronization between operational data in Azure Cosmos DB, Azure SQL Database, SQL Server, and Microsoft Power Platform Dataverse and analytical data storage that can be queried in Azure Synapse Analytics.\nMicrosoft Power BI integration enables data analysts to integrate a Power BI workspace into a Synapse workspace, and perform interactive data visualization in Azure Synapse Studio.\nMicrosoft Purview integration enables organizations to catalog data assets in Azure Synapse Analytics, and makes it easier for data engineers to find data assets and track data lineage when implementing data pipelines that ingest data into Azure Synapse Analytics.\nAzure Machine Learning integration enables data analysts and data scientists to integrate predictive model training and consumption into analytical solutions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "What is Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/2-what-happening-business",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhat is Azure Synapse Analytics\n5 minutes\n\nThe technological research and consulting firm Gartner defines four common types of analytical technique that organizations commonly use:\n\nDescriptive analytics, which answers the question “What is happening in my business?”. The data to answer this question is typically answered through the creation of a data warehouse in which historical data is persisted in relational tables for multidimensional modeling and reporting.\n\nDiagnostic analytics, which deals with answering the question “Why is it happening?”. This may involve exploring information that already exists in a data warehouse, but typically involves a wider search of your data estate to find more data to support this type of analysis.\n\nPredictive analytics, which enables you to answer the question “What is likely to happen in the future based on previous trends and patterns?”\n\nPrescriptive analytics, which enables autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics.\n\nAzure Synapse Analytics provides a cloud platform for all of these analytical workloads through support for multiple data storage, processing, and analysis technologies in a single, integrated solution. The integrated design of Azure Synapse Analytics enables organizations to leverage investments and skills in multiple commonly used data technologies, including SQL, Apache Spark, and others; while providing a centrally managed service and a single, consistent user interface.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nThe volume of data generated by individuals and organizations is growing at a phenomenal rate. This data powers businesses and other organizations by providing a basis for descriptive, diagnostic, predictive, and prescriptive analytical solutions that support decision making and autonomous systems by providing real-time insights into established and emerging patterns.\n\nOrganizations have a choice of many tools and techniques for data analytics, often requiring expertise across multiple systems and complex integration of infrastructure and administrative operations. Azure Synapse Analytics provides a single, cloud-scale platform that supports multiple analytical technologies; enabling a consolidated and integrated experience for data engineers, data analysts, data scientists, and other professionals who need to work with data.\n\nIn this module, you'll learn how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\nPrerequisites\n\nBefore completing this module, you should have the following prerequisite knowledge and experience:\n\nFamiliarity with cloud computing concepts and Microsoft Azure.\nFamiliarity with fundamental data concepts.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction to Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIntroduction to Azure Synapse Analytics\nModule\n7 Units\nFeedback\nBeginner\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nLearn about the features and capabilities of Azure Synapse Analytics - a cloud-based platform for big data processing and analysis.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\nAdd\nPrerequisites\n\nBefore completing this module, you should have the following prerequisite knowledge and experience:\n\nFamiliarity with cloud computing concepts and Microsoft Azure.\nFamiliarity with fundamental data concepts.\nIntroduction\nmin\nWhat is Azure Synapse Analytics\nmin\nHow Azure Synapse Analytics works\nmin\nWhen to use Azure Synapse Analytics\nmin\nExercise - Explore Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Data Lake Storage Gen2 provides a cloud storage service that is available, secure, durable, scalable, and redundant. It's a comprehensive data lake solution.\n\nAzure Data Lake Storage brings efficiencies to process big data analytics workloads and can provide data to many compute technologies including Azure Synapse Analytics, Azure HDInsight, and Azure Databricks without needing to move the data around. Creating an Azure Data Lake Storage Gen2 data store can be an important tool in building a big data analytics solution.\n\n Tip\n\nTo learn more about Azure Data Lake Storage Gen2, see Introduction to Azure Data Lake Storage Gen2 in the Microsoft Azure documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nAzure Data Lake Storage Gen2 stores data in…\n\n \n\nA document database hosted in Azure Cosmos DB.\n\nAn HDFS-compatible file system hosted in Azure Storage.\n\nA relational data warehouse hosted in Azure Synapse Analytics.\n\n2. \n\nWhat option must you enable to use Azure Data Lake Storage Gen2?\n\n \n\nGlobal replication\n\nData encryption\n\nHierarchical namespace\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Data Lake Storage Gen2 in data analytics workloads - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/6-use-cases",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Introduction to Azure Data Lake Storage Gen2 \nAdd\nPrevious\nUnit 6 of 8\nNext\nUse Azure Data Lake Storage Gen2 in data analytics workloads\nCompleted\n100 XP\n5 minutes\n\nAzure Data Lake Store Gen2 is an enabling technology for multiple data analytics use cases. Let's explore a few common types of analytical workload, and identify how Azure Data Lake Storage Gen2 works with other Azure services to support them.\n\nBig data processing and analytics\n\nBig data scenarios usually refer to analytical workloads that involve massive volumes of data in a variety of formats that needs to be processed at a fast velocity - the so-called \"three v's\". Azure Data Lake Storage Gen 2 provides a scalable and secure distributed data store on which big data services such as Azure Synapse Analytics, Azure Databricks, and Azure HDInsight can apply data processing frameworks such as Apache Spark, Hive, and Hadoop. The distributed nature of the storage and the processing compute enables tasks to be performed in parallel, resulting in high-performance and scalability even when processing huge amounts of data.\n\nData warehousing\n\nData warehousing has evolved in recent years to integrate large volumes of data stored as files in a data lake with relational tables in a data warehouse. In a typical example of a data warehousing solution, data is extracted from operational data stores, such as Azure SQL database or Azure Cosmos DB, and transformed into structures more suitable for analytical workloads. Often, the data is staged in a data lake in order to facilitate distributed processing before being loaded into a relational data warehouse. In some cases, the data warehouse uses external tables to define a relational metadata layer over files in the data lake and create a hybrid \"data lakehouse\" or \"lake database\" architecture. The data warehouse can then support analytical queries for reporting and visualization.\n\nThere are multiple ways to implement this kind of data warehousing architecture. The diagram shows a solution in which Azure Synapse Analytics hosts pipelines to perform extract, transform, and load (ETL) processes using Azure Data Factory technology. These processes extract data from operational data sources and load it into a data lake hosted in an Azure Data Lake Storage Gen2 container. The data is then processed and loaded into a relational data warehouse in an Azure Synapse Analytics dedicated SQL pool, from where it can support data visualization and reporting using Microsoft Power BI.\n\nReal-time data analytics\n\nIncreasingly, businesses and other organizations need to capture and analyze perpetual streams of data, and analyze it in real-time (or as near to real-time as possible). These streams of data can be generated from connected devices (often referred to as internet-of-things or IoT devices) or from data generated by users in social media platforms or other applications. Unlike traditional batch processing workloads, streaming data requires a solution that can capture and process a boundless stream of data events as they occur.\n\nStreaming events are often captured in a queue for processing. There are multiple technologies you can use to perform this task, including Azure Event Hubs as shown in the image. From here, the data is processed, often to aggregate data over temporal windows (for example to count the number of social media messages with a given tag every five minutes, or to calculate the average reading of an Internet connected sensor per minute). Azure Stream Analytics enables you to create jobs that query and aggregate event data as it arrives, and write the results in an output sink. One such sink is Azure Data Lake Storage Gen2; from where the captured real-time data can be analyzed and visualized.\n\nData science and machine learning\n\nData science involves the statistical analysis of large volumes of data, often using tools such as Apache Spark and scripting languages such as Python. Azure Data Lake Storage Gen 2 provides a highly scalable cloud-based data store for the volumes of data required in data science workloads.\n\nMachine learning is a subarea of data science that deals with training predictive models. Model training requires huge amounts of data, and the ability to process that data efficiently. Azure Machine Learning is a cloud service in which data scientists can run Python code in notebooks using dynamically allocated distributed compute resources. The compute processes data in Azure Data Lake Storage Gen2 containers to train models, which can then be deployed as production web services to support predictive analytical workloads.\n\nNext unit: Knowledge check\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand the stages for processing big data - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/5-stages-for-processing-big-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand the stages for processing big data\n5 minutes\n\nData lakes have a fundamental role in a wide range of big data architectures. These architectures can involve the creation of:\n\nAn enterprise data warehouse.\nAdvanced analytics against big data.\nA real-time analytical solution.\n\nThere are four stages for processing big data solutions that are common to all architectures:\n\nIngest - The ingestion phase identifies the technology and processes that are used to acquire the source data. This data can come from files, logs, and other types of unstructured data that must be put into the data lake. The technology that is used will vary depending on the frequency that the data is transferred. For example, for batch movement of data, pipelines in Azure Synapse Analytics or Azure Data Factory may be the most appropriate technology to use. For real-time ingestion of data, Apache Kafka for HDInsight or Stream Analytics may be an appropriate choice.\nStore - The store phase identifies where the ingested data should be placed. Azure Data Lake Storage Gen2 provides a secure and scalable storage solution that is compatible with commonly used big data processing technologies.\nPrep and train - The prep and train phase identifies the technologies that are used to perform data preparation and model training and scoring for machine learning solutions. Common technologies that are used in this phase are Azure Synapse Analytics, Azure Databricks, Azure HDInsight, and Azure Machine Learning.\nModel and serve - Finally, the model and serve phase involves the technologies that will present the data to users. These technologies can include visualization tools such as Microsoft Power BI, or analytical data stores such as Azure Synapse Analytics. Often, a combination of multiple technologies will be used depending on the business requirements.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enable Azure Data Lake Storage Gen2 in Azure Storage - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-data-lake-storage/3-create-data-lake-account",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEnable Azure Data Lake Storage Gen2 in Azure Storage\n5 minutes\n\nAzure Data Lake Storage Gen2 isn't a standalone Azure service, but rather a configurable capability of a StorageV2 (General Purpose V2) Azure Storage.\n\nTo enable Azure Data Lake Storage Gen2 in an Azure Storage account, you can select the option to Enable hierarchical namespace in the Advanced page when creating the storage account in the Azure portal:\n\nAlternatively, if you already have an Azure Storage account and want to enable the Azure data Lake Storage Gen2 capability, you can use the Data Lake Gen2 upgrade wizard in the Azure portal page for your storage account resource.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]