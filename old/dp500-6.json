[
  {
    "title": "Create data warehouse tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-create-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate data warehouse tables\n10 minutes\n\nNow that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse.\n\nCreating a dedicated SQL pool\n\nTo create a relational data warehouse in Azure Synapse Analytics, you must create a dedicated SQL Pool. The simplest way to do this in an existing Azure Synapse Analytics workspace is to use the Manage page in Azure Synapse Studio, as shown here:\n\nWhen provisioning a dedicated SQL pool, you can specify the following configuration settings:\n\nA unique name for the dedicated SQL pool.\nA performance level for the SQL pool, which can range from DW100c to DW30000c and which determines the cost per hour for the pool when it's running.\nWhether to start with an empty pool or restore an existing database from a backup.\nThe collation of the SQL pool, which determines sort order and string comparison rules for the database. (You can't change the collation after creation).\n\nAfter creating a dedicated SQL pool, you can control its running state in the Manage page of Synapse Studio; pausing it when not required to prevent unnecessary costs.\n\nWhen the pool is running, you can explore it on the Data page, and create SQL scripts to run in it.\n\nConsiderations for creating tables\n\nTo create tables in the dedicated SQL pool, you use the CREATE TABLE (or sometimes the CREATE EXTERNAL TABLE) Transact-SQL statement. The specific options used in the statement depend on the type of table you're creating, which can include:\n\nFact tables\nDimension tables\nStaging tables\n\n Note\n\nThe data warehouse is composed of fact and dimension tables as discussed previously. Staging tables are often used as part of the data warehousing loading process to ingest data from source systems.\n\nWhen designing a star schema model for small or medium sized datasets you can use your preferred database, such as Azure SQL. For larger data sets you may benefit from implementing your data warehouse in Azure Synapse Analytics instead of SQL Server. It's important to understand some key differences when creating tables in Synapse Analytics.\n\nData integrity constraints\n\nDedicated SQL pools in Synapse Analytics don't support foreign key and unique constraints as found in other relational database systems like SQL Server. This means that jobs used to load data must maintain uniqueness and referential integrity for keys, without relying on the table definitions in the database to do so.\n\n Tip\n\nFor more information about constraints in Azure Synapse Analytics dedicated SQL pools, see Primary key, foreign key, and unique key using dedicated SQL pool in Azure Synapse Analytics.\n\nIndexes\n\nWhile Synapse Analytics dedicated SQL pools support clustered indexes as found in SQL Server, the default index type is clustered columnstore. This index type offers a significant performance advantage when querying large quantities of data in a typical data warehouse schema and should be used where possible. However, some tables may include data types that can't be included in a clustered columnstore index (for example, VARBINARY(MAX)), in which case a clustered index can be used instead.\n\n Tip\n\nFor more information about indexing in Azure Synapse Analytics dedicated SQL pools, see Indexes on dedicated SQL pool tables in Azure Synapse Analytics.\n\nDistribution\n\nAzure Synapse Analytics dedicated SQL pools use a massively parallel processing (MPP) architecture, as opposed to the symmetric multiprocessing (SMP) architecture used in most OLTP database systems. In an MPP system, the data in a table is distributed for processing across a pool of nodes. Synapse Analytics supports the following kinds of distribution:\n\nHash: A deterministic hash value is calculated for the specified column and used to assign the row to a compute node.\nRound-robin: Rows are distributed evenly across all compute nodes.\nReplicated: A copy of the table is stored on each compute node.\n\nThe table type often determines which option to choose for distributing the table.\n\nExpand table\nTable type\tRecommended distribution option\nDimension\tUse replicated distribution for smaller tables to avoid data shuffling when joining to distributed fact tables. If tables are too large to store on each compute node, use hash distribution.\nFact\tUse hash distribution with clustered columnstore index to distribute fact tables across compute nodes.\nStaging\tUse round-robin distribution for staging tables to evenly distribute data across compute nodes.\n\n Tip\n\nFor more information about distribution strategies for tables in Azure Synapse Analytics, see Guidance for designing distributed tables using dedicated SQL pool in Azure Synapse Analytics.\n\nCreating dimension tables\n\nWhen you create a dimension table, ensure that the table definition includes surrogate and alternate keys as well as columns for the attributes of the dimension that you want to use to group aggregations. It's often easiest to use an IDENTITY column to auto-generate an incrementing surrogate key (otherwise you need to generate unique keys every time you load data). The following example shows a CREATE TABLE statement for a hypothetical DimCustomer dimension table.\n\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Note\n\nIf desired, you can create a specific schema as a namespace for your tables. In this example, the default dbo schema is used.\n\nIf you intend to use a snowflake schema in which dimension tables are related to one another, you should include the key for the parent dimension in the definition of the child dimension table. For example, the following SQL code could be used to move the geographical address details from the DimCustomer table to a separate DimGeography dimension table:\n\nCREATE TABLE dbo.DimGeography\n(\n    GeographyKey INT IDENTITY NOT NULL,\n    GeographyAlternateKey NVARCHAR(10) NULL,\n    StreetAddress NVARCHAR(100),\n    City NVARCHAR(20),\n    PostalCode NVARCHAR(10),\n    CountryRegion NVARCHAR(20)\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE dbo.DimCustomer\n(\n    CustomerKey INT IDENTITY NOT NULL,\n    CustomerAlternateKey NVARCHAR(15) NULL,\n    GeographyKey INT NULL,\n    CustomerName NVARCHAR(80) NOT NULL,\n    EmailAddress NVARCHAR(50) NULL,\n    Phone NVARCHAR(25) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nTime dimension tables\n\nMost data warehouses include a time dimension table that enables you to aggregate data by multiple hierarchical levels of time interval. For example, the following example creates a DimDate table with attributes that relate to specific dates.\n\nCREATE TABLE dbo.DimDate\n( \n    DateKey INT NOT NULL,\n    DateAltKey DATETIME NOT NULL,\n    DayOfMonth INT NOT NULL,\n    DayOfWeek INT NOT NULL,\n    DayName NVARCHAR(15) NOT NULL,\n    MonthOfYear INT NOT NULL,\n    MonthName NVARCHAR(15) NOT NULL,\n    CalendarQuarter INT  NOT NULL,\n    CalendarYear INT NOT NULL,\n    FiscalQuarter INT NOT NULL,\n    FiscalYear INT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n\n Tip\n\nA common pattern when creating a dimension table for dates is to use the numeric date in DDMMYYYY or YYYYMMDD format as an integer surrogate key, and the date as a DATE or DATETIME datatype as the alternate key.\n\nCreating fact tables\n\nFact tables include the keys for each dimension to which they're related, and the attributes and numeric measures for specific events or observations that you want to analyze.\n\nThe following code example creates a hypothetical fact table named FactSales that is related to multiple dimensions through key columns (date, customer, product, and store)\n\nCREATE TABLE dbo.FactSales\n(\n    OrderDateKey INT NOT NULL,\n    CustomerKey INT NOT NULL,\n    ProductKey INT NOT NULL,\n    StoreKey INT NOT NULL,\n    OrderNumber NVARCHAR(10) NOT NULL,\n    OrderLineItem INT NOT NULL,\n    OrderQuantity SMALLINT NOT NULL,\n    UnitPrice DECIMAL NOT NULL,\n    Discount DECIMAL NOT NULL,\n    Tax DECIMAL NOT NULL,\n    SalesAmount DECIMAL NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(OrderNumber),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCreating staging tables\n\nStaging tables are used as temporary storage for data as it's being loaded into the data warehouse. A typical pattern is to structure the table to make it as efficient as possible to ingest the data from its external source (often files in a data lake) into the relational database, and then use SQL statements to load the data from the staging tables into the dimension and fact tables.\n\nThe following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n\nCREATE TABLE dbo.StageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nUsing external tables\n\nIn some cases, if the data to be loaded is in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Synapse workspace:\n\n\n-- External data source links to data lake location\nCREATE EXTERNAL DATA SOURCE StagedFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/stagedfiles/'\n);\nGO\n\n-- External format specifies file format\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\nGO\n\n-- External table references files in external data source\nCREATE EXTERNAL TABLE dbo.ExternalStageProduct\n(\n    ProductID NVARCHAR(10) NOT NULL,\n    ProductName NVARCHAR(200) NOT NULL,\n    ProductCategory NVARCHAR(200) NOT NULL,\n    Color NVARCHAR(10),\n    Size NVARCHAR(10),\n    ListPrice DECIMAL NOT NULL,\n    Discontinued BIT NOT NULL\n)\nWITH\n(\n    DATA_SOURCE = StagedFiles,\n    LOCATION = 'products/*.parquet',\n    FILE_FORMAT = ParquetFormat\n);\nGO\n\n\n Note\n\nFor more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nRelational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse.\n\nAzure Synapse Analytics includes a highly scalable relational database engine that is optimized for data warehousing workloads. By using dedicated SQL pools in Azure Synapse Analytics, you can create databases that are capable of hosting and querying huge volumes of data in relational tables.\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Design a data warehouse schema - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/2-design-star-schema",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nDesign a data warehouse schema\n7 minutes\n\nLike all relational databases, a data warehouse contains tables in which the data you want to analyze is stored. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling, in which numerical measures associated with events known as facts can be aggregated by the attributes of associated entities across multiple dimensions. For example, measures associated with a sales order (such as the amount paid or the quantity of items ordered) can be aggregated by attributes of the date on which the sale occurred, the customer, the store, and so on.\n\nTables in a data warehouse\n\nA common pattern for relational data warehouses is to define a schema that includes two kinds of table: dimension tables and fact tables.\n\nDimension tables\n\nDimension tables describe business entities, such as products, people, places, and dates. Dimension tables contain columns for attributes of an entity. For example, a customer entity might have a first name, a last name, an email address, and a postal address (which might consist of a street address, a city, a postal code, and a country or region). In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include two key columns:\n\na surrogate key that is specific to the data warehouse and uniquely identifies each row in the dimension table in the data warehouse - usually an incrementing integer number.\nAn alternate key, often a natural or business key that is used to identify a specific instance of an entity in the transactional source system from which the entity record originated - such as a product code or a customer ID.\n\n Note\n\nWhy have two keys? There are a few good reasons:\n\nThe data warehouse may be populated with data from multiple source systems, which can lead to the risk of duplicate or incompatible business keys.\nSimple numeric keys generally perform better in queries that join lots of tables - a common pattern in data warehouses.\nAttributes of entities may change over time - for example, a customer might change their address. Since the data warehouse is used to support historic reporting, you may want to retain a record for each instance of an entity at multiple points in time; so that, for example, sales orders for a specific customer are counted for the city where they lived at the time the order was placed. In this case, multiple customer records would have the same business key associated with the customer, but different surrogate keys for each discrete address where the customer lived at various times.\n\nAn example of a dimension table for customer might contain the following data:\n\nExpand table\nCustomerKey\tCustomerAltKey\tName\tEmail\tStreet\tCity\tPostalCode\tCountryRegion\n123\tI-543\tNavin Jones\tnavin1@contoso.com\t1 Main St.\tSeattle\t90000\tUnited States\n124\tR-589\tMary Smith\tmary2@contoso.com\t234 190th Ave\tBuffalo\t50001\tUnited States\n125\tI-321\tAntoine Dubois\tantoine1@contoso.com\t2 Rue Jolie\tParis\t20098\tFrance\n126\tI-543\tNavin Jones\tnavin1@contoso.com\t24 125th Ave.\tNew York\t50000\tUnited States\n...\t...\t...\t...\t...\t...\t...\t...\n\n Note\n\nObserve that the table contains two records for Navin Jones. Both records use the same alternate key to identify this person (I-543), but each record has a different surrogate key. From this, you can surmise that the customer moved from Seattle to New York. Sales made to the customer while living in Seattle are associated with the key 123, while purchases made after moving to New York are recorded against record 126.\n\nIn addition to dimension tables that represent business entities, it's common for a data warehouse to include a dimension table that represents time. This table enables data analysts to aggregate data over temporal intervals. Depending on the type of data you need to analyze, the lowest granularity (referred to as the grain) of a time dimension could represent times (to the hour, second, millisecond, nanosecond, or even lower), or dates.\n\nAn example of a time dimension table with a grain at the date level might contain the following data:\n\nExpand table\nDateKey\tDateAltKey\tDayOfWeek\tDayOfMonth\tWeekday\tMonth\tMonthName\tQuarter\tYear\n19990101\t01-01-1999\t6\t1\tFriday\t1\tJanuary\t1\t1999\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20220101\t01-01-2022\t7\t1\tSaturday\t1\tJanuary\t1\t2022\n20220102\t02-01-2022\t1\t2\tSunday\t1\tJanuary\t1\t2022\n...\t...\t...\t...\t...\t...\t...\t...\t...\n20301231\t31-12-2030\t3\t31\tTuesday\t12\tDecember\t4\t2030\n\nThe timespan covered by the records in the table must include the earliest and latest points in time for any associated events recorded in a related fact table. Usually there's a record for every interval at the appropriate grain in between.\n\nFact tables\n\nFact tables store details of observations or events; for example, sales orders, stock balances, exchange rates, or recorded temperatures. A fact table contains columns for numeric values that can be aggregated by dimensions. In addition to the numeric columns, a fact table contains key columns that reference unique keys in related dimension tables.\n\nFor example, a fact table containing details of sales orders might contain the following data:\n\nExpand table\nOrderDateKey\tCustomerKey\tStoreKey\tProductKey\tOrderNo\tLineItemNo\tQuantity\tUnitPrice\tTax\tItemTotal\t\n20220101\t123\t5\t701\t1001\t1\t2\t2.50\t0.50\t5.50\t\n20220101\t123\t5\t765\t1001\t2\t1\t2.00\t0.20\t2.20\t\n20220102\t125\t2\t723\t1002\t1\t1\t4.99\t0.49\t5.48\t\n20220103\t126\t1\t823\t1003\t1\t1\t7.99\t0.80\t8.79\t\n...\t...\t...\t...\t...\t...\t...\t...\t\t...\t...\n\nA fact table's dimension key columns determine its grain. For example, the sales orders fact table includes keys for dates, customers, stores, and products. An order might include multiple products, so the grain represents line items for individual products sold in stores to customers on specific days.\n\nData warehouse schema designs\n\nIn most transactional databases that are used in business applications, the data is normalized to reduce duplication. In a data warehouse however, the dimension data is generally de-normalized to reduce the number of joins required to query the data.\n\nOften, a data warehouse is organized as a star schema, in which a fact table is directly related to the dimension tables, as shown in this example:\n\n]\n\nThe attributes of an entity can be used to aggregate measures in fact tables over multiple hierarchical levels - for example, to find total sales revenue by country or region, city, postal code, or individual customer. The attributes for each level can be stored in the same dimension table. However, when an entity has a large number of hierarchical attribute levels, or when some attributes can be shared by multiple dimensions (for example, both customers and stores have a geographical address), it can make sense to apply some normalization to the dimension tables and create a snowflake schema, as shown in the following example:\n\nIn this case, the DimProduct table has been normalized to create separate dimension tables for product categories and suppliers, and a DimGeography table has been added to represent geographical attributes for both customers and stores. Each row in the DimProduct table contains key values for the corresponding rows in the DimCategory and DimSupplier tables; and each row in the DimCustomer and DimStore tables contains a key value for the corresponding row in the DimGeography table.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data in a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAnalyze data in a relational data warehouse\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nRelational data warehouses are a core element of most enterprise Business Intelligence (BI) solutions, and are used as the basis for data models, reports, and analysis.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nDesign a schema for a relational data warehouse.\nCreate fact, dimension, and staging tables.\nUse SQL to load data into data warehouse tables.\nUse SQL to query relational data warehouse tables.\nAdd\nPrerequisites\n\nBefore taking this module, you should have:\n\nAn understanding of data fundamentals.\nExperience of querying data with Transact-SQL.\nIntroduction\nmin\nDesign a data warehouse schema\nmin\nCreate data warehouse tables\nmin\nLoad data warehouse tables\nmin\nQuery a data warehouse\nmin\nExercise - Explore a data warehouse\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nApache Spark is a key technology used in big data analytics, and the Spark pool support in Azure Synapse Analytics enables you to combine big data processing in Spark with large-scale data warehousing in SQL.\n\nIn this module, you learned how to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition best describes Apache Spark?\n\n \n\nA highly scalable relational database management system.\n\nA virtual server with a Python runtime.\n\nA distributed platform for parallel data processing using multiple languages.\n\n2. \n\nYou need to use Spark to analyze data in a parquet file. What should you do?\n\n \n\nLoad the parquet file into a dataframe.\n\nImport the data into a table in a serverless SQL pool.\n\nConvert the data to CSV format.\n\n3. \n\nYou want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?\n\n \n\n%%spark\n\n%%pyspark\n\n%%sql\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Visualize data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/5-visualize-data",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nVisualize data with Spark\n5 minutes\n\nOne of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Synapse Analytics provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.\n\nUsing built-in notebook charts\n\nWhen you display a dataframe or run a SQL query in a Spark notebook in Azure Synapse Analytics, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here:\n\nThe built-in charting functionality in notebooks is useful when you're working with results of a query that don't include any existing groupings or aggregations, and you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.\n\nUsing graphics packages in code\n\nThere are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n\nFor example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.\n\nfrom matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n\n\nThe Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n\nThe chart produced by the code would look similar to the following image:\n\nYou can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/6-exercise-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Analyze data with Spark\n45 minutes\n\nNow it's your opportunity to use a Spark pool in Azure Synapse Analytics. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a Spark pool to analyze and visualize data from files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/4-write-spark-code",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nAnalyze data with Spark\n5 minutes\n\nOne of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Synapse Analytics Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java and Microsoft .NET.\n\nExploring data with dataframes\n\nNatively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.\n\n Note\n\nIn addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.\n\nLoading data into a dataframe\n\nLet's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the primary storage account for an Azure Synapse Analytics workspace:\n\nProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nIn a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:\n\n%%pyspark\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n\n\nThe %%pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example:\n\n%%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://container@store.dfs.core.windows.net/products.csv\")\ndisplay(df.limit(10))\n\n\nThe magic %%spark is used to specify Scala.\n\nBoth of these code samples would produce output like this:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nSpecifying a dataframe schema\n\nIn the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n...\n\n\nThe following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('abfss://container@store.dfs.core.windows.net/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n\n\nThe results would once again be similar to:\n\nExpand table\nProductID\tProductName\tCategory\tListPrice\n771\tMountain-100 Silver, 38\tMountain Bikes\t3399.9900\n772\tMountain-100 Silver, 42\tMountain Bikes\t3399.9900\n773\tMountain-100 Silver, 44\tMountain Bikes\t3399.9900\n...\t...\t...\t...\nFiltering and grouping dataframes\n\nYou can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:\n\npricelist_df = df.select(\"ProductID\", \"ListPrice\")\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductID\tListPrice\n771\t3399.9900\n772\t3399.9900\n773\t3399.9900\n...\t...\n\nIn common with most data manipulation methods, select returns a new dataframe object.\n\n Tip\n\nSelecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n\npricelist_df = df[\"ProductID\", \"ListPrice\"]\n\nYou can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n\nbikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nProductName\tListPrice\nMountain-100 Silver, 38\t3399.9900\nRoad-750 Black, 52\t539.9900\n...\t...\n\nTo group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:\n\ncounts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n\n\nThe results from this code example would look something like this:\n\nExpand table\nCategory\tcount\nHeadsets\t3\nWheels\t14\nMountain Bikes\t32\n...\t...\nUsing SQL expressions in Spark\n\nThe Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\n\nCreating database objects in the Spark catalog\n\nThe Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\n\nOne of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:\n\ndf.createOrReplaceTempView(\"products\")\n\n\nA view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.\n\n Note\n\nWe won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:\n\nYou can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.\nYou can save a dataframe as a table by using its saveAsTable method.\nYou can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.\nUsing the Spark SQL API to query data\n\nYou can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.\n\nbikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n\n\nThe results from the code example would look similar to the following table:\n\nExpand table\nProductID\tProductName\tListPrice\n38\tMountain-100 Silver, 38\t3399.9900\n52\tRoad-750 Black, 52\t539.9900\n...\t...\t...\nUsing SQL code\n\nThe previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this:\n\n%%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n\n\nThe SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:\n\nExpand table\nCategory\tProductCount\nBib-Shorts\t3\nBike Racks\t1\nBike Stands\t1\n...\t...\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/3-use-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse Spark in Azure Synapse Analytics\n3 minutes\n\nYou can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:\n\nBatch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.\nInteractive analytics sessions to explore, analyze, and visualize data.\nRunning Spark code in notebooks\n\nAzure Synapse Studio includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Synapse Studio is similar to that of Jupyter notebooks - a popular open source notebook platform.\n\n Note\n\nWhile usually used interactively, notebooks can be included in automated pipelines and run as an unattended script.\n\nNotebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:\n\nSyntax highlighting and error support.\nCode auto-completion​.\nInteractive data visualizations.\nThe ability to export results.\n\n Tip\n\nTo learn more about working with notebooks in Azure Synapse Analytics, see the Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nAccessing data from a Synapse Spark pool\n\nYou can use Spark in Azure Synapse Analytics to work with data from various sources, including:\n\nA data lake based on the primary storage account for the Azure Synapse Analytics workspace.\nA data lake based on storage defined as a linked service in the workspace.\nA dedicated or serverless SQL pool in the workspace.\nAn Azure SQL or SQL Server database (using the Spark connector for SQL Server)\nAn Azure Cosmos DB analytical database defined as a linked service and configured using Azure Synapse Link for Cosmos DB.\nAn Azure Data Explorer Kusto database defined as a linked service in the workspace.\nAn external Hive metastore defined as a linked service in the workspace.\n\nOne of the most common uses of Spark is to work with data in a data lake, where you can read and write files in multiple commonly used formats, including delimited text, Parquet, Avro, and others.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get to know Apache Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/2-get-to-know-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nGet to know Apache Spark\n3 minutes\n\nApache Spark is distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster.\n\nHow Spark works\n\nApache Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). The SparkContext connects to the cluster manager, which allocates resources across applications using an implementation of Apache Hadoop YARN. Once connected, Spark acquires executors on nodes in the cluster to run your application code.\n\nThe SparkContext runs the main function and parallel operations on the cluster nodes, and then collects the results of the operations. The nodes read and write data from and to the file system and cache transformed data in-memory as Resilient Distributed Datasets (RDDs).\n\nThe SparkContext is responsible for converting an application to a directed acyclic graph (DAG). The graph consists of individual tasks that get executed within an executor process on the nodes. Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.\n\nSpark pools in Azure Synapse Analytics\n\nIn Azure Synapse Analytics, a cluster is implemented as a Spark pool, which provides a runtime for Spark operations. You can create one or more Spark pools in an Azure Synapse Analytics workspace by using the Azure portal, or in Azure Synapse Studio. When defining a Spark pool, you can specify configuration options for the pool, including:\n\nA name for the spark pool.\nThe size of virtual machine (VM) used for the nodes in the pool, including the option to use hardware accelerated GPU-enabled nodes.\nThe number of nodes in the pool, and whether the pool size is fixed or individual nodes can be brought online dynamically to auto-scale the cluster; in which case, you can specify the minimum and maximum number of active nodes.\nThe version of the Spark Runtime to be used in the pool; which dictates the versions of individual components such as Python, Java, and others that get installed.\n\n Tip\n\nFor more information about Spark pool configuration options, see Apache Spark pool configurations in Azure Synapse Analytics in the Azure Synapse Analytics documentation.\n\nSpark pools in an Azure Synapse Analytics Workspace are serverless - they start on-demand and stop when idle.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nApache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become extremely popular in \"big data\" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Databricks, and Azure Synapse Analytics.\n\nThis module explores how you can use Spark in Azure Synapse Analytics to ingest, process, and analyze data from a data lake. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other Synapse analytical runtimes are specific to Azure Synapse Analytics.\n\nAfter completing this module, you'll be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Analyze data with Apache Spark in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAnalyze data with Apache Spark in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure Synapse Analytics\n\nApache Spark is a core technology for large-scale data analytics. Learn how to use Spark in Azure Synapse Analytics to analyze and visualize data in a data lake.\n\nLearning objectives\n\nAfter completing this module, you will be able to:\n\nIdentify core features and capabilities of Apache Spark.\nConfigure a Spark pool in Azure Synapse Analytics.\nRun code to load, analyze, and visualize data in a Spark notebook.\nAdd\nPrerequisites\n\nIf you are not already familiar with Azure Synapse Analytics, consider completing the Introduction to Azure Synapse Analytics module before starting this module.\n\nIntroduction\nmin\nGet to know Apache Spark\nmin\nUse Spark in Azure Synapse Analytics\nmin\nAnalyze data with Spark\nmin\nVisualize data with Spark\nmin\nExercise - Analyze data with Spark\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nServerless SQL pools enable you to easily query files in data lake. You can query various file formats CSV, JSON, Parquet, and create external database objects to provide a relational abstraction layer over the raw files.\n\nIn this module, you've learned how to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nLearn more\n\nTo learn more about using serverless SQL pools to query files, refer to the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nWhat function is used to read the data in files stored in a data lake?\n\n \n\nFORMAT\n\nROWSET\n\nOPENROWSET\n\n2. \n\nWhat character in file path can be used to select all the file/folders that match rest of the path?\n\n \n\n&\n\n*\n\n/\n\n3. \n\nWhich external database object encapsulates the connection information to a file location in a data lake store?\n\n \n\nFILE FORMAT\n\nDATA SOURCE\n\nEXTERNAL TABLE\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/5-exercise-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Query files using a serverless SQL pool\n40 minutes\n\nNow it's your opportunity to try using a serverless SQL pool for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to query data files in a data lake.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create external database objects - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/4-external-objects",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate external database objects\n6 minutes\n\nYou can use the OPENROWSET function in SQL queries that run in the default master database of the built-in serverless SQL pool to explore data in the data lake. However, sometimes you may want to create a custom database that contains some objects that make it easier to work with external data in the data lake that you need to query frequently.\n\nCreating a database\n\nYou can create a database in a serverless SQL pool just as you would in a SQL Server instance. You can use the graphical interface in Synapse Studio, or a CREATE DATABASE statement. One consideration is to set the collation of your database so that it supports conversion of text data in files to appropriate Transact-SQL data types.\n\nThe following example code creates a database named salesDB with a collation that makes it easier to import UTF-8 encoded text data into VARCHAR columns.\n\nCREATE DATABASE SalesDB\n    COLLATE Latin1_General_100_BIN2_UTF8\n\nCreating an external data source\n\nYou can use the OPENROWSET function with a BULK path to query file data from your own database, just as you can in the master database; but if you plan to query data in the same location frequently, it's more efficient to define an external data source that references that location. For example, the following code creates a data source named files for the hypothetical https://mydatalake.blob.core.windows.net/data/files/ folder:\n\nCREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/'\n)\n\n\nOne benefit of an external data source, is that you can simplify an OPENROWSET query to use the combination of the data source and the relative path to the folders or files you want to query:\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK 'orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'csv',\n        PARSER_VERSION = '2.0'\n    ) AS orders\n\n\nIn this example, the BULK parameter is used to specify the relative path for all .csv files in the orders folder, which is a subfolder of the files folder referenced by the data source.\n\nAnother benefit of using a data source is that you can assign a credential for the data source to use when accessing the underlying storage, enabling you to provide access to data through SQL without permitting users to access the data directly in the storage account. For example, the following code creates a credential that uses a shared access signature (SAS) to authenticate against the underlying Azure storage account hosting the data lake.\n\nCREATE DATABASE SCOPED CREDENTIAL sqlcred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\nGO\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = sqlcred\n);\nGO\n\n\n Tip\n\nIn addition to SAS authentication, you can define credentials that use managed identity (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the Control storage account access for serverless SQL pool in Azure Synapse Analytics article in Azure Synapse Analytics documentation.\n\nCreating an external file format\n\nWhile an external data source simplifies the code needed to access files with the OPENROWSET function, you still need to provide format details for the file being access; which may include multiple settings for delimited text files. You can encapsulate these settings in an external file format, like this:\n\nCREATE EXTERNAL FILE FORMAT CsvFormat\n    WITH (\n        FORMAT_TYPE = DELIMITEDTEXT,\n        FORMAT_OPTIONS(\n            FIELD_TERMINATOR = ',',\n            STRING_DELIMITER = '\"'\n        )\n    );\nGO\n\n\nAfter creating file formats for the specific data files you need to work with, you can use the file format to create external tables, as discussed next.\n\nCreating an external table\n\nWhen you need to perform a lot of analysis or reporting from files in the data lake, using the OPENROWSET function can result in complex code that includes data sources and file paths. To simplify access to the data, you can encapsulate the files in an external table; which users and reporting applications can query using a standard SQL SELECT statement just like any other database table. To create an external table, use the CREATE EXTERNAL TABLE statement, specifying the column schema as for a standard table, and including a WITH clause specifying the external data source, relative path, and external file format for your data.\n\nCREATE EXTERNAL TABLE dbo.products\n(\n    product_id INT,\n    product_name VARCHAR(20),\n    list_price DECIMAL(5,2)\n)\nWITH\n(\n    DATA_SOURCE = files,\n    LOCATION = 'products/*.csv',\n    FILE_FORMAT = CsvFormat\n);\nGO\n\n-- query the table\nSELECT * FROM dbo.products;\n\n\nBy creating a database that contains the external objects discussed in this unit, you can provide a relational database layer over files in a data lake, making it easier for many data analysts and reporting tools to access the data by using standard SQL query semantics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query files using a serverless SQL pool - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery files using a serverless SQL pool\n10 minutes\n\nYou can use a serverless SQL pool to query data files in various common file formats, including:\n\nDelimited text, such as comma-separated values (CSV) files.\nJavaScript object notation (JSON) files.\nParquet files.\n\nThe basic syntax for querying is the same for all of these types of file, and is built on the OPENROWSET SQL function; which generates a tabular rowset from data in one or more files. For example, the following query could be used to extract data from CSV files.\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv') AS rows\n\n\nThe OPENROWSET function includes more parameters that determine factors such as:\n\nThe schema of the resulting rowset\nAdditional formatting options for delimited text files.\n\n Tip\n\nYou'll find the full syntax for the OPENROWSET function in the Azure Synapse Analytics documentation.\n\nThe output from OPENROWSET is a rowset to which an alias must be assigned. In the previous example, the alias rows is used to name the resulting rowset.\n\nThe BULK parameter includes the full URL to the location in the data lake containing the data files. This can be an individual file, or a folder with a wildcard expression to filter the file types that should be included. The FORMAT parameter specifies the type of data being queried. The example above reads delimited text from all .csv files in the files folder.\n\n Note\n\nThis example assumes that the user has access to the files in the underlying store, If the files are protected with a SAS key or custom identity, you would need to create a server-scoped credential.\n\nAs seen in the previous example, you can use wildcards in the BULK parameter to include or exclude files in the query. The following list shows a few examples of how this can be used:\n\nhttps://mydatalake.blob.core.windows.net/data/files/file1.csv: Only include file1.csv in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/file*.csv: All .csv files in the files folder with names that start with \"file\".\nhttps://mydatalake.blob.core.windows.net/data/files/*: All files in the files folder.\nhttps://mydatalake.blob.core.windows.net/data/files/**: All files in the files folder, and recursively its subfolders.\n\nYou can also specify multiple file paths in the BULK parameter, separating each path with a comma.\n\nQuerying delimited text files\n\nDelimited text files are a common file format within many businesses. The specific formatting used in delimited files can vary, for example:\n\nWith and without a header row.\nComma and tab-delimited values.\nWindows and Unix style line endings.\nNon-quoted and quoted values, and escaping characters.\n\nRegardless of the type of delimited file you're using, you can read data from them by using the OPENROWSET function with the csv FORMAT parameter, and other parameters as required to handle the specific formatting details for your data. For example:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    FIRSTROW = 2) AS rows\n\n\nThe PARSER_VERSION is used to determine how the query interprets the text encoding used in the files. Version 1.0 is the default and supports a wide range of file encodings, while version 2.0 supports fewer encodings but offers better performance. The FIRSTROW parameter is used to skip rows in the text file, to eliminate any unstructured preamble text or to ignore a row containing column headings.\n\nAdditional parameters you might require when working with delimited text files include:\n\nFIELDTERMINATOR - the character used to separate field values in each row. For example, a tab-delimited file separates fields with a TAB (\\t) character. The default field terminator is a comma (,).\nROWTERMINATOR - the character used to signify the end of a row of data. For example, a standard Windows text file uses a combination of a carriage return (CR) and line feed (LF), which is indicated by the code \\n; while UNIX-style text files use a single line feed character, which can be indicated using the code 0x0a.\nFIELDQUOTE - the character used to enclose quoted string values. For example, to ensure that the comma in the address field value 126 Main St, apt 2 isn't interpreted as a field delimiter, you might enclose the entire field value in quotation marks like this: \"126 Main St, apt 2\". The double-quote (\") is the default field quote character.\n\n Tip\n\nFor details of additional parameters when working with delimited text files, refer to the Azure Synapse Analytics documentation.\n\nSpecifying the rowset schema\n\nIt's common for delimited text files to include the column names in the first row. The OPENROWSET function can use this to define the schema for the resulting rowset, and automatically infer the data types of the columns based on the values they contain. For example, consider the following delimited text:\n\nproduct_id,product_name,list_price\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThe data consists of the following three columns:\n\nproduct_id (integer number)\nproduct_name (string)\nlist_price (decimal number)\n\nYou could use the following query to extract the data with the correct column names and appropriately inferred SQL Server data types (in this case INT, NVARCHAR, and DECIMAL)\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE) AS rows\n\n\nThe HEADER_ROW parameter (which is only available when using parser version 2.0) instructs the query engine to use the first row of data in each file as the column names, like this:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nNow consider the following data:\n\n123,Widget,12.99\n124,Gadget,3.99\n\n\nThis time, the file doesn't contain the column names in a header row; so while the data types can still be inferred, the column names will be set to C1, C2, C3, and so on.\n\nExpand table\nC1\tC2\tC3\n123\tWidget\t12.9900\n124\tGadget\t3.9900\n\nTo specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a WITH clause, like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0')\nWITH (\n    product_id INT,\n    product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8,\n    list_price DECIMAL(5,2)\n) AS rows\n\n\nThis query produces the expected results:\n\nExpand table\nproduct_id\tproduct_name\tlist_price\n123\tWidget\t12.99\n124\tGadget\t3.99\n\n Tip\n\nWhen working with text files, you may encounter some incompatibility with UTF-8 encoded data and the collation used in the master database for the serverless SQL pool. To overcome this, you can specify a compatible collation for individual VARCHAR columns in the schema. See the troubleshooting guidance for more details.\n\nQuerying JSON files\n\nJSON is a popular format for web applications that exchange data through REST interfaces or use NoSQL data stores such as Azure Cosmos DB. So, it's not uncommon to persist data as JSON documents in files in a data lake for analysis.\n\nFor example, a JSON file that defines an individual product might look like this:\n\n{\n    \"product_id\": 123,\n    \"product_name\": \"Widget\",\n    \"list_price\": 12.99\n}\n\n\nTo return product data from a folder containing multiple JSON files in this format, you could use the following SQL query:\n\nSELECT doc\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nOPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:\n\nExpand table\ndoc\n{\"product_id\":123,\"product_name\":\"Widget\",\"list_price\": 12.99}\n{\"product_id\":124,\"product_name\":\"Gadget\",\"list_price\": 3.99}\n\nTo extract individual values from the JSON, you can use the JSON_VALUE function in the SELECT statement, as shown here:\n\nSELECT JSON_VALUE(doc, '$.product_name') AS product,\n           JSON_VALUE(doc, '$.list_price') AS price\nFROM\n    OPENROWSET(\n        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n        FORMAT = 'csv',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (doc NVARCHAR(MAX)) as rows\n\n\nThis query would return a rowset similar to the following results:\n\nExpand table\nproduct\tprice\nWidget\t12.99\nGadget\t3.99\nQuerying Parquet files\n\nParquet is a commonly used format for big data processing on distributed file storage. It's an efficient data format that is optimized for compression and analytical querying.\n\nIn most cases, the schema of the data is embedded within the Parquet file, so you only need to specify the BULK parameter with a path to the file(s) you want to read, and a FORMAT parameter of parquet; like this:\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.*',\n    FORMAT = 'parquet') AS rows\n\nQuery partitioned data\n\nIt's common in a data lake to partition data by splitting across multiple files in subfolders that reflect partitioning criteria. This enables distributed processing systems to work in parallel on multiple partitions of the data, or to easily eliminate data reads from specific folders based on filtering criteria. For example, suppose you need to efficiently process sales order data, and often need to filter based on the year and month in which orders were placed. You could partition the data using folders, like this:\n\n/orders\n/year=2020\n/month=1\n/01012020.parquet\n/02012020.parquet\n...\n/month=2\n/01022020.parquet\n/02022020.parquet\n...\n...\n/year=2021\n/month=1\n/01012021.parquet\n/02012021.parquet\n...\n...\n\nTo create a query that filters the results to include only the orders for January and February 2020, you could use the following code:\n\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*',\n    FORMAT = 'parquet') AS orders\nWHERE orders.filepath(1) = '2020'\n    AND orders.filepath(2) IN ('1','2');\n\n\nThe numbered filepath parameters in the WHERE clause reference the wildcards in the folder names in the BULK path -so the parameter 1 is the * in the year=* folder name, and parameter 2 is the * in the month=* folder name.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand Azure Synapse serverless SQL pool capabilities and use cases - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/2-understand-serverless-pools",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\n5 minutes\n\nAzure Synapse Analytics is an integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale. One of the most prevalent technologies used in data solutions is SQL - an industry standard language for querying and manipulating data.\n\nServerless SQL pools in Azure Synapse Analytics\n\nAzure Synapse SQL is a distributed query system in Azure Synapse Analytics that offers two kinds of runtime environments:\n\nServerless SQL pool: on-demand SQL query processing, primarily used to work with data in a data lake.\nDedicated SQL pool: Enterprise-scale relational database instances used to host data warehouses in which data is stored in relational tables.\n\nIn this module, we'll focus on serverless SQL pool, which provides a pay-per-query endpoint to query the data in your data lake. The benefits of using serverless SQL pool include:\n\nA familiar Transact-SQL syntax to query data in place without the need to copy or load data into a specialized store.\nIntegrated connectivity from a wide range of business intelligence and ad-hoc querying tools, including the most popular drivers.\nDistributed query processing that is built for large-scale data, and computational functions - resulting in fast query performance.\nBuilt-in query execution fault-tolerance, resulting in high reliability and success rates even for long-running queries involving large data sets.\nNo infrastructure to setup or clusters to maintain. A built-in endpoint for this service is provided within every Azure Synapse workspace, so you can start querying data as soon as the workspace is created.\nNo charge for resources reserved, you're only charged for the data processed by queries you run.\nWhen to use serverless SQL pools\n\nServerless SQL pool is tailored for querying the data residing in the data lake, so in addition to eliminating management burden, it eliminates a need to worry about ingesting the data into the system. You just point the query to the data that is already in the lake and run it.\n\nSynapse SQL serverless resource model is great for unplanned or \"bursty\" workloads that can be processed using the always-on serverless SQL endpoint in your Azure Synapse Analytics workspace. Using the serverless pool helps when you need to know exact cost for each query executed to monitor and attribute costs.\n\n Note\n\nServerless SQL pool is an analytics system and is not recommended for OLTP workloads such as databases used by applications to store transactional data. Workloads that require millisecond response times and are looking to pinpoint a single row in a data set are not good fit for serverless SQL pool.\n\nCommon use cases for serverless SQL pools include:\n\nData exploration: Data exploration involves browsing the data lake to get initial insights about the data, and is easily achievable with Azure Synapse Studio. You can browse through the files in your linked data lake storage, and use the built-in serverless SQL pool to automatically generate a SQL script to select TOP 100 rows from a file or folder just as you would do with a table in SQL Server. From there, you can apply projections, filtering, grouping, and most of the operation over the data as if the data were in a regular SQL Server table.\nData transformation: While Azure Synapse Analytics provides great data transformations capabilities with Synapse Spark, some data engineers might find data transformation easier to achieve using SQL. Serverless SQL pool enables you to perform SQL-based data transformations; either interactively or as part of an automated data pipeline.\nLogical data warehouse: After your initial exploration of the data in the data lake, you can define external objects such as tables and views in a serverless SQL database. The data remains stored in the data lake files, but are abstracted by a relational schema that can be used by client applications and analytical tools to query the data as they would in a relational database hosted in SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Analytics includes serverless SQL pools, which are tailored for querying data in a data lake. With a serverless SQL pool you can use SQL code to query data in files of various common formats without needing to load the file data into database storage. This capability helps data analysts and data engineers analyze and process file data in the data lake using a familiar data processing language, without the need to create or maintain a relational database store.\n\nAfter completing this module, you'll be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nPrerequisites\n\nBefore starting this module, you should have the following prerequisite skills and knowledge:\n\nFamiliarity with the Microsoft Azure portal\nFamiliarity with data lake and data warehouse concepts\nExperience of using SQL to query database tables\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Synapse serverless SQL pool to query files in a data lake - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nUse Azure Synapse serverless SQL pool to query files in a data lake\nModule\n7 Units\nFeedback\nBeginner\nData Engineer\nAzure Synapse Analytics\n\nWith Azure Synapse serverless SQL pool, you can leverage your SQL skills to explore and analyze data in files, without the need to load the data into a relational database.\n\nLearning objectives\n\nAfter the completion of this module, you will be able to:\n\nIdentify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\nQuery CSV, JSON, and Parquet files using a serverless SQL pool\nCreate external database objects in a serverless SQL pool\nAdd\nPrerequisites\n\nConsider completing the Explore data analytics in Azure and Get started querying with Transact-SQL learning paths before starting this module. You will need knowledge of:\n\nAnalytical data workloads in Microsoft Azure\nQuerying data with Transact-SQL\nIntroduction\nmin\nUnderstand Azure Synapse serverless SQL pool capabilities and use cases\nmin\nQuery files using a serverless SQL pool\nmin\nCreate external database objects\nmin\nExercise - Query files using a serverless SQL pool\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Analytics provides an integrated cloud-based platform for big data processing and analysis. You can use it to build descriptive, diagnostic, predictive, and prescriptive analytics solutions.\n\nIn this module, you learned how to:\n\nIdentify the business problems that Azure Synapse Analytics addresses.\nDescribe core capabilities of Azure Synapse Analytics.\nDetermine when to use Azure Synapse Analytics.\n\n Tip\n\nTo learn more about the capabilities of Azure Synapse Analytics, see What is Azure Synapse Analytics? in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich feature of Azure Synapse Analytics enables you to transfer data from one store to another and apply transformations to the data at scheduled intervals?\n\n \n\nServerless SQL pool\n\nApache Spark pool\n\nPipelines\n\n2. \n\nYou want to create a data warehouse in Azure Synapse Analytics in which the data is stored and queried in a relational data store. What kind of pool should you create?\n\n \n\nServerless SQL pool\n\nDedicated SQL pool\n\nApache Spark pool\n\n3. \n\nA data analyst wants to analyze data by using Python code combined with text descriptions of the insights gained from the analysis. What should they use to perform the analysis?\n\n \n\nA notebook connected to an Apache Spark pool.\n\nA SQL script connected to a serverless SQL pool.\n\nA KQL script connected to a Data Explorer pool.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Explore Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-azure-synapse-analytics/4a-exercise-explore-synapse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Explore Azure Synapse Analytics\n60 minutes\n\nNow it's your chance to explore the capabilities of Azure Synapse Analytics for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use Azure Synapse Studio to perform core data analytics tasks.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]