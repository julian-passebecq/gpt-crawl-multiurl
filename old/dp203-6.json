[
  {
    "title": "Configure authentication - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/4-configure-authentication",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 4 of 10\nNext\nConfigure authentication\nCompleted\n100 XP\n8 minutes\n\nAuthentication is the process of validating credentials as you access resources in a digital infrastructure. This ensures that you can validate that an individual, or a service that wants to access a service in your environment can prove who they are. Azure Synapse Analytics provides several different methods for authentication.\n\nWhat needs to be authenticated\n\nThere are a variety of scenarios that means that authentication must take place to protect the data that is stored in your Azure Synapse Analytics estate.\n\nThe common form of authentication is that of individuals who want to access the data in the service. This is typically seen as an individual providing a username and password to authenticate against a service. However, this is also becoming more sophisticated with authentication requests working in combination with Conditional Access policies to further secure the authentication process with additional security steps.\n\nWhat is less obvious is the fact that services must authenticate with other services so that they can operate seamlessly. An example of this is using an Azure Synapse Spark or serverless SQL pool to access data in an Azure Data Lake store. An authentication mechanism must take place in the background to ensure that Azure Synapse Analytics can access the data in the data lake in an authenticated manner.\n\nFinally, there are situations where users and services operate together at the same time. Here you have a combination of both user and service authentication taking place under the hood to ensure that the user is getting access to the data seamlessly. An example of this is using Power BI to view reports in a dashboard that is being serviced by a dedicated SQL pool. Here you have multiple levels of authentication taking place that needs to be managed.\n\nTypes of security\n\nThe following are the types of authentication that you should be aware of when working with Azure Synapse Analytics.\n\nMicrosoft Entra ID\n\nMicrosoft Entra ID is a directory service that allows you to centrally maintain objects that can be secured. The objects can include user accounts and computer accounts. An employee of an organization will typically have a user account that represents them in the organizations Microsoft Entra tenant, and they then use the user account with a password to authenticate against other resources that are stored within the directory using a process known as single sign-on.\n\nThe power of Microsoft Entra ID is that they only have to log in once, and Microsoft Entra ID will manage access to other resources based on the information held within it using pass through authentication. If a user and an instance of Azure Synapse Analytics are part of the same Microsoft Entra ID, it is possible for the user to access Azure Synapse Analytics without an apparent login. If managed correctly, this process is seamless as the administrator would have given the user authorization to access Azure Synapse Analytics dedicated SQL pool as an example.\n\nIn this situation, it is normal for an Azure Administrator to create the user accounts and assign them to the appropriate roles and groups in Microsoft Entra ID. The Data Engineer will then add the user, or a group to which the user belongs to access a dedicated SQL pool.\n\nManaged identities\n\nManaged identity for Azure resources is a feature of Microsoft Entra ID. The feature provides Azure services with an automatically managed identity in Microsoft Entra ID. You can use the Managed Identity capability to authenticate to any service that supports Microsoft Entra authentication.\n\nManaged identities for Azure resources are the new name for the service formerly known as Managed Service Identity (MSI). A system-assigned managed identity is created for your Azure Synapse workspace when you create the workspace.\n\nAzure Synapse also uses the managed identity to integrate pipelines. The managed identity lifecycle is directly tied to the Azure Synapse workspace. If you delete the Azure Synapse workspace, then the managed identity is also cleaned up.\n\nThe workspace managed identity needs permissions to perform operations in the pipelines. You can use the object ID or your Azure Synapse workspace name to find the managed identity when granting permissions.\n\nYou can retrieve the managed identity in the Azure portal. Open your Azure Synapse workspace in Azure portal and select Overview from the left navigation. The managed identity's object ID is displayed to in the main screen.\n\nThe managed identity information will also show up when you create a linked service that supports managed identity authentication from Azure Synapse Studio.\n\nLaunch Azure Synapse Studio and select the Manage tab from the left navigation. Then select Linked services and choose the + New option to create a new linked service.\n\nIn the New linked service window, type Azure Data Lake Storage Gen2. Select the Azure Data Lake Storage Gen2 resource type from the list below and choose Continue.\n\nIn the next window, choose Managed Identity for Authentication method. You'll see the managed identity's Name and Object ID.\n\nSQL Authentication\n\nFor user accounts that are not part of a Microsoft Entra ID, then using SQL Authentication will be an alternative. In this instance, a user is created in the instance of a dedicated SQL pool. If the user in question requires administrator access, then the details of the user are held in the master database. If administrator access is not required, you can create a user in a specific database. A user then connects directly to the Azure Synapse Analytics dedicated SQL pool where they are prompted to use a username and password to access the service.\n\nThis approach is typically useful for external users who need to access the data, or if you are using third party or legacy applications against the Azure Synapse Analytics dedicated SQL pool\n\nMultifactor authentication\n\nSynapse SQL support connections from SQL Server Management Studio (SSMS) using Active Directory Universal Authentication.\n\nThis enables you to operate in environments that use Conditional Access policies that enforce multifactor authentication as part of the policy.\n\nKeys\n\nIf you are unable to use a managed identity to access resources such as Azure Data Lake then you can use storage account keys and shared access signatures.\n\nWith a storage account key. Azure creates two of these keys (primary and secondary) for each storage account you create. The keys give access to everything in the account. You'll find the storage account keys in the Azure portal view of the storage account. Just select Settings, and then click Access keys.\n\nAs a best practice, you shouldn't share storage account keys, and you can use Azure Key Vault to manage and secure the keys.\n\nAzure Key Vault is a secret store: a centralized cloud service for storing app secrets - configuration values like passwords and connection strings that must remain secure at all times. Key Vault helps you control your apps' secrets by keeping them in a single central location and providing secure access, permissions control, and access logging.\n\nThe main benefits of using Key Vault are:\n\nSeparation of sensitive app information from other configuration and code, reducing risk of accidental leaks\nRestricted secret access with access policies tailored to the apps and individuals that need them\nCentralized secret storage, allowing required changes to happen in only one place\nAccess logging and monitoring to help you understand how and when secrets are accessed\n\nSecrets are stored in individual vaults, which are Azure resources used to group secrets together. Secret access and vault management is accomplished via a REST API, which is also supported by all of the Azure management tools as well as client libraries available for many popular languages. Every vault has a unique URL where its API is hosted.\n\nShared access signatures\n\nIf an external third-party application needs access to your data, you'll need to secure their connections without using storage account keys. For untrusted clients, use a shared access signature (SAS). A shared access signature is a string that contains a security token that can be attached to a URI. Use a shared access signature to delegate access to storage objects and specify constraints, such as the permissions and the time range of access. You can give a customer a shared access signature token.\n\nTypes of shared access signatures\n\nYou can use a service-level shared access signature to allow access to specific resources in a storage account. You'd use this type of shared access signature, for example, to allow an app to retrieve a list of files in a file system or to download a file.\n\nUse an account-level shared access signature to allow access to anything that a service-level shared access signature can allow, plus additional resources and abilities. For example, you can use an account-level shared access signature to allow the ability to create file systems.\n\nNext unit: Manage authorization through column and row level security\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage authorization through column and row level security - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/5-manage-authorization-through-column-row-level-security",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nManage authorization through column and row level security\n6 minutes\n\nIn this topic, we'll go through how you can manage authorization through column and row level security within Azure Synapse Analytics. We'll start off by talking about column level security in Azure Synapse Analytics, and finish with row level security.\n\nColumn level security in Azure Synapse Analytics\n\nGenerally speaking, column level security is simplifying a design and coding for the security in your application. It allows you to restrict column access in order to protect sensitive data. For example, if you want to ensure that a specific user 'Leo' can only access certain columns of a table because he's in a specific department. The logic for 'Leo' only to access the columns specified for the department he works in, is a logic that is located in the database tier, rather than on the application level data tier. If he needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system. Column level security will also eliminate the necessity for the introduction of view, where you would filter out columns, to impose access restrictions on 'Leo'\n\nThe way to implement column level security is by using the GRANT T-SQL statement. Using this statement, SQL and Microsoft Entra ID support the authentication.\n\nSyntax\n\nThe syntax to use for implementing column level security looks as follows:\n\nGRANT <permission> [ ,...n ] ON\n    [ OBJECT :: ][ schema_name ]. object_name [ ( column [ ,...n ] ) ] // specifying the column access\n    TO <database_principal> [ ,...n ]\n    [ WITH GRANT OPTION ]\n    [ AS <database_principal> ]\n<permission> ::=\n    SELECT\n  | UPDATE\n<database_principal> ::=\n      Database_user // specifying the database user\n    | Database_role // specifying the database role \n    | Database_user_mapped_to_Windows_User\n    | Database_user_mapped_to_Windows_Group\n\n\nSo when would you use column-level security? Let's say that you are a financial services firm, and can only have an account manager allowed to have access to a customer's social security number, phone number, or other personally identifiable information. It is imperative to distinguish the role of an account manager versus the manager of the account managers.\n\nAnother use case might be related to the Healthcare Industry. Let's say you have a specific health care provider. This healthcare provider only wants doctors and nurses to be able to access medical records. The billing department should not have access to view this data. Column-level security might be the option to use.\n\nSo how does column level security distinguishes from row-level security? Let's look into that.\n\nRow level security in Azure Synapse Analytics\n\nRow-level security (RLS) can help you to create a group membership or execution context in order to control not just columns in a database table, but actually, the rows. RLS, just like column-level security, can simply help and enable your design and coding of your application security. However, compared to column-level security where it's focused on the columns (parameters), RLS helps you implement restrictions on data row access. Let's say that your employee can only access rows of data that are important to the department, you should implement RLS. If you want to restrict, for example, customer data access that is only relevant to the company, you can implement RLS. The restriction on the access of the rows is a logic that is located in the database tier, rather than on the application level data tier. If 'Leo' needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system.\n\nThe way to implement RLS is by using the CREATE SECURITY POLICY[!INCLUDEtsql] statement. The predicates are created as inline table-valued functions. It is imperative to understand that within Azure Synapse, only supports filter predicates. If you need to use a block predicate, you won't be able to find support at this moment within Azure synapse.\n\nDescription of row level security in relation to filter predicates\n\nRLS within Azure Synapse supports one type of security predicates, which are Filter predicates, not block predicates.\nWhat filter predicates do, is silently filtering the rows that are available for reading operations such as SELECT, UPDATE, DELETE.\n\nThe access to row-level data in a table is restricted as an inline table-valued function, which is a security predicate. This table-valued function will then be invoked and enforced by the security policy that you need. An application is not aware of rows that are filtered from the result set for filter predicates. So what will happen is that if all rows are filtered, a null set is returned.\n\nWhen you are using filter predicates, it will be applied when data is read from the base table. The filter predicate affects all get operations such as SELECT, DELETE, UPDATE. You are unable to select or delete rows that have been filtered. It is not possible for you to update a row that has been filtered. What you can do, is update rows in a way that they will be filtered afterward.\n\nUse cases\n\nWe've already mentioned some use cases for RLS. Another use case might where you have created a multi-tenant application where you create a policy where logical separations of a tenant's data rows from another tenant's data rows are enforced. In order to implement this efficiently, it is highly recommended to store data for many tenants in a single table.\n\nWhen we look at RLS filter predicates, they are functionally equivalent to appending a WHERE clause. The predicate can be as sophisticated as business practices dictate, or the clause can be as simple as WHERE TenantId = 42.\n\nWhen we look at RLS more formally, RLS introduces predicate based access control. The reason why RLS can be used for predicate access control is that it is a flexible, centralized, predicate-based evaluation. The filter predicate can be based on metadata or any other criteria you would determine as appropriate. The predicate is used as a criterion to determine if the user has the appropriate access to the data based on user attributes. Label-based access control can be implemented by using predicate-based access control.\n\nPermissions\n\nIf you want to create, alter or drop the security policies, you would have to use the ALTER ANY SECURITY POLICY permission. The reason for that is when you are creating or dropping a security policy it requires ALTER permissions on the schema.\n\nIn addition to that, there are other permissions required for each predicate that you would add:\n\nSELECT and REFERENCES permissions on the inline table-valued function being used as a predicate.\n\nREFERENCES permission on the table that you target to be bound to the policy.\n\nREFERENCES permission on every column from the target table used as arguments.\n\nOnce you've set up the security policies, they will apply to all the users (including dbo users in the database) Even though DBO users can alter or drop security policies, their changes to the security policies can be audited. If you have special circumstances where highly privileged users, like a sysadmin or db_owner, need to see all rows to troubleshoot or validate data, you would still have to write the security policy in order to allow that.\n\nIf you have created a security policy where SCHEMABINDING = OFF, in order to query the target table, the user must have the SELECT or EXECUTE permission on the predicate function. They also need permissions to any additional tables, views, or functions used within the predicate function. If a security policy is created with SCHEMABINDING = ON (the default), then these permission checks are bypassed when users query the target table.\n\nBest practices\n\nThere are some best practices to take in mind when you want to implement RLS. We recommended creating a separate schema for the RLS objects. RLS objects in this context would be the predicate functions, and security policies. Why is that a best practice? It helps to separate the permissions that are required on these special objects from the target tables. In addition to that, separation for different policies and predicate functions may be needed in multi-tenant-databases. However, it is not a standard for every case.\n\nAnother best practice to bear in mind is that the ALTER ANY SECURITY POLICY permission should only be intended for highly privileged users (such as a security policy manager). The security policy manager should not require SELECT permission on the tables they protect.\n\nIn order to avoid potential runtime errors, you should take into mind type conversions in predicate functions that you write. Also, you should try to avoid recursion in predicate functions. The reason for this is to avoid performance degradation. Even though the query optimizer will try to detect the direct recursions, there is no guarantee to find the indirect recursions. With indirect recursion, we mean where a second function calls the predicate function.\n\nIt would also be recommended to avoid the use of excessive table joins in predicate functions. This would maximize performance.\n\nGenerally speaking when it comes to the logic of predicates, you should try to avoid logic that depends on session-specific SET options. Even though this is highly unlikely to be used in practical applications, predicate functions whose logic depends on certain session-specific SET options can leak information if users are able to execute arbitrary queries. For example, a predicate function that implicitly converts a string to datetime could filter different rows based on the SET DATEFORMAT option for the current session.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Conditional Access - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/3-configure-conditional-access",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 3 of 10\nNext\nConfigure Conditional Access\nCompleted\n100 XP\n7 minutes\n\nConditional Access is a feature that enables you to define the conditions under which a user can connect to your Azure subscription and access services. Conditional Access provides an additional layer of security that can be used in combination with authentication to strengthen the security access to your network.\n\nConditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. As an example, if a Data Engineer wishes to access services in Azure Synapse Analytics, they may be requested by the Conditional Access policy to perform an additional step of multifactor authentication (MFA) to complete the authentication to get onto the service\n\nConditional Access policies use signals as a basis to determine if Conditional Access should first be applied. Common signals include:\n\nUser or group membership names\nIP address information\nDevice platforms or type\nApplication access requests\nReal-time and calculated risk detection\nMicrosoft Cloud App Security (MCAS)\n\nBased on these signals, you can then choose to block access. The alternative is you can grant access, and at the same time request that the user perform an additional action including:\n\nPerform multifactor authentication\nUse a specific device to connect\n\nGiven the amount of data that could potentially be stored, Azure Synapse Analytics dedicated SQL pools supports Conditional Access to provide protection for your data. It does require that Azure Synapse Analytics is configured to support Microsoft Entra ID, and that if you chose multifactor authentication, that the tool you are using support it.\n\nTo configure Conditional Access, you can perform the following steps:\n\nSign in to the Azure portal, select Microsoft Entra ID, and then select Conditional Access.\n\nIn the Conditional Access-Policies blade, click New policy, provide a name, and then click Configure rules.\n\nUnder Assignments, select Users and groups, check Select users and groups, and then select the user or group for Conditional Access. Click Select, and then click Done to accept your selection.\n\nSelect Cloud apps, click Select apps. You see all apps available for Conditional Access. Select Azure SQL Database, at the bottom click Select, and then click Done.\n\nIf you can't find Azure SQL Database listed in the following third screenshot, complete the following steps:\n\nConnect to your database in Azure SQL Database by using SSMS with a Microsoft Entra admin account.\nExecute CREATE USER [user@yourtenant.com] FROM EXTERNAL PROVIDER.\nSign into Microsoft Entra ID and verify that Azure SQL Database, SQL Managed Instance, or Azure Synapse are listed in the applications in your Microsoft Entra instance.\n\nSelect Access controls, select Grant, and then check the policy you want to apply. For this example, we select Require multifactor authentication.\n\nNext unit: Configure authentication\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand network security options for Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics \nAdd\nPrevious\nUnit 2 of 10\nNext\nUnderstand network security options for Azure Synapse Analytics\nCompleted\n100 XP\n5 minutes\n\nThere are a range of network security steps that you should consider to secure Azure Synapse Analytics. One of the first aspects that you will consider is securing access to the service itself. This can be achieved by creating the following network objects including:\n\nFirewall rules\nVirtual networks\nPrivate endpoints\nFirewall rules\n\nFirewall rules enable you to define the type of traffic that is allowed or denied access to an Azure Synapse workspace using the originating IP address of the client that is trying to access the Azure Synapse Workspace. IP firewall rules configured at the workspace level apply to all public endpoints of the workspace including dedicated SQL pools, serverless SQL pool, and the development endpoint.\n\nYou can choose to allow connections from all IP addresses as you are creating the Azure Synapse Workspaces, although this is not recommended as it does not allow for control access to the workspace. Instead, within the Azure portal, you can configure specific IP address ranges and associate them with a rule name so that you have greater control.\n\nMake sure that the firewall on your network and local computer allows outgoing communication on TCP ports 80, 443 and 1443 for Synapse Studio.\n\nAlso, you need to allow outgoing communication on UDP port 53 for Synapse Studio. To connect using tools such as SSMS and Power BI, you must allow outgoing communication on TCP port 1433.\n\nVirtual networks\n\nAzure Virtual Network (VNet) enables private networks in Azure. VNet enables many types of Azure resources, such as Azure Synapse Analytics, to securely communicate with other virtual networks, the internet, and on-premises networks. When you create your Azure Synapse workspace, you can choose to associate it to a Microsoft Azure Virtual Network. The Virtual Network associated with your workspace is managed by Azure Synapse. This Virtual Network is called a Managed workspace Virtual Network.\n\nUsing a managed workspace virtual network provides the following benefits:\n\nWith a Managed workspace Virtual Network, you can offload the burden of managing the Virtual Network to Azure Synapse.\nYou don't have to configure inbound NSG rules on your own Virtual Networks to allow Azure Synapse management traffic to enter your Virtual Network. Misconfiguration of these NSG rules causes service disruption for customers.\nYou don't need to create a subnet for your Spark clusters based on peak load.\nManaged workspace Virtual Network along with Managed private endpoints protects against data exfiltration. You can only create Managed private endpoints in a workspace that has a Managed workspace Virtual Network associated with it.\nit ensures that your workspace is network isolated from other workspaces.\n\nIf your workspace has a Managed workspace Virtual Network, Data integration and Spark resources are deployed in it. A Managed workspace Virtual Network also provides user-level isolation for Spark activities because each Spark cluster is in its own subnet.\n\nDedicated SQL pool and serverless SQL pool are multi-tenant capabilities and therefore reside outside of the Managed workspace Virtual Network. Intra-workspace communication to dedicated SQL pool and serverless SQL pool use Azure private links. These private links are automatically created for you when you create a workspace with a Managed workspace Virtual Network associated to it.\n\nYou can only choose to enable managed virtual networks as you are creating the Azure Synapse Workspaces.\n\nPrivate endpoints\n\nAzure Synapse Analytics enables you to connect up its various components through endpoints. You can set up managed private endpoints to access these components in a secure manner known as private links. This can only be achieved in an Azure Synapse workspace with a Managed workspace Virtual Network. Private link enables you to access Azure services (such as Azure Storage and Azure Cosmos DB) and Azure hosted customer/partner services from your Azure Virtual Network securely.\n\nWhen you use a private link, traffic between your Virtual Network and workspace traverses entirely over the Microsoft backbone network. Private Link protects against data exfiltration risks. You establish a private link to a resource by creating a private endpoint.\n\nPrivate endpoint uses a private IP address from your Virtual Network to effectively bring the service into your Virtual Network. Private endpoints are mapped to a specific resource in Azure and not the entire service. Customers can limit connectivity to a specific resource approved by their organization. You can manage the private endpoints in the Azure Synapse Studio manage hub.\n\nNext unit: Configure Conditional Access\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this module, you will learn how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nIn this module, you will:\n\nUnderstand network security options for Azure Synapse Analytics\nConfigure Conditional Access\nConfigure Authentication\nManage authorization through column and row level security\nManage sensitive data with Dynamic Data masking\nImplement encryption in Azure Synapse Analytics\nUnderstand advanced data security options for Azure Synapse Analytics\nPrerequisites\n\nBefore taking this module, it is recommended that the student is able to:\n\nLog into the Azure portal\nCreate a Synapse Analytics Workspace\nCreate an Azure Synapse Analytics SQL Pool\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Secure a data warehouse in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nSecure a data warehouse in Azure Synapse Analytics\nModule\n10 Units\nFeedback\nIntermediate\nData Engineer\nAzure Synapse Analytics\n\nLearn how to approach and implement security to protect your data with Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you will:\n\nUnderstand network security options for Azure Synapse Analytics\nConfigure Conditional Access\nConfigure Authentication\nManage authorization through column and row level security\nManage sensitive data with Dynamic Data masking\nImplement encryption in Azure Synapse Analytics\nAdd\nPrerequisites\nBefore taking this module, it is recommended that you complete Data Fundamentals.\nIntroduction\nmin\nUnderstand network security options for Azure Synapse Analytics\nmin\nConfigure Conditional Access\nmin\nConfigure authentication\nmin\nManage authorization through column and row level security\nmin\nExercise - Manage authorization through column and row level security\nmin\nManage sensitive data with Dynamic Data Masking\nmin\nImplement encryption in Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/8-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n3 minutes\n\nIn this module, you have learned some of the features you can use to manage and monitor Azure Synapse Analytics, including:\n\nHow to Scale compute resources in Azure Synapse Analytics\nPausing the compute in Azure Synapse Analytics\nHow to manage workloads in Azure Synapse Analytics\nUsing the Azure Advisor to review recommendations\nUsing Dynamic Management Views to identify and troubleshoot query performance\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/7-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nWhich ALTER DATABASE statement parameter allows a dedicated SQL pool to scale?\n\n \n\nSCALE.\n\nMODIFY\n\nCHANGE.\n\n2. \n\nWhich workload management feature influences the order in which a request gets access to resources?\n\n \n\nWorkload classification.\n\nWorkload importance.\n\nWorkload isolation.\n\n3. \n\nWhich Dynamic Management View enables the view of the active connections against a dedicated SQL pool?\n\n \n\nsys.dm_pdw_exec_requests.\n\nsys.dm_pdw_dms_workers.\n\nDBCC PDW_SHOWEXECUTIONPLAN.\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use dynamic management views to identify and troubleshoot query performance - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/6-use-dynamic-management-views-to-identify-troubleshoot-query-performance",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUse dynamic management views to identify and troubleshoot query performance\n8 minutes\n\nDynamic Management Views provide a programmatic experience for monitoring the Azure Synapse Analytics SQL pool activity by using the Transact-SQL language. The views that are provided, not only enable you to troubleshoot and identify performance bottlenecks with the workloads working on your system, but they are also used by other services such as Azure Advisor to provide recommendations about Azure Synapse Analytics.\n\nThere are over 90 Dynamic Management Views that can queried against dedicated SQL pools to retrieve information about the following areas of the service:\n\nConnection information and activity\nSQL execution requests and queries\nIndex and statistics information\nResource blocking and locking activity\nData movement service activity\nErrors\n\nThe following is an example of monitoring query execution of the Azure Synapse Analytics SQL pools. The first step involves checking the connections against the server first, before checking the query execution activity. \n\nMonitoring connections\n\nAll logins to your data warehouse are logged to sys.dm_pdw_exec_sessions. The session_id is the primary key and is assigned sequentially for each new logon.\n\n-- Other Active Connections\nSELECT * FROM sys.dm_pdw_exec_sessions where status <> 'Closed' and session_id <> session_id();\n\nMonitor query execution\n\nAll queries executed on SQL pool are logged to sys.dm_pdw_exec_requests. The request_id uniquely identifies each query and is the primary key for this DMV. The request_id is assigned sequentially for each new query and is prefixed with QID, which stands for query ID. Querying this DMV for a given session_id shows all queries for a given logon.\n\nStep 1\n\nThe first step is to identify the query you want to investigate\n\n-- Monitor active queries\nSELECT *\nFROM sys.dm_pdw_exec_requests\nWHERE status not in ('Completed','Failed','Cancelled')\n  AND session_id <> session_id()\nORDER BY submit_time DESC;\n\n-- Find top 10 queries longest running queries\nSELECT TOP 10 *\nFROM sys.dm_pdw_exec_requests\nORDER BY total_elapsed_time DESC;\n\n\nFrom the preceding query results, note the Request ID of the query that you would like to investigate.\n\nQueries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceType. For information on concurrency limits, see Memory and concurrency limits or Resource classes for workload management. Queries can also wait for other reasons such as for object locks. If your query is waiting for a resource, see Investigating queries waiting for resources further down in this article.\n\nTo simplify the lookup of a query in the sys.dm_pdw_exec_requests table, use LABEL to assign a comment to your query, which can be looked up in the sys.dm_pdw_exec_requests view.\n\n-- Query with Label\nSELECT *\nFROM sys.tables\nOPTION (LABEL = 'My Query')\n;\n\n-- Find a query with the Label 'My Query'\n-- Use brackets when querying the label column, as it it a key word\nSELECT  *\nFROM    sys.dm_pdw_exec_requests\nWHERE   [label] = 'My Query';\n\nStep 2\n\nUse the Request ID to retrieve the queries distributed SQL (DSQL) plan from sys.dm_pdw_request_steps\n\n-- Find the distributed query plan steps for a specific query.\n-- Replace request_id with value from Step 1.\n\nSELECT * FROM sys.dm_pdw_request_steps\nWHERE request_id = 'QID####'\nORDER BY step_index;\n\n\nWhen a DSQL plan is taking longer than expected, the cause can be a complex plan with many DSQL steps or just one step taking a long time. If the plan is many steps with several move operations, consider optimizing your table distributions to reduce data movement.\n\nThe Table distribution article explains why data must be moved to solve a query. The article also explains some distribution strategies to minimize data movement.\n\nTo investigate further details about a single step, the operation_type column of the long-running query step and note the Step Index:\n\nProceed with Step 3 for SQL operations: OnOperation, RemoteOperation, ReturnOperation.\nProceed with Step 4 for Data Movement operations: ShuffleMoveOperation, BroadcastMoveOperation, TrimMoveOperation, PartitionMoveOperation, MoveOperation, CopyOperation.\nStep 3\n\nUse the Request ID and the Step Index to retrieve details from sys.dm_pdw_sql_requests, which contains execution information of the query step on all of the distributed databases.\n\n-- Find the distribution run times for a SQL step.\n-- Replace request_id and step_index with values from Step 1 and 3.\n\nSELECT * FROM sys.dm_pdw_sql_requests\nWHERE request_id = 'QID####' AND step_index = 2;\n\n\nWhen the query step is running, DBCC PDW_SHOWEXECUTIONPLAN can be used to retrieve the SQL Server estimated plan from the SQL Server plan cache for the step running on a particular distribution.\n\n-- Find the SQL Server execution plan for a query running on a specific SQL pool or control node.\n-- Replace distribution_id and spid with values from previous query.\n\nDBCC PDW_SHOWEXECUTIONPLAN(1, 78);\n\nStep 4\n\nUse the Request ID and the Step Index to retrieve information about a data movement step running on each distribution from sys.dm_pdw_dms_workers.\n\n-- Find information about all the workers completing a Data Movement Step.\n-- Replace request_id and step_index with values from Step 1 and 3.\n\nSELECT * FROM sys.dm_pdw_dms_workers\nWHERE request_id = 'QID####' AND step_index = 2;\n\nCheck the total_elapsed_time column to see if a particular distribution is taking longer than others for data movement.\nFor the long-running distribution, check the rows_processed column to see if the number of rows being moved from that distribution is larger than others. If so, this finding might indicate skew of your underlying data. One cause for data skew is distributing on a column with many NULL values (whose rows will all land in the same distribution). Prevent slow queries by avoiding distribution on these types of columns or filtering your query to eliminate NULLs when possible.\n\nIf the query is running, you can use DBCC PDW_SHOWEXECUTIONPLAN to retrieve the SQL Server estimated plan from the SQL Server plan cache for the currently running SQL Step within a particular distribution.\n\n-- Find the SQL Server estimated plan for a query running on a specific SQL pool Compute or control node.\n-- Replace distribution_id and spid with values from previous query.\n\nDBCC PDW_SHOWEXECUTIONPLAN(55, 238);\n\n\nDynamic Management Views (DMV) only contains 10,000 rows of data. On heavily utilized systems this means that data held in this table may be lost with hours, or even minutes as data is managed in a first in, first out system. As a result you can potentially lose meaningful information that can help you diagnose query performance issues on your system. In this situation, you should use the Query Store.\n\nYou can also monitor additional aspects of Azure Synapse SQL pools including:\n\nMonitoring waits\nMonitoring tempdb\nMonitoring memory\nMonitoring transaction log\nMonitoring PolyBase\n\nYou can view information about monitoring these areas here\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage workloads in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/4-manage-workloads",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 4 of 8\nNext\nManage workloads in Azure Synapse Analytics\nCompleted\n100 XP\n10 minutes\n\nAzure Synapse Analytics allows you to create, control and manage resource availability when workloads are competing. This allows you to manage the relative importance of each workload when waiting for available resources.\n\nTo facilitate faster load times, you can create a workload classifier for the load user with the “importance” set to above_normal or High. Workload importance ensures that the load takes precedence over other waiting tasks of a lower importance rating. Use this in conjunction with your own workload group definitions for workload isolation to manage minimum and maximum resource allocations during peak and quiet periods.\n\nDedicated SQL pool workload management in Azure Synapse consists of three high-level concepts:\n\nWorkload Classification\nWorkload Importance\nWorkload Isolation\n\nThese capabilities give you more control over how your workload utilizes system resources.\n\nWorkload classification\n\nWorkload management classification allows workload policies to be applied to requests through assigning resource classes and importance.\n\nWhile there are many ways to classify data warehousing workloads, the simplest and most common classification is load and query. You load data with insert, update, and delete statements. You query the data using selects. A data warehousing solution will often have a workload policy for load activity, such as assigning a higher resource class with more resources. A different workload policy could apply to queries, such as lower importance compared to load activities.\n\nYou can also subclassify your load and query workloads. Subclassification gives you more control of your workloads. For example, query workloads can consist of cube refreshes, dashboard queries or ad-hoc queries. You can classify each of these query workloads with different resource classes or importance settings. Load can also benefit from subclassification. Large transformations can be assigned to larger resource classes. Higher importance can be used to ensure key sales data is loaded before weather data or a social data feed.\n\nNot all statements are classified as they do not require resources or need importance to influence execution. DBCC commands, BEGIN, COMMIT, and ROLLBACK TRANSACTION statements are not classified.\n\nWorkload importance\n\nWorkload importance influences the order in which a request gets access to resources. On a busy system, a request with higher importance has first access to resources. Importance can also ensure ordered access to locks. There are five levels of importance: low, below_normal, normal, above_normal, and high. Requests that don't set importance are assigned the default level of normal. Requests that have the same importance level have the same scheduling behavior that exists today.\n\nWorkload isolation\n\nWorkload isolation reserves resources for a workload group. Resources reserved in a workload group are held exclusively for that workload group to ensure execution. Workload groups also allow you to define the amount of resources that are assigned per request, much like resource classes do. Workload groups give you the ability to reserve or cap the amount of resources a set of requests can consume. Finally, workload groups are a mechanism to apply rules, such as query timeout, to requests.\n\nYou can perform the following steps to implement workload management\n\nCreate a workload classifier to add importance to certain queries\n\nYour organization has asked you if there is a way to mark queries executed by the CEO as more important than others, so they don't appear slow due to heavy data loading or other workloads in the queue. You decide to create a workload classifier and add importance to prioritize the CEO's queries.\n\nSelect the Develop hub.\n\nFrom the Develop menu, select the + button (1) and choose SQL Script (2) from the context menu.\n\nIn the toolbar menu, connect to the SQL Pool database to execute the query.\n\nIn the query window, replace the script with the following to confirm that there are no queries currently being run by users logged in as asa.sql.workload01, representing the CEO of the organization or asa.sql.workload02 representing the data analyst working on the project:\n\nSQL\nCopy\n--First, let's confirm that there are no queries currently being run by users logged in workload01 or workload02\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, \nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') \n--and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,s.login_name\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nNow that we have confirmed that there are no running queries, we need to flood the system with queries and see what happens for asa.sql.workload01 and asa.sql.workload02. To do this, we'll run a Azure Synapse Pipeline which triggers queries.\n\nSelect the Integrate hub.\n\nSelect the Lab 08 - Execute Data Analyst and CEO Queries Pipeline (1), which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.\n\nLet's see what happened to all the queries we just triggered as they flood the system. In the query window, replace the script with the following:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following:\n\nNotice that the Importance level for all queries is set to normal.\n\nWe will give our asa.sql.workload01 user queries priority by implementing the Workload Importance feature. In the query window, replace the script with the following:\n\nSQL\nCopy\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n\nWe are executing this script to create a new Workload Classifier named CEO that uses the largerc Workload Group and sets the Importance level of the queries to High.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nLet's flood the system again with queries and see what happens this time for asa.sql.workload01 and asa.sql.workload02 queries. To do this, we'll run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab, run the Lab 08 - Execute Data Analyst and CEO Queries Pipeline, which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries.\n\nIn the query window, replace the script with the following to see what happens to the asa.sql.workload01 queries this time:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status desc\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following:\n\nNotice that the queries executed by the asa.sql.workload01 user have a high importance.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nReserve resources for specific workloads through workload isolation\n\nWorkload isolation means resources are reserved, exclusively, for a workload group. Workload groups are containers for a set of requests and are the basis for how workload management, including workload isolation, is configured on a system. A simple workload management configuration can manage data loads and user queries.\n\nIn the absence of workload isolation, requests operate in the shared pool of resources. Access to resources in the shared pool is not guaranteed and is assigned on an importance basis.\n\nGiven the workload requirements provided by Tailwind Traders, you decide to create a new workload group called CEODemo to reserve resources for queries executed by the CEO.\n\nLet's start by experimenting with different parameters.\n\nIn the query window, replace the script with the following:\n\nSQL\nCopy\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_groups where name = 'CEODemo')\nBEGIN\n    Create WORKLOAD GROUP CEODemo WITH  \n    ( MIN_PERCENTAGE_RESOURCE = 50        -- integer value\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25 --  \n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\nEND\n\n\nThe script creates a workload group called CEODemo to reserve resources exclusively for the workload group. In this example, a workload group with a MIN_PERCENTAGE_RESOURCE set to 50% and REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 25% is guaranteed 2 concurrency.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nIn the query window, replace the script with the following to create a Workload Classifier called CEODreamDemo that assigns a workload group and importance to incoming requests:\n\nSQL\nCopy\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where  name = 'CEODreamDemo')\nBEGIN\n    Create Workload Classifier CEODreamDemo with\n    ( Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n\nThis script sets the Importance to BELOW_NORMAL for the asa.sql.workload02 user, through the new CEODreamDemo Workload Classifier.\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nIn the query window, replace the script with the following to confirm that there are no active queries being run by asa.sql.workload02 (suspended queries are OK):\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nSelect the Integrate hub.\n\nSelect the Lab 08 - Execute Business Analyst Queries Pipeline (1), which will run / trigger asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.\n\nIn the query window, replace the script with the following to see what happened to all the asa.sql.workload02 queries we just triggered as they flood the system:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nYou should see an output similar to the following that shows the importance for each session set to below_normal:\n\nNotice that the running scripts are executed by the asa.sql.workload02 user (1) with an Importance level of below_normal (2). We have successfully configured the business analyst queries to execute at a lower importance than the CEO queries. We can also see that the CEODreamDemo Workload Classifier works as expected.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nReturn to the query window under the Develop hub. In the query window, replace the script with the following to set 3.25% minimum resources per request:\n\nSQL\nCopy\nIF  EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where group_name = 'CEODemo')\nBEGIN\n    Drop Workload Classifier CEODreamDemo\n    DROP WORKLOAD GROUP CEODemo\n    --- Creates a workload group 'CEODemo'.\n        Create  WORKLOAD GROUP CEODemo WITH  \n    (MIN_PERCENTAGE_RESOURCE = 26 -- integer value\n        ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 3.25 -- factor of 26 (guaranteed more than 4 concurrencies)\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\n    --- Creates a workload Classifier 'CEODreamDemo'.\n    Create Workload Classifier CEODreamDemo with\n    (Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n\n Note\n\nConfiguring workload containment implicitly defines a maximum level of concurrency. With a CAP_PERCENTAGE_RESOURCE set to 60% and a REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 1%, up to a 60-concurrency level is allowed for the workload group. Consider the method included below for determining the maximum concurrency: [Max Concurrency] = [CAP_PERCENTAGE_RESOURCE] / [REQUEST_MIN_RESOURCE_GRANT_PERCENT]\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nLet's flood the system again and see what happens for asa.sql.workload02. To do this, we will run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab. Run the Lab 08 - Execute Business Analyst Queries Pipeline, which will run / trigger asa.sql.workload02 queries.\n\nIn the query window, replace the script with the following to see what happened to all of the asa.sql.workload02 queries we just triggered as they flood the system:\n\nSQL\nCopy\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis  not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n\nSelect Run from the toolbar menu to execute the SQL command.\n\nAfter several moments (up to a minute), we should see several concurrent executions by the asa.sql.workload02 user running at below_normal importance. We have validated that the modified Workload Group and Workload Classifier works as expected.\n\nSelect the Monitor hub.\n\nSelect Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.\n\nNext unit: Use Azure Advisor to review recommendations\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Use Azure Advisor to review recommendations - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/5-use-azure-advisor-to-review-recommendations",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 5 of 8\nNext\nUse Azure Advisor to review recommendations\nCompleted\n100 XP\n9 minutes\n\nAzure Advisor provides you with personalized messages that provide information on best practices to optimize the setup of your Azure services. It analyzes your resource configuration and usage telemetry and then recommends solutions that can help you improve the cost effectiveness, performance, Reliability (formerly called High availability), and security of your Azure resources.\n\nThe Advisor may appear when you log into the Azure portal, but you can also access the Advisor by selecting Advisor in the navigation menu.\n\nOn accessing Advisor, a dashboard is presented that provides recommendations in the following areas:\n\nCost\nSecurity\nReliability\nOperational excellence\nPerformance\n\nYou can click on any of the dashboard items for more information. In the following example, the performance dashboard item is showing more information on two high impact items in Azure Synapse Analytics.\n\nYou can also click on each item to get even more information that can help you resolve the issue. In the following example, this is the information that is shown when clicking on the Create statistics on table columns recommendation.\n\nIn this screen, you can click on the view impacted tables to see which tables are being impacted specifically, and there are also links to the help in the Azure documentation that you can use to get more understanding of the issue.\n\nHow Azure Synapse Analytics works with Azure Advisor\n\nAzure Advisor recommendations are free, and the recommendations are based on telemetry data that is generated by Azure Synapse Analytics. The telemetry data that is captured by Azure Synapse Analytics include\n\nData Skew and replicated table information.\nColumn statistics data.\nTempDB utilization data.\nAdaptive Cache.\n\nAzure Advisor recommendations are checked every 24 hours, as the recommendation API is queried against the telemetry generated from with Azure Synapse Analytics, and the recommendation dashboards are then updated to reflect the information that the telemetry has generated. This can then be viewed in the Azure Advisor dashboard.\n\nNext unit: Use dynamic management views to identify and troubleshoot query performance\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Scale compute resources in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/2-scale-compute-resources",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 2 of 8\nNext\nScale compute resources in Azure Synapse Analytics\nCompleted\n100 XP\n8 minutes\n\nOne of the key management features that you have at your disposal within Azure Synapse Analytics, is the ability to scale the compute resources for SQL or Spark pools to meet the demands of processing your data. In SQL pools, the unit of scale is an abstraction of compute power that is known as a data warehouse unit. Compute is separate from storage, which enables you to scale compute independently of the data in your system. This means you can scale up and scale down the compute power to meet your needs.\n\nYou can scale a Synapse SQL pool either through the Azure portal, Azure Synapse Studio or programmatically using TSQL or PowerShell.\n\nIn the Azure portal, you can click on scale icon\n\nAnd then you can adjust the slider to scale the SQL Pool\n\nAnother option to scale is within Azure Synapse Studio, click on the scale icon:\n\nAnd then move the slider as follows:\n\nYou can also make the modification using Transact-SQL\n\nSQL\nCopy\nALTER DATABASE mySampleDataWarehouse\nMODIFY (SERVICE_OBJECTIVE = 'DW300c');\n\n\nOr by using PowerShell\n\nPowerShell\nCopy\nSet-AzSqlDatabase -ResourceGroupName \"resourcegroupname\" -DatabaseName \"mySampleDataWarehouse\" -ServerName \"sqlpoolservername\" -RequestedServiceObjectiveName \"DW300c\"\n\nScaling Apache Spark pools in Azure Synapse Analytics\n\nApache Spark pools for Azure Synapse Analytics uses an Autoscale feature that automatically scales the number of nodes in a cluster instance up and down. During the creation of a new Spark pool, a minimum and maximum number of nodes can be set when Autoscale is selected. Autoscale then monitors the resource requirements of the load and scales the number of nodes up or down. To enable the Autoscale feature, complete the following steps as part of the normal pool creation process:\n\nOn the Basics tab, select the Enable autoscale checkbox.\nEnter the desired values for the following properties:\nMin number of nodes.\nMax number of nodes.\n\nThe initial number of nodes will be the minimum. This value defines the initial size of the instance when it's created. The minimum number of nodes can't be fewer than three.\n\nYou can also modify this in the Azure portal, you can click on auto-scale settings icon\n\nChoose the node size and the number of nodes\n\nand for Azure Synapse Studio as follows\n\nAnd Choose the node size and the number of nodes\n\nAutoscale continuously monitors the Spark instance and collects the following metrics:\n\nExpand table\nMetric\tDescription\nTotal Pending CPU\tThe total number of cores required to start execution of all pending nodes.\nTotal Pending Memory\tThe total memory (in MB) required to start execution of all pending nodes.\nTotal Free CPU\tThe sum of all unused cores on the active nodes.\nTotal Free Memory\tThe sum of unused memory (in MB) on the active nodes.\nUsed Memory per Node\tThe load on a node. A node on which 10 GB of memory is used, is considered under more load than a worker with 2 GB of used memory.\n\nThe following conditions will then autoscale the memory or CPU\n\nExpand table\nScale-up\tScale-down\nTotal pending CPU is greater than total free CPU for more than 1 minute.\tTotal pending CPU is less than total free CPU for more than 2 minutes.\nTotal pending memory is greater than total free memory for more than 1 minute.\tTotal pending memory is less than total free memory for more than 2 minutes.\n\nThe scaling operation can take between 1 -5 minutes. During an instance where there is a scale down process, Autoscale will put the nodes in decommissioning state so that no new executors can launch on that node.\n\nThe running jobs will continue to run and finish. The pending jobs will wait to be scheduled as normal with fewer available nodes.\n\nNext unit: Pause compute in Azure Synapse Analytics\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Pause compute in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/3-pause-compute",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Manage and monitor data warehouse activities in Azure Synapse Analytics \nAdd\nPrevious\nUnit 3 of 8\nNext\nPause compute in Azure Synapse Analytics\nCompleted\n100 XP\n3 minutes\n\nWhen performing the batch movement of data to populate a data warehouse, it is typical for the data engineer to understand the schedule on which the data loads take place. In these circumstances, you may be able to predict the periods of downtime in the data loading and querying process and take advantage of the pause operations to minimize your costs.\n\nIn the Azure portal you can use the Pause command within the dedicated SQL pool\n\nAnd this can also be used within Azure Synapse Studio for Apache Spark pools too, in the Manage hub.\n\nWhich allows you to enable it, and set the number of minutes idle\n\nNext unit: Manage workloads in Azure Synapse Analytics\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n3 minutes\n\nIn this module, you will learn some of the features you can use to manage and monitor Azure Synapse Analytics.\n\nAt the end of this module, you will\n\nScale compute resources in Azure Synapse Analytics\nPause compute in Azure Synapse Analytics\nManage workloads in Azure Synapse Analytics\nUse Azure Advisor to review recommendations\nUse Dynamic Management Views to identify and troubleshoot query performance\nPrerequisites\n\nBefore taking this module, it is recommended that the student is able to:\n\nLog into the Azure portal\nCreate a Synapse Analytics Workspace\nCreate an Azure Synapse Analytics SQL Pool\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Manage and monitor data warehouse activities in Azure Synapse Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nManage and monitor data warehouse activities in Azure Synapse Analytics\nModule\n8 Units\nFeedback\nBeginner\nData Engineer\nAzure Synapse Analytics\n\nLearn how to manage and monitor Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you will:\n\nScale compute resources in Azure Synapse Analytics\nPause compute in Azure Synapse Analytics\nManage workloads in Azure Synapse Analytics\nUse Azure Advisor to review recommendations\nUse Dynamic Management Views to identify and troubleshoot query performance\nAdd\nPrerequisites\nBefore taking this module, it is recommended that you complete Data Fundamentals.\nIntroduction\nmin\nScale compute resources in Azure Synapse Analytics\nmin\nPause compute in Azure Synapse Analytics\nmin\nManage workloads in Azure Synapse Analytics\nmin\nUse Azure Advisor to review recommendations\nmin\nUse dynamic management views to identify and troubleshoot query performance\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/10-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nLoading data warehouse tables is a core task for data engineers. This module introduced some common SQL-based techniques that you can use to stage and load data in a relational data warehouse that's hosted in a dedicated SQL pool in Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/9-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n\nChoose the best response for each of the questions, then select Check your answers.\n\n1. \n\nIn which order should you load tables in the data warehouse?\n\n \n\nStaging tables, then dimension tables, then fact tables\n\nStaging tables, then fact tables, then dimension tables\n\nDimension tables, then staging tables, then fact tables\n\n2. \n\nWhich command should you use to load a staging table with data from files in the data lake?\n\n \n\nCOPY\n\nLOAD\n\nINSERT\n\n3. \n\nWhen a customer changes their phone number, the change should be made in the existing row for that customer in the dimension table. What type of slowly changing dimension does this scenario require?\n\n \n\nType 0\n\nType 1\n\nType 2\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - load data into a relational data warehouse - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/8-exercise-load-data-into-relational-data-warehouse",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Load data into a relational data warehouse \nAdd\nPrevious\nUnit 8 of 10\nNext\nExercise - load data into a relational data warehouse\nCompleted\n100 XP\n40 minutes\n\nNow it's your chance to explore loading and updating data into a relational data warehouse for yourself. In this exercise, you'll create and update fact and dimension tables into a dedicated SQL pool using the various techniques described in this module.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNext unit: Knowledge check\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Perform post load optimization - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/7-perform-post-load-optimization",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nPerform post load optimization\n3 minutes\n\nAfter loading new data into the data warehouse, it's a good idea to rebuild the table indexes and update statistics on commonly queried columns.\n\nRebuild indexes\n\nThe following example rebuilds all indexes on the DimProduct table.\n\nALTER INDEX ALL ON dbo.DimProduct REBUILD\n\n\n Tip\n\nFor more information about rebuilding indexes, see the Indexes on dedicated SQL pool tables in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nUpdate statistics\n\nThe following example creates statistics on the ProductCategory column of the DimProduct table:\n\nCREATE STATISTICS productcategory_stats\nON dbo.DimProduct(ProductCategory);\n\n\n Tip\n\nFor more information about updating statistics, see the Table statistics for dedicated SQL pool in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load fact tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/6-load-fact-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad fact tables\n3 minutes\n\nTypically, a regular data warehouse load operation loads fact tables after dimension tables. This approach ensures that the dimensions to which the facts will be related are already present in the data warehouse.\n\nThe staged fact data usually includes the business (alternate) keys for the related dimensions, so your logic to load the data must look up the corresponding surrogate keys. When the data warehouse slowly changing dimensions, the appropriate version of the dimension record must be identified to ensure the correct surrogate key is used to match the event recorded in the fact table with the state of the dimension at the time the fact occurred.\n\nIn many cases, you can retrieve the latest \"current\" version of the dimension; but in some cases you might need to find the right dimension record based on DateTime columns that indicate the period of validity for each version of the dimension.\n\nThe following example assumes that the dimension records have an incrementing surrogate key, and that the most recently added version of a specific dimension instance (which will have the highest key value) should be used.\n\nINSERT INTO dbo.FactSales\nSELECT  (SELECT MAX(DateKey)\n         FROM dbo.DimDate\n         WHERE FullDateAlternateKey = stg.OrderDate) AS OrderDateKey,\n        (SELECT MAX(CustomerKey)\n         FROM dbo.DimCustomer\n         WHERE CustomerAlternateKey = stg.CustNo) AS CustomerKey,\n        (SELECT MAX(ProductKey)\n         FROM dbo.DimProduct\n         WHERE ProductAlternateKey = stg.ProductID) AS ProductKey,\n        (SELECT MAX(StoreKey)\n         FROM dbo.DimStore\n         WHERE StoreAlternateKey = stg.StoreID) AS StoreKey,\n        OrderNumber,\n        OrderLineItem,\n        OrderQuantity,\n        UnitPrice,\n        Discount,\n        Tax,\n        SalesAmount\nFROM dbo.StageSales AS stg\n\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load slowly changing dimensions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/5-load-slowly-changing-dimensions",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad slowly changing dimensions\n5 minutes\n\nIn most relational data warehouses, you need to handle updates to dimension data and support what are commonly referred to as slowly changing dimensions (SCDs).\n\nTypes of slowly changing dimension\n\nThere are multiple kinds of slowly changing dimension, of which three are commonly implemented:\n\nType 0\n\nType 0 dimension data can't be changed. Any attempted changes fail.\n\nExpand table\nDateKey\tDateAltKey\tDay\tMonth\tYear\n20230101\t01-01-2023\tSunday\tJanuary\t2023\nType 1\n\nIn type 1 dimensions, the dimension record is updated in-place. Changes made to an existing dimension row apply to all previously loaded facts related to the dimension.\n\nExpand table\nStoreKey\tStoreAltKey\tStoreName\n123\tEH199J\tHigh Street Store Town Central Store\nType 2\n\nIn a type 2 dimension, a change to a dimension results in a new dimension row. Existing rows for previous versions of the dimension are retained for historical fact analysis and the new row is applied to future fact table entries.\n\nExpand table\nCustomerKey\tCustomerAltKey\tName\tAddress\tCity\tDateFrom\tDateTo\tIsCurrent\n1211\tjo@contoso.com\tJo Smith\t999 Main St\tSeattle\t20190101\t20230105\tFalse\n2996\tjo@contoso.com\tJo Smith\t1234 9th Ave\tBoston\t20230106\t\tTrue\n\n Note\n\nType 2 dimensions often include columns to track the effective time periods for each version of an entity, and/or a flag to indicate which row represents the current version of the entity. If you’re using an incrementing surrogate key and you only need to track the most recently added version of an entity, then you may not need these columns; but before making that decision, consider how you’ll look up the appropriate version of an entity when a new fact is entered based on the time at which the event the fact relates to occurred.\n\nCombining INSERT and UPDATE statements\n\nLogic to implement Type 1 and Type 2 updates can be complex, and there are various techniques you can use. For example, you could use a combination of UPDATE and INSERT statements.\n\n-- New Customers\nINSERT INTO dbo.DimCustomer\nSELECT stg.*\nFROM dbo.StageCustomers AS stg\nWHERE NOT EXISTS\n    (SELECT * FROM dbo.DimCustomer AS dim\n    WHERE dim.CustomerAltKey = stg.CustNo)\n\n-- Type 1 updates (name)\nUPDATE dbo.DimCustomer\nSET CustomerName = stg.CustomerName\nFROM dbo.StageCustomers AS stg\nWHERE dbo.DimCustomer.CustomerAltKey = stg.CustomerNo;\n\n-- Type 2 updates (StreetAddress)\nINSERT INTO dbo.DimCustomer\nSELECT stg.*\nFROM dbo.StageCustomers AS stg\nJOIN dbo.DimCustomer AS dim\nON stg.CustNo = dim.CustomerAltKey\nAND stg.StreetAddress <> dim.StreetAddress;\n\n\n\nIn the previous example, it's assumed that an incrementing surrogate key based on an IDENTITY column identifies each row, and that the highest value surrogate key for a given alternate key indicates the most recent or \"current\" instance of the dimension entity associated with that alternate key. In practice, many data warehouse designers include a Boolean column to indicate the current active instance of a changing dimension or use DateTime fields to indicate the active time periods for each version of the dimension instance. With these approaches, the logic for a type 2 change must include an INSERT of the new dimension row and an UPDATE to mark the current row as inactive.\n\nUsing a MERGE statement\n\nAs an alternative to using multiple INSERT and UPDATE statements, you can use a single MERGE statement to perform an \"upsert\" operation to insert new records and update existing ones.\n\nMERGE dbo.DimProduct AS tgt\n    USING (SELECT * FROM dbo.StageProducts) AS src\n    ON src.ProductID = tgt.ProductBusinessKey\nWHEN MATCHED THEN\n    -- Type 1 updates\n    UPDATE SET\n        tgt.ProductName = src.ProductName,\n        tgt.ProductCategory = src.ProductCategory,\n        tgt.Color = src.Color,\n        tgt.Size = src.Size,\n        tgt.ListPrice = src.ListPrice,\n        tgt.Discontinued = src.Discontinued\nWHEN NOT MATCHED THEN\n    -- New products\n    INSERT VALUES\n        (src.ProductID,\n        src.ProductName,\n        src.ProductCategory,\n        src.Color,\n        src.Size,\n        src.ListPrice,\n        src.Discontinued);\n\n\n Note\n\nFor more information about the MERGE statement, see the MERGE documentation for Azure Synapse Analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Load time dimension tables - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/4-load-time-dimension-tables",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nLoad time dimension tables\n3 minutes\n\nTime dimension tables store a record for each time interval based on the grain by which you want to aggregate data over time. For example, a time dimension table at the date grain contains a record for each date between the earliest and latest dates referenced by the data in related fact tables.\n\nThe following code example shows how you can generate a sequence of time dimension values based on a date grain.\n\n-- Create a temporary table for the dates we need\nCREATE TABLE #TmpStageDate (DateVal DATE NOT NULL)\n\n-- Populate the temp table with a range of dates\nDECLARE @StartDate DATE\nDECLARE @EndDate DATE\nSET @StartDate = '2019-01-01'\nSET @EndDate = '2023-12-31'\nDECLARE @LoopDate = @StartDate\nWHILE @LoopDate <= @EndDate\nBEGIN\n    INSERT INTO #TmpStageDate VALUES\n    (\n        @LoopDate\n    )\n    SET @LoopDate = DATEADD(dd, 1, @LoopDate)\nEND\n\n-- Insert the dates and calculated attributes into the dimension table\nINSERT INTO dbo.DimDate\nSELECT CAST(CONVERT(VARCHAR(8), DateVal, 112) as INT), -- date key\n    DateVal, --date alt key\n    Day(DateVal) -- day number of month\n    --,  other derived temporal fields as required\nFROM #TmpStageDate\nGO\n\n--Drop temporary table\nDROP TABLE #TmpStageDate\n\n\n Tip\n\nScripting this in SQL may be time-consuming in a dedicated SQL pool – it may be more efficient to prepare the data in Microsoft Excel or an external script and import it using the COPY statement.\n\nAs the data warehouse is populated in the future with new fact data, you periodically need to extend the range of dates related time dimension tables.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]