[
  {
    "title": "Stream ingestion scenarios - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/2-stream-ingestion-scenarios",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nStream ingestion scenarios\n5 minutes\n\nAzure Synapse Analytics provides multiple ways to analyze large volumes of data. Two of the most common approaches to large-scale data analytics are:\n\nData warehouses - relational databases, optimized for distributed storage and query processing. Data is stored in tables and queried using SQL.\nData lakes - distributed file storage in which data is stored as files that can be processed and queried using multiple runtimes, including Apache Spark and SQL.\nData warehouses in Azure Synapse Analytics\n\nAzure Synapse Analytics provides dedicated SQL pools that you can use to implement enterprise-scale relational data warehouses. Dedicated SQL pools are based on a massively parallel processing (MPP) instance of the Microsoft SQL Server relational database engine in which data is stored and queried in tables.\n\nTo ingest real-time data into a relational data warehouse, your Azure Stream Analytics query must write its results to an output that references the table into which you want to load the data.\n\nData lakes in Azure Synapse Analytics\n\nAn Azure Synapse Analytics workspace typically includes at least one storage service that is used as a data lake. Most commonly, the data lake is hosted in an Azure Storage account using a container configured to support Azure Data Lake Storage Gen2. Files in the data lake are organized hierarchically in directories (folders), and can be stored in multiple file formats, including delimited text (such as comma-separated values, or CSV), Parquet, and JSON.\n\nWhen ingesting real-time data into a data lake, your Azure Stream Analytics query must write its results to an output that references the location in the Azure Data Lake Gen2 storage container where you want to save the data files. Data analysts, engineers, and scientists can then process and query the files in the data lake by running code in an Apache Spark pool, or by running SQL queries using a serverless SQL pool.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Ingest streaming data using Azure Stream Analytics and Azure Synapse Analytic - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nIngest streaming data using Azure Stream Analytics and Azure Synapse Analytics\nModule\n8 Units\nFeedback\nIntermediate\nData Engineer\nAzure Stream Analytics\nAzure Synapse Analytics\n\nAzure Stream Analytics provides a real-time data processing engine that you can use to ingest streaming event data into Azure Synapse Analytics for further analysis and reporting.\n\nLearning objectives\n\nAfter completing this module, you'll be able to:\n\nDescribe common stream ingestion scenarios for Azure Synapse Analytics.\nConfigure inputs and outputs for an Azure Stream Analytics job.\nDefine a query to ingest real-time data into Azure Synapse Analytics.\nRun a job to ingest real-time data, and consume that data in Azure Synapse Analytics.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Stream Analytics and Azure Synapse Analytics. Consider completing the following modules first:\n\nGet started with Azure Stream Analytics\nIntroduction to Azure Synapse Analytics\nIntroduction\nmin\nStream ingestion scenarios\nmin\nConfigure inputs and outputs\nmin\nDefine a query to select, filter, and aggregate data\nmin\nRun a job to ingest data\nmin\nExercise - Ingest streaming data into Azure Synapse Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/6-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n5 minutes\n\nAzure Stream Analytics is a platform-as-a-service (PaaS) solution that you can use to process a perpetual stream of data for real-time reporting, automated action, or integration into an enterprise analytical solution.\n\nIn this module, you learned how to:\n\nUnderstand data streams.\nUnderstand event processing.\nGet started with Azure Stream Analytics.\n\nTo learn more about the capabilities of Azure Stream Analytics, see the Azure Stream Analytics documentation.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/5-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\nCheck your knowledge\n1. \n\nWhich definition of stream processing is correct?\n\n \n\nData is processed continually as new data records arrive.\n\nData is collected in a temporary store, and all records are processed together as a batch.\n\nData that is incomplete or contains errors is redirected to separate storage for correction by a human operator.\n\n2. \n\nYou need to process a stream of sensor data, aggregating values over one minute windows and storing the results in a data lake. Which service should you use?\n\n \n\nAzure SQL Database\n\nAzure Cosmos DB\n\nAzure Stream Analytics\n\n3. \n\nYou want to aggregate event data by contiguous, fixed-length, non-overlapping temporal intervals. What kind of window should you use?\n\n \n\nSliding\n\nSession\n\nTumbling\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Get started with Azure Stream Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/4-process-events-azure-stream-analytics",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Get started with Azure Stream Analytics\n15 minutes\n\nNow it's your opportunity to explore Azure Stream Analytics in a sample solution that aggregates streaming data events.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand window functions - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3b-understand-windows",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Implement a Data Streaming Solution with Azure Stream Analytics  Get started with Azure Stream Analytics \nAdd\nPrevious\nUnit 4 of 7\nNext\nUnderstand window functions\nCompleted\n100 XP\n6 minutes\n\nA common goal of stream processing is to aggregate events into temporal intervals, or windows. For example, to count the number of social media posts per minute or to calculate the average rainfall per hour.\n\nAzure Stream Analytics includes native support for five kinds of temporal windowing functions. These functions enable you to define temporal intervals into which data is aggregated in a query. The supported windowing functions are Tumbling, Hopping, Sliding, Session, and Snapshot.\n\nTumbling\n\nTumbling window functions segment a data stream into a contiguous series of fixed-size, non-overlapping time segments and operate against them. Events can't belong to more than one tumbling window.\n\nThe Tumbling window example, represented by the following query, finds the maximum reading value in each one-minute window. Windowing functions are applied in Stream Analytics jobs using the GROUP BY clause of the query syntax. The GROUP BY clause in the following query contains the TumblingWindow() function, which specifies a one-minute window size.\n\nSQL\nCopy\nSELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY TumblingWindow(minute, 1)\n\nHopping\n\nHopping window functions model scheduled overlapping windows, jumping forward in time by a fixed period. It's easiest to think of them as Tumbling windows that can overlap and be emitted more frequently than the window size. In fact, tumbling windows are simply a hopping window whose hop is equal to its size. When you use Hopping windows, events can belong to more than one window result set.\n\nTo create a hopping window, you must specify three parameters. The first parameter indicates the time unit, such as second, minute, or hour. The following parameter sets the window size, which designates how long each window lasts. The final required parameter is the hop size, which specifies how much each window moves forward relative to the previous one. An optional fourth parameter denoting the offset size may also be used.\n\nThe following query demonstrates using a HoppingWindow() where the timeunit is set to second. The windowsize is 60 seconds, and the hopsize is 30 seconds. This query outputs an event every 30 seconds containing the maximum reading value that occurred over the last 60 seconds.\n\nSQL\nCopy\nSELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY HoppingWindow(second, 60, 30)\n\n\nSliding\n\nSliding windows generate events for points in time when the content of the window actually changes. This function model limits the number of windows that need to be considered. Azure Stream Analytics outputs events for only those points in time when an event entered or exited the window. As such, every window contains a minimum of one event. Events in Sliding windows can belong to more than one sliding window, similar to Hopping windows.\n\nThe following query uses the SlidingWindow() function to find the maximum reading value in each one-minute window in which an event occurred.\n\nSQL\nCopy\nSELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY SlidingWindow(minute, 1)\n\nSession\n\nSession window functions cluster together events that arrive at similar times, filtering out periods of time where there's no data. It has three primary parameters: timeout, maximum duration, and partitioning key (optional).\n\nThe occurrence of the first event starts a session window. Suppose another event occurs within the specified timeout from the last ingested event. In that case, the window will be extended to incorporate the new event. However, if no other events occur within the specified timeout period, the window will be closed at the timeout. If events keep happening within the specified timeout, the session window will extend until the maximum duration is reached.\n\nThe following query measures user session length by creating a SessionWindow over clickstream data with a timeoutsize of 20 seconds and a maximumdurationsize of 60 seconds.\n\nSQL\nCopy\nSELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,\n       System.TimeStamp() AS WindowEnd,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY SessionWindow(second, 20, 60)\n\nSnapshot\n\nSnapshot windows groups events by identical timestamp values. Unlike other windowing types, a specific window function isn't required. You can employ a snapshot window by specifying the System.Timestamp() function to your query's GROUP BY clause.\n\nFor example, the following query finds the maximum reading value for events that occur at precisely the same time.\n\nSQL\nCopy\nSELECT System.TimeStamp() AS WindowTime,\n       MAX(Reading) AS MaxReading\nINTO\n    [output]\nFROM\n    [input] TIMESTAMP BY EventProcessedUtcTime\nGROUP BY System.Timestamp()\n\n\nSystem.Timestamp() is considered in the GROUP BY clause as a snapshot window definition because it groups events into a window based on the equality of timestamps.\n\nNext unit: Exercise - Get started with Azure Stream Analytics\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand event processing - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3-understand-event-processing",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand event processing\n5 minutes\n\nAzure Stream Analytics is a service for complex event processing and analysis of streaming data. Stream Analytics is used to:\n\nIngest data from an input, such as an Azure event hub, Azure IoT Hub, or Azure Storage blob container.\nProcess the data by using a query to select, project, and aggregate data values.\nWrite the results to an output, such as Azure Data Lake Gen 2, Azure SQL Database, Azure Synapse Analytics, Azure Functions, Azure event hub, Microsoft Power BI, or others.\n\nOnce started, a Stream Analytics query will run perpetually, processing new data as it arrives in the input and storing results in the output.\n\nStream Analytics guarantees exactly once event processing and at-least-once event delivery, so events are never lost. It has built-in recovery capabilities in case the delivery of an event fails. Also, Stream Analytics provides built-in checkpointing to maintain the state of your job and produces repeatable results. Because Azure Stream Analytics is a platform-as-a-service (PaaS) solution, it's fully managed and highly reliable. Its built-in integration with various sources and destinations and provides a flexible programmability model. The Stream Analytics engine enables in-memory compute, so it offers high performance.\n\nAzure Stream Analytics jobs and clusters\n\nThe easiest way to use Azure Stream Analytics is to create a Stream Analytics job in an Azure subscription, configure its input(s) and output(s), and define the query that the job will use to process the data. The query is expressed using structured query language (SQL) syntax, and can incorporate static reference data from multiple data sources to supply lookup values that can be combined with the streaming data ingested from an input.\n\nIf your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.\n\nInputs\n\nAzure Stream Analytics can ingest data from the following kinds of input:\n\nAzure Event Hubs\nAzure IoT Hub\nAzure Blob storage\nAzure Data Lake Storage Gen2\n\nInputs are generally used to reference a source of streaming data, which is processed as new event records are added. Additionally, you can define reference inputs that are used to ingest static data to augment the real-time event stream data. For example, you could ingest a stream of real-time weather observation data that includes a unique ID for each weather station, and augment that data with a static reference input that matches the weather station ID to a more meaningful name.\n\nOutputs\n\nOutputs are destinations to which the results of stream processing are sent. Azure Stream Analytics supports a wide range of outputs, which can be used to:\n\nPersist the results of stream processing for further analysis; for example by loading them into a data lake or data warehouse.\nDisplay a real-time visualization of the data stream; for example by appending data to a dataset in Microsoft Power BI.\nGenerate filtered or summarized events for downstream processing; for example by writing the results of stream processing to an event hub.\nQueries\n\nThe stream processing logic is encapsulated in a query. Queries are defined using SQL statements that SELECT data fields FROM one or more inputs, filter or aggregate the data, and write the results INTO an output. For example, the following query filters the events from the weather-events input to include only data from events with a temperature value less than 0, and writes the results to the cold-temps output:\n\nSELECT observation_time, weather_station, temperature\nINTO cold-temps\nFROM weather-events TIMESTAMP BY observation_time\nWHERE temperature < 0\n\n\nA field named EventProcessedUtcTime is automatically created to define the time when the event is processed by your Azure Stream Analytics query. You can use this field to determine the timestamp of the event, or you can explicitly specify another DateTime field by using the TIMESTAMP BY clause, as shown in this example. Depending on the input from which the streaming data is read, one or more potential timestamp fields may be created automatically; for example, when using an Event Hubs input, a field named EventQueuedUtcTime is generated to record the time when the event was received in the event hub queue.\n\nThe field used as a timestamp is important when aggregating data over temporal windows, which is discussed next.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Understand data streams - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/2-understand-data-streams",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nUnderstand data streams\n5 minutes\n\nA data stream consists of a perpetual series of data, typically related to specific point-in-time events. For example, a stream of data might contain details of messages submitted to a social media micro-blogging site, or a series of environmental measurements recorded by an internet-connected weather sensor. Streaming data analytics is most often used to better understand change over time. For example, a marketing organization may perform sentiment analysis on social media messages to see if an advertising campaign results in more positive comments about the company or its products, or an agricultural business might monitor trends in temperature and rainfall to optimize irrigation and crop harvesting.\n\nCommon goals for stream analytics include\n\nContinuously analyzing data to report issues or trends.\nUnderstanding component or system behavior under various conditions to help plan future enhancements.\nTriggering specific actions or alerts when certain events occur or thresholds are exceeded.\nCharacteristics of stream processing solutions\n\nStream processing solutions typically exhibit the following characteristics:\n\nThe source data stream is unbounded - data is added to the stream perpetually.\nEach data record in the stream includes temporal (time-based) data indicating when the event to which the record relates occurred (or was recorded).\nAggregation of streaming data is performed over temporal windows - for example, recording the number of social media posts per minute or the average rainfall per hour.\nThe results of streaming data processing can be used to support real-time (or near real-time) automation or visualization, or persisted in an analytical store to be combined with other data for historical analysis. Many solutions combine these approaches to support both real-time and historical analytics.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nToday, massive amounts of real-time data are generated by connected applications, Internet of Things (IoT) devices and sensors, and various other sources. The proliferation of streaming data sources has made the ability to consume and make informed decisions from these data in near-real-time an operational necessity for many organizations.\n\nSome typical examples of streaming data workloads include:\n\nOnline stores analyzing real-time clickstream data to provide product recommendations to consumers as they browse the website.\nManufacturing facilities using telemetry data from IoT sensors to remotely monitor high-value assets.\nCredit card transactions from point-of-sale systems being scrutinized in real-time to detect and prevent potentially fraudulent activities.\n\nAzure Stream Analytics provides a cloud-based stream processing engine that you can use to filter, aggregate, and otherwise process a real-time stream of data from various sources. The results of this processing can then be used to trigger automated activity by a service or application, generate real-time visualizations, or integrate streaming data into an enterprise analytics solution.\n\nIn this module, you'll learn how to get started with Azure Stream Analytics, and use it to process a stream of event data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Get started with Azure Stream Analytics - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nGet started with Azure Stream Analytics\nModule\n7 Units\nFeedback\nBeginner\nData Engineer\nAzure\nAzure Stream Analytics\n\nAzure Stream Analytics enables you to process real-time data streams and integrate the data they contain into applications and analytical solutions.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nUnderstand data streams.\nUnderstand event processing.\nUnderstand window functions.\nGet started with Azure Stream Analytics.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Microsoft Azure and have a basic knowledge of data storage and querying using SQL.\n\nIntroduction\nmin\nUnderstand data streams\nmin\nUnderstand event processing\nmin\nUnderstand window functions\nmin\nExercise - Get started with Azure Stream Analytics\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/7-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nAzure Synapse Link for SQL makes it possible to replicate data from SQL Server 2022 or Azure SQL Database to a dedicated pool in Azure Synapse Analytics with low latency. This replication enables you to analyze operational data in near-real-time without incurring a large resource utilization overhead on your transactional data store.\n\nIn this module, you learned how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/6-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n3 minutes\n1. \n\nFrom which of the following data sources can you use Azure Synapse Link for SQL to replicate data to Azure Synapse Analytics?\n\n \n\nAzure Cosmos DB\n\nSQL Server 2022\n\nAzure SQL Managed Instance\n\n2. \n\nWhat must you create in your Azure Synapse Analytics workspace as a target database for Azure Synapse Link for Azure SQL Database?\n\n \n\nA serverless SQL pool\n\nAn Apache Spark pool\n\nA dedicated SQL pool\n\n3. \n\nYou plan to use Azure Synapse Link for SQL to replicate tables from SQL Server 2022 to Azure Synapse Analytics. What additional Azure resource must you create?\n\n \n\nAn Azure Storage account with an Azure Data Lake Storage Gen2 container\n\nAn Azure Key Vault containing the SQL Server admin password\n\nAn Azure Application Insights resource\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Implement Azure Synapse Link for SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/5-exercise-synapse-link-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Implement Azure Synapse Link for SQL\n35 minutes\n\nNow it's your chance to explore Azure Synapse Link for SQL for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure SQL Database resource in your Azure subscription; and then you'll enable Azure Synapse Link for Azure SQL Database and use it to synchronize data.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Azure Synapse Link for SQL Server 2022 - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/4-synapse-link-sql-server",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure Azure Synapse Link for SQL Server 2022\n5 minutes\n\nMicrosoft SQL Server is one of the world's most commonly used relational database systems. SQL Server 2022 is the latest release, and includes many enhancements and new features; including the ability to be used as a source for Azure Synapse Link.\n\nAzure Synapse Link for SQL Server uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a .parquet file for each source table to a landing zone in Azure Data Lake Storage Gen2; from where the data is imported into tables in the dedicated SQL pool. Subsequently, the change feed process copies all changes as .csv files to the landing zone where they're applied to the target tables.\n\nSynchronization between SQL Server (which can be on-premises or in a private network) and Azure Synapse Analytics is achieved through a self-hosted integration runtime. An integration runtime is a software agent that handles secure connectivity when using Azure Data Factory or Azure Synapse Analytics to transfer data across networks. It must be installed on a Microsoft Windows computer with direct access to your SQL Server instance.\n\n Tip\n\nFor more information about using a self-hosted integration runtime to work with Azure Synapse Analytics, see Create and configure a self-hosted integration runtime.\n\nImplementing Azure Synapse Link for SQL Server 2022\n\nTo use Azure Synapse Link for SQL Server 2022, you need to create storage for the landing zone in Azure and configure your SQL Server instance before creating a link connection in Azure Synapse Analytics.\n\nCreate landing zone storage\n\nYou need to create an Azure Data Lake Storage Gen2 account in your Azure subscription to use as a landing zone. You can't use the default storage for your Azure Synapse Analytics workspace.\n\n Tip\n\nFor more information about provisioning an Azure Data Lake Storage Gen2 account, see Create a storage account to use with Azure Data Lake Storage Gen2.\n\nCreate a master key in the SQL Server database\n\nTo support Azure Synapse Link, your SQL Server database must contain a master key. You can use a CREATE MASTER KEY SQL statement like the following example to create one:\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'my$ecretPa$$w0rd';\n\nCreate a dedicated SQL pool in Azure Synapse Analytics\n\nIn your Azure Synapse Analytics workspace, you need to create a dedicated SQL pool where the target tables will be created. You also need to create master key in this database by using the following SQL statement:\n\nCREATE MASTER KEY\n\nCreate a linked service for the SQL Server source database\n\nNext, in Azure Synapse Analytics, create a linked service for your SQL Server database. When you do this, you need to specify the self-hosted integration runtime to be used for connectivity between SQL Server and Azure Synapse Analytics. If you haven't already configured a self-hosted integration runtime, you can create one now, and then download and install the agent onto a Windows machine in the network where your SQL Server instance is located.\n\nCreate a linked service for your Data Lake Storage Gen2 account\n\nIn addition to the linked service for SQL Server, you need a linked service for the Data Lake Storage Gen2 account that will be used as a landing zone. To support this, you need to add the managed identity of your Azure Synapse Analytics Workspace to the Storage Blob Data Contributor role for your storage account and configure the linked service to use the managed identity for authentication.\n\nCreate a link connection for Azure Synapse Link\n\nFinally, you're ready to create a link connection for Azure Synapse Link data synchronization. As you do so, you'll specify the service link for the SQL Server source database, the individual tables to be replicated, the number of CPU cores to be used for the synchronization process, and the Azure Data Lake Storage Gen2 linked service and folder location for the landing zone.\n\nAfter the link connection is created, you can start it to initialize synchronization. After a short time, the tables will be available to query in the dedicated SQL pool, and will be kept in sync with modifications in the source database by the change feed process.\n\n Tip\n\nLearn more:\n\nFor more information about Synapse Link for SQL Server 2022, see Azure Synapse Link for SQL Server 2022.\nTo learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.\nFor a step-by-step guide to setting up Synapse Link for SQL Server 2022, see Get started with Azure Synapse Link for SQL Server 2022.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Configure Azure Synapse Link for Azure SQL Database - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/3-synapse-link-azure-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nConfigure Azure Synapse Link for Azure SQL Database\n5 minutes\n\nAzure SQL Database is a platform-as-a-service (PaaS) relational database service based on the SQL Server database engine. It's commonly used in cloud-native applications as a scalable, secure, and easy to manage relational database store for operational data.\n\nAzure Synapse Link for Azure SQL Database uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a snapshot of the source tables to the target tables. Subsequently, the change feed process applies all modifications made in the source tables to the target tables.\n\nImplementing Azure Synapse Link for Azure SQL Database\n\nTo use Azure Synapse Link for Azure SQL Database, you need to configure some settings in your Azure SQL Database server, before creating a link connection in Azure Synapse Analytics.\n\nConfigure Azure SQL Database\n\nBefore you can use Azure SQL Database as a source for a linked connection in Azure Synapse Analytics, you must ensure the following settings are configured in the Azure SQL Database server that hosts the database you want to synchronize:\n\nSystem assigned managed identity - enable this option so that your Azure SQL Database server uses a system assigned managed identity.\nFirewall rules - ensure that Azure services can access your Azure SQL Database server.\n\nIn addition to these server-level settings, if you plan to configure the link connection from Azure Synapse Analytics to use a managed identity when connecting to Azure SQL Database, you must create a user for the workspace identity in the database and add it to the db_owner role, as shown in the following code example:\n\nCREATE USER my_synapse_workspace FROM EXTERNAL PROVIDER;\nALTER ROLE [db_owner] ADD MEMBER my_synapse_workspace;\n\n\n Tip\n\nIf you intend to use SQL authentication, you can omit this step.\n\nPrepare the target SQL pool\n\nAzure Synapse Link for Azure SQL Database synchronizes the source data to tables in a dedicated SQL pool in Azure Synapse Analytics. You therefore need to create and start a dedicated SQL pool in your Azure Synapse Analytics workspace before you can create the link connection.\n\nThe database associated with the dedicated SQL pool must include the appropriate schema for the target table. If source tables are defined in a schema other than the default dbo schema, you must create a schema of the same name in the dedicated SQL pool database:\n\nCREATE SCHEMA myschema;\n\nCreate a link connection\n\nTo create a linked connection, add a linked connection on the Integrate page in Azure Synapse Studio. You'll need to:\n\nSelect or create a linked service for your Azure SQL Database. You can create this separately ahead of time, or as part of the process of creating a linked connection for Azure Synapse Link. You can use a managed identity or SQL authentication to connect the linked service to Azure SQL Database.\nSelect the tables in the source database that you want to include in the linked connection.\nSelect the target dedicated SQL pool in which the target tables should be created.\nSpecify the number of CPU cores you want to use to process synchronization. Four driver cores will be used in addition to the number of cores you specify.\n\nAfter creating the linked connection, you can configure the mappings between the source and target tables. In particular, you can specify the table structure (index) type and distribution configuration for the target tables.\n\n Note\n\nSome data types in your source tables may not be supported by specific dedicated SQL pool index types. For example, you cannot use a clustered columnstore index for tables that include VARBINARY(MAX) columns. You can map such tables to a heap (an unindexed table) in the dedicated SQL pool.\n\nWhen the linked connection is configured appropriately, you can start it to initialize synchronization. The source tables are initially copied to the target database as snapshots, and then subsequent data modifications are replicated.\n\n Tip\n\nLearn more:\n\nFor more information about Synapse Link for Azure SQL Database, see Azure Synapse Link for Azure SQL Database.\nTo learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.\nFor a step-by-step guide to setting up Synapse Link for Azure SQL Database, see Get started with Azure Synapse Link for Azure SQL Database. You'll also get a chance to try configuring Synapse Link for Azure SQL Database in the exercise, later in this module.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Link for SQL is a hybrid transactional / analytical processing (HTAP) capability in Azure Synapse Analytics that you can use to synchronize transactional data in Azure SQL Database or Microsoft SQL Server with a dedicated SQL pool in Azure Synapse Analytics. This synchronization enables you to perform near real-time analytical workloads on operational data with minimal impact on the transactional store used by business applications.\n\nIn this module, you'll learn how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "What is Azure Synapse Link for SQL? - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/2-understand-synapse-link-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nWhat is Azure Synapse Link for SQL?\n5 minutes\n\nMany organizations use a relational database in Azure SQL Database or Microsoft SQL Server to support business applications. These databases are optimized for transactional workloads that store and manipulate operational data. Performing analytical queries on the data in these databases to support reporting and data analysis incurs resource contention that can be detrimental to application performance.\n\nA traditional approach to resolving this problem is to implement an extract, transform, and load (ETL) solution that loads data from the operational data store into an analytical store as a batch operation at regular intervals. While this solution supports the analytical workloads required for reporting and data analysis, it suffers from the following limitations:\n\nThe ETL process can be complex to implement and operate.\nThe analytical store is only updated at periodic intervals, so reporting doesn't reflect the most up-to-date operational data.\nAzure Synapse Link for SQL\n\nAzure Synapse Link for SQL addresses the limitations of a traditional ETL process by automatically replicating changes made to tables in the operational database to corresponding tables in an analytical database. After the initial synchronization process, the changes are replicated in near real-time without the need for a complex ETL batch process.\n\nIn the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:\n\nAn Azure SQL Database or SQL Server 2022 instance contains a relational database in which transactional data is stored in tables.\nAzure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.\nThe replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.\nSource and target databases\n\nAzure Synapse Link for SQL supports the following source databases (used as operational data stores):\n\nAzure SQL Database\nMicrosoft SQL Server 2022\n\n Note\n\nAzure Synapse link for SQL is not supported for Azure SQL Managed Instance.\n\nThe target database (used as an analytical data store) must be a dedicated SQL pool in an Azure Synapse Analytics workspace.\n\nThe implementation details for Azure Synapse Link vary between the two types of data source, but the high-level principle is the same - changes made to tables in the source database are synchronized to the target database.\n\nChange feed\n\nAzure Synapse Link for SQL uses the change feed feature in Azure SQL Database and Microsoft SQL Server 2022 to capture changes to the source tables. All data modifications are recorded in the transaction log for the source database. The change feed feature monitors the log and applies the same data modifications in the target database. In the case of Azure SQL Database, the modifications are made directly to the target database. When using Azure Synapse Link for SQL Server, the changes are recorded in files and saved to a landing zone in Azure Data Lake Gen2 storage before being applied to the target database.\n\n Note\n\nChange feed is similar to the change data capture (CDC) feature in SQL Server. The key difference is that CDC is used to reproduce data modifications in a table in the same database as the modified table. Change feed caches the data modification in memory and forwards it to Azure Synapse Analytics.\n\nAfter implementing Azure Synapse Link for SQL, you can use system views and stored procedures in your Azure SQL Database or SQL Server database to monitor and manage change feed activity.\n\n Tip\n\nLearn more:\n\nFor more information about change feed, see Azure Synapse Link for SQL change feed.\nTo learn more about monitoring and managing change feed, see Manage Azure Synapse Link for SQL Server and Azure SQL Database.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Implement Azure Synapse Link for SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nImplement Azure Synapse Link for SQL\nModule\n7 Units\nFeedback\nIntermediate\nData Analyst\nData Engineer\nAzure SQL Database\nAzure Synapse Analytics\nSQL Server\n\nAzure Synapse Link for SQL enables low-latency synchronization of operational data in a relational database to Azure Synapse Analytics.\n\nLearning objectives\n\nIn this module, you'll learn how to:\n\nUnderstand key concepts and capabilities of Azure Synapse Link for SQL.\nConfigure Azure Synapse Link for Azure SQL Database.\nConfigure Azure Synapse Link for Microsoft SQL Server.\nAdd\nPrerequisites\n\nBefore starting this module, you should be familiar with Azure Synapse Analytics, Azure SQL Database, and SQL Server. Consider completing Explore relational database services in Azure and Introduction to Azure Synapse Analytics first.\n\nIntroduction\nmin\nWhat is Azure Synapse Link for SQL?\nmin\nConfigure Azure Synapse Link for Azure SQL Database\nmin\nConfigure Azure Synapse Link for SQL Server 2022\nmin\nExercise - Implement Azure Synapse Link for SQL\nmin\nKnowledge check\nmin\nSummary\nmin\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Summary - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/9-summary",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nSummary\n1 minute\n\nHTAP integration between Azure Cosmos DB and Azure Synapse Analytics enables a range of analytical workloads, including:\n\nSupply chain analytics, forecasting & reporting.\nReal-time personalization.\nIOT predictive maintenance.\n\nIn this module, you learned how to:\n\nConfigure an Azure Cosmos DB Account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\n\nTo learn more about using Azure Synapse Link to enable analytics scenarios, see Azure Synapse Link for Azure Cosmos DB: Near real-time analytics use cases.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Knowledge check - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/8-knowledge-check",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nKnowledge check\n5 minutes\n1. \n\nYou have an Azure Cosmos DB for NoSQL account and an Azure Synapse Analytics workspace. What must you do first to enable HTAP integration with Azure Synapse Analytics?\n\n \n\nConfigure global replication in Azure Cosmos DB.\n\nCreate a dedicated SQL pool in Azure Synapse Analytics.\n\nEnable Azure Synapse Link in Azure Cosmos DB.\n\n2. \n\nYou have an existing container in a Cosmos DB core (SQL) database. What must you do to enable analytical queries over Azure Synapse Link from Azure Synapse Analytics?\n\n \n\nDelete and recreate the container.\n\nEnable Azure Synapse Link in the container to create an analytical store.\n\nAdd an item to the container.\n\n3. \n\nYou plan to use a Spark pool in Azure Synapse Analytics to query an existing analytical store in Azure Cosmos DB. What must you do?\n\n \n\nCreate a linked service for the Azure Cosmos DB database where the analytical store enabled container is defined.\n\nDisable automatic pausing for the Spark pool in Azure Synapse Analytics.\n\nInstall the Azure Cosmos DB SDK for Python package in the Spark pool.\n\n4. \n\nYou're writing PySpark code to load data from an Azure Cosmos DB analytical store into a dataframe. What format should you specify?\n\n \n\ncosmos.json\n\ncosmos.olap\n\ncosmos.sql\n\n5. \n\nYou're writing a SQL code in a serverless SQL pool to query an analytical store in Azure Cosmos DB. What function should you use?\n\n \n\nOPENDATASET\n\nROW\n\nOPENROWSET\n\nCheck your answers\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Exercise - Implement Azure Synapse Link for Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/7-exercise-synapse-link-cosmos-db",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nExercise - Implement Azure Synapse Link for Cosmos DB\n35 minutes\n\nNow it's your chance to explore Azure Synapse Link for Azure Cosmos DB for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure Cosmos DB account in your Azure subscription; and then you'll enable Azure Synapse Link for Azure Cosmos DB and use it to analyze data with Spark and SQL.\n\n Note\n\nTo complete this lab, you will need an Azure subscription in which you have administrative access.\n\nLaunch the exercise and follow the instructions.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query Cosmos DB with Synapse SQL - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/6-query-with-sql",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery Cosmos DB with Synapse SQL\n5 minutes\n\nIn addition to using a Spark pool, you can also query an Azure Cosmos DB analytical container by using a built-in serverless SQL pool in Azure Synapse Analytics. To do this, you can use the OPENROWSET SQL function to connect to the linked service for your Azure Cosmos DB database.\n\nUsing OPENROWSET with an authentication key\n\nBy default, access to an Azure Cosmos DB account is authenticated by an authentication key. You can use this key as part of a connection string in an OPENROWSET statement to connect through a linked service from a SQL pool, as shown in the following example:\n\nSELECT *\nFROM OPENROWSET(​\n    'CosmosDB',\n    'Account=my-cosmos-db;Database=my-db;Key=abcd1234....==',\n    [my-container]) AS products_data\n\n\n Tip\n\nYou can find a primary and secondary key for your Cosmos DB account on its Keys page in the Azure portal.\n\nThe results of this query might look something like the following, including metadata and application-defined fields from the items in the Azure Cosmos DB container:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\n\nThe data is retrieved from the analytical store, and the query doesn't impact the operational store.\n\nUsing OPENROWSET with a credential\n\nInstead of including the authentication key in each call to OPENROWSET, you can define a credential that encapsulates the authentication information for your Cosmos DB account, and use the credential in subsequent queries. To create a credential, use the CREATE CREDENTIAL statement as shown in this example:\n\n CREATE CREDENTIAL my_credential\n WITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n SECRET = 'abcd1234....==';\n\n\nWith the credential in place, you can use it in an OPENROWSET function like this:\n\nSELECT *\nFROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                OBJECT = 'my-container',\n                SERVER_CREDENTIAL = 'my_credential'\n) AS products_data\n\n\nOnce again, the results include metadata and application-defined fields from the analytical store:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\nSpecifying a schema\n\nThe OPENROWSET syntax includes a WITH clause that you can use to define a schema for the resulting rowset. You can use this to specify individual fields and assign data types as shown in the following example:\n\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    productID INT,\n    productName VARCHAR(20)\n ) AS products_data\n\n\nIn this case, assuming the fields in the analytical store include productID and productName, the resulting rowset will resemble the following table:\n\nExpand table\nproductID\tproductName\n123\tWidget\n124\tWotsit\n125\tThingumy\n...\t...\n\nYou can of course specify individual column names in the SELECT clause (for example, SELECT productID, productName ...), so this ability to specify individual columns may seem of limited use. However, consider cases where the source JSON documents stored in the operational store include multiple levels of fields, as show in the following example:\n\n{\n    \"productID\": 126,\n    \"productName\": \"Sprocket\",\n    \"supplier\": {\n        \"supplierName\": \"Contoso\",\n        \"supplierPhone\": \"555-123-4567\"\n    }\n    \"id\": \"62588f072-11c3-42b1-a738-...\",\n    \"_rid\": \"mjMaAL...==\",\n    ...\n}\n\n\nThe WITH clause supports the inclusion of explicit JSON paths, enabling you to handle nested fields and to assign aliases to field names; as shown in this example:\n\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    ProductNo INT '$.productID',\n    ProductName VARCHAR(20) '$.productName',\n    Supplier VARCHAR(20) '$.supplier.supplierName',\n    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'\n ) AS products_data\n\n\nThe results of this query would include the following row for product 126:\n\nExpand table\nProductNo\tProductName\tSupplier\tSupplierPhoneNo\n126\tSprocket\tContoso\t555-123-4567\nCreating a view in a database\n\nIf you need to query the same data frequently, or you need to use reporting and visualization tools that rely on SELECT statements that don't include the OPENROWSET function, you can use a view to abstract the data. To create a view, you should create a new database in which to define it (user-defined views in the master database aren't supported), as shown in the following example:\n\nCREATE DATABASE sales_db\n   COLLATE Latin1_General_100_BIN2_UTF8;\n GO;\n\n USE sales_db;\n GO;\n\n CREATE VIEW products\n AS\n SELECT *\n FROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',\n                 OBJECT = 'my-container',\n                 SERVER_CREDENTIAL = 'my_credential'\n )\n WITH (\n    ProductNo INT '$.productID',\n    ProductName VARCHAR(20) '$.productName',\n    Supplier VARCHAR(20) '$.supplier.supplierName',\n    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'\n ) AS products_data\n GO\n\n\n Tip\n\nWhen creating a database that will access data in Cosmos DB, it's best to use a UTF-8 based collation to ensure compatibility with strings in Cosmos DB.\n\nAfter the view has been created, users and client applications can query it like any other SQL view or table:\n\nSELECT * FROM products;\n\nConsiderations for Serverless SQL pools and Azure Cosmos DB\n\nWhen planning to use a serverless SQL pool to query data in an Azure Cosmos DB analytical store, consider the following best practices:\n\nProvision your Azure Cosmos DB analytical storage and any client applications (for example Microsoft Power BI) in the same region as serverless SQL pool.\n\nAzure Cosmos DB containers can be replicated to multiple regions. If you have a multi-region container, you can specify a region parameter in the OPENROWSET connection string to ensure queries are sent to a specific regional replica of the container.\n\nWhen working with string columns, use the OPENROWSET function with the explicit WITH clause and specify an appropriate data length for the string data.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Query Cosmos DB data with Spark - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/5-query-with-spark",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nQuery Cosmos DB data with Spark\n5 minutes\n\nAfter you've added a linked service for your analytical store enabled Azure Cosmos DB database, you can use it to query the data using a Spark pool in your Azure Synapse Analytics workspace.\n\nLoading Azure Cosmos DB analytical data into a dataframe\n\nFor initial exploration or quick analysis of data from an Azure Cosmos DB linked service, it's often easiest to load data from a container into a dataframe using a Spark-supported language like PySpark (A Spark-specific implementation of Python) or Scala (a Java-based language often used on Spark).\n\nFor example, the following PySpark code could be used to load a dataframe named df from the data in the my-container container connected to using the my_linked_service linked service, and display the first 10 rows of data:\n\n df = spark.read\n     .format(\"cosmos.olap\")\\\n     .option(\"spark.synapse.linkedService\", \"my_linked_service\")\\\n     .option(\"spark.cosmos.container\", \"my-container\")\\\n     .load()\n\ndisplay(df.limit(10))\n\n\nLet's suppose the my-container container is used to store items similar to the following example:\n\n{\n    \"productID\": 123,\n    \"productName\": \"Widget\",\n    \"id\": \"7248f072-11c3-42b1-a368-...\",\n    \"_rid\": \"mjMaAL...==\",\n    \"_self\": \"dbs/mjM...==/colls/mjMaAL...=/docs/mjMaAL...==/\",\n    \"_etag\": \"\\\"54004b09-0000-2300-...\\\"\",\n    \"_attachments\": \"attachments/\",\n    \"_ts\": 1655414791\n}\n\n\nThe output from the PySpark code would be similar to the following table:\n\nExpand table\n_rid\t_ts\tproductID\tproductName\tid\t_etag\nmjMaAL...==\t1655414791\t123\tWidget\t7248f072-11c3-42b1-a368-...\t54004b09-0000-2300-...\nmjMaAL...==\t1655414829\t124\tWotsit\tdc33131c-65c7-421a-a0f7-...\t5400ca09-0000-2300-...\nmjMaAL...==\t1655414835\t125\tThingumy\tce22351d-78c7-428a-a1h5-...\t5400ca09-0000-2300-...\n...\t...\t...\t...\t...\t...\n\nThe data is loaded from the analytical store in the container, not from the operational store; ensuring that there's no querying overhead on the operational store. The fields in the analytical data store include the application-defined fields (in this case productID and productName) and automatically created metadata fields.\n\nAfter loading the dataframe, you can use its native methods to explore the data. For example, the following code creates a new dataframe containing only the productID and productName columns, ordered by the productName:\n\nproducts_df = df.select(\"productID\", \"productName\").orderBy(\"productName\")\n\ndisplay(products_df.limit(10))\n\n\nThe output of this code would look similar this table:\n\nExpand table\nproductID\tproductName\n125\tThingumy\n123\tWidget\n124\tWotsit\n...\t...\nWriting a dataframe to a Cosmos DB container\n\nIn most HTAP scenarios, you should use the linked service to read data into Spark from the analytical store. However you can write the contents of a dataframe to the container as shown in the following example:\n\nmydf.write.format(\"cosmos.oltp\")\\\n    .option(\"spark.synapse.linkedService\", \"my_linked_service\")\\\n    .option(\"spark.cosmos.container\", \"my-container\")\\\n    .mode('append')\\\n    .save()\n\n\n Note\n\nWriting a dataframe to a container updates the operational store and can have an impact on its performance. The changes are then synchronized to the analytical store.\n\nUsing Spark SQL to query Azure Cosmos DB analytical data\n\nSpark SQL is a Spark API that provides SQL language syntax and relational database semantics in a Spark pool. You can use Spark SQL to define metadata for tables that can be queried using SQL.\n\nFor example, the following code creates a table named Products based on the hypothetical container used in the previous examples:\n\n%%sql\n\n-- Create a logical database in the Spark metastore\nCREATE DATABASE mydb;\n\nUSE mydb;\n\n-- Create a table from the Cosmos DB container\nCREATE TABLE products using cosmos.olap options (\n    spark.synapse.linkedService 'my_linked_service',\n    spark.cosmos.container 'my-container'\n);\n\n-- Query the table\nSELECT productID, productName\nFROM products;\n\n\n Tip\n\nThe %%sql keyword at the beginning of the code is a magic that instructs the Spark pool to run the code as SQL rather than the default language (which is usually set to PySpark).\n\nBy using this approach, you can create a logical database in your Spark pool that you can then use to query the analytical data in Azure Cosmos DB to support data analysis and reporting workloads without impacting the operational store in your Azure Cosmos DB account.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create a linked service for Cosmos DB - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/4-implement-synapse-link",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nLearn  Training  Browse  Work with Hybrid Transactional and Analytical Processing Solutions using Azure Synapse Analytics  Implement Azure Synapse Link with Azure Cosmos DB \nAdd\nPrevious\nUnit 4 of 9\nNext\nCreate a linked service for Cosmos DB\nCompleted\n100 XP\n5 minutes\n\nWhen you have an Azure Cosmos DB container with analytical store support, you can create a linked service in an Azure Synapse Analytics workspace to connect to it.\n\nTo create a linked service to an Azure Cosmos DB analytical data store, use Azure Synapse Studio, and add a linked service on the Data page by selecting the Connect to external data option, as shown here:\n\nAs you complete the steps to create your linked service, select the type of Azure Cosmos DB account and then assign your linked service a meaningful name and provide the necessary information to connect to your Azure Cosmos DB database.\n\nTo connect to the Azure Cosmos DB database, you can use any of the following authentication options:\n\nAccount key: Specify an authentication key for your Cosmos DB account.\nService Principal: Use the identity of the Azure Synapse Analytics service.\nSystem Assigned Managed Identity: Use system-assigned managed identity.\nUser Managed Identity: Use a user-defined managed identity.\n\n Tip\n\nFor more information about using managed identities in Microsoft Entra ID, see What are managed identities for Azure resources?\n\nAfter creating a linked service, the Azure Cosmos DB database and its containers will be shown in the Data page of Azure Synapse Studio, as shown here:\n\n Note\n\nThe user interface differentiates between containers with analytical store support and those without by using the following icons:\n\nExpand table\nAnalytical store enabled\tAnalytical store not enabled\n\t\n\nYou can query a container without an analytical store, but you won't benefit from the advantages of an HTAP solution that offloads analytical query overhead from the operational data store.\n\nNext unit: Query Cosmos DB data with Spark\n\nContinue\n\nHaving an issue? We can help!\n\nFor issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .\nFor issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Create an analytical store enabled container - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/3-create-analytical-store-enabled-container",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nCreate an analytical store enabled container\n5 minutes\n\nAfter enabling Azure Synapse Link in an Azure Cosmos DB account, you can create or update a container with support for an analytical store.\n\nAn analytical store is a column-based store within the same container as a row-based operational store. An auto-sync process synchronizes changes in the operational store to the analytical store; from where it can be queried without incurring processing overhead in the operational store.\n\nAnalytical store schema types\n\nAs the data from the operational store is synchronized to the analytical store, the schema is updated dynamically to reflect the structure of the documents being synchronized. The specific behavior of this dynamic schema maintenance depends on the analytical store schema type configured for the Azure Cosmos DB account. Two types of schema representation are supported:\n\nWell-defined: The default schema type for an Azure Cosmos DB for NoSQL account.\nFull fidelity: The default (and only supported) schema type for an Azure Cosmos DB for MongoDB account.\n\nThe analytical store receives JSON data from the operational store and organizes it into a column-based structure. In a well-defined schema, the first non-null occurrence of a JSON field determines the data type for that field. Subsequent occurrences of the field that aren't compatible with the assigned data type aren't ingested into the analytical store.\n\nFor example, consider the following two JSON documents:\n\n{\"productID\": 123, \"productName\": \"Widget\"}\n{\"productID\": \"124\", \"productName\": \"Wotsit\"}\n\n\nThe first document determines that the productID field is a numeric (integer) value. When the second document is encountered, its productID field has a string value, and so isn't imported into the analytical store. The document and the rest of its field is imported, but the incompatible field is dropped. The following columns represent the data in the analytical store:\n\nExpand table\nproductID\tproductName\n123\tWidget\n\tWotsit\n\nIn a full fidelity schema, the data type is appended to each instance of the field, with new columns created as necessary; enabling the analytical store to contain multiple occurrences of a field, each with a different data type, as shown in the following table:\n\nExpand table\nproductID.int32\tproductName.string\tproductID.string\n123\tWidget\t\n\tWotsit\t124\n\n Note\n\nFor more information, see What is Azure Cosmos DB analytical store?.\n\nEnabling analytical store support in a container\n\nYou can enable analytical store support when creating a new container or for an existing container. To enable analytical store support, you can use the Azure portal, or you can use the Azure CLI or Azure PowerShell from a command line or in a script.\n\nUsing the Azure portal\n\nTo enable analytical store support when creating a new container in the Azure portal, select the On option for Analytical Store, as shown here:\n\nAlternatively, you can enable analytical store support for an existing container in the Azure Synapse Link page in the Integrations section of the page for your Cosmos DB account, as shown here:\n\nUsing the Azure CLI\n\nTo use the Azure CLI to enable analytical store support in an Azure Cosmos DB for NoSQL container, run the az cosmosdb sql container create command (to create a new container) or az cosmosdb sql container update command (to configure an existing container) with the --analytical-storage-ttl parameter, assigning a retention time for analytical data. Specifying an -analytical-storage-ttl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.\n\naz cosmosdb sql container create --resource-group my-rg --account-name my-cosmos-db --database-name my-db --name my-container --partition-key-path \"/productID\" --analytical-storage-ttl -1\n\n\nFor an Azure Cosmos DB for MongoDB account, you can use the az cosmosdb mongodb collection create or az cosmosdb mongodb collection update command with the --analytical-storage-ttl parameter. For an Azure Cosmos DB for Apache Gremlin account, use the az cosmosdb gremlin graph create or az cosmosdb gremlin graph update command with the --analytical-storage-ttl parameter.\n\nUsing Azure PowerShell\n\nTo use Azure PowerShell to enable analytical store support in n Azure Cosmos DB for NoSQL container, run the New-AzCosmosDBSqlContainer cmdlet (to create a new container) or Update-AzCosmosDBSqlContainer cmdlet (to configure an existing container) with the -AnalyticalStorageTtl parameter, assigning a retention time for analytical data. Specifying an -AnalyticalStorageTtl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.\n\nNew-AzCosmosDBSqlContainer -ResourceGroupName \"my-rg\" -AccountName \"my-cosmos-db\" -DatabaseName \"my-db\" -Name \"my-container\" -PartitionKeyKind \"hash\" -PartitionKeyPath \"/productID\" -AnalyticalStorageTtl -1\n\n\nFor an Azure Cosmos DB for MongoDB API account, use the New-AzCosmosDBMongoDBCollection or Update-AzCosmosDBMongoDBCollection cmdlet with the -AnalyticalStorageTtl parameter.\n\nConsiderations for enabling analytical store support\n\nAnalytical store support can't be disabled without deleting the container. Setting the analytical store TTL value to 0 or null effectively disables the analytical store by no longer synchronizing new items to it from the operational store and deleting items already synchronized from the analytical store. After setting this value to 0, you can't re-enable analytical store support in the container.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Enable Cosmos DB account to use Azure Synapse Link - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/2-enable-cosmos-db-account-to-use",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nEnable Cosmos DB account to use Azure Synapse Link\n5 minutes\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables integration between Azure Cosmos DB and Azure Synapse Analytics.\n\nIn the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:\n\nAn Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.\nThe container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.\nAzure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.\nAzure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.\nEnabling Azure Synapse Link in Azure Cosmos DB\n\nThe first step in using Azure Synapse Link for Cosmos DB is to enable it in an Azure Cosmos DB account. Azure Synapse Link is supported in the following types of Azure Cosmos DB account:\n\nAzure Cosmos DB for NoSQL\nAzure Cosmos DB for MongoDB\nAzure Cosmos DB for Apache Gremlin (preview)\n\nYou can enable Azure Synapse Link in the Azure portal page for your Cosmos DB account, or by using the Azure CLI or Azure PowerShell from a command line or in a script.\n\nUsing the Azure portal\n\nIn the Azure portal, you can enable Azure Synapse Link for a Cosmos DB account on the Azure Synapse Link page in the Integrations section, as shown below.\n\n Tip\n\nFor Azure Cosmos DB for NoSQL accounts, there's also a link on the Data Explorer page.\n\nUsing the Azure CLI\n\nTo enable Azure Synapse Link using the Azure CLI, run the az cosmosdb create command (to create a new Cosmos DB account) or az cosmosdb update command (to configure an existing Cosmos DB account) with the --enable-analytical-storage true parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.\n\naz cosmosdb update --name my-cosmos-db --resource-group my-rg --enable-analytical-storage true\n\n\nTo enable Azure Synapse Link for an Azure Cosmos DB for Apache Gremlin account, include the --capabilities EnableGremlin parameter.\n\nUsing Azure PowerShell\n\nTo enable Azure Synapse Link using Azure PowerShell, run the New-AzCosmosDBAccount cmdlet (to create a new Cosmos DB account) or Update-AzCosmosDBAccount cmdlet (to configure an existing Cosmos DB account) with the -EnableAnalyticalStorage 1 parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.\n\nUpdate-AzCosmosDBAccount -Name \"my-cosmos-db\" -ResourceGroupName \"my-rg\" -EnableAnalyticalStorage 1\n\nConsiderations for enabling Azure Synapse Link\n\nWhen planning to enable Azure Synapse Link for a Cosmos DB account, consider the following facts:\n\nAfter enabling Azure Synapse Link for an account, you can't disable it.\n\nEnabling Azure Synapse Link doesn't start synchronization of operational data to an analytical store - you must also create or update a container with support for an analytical store.\n\nWhen enabling Azure Synapse Link for a Cosmos DB for NoSQL account using the Azure CLI or PowerShell, you can use the --analytical-storage-schema-type (Azure CLI) or -AnalyticalStorageSchemaType (PowerShell) parameter to specify the schema type as WellDefined (default) or FullFidelity. For a Cosmos DB for MongoDB account, the default (and only supported) schema type is FullFidelity.\n\nAfter a schema type has been assigned, you can't change it.\n\n Note\n\nYou'll learn more about the analytical store and its schema types in the next unit.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  },
  {
    "title": "Introduction - Training | Microsoft Learn",
    "url": "https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/1-introduction",
    "html": "Skip to main content\n\tWe use optional cookies to improve your experience on our websites, such as through social media connections, and to display personalized advertising based on your online activity. If you reject optional cookies, only cookies necessary to provide you the services will be used. You may change your selection by clicking “Manage Cookies” at the bottom of the page. Privacy Statement Third-Party Cookies\nAccept Reject Manage cookies\nLearn\nDocumentation\nTraining\nCredentials\nQ&A\nCode Samples\nAssessments\nShows\nSign in\nTraining\nProducts\nCareer Paths\nBrowse all training\nEducator Center\nStudent Hub\nFAQ & Help\nAdd\nIntroduction\n1 minute\n\nAzure Synapse Analytics Link for Cosmos DB enables hybrid transactional/analytical processing (HTAP) integration between Azure Cosmos DB and Azure Synapse Analytics. By using this HTAP solution, organizations can make operational data in Azure Cosmos DB available for analysis and reporting in Azure Synapse Analytics in near-real time without the need to develop a complex ETL pipeline.\n\nIn this module, you'll learn how to:\n\nConfigure an Azure Cosmos DB account to use Azure Synapse Link.\nCreate an analytical store enabled container.\nCreate a linked service for Azure Cosmos DB.\nAnalyze linked data using Spark.\nAnalyze linked data using Synapse SQL.\n\nNeed help? See our troubleshooting guide or provide specific feedback by reporting an issue.\n\nFeedback\n\nWas this page helpful?\n\nYes\nNo\nEnglish (United States)\nYour Privacy Choices\nTheme\nManage cookies\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2023"
  }
]