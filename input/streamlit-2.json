[
  {
    "title": "imp.png (1462√ó825)",
    "url": "https://blog.streamlit.io/content/images/2022/09/imp.png#border",
    "html": ""
  },
  {
    "title": "new.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/new.gif",
    "html": ""
  },
  {
    "title": "rerun.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/rerun.gif",
    "html": ""
  },
  {
    "title": "test_1.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/test_1.gif#browser",
    "html": ""
  },
  {
    "title": "require_file.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/require_file.gif#browser",
    "html": ""
  },
  {
    "title": "shell.png (1900√ó896)",
    "url": "https://blog.streamlit.io/content/images/2022/09/shell.png#browser",
    "html": ""
  },
  {
    "title": "create_repl.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/create_repl.gif#browser",
    "html": ""
  },
  {
    "title": "app_demo.gif (1920√ó1032)",
    "url": "https://blog.streamlit.io/content/images/2022/09/app_demo.gif#browser",
    "html": ""
  },
  {
    "title": "How to build Streamlit apps on Replit",
    "url": "https://blog.streamlit.io/how-to-build-streamlit-apps-on-replit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build Streamlit apps on Replit\n\nLearn Streamlit by building the Beginner Template Tour\n\nBy Shruti Agarwal\nPosted in Advocate Posts, September 29 2022\nBuild a Streamlit app on Replit\nStep 1: Create a new Repl\nStep 2: Install Streamlit\nStep 3: Write libraries\nStep 4: Write ‚ÄúHello World!‚Äù üëã\nStep 5: Run your Streamlit app\nStep 6: Add more code! ‚ú®\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Shruti Agarwal, and I‚Äôm a Streamlit Creator.\n\nI love to use Streamlit when it comes to building and deploying beautiful apps in minutes.\n\nI can vividly recall when I first tried to build a Streamlit app by using the online IDE (Integrated Development Environment) Replit. The Repl couldn‚Äôt load the app in a browser. It turned out that many Replit users have faced the same issue. So I built the Streamlit Beginner Template Tour (a guide for Streamlit basics) and created a Replit template. It successfully loaded in a browser! üéâ\n\nIn this post, I‚Äôll show you how to do this step-by-step:\n\nStep 1. Create a new Repl\nStep 2. Install Streamlit\nStep 3. Write libraries\nStep 4. Write ‚ÄúHello World!‚Äù üëã\nStep 5. Run your Streamlit app\nStep 6. Add more code! ‚ú®\n\nIf you can‚Äôt wait to try it, here's the app and here‚Äôs the repo.\n\nBuild a Streamlit app on Replit\nStep 1: Create a new Repl\n\nIf you don‚Äôt already have a Replit account, sign up for it and click on ‚Äú+‚Äù to create a Repl. Select ‚ÄúPython template‚Äù and name it ‚Äústreamlit_test‚Äù:\n\nStep 2: Install Streamlit\n\nHead to the ‚ÄúShell‚Äù section of your Repl and type the following commands:\n\n$ pip install streamlit\n$ streamlit --version\n\n\nStep 3: Write libraries\n\nAdd a new file as ¬†requirements.txt ¬†for writing libraries. Write these libraries to use inside your main code:\n\nstreamlit==1.12.2\npandas==1.4.4\nnumpy==1.23.2\n\n\nIt will look something like this:\n\nStep 4: Write ‚ÄúHello World!‚Äù üëã\n\nWrite the below code in main.py file:\n\nimport streamlit as st\nst.title('Hello World!')\nst.write('This is a simple text')\n\n\nThis imports the Streamlit library and adds a title along with the simple text. Your Repl will automatically save your work! Now, it‚Äôs time to run your app. ü§û\n\nStep 5: Run your Streamlit app\n\nGo back to ‚ÄúShell‚Äùand type $streamlit run main.py. If it asks you to register your email, press the Enter key, and your app will open in a new browser:\n\nStep 6: Add more code! ‚ú®\n\nGo ahead and add more code:\n\nimport pandas as pd\nimport numpy as np\n\n# Expander section\nwith st.expander(\"About\"):\n  st.write(\"\"\"Trying to add a data table, chart, sidebar button with \n          ballons, an image, text input & exploring tabs!\"\"\")\n\n# Sidebar section\nwith st.sidebar:\n  st.subheader('This is a Sidebar')\n  st.write('Button with Balloons üéà')\n  if st.button('Click me!‚ú®'):\n    st.balloons()\n  else:\n    st.write(' ')\n\n# Dataframe and Chart display section\nst.subheader('Interactive Data Table')\ndf = pd.DataFrame(\n    np.random.randn(50, 3),  # generates random numeric values!\n    columns=[\"a\", \"b\", \"c\"])\nst.dataframe(df) \n\nst.subheader('Bar Chart üìä')\nst.bar_chart(df)\n\n# Image upload and text input section\nst.subheader('An Image')\nst.image('https://www.scoopbyte.com/wp-content/uploads/2019/12/tom-and-jerry.jpg')\n\nst.subheader('Text Input')\ngreet = st.text_input('Write your name, please!')\nst.write('üëã Hey!', greet)\n\n\n# Tabs section\nst.subheader('Tabs')\ntab1, tab2 = st.tabs([\"TAB 1\", \"TAB 2\"])\n\nwith tab1:\n  st.write('WOW!')\n  st.image(\"https://i.gifer.com/DJR3.gif\", width=400)\n\nwith tab2:\n  st.write('Do you like ice cream? üç®')\n  agree = st.checkbox('Yes! I love it')\n  disagree = st.checkbox(\"Nah! üòÖ\")\n  if agree:\n    st.write('Even I love it ü§§')\n  if disagree:\n    st.write('You are boring üòí')\n\n\nHere‚Äôs the code breakdown:\n\nLibraries\n\npandas ‚Äî for writing a dataframe\n\nnumpy ‚Äî to generate random numbers\n\nContainers\n\nst.expander ‚Äî to add an ‚ÄúAbout‚Äù section\n\nst.sidebar ‚Äî for passing Streamlit elements by using with notation\n\nst.tabs ‚Äî separated tabs to pass Streamlit elements by using with notation\n\nWidgets\n\nst.button and st.balloons ‚Äî a button for throwing balloons üéà\n\nst.text_input ‚Äî a single-line text input\n\nst.checkbox ‚Äî to select multiple options\n\nData Display Elements\n\nst.dataframe ‚Äî displays pandas dataframe as a data table\n\nst.bar_chart‚Äî displays a beautiful bar chart\n\nst.image ‚Äî displays an image\n\nOnce you make these changes, the app will show a message in the top right corner. Click on ‚ÄúAlways rerun‚Äù to see the changes:\n\nCongratulations! ü•≥\n\nNow you know how to build an interactive and beautiful Streamlit app. It‚Äôll look something like this:\n\nExplore more with my Replit template in just two steps:\n\n1. Click on ‚ÄúUse Template‚Äù to fork it and add your own flair!\n\n2. In your forked Repl, go to ‚ÄúShell‚Äù and type:\n\n$ pip install -r requirements.txt\n$ streamlit run streamlit_app.py\n\n\nYou‚Äôll be able to view your app in a browser.\n\nWrapping up\n\nThank you for reading my post! I had so much fun building the Streamlit app and the Replit template. I hope you'll use it to create your own apps. If you want to share what you built or have any questions, please post them in the comments below or connect with me on Twitter, LinkedIn, or GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Classifier_A.gif (1726√ó840)",
    "url": "https://blog.streamlit.io/content/images/2022/01/Classifier_A.gif",
    "html": ""
  },
  {
    "title": "Sequence_Sample_A.gif (1726√ó897)",
    "url": "https://blog.streamlit.io/content/images/2022/01/Sequence_Sample_A.gif",
    "html": ""
  },
  {
    "title": "swarmplot.png (1672√ó858)",
    "url": "https://blog.streamlit.io/content/images/2022/01/swarmplot.png#browser",
    "html": ""
  },
  {
    "title": "electrophorogram.png (1516√ó780)",
    "url": "https://blog.streamlit.io/content/images/2022/01/electrophorogram.png#browser",
    "html": ""
  },
  {
    "title": "input-table.png (1532√ó816)",
    "url": "https://blog.streamlit.io/content/images/2022/01/input-table.png#border",
    "html": ""
  },
  {
    "title": "How to diagnose blood cancer with Streamlit",
    "url": "https://blog.streamlit.io/how-to-diagnose-blood-cancer-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to diagnose blood cancer with Streamlit\n\nBuild a molecular pathology diagnostics app in 4 simple steps\n\nBy Eitan Halper-Stromberg\nPosted in Advocate Posts, January 25 2022\nWhat is the CloneRetriever app?\n1. How to familiarize yourself with the data\n2. How the ‚Äúlymphoma‚Äù tissue differs from the ‚Äúnormal‚Äù tissue\n3. How to combine samples in a SwarmPlot\n4. How to make a classification algorithm\nWrapping up\nReferences\nContents\nShare this post\n‚Üê All posts\n\nEver wondered how to use a bit of tissue (a biopsy, a surgery specimen, or a tube of blood) to diagnose lymphoma? Cue the CloneRetriever app!\n\nIn this post, you‚Äôll learn:\n\nHow to familiarize yourself with the data\nHow the ‚Äúlymphoma‚Äù tissue differs from the ‚Äúnormal‚Äù tissue\nHow to combine samples in a SwarmPlot\nHow to make a classification algorithm\n\nWant to jump right in? Here's the CloneRetriever app.\n\nWhat is the CloneRetriever app?\n\nI‚Äôm a molecular pathologist. I look at a lot of tissue samples. Annotating them by hand as ‚Äúlymphoma‚Äù or ‚Äúnormal‚Äù is very painful. I built the CloneRetriever app to diagnose blood cancer by using a semi-processed Excel file as data input.\n\nHow do you diagnose blood cancers like lymphoma? You examine the tissue under a microscope or sequence its DNA to look for abnormal cells. Alternatively, you can make algorithms look for dominant clonal expansions in DNA sequences consistent with lymphoma.\n\nTo interpret this DNA data, you‚Äôll need heavy-duty lab equipment like a PCR machine, a DNA sequencer, or reagents. If you work in a place where it'd available (for example, a hospital by a university), you‚Äôll get a file containing DNA information.\n\nLet‚Äôs take a look at how to read it.\n\n1. How to familiarize yourself with the data\n\nInside the app, take a look at the video tutorial. Below it, you‚Äôll see the input table. That‚Äôs your DNA data:\n\nThis is four samples worth of data‚Äîsixty rows per sample. Each row represents the quantity of a particular piece of DNA.\n\nIn lymphoma, immune cells proliferate out of control. To diagnose it, you‚Äôd measure a piece of DNA that codes for a certain type of molecule known as immunoglobulin. This bit of DNA is specific to different populations of immune cells. Hence, lymphoma immunoglobulin will be different from the immune cells immunoglobulin.\n\nEach row contains a measurement of a unique piece of immunoglobulin DNA. But how do you know if it‚Äôs lymphoma?\n\nLook for the bits of DNA that are much more abundant. That‚Äôs lymphoma. See the column name descriptions? You‚Äôre solving a classification problem. This sample data is already annotated as lymphoma or not:\n\n2. How the ‚Äúlymphoma‚Äù tissue differs from the ‚Äúnormal‚Äù tissue\n\nBelow the input table, you‚Äôll see a container titled ‚ÄúReconstructed electropherogram.‚Äù It represents one single sample:\n\nThis plot is a histogram.\n\nThe X-axis shows the sequence length. The Y-axis shows the number of particular length DNA sequences. Each histogram has ten bars‚Äîone per sample ‚Äúunit‚Äù sequence. The multicolored stacked bars represent the sequences of the same length.\n\nCancer looks like a tall bar‚Äîa spike. It‚Äôs because cancer sequences come from the cells that proliferate much faster than normal cells. Note: the sample FR1_R19-22-A_S21 is plotted by default. It shows cancer (indicated in the box 'sample to plot'). If you switch the view to the third one down the list, FR1_R19-23-A_S23, you‚Äôll see normal cells. No massive spike!\n\n3. How to combine samples in a SwarmPlot\n\nWhat is a SwarmPlot? It‚Äôs the aggregated view of many DNA samples in one plot.\n\nIn the app, you‚Äôll see a static SwarmPlot and an interactive SwarmPlot with two side-by-side panels. It has your data plotted together. Each point represents 1-3 sequences within a discrete unit of ten sequences.\n\nFor example, change the ‚ÄúEvaluate data at the level of‚Äù from ‚ÄúEvery Sequence‚Äù to ‚ÄúSample‚Äù and hit submit:\n\nYou‚Äôll see one point for each of the four samples. It represents the piece of DNA that‚Äôs most likely derived from lymphoma.\n\nAnd look! The two lymphoma samples in the left panel are within the \"Yes\" column! The app correctly classified them according to these three values: ‚Äúfold change to 4th most prevalent sequence,‚Äù ‚Äúfrac of total,‚Äù and ‚Äúsequence is present in replicate.‚Äù\n\nSo why use ‚ÄúFramework‚Äù or ‚ÄúEvery Sequence‚Äù when you can view the four sample points?\n\nBecause sometimes there are many different DNA pieces that can indicate lymphoma. From a pathologist‚Äôs perspective, they should be reported in a test result. For a longer explanation of the SwarmPlot, read my manuscript.\n\n4. How to make a classification algorithm\n\nUse the three classifier buttons to predict lymphoma:\n\n‚ÄúFold change to 4th most prevalent sequence.‚Äù The ratio of the DNA sequence to the fourth sequence, by rank.\n‚ÄúFrac of total.‚Äù The fraction of the DNA sequence relative to the combined total of every sequence in its measurement set.\n‚ÄúSequence is present in replicate.‚Äù A boolean of ‚Äúthe sequence in question that‚Äôs also present in its replicate.‚Äù\n\nYou can also use these buttons to classify the data in the SwarmPlots. All points in the left SwarmPlot (the positives) appear in the ‚ÄúYes‚Äù column. ¬†All points in the right SwarmPlot (the negatives) appear in the ‚ÄúNo‚Äù column.\n\nThe classifier function result is our diagnosis: ‚Äúlymphoma,‚Äù ‚Äúno-lymphoma‚Äù, or ‚Äúunclear even after looking at the result manually.‚Äù To view it, scroll down to the ‚ÄúNew Classifier True‚Äù output. Right below it, you‚Äôll see the classifier output showing the values of the three most appropriate predictors.\n\nChanging the evaluation level from ‚ÄúEvery Sequence‚Äù to ‚ÄúFramework‚Äù or ‚ÄúSample‚Äù will change the classifier result. At the sample level, the classifier sees only four data points (two positive and two negative), with each contributing one point to the most worrisome lymphoma sequence.\n\nThe default values ‚Äúfold change to 4th most prevalent sequence‚Äù ‚â•4, ‚Äúfrac of total‚Äù ‚â•0, and ‚Äúin replicate‚Äù separate the two positives from one negative but not from the other. In the plot on the right (with negatives), you‚Äôll see one point in the first column (correctly classified as negative) and one point in the third column (incorrectly classified as positive).\n\nHow do you get the perfect separation of the positives and the negatives? The SwarmPlot on the left will show all data points in the third ‚ÄúYes‚Äù column. The SwarmPlot on the right will show all data points in the first ‚ÄúNo‚Äù column.\n\nWrapping up\n\nCongratulations! Now you know how pathologists look at a file containing pieces of DNA and use it to diagnose lymphoma. You learned how to look at the data from the perspective of a single sample and in a combined view. You played around with the predictor threshold values to see how that changed the outcome of a classification.\n\nThank you for reading this article. I hope it helped you see how molecular pathologists think about blood cancer in the context of looking at someone‚Äôs DNA. If you have any questions or feedback, please leave them in the comments below or reach out to me at cloneRetriever@jhmi.edu.\n\nReferences\n\nHalper-Stromberg, Eitan (2021). CloneRetriever: An Automated Algorithm to Identify Clonal B and T Cell Gene Rearrangements by Next-Generation Sequencing for the Diagnosis of Lymphoid Malignancies\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "streamlit-st_labelling-2021-07-29-11-07-52.gif (1440√ó772)",
    "url": "https://blog.streamlit.io/content/images/2021/09/streamlit-st_labelling-2021-07-29-11-07-52.gif#browser",
    "html": ""
  },
  {
    "title": "game-1-1.gif (1200√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/01/game-1-1.gif#border",
    "html": ""
  },
  {
    "title": "Streamlit (Page 21)",
    "url": "https://blog.streamlit.io/page/21/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAdding beta and experimental ‚Äúchannels‚Äù to Streamlit\n\nIntroducing the st.beta and st.experimental namespaces\n\nProduct\nby\nTC Ricks\n,\nMay 6 2020\nTry Nightly Build for cutting-edge Streamlit\n\nA new style of release for anyone who wants the most up-to-date Streamlit version\n\nProduct\nby\nTC Ricks\n,\nApril 17 2020\nThe Streamlit roadmap‚Äîbig plans for 2020!\n\nDevoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers\n\nProduct\nby\nAdrien Treuille\n,\nFebruary 27 2020\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "py3dmol.png (1754√ó926)",
    "url": "https://blog.streamlit.io/content/images/2022/05/py3dmol.png#border",
    "html": ""
  },
  {
    "title": "Streamlit (Page 20)",
    "url": "https://blog.streamlit.io/page/20/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nGravitational-wave apps help students learn about black holes\n\nExploring distant space with gravitational waves\n\nAdvocate Posts\nby\nJonah Kanner\n,\nDecember 15 2020\nElm, meet Streamlit\n\nA tutorial on how to build Streamlit components using Elm\n\nTutorials\nby\nHenrikh Kantuni\n,\nDecember 8 2020\nBuild knowledge graphs with the Streamlit Agraph component\n\nA powerful and lightweight library for visualizing networks/graphs\n\nAdvocate Posts\nby\nChristian Klose\n,\nNovember 25 2020\nTesting Streamlit apps using SeleniumBase\n\nHow to create automated visual tests\n\nTutorials\nby\nRandy Zwitch\n,\nNovember 23 2020\nNew UC Davis tool tracks California's COVID-19 cases by region\n\nRegional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine\n\nAdvocate Posts\nby\nPranav Pandit\n,\nNovember 19 2020\nIntroducing Streamlit Sharing\n\nThe new Streamlit platform for deploying, managing, and sharing your apps\n\nProduct\nby\nAdrien Treuille\n,\nOctober 15 2020\nDeploying Streamlit apps using Streamlit sharing\n\nA sneak peek into Streamlit's new deployment platform\n\nTutorials\nby\nTyler Richards\n,\nOctober 15 2020\nNew layout options for Streamlit\n\nIntroducing new layout primitives‚Äîcolumns, containers, and expanders!\n\nProduct\nby\nAustin Chen\n,\nOctober 8 2020\nIntroducing Streamlit Components\n\nA new way to add and share custom functionality for Streamlit apps\n\nProduct\nby\nAdrien Treuille\n,\nJuly 14 2020\nAnnouncing Streamlit's $21M Series¬†A\n\nDeveloping new superpowers for the data science community\n\nProduct\nby\nAdrien Treuille\n,\nJune 16 2020\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "plot.png (1850√ó730)",
    "url": "https://blog.streamlit.io/content/images/2022/05/plot.png#border",
    "html": ""
  },
  {
    "title": "Graphical-Abstract.png (1886√ó2026)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Graphical-Abstract.png#border",
    "html": ""
  },
  {
    "title": "annotation-1.png (2000√ó807)",
    "url": "https://blog.streamlit.io/content/images/2022/05/annotation-1.png#browser",
    "html": ""
  },
  {
    "title": "Roland Dunbrack - Streamlit",
    "url": "https://blog.streamlit.io/author/roland/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Roland Dunbrack\n1 post\nHow to share scientific analysis through a Streamlit app\n\n3 easy steps to share your study results with fellow scientists\n\nAdvocate Posts\nby\nMitchell Parker and¬†\n1\n¬†more,\nMay 12 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mitchell Parker - Streamlit",
    "url": "https://blog.streamlit.io/author/mitchell/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mitchell Parker\n1 post\nHow to share scientific analysis through a Streamlit app\n\n3 easy steps to share your study results with fellow scientists\n\nAdvocate Posts\nby\nMitchell Parker and¬†\n1\n¬†more,\nMay 12 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 19)",
    "url": "https://blog.streamlit.io/page/19/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nOur $35 million Series B\n\nWe‚Äôre excited to announce a new funding round led by Sequoia üå≤\n\nProduct\nby\nAdrien Treuille\n,\nApril 7 2021\nMonthly rewind > March 2021\n\nYour March look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 5 2021\nAnnouncing Theming for Streamlit apps! üé®\n\nTry out the new dark mode and custom theming capabilities\n\nProduct\nby\nAbhi Saini\n,\nMarch 18 2021\nMonthly rewind > February 2021\n\nYour February look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 8 2021\nHow to use Roboflow and Streamlit to visualize object detection output\n\nBuilding an app for blood cell count detection\n\nAdvocate Posts\nby\nMatt Brems\n,\nFebruary 23 2021\nDeveloping a streamlit-webrtc component for real-time video processing\n\nIntroducing the WebRTC component for real-time media streams\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nFebruary 12 2021\nMonthly rewind > January 2021\n\nYour January look back at new features and great community content\n\nMonthly Rewind\nby\nTC Ricks\n,\nFebruary 8 2021\nStreamlit ‚ù§Ô∏è Firestore\n\nUse Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!\n\nTutorials\nby\nAustin Chen\n,\nJanuary 27 2021\nStreamlit Components, security, and a five-month quest to ship a single line of code\n\nThe story of allow-same-origin\n\nTutorials\nby\nTim Conkling\n,\nJanuary 20 2021\nArup and New Story use data to help combat pandemic related evictions\n\nMaking data accessible to help address the eviction crisis\n\nAdvocate Posts\nby\nJared Stock\n,\nJanuary 7 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to share scientific analysis through a Streamlit app",
    "url": "https://blog.streamlit.io/how-to-share-scientific-analysis-through-a-streamlit-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to share scientific analysis through a Streamlit app\n\n3 easy steps to share your study results with fellow scientists\n\nBy Mitchell Parker and Roland Dunbrack\nPosted in Advocate Posts, May 12 2022\nWhat is Rascore?\nHow to share an explorable scientific dataset\nHow to visualize the 3D structure of human proteins\nHow to make informative data plots\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHave you ever done an amazing scientific analysis and wanted to share it? We wanted the same. That‚Äôs why we built Rascore, a Streamlit app for sharing study results with fellow researchers to make new discoveries.\n\nIn this post, you‚Äôll learn:\n\nHow to share an explorable scientific dataset\nHow to visualize the 3D structure of human proteins\nHow to make informative data plots\n\nTLDR? Here is our app. Or jump straight into the repo code! üßë‚Äçüíª\n\nBut before we get into the exciting stuff, let‚Äôs talk about...\n\nWhat is Rascore?\n\nRascore is an app for analyzing the 3D structure of the tumor-associated RAS proteins (KRAS, NRAS, and HRAS‚Äîthe most common cancer drivers). Rascore helps scientists explore and compare published structural models of RAS proteins in the Protein Data Bank (PDB), as well as simplify biological study and facilitate drug discovery.\n\nAlmost all RAS structures are determined by X-ray crystallography. Because of the experiment conditions like mutation status or bound inhibitors, the structures come out differently. In Rascore, we group similar structures by their 3D configuration to examine their properties and how they‚Äôre correlated with conditions.\n\nHow to share an explorable scientific dataset\n\nYou can download all RAS protein structural models from the PDB, but they‚Äôre not annotated. We wanted to automate the annotation of each RAS structure by its biological features (read more in our paper ‚ÄúDelineating The RAS Conformational Landscape‚Äù).\n\nWe also wanted to let researchers explore our annotated dataset and download subsets‚Äîlike all RAS structures with a specific mutation or bound drugs at a certain site.\n\nHere is the code to display datasets as a table and download it (replace st.table with st.dataframe to make it scrollable):\n\nUse this code to display a table:\n\nimport streamlit as st\n\ndef show_st_table(df, st_col=None, hide_index=True):\n\t\t\"\"\"\n\t\tShow table in Streamlit application\n\n\t\tParameters\n\t\t----------\n\t\tdf: pandas.DataFrame\n\t\tst_col: st.columns object\n\t\thide_index: bool\n\t\t\tWhether to display (True) or hide (False)\n\t\t\tthe indices of the displayed pandas\n\t\t\tDataFrame\n\t\t\"\"\"\n    if hide_index:\n        hide_table_row_index = \"\"\"\n                <style>\n                tbody th {display:none}\n                .blank {display:none}\n                </style>\n                \"\"\"\n        st.markdown(hide_table_row_index, unsafe_allow_html=True)\n\n    if st_col is None:\n        st.table(df)\n    else:\n        st_col.table(df)\n\n\nUse this code to download a table:\n\ndef encode_st_df(df):\n\t\t\"\"\"\n\t\tEncode pandas DataFrame in utf-8 format\n\n\t\tParameters\n\t\t----------\n\t\tdf: pandas.DataFrame\n\t\t\"\"\"\n    return df.to_csv(sep=\"\\\\t\", index=False).encode(\"utf-8\")\n\ndef download_st_df(df, file_name, download_text, st_col=None):\n\t\t\"\"\"\n\t\tDownload pandas DataFrame in Streamlit application\n\n\t\tParameters\n\t\t----------\n\t\tdf: pandas.DataFrame\n\t\tfile_name: str\n\t\t\tName of file (e.g., rascore_table.tsv)\n\t\tdownload_text: str\n\t\t\tText on download button (e.g., Download Table)\n\t\tst_col: st.columns object\n\t\t\"\"\"\n    if st_col is None:\n        st.download_button(\n            label=download_text,\n            data=encode_st_df(df),\n            file_name=file_name,\n        )\n    else:\n        st_col.download_button(\n            label=download_text,\n            data=encode_st_df(df),\n            file_name=file_name,\n        )\n\nHow to visualize the 3D structure of human proteins\n\nThe data in Rascore relates only to the 3D structure of RAS proteins. We wanted researchers to compare structural models with different cancer-associated mutations or bound drugs.\n\nLuckily, Jos√© Manuel N√°poles Duarte made a Streamlit plugin for visualizing protein structures by using Py3DMol. But Py3DMol doesn‚Äôt highlight protein structure parts like drug binding sites. So we created a highlighting function.\n\nBelow are the input values for parameters ending in ‚Äú_lst‚Äù. They‚Äôre non-intuitive and relate to highlighting selected parts of protein structures. Each ‚Äú_lst‚Äù takes a nested list as the input with a required object at each index of each sublist (see this doc for making a selection and coloring dictionaries):\n\nParameter\tPurpose\tIndex 0\tIndex 1\tIndex 2\nstyle_lst\tTo stylize parts of protein structures by changing 3D representation or coloring scheme\tSelection Dictionary (e.g., {\"chain\":\"A\", \"rest\": \"25-40\"]\\}\tColoring Dictionary (e.g., {\"stick\": {\"colorscheme\": \"amino\", \"radius\": 0.2}})\tNA\nlabel_lst\tTo apply custom labels to certain parts of protein structure\tLabel String\tColoring Dictionary\tSelection Dictionary\nreslabel_lst\tTo apply standard labels to residue (amino acid identity and linear position)\tSelection Dictionary\tColoring Dictionary\tNA\nsurface_lst\tTo add surface over 3D representation of protein structures\tColoring Dictionary\tSelection Dictionary\tNA\n\nHere is the code:\n\nimport py3Dmol\nfrom stmol import showmol\n\ndef show_st_3dmol(\n    pdb_code,\n    style_lst=None,\n    label_lst=None,\n    reslabel_lst=None,\n    zoom_dict=None,\n    surface_lst=None,\n    cartoon_style=\"trace\",\n    cartoon_radius=0.2,\n    cartoon_color=\"lightgray\",\n    zoom=1,\n    spin_on=False,\n    width=900,\n    height=600,\n):\n\n\"\"\"\nShow 3D view of protein structure from the \nProtein Data Bank (PDB)\n\nParameters\n----------\npdb_code: str\n\tFour-letter code of protein structure in the PDB\n\t(e.g., 5P21)\nstyle_lst: list of lists of dicts\n\tA nested list with each sublist containing a \n\tselection dictionary at index 0 and coloring\n\tdictionary at index 1\nlabel_lst: list of lists of dicts\n\tA nested list with each sublist containing a \n\tlabel string at index 0, coloring dictionary\n\tat index 1, and selection dictionary at\n\tindex 2\nreslabel_lst: list of lists of dicts\n\tA nested list with each sublist containing a \n\tselection dictionary at index 0 and coloring\n\tdictionary at index 1\nzoom_dict: dict\nsurface_lst: list of lists of dicts\n\tA nested list with each sublist containing a \n\tcoloring dictionary at index 0 and selection\n\tdictionary at index 1\ncartoon_style: str\n\tStyle of protein structure backbone cartoon \n\trendering, which can be \"trace\", \"oval\", \"rectangle\", \n\t\"parabola\", or \"edged\"\ncartoon_radius: float\n\tRadius of backbone cartoon rendering\ncartoon_color: str\n\tColor of backbone cartoon rendering\nzoom: float\n\tLevel of zoom into protein structure\n\tin unit of Angstroms\nspin_on: bool\n\tBoolean specifying whether the visualized\n\tprotein structure should be continually \n\tspinning (True) or not (False)\nwidth: int\n\tWidth of molecular viewer\nheight: int\n\tHeight of molecular viewer\n\"\"\"\n    view = py3Dmol.view(query=f\"pdb:{pdb_code.lower()}\", width=width, height=height)\n\n    view.setStyle(\n        {\n            \"cartoon\": {\n                \"style\": cartoon_style,\n                \"color\": cartoon_color,\n                \"thickness\": cartoon_radius,\n            }\n        }\n    )\n\n    if surface_lst is not None:\n        for surface in surface_lst:\n            view.addSurface(py3Dmol.VDW, surface[0], surface[1])\n\n    if style_lst is not None:\n        for style in style_lst:\n            view.addStyle(\n                style[0],\n                style[1],\n            )\n\n    if label_lst is not None:\n        for label in label_lst:\n            view.addLabel(label[0], label[1], label[2])\n\n    if reslabel_lst is not None:\n        for reslabel in reslabel_lst:\n            view.addResLabels(reslabel[0], reslabel[1])\n\n    if zoom_dict is None:\n        view.zoomTo()\n    else:\n        view.zoomTo(zoom_dict)\n\n    view.spin(spin_on)\n\n    view.zoom(zoom)\n    showmol(view, height=height, width=width)\n\n\nHow to make informative data plots\n\nVisual comparison of individual RAS structures is great, but there are hundreds of them to sift through.\n\nWe created an easy way to compare calculated metrics across RAS structures of different groups such as druggability score or pocket volumes. Possible data visualizations are scatterplots and box plots. You can make them with Matplotlib and load them into your app by using this function:\n\nimport streamlit as st\nfrom io import BytesIO\n\ndef show_st_fig(fig, st_col=None):\n    byt = BytesIO()\n    fig.savefig(byt, format=\"png\")\n    if st_col is None:\n        st.image(byt)\n    else:\n        st_col.image(byt)\n\n\nWrapping up\n\nRascore is an app for researchers to explore the 3D structural models of cancer-associated RAS proteins. Streamlit gave us easy dataset navigation, 3D protein structures visualization, and plotted data display. We hope more researchers use Streamlit to share their study results with the scientific community!\n\nIf you have questions, please post them in the comments below or reach out to us on Twitter at @Mitch_P and @RolandDunbrack or email us at mip34@drexel.edu and roland.dunbrack@gmail.com.\n\nThank you for reading our story, and happy app-building! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 18)",
    "url": "https://blog.streamlit.io/page/18/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to make a great Streamlit app: Part II\n\nA few layout and style tips to make your apps look even more visually appealing!\n\nTutorials\nby\nAbhi Saini\n,\nJune 22 2021\nEasy monitoring of dbt Cloud jobs with Streamlit\n\nHow the Cazoo data science team built their dbt Cloud + Streamlit app\n\nAdvocate Posts\nby\nMartin Campbell\n,\nJune 11 2021\nMonthly rewind > May 2021\n\nYour May look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2021\nHow to make a great Streamlit app\n\nDesigning an app your users will love\n\nTutorials\nby\nAbhi Saini\n,\nJune 2 2021\nMaking apps for the Rasa research team (and open source community!)\n\nHelping Rasa users understand their models\n\nAdvocate Posts\nby\nVincent D. Warmerdam\n,\nMay 12 2021\nMonthly rewind > April 2021\n\nYour April look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2021\nIntroducing Submit button and Forms üìÉ\n\nWe're releasing a pair of new commands called st.form and st.form_submit_button!\n\nTutorials\nby\nAbhi Saini\n,\nApril 29 2021\nStreamlit ‚ù§Ô∏è Firestore (continued)\n\nAka the NoSQL sequel: Building a Reddit clone and deploying it securely\n\nTutorials\nby\nAustin Chen\n,\nApril 22 2021\nBuild a Jina neural search with Streamlit\n\nUse Jina to search text or images with the power of deep learning\n\nAdvocate Posts\nby\nAlex C-G\n,\nApril 15 2021\nAdd secrets to your Streamlit apps\n\nUse Secrets Management in Streamlit sharing to securely connect to private data sources\n\nTutorials\nby\nJames Thompson\n,\nApril 9 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 17)",
    "url": "https://blog.streamlit.io/page/17/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > August 2021\n\nYour August look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2021\nDeploying a cloud-native Coiled app\n\nHow Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users\n\nAdvocate Posts\nby\nRichard Pelgrim\n,\nSeptember 7 2021\n0.88.0 release notes\n\nThis release launches st.download_button as well as other improvements and bug fixes\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 3 2021\nLabeling ad videos with Streamlit\n\nHow Wavo.me uses Streamlit‚Äôs Session State to create labeling tasks\n\nAdvocate Posts\nby\nAnastasia Glushko\n,\nSeptember 2 2021\nStreamlit gains a major new spell book\n\nA tome to the magical fields of Python, algorithms, visualization, and machine learning\n\nProduct\nby\nAdrien Treuille\n,\nAugust 20 2021\nMonthly rewind > July 2021\n\nYour July look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 5 2021\nAll in on Apache Arrow\n\nHow we improved performance by deleting over 1k lines of code\n\nProduct\nby\nHenrikh Kantuni\n,\nJuly 22 2021\n6 tips for improving your Streamlit app performance\n\nMoving your Streamlit app from analysis to production\n\nTutorials\nby\nRandy Zwitch\n,\nJuly 20 2021\nMonthly rewind > June 2021\n\nYour June look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 5 2021\nSession State for Streamlit üéà\n\nYou can now store information across app interactions and reruns!\n\nProduct\nby\nAbhi Saini\n,\nJuly 1 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 16)",
    "url": "https://blog.streamlit.io/page/16/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > October 2021\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2021\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\n\nStreamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.\n\nProduct\nby\nAdrien Treuille\n,\nNovember 2 2021\nDetecting parking spots with Streamlit\n\nHow to build a Streamlit parking spot app in 8 simple steps\n\nAdvocate Posts\nby\nJeffrey Jex\n,\nOctober 26 2021\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\nLaunching a brand-new docs site ü•≥\n\nImproved layout, easier navigation, and faster search\n\nProduct\nby\nSnehan Kekre\n,\nOctober 13 2021\nMonthly rewind > September 2021\n\nYour September look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2021\nAnnouncing Streamlit 1.0! üéà\n\nStreamlit used to be the simplest way to write data apps. Now it's the most powerful\n\nProduct\nby\nAdrien Treuille\n,\nOctober 5 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nNew experimental primitives for caching (that make your app 10x faster!)\n\nHelp us test the latest evolution of st.cache\n\nProduct\nby\nAbhi Saini and¬†\n1\n¬†more,\nSeptember 22 2021\nCommon app problems: Resource limits\n\n5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud\n\nTutorials\nby\nJohannes Rieke\n,\nSeptember 9 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "markdown-1.gif (1200√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/01/markdown-1.gif#border",
    "html": ""
  },
  {
    "title": "Streamlit (Page 15)",
    "url": "https://blog.streamlit.io/page/15/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to create interactive books with Streamlit in 5 steps\n\nUse streamlit_book library to create interactive books and presentations\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nJanuary 20 2022\nHow to master Streamlit for data¬†science\n\nThe essential Streamlit for all your data¬†science needs\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 18 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\nStreamlit Cloud is now SOC 2 Type 1 compliant\n\nWe have completed a full external audit of our security practices\n\nProduct\nby\nAmanda Kelly\n,\nJanuary 11 2022\nMonthly rewind > December 2021\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 7 2022\nCreating satellite timelapse with Streamlit and Earth Engine\n\nHow to create a satellite timelapse for any location around the globe in 60 seconds\n\nAdvocate Posts\nby\nQiusheng Wu\n,\nDecember 15 2021\nDeploy a private app for free! üéâ\n\nAnd... get unlimited public apps\n\nProduct\nby\nAbhi Saini\n,\nDecember 9 2021\nMonthly rewind > November 2021\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2021\nFinding your look-alikes with semantic search\n\nHow Pinecone used Streamlit to create a Hacker News Doppelg√§nger app\n\nAdvocate Posts\nby\nGreg Kogan\n,\nDecember 1 2021\nForecasting with Streamlit Prophet\n\nHow Artefact built a Streamlit app to train time-series forecasting models\n\nAdvocate Posts\nby\nMaxime Lutel\n,\nNovember 10 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 14)",
    "url": "https://blog.streamlit.io/page/14/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n3 steps to fix app memory leaks\n\nHow to detect if your Streamlit app leaks memory and identify faulty code\n\nTutorials\nby\nGeorge Merticariu\n,\nApril 14 2022\nMonthly rewind > March 2022\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 7 2022\n30 Days of Streamlit\n\nA fun challenge to learn and practice using Streamlit\n\nAdvocate Posts\nby\nChanin Nantasenamat\n,\nApril 1 2022\nSogeti creates an educational Streamlit app for data preprocessing\n\nLearn how to use Sogeti‚Äôs Data Quality Wrapper\n\nAdvocate Posts\nby\nTijana Nikolic\n,\nMarch 8 2022\nMonthly rewind > February 2022\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 7 2022\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nProduct\nby\nAdrien Treuille and¬†\n2\n¬†more,\nMarch 2 2022\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nAdvocate Posts\nby\nNikolina Sarcevic and¬†\n2\n¬†more,\nFebruary 17 2022\nMonthly rewind > January 2022\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 7 2022\nHow Delta Dental uses Streamlit to make lightning-fast decisions\n\nFrom an idea to a prototype to production in just two weeks\n\nCase study\nby\nAmanda Kelly\n,\nFebruary 1 2022\nHow to diagnose blood cancer with Streamlit\n\nBuild a molecular pathology diagnostics app in 4 simple steps\n\nAdvocate Posts\nby\nEitan Halper-Stromberg\n,\nJanuary 25 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 13)",
    "url": "https://blog.streamlit.io/page/13/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow one finance intern launched his data science career from a coding bootcamp in Brazil\n\nLearn how Marcelo Jannuzzi of iFood got his dream job in data science\n\nCase study\nby\nMarcelo Jannuzzi and¬†\n1\n¬†more,\nJune 9 2022\nMonthly rewind > May 2022\n\nYour May look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2022\nIntroducing multipage apps! üìÑ\n\nQuickly and easily add more pages to your Streamlit apps\n\nProduct\nby\nVincent Donato\n,\nJune 2 2022\nHow Streamlit uses Streamlit: Sharing contextual apps\n\nLearn about session state and query parameters!\n\nTutorials\nby\nTyler Richards\n,\nMay 26 2022\nLeverage your user analytics on Streamlit Community Cloud\n\nSee who viewed your apps, when, and how popular they are\n\nProduct\nby\nDiana Wang and¬†\n1\n¬†more,\nMay 17 2022\nHow to share scientific analysis through a Streamlit app\n\n3 easy steps to share your study results with fellow scientists\n\nAdvocate Posts\nby\nMitchell Parker and¬†\n1\n¬†more,\nMay 12 2022\nMonthly rewind > April 2022\n\nYour April look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2022\nWissam Siblini uses Streamlit for pathology detection in chest radiographs\n\nLearn how Wissam detected thoracic pathologies in medical images\n\nCase study\nby\nWissam Siblini and¬†\n1\n¬†more,\nMay 3 2022\nThe Stable solves its data scalability problem with Streamlit\n\nHow Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days\n\nCase study\nby\nMark von Oven and¬†\n1\n¬†more,\nApril 28 2022\nHow to build a real-time live dashboard with Streamlit\n\n5 easy steps to make your own data dashboard\n\nAdvocate Posts\nby\nAbdulMajedRaja RS\n,\nApril 21 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "satschool-gif-2.gif (1918√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/06/satschool-gif-2.gif#browser",
    "html": ""
  },
  {
    "title": "Streamlit (Page 12)",
    "url": "https://blog.streamlit.io/page/12/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nTutorials\nby\nTyler Richards and¬†\n2\n¬†more,\nAugust 18 2022\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nMonthly rewind > July 2022\n\nYour July look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 9 2022\nThe magic of working in open source\n\nHow we build our open-source library and release new features\n\nTutorials\nby\nKen McGrady\n,\nAugust 4 2022\nHow to enhance Google Search Console data exports with Streamlit\n\nConnect to the GSC API in one click and go beyond the 1,000-row UI limit!\n\nTutorials\nby\nCharly Wargnier\n,\nJuly 28 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nMonthly rewind > June 2022\n\nYour June look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 6 2022\nJULO improves financial inclusion in Indonesia with Streamlit\n\nLearn how JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nCase study\nby\nMartijn Wieriks and¬†\n1\n¬†more,\nJune 30 2022\nMake your st.pyplot interactive!\n\nLearn how to make your pyplot charts interactive in a few simple steps\n\nTutorials\nby\nWilliam Huang\n,\nJune 23 2022\nObserving Earth from space with Streamlit\n\nLearn how Samuel Bancroft made the SatSchool app to teach students Earth observation\n\nAdvocate Posts\nby\nSamuel Bancroft\n,\nJune 16 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "satschool-3.png (1914√ó857)",
    "url": "https://blog.streamlit.io/content/images/2022/06/satschool-3.png#browser",
    "html": ""
  },
  {
    "title": "satschool-2.png (1663√ó857)",
    "url": "https://blog.streamlit.io/content/images/2022/06/satschool-2.png#browser",
    "html": ""
  },
  {
    "title": "satschool-1.png (1665√ó860)",
    "url": "https://blog.streamlit.io/content/images/2022/06/satschool-1.png#browser",
    "html": ""
  },
  {
    "title": "Observing Earth from space with Streamlit",
    "url": "https://blog.streamlit.io/observing-earth-from-space-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nObserving Earth from space with Streamlit\n\nLearn how Samuel Bancroft made the SatSchool app to teach students Earth observation\n\nBy Samuel Bancroft\nPosted in Advocate Posts, June 16 2022\nWhat is the SatSchool app?\nMaking a website with satellite data\nLand\nOceans\nIce\nMaking a quiz\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nWhy use Streamlit?\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHave you ever tried teaching teenagers Earth observation and environmental science in an interactive way? That was my goal at SatSchool, and that's why I made the SatSchool app!\n\nIn this post, we‚Äôll talk about:\n\nMaking a website with satellite data\nMaking a quiz\nWhy use Streamlit?\n\nCan‚Äôt wait to dive in? Here is the SatSchool app and here is the repo code.\n\nBut before we get to the fun stuff‚Ä¶\n\nWhat is the SatSchool app?\n\nSatSchool is a school outreach program that introduces Earth observation concepts and career pathways to students (11-15 years old) in the UK. Students get their hands on satellite data to learn how satellites help us study the Earth from space.\n\nAs part of this program, I developed the SatSchool app‚Äî a ‚ÄúHands on with Data‚Äù module‚Äîthat introduces technical approaches to Earth observation challenges.\n\nMaking a website with satellite data\n\nThe SatSchool app features three themes: land, oceans, and ice (check out the source code here).\n\nLand\n\nThis theme features an Amazon deforestation page.\n\nHere I used:\n\nSupport Vector Machine (SVM) classifier to make a deforestation map\nInput widgets for more forest/not-forest training points\nst.session_state to remember the training points (as students label more data based on what they see)\n\nStudents can view Landsat-8 satellite imagery centered over a region in Brazil and compare the deforested areas with protected areas. Many explore machine learning and experiment with classifiers for the first time!\n\nThey learn how their training data makes the classifier better or worse, how satellite imagery can solve problems, and how to dynamically measure deforestation over thousands of square kilometers without installation, powerful computers, or technical know-how!\n\nOceans\n\nThis theme explores the relationship between the sea surface temperature and chlorophyll.\n\nHere I used:\n\nAltair charts to present simple graphs\nst.number_input and st.form to make answering questions interactive (with st.balloons for correct answers! üéà).\n\nBy using geemap with its split-panel map, students explore two global datasets and the connection between them.\n\nIce\n\nThis theme explores coding.\n\nHere I used:\n\nstreamlit_ace for the terminal\nst.session_state for step-by-step instructions in an st.warning box for aesthetic appeal\n\nStudents get introduced to radar satellite data and can bring up satellite images by using code before they realize they‚Äôre programming!\n\nMaking a quiz\n\nI wanted to bring gamification into the learning process. Satellite imagery is cool, but how could I keep young people engaged so that they had fun learning?\n\nOur quiz tests students on the SatSchool curriculum concepts. To get the best score, they have 30 seconds to answer as many questions as they can.\n\nHere is how I did it in five steps:\n\nStep 1\n\nI did the following imports and variables:\n\nimport streamlit as st \nimport time\nimport random\nimport math\n\nTIME_LIMIT = 30 #seconds\nrow = None\n\nStep 2\n\nI randomly generated math questions using the following code:\n\ndef gen_question():\n    #randomly choose two numbers in a range, alongside an operator\n    operators = ['+','-','//','*']\n    a,b = random.randint(1,10), random.randint(1,10)\n    op = random.choice(operators)\n    \n    #construct the question text and evaluate the calculation\n    ques = f\"What is {a} {op} {b}?\"\n    ans = eval(f\"{a}{op}{b}\")\n\n    #we create some purposely incorrect answer options\n    option2 = eval(f\"{b}{op}{a}\")\n    option3 = eval(f\"{b-2}{op}{a+5}\")\n    option4 = eval(f\"{b}{op}{a}-{a}\")\n    #we want to avoid duplicate answer options, so use this inelegant solution\n    while option2 == ans:\n       option2 += 1\n    while option3 == ans or option3 == option2:\n       option3 += 1\n    while option4 == ans or option4 == option2 or option4 == option3:\n       option4 += 1\n    \n    return {'question': ques,\n            'options': {\n            ans: True,\n            option2: False,\n            option3: False,\n            option4: False\n            }\n            }\n\nStep 3\n\nI initialised values in st.session_state to keep track of timings, scores, etc.:\n\n#initialise the session state if keys don't yet exist\nif 'correct' not in st.session_state.keys():\n    st.session_state['correct'] = None\nif \"quiz_active\" not in st.session_state.keys():\n    st.session_state[\"quiz_active\"] = False\n\ni,j,_ = st.columns([1,1,5])\nif i.button(\"Start quiz\", key='start_quiz', disabled=st.session_state['quiz_active']):\n    st.session_state['quiz_active'] = True\n    st.session_state['total_score'] = 0\n    st.session_state['count'] = 0\n    st.session_state['time_start'] = time.time()\n    st.session_state['time_now'] = time.time()\n    st.session_state['score'] = 0\n    st.session_state['correct'] = None\n    st.experimental_rerun()\nif j.button(\"End quiz and reset\", key='reset', disabled=not st.session_state['quiz_active']):\n    st.session_state['total_score'] = 0\n    st.session_state['count'] = 0\n    st.session_state['correct'] = None\n    st.session_state['quiz_active'] = False\n    st.session_state['time_start'] = None\n    st.experimental_rerun()\n\nif not st.session_state['quiz_active']:\n    st.write(f'\\\\n Welcome to the quiz! You have {TIME_LIMIT} seconds to answer as many questions as you can.')\n\nStep 4\n\nI controlled the button layout by using a mix of st.columns(), st.container(), and st.empty():\n\nquestion_empty = st.empty()\n\nd,e,_ = st.columns([2,2,6])\nwith d:\n    total_score_empty = st.empty()\nwith e:\n    st.write('')\n    answer_empty = st.empty()\n\nif st.session_state['quiz_active']:\n    #check for time up upon page update\n    if time.time() - st.session_state['time_start'] > TIME_LIMIT:\n        st.info(f\"Time's up! You scored a total of **{st.session_state['total_score']:.2f}** \\\\\n                    and answered **{st.session_state['count']}** questions.\")\n    else:\n        with question_empty:\n            with st.container():\n                #get a newly generated question with answer options\n                row = gen_question()\n                \n                st.markdown(f\"Question {st.session_state['count']+1}: {row['question']}\")\n\n                options = list(row['options'].keys())\n                random.shuffle(options)\n\n                a,b,_ = st.columns([2,2,6])\n                #construct the answer buttons, and pass in whether the answer is correct or not in the args\n                a.button(f\"{options[0]}\", on_click=answer, args=(str(row['options'][options[0]]),))\n                a.button(f\"{options[1]}\", on_click=answer, args=(str(row['options'][options[1]]),))\n                b.button(f\"{options[2]}\", on_click=answer, args=(str(row['options'][options[2]]),))\n                b.button(f\"{options[3]}\", on_click=answer, args=(str(row['options'][options[3]]),))\n                \n                if st.session_state['correct']  == 'True' and st.session_state['count'] > 0:\n                    answer_empty.success(f\"Question {st.session_state['count']} correct!\")\n                elif st.session_state['correct'] == 'False' and st.session_state['count'] > 0:\n                    answer_empty.error(f\"Question {st.session_state['count']} incorrect!\")\n                total_score_empty.metric('Total score', value=f\"{st.session_state['total_score']:.2f}\", delta=f\"{st.session_state['score']:.2f}\")\n\nStep 5\n\nFinally, I defined a function that takes an answer and does something with the score:\n\ndef answer(ans):\n    st.session_state['correct'] = ans\n    if ans == 'True':\n        #motivate quickfire answers with an exponential decay\n        score = (1 * math.exp(-0.05*(time.time()-st.session_state['time_now'])))*10\n        st.session_state['score'] = max(1, score)\n    else:\n        #penalise wrong answers with a negative score\n        st.session_state['score'] = -10\n    #update the score but prevent it from becoming negative\n    st.session_state['total_score'] += st.session_state['score']\n    st.session_state['total_score'] = max(0, st.session_state['total_score'])\n    \n    st.session_state['time_now'] = time.time()\n    st.session_state['count'] += 1\n\n\nI saved my completed script and viewed my app with streamlit run.\n\nAlso, if I wanted to, I could randomly choose from a selection of pre-written questions and implement a leaderboard functionality so that students could see their scores alongside others in the classroom.\n\nWhy use Streamlit?\n\nBefore I wrap this up‚Ä¶one more thing.\n\nWhy did I choose Streamlit? I‚Äôve been using Streamlit for the past few years and knew it‚Äôd be perfect to make the SatSchool app because:\n\nI can quickly deploy my Python code without any front-end headaches.\nI can make a good-looking site and deploy my app to the Streamlit Community Cloud to share my work.\nI can easily extend and modify the site. When my SatSchool colleagues want to add new content or demos, I can quickly prototype it (especially with the help of Sebastian Flores Benner‚Äôs fantastic streamlit_book library).\nStreamlit‚Äôs amazing ecosystem of demos and third-party libraries make coding fast and fun. Third-party libraries are easy to integrate and offer lots of exciting functionality. Alongside the above-mentioned streamlit_book library, there is Qiusheng Wu‚Äôs geemap for the visualisation of satellite data, streamlit_timeline, streamlit_ace, streamlit-lottie, streamlit_juxtapose, and more.\nWrapping up\n\nI had a lot of fun making the SatSchool app and I‚Äôm excited to hear what you think! There‚Äôs still a lot to do. Expect updated app demos as we take our program into schools and show people the power of satellite imagery.\n\nI hope this post can show you how easy it is to make great apps with Streamlit. If you have any questions or want to share what you built, contact me on Twitter at @spiruel and check out SatSchool at @SatSchool_ and on our website.\n\nHappy coding! üõ∞Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 11)",
    "url": "https://blog.streamlit.io/page/11/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDiscover and share useful bits of code with the¬†ü™¢¬†streamlit-extras library\n\nHow to extend the native capabilities of Streamlit apps\n\nTutorials\nby\nArnaud Miribel\n,\nOctober 25 2022\nBuild a Streamlit Form Generator app to avoid writing code by hand\n\nLearn how to make extendable starter Forms\n\nby\nGerard Bentley\n,\nOctober 24 2022\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nMonthly rewind > September 2022\n\nYour September look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nHow to build Streamlit apps on Replit\n\nLearn Streamlit by building the Beginner Template Tour\n\nAdvocate Posts\nby\nShruti Agarwal\n,\nSeptember 29 2022\nStreamlit App Starter Kit: How to build apps faster\n\nSave 10 minutes every time you build an app\n\nTutorials\nby\nChanin Nantasenamat\n,\nSeptember 27 2022\nHow to build your own Streamlit component\n\nLearn how to make a component from scratch!\n\nTutorials\nby\nZachary Blackwood\n,\nSeptember 15 2022\nMonthly rewind > August 2022\n\nYour August look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2022\nMake dynamic filters in Streamlit and show their effects on the original dataset\n\nQuickly and easily add dynamic filters to your Streamlit app\n\nTutorials\nby\nVladimir Timofeenko\n,\nAugust 25 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 10)",
    "url": "https://blog.streamlit.io/page/10/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to quickly deploy and share your machine learning model for drug discovery\n\nShare your ML model in 3 simple steps\n\nAdvocate Posts\nby\nSebastian Ayala Ruano\n,\nDecember 15 2022\nFind the top songs from your high school years with a Streamlit app\n\nUse the Spotify API to generate 1,000+ playlists!\n\nAdvocate Posts\nby\nRobert Ritz\n,\nDecember 8 2022\nMonthly rewind > November 2022\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2022\nStreamlit-Authenticator, Part 1: Adding an authentication component to your app\n\nHow to securely authenticate users into your Streamlit app\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nDecember 6 2022\nStreamlit Quests: Getting started with Streamlit\n\nThe guided path for learning Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nNovember 18 2022\nBuilding robust Streamlit apps with type-checking\n\nHow to make type-checking part of your app-building flow\n\nAdvocate Posts\nby\nHarald Husum\n,\nNovember 10 2022\nMonthly rewind > October 2022\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2022\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nAnnouncing the Figma-to-Streamlit plugin üé®\n\nGo from prototype to code as easy as 1-2-3 with our new community resource!\n\nProduct\nby\nJuan Mart√≠n Garc√≠a\n,\nNovember 1 2022\nPrototype your app in Figma! üñåÔ∏è\n\nQuickly and easily design your app with the Streamlit Design system\n\nTutorials\nby\nJessi Shamis\n,\nOctober 27 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 9)",
    "url": "https://blog.streamlit.io/page/9/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nLLMs\nby\nArvindra Sehmi\n,\nFebruary 9 2023\nStreamlit-Authenticator, Part 2: Adding advanced features to your authentication component\n\nHow to add advanced functionality to your Streamlit app‚Äôs authentication component\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nFebruary 7 2023\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nAdvocate Posts\nby\nSven Koerner\n,\nFebruary 2 2023\nHost your Streamlit app for free\n\nLearn how to transfer your apps from paid platforms to Streamlit Community Cloud\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 24 2023\nCreate a color palette from any image\n\nLearn how to come up with the perfect colors for your data visualization\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nJanuary 19 2023\nHow to make a culture map\n\nAnalyze multidimensional data with Steamlit!\n\nTutorials\nby\nMicha≈Ç Nowotka\n,\nJanuary 12 2023\nBuild an image background remover in Streamlit\n\nSkip the fees and do it for free! üéà\n\nTutorials\nby\nTyler Simons\n,\nJanuary 10 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sidebar.gif (336√ó684)",
    "url": "https://blog.streamlit.io/content/images/2021/11/sidebar.gif",
    "html": ""
  },
  {
    "title": "Streamlit-Prophet-1.gif (1200√ó675)",
    "url": "https://blog.streamlit.io/content/images/2021/11/Streamlit-Prophet-1.gif",
    "html": ""
  },
  {
    "title": "Sogeti_compare-GIF.gif (799√ó397)",
    "url": "https://blog.streamlit.io/content/images/2022/03/Sogeti_compare-GIF.gif",
    "html": ""
  },
  {
    "title": "Sogeti_topic-GIF-1.gif (799√ó397)",
    "url": "https://blog.streamlit.io/content/images/2022/03/Sogeti_topic-GIF-1.gif",
    "html": ""
  },
  {
    "title": "image-copy.png (1188√ó879)",
    "url": "https://blog.streamlit.io/content/images/2022/03/image-copy.png#browser",
    "html": ""
  },
  {
    "title": "Sogeti_preprocess-GIF.gif (799√ó397)",
    "url": "https://blog.streamlit.io/content/images/2022/03/Sogeti_preprocess-GIF.gif",
    "html": ""
  },
  {
    "title": "audio-copy.png (1195√ó883)",
    "url": "https://blog.streamlit.io/content/images/2022/03/audio-copy.png#browser",
    "html": ""
  },
  {
    "title": "polarity-copy.png (1193√ó885)",
    "url": "https://blog.streamlit.io/content/images/2022/03/polarity-copy.png#browser",
    "html": ""
  },
  {
    "title": "pp-copy.png (1191√ó883)",
    "url": "https://blog.streamlit.io/content/images/2022/03/pp-copy.png#browser",
    "html": ""
  },
  {
    "title": "te-copy.png (1908√ó892)",
    "url": "https://blog.streamlit.io/content/images/2022/03/te-copy.png#browser",
    "html": ""
  },
  {
    "title": "Live-Data-Science-Dashboard-GIF-1.gif (1717√ó997)",
    "url": "https://blog.streamlit.io/content/images/2022/04/Live-Data-Science-Dashboard-GIF-1.gif",
    "html": ""
  },
  {
    "title": "subsections-copy.png (1757√ó817)",
    "url": "https://blog.streamlit.io/content/images/2022/03/subsections-copy.png#browser",
    "html": ""
  },
  {
    "title": "table-1.png (1600√ó217)",
    "url": "https://blog.streamlit.io/content/images/2022/04/table-1.png#browser",
    "html": ""
  },
  {
    "title": "1_tNZWLX9aUinvjYYCxfih3g.png (951√ó734)",
    "url": "https://blog.streamlit.io/content/images/2022/03/1_tNZWLX9aUinvjYYCxfih3g.png#browser",
    "html": ""
  },
  {
    "title": "Gerard Bentley - Streamlit",
    "url": "https://blog.streamlit.io/author/gerard/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Gerard Bentley\n1 post\nBuild a Streamlit Form Generator app to avoid writing code by hand\n\nLearn how to make extendable starter Forms\n\nby\nGerard Bentley\n,\nOctober 24 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "local_decomposition.png (967√ó496)",
    "url": "https://blog.streamlit.io/content/images/2021/11/local_decomposition.png",
    "html": ""
  },
  {
    "title": "external_regressors.png (657√ó343)",
    "url": "https://blog.streamlit.io/content/images/2021/11/external_regressors.png",
    "html": ""
  },
  {
    "title": "dayofweek_perf.png (923√ó226)",
    "url": "https://blog.streamlit.io/content/images/2021/11/dayofweek_perf.png",
    "html": ""
  },
  {
    "title": "overview.png (973√ó460)",
    "url": "https://blog.streamlit.io/content/images/2021/11/overview.png",
    "html": ""
  },
  {
    "title": "detect_frame2_annotations.png (961√ó538)",
    "url": "https://blog.streamlit.io/content/images/2021/10/detect_frame2_annotations.png#browser",
    "html": ""
  },
  {
    "title": "scatter.png (795√ó372)",
    "url": "https://blog.streamlit.io/content/images/2021/11/scatter.png",
    "html": ""
  },
  {
    "title": "performance.png (957√ó498)",
    "url": "https://blog.streamlit.io/content/images/2021/11/performance.png",
    "html": ""
  },
  {
    "title": "seasonality.png (718√ó317)",
    "url": "https://blog.streamlit.io/content/images/2021/11/seasonality.png",
    "html": ""
  },
  {
    "title": "ruffa-jane-reyes-dlGhQPIstkQ-unsplash.jpg (2000√ó1333)",
    "url": "https://blog.streamlit.io/content/images/2021/10/ruffa-jane-reyes-dlGhQPIstkQ-unsplash.jpg",
    "html": ""
  },
  {
    "title": "dataset-1.png (193√ó163)",
    "url": "https://blog.streamlit.io/content/images/2021/11/dataset-1.png",
    "html": ""
  },
  {
    "title": "youtube_withWagon.png (969√ó636)",
    "url": "https://blog.streamlit.io/content/images/2021/10/youtube_withWagon.png#browser",
    "html": ""
  },
  {
    "title": "-New--Hacker-News-Doppelga-nger-GIF--1920.1080-px--1.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/11/-New--Hacker-News-Doppelga-nger-GIF--1920.1080-px--1.gif",
    "html": ""
  },
  {
    "title": "04_form-1.gif (856√ó833)",
    "url": "https://blog.streamlit.io/content/images/2022/10/04_form-1.gif#browser",
    "html": ""
  },
  {
    "title": "parkingBoxes_frame2.jpeg (640√ó360)",
    "url": "https://blog.streamlit.io/content/images/2021/10/parkingBoxes_frame2.jpeg#browser",
    "html": ""
  },
  {
    "title": "parkingBoxes_frame1.jpeg (640√ó360)",
    "url": "https://blog.streamlit.io/content/images/2021/10/parkingBoxes_frame1.jpeg#browser",
    "html": ""
  },
  {
    "title": "jackson-hole.jpeg (1300√ó1014)",
    "url": "https://blog.streamlit.io/content/images/2021/10/jackson-hole.jpeg",
    "html": ""
  },
  {
    "title": "03_parsing.gif (856√ó833)",
    "url": "https://blog.streamlit.io/content/images/2022/10/03_parsing.gif#browser",
    "html": ""
  },
  {
    "title": "02_get_input.gif (857√ó832)",
    "url": "https://blog.streamlit.io/content/images/2022/10/02_get_input.gif#browser",
    "html": ""
  },
  {
    "title": "05_demo.gif (853√ó715)",
    "url": "https://blog.streamlit.io/content/images/2022/10/05_demo.gif#browser",
    "html": ""
  },
  {
    "title": "st-typing-playground.gif (1680√ó818)",
    "url": "https://blog.streamlit.io/content/images/2022/12/st-typing-playground.gif#browser",
    "html": ""
  },
  {
    "title": "01_selection.gif (856√ó833)",
    "url": "https://blog.streamlit.io/content/images/2022/10/01_selection.gif#browser",
    "html": ""
  },
  {
    "title": "Streamlit-quests-hero.png (2000√ó944)",
    "url": "https://blog.streamlit.io/content/images/2022/12/Streamlit-quests-hero.png#shadow",
    "html": ""
  },
  {
    "title": "Content-analyzer-hero.png (2000√ó943)",
    "url": "https://blog.streamlit.io/content/images/2022/12/Content-analyzer-hero.png#shadow",
    "html": ""
  },
  {
    "title": "FigmaPluginhero.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2022/12/FigmaPluginhero.png#shadow",
    "html": ""
  },
  {
    "title": "indexability-image.png (2160√ó1022)",
    "url": "https://blog.streamlit.io/content/images/2022/12/indexability-image.png#shadow",
    "html": ""
  },
  {
    "title": "6-5.png (1146√ó328)",
    "url": "https://blog.streamlit.io/content/images/2021/08/6-5.png#border",
    "html": ""
  },
  {
    "title": "5-7.png (1543√ó1226)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-7.png#browser",
    "html": ""
  },
  {
    "title": "Cosmoracle_Sneak_Peak.gif (1820√ó1004)",
    "url": "https://blog.streamlit.io/content/images/2022/02/Cosmoracle_Sneak_Peak.gif",
    "html": ""
  },
  {
    "title": "craig-mckay-p3dGOGBFbP4-unsplash.jpg (2000√ó1269)",
    "url": "https://blog.streamlit.io/content/images/2022/02/craig-mckay-p3dGOGBFbP4-unsplash.jpg",
    "html": ""
  },
  {
    "title": "map.png (2000√ó1479)",
    "url": "https://blog.streamlit.io/content/images/2022/02/map.png",
    "html": ""
  },
  {
    "title": "Cosmoracle-GIF--1920-px-.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/02/Cosmoracle-GIF--1920-px-.gif",
    "html": ""
  },
  {
    "title": "Sogeti creates an educational Streamlit app for data preprocessing",
    "url": "https://blog.streamlit.io/sogeti-creates-an-educational-streamlit-app-for-data-preprocessing/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSogeti creates an educational Streamlit app for data preprocessing\n\nLearn how to use Sogeti‚Äôs Data Quality Wrapper\n\nBy Tijana Nikolic\nPosted in Advocate Posts, March 8 2022\nWhat‚Äôs the purpose of DQW?\nHow to preprocess structured data\nHow to analyze one file\nHow to preprocess one file\nHow to compare two files\nHow to evaluate synthetic data\nHow to preprocess text data\nHow to preprocess data\nHow to do topic analysis with LDA\nHow to do sentiment analysis\nHow to preprocess audio data\nHow to analyze one file\nHow to augment one file\nHow to compare two files\nHow to preprocess image data\nHow to augment images\nBONUS: A few handy tricks for app design\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nTrying to find the best approach to improving data quality? We know this pain. That‚Äôs why we created an app to automate data preprocessing and educate aspiring and experienced data scientists about improving data quality. We chose Streamlit for its ease of development and called our app Sogeti‚Äôs Data Quality Wrapper (DQW).\n\nIn this post, you‚Äôll learn how to use DQW to preprocess:\n\nStructured data\nText data\nAudio data\nImage data\n\nTL;DR? Go to the app straight away.üöÄ ¬†Or jump into the code! üë©üèΩ‚Äçüíª\n\nBut before we dive into the fun stuff, let‚Äôs talk about...\n\nWhat‚Äôs the purpose of DQW?\n\nSogeti is the Technology and Engineering Services Division of Capgemini. We‚Äôre an IT consultancy for testing, cloud, cyber security, and AI. Sogeti NL‚Äôs data science team (where I work) is always looking for ways to automate our workflow. We made DQW as part of ITEA IVVES‚Äîa project that focuses on testing AI models in various development phases.\n\nAnother product we‚Äôre developing is in this project is the Quality AI Framework. To ensure data quality, we use DQW in its initial Data Understanding and Data Preparation phases:\n\nDQW is an accelerator for the Quality AI Framework. It demonstrates the functionality of useful packages and methods around data handling and preprocessing:\n\nApp (sub)section\tDescription\tVisualization\tPreprocessing\tPackage\nSynthetic structured\tx\tx\t\ttable-evaluator\nStructured\tx\tx\t\tsweetviz\nStructured\tx\tx\t\tpandas-profiling\nStructured, text\t\t\tx\tPyCaret\nText\tx\t\tx\tNLTK\nText\tx\t\tx\tspaCy\nText\tx\t\tx\tTextBlob\nText\tx\tx\t\tword_cloud\nText\tx\t\tx\tTextstat\nImage\tx\tx\tx\tPillow\nAudio\tx\tx\t\tlibrosa\nAudio\tx\tx\t\tdtw\nAudio\t\t\tx\taudiomentations\nAudio\tx\tx\t\tAudioAnalyzer\nReport generation\tx\t\t\tFPDF\nReport generation\tx\t\t\twkhtmltopdf\nReport generation\tx\t\t\tPDFKit\n\nYou can use DQW to preprocess structured data (synthetic data included!), text, images, and audio. Use the main selectbox to navigate through these data formats. Follow the Steps in the sidebar to navigate through the subsections:\n\nHow to preprocess structured data\n\nThe main page of the app offers structured data analysis. Structured data is data in a well-defined format used in various ML applications. The app offers one-file analysis and preprocessing, two-file comparison, and synthetic data evaluation.\n\nHow to analyze one file\n\nSelect one file analysis in Step 1, upload your file in Step 2, and select EDA in Step 3. You can also download the report in Step 4. Coding this was easy thanks to the Streamlit pandas-profiling component:\n\nimport streamlit as st\nimport pandas_profiling\nfrom streamlit_pandas_profiling import st_profile_report\n\nuploaded_data = st.sidebar.file_uploader('Upload dataset', type='csv')\ndata = pd.read_csv(uploaded_data)\n\n# create the pandas profiling report\npr = data.profile_report()\nst_profile_report(pr)\n# optional, save to file\npr.to_file('pandas_prof.html')\n\nHow to preprocess one file\n\nSelect one file analysis in Step 1, upload your file in Step 2, select Preprocess and compare in Step 3. In Steps 4 and 5 you can download the report, the files, and the pipeline pickle:\n\nIn this subsection, we use PyCaret‚Äîa workflow automation package. Streamlit widgets make it easy to select which preprocessing steps you want to run. You can display these steps as a diagram, compare the original and the preprocessed file, and download the report, the files, and the pipeline pickle. The pipeline pickle helps you use PyCaret modeling functions, especially in the case of imbalanced class mitigation with SMOTE. The code used is in the structured_data.py script (see preprocess and show_pp_file functions).\n\nHow to compare two files\n\nSelect two file comparison in Step 1, upload your files in Step 2, and download the report in Step 3. In this subsection, we use ¬†Sweetviz‚Äîan automated EDA library in Python. And to show the report on the app, we use the Streamlit HTML components function:\n\nimport streamlit as st\nimport streamlit.components.v1 as components\nimport sweetviz as sv\n\nuploaded_ref = st.sidebar.file_uploader('Upload reference dataset', type='csv')\nref= pd.read_csv(uploaded_ref)\n\nuploaded_comparison = st.sidebar.file_uploader('Upload comparison dataset', type='csv')\ncomparison = pd.read_csv(uploaded_comparison)\n\nsw = sv.compare([ref, 'Reference'], [comparison, 'Comparison'])\n\nsw.show_html(open_browser=False, layout='vertical', scale=1.0)\n\ndisplay = open('SWEETVIZ_REPORT.html', 'r', encoding='utf-8')\n\nsource_code = display.read()\n\n# you can pass width as well to configure the size of the report\ncomponents.html(source_code, height=1200, scrolling=True)\n\nHow to evaluate synthetic data\n\nSelect synthetic data comparison in Step 1, upload your files in Step 2, choose one of the two package methods in Step 3, and download the report and the files in Step 4. Here we use the table-evaluator package. It checks all statistical properties (PCA included) and offers multiple model performance comparisons with the original and synthetic dataset:\n\nThe code is in the structured_data.py (table_evaluator_comparison), te.py, viz.py, and metrics.py. The report and the files are zipped. Here is the code used in the app:\n\ndef generate_zip_structured(original, comparison):\n    \"\"\" A function to write files to disk and zip 'em \"\"\"\n    original.to_csv('pdf_files/synthetic_data/reference_file_dqw.csv', \n               index=False)\n    comparison.to_csv('pdf_files/synthetic_data/comparison_file_dqw.csv', \n               index=False)\n    # create a ZipFile object\n    dirName = \"pdf_files/synthetic_data\"\n    with ZipFile('pdf_files/synthetic_data.zip', 'w') as zipObj:\n        # Iterate over all the files in directory\n        for folderName, subfolders, filenames in os.walk(dirName):\n        \tfor filename in filenames:\n        \t\t#create complete filepath of file in directory\n        \t\tfilePath = os.path.join(folderName, filename)\n        \t\t# Add file to zip\n        \t\tzipObj.write(filePath, basename(filePath))\n\nzip = generate_zip_structured(original, comparison)\n\n# sidebar download, you can remove the sidebar api to have the normal download button\nwith open('pdf_files/synthetic_data/report_files_dqw.zip', 'rb') as fp:\n\tst.sidebar.download_button(\n\t'‚¨áÔ∏è',\n\tdata=fp,\n\tfile_name='te_compare_files_dqw.zip',\n\tmime='application/zip'\n\t)\n\nHow to preprocess text data\n\nText data is unstructured and is used in NLP models. The text data section offers the flexibility of pasting a body of text or uploading a CSV/JSON file for analysis. Currently, it supports only English, but it offers a lot of analysis methods and automated data preprocessing.\n\nHow to preprocess data\n\nSelect your data input method in Step 1 and run preprocessing:\n\nThis subsection relies on multiple text-preprocessing functions like stemming, lemmatization, de-noising, and stop-word removal. These steps prepare text data in a machine-readable way. You can download the preprocessed file. The code is in the preprocessor.py script.\n\nHow to do topic analysis with LDA\n\nSelect your data input method in Step 1, run preprocessing (optional), and select topic modeling in Step 2. Choose the topics you want to run or calculate the optimal number of topics based on the u_mass coherence score. LDA topics are visualized in an interactive plot by using pyLDAvis. The code is in the lda.py script:\n\nHow to do sentiment analysis\n\nSelect your data input method in Step 1, run preprocessing (optional) and select sentiment in Step 2. You can do sentiment analysis with Vader and textblob. It‚Äôs an easy way to get the polarity of input text data. The code is in the polarity.py script:\n\nHow to preprocess audio data\n\nAudio data is unstructured and used in audio signal processing algorithms such as music genre recognition and automatic speech recognition. The audio data section offers data augmentation, EDA, and comparison of two audio files.\n\nHow to analyze one file\n\nUpload one file in Step 1 and select EDA only in Step 2. We use plots with librosa to describe the input audio file. The plot descriptions are in the app. The code is in the audio_data.py script, function audio_eda:\n\nTo upload and display the audio file widget, use this code:\n\nimport streamlit as st \n\naudio_file = st.sidebar.file_uploader(label=\"\", \ntype=[\".wav\", \".wave\", \".flac\", \".mp3\", \".ogg\"])\n\nst.audio(audio_file , format=\"audio/wav\", start_time=0)\n\nHow to augment one file\n\nUpload one file in Step 1 and select Augmentation in Step 2. We use audiomentations‚Äîa library for the augmentation of audio files. It increases the robustness of the dataset in the case of a lack of training data. The app also runs EDA on the augmented file.\n\nThe code is in the audio_data.py script, function augment_audio. Pass the selected augmentation methods to this function with the Streamlit multiselect API, parse the user input as an expression argument, and evaluate it as a Python expression:\n\nimport streamlit as st \nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n\naudio_file = st.sidebar.file_uploader(label=\"\", \ntype=[\".wav\", \".wave\", \".flac\", \".mp3\", \".ogg\"])\n\naugmentation_methods = st.multiselect('Select augmentation method:', \n['AddGaussianNoise', \n'TimeStretch', \n'PitchShift', \n'Shift'])  \n\n# add p values to each method and eval parse all list elements\n# so they are pushed to global environment as audiomentation methods\naugmentation_list = [i + \"(p=1.0)\" for i in augmentation_methods]\naugmentation_final = [eval(i) for i in augmentation_list]\n\n# pass the list to augmentation\naugment = Compose(augmentation_list)\nHow to compare two files\n\nUpload two files in Step 1 and select Spectrum Compare or DTW in Step 2. We compare two files with Dynamic Time Warping (DTW), a method for analyzing the maximum path to the similarity of two time-series inputs. The code is in the audio_data.py script and the function is compare_files. Or we compare two files with audio analyser, a method that compares two spectrums with an applied threshold. The code is in the audio_data.py script and the function is audio_compare:\n\nHow to preprocess image data\n\nThe image data section offers data augmentation and EDA of your images.\n\nHow to augment images\n\nUpload a dataset in Step 1, select Augmentations in Step 2, and download data in Step 3. here we use Pillow. It offers image resizing, noise application, and contrast and brightness adjustment. Thanks to session state you can apply multiple augmentations in a sequence and go back to the previous state if you need to. The code is in the augment.py script:\n\nBONUS: A few handy tricks for app design\n\nYou can find a lot of helpful design functions in the helper_functions.py script. Here are a few tricks I use for all my Streamlit apps:\n\nLocal file as a background hack. This is a lifesaver. Simply use this code to pass a local file and open it as background:\n\ndef set_bg_hack(main_bg):\n\t'''\n\tA function to unpack an image from root folder and set as bg.\n\t\n\tReturns\n\t-------\n\tThe background.\n\t'''\n\t# set bg name\n\tmain_bg_ext = \"png\"\n\t  \n\tst.markdown(\n\t   f\"\"\"\n\t   <style>\n\t   .stApp {{\n\t       background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, \"rb\").read()).decode()});\n\t       background-size: cover\n\t   }}\n\t   </style>\n\t   \"\"\",\n\t   unsafe_allow_html=True\n\t)\n\n\nCustom themes. Streamlit offers an easy way of secondary app styling through their UI. Check it out here.\n\nThe sidebar design. You can change the width of your sidebar with this simple code:\n\n# set sidebar width\nst.markdown(\n\"\"\"\n<style>\n[data-testid=\"stSidebar\"][aria-expanded=\"true\"] > div:first-child {\n    width: 300px;\n}\n[data-testid=\"stSidebar\"][aria-expanded=\"false\"] > div:first-child {\n    width: 300px;\n    margin-left: -300px;\n}\n</style>\n\"\"\",\nunsafe_allow_html=True,\n)\n\n\nI prefer to move all of the high-level user-defined steps to the sidebar: upload widgets, select boxes, etc. It‚Äôs very simple. Just add .sidebar to call the relevant API:\n\nimport streamlit as st\n\n# add a logo to the sidebar\nlogo = Image.open(\"logo.png\")\nst.sidebar.image(logo, use_column_width=True)\n\n# upload widget\nfile = st.sidebar.file_uploader(\"Upload file here\")\n\n# selectbox\nadd_selectbox = st.sidebar.selectbox(\n    \"What would you like to do?\",\n    (\"EDA\", \"Preprocess\", \"Report\")\n)\n\nPassing HTML code as text. Since this is a very robust app with a lot of components that need to be explained, in some cases it‚Äôs useful to pass HTML code as text. Use this function:\n\ndef sub_text(text):\n    '''\n    A function to neatly display text in app.\n    Parameters\n    ----------\n    text : Just plain text.\n    Returns\n    -------\n    Text defined by html5 code below.\n    '''\n    \n    html_temp = f\"\"\"\n    <p style = \"color:#1F4E79; text_align:justify;\"> {text} </p>\n    </div>\n    \"\"\"\n    \n    st.markdown(html_temp, unsafe_allow_html = True)\n\n\nExpanders for more information and references. The expanders are a space-saver in robust apps with a lot of text:\n\nimport streamlit as st\n\ninfo = st.expander(\"Click here for more info on methods used\")\nwith info:\n  st.markdown(\"More information\")\nWrapping up\n\nUse the DQW app to automate your data preprocessing during AI model development. It‚Äôll streamline your workflow and ensure transparency and quality. The app is still in development and is one of the many Streamlit apps we‚Äôve created. We hope it‚Äôll help educate the data science community about data-centric model development and data quality methods.\n\nIf you have questions, please leave them in the comments below or reach out to me at tia.nikolic@sogeti.com or on LinkedIn.\n\nThank you for reading this post, and happy app-building! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to build a real-time live dashboard with Streamlit",
    "url": "https://blog.streamlit.io/how-to-build-a-real-time-live-dashboard-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build a real-time live dashboard with Streamlit\n\n5 easy steps to make your own data dashboard\n\nBy AbdulMajedRaja RS\nPosted in Advocate Posts, April 21 2022\nWhat‚Äôs a real-time live dashboard?\n1. How to import the required libraries and read input data\n2. How to do a basic dashboard setup\n3. How to design a user interface\n4. How to refresh the dashboard for real-time or live data feed\n5. How to auto-update components\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nEver thought you could build a real-time dashboard in Python without writing a single line of HTML, CSS, or Javascript?\n\nYes, you can! In this post, you‚Äôll learn:\n\nHow to import the required libraries and read input data\nHow to do a basic dashboard setup\nHow to design a user interface\nHow to refresh the dashboard for real-time or live data feed\nHow to auto-update components\n\nCan‚Äôt wait and want to jump right in? Here's the code repo and the video tutorial.\n\nWhat‚Äôs a real-time live dashboard?\n\nA real-time live dashboard is a web app used to display Key Performance Indicators (KPIs).\n\nIf you want to build a dashboard to monitor the stock market, IoT Sensor Data, AI Model Training, or anything else with streaming data, then this tutorial is for you.\n\n1. How to import the required libraries and read input data\n\nHere are the libraries that you‚Äôll need for this dashboard:\n\nStreamlit (st). As you might‚Äôve guessed, you‚Äôll be using Streamlit for building the web app/dashboard.\nTime, NumPy (np). Because you don‚Äôt have a data source, you‚Äôll need to simulate a live data feed. Use NumPy to generate data and make it live (looped) with the Time library (unless you already have a live data feed).\nPandas (pd). You‚Äôll use pandas to read the input data source. In this case, you‚Äôll use a Comma Separated Values (CSV) file.\n\nGo ahead and import all the required libraries:\n\nimport time  # to simulate a real time data, time loop\n\nimport numpy as np  # np mean, np random\nimport pandas as pd  # read csv, df manipulation\nimport plotly.express as px  # interactive charts\nimport streamlit as st  # üéà data web app development\n\n\nYou can read your input data in a CSV by using pd.read_csv(). But remember, this data source could be streaming from an API, a JSON or an XML object, or even a CSV that gets updated at regular intervals.\n\nNext, add the pd.read_csv() call within a new function get_data() so that it gets properly cached.\n\nWhat's caching? It's simple. Adding the decorator @st.experimental_memo will make the function get_data() run once. Then every time you rerun your app, the data will stay memoized! This way you can avoid downloading the dataset again and again. Read more about caching in Streamlit docs.\n\ndataset_url = \"https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv\"\n\n# read csv from a URL\n@st.experimental_memo\ndef get_data() -> pd.DataFrame:\n    return pd.read_csv(dataset_url)\n\ndf = get_data()\n\n\n2. How to do a basic dashboard setup\n\nNow let‚Äôs set up a basic dashboard. Use st.set_page_config() with parameters serving the following purpose:\n\nThe web app title page_title in the HTML tag <title> and in the browser tab\nThe favicon that uses the argument page_icon (also in the browser tab)\nThe layout = \"wide\" that renders the web app/dashboard with a wide-screen layout\nst.set_page_config(\n    page_title=\"Real-Time Data Science Dashboard\",\n    page_icon=\"‚úÖ\",\n    layout=\"wide\",\n)\n\n3. How to design a user interface\n\nA typical dashboard contains the following basic UI design components:\n\nA page title\nA top-level filter\nKPIs/summary cards\nInteractive charts\nA data table\n\nLet‚Äôs drill into them in detail.\n\nPage title\n\nThe title is rendered as the <h1> tag. To display the title, use st.title(). It‚Äôll take the string ‚ÄúReal-Time / Live Data Science Dashboard‚Äù and display it in the Page Title.\n\n# dashboard title\nst.title(\"Real-Time / Live Data Science Dashboard\")\n\n\nTop-level filter\n\nFirst, create the filter by using st.selectbox(). It‚Äôll display a dropdown with a list of options. To generate it, take the unique elements of the job column from the dataframe df. The selected item is saved in an object named job_filter:\n\n# top-level filters\njob_filter = st.selectbox(\"Select the Job\", pd.unique(df[\"job\"]))\n\nNow that your filter UI is ready, use job_filter to filter your dataframe df.\n\n# dataframe filter\ndf = df[df[\"job\"] == job_filter]\n\n\nKPIs/summary cards\n\nBefore you can design your KPIs, divide your layout into a 3 column layout by using st.columns(3). The three columns are kpi1, kpi2, and kpi3. st.metric() helps you create a KPI card. Use it to fill one KPI in each of those columns.\n\nst.metric()‚Äôs label helps you display the KPI title. The value **is the argument that helps you show the actual metric (value) and add-ons like delta to compare the KPI value with the KPI goal.\n\n# create three columns\nkpi1, kpi2, kpi3 = st.columns(3)\n\n# fill in those three columns with respective metrics or KPIs\nkpi1.metric(\n    label=\"Age ‚è≥\",\n    value=round(avg_age),\n    delta=round(avg_age) - 10,\n)\n\nkpi2.metric(\n    label=\"Married Count üíç\",\n    value=int(count_married),\n    delta=-10 + count_married,\n)\n\nkpi3.metric(\n    label=\"A/C Balance ÔºÑ\",\n    value=f\"$ {round(balance,2)} \",\n    delta=-round(balance / count_married) * 100,\n)\n\nInteractive charts\n\nSplit your layout into 2 columns and fill them with charts. Unlike the metric above, use the with clause to fill the interactive charts in the respective columns:\n\nDensity_heatmap in fig_col1\nHistogram in fig_col2\n# create two columns for charts\nfig_col1, fig_col2 = st.columns(2)\n\nwith fig_col1:\n    st.markdown(\"### First Chart\")\n    fig = px.density_heatmap(\n        data_frame=df, y=\"age_new\", x=\"marital\"\n    )\n    st.write(fig)\n   \nwith fig_col2:\n    st.markdown(\"### Second Chart\")\n    fig2 = px.histogram(data_frame=df, x=\"age_new\")\n    st.write(fig2)\n\nData table\n\nUse st.dataframe() to display the data frame. Remember, your data frame gets filtered based on the filter option selected at the top:\n\nst.markdown(\"### Detailed Data View\")\nst.dataframe(df)\n\n4. How to refresh the dashboard for real-time or live data feed\n\nSince you don‚Äôt have a real-time or live data feed yet, you‚Äôre going to simulate your existing data frame (unless you already have a live data feed or real-time data flowing in).\n\nTo simulate it, use a for loop from 0 to 200 seconds (as an option, on every iteration you‚Äôll have a second sleep/pause):\n\nfor seconds in range(200):\n\n    df[\"age_new\"] = df[\"age\"] * np.random.choice(range(1, 5))\n    df[\"balance_new\"] = df[\"balance\"] * np.random.choice(range(1, 5))\n    time.sleep(1)\n\nInside the loop, use NumPy's random.choice to generate a random number between 1 to 5. Use it as a multiplier to randomize the values of age and balance columns that you‚Äôve used for your metrics and charts.\n\n5. How to auto-update components\n\nNow you know how to do a Streamlit web app!\n\nTo display the live data feed with auto-updating KPIs/Metrics/Charts, put all these components inside a single-element container using st.empty(). Call it placeholder:\n\n# creating a single-element container.\nplaceholder = st.empty()\n\n\nPut your components inside the placeholder by using a with clause. This way you‚Äôll replace them in every iteration of the data update. The code below contains the placeholder.container() along with the UI components you created above:\n\nwith placeholder.container():\n\n    # create three columns\n    kpi1, kpi2, kpi3 = st.columns(3)\n\n    # fill in those three columns with respective metrics or KPIs\n    kpi1.metric(\n        label=\"Age ‚è≥\",\n        value=round(avg_age),\n        delta=round(avg_age) - 10,\n    )\n    \n    kpi2.metric(\n        label=\"Married Count üíç\",\n        value=int(count_married),\n        delta=-10 + count_married,\n    )\n    \n    kpi3.metric(\n        label=\"A/C Balance ÔºÑ\",\n        value=f\"$ {round(balance,2)} \",\n        delta=-round(balance / count_married) * 100,\n    )\n\n    # create two columns for charts\n    fig_col1, fig_col2 = st.columns(2)\n    \n    with fig_col1:\n        st.markdown(\"### First Chart\")\n        fig = px.density_heatmap(\n            data_frame=df, y=\"age_new\", x=\"marital\"\n        )\n        st.write(fig)\n        \n    with fig_col2:\n        st.markdown(\"### Second Chart\")\n        fig2 = px.histogram(data_frame=df, x=\"age_new\")\n        st.write(fig2)\n\n    st.markdown(\"### Detailed Data View\")\n    st.dataframe(df)\n    time.sleep(1)\n\nAnd...here is the full code!\n\nimport time  # to simulate a real time data, time loop\n\nimport numpy as np  # np mean, np random\nimport pandas as pd  # read csv, df manipulation\nimport plotly.express as px  # interactive charts\nimport streamlit as st  # üéà data web app development\n\nst.set_page_config(\n    page_title=\"Real-Time Data Science Dashboard\",\n    page_icon=\"‚úÖ\",\n    layout=\"wide\",\n)\n\n# read csv from a github repo\ndataset_url = \"https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv\"\n\n# read csv from a URL\n@st.experimental_memo\ndef get_data() -> pd.DataFrame:\n    return pd.read_csv(dataset_url)\n\ndf = get_data()\n\n# dashboard title\nst.title(\"Real-Time / Live Data Science Dashboard\")\n\n# top-level filters\njob_filter = st.selectbox(\"Select the Job\", pd.unique(df[\"job\"]))\n\n# creating a single-element container\nplaceholder = st.empty()\n\n# dataframe filter\ndf = df[df[\"job\"] == job_filter]\n\n# near real-time / live feed simulation\nfor seconds in range(200):\n\n    df[\"age_new\"] = df[\"age\"] * np.random.choice(range(1, 5))\n    df[\"balance_new\"] = df[\"balance\"] * np.random.choice(range(1, 5))\n\n    # creating KPIs\n    avg_age = np.mean(df[\"age_new\"])\n\n    count_married = int(\n        df[(df[\"marital\"] == \"married\")][\"marital\"].count()\n        + np.random.choice(range(1, 30))\n    )\n\n    balance = np.mean(df[\"balance_new\"])\n\n    with placeholder.container():\n\n        # create three columns\n        kpi1, kpi2, kpi3 = st.columns(3)\n\n        # fill in those three columns with respective metrics or KPIs\n        kpi1.metric(\n            label=\"Age ‚è≥\",\n            value=round(avg_age),\n            delta=round(avg_age) - 10,\n        )\n        \n        kpi2.metric(\n            label=\"Married Count üíç\",\n            value=int(count_married),\n            delta=-10 + count_married,\n        )\n        \n        kpi3.metric(\n            label=\"A/C Balance ÔºÑ\",\n            value=f\"$ {round(balance,2)} \",\n            delta=-round(balance / count_married) * 100,\n        )\n\n        # create two columns for charts\n        fig_col1, fig_col2 = st.columns(2)\n        with fig_col1:\n            st.markdown(\"### First Chart\")\n            fig = px.density_heatmap(\n                data_frame=df, y=\"age_new\", x=\"marital\"\n            )\n            st.write(fig)\n            \n        with fig_col2:\n            st.markdown(\"### Second Chart\")\n            fig2 = px.histogram(data_frame=df, x=\"age_new\")\n            st.write(fig2)\n\n        st.markdown(\"### Detailed Data View\")\n        st.dataframe(df)\n        time.sleep(1)\n\n\nTo run this dashboard on your local computer:\n\nSave the code as a single monolithic app.py.\nOpen your Terminal or Command Prompt in the same path where the app.py is stored.\nExecute streamlit run app.py for the dashboard to start running on your localhost and the link would be displayed in your Terminal and also opened as a new Tab in your default browser.\nWrapping up\n\nCongratulations! You have learned how to build your own real-time live dashboard with Streamlit. I hope you had fun along the way.\n\nIf you have any questions, please leave them below in the comments or reach out to me at 1littlecoder@gmail.com or on Linkedin.\n\nThank you for reading, and Happy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Matthijs van der Wild - Streamlit",
    "url": "https://blog.streamlit.io/author/matthijs/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Matthijs van der Wild\n1 post\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nAdvocate Posts\nby\nNikolina Sarcevic and¬†\n2\n¬†more,\nFebruary 17 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Marco Bonici - Streamlit",
    "url": "https://blog.streamlit.io/author/marco/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Marco Bonici\n1 post\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nAdvocate Posts\nby\nNikolina Sarcevic and¬†\n2\n¬†more,\nFebruary 17 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Nikolina Sarcevic - Streamlit",
    "url": "https://blog.streamlit.io/author/nikolina/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Nikolina Sarcevic\n1 post\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nAdvocate Posts\nby\nNikolina Sarcevic and¬†\n2\n¬†more,\nFebruary 17 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "markdown-2.gif (1200√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/01/markdown-2.gif#border",
    "html": ""
  },
  {
    "title": "install-1.gif (1200√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/01/install-1.gif#border",
    "html": ""
  },
  {
    "title": "content_analyzer_app.gif (3840√ó2160)",
    "url": "https://blog.streamlit.io/content/images/2022/11/content_analyzer_app.gif#browser",
    "html": ""
  },
  {
    "title": "Forecasting with Streamlit Prophet",
    "url": "https://blog.streamlit.io/forecasting-with-streamlit-prophet/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nForecasting with Streamlit Prophet\n\nHow Artefact built a Streamlit app to train time-series forecasting models\n\nBy Maxime Lutel\nPosted in Advocate Posts, November 10 2021\nWhat is Streamlit Prophet?\nStep 1. Data exploration\nStep 2. Performance evaluation\nStep 3. Error diagnosis\nStep 4. Model optimization\nStep 5. Forecast interpretability\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nNeed a baseline for your latest time-series forecasting project? Want to explain the decision-making process of a predictive model to a business audience? Struggling to understand if car prices are seasonal before buying a new one? I might have something for you...\n\nA forecasting app Streamlit Prophet! üîÆ\n\nIn this post, I'll teach you how to build it in 5 simple steps:\n\nStep 1. Data exploration\nStep 2. Performance evaluation\nStep 3. Error diagnosis\nStep 4. Model optimization\nStep 5. Forecast interpretability\n\nWant to jump right in? Test the app online or install the python package and run it locally.\n\nBut before we get to the fun stuff, let's talk about...\n\nWhat is Streamlit Prophet?\n\nStreamlit Prophet is a Streamlit app that helps data scientists create forecasting models without coding. Simply upload a dataset with historical values of the signal. The app will train a predictive model in a few clicks. And you get several visualizations to evaluate its performance and for further insights.\n\nThe underlying model is built with Prophet. Prophet is an open-source library developed by Facebook to forecast time-series data. The signal is broken down into several components such as trend, seasonalities, and holidays effects. The estimator learns how to model each of these blocks, then adds their contributions to produce an interpretable forecast. It performs better when the series has strong seasonal patterns and several cycles of historical data.\n\nLet‚Äôs say, you want to predict the future sales for consumer goods in a store. Your historical data ranges from 2011 to 2015. A baseline model with default parameters fits on the data as you upload it. Your dataset will look like this:\n\nFirst rows of the dataset with historical sales\n\nNow, let's dive into building it!\n\nStep 1. Data exploration\n\nThe first step in any forecasting project is to make sure that you know the dataset perfectly. Prophet provides a nice decomposition of the signal. The app has several charts to show you these insights at a glance.\n\nThe following graph gives you a global representation of the uploaded time series:\n\nGlobal visualization showing the trend, the forecasts, and the true values\nActual sales. The black points are actual historical sales. Often they're between 75 and 225 units per day.\nOutliers. At Christmas time, when the stores are closed, you can spot some outliers with low or no sales.\nTrend. The red line shows the trend. It gives you a synthetic vision of the signal and helps visualize the global evolutions.\nProphet predictions. The blue line represents the forecasts made by a Prophet model that's automatically trained on your dataset. The model expects the sales to increase in 2016, following the growing trend that started in 2015.\n\nThese forecasts seem to be seasonal, but it's hard to distinguish between the different periodic components on this first plot. Let‚Äôs check another visualization to understand how these seasonal patterns affect the model output:\n\nProphet model's seasonal components\n\nTwo periodicities have been detected. They give you interesting insights into consumers‚Äô habits:\n\nWeekly cycle. The weekly cycle shows that most people shop on weekends, during which forecasts are increased by nearly 40 units per day.\nYearly seasonality. The graph also suggests that the sold products have a yearly seasonality, with more sales during the summer as compared to the rest of the year. The estimator will combine these periodic components and the global trend to produce the forecasts for future days.\nStep 2. Performance evaluation\n\nThese plots synthesize the way data is modeled by Prophet. But is this representation reliable?\n\nTo answer this question, a section of the app is dedicated to the evaluation of the model quality. It shows a baseline forecasting performance. The time series is split into several parts. First, the model is fitted on a training set. Then it gets tested on a validation set. You also have advanced options like cross-validation.\n\nThere are different metrics to assess the model quality. Absolute metrics such as the root mean squared error (RMSE) calculate the magnitude of errors in the number of sales. At the same time, the relative metrics like the mean absolute percentage error (MAPE) are more interpretable. It's up to you to select the metric that's most relevant for your use case.\n\nBecause performance is unlikely to be uniform over all data points, getting a global indicator is not enough. Compute the metrics at a more detailed granularity to get a clear understanding of the model quality.\n\nLet‚Äôs start with an in-depth analysis at the daily level, which is the lowest possible granularity in our case, as the model makes one prediction per day:\n\nApp's performance evaluation section\n\nObserve an important variability. There are days when the error is bigger than 20% while some other forecasts are almost perfectly accurate. With this information in mind, you're probably wondering if there are patterns in the way the model makes mistakes. Are there days when it performs poorly? Fortunately, the app provides handy charts to satisfy our curiosity.\n\nStep 3. Error diagnosis\n\nThe error diagnosis section is the most useful one. Here you can highlight the areas where forecasts could be improved and identify the challenges while building a reliable forecasting model. Use interactive visualizations to focus on specific areas.\n\nFor example, the scatter plot below represents each forecast made on the validation set by a single point (hover over the ones away from the red line to see which data points forecasts are far from the truth):\n\nScatter plot showing the true values versus the forecasted ones\n\nIn this example, hovering over the top right area shows that the points furthest from the red line are Saturdays and Sundays. This suggests that the model performs better during the week.\n\nLet‚Äôs aggregate daily performance metrics to validate this intuition:\n\nModel performance aggregated by days\n\nOn average, errors are bigger on the weekends. Keep this in mind when optimizing the model. Performance might also evolve over time. Select other levels of aggregation in the app to check it out. For example, compute metrics at a weekly or monthly granularity, or over the period of time when you suspect it to perform differently.\n\nStep 4. Model optimization\n\nNow you know the model‚Äôs main weaknesses. How do you improve it? In the app‚Äôs sidebar edit the default configuration and enter your specifications. All performance metrics and visualizations are updated each time you change settings, so you can get quick feedback.\n\nUser specifications entered in the sidebar\n\nFirst, apply some customized pre-processing to your dataset. You have several alternatives to get around the challenges identified earlier. For example, a cleaning section lets you get rid of the outliers observed at Christmas. The outliers might confuse the model. You can also filter out particular days and train distinct models for the week and the weekends, as they seemed to be associated with different purchasing behaviors. Some other filtering and resampling options are available as well, in case they're relevant to your problem.\n\nTo help the model better fit the data, you can tune Prophet hyper-parameters. They influence how the estimator learns to represent the trend and the seasonalities from historical sales, and the relative weight of these components in the global forecast. Don‚Äôt worry if you‚Äôre not familiar with Prophet models. Tooltips explain the intuition behind each parameter and guide you through the tuning process.\n\nIn the modeling section, you can also feed the model with external information such as holidays or variables related to the signal to be forecasted (like the products‚Äô selling price). These regressors are likely to improve performance as they provide the model with additional knowledge about a phenomenon that impacts sales.\n\nStep 5. Forecast interpretability\n\nHaving an accurate forecasting model is nice, but being able to explain the main factors that contribute to its predictions is even better.\n\nThe last section of the app aims at helping you understand how your model makes decisions. You can look at a single component and see how its contribution to the overall forecasts evolves over time. Or you can take a single forecast and decompose it into the sum of contributions from several components.\n\nLet‚Äôs start with the first option. The different components that influence forecasts are the trend, the seasonalities, and the external regressors. You already observed the impact of the weekly and the yearly seasonalities, so let‚Äôs take a look at the external regressors like the holidays and the products‚Äô selling price:\n\nGlobal impact of the external regressors in the model's forecasts\n\nThe impact of the public holidays is important. For example, every year at the beginning of September Labor Day increases forecasts by 50 sales. On Christmas, the dips show that the model has taken into account the fact that stores are closed on that day. And the price has increased year after year. So its impact on sales has shifted from positive to negative.\n\nLet's take a look at how the model produces one specific forecast, especially when a particular event influences the prediction. This waterfall chart shows this decomposition for the forecast made on October 31st, 2012:\n\nContribution of the different forecast components on October 31st, 2012\n\nIn this example, the model ended up forecasting 96 sales. This is the sum of the contributions from five different components:\n\nGlobal trend (+134): the most influential factor.\nHalloween effect (-12): fewer sales on Halloween than on regular days.\nSell price (+2): the price must've been lower than on average that day.\nWeekly seasonality (-23): this was a Wednesday, so not the weekend.\nYearly seasonality (-5): October is a low season for this product.\n\nThat kind of decomposition is not only useful for sharing insights with collaborators, it can also help analysts understand why their model doesn‚Äôt perform as expected. The app‚Äôs sidebar has parameters to increase or decrease the different components‚Äô relative weights.\n\nWrapping up\n\nI hope you add Streamlit Prophet to your forecasting toolbox. Look at the source code to learn more about how this app was built. And visit the Artefact tech blog for more information about our data science projects.\n\nHave questions or improvement ideas? Please post them in the comments below or reach out to me through Medium or LinkedIn.\n\nThanks a lot for reading! ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Screen-Recording-2021-10-12-at-12.33.21-PM--1-.gif (4096√ó2304)",
    "url": "https://blog.streamlit.io/content/images/2021/11/Screen-Recording-2021-10-12-at-12.33.21-PM--1-.gif#shadow",
    "html": ""
  },
  {
    "title": "Detecting parking spots with Streamlit",
    "url": "https://blog.streamlit.io/detecting-parking-spots-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDetecting parking spots with Streamlit\n\nHow to build a Streamlit parking spot app in 8 simple steps\n\nBy Jeffrey Jex\nPosted in Advocate Posts, October 26 2021\n1. How to stream from a YouTube camera to Streamlit with OpenCV\n2. How to detect parking spots with Mask R-CNN\n3. How to connect it all in Streamlit\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nStruggling to find a parking spot in a busy place? Then keep reading!\n\nIn this post, I'll teach you how to build a Streamlit app that detects parking spots from a live YouTube stream. You'll learn:\n\nHow to stream from a YouTube camera to Streamlit with OpenCV\nHow to detect parking spots with Mask R-CNN\nHow to connect it all in Streamlit\n\nCan't wait to try it for yourself? Here's a sample app and repo code.\n\nLet's get started!\n\n1. How to stream from a YouTube camera to Streamlit with OpenCV\n\nLet's say, you're on a road trip. You've arrived in Jackson Hole, Wyoming. You want to take a selfie with the elk antler arches. But there are no parking spots. What do you do?\n\nIconic antler arch in Jackson Hole, Wyoming. Photo by Carol M Highsmith on Rawpixel.\n\nThis is 2021. There's data for that. üòâ\n\nJackson Hole's town square has a public webcam. Let's analyze its camera's YouTube video stream. If Google can recognize faces in photographs, why not recognize cars in parking spots? üöó\n\nStep 1: Import libraries for pulling the video stream into Python\n\n# Video getting and saving\nimport cv2  # open cvs, image processing\nimport urllib\nimport m3u8\nimport time\nimport pafy  # needs youtube_dl\n\n\nFor our next app, can we help horse-drawn wagons find parking? ‚òùÔ∏è\n\nStep 2: Make a function to grab and display a video clip\n\ndef watch_video(video_url, image_placeholder, n_segments=1, \n\t\t\t\t\t\t\t n_frames_per_segment=60):\n    \"\"\"Gets a video clip, \n        video_url: YouTube video URL\n\t\t\t\timage_placeholder: streamlit image placeholder where clip will display\n\t\t\t\tn_segments: how many segments of the YouTube stream to pull (max 7)\n\t\t\t\tn_frames_per_segment: how many frames each segment should be, max ~110\n\t\t\t\t\"\"\"\n\t\t####################################\n\t\t# SETUP BEFORE PLAYING VIDEO TO GET VIDEO URL\n\t\t####################################\n\n\t\t# Speed up playing by only showing every nth frame\n    skip_n_frames=10\n    video_warning = st.warning(\"showing a clip from youTube...\")\n\n    # Use pafy to get the 360p url\n    url=video_url\n    video = pafy.new(url)\n\n    # best = video.getbest(preftype=\"mp4\")  #  Get best resolution stream available\n\t\t# In this specific case, the 3rd entry is the 360p stream,\n\t\t#   But that's not ALWAYS true.\n    medVid = video.streams[2]\n\n    #  load a list of current segments for live stream\n    playlist = m3u8.load(medVid.url)\n\n    # will hold all frames at the end\n    # can be memory intestive, so be careful here\n    frame_array = []\n\n    # Speed processing by skipping n frames, so we need to keep track\n    frame_num = 0\n\n\t\t###################################\n    # Loop over each frame of video, and each segment\n\t\t###################################\n    #  Loop through all segments\n    for i in playlist.segments[0:n_segments]:\n\n        capture = cv2.VideoCapture(i.uri)\n\n        # go through every frame in each segment\n        for i in range(n_frames_per_segment):\n\t\t\t\t\t\n\t\t\t\t\t\t# open CV function to pull a frame out of video\n            success, frame = capture.read()\n            if not success:\n                break\n\n            # Skip every nth frame to speed processing up\n            if (frame_num % skip_n_frames != 0):\n                frame_num += 1\n                pass\n            else:\n                frame_num += 1\n\n\t\t\t\t\t\t\t\t# Show the image in streamlit, then pause\n                image_placeholder.image(frame, channels=\"BGR\")\n                time.sleep(0.5)\n              \n\n    # Clean up everything when finished\n    capture.release()  # free the video\n\n\t\t# Removes the \"howing a clip from youTube\" message\"\n    video_warning.empty()  \n\n    st.write(\"Done with clip, frame length\", frame_num)\n\n    return None\n\n\nWow, lots going on here! You've got some initial setup, then a loop where the video is being displayed one frame at a time, followed by closing the video and removing the messages the user saw while the video played.\n\nNOTE: YouTube videos are copyrighted! YouTube terms were updated earlier in 2021 to restrict people from doing machine learning 'face harvesting' from videos. I received permission from the video poster I'm using in the example.\n\nStep 3: Wrap this in another function to build a sidebar menu\n\ndef camera_view():\n    # streamlit placeholder for image/video\n    image_placeholder = st.empty()\n\n    # url for video\n    # Jackson hole town square, live stream\n    video_url = \"<https://youtu.be/DoUOrTJbIu4>\"\n\n    # Description for the sidebar options\n    st.sidebar.write(\"Set options for processing video, then process a clip.\")\n    \n\t\t# Make a slider bar from 60-360 with a step size of 60\n\t\t#  st.sidebar is going to make this appear on the sidebar\n    n_frames=60  # Frames per 'segment\"\n    n_segments = st.sidebar.slider(\"How many frames should this video be:\",\n        n_frames, n_frames*6, n_frames, step=n_frames, key=\"spots\", help=\"It comes in 7 segments, 100 frames each\")\n\n\t\t# We actually need to know the number of segments, \n\t\t#   so convert the total number of frames to the number of segments we want\n    n_segments = int(n_segments/n_frames)\n\n\t\t# Add a \"go\" button to show the clip in streamlit\n    if st.sidebar.button(\"watch video clip\"):\n        watch_video(video_url=video_url,\n                            image_placeholder=image_placeholder,\n                            n_segments=n_segments,\n                            n_frames=n_frames)\n\n\nThis wrapper lets the user control when to start the video and how long it should be.\n\nNOTE: For Streamlit Cloud to work, you'll need a requirements.txt and a packages.txt files. This tells the server what Python libraries and packages you'll need. To make requirements.txt use pip list --format=freeze > requirements.txt. The packages file contains anything you needed to install (i.e., not from PyPi or conda). This is for openCV. Use virtual environments to manage Python packages. Here's my sample requirements file and packages file.\n\n2. How to detect parking spots with Mask R-CNN\n\nBefore you find available spots, you'll need to map out where the spots are. Following Adam Geitgey's method, I've used a video clip during 'rush hour' when all spots were full and ran Mask R-CNN to detect vehicles. The idea was to identify cars that didn't move and assume they were parked.\n\nIn technical terms, you'll detect all the cars in the first frame and make a bounding box around them. A few frames later you'll detect where all the cars are again. If any bounding boxes from the first frame are still full in the later frame, it's a parking spot. Any boxes that weren't full are considered noise and are thrown away (i.e., a car driving on the road).\n\nStep 4: Do imports for this section, which includes Mask R-CNN\n\n# IMPORTS\n# general\nimport numpy as np\nimport pandas as pd\n\n# File handling\nfrom pathlib import Path\nimport os\nimport pickle\nfrom io import BytesIO\nimport requests\nimport sys\n\n# Import mrcnn libraries\n# I am using Matterport mask R-CNN modified for tensor flow 2, see source here:\n# <https://raw.githubusercontent.com/akTwelve/Mask_RCNN/master>'\n# You'll need a copy of mrcnn in your working directory, it can't be installed\n#  from pypi yet.\nimport mrcnn.config\nimport mrcnn.utils\nfrom mrcnn.model import MaskRCNN\n\n\n\nMatterport's Mask R-CNN model hasn't been updated for TensorFlow 2. Luckily, someone else has done it. Use these installation instructions.\n\nHow is it going so far?\n\nLook! You're standing on the shoulders of giants! Your tool includes code from Google (TensorFlow 2) and Facebook (Mask R-CNN). Next, you'll add pre-trained weights that enable the model to classify 80 different object categories. Two of those categories are cars and trucks. This is what will tell you if parking spots are vacant or not.\n\nTime to make your model and load the weights.\n\nStep 5: Create your Mask R-CNN model\n\ndef maskRCNN_model():\n    \"\"\"Makes a Mask R-CNN model, ideally save to cache for speed\"\"\"\n    weights = get_weights()\n    # Create a Mask-RCNN model in inference mode\n    model = MaskRCNN(mode=\"inference\", model_dir=\"model\", config=MaskRCNNConfig())\n    \n    # Load pre-trained model\n    model.load_weights(weights, by_name=True)\n    model.keras_model._make_predict_function()\n    \n    return model\n\n#Let's invoke it like so, with a nice message for streamlit users.\n# Give message while loading weights\nweight_warning = st.warning(\"Loading model, might take a few minutes, hold on...\")\n\n#Create model with saved weights\nmodel = maskRCNN_model()\n\nweight_warning.empty()  # Make the warning go away, done loading\n\n\nPass this model around to your other functions to do the object detection work.\n\nStep 6: Combine your YouTube streaming code with the model\n\ndef detectSpots(video_file, model,\n                 show_video=True, initial_check_frame_cutoff=10):\n    '''detectSpots(video_file, initial_check_frame_cutoff=10)\n    Returns: np 2D array of all bounding boxes that are still occupied\n    after initial_check_frame_cutoff frames.  These can be considered \"parking spaces\".\n    An update might identify any spaces that get occupied at some point and stay occupied \n    for a set length of time, in case some areas start off vacant.\n\t\tvideo_file: saved or online video file to detect on\n\t\tmodel: mask R-CNN model\n\t\tshow_video: Boolean, display to streamlit or not\n\t\tinitial_check_frame_cutoff: The frame number to compare against the 1st. Frames\n\t\t\t\t\tAfter this number will be ignored. i.e. =55, then frame 55 will have detection\n\t\t\t\t\trun to compare which boxes from the first frame still have a car in them.\n\t\t'''\n    # Load the video file we want to run detection on\n    video_capture = cv2.VideoCapture(video_file)\n\n    # Store the annotated frames for output to video/counting how many frames we've seen\n    frame_array = []\n\n    # Will contain bounding boxes of parked cars to identify 'parkable spots'\n    parked_car_boxes = []\n\t\t# same as above, but for final pass\n    parked_car_boxes_updated = []\n\n    # Make image appear in streamlit\n    image_placeholder_processing = st.empty()\n\n    # Loop over each frame of video\n    while video_capture.isOpened():\n        success, frame = video_capture.read()\n        if not success:\n            st.write(f\"Processed {len(frame_array)} frames of video, exiting.\")\n            return parked_car_boxes\n\n        # Convert the image from BGR color (which OpenCV uses) to RGB color\n        rgb_image = frame[:, :, ::-1]\n\n        # ignore the inbetween frames 0 to x, don't run the model on them and save processing time\n        if 0 < len(frame_array) < initial_check_frame_cutoff:\n            print(f\"ignore this frame for processing, #{len(frame_array)}\")\n        else:\n            print(f\"Processing frame: #{len(frame_array)}\")\n            # Run the image through the Mask R-CNN model to get results\n\n            results = model.detect([rgb_image], verbose=0)\n\n            # Mask R-CNN assumes we are running detection on multiple images.\n            # We only passed in one image to detect, so only grab the first result.\n            r = results[0]\n\n            # The r variable will now have the results of detection:\n            # - r['rois'] are the bounding box of each detected object\n            # - r['class_ids'] are the class id (type) of each detected object\n            # - r['scores'] are the confidence scores for each detection\n            # - r['masks'] are the object masks for each detected object (which gives you the object outline)\n\n            if len(frame_array) == 0:\n                # This is the first frame of video,\n                # Save the location of each car as a parking space box and go to the next frame of video.\n                # We check if any of those cars moved in the next 5 frames and assume those that don't are parked\n                parked_car_boxes = get_car_boxes(r['rois'], r['class_ids'])\n                parked_car_boxes_init = parked_car_boxes\n                print('Parking spots 1st frame:', len(parked_car_boxes))\n\n            # If we are past the xth initial frame, already know where parked cars are, then check if any cars moved:\n            else:\n                # We already know where the parking spaces are. Check if any are currently unoccupied.\n\n                # Get where cars are currently located in the frame\n                car_boxes = get_car_boxes(r['rois'], r['class_ids'])\n\n                # See how much those cars overlap with the known parking spaces\n                overlaps = mrcnn.utils.compute_overlaps(parked_car_boxes, car_boxes)\n\n                # Loop through each known parking space box\n                for row, areas in enumerate(zip(parked_car_boxes, overlaps)):\n                    parking_area, overlap_areas = areas\n                    # For this parking space, find the max amount it was covered by any\n                    # car that was detected in our image (doesn't really matter which car)\n                    max_IoU_overlap = np.max(overlap_areas)\n\n                    # Get the top-left and bottom-right coordinates of the parking area\n                    y1, x1, y2, x2 = parking_area\n\n                    # Check if the parking space is occupied by seeing if any car overlaps\n                    # it by more than x amount using IoU\n                    if max_IoU_overlap < 0.20:\n                        # In the first few frames, remove this 'spot' and consider it as a moving car instead\n                        # Transient event, draw green box\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (0, 255, 0), 3)\n                    else:\n                        # Consider this a parking spot, car is still in it!\n                        # Dangerous to mutate array while using it! So using a copy\n                        parked_car_boxes_updated.append(list(parking_area))\n\n                        # Parking space is still occupied -> draw a red box around it\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (0, 0, 255), 1)\n\n                    # Write the top and bottom corner locations in the box for ref\n                    font = cv2.FONT_HERSHEY_DUPLEX\n                    cv2.putText(frame, str(parking_area),\n                                (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255))\n\n                parked_car_boxes = np.array(\n                    parked_car_boxes_updated)  # only happens once\n\n        # print number of frames\n        font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n        cv2.putText(frame, f\"Frame: {len(frame_array)}\",\n                    (10, 340), font, 0.5, (0, 255, 0), 2, cv2.FILLED)\n\n        # Show the frame of video on the screen\n        if show_video:\n            image_placeholder_processing.image(frame, channels=\"BGR\")\n            time.sleep(0.01)\n\n        # Append frame to outputvideo\n        frame_array.append(frame)\n\n        # stop when cutoff reached\n        if len(frame_array) > initial_check_frame_cutoff:\n            print(f\"Finished, processed frames: 0 - {len(frame_array)}\")\n            break\n\n    # Clean up everything when finished\n    video_capture.release()\n    #write_frames_to_file(frame_array=frame_array, file_name=video_save_file)\n\n    # Show final image in matplotlib for ref\n    return parked_car_boxes\n\n\nThe machine learning toolbox has something called \"intersection over union\" or IoU. IoU calculates the number of pixels where two bounding boxes overlap. IoU is used heavily inside of CNNs to find the 'best' bounding box around detected objects, removing all the initial overlapping guesses.\n\nSet a cutoff to determine how 'occupied' a box needs to be, to be considered full. Think of this as a 'fraction of overlap' where 1 = perfect overlap and 0 = no overlap.\n\nHere is what it looks like.\n\nFirst frame:\n\nIdentify ALL cars in this frame and save their bounding box.\n\nNth frame:\n\nCompare IoU on the boxes from the first frame with the cars in this frame. If IoU is high (i.e., there's still a car in the spot), count it as a parking spot. Red = 'parking spot' (had a car in both frames). Green = 'not a valid spot' (didn't have cars in the nth frame).\n\nBut look...\n\nThis method isn't perfect! Notice the red boxes that aren't parking spots? One was never a car. And in another one, two separate cars happened to be in the same place. This caused the algorithm to consider it a parking spot.\n\nStep 7: Combine your YouTube clip with the detected parking spot to see which spots are full or empty\n\nDetect all cars in the given frame.\nFor each car, see if it's overlapping a parking spot.\nColor the parking spot boxes red if filled and green if vacant.\nRepeat for all frames.\n\nThis code is similar to the detect spots function. It's doing the same things but on all processed frames instead of only two. Need help? See the function countSpots() in my repo code linked at the top.\n\n3. How to connect it all in Streamlit\n\nWell done! You've built the framework for a solid app.\n\nStep 8: Tie all these functions together with a Streamlit interface\n\nst.title(\"Title: Spot Or Not?\")\nst.write(\"Subtitle: Parking Spot Vacancy with Machine Learning\")\n\n# Render the readme as markdown using st.markdown as default\n#  This loads a markdown file from your github.  Much cleaner than putting loads\n#  of text in your Streamlit app! Easier to edit too!\nreadme_text = st.markdown(get_file_content_as_string(\"instructions.md\"))\n\n # Add a menu selector for the app mode on the sidebar.\nst.sidebar.title(\"Settings\")\napp_mode = st.sidebar.selectbox(\"Choose the app mode\",\n    [\"Show instructions\", \"Live data\", \"Camera viewer\",\"Show the source code\"])\nif app_mode == \"Show instructions\":\n    st.sidebar.success('Next, try selecting \"Camera viewer\".')\n# Loads the source code and displays it\nelif app_mode == \"Show the source code\":\n    readme_text.empty()\n    st.code(get_file_content_as_string(\"streamlit_app.py\"))\nelif app_mode == \"Live data\":\n    # Add horizontal line to sidebar\n    st.sidebar.markdown(\"___\")\n    readme_text.empty()\n    live_mode()\nelif app_mode == \"Camera viewer\":\n    # Add horizontal line to sidebar\n    st.sidebar.markdown(\"___\")\n    readme_text.empty()\n    camera_view()\n\n\nI didn't go over the live_mode() function, but it's essentially a wrapper for detectSpots() and countSpots(), similar to what you've built for the camera viewer. Streamlit reruns the ENTIRE app every time you make a menu selection. Newer versions of Streamlit (such as 1.0 and later) can store session state. To keep things simple, stick with this menu.\n\nLet's walk through what happens here.\n\nAfter the app loads, it'll display the \"show instructions\" (the first item in your selectbox). If a user selects a different option, you need to clear the readme_text using readme_text.empty(). ¬†.empty()tells Streamlit to clear that particular object. Then you run your wrapper functions to show whatever it is the user selected in the main area. Since you don't have a session state, you don't need to 'erase' the previous choice, except the default- readme_text.\n\nWrapping up\nPhoto by Ruffa Jane Reyes on Unsplash\n\nCongratulations! You did it. You got a video stream from YouTube, used unsupervised machine learning to detect parking spaces from a video, identified which of those spaces are vacant, and tied it all together in Streamlit.\n\nIf you plan to deploy to Streamlit Cloud, do it early and test it regularly to avoid package dependency despair. Keep building and don't give up!\n\nWhat's next?\n\nParking lot vacancy data has many potential uses:\n\nVehicle routing‚Äîsend vehicles to open spots via navigation app or digital signage.\nProxy for how busy an area is (shopping malls, airports, conference centers, etc.) to inform visitors.\nIdentification of the commonly used \"parking spots\" that aren't allowed (illegal parking).\n\nIf you found working with these tools amazing and frustrating at the same time, let's agree to ride a bike next time and avoid the parking hassle! üö≤\n\nGot questions? Let me know in the comments below or message me on LinkedIn.\n\nHappy Streamlit-ing! ‚ù§Ô∏è\n\nReferences:\n\nAcharya, Yan. Real-time image-based parking occupancy detection using deep learning.\nCazamias, Marek (2016). Parking Space Classification using Convolutional Neural Networks.\nAmato, Giuseppe; Carrara, Fabio; Falchi, Fabrizio; Gennaro, Claudio; Vairo, Claudio. Accessed June 2021, CNR park.\nCOCO dataset, used to train Matterport model.\nDwivedi, Priya (2018). Find where to park in real time using OpenCV and Tensorflow.\nGeitgey, Adam (2019). Snagging parking spaces with mask R CNN and Python.\nHe, Kaiming; Gkioxari, Georgia; Doll√°r, Piotr; Girshick, Ross (2017). Original Mask R-CNN paper by Facebook research.\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Finding your look-alikes with semantic search",
    "url": "https://blog.streamlit.io/finding-your-look-alikes-with-semantic-search/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nFinding your look-alikes with semantic search\n\nHow Pinecone used Streamlit to create a Hacker News Doppelg√§nger app\n\nBy Greg Kogan\nPosted in Advocate Posts, December 1 2021\nWhy a Doppelg√§nger app?\nStep 1. Create a vector database of Hacker News users\n1. Create a database in Pinecone\n2. Retrieve the data\n3. Prepare and embed the data\n4. Insert the data\n5. Query Pinecone\nStep 2. ¬†Build the app in Streamlit\n1. Install Streamlit\n2. Create a base Streamlit app\n3. Create Store and Effects\n4. Layout the page\nStep 3. Combine the two together\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nDo you want to find like-minded people on Hacker News with a similar commenting history?\n\nWe've got you covered!\n\nIn this post, you'll learn how to build a Doppelg√§nger app in three simple steps:\n\nCreate a vector database in Pinecone.\nBuild an app in Streamlit.\nCombine the two together.\n\nCan't wait and want to see how it works? Try the app right here.\n\nBut before we get into building it, let's answer one question...\n\nWhy a Doppelg√§nger app?\n\nSearching for your celebrity doppelg√§nger isn‚Äôt a new idea. In fact, it‚Äôs so unoriginal that no one has updated the celebrity-face dataset in three years!\n\nBut we weren't looking for celebrities. We were looking for users with matching comment histories‚ÄîHacker News \"celebrities\" like patio11, tptacek, and pc.\n\nAt Pinecone, we've built a vector database that makes it easy to add semantic search to production applications. We were intrigued by the idea of making a semantic search app for Hacker News. Could it compare the semantic meaning of your commenting history with the histories of all the other users?\n\nSo we thought, \"How about the doppelg√§nger idea but for Hacker News?\"\n\nIt took only a few hours to build it, with most of that time being spent on converting raw data into vector embeddings (more below) and debating which users to feature as examples. The app got a lot of attention on Hacker News (Surprise!), getting thousands of visitors and 215 comments. Many people asked how it works, so here's an inside look at how we made it and how you can make your own version.\n\nStep 1. Create a vector database of Hacker News users\n1. Create a database in Pinecone\n\nCreate a new vector index for storing and retrieving data by semantic similarity. We use cosine similarity as it's more intuitive and widely used with word vectors.\n\n!pip install -qU pinecone-client\n!pip install -qU sentence-transformers\n!pip install -qU google-cloud-bigquery\n!pip install -q pyarrow pandas numpy\n\nimport pinecone\nimport os\n\n# Load Pinecone API key\napi_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'\n# Set Pinecone environment. Default environment is us-west1-gcp\nenv = os.getenv('PINECONE_ENVIRONMENT') or 'us-west1-gcp'\npinecone.init(api_key=api_key, environment=env)\n\nindex_name = 'hackernews-doppel-demo'\npinecone.create_index(index_name, dimension=300, metric=\"cosine\", shards=1)\nindex = pinecone.Index(index_name)\n\n2. Retrieve the data\n\nCreate a class to collect the data from the publicly available dataset on BigQuery. Get every comment and story from every user that hasn't been deleted or labeled as \"dead\" in the last three years (stories and comments killed by software, moderators, or user flags).\n\nfrom google.cloud.bigquery import Client\n\nclass BigQueryClient:\n    __client = None\n\n    @classmethod\n    def connect(cls):\n        os.environ[\n            'GOOGLE_APPLICATION_CREDENTIALS'] = '<file_name>'\n        cls.__client = Client()\n\n    @classmethod\n    def get_client(cls):\n        if cls.__client is None:\n            cls.connect()\n        return cls.__client\n\n3. Prepare and embed the data\n\nCollect and merge all available data for each user‚Äîwith no additional processing steps and no weights added to comments or stories.\n\nYou'll face two limitations:\n\nCaring about all comments and stories equally.\nCapturing exactly why a user was matched with someone else if they've changed interests in the last three years.\n\nNext, create a single embedding for each user with the help of the average word embeddings of Komninos and Manandhar (about three hours). This algorithm works much faster when compared to other state-of-the-art approaches (such as the commonly used BERT model).\n\nfrom sentence_transformers import SentenceTransformer\nMODEL = SentenceTransformer('average_word_embeddings_komninos')\n\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\nclass NewsDataPrep():\n\n    def load_data(self, interval: int) -> pd.DataFrame:\n        news_data = pd.DataFrame()\n\n        try:\n            print('Retrieving data from bigquery..')\n            query = f\"\"\"\n                    SELECT distinct b.by as user, b.title, b.text\n                    FROM `bigquery-public-data.hacker_news.full` as b\n                    WHERE b.timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(),\n                    INTERVAL {interval} DAY) \n                    AND b.timestamp <= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), \n                    INTERVAL {interval - 90} DAY)\n                    AND (b.deleted IS NULL AND b.dead IS NULL)\n                    AND (b.title IS NOT NULL OR b.text IS NOT NULL)\n                    AND b.type in ('story', 'comment');\n                    \"\"\"\n\n            query_job = BigQueryClient.get_client().query(query)\n            news_data = query_job.to_dataframe()\n        except Exception as e:\n            if '403' in str(e):\n                print('Exceeded quota for BigQuery! (403)')\n\n            if '_InactiveRpcError' in str(e):\n                print('Pinecone service is not active '\n                          'at the moment (_InactiveRpcError)')\n            print(e)\n        return news_data\n\n    def get_embeddings(self, news_data: pd.DataFrame) -> List:\n        def func(x):\n            return [*map(np.mean, zip(*x))]\n\n        news_data.title.fillna(value='', inplace=True)\n        news_data.text.fillna(value='', inplace=True)\n        news_data['content'] = news_data.apply(\n            lambda x: str(x.title) + \" \" + str(x.text), axis=1)\n        vectors = MODEL.encode(news_data.content.tolist())\n        news_data['vectors'] = vectors.tolist()\n        news_data_agg = (news_data.groupby(['user'], as_index=False)\n                         .agg({'vectors': func}))\n\n        # return user_embeddings\n        return list(zip(news_data_agg.user, news_data_agg.vectors))\n\n4. Insert the data\n\nNow insert the data (as vector embeddings) into the Pinecone database. Our total index size (the number of inserted embeddings) was around 230,000. Each data point was represented as a single tuple that contained a user ID and a corresponding vector. Each vector contained 300 dimensions.\n\nimport itertools\n\ndef chunks(iterable, batch_size=100):\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n\ndata_days_download= 1100\n\nnews_data_prep = NewsDataPrep()\n\nfor i in range(data_days_download, 0, -90):\n    print(f'Loading data from {i - 90} to {i} days in the past')\n    news_data = news_data_prep.load_data(interval=i)\n    print('Creating embeddings. It will take a few minutes')\n    embeddings = news_data_prep.get_embeddings(news_data)\n    print('Starting upsert')\n    for batch in chunks(embeddings, 500):\n        index.upsert(vectors=batch)\n    print('Upsert completed')\n\n5. Query Pinecone\n\nYour database is ready to be queried for the top 10 similar users given any user ID (represented as a vector embedding). Let's build a Streamlit app so that anyone can do this through their browser.\n\nStep 2. ¬†Build the app in Streamlit\n\nThe above summarized the data preparation and the database configuration steps (see the Pinecone quickstart guide for instructions). With the data vectorized and loaded into Pinecone, you can now build a Streamlit app to let anyone query that database through the browser.\n\n1. Install Streamlit\n\nInstall Streamlit by running:\n\npip install streamlit\n\n\nTo see some examples of what Streamlit is capable of, run:\n\nstreamlit hello\n\n2. Create a base Streamlit app\n\nCreate a base class to represent your Streamlit app. It'll contain a store and an effect object. You'll use the effect object to initialize Pinecone and to save the index name in the store. Next, add a render method to handle the page layout.\n\nIn a Streamlit app, each user action prompts the screen to be cleared and the main function to be run. Create the app and call render. In render, use st.title to display a title, then call render on the home page.\n\nimport streamlit as st\n\nclass App:\n\ttitle = \"Hacker News Doppelg√§nger\"\n\n\tdef __init__(self):\n\t\tself.store = AppStore()\n\t\tself.effect = AppEffect(self.store)\n\t\tself.effect.init_pinecone()\n\n\tdef render(self):\n\t\tst.title(self.title)\n\t\tPageHome(self).render()\n\nif __name__ == \"__main__\":\n\tApp().render()\n\n3. Create Store and Effects\n\nThe store will be used to hold all the data needed to connect to Pinecone. To connect to a Pinecone index, you'll need your API key and the name of your index. You'll take this data from environment variables.\n\nTo set these locally, run:\n\nexport PINECONE_API_KEY=<api-key> && export PINECONE_INDEX_NAME=<index-name>\n\n\nThese can be set in a published Streamlit app during the creation process or by changing the settings on a running app:\n\nimport os\nfrom dataclasses import dataclass\n\nAPI_KEY = os.getenv(\"PINECONE_API_KEY\")\nINDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n\n@dataclass\nclass AppStore:\n\tapi_key = API_KEY\n\tindex_name = INDEX_NAME\n\n\nUse the AppEffect class to connect your app to Pinecone (with init) and to the index (docs):\n\nclass AppEffect:\n\t\n\tdef __init__(self, store: AppStore):\n\t\tself.store = store\n\n\tdef init_pinecone(self):\n\t\tpinecone.init(api_key=self.store.api_key)\n\n\tdef init_pinecone_index(self):\n\t\treturn pinecone.Index(self.store.index_name)\n\n4. Layout the page\n\nCreate and fill out the render method of the PageHome class.\n\nFirst, use st.markdown to display instructions. Under it, display the buttons for suggested usernames. Use st.beta_columns to organize Streamlit elements in columns and st.button to place a clickable button on the page.\n\nIf the app's last action was clicking on that button, then st.button will return True. Save the value of that user in st.session_state (to save and use this value between renderings):\n\ndef render_suggested_users(self):\n\tst.markdown(\"Try one of these users:\")\n\tcolumns = st.beta_columns(len(SUGGESTED_USERNAMES))\n\tfor col, user in zip(columns, SUGGESTED_USERNAMES):\n\t\twith col:\n\t\t\tif st.button(user):\n\t\t\t\tst.session_state.username = user\n\n\nBelow the suggested users, show a text entry where the user can enter any username and a submit button which they can click on, to search.\n\nTo do this, use st.form with st.text_input and st.form_submit_buttonm. If you have a selected username saved in st.session_state.markdown, put that value in the text box. Otherwise, leave it empty for user input.\n\nNow, return the value from st.form_submit_button . It'll return true if the user clicked the submit button on the last run:\n\ndef render_search_form(self):\n\tst.markdown(\"Or enter a username:\")\n\twith st.form(\"search_form\"):\n\t\tif st.session_state.get('username'):\n\t\t\tst.session_state.username = st.text_input(\"Username\", value=st.session_state.username)\n\t\telse:\n\t\t\tst.session_state.username = st.text_input(\"Username\")\n\t\treturn st.form_submit_button(\"Search\")\n\n\nOnce the user searches, render the results. Use st.spinner to show a progress indicator to the user while loading the results. Because of Pinecone's blazing-fast search speeds, the loading icon won't be visible for long!\n\nTo complete the search, fetch the user from your Pinecone index using the entered username as the ID. No vector for the user? That means they didn't have any activity on Hacker News in the last three years, so you'll see an error message.\n\nIf you find a user, query Pinecone for the closest matches. Use a Markdown table to display the results and include a link to their Hacker News comment history as well as the proximity score for each result:\n\ndef render_search_results(self):\n\twith st.spinner(\"Searching for \" + st.session_state.username):\n\t\tresult = self.index.fetch(ids=st.session_state.username)\n\t\thas_user = len(result.vector) != 0\n\tif !has_user:\n\t\treturn st.markdown(\"This user does not exist or does not have any recent activity.\")\n\twith st.spinner(\"Found user history, searching for doppelg√§nger\"):\n\t\tclosest = self.index.queries(queries=result.vector, top_k=11)\n\tresults = [{'username': id, 'score': round(score, 3)}\n\t\t\tfor id, score in zip(closest.ids, closest.scores)\n\t\t\tif id != st.session_state.username][:10]\n\tresult_strings = \"\\\\n\".join([\n\nf\"|[{result.get('username')}](<https://news.ycombinator.com/threads?id={result.get('username')}>)|{result.get('score')}|\" for result in results\n])\n\tmarkdown = f\"\"\"\n\t| Username | Similarity Score |\n\t|----------|------:|\n\t{result_strings}\n\t\"\"\"\n\twith st.beta_container():\n\t\tst.markdown(markdown)\n\nStep 3. Combine the two together\n\nYou're almost done! All that's left is to tie it all together in a single render method:\n\nclass PageHome:\n\n\tdef __init__(self, app):\n\t\tself.app = app\n\t\n\t@property\n\tdef index(self):\n\t\treturn self.app.effect.init_pinecone_index()\n\n\tdef render(self):\n\t\tself.render_suggested_users()\n\t\tsubmitted = self.render_search_form()\n\t\tif submitted:\n\t\t\tself.render_search_results()```\n\n\nCongratulations! ü•≥\n\nYou now have a fully functioning Hacker News Doppelg√§nger app. Run streamlit.app.py and navigate to localhost:8051 to see your app in action.\n\nWrapping up\n\nThank you for reading this post. We're very excited to have shared this with you and we hope this inspires you to build your own semantic search application with Pinecone and Streamlit.\n\nHave questions or improvement ideas? Please leave them in the comments below or send them to info@pinecone.io or @pinecone.\n\nHappy app-building! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Spot-Or-Not-GIF.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/11/Spot-Or-Not-GIF.gif#border",
    "html": ""
  },
  {
    "title": "Cosmoracle-social.gif (1266√ó636)",
    "url": "https://blog.streamlit.io/content/images/2022/02/Cosmoracle-social.gif#browser",
    "html": ""
  },
  {
    "title": "Timelapse--Original-Res-1452-px----1--1.gif (1452√ó844)",
    "url": "https://blog.streamlit.io/content/images/2021/12/Timelapse--Original-Res-1452-px----1--1.gif#shadow",
    "html": ""
  },
  {
    "title": "empty-1.png (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/01/empty-1.png#border",
    "html": ""
  },
  {
    "title": "Streamlit-Prophet.gif (1200√ó675)",
    "url": "https://blog.streamlit.io/content/images/2021/12/Streamlit-Prophet.gif#shadow",
    "html": ""
  },
  {
    "title": "Build a Streamlit Form Generator app to avoid writing code by hand",
    "url": "https://blog.streamlit.io/build-a-streamlit-form-generator-app-to-avoid-writing-code-by-hand/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild a Streamlit Form Generator app to avoid writing code by hand\n\nLearn how to make extendable starter Forms\n\nBy Gerard Bentley\nOctober 24 2022\nWhat is Streamlit Form Generator app?\n1. Get an OpenAPI Specification (OAS) from the user by upload, text box, or URL.\n2. Generate Python code with Pydantic BaseModel classes from objects in the OAS API schema.\n3. Create a ZIP archive of the generated code for users to download and make demos with\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Gerard Bentley. I‚Äôm a backend engineer at Sensible Weather and a Streamlit Creator.\n\nI made lots of Streamlit demos to understand our web API endpoints‚Äîthat took writing a bunch of similar code. So instead of writing Python models and starter Streamlit code by hand, I combined Streamlit-Pydantic, Datamodel Code Generator, and the OpenAPI Specification and built the Streamlit Form Generator app.\n\nIn this post, I‚Äôll show you how to build it step-by-step:\n\nGet an OpenAPI Specification (OAS) from the user by upload, text box, or URL.\nGenerate Python code with Pydantic BaseModel classes from objects in the OAS API schema.\nCreate a ZIP archive of the generated code for users to download and make demos with.\n\nCan‚Äôt wait to get started? Here's the app and the repo.\n\nWhat is Streamlit Form Generator app?\n\nThe Streamlit Form Generator app makes code that accepts and validates user inputs according to an API spec. It will let you:\n\nGet multiple user inputs of an API spec.\nParse it into Pydantic models.\nTemplatize a new Streamlit-Pydantic form-based app with user-selected options.\n\nHere is the template generated by the example OAS:\n\nIn an internal dogfooding demo, I use this form input for making HTTP requests to my development server. I‚Äôve included Streamlit-Folium to let users select the latitude and the longitude with a pin on a map.\n\nLet‚Äôs skip writing similar Streamlit Forms and build this app together:\n\n1. Get an OpenAPI Specification (OAS) from the user by upload, text box, or URL.\n\nYou can get user input for the app by doing the following:\n\nUse an example OAS (from the Weather Insurance Quote API that I work on at Sensible Weather).\nUpload a file containing an OAS.\nManually enter an OAS in a text box.\nFetch a file containing an OAS from a URL.\n\nTo start, install Streamlit. Use pip install streamlit in your Python environment of choice or see the detailed Get started guide for more options. After a conventional import streamlit as st, use a radio button to let the user choose their input method:\n\nimport streamlit as st\n\nuse_example = \"Example OpenAPI Specification\"\nuse_upload = \"Upload an OpenAPI Specification\"\nuse_text_input = \"Enter OpenAPI Specification in Text Input\"\nuse_url = \"Fetch OpenAPI Specification from a URL\"\n\nst.header('OAS -> Pydantic -> Streamlit Form Code Generator')\ninput_method = st.radio(\n    label=\"How will you select your API Spec\",\n    options=[use_example, use_upload, use_text_input, use_url],\n)\n\nst.subheader(input_method)\n\n\n\nYour app will look something like this:\n\nChange st.radio() to st.sidebar.radio() to see if the app looks better with this option in the sidebar. Or change it to st.selectbox() if you prefer the look of select boxes.\n\nTo get the raw OAS text, present the data at each step in an st.expander() for the user to inspect. The expander will hide the content if the user doesn't care:\n\nfrom pathlib import Path\n\nimport httpx\nfrom pydantic import BaseModel, HttpUrl\nfrom streamlit.runtime.uploaded_file_manager import UploadedFile\n\n# ...\n\n@st.experimental_memo\ndef decode_uploaded_file(oas_file: UploadedFile) -> str:\n    return oas_file.read().decode()\n\n@st.experimental_memo\ndef decode_text_from_url(oas_url: str) -> str:\n    try:\n        response = httpx.get(oas_url, follow_redirects=True, timeout=10)\n        return response.text\n    except Exception as e:\n        print(repr(e))\n        return \"\"\n\nclass ValidURL(BaseModel):\n    url: HttpUrl\n\ndef get_raw_oas(input_method: str) -> str:\n    if input_method == use_example:\n        st.write(\"This will demo how the app works!\")\n        oas_file = Path(\"quote-oas.json\")\n        raw_oas = oas_file.read_text()\n    elif input_method == use_upload:\n        st.write(\"This will let you use your own JSON or YAML OAS!\")\n        oas_file = st.file_uploader(\n            label=\"Upload an OAS\",\n            type=[\"json\", \"yaml\", \"yml\"],\n            accept_multiple_files=False,\n        )\n        if oas_file is None:\n            st.warning(\"Upload a file to continue!\")\n            st.stop()\n        raw_oas = decode_uploaded_file(oas_file)\n    elif input_method == use_text_input:\n        st.write(\"This will parse raw text input into JSON or YAML OAS!\")\n        raw_oas = st.text_area(label=\"Enter OAS JSON or YAML text\")\n        if not len(raw_oas):\n            st.warning(\"Enter OAS text to continue!\")\n            st.stop()\n    elif input_method == use_url:\n        st.write(\"This will fetch text from the URL containing a JSON or YAML OAS!\")\n        raw_oas_url = st.text_input(label=\"Enter the URL that hosts the OAS\")\n        try:\n            oas_url = ValidURL(url=raw_oas_url)\n        except Exception as e:\n            print(repr(e))\n            st.warning(\"Enter a valid HTTP(S) URL to continue!\")\n            st.stop()\n        raw_oas = decode_text_from_url(oas_url.url)\n    else:\n        raise Exception(\"Unknown input_method\")\n    return raw_oas\n\nraw_oas = get_raw_oas()\nwith st.expander(\"Show input OAS\"):\n    st.code(raw_oas)\n\n\n\nNow your app should look something like this:\n\nThe point of this get_raw_oas block is to get the raw OAS text regardless of the input method:\n\nIn the \"Example\" branch, use the built-in Path class to read_text() from a file in the same folder as your Streamlit app (this works when the app is deployed to Streamlit Community Cloud and the file is in GitHub).\nIn the \"Upload\" branch, use st.file_uploader() to let the user drag and drop or choose their file. This provides None if the user hasn‚Äôt uploaded anything and a BytesIO-like object if they have.\nIn the \"Text Input\" branch, use st.text_area() to allow free text input. You can opt for something fancier such as Streamlit Quill for a rich text editor.\nIn the \"URL\" branch, let the user input URL as text, validate it with Pydantic's HttpURL class, and 'GET' the file from the URL using [httpx](<https://github.com/encode/httpx/>).\nFor the \"Upload\" and the \"URL\" branches, use st.experimental_memo to cache the results between re-runs. The \"Example\" branch runs fast enough, and the \"Text Input\" branch is already cached until changed (because that‚Äôs how Streamlit works).\n\nIn all of the user upload branches warn the user and stop the script if they haven't added anything. You can use this technique with Streamlit forms as well.\n\n2. Generate Python code with Pydantic BaseModel classes from objects in the OAS API schema.\n\nNow, let‚Äôs go over using Datamodel Code Generator to parse the raw input text into the Python Pydantic classes. It‚Äôs super convenient for making Python clients that communicate with APIs. You can do the following:\n\nGet type validation and autocomplete.\nDump Pydantic models into the JSON format to call your API.\nParse the JSON response into Pydantic models for validation and further handling.\n\nDictionaries are more convenient for hacking around, but one misspelled key can make bug-hunting hard. Import the module and follow this documentation example of calling it a module. Use a TemporaryDirectory so as not to pollute the Streamlit Community Cloud app (you‚Äôll allow downloading at the end).\n\nIf the OAS is very large, cache this function:\n\nimport ast\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import List\n\nfrom datamodel_code_generator import InputFileType, generate\n\n# ...\n\n@dataclass\nclass ModuleWithClasses:\n    name: str\n    code: str\n    classes: List[str]\n\n@st.experimental_memo()\ndef parse_into_modules(raw_oas: str) -> List[ModuleWithClasses]:\n    with TemporaryDirectory() as temporary_directory_name:\n        temporary_directory = Path(temporary_directory_name)\n        module_files = generate_module_or_modules(raw_oas, temporary_directory)\n\n        modules = []\n        for module in module_files:\n            module_code = module.read_text()\n\n            module_ast = ast.parse(module_code)\n            module_class_names = [\n                x.name for x in module_ast.body if isinstance(x, ast.ClassDef)\n            ]\n            modules.append(\n                ModuleWithClasses(\n                    name=module.stem,\n                    code=module_code,\n                    classes=module_class_names,\n                )\n            )\n    return modules\n\ndef generate_module_or_modules(raw_oas: str, output_directory: Path) -> List[Path]:\n    output = Path(output_directory / \"models.py\")\n    try:\n        generate(\n            raw_oas,\n            input_file_type=InputFileType.OpenAPI,\n            output=output,\n        )\n        return [output]\n    except Exception as e:\n        print(repr(e))\n        try:\n            generate(\n                raw_oas,\n                input_file_type=InputFileType.OpenAPI,\n                output=output_directory,\n            )\n            return list(output_directory.iterdir())\n        except Exception as e:\n            print(repr(e))\n            return []\n\nmodules = parse_into_modules(raw_oas)\nif not len(modules):\n    st.error(\"Couldn't find any models in the input!\")\n    st.stop()\n\nst.success(f\"Generated {len(modules)} module files\")\n\nall_module_models = []\nfor module in modules:\n    import_name = module.name\n    if import_name != \"models\":\n        import_name = f\"models.{import_name}\"\n    with st.expander(f\"Show Generated Module Code: {import_name}\"):\n        st.code(module.code)\n\n    for model_name in module.classes:\n        all_module_models.append((module.name, model_name))\n\n\n\nTo let the user pick classes for Streamlit form inputs, use Python's Abstract Syntax Tree module to parse (ast.parse()) each generated module and grab them into a list.\n\nTry the generate() function with an output target of a single file Path and a directory Path. Showing the full error text to the user might reveal sensitive data, so printing it to the console is good enough.\n\nThere are more full-featured code generators such as this open-source OpenAPITools project with over 30 languages. I used Datamodel Code Generator to make a flexible Streamlit + Pydantic template. It also produces idiomatic Pydantic code and has the flexibility to handle JSON schema, raw JSON/CSV/YAML data, and even a Python dictionary. (It‚Äôs a Python library, and I don't know how to run OpenAPI generator as an npm package or Java jar on Streamlit Community Cloud üòÖ.)\n\nAfter you generate Pydantic models from the OAS, your app will look something like this:\n\n3. Create a ZIP archive of the generated code for users to download and make demos with\n\nGenerate code for a Streamlit form with Streamlit-Pydantic and whatever generated classes the user selects as input possibilities.\n\nSince most API specs will have some models that are only used within other models, let the user decide what they want to use for their Streamlit forms. You can further utilize the OpenAPI specification documentation to check what schema models are used as request bodies, but this will take a lot of modifications to the Datamodel Code Generator.\n\nUse Python string manipulation to make a starting point for using the selected models in Streamlit. The guts are not very exciting and should be replaced by a templating library such as jinja2 if you want to make more complicated starter code:\n\nif len(all_module_models) > 1:\n    selections = st.multiselect(\n        label=\"Select Models that will be Form Inputs\",\n        options=all_module_models,\n        default=all_module_models[0],\n        format_func=lambda x: f\"{x[0]}.{x[1]}\",\n    )\nelse:\n    selections = list(all_module_models)\n\ndef generate_header(models_with_modules: List[Tuple[str, str]]) -> str:\n# ...\n\ndef generate_single_model_form(model: str) -> str:\n# ...\n\ndef generate_multi_model_form(models: List[str]) -> str:\n# ...\n\n@st.experimental_memo\ndef generate_streamlit_code(selected_module_models: List[Tuple[str, str]]) -> str:\n    streamlit_code = generate_header(selected_module_models)\n    if len(selected_module_models) == 1:\n        model_module, model = selected_module_models[0]\n        streamlit_code += generate_single_model_form(model)\n    else:\n        models = [model for _, model in selected_module_models]\n        streamlit_code += generate_multi_model_form(models)\n    return streamlit_code\n\nstreamlit_code = generate_streamlit_code(selections)\nwith st.expander(\"Show Generated Streamlit App Code\", True):\n    st.code(body=streamlit_code, language=\"python\")\n\n\n\nOne lacking aspect of templating is repetitive imports. Do a quick run of isort on the generated code to clean this up.\n\nFor the user to choose multiple models as form inputs, give the template a radio selector in the sidebar. You can adapt it to multi-page apps (I kept it in one page).\n\nAt this point, your app should look something like this:\n\nBuild the models.py if it‚Äôs contained in a single module, or build the models directory structure with all of the modules.\n\nStreamlit's download_button works great with a bytes object:\n\n@st.experimental_memo()\ndef zip_generated_code(modules: List[ModuleWithClasses], streamlit_code: str) -> bytes:\n# ...\n\nzip_bytes = zip_generated_code(modules, streamlit_code)\nst.download_button(\n    label=\"Download Zip of Generated Code\",\n    data=zip_bytes,\n    file_name=\"generated_code.zip\",\n)\n\nWrapping up\n\nCongratulations! You did it. You learned how to build your own Streamlit Form Generator app. I hope it will help you make user-friendly demos of your favorite APIs!\n\nIf you have any questions, please drop them below in the comments or reach out to me on GitHub, LinkedIn, or Twitter with your ideas or inspiring projects!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "memory.png (2000√ó1125)",
    "url": "https://blog.streamlit.io/content/images/2021/11/memory.png#border",
    "html": ""
  },
  {
    "title": "image-1.png (2000√ó755)",
    "url": "https://blog.streamlit.io/content/images/2021/12/image-1.png#browser",
    "html": ""
  },
  {
    "title": "session_state_0x7fedb68992c8.png (815√ó1668)",
    "url": "https://blog.streamlit.io/content/images/2021/12/session_state_0x7fedb68992c8.png#browser",
    "html": ""
  },
  {
    "title": "Untitled.png (1588√ó952)",
    "url": "https://blog.streamlit.io/content/images/2021/12/Untitled.png#browser",
    "html": ""
  },
  {
    "title": "4.png (700√ó295)",
    "url": "https://blog.streamlit.io/content/images/2021/12/4.png",
    "html": ""
  },
  {
    "title": "5.png (472√ó506)",
    "url": "https://blog.streamlit.io/content/images/2021/12/5.png",
    "html": ""
  },
  {
    "title": "10.png (880√ó424)",
    "url": "https://blog.streamlit.io/content/images/2021/12/10.png",
    "html": ""
  },
  {
    "title": "9-1.png (800√ó158)",
    "url": "https://blog.streamlit.io/content/images/2021/12/9-1.png",
    "html": ""
  },
  {
    "title": "8-1.png (800√ó175)",
    "url": "https://blog.streamlit.io/content/images/2021/12/8-1.png",
    "html": ""
  },
  {
    "title": "7.png (480√ó980)",
    "url": "https://blog.streamlit.io/content/images/2021/12/7.png",
    "html": ""
  },
  {
    "title": "6.png (494√ó590)",
    "url": "https://blog.streamlit.io/content/images/2021/12/6.png",
    "html": ""
  },
  {
    "title": "3-1.png (496√ó362)",
    "url": "https://blog.streamlit.io/content/images/2021/12/3-1.png",
    "html": ""
  },
  {
    "title": "Streamlit (Page 8)",
    "url": "https://blog.streamlit.io/page/8/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing Streamlit to the Polish Python community\n\nMy Streamlit presentation at PyWaW #103\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 4 2023\nBuilding an Instagram hashtag generation app with Streamlit\n\n5 simple steps on how to build it\n\nAdvocate Posts\nby\nWilliam Mattingly\n,\nMarch 29 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nHackathon 101: 5 simple tips for beginners\n\nPrepare to win your first hackathon!\n\nTutorials\nby\nChanin Nantasenamat\n,\nMarch 16 2023\nCreate a search engine with Streamlit and Google Sheets\n\nYou‚Äôre sitting on a goldmine of knowledge!\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nMarch 14 2023\n10 most common explanations on the Streamlit forum\n\nA guide for Streamlit beginners\n\nAdvocate Posts\nby\nDebbie Matthews\n,\nMarch 9 2023\nBuilding a PivotTable report with Streamlit and AG Grid\n\nHow to build a PivotTable app in 4 simple steps\n\nAdvocate Posts\nby\nPablo Fonseca\n,\nMarch 7 2023\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nProduct\nby\nLukas Masuch and¬†\n2\n¬†more,\nFebruary 28 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nProduct\nby\nTim Conkling and¬†\n2\n¬†more,\nFebruary 14 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1.png (939√ó599)",
    "url": "https://blog.streamlit.io/content/images/2021/12/1.png",
    "html": ""
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/7/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nBuild a Jina neural search with Streamlit\n\nUse Jina to search text or images with the power of deep learning\n\nAdvocate Posts\nby\nAlex C-G\n,\nApril 15 2021\nHow to use Roboflow and Streamlit to visualize object detection output\n\nBuilding an app for blood cell count detection\n\nAdvocate Posts\nby\nMatt Brems\n,\nFebruary 23 2021\nDeveloping a streamlit-webrtc component for real-time video processing\n\nIntroducing the WebRTC component for real-time media streams\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nFebruary 12 2021\nArup and New Story use data to help combat pandemic related evictions\n\nMaking data accessible to help address the eviction crisis\n\nAdvocate Posts\nby\nJared Stock\n,\nJanuary 7 2021\nGravitational-wave apps help students learn about black holes\n\nExploring distant space with gravitational waves\n\nAdvocate Posts\nby\nJonah Kanner\n,\nDecember 15 2020\nBuild knowledge graphs with the Streamlit Agraph component\n\nA powerful and lightweight library for visualizing networks/graphs\n\nAdvocate Posts\nby\nChristian Klose\n,\nNovember 25 2020\nNew UC Davis tool tracks California's COVID-19 cases by region\n\nRegional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine\n\nAdvocate Posts\nby\nPranav Pandit\n,\nNovember 19 2020\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Jeffrey Jex - Streamlit",
    "url": "https://blog.streamlit.io/author/jeffrey/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jeffrey Jex\n1 post\nDetecting parking spots with Streamlit\n\nHow to build a Streamlit parking spot app in 8 simple steps\n\nAdvocate Posts\nby\nJeffrey Jex\n,\nOctober 26 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Download_Button--Full-Screen--1.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Download_Button--Full-Screen--1.gif#browser",
    "html": ""
  },
  {
    "title": "Maxime Lutel - Streamlit",
    "url": "https://blog.streamlit.io/author/maxime/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Maxime Lutel\n1 post\nWebsite\nForecasting with Streamlit Prophet\n\nHow Artefact built a Streamlit app to train time-series forecasting models\n\nAdvocate Posts\nby\nMaxime Lutel\n,\nNovember 10 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "2.png (967√ó178)",
    "url": "https://blog.streamlit.io/content/images/2021/12/2.png",
    "html": ""
  },
  {
    "title": "Greg Kogan - Streamlit",
    "url": "https://blog.streamlit.io/author/greg/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Greg Kogan\n1 post\nFinding your look-alikes with semantic search\n\nHow Pinecone used Streamlit to create a Hacker News Doppelg√§nger app\n\nAdvocate Posts\nby\nGreg Kogan\n,\nDecember 1 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Eitan Halper-Stromberg - Streamlit",
    "url": "https://blog.streamlit.io/author/eitan/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Eitan Halper-Stromberg\n1 post\nHow to diagnose blood cancer with Streamlit\n\nBuild a molecular pathology diagnostics app in 4 simple steps\n\nAdvocate Posts\nby\nEitan Halper-Stromberg\n,\nJanuary 25 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "soc-2-small-long-1.png (1500√ó506)",
    "url": "https://blog.streamlit.io/content/images/2022/01/soc-2-small-long-1.png",
    "html": ""
  },
  {
    "title": "coiled2-3.gif (1600√ó900)",
    "url": "https://blog.streamlit.io/content/images/2021/10/coiled2-3.gif#browser",
    "html": ""
  },
  {
    "title": "Balloons_Snowflake.gif (1515√ó852)",
    "url": "https://blog.streamlit.io/content/images/2022/03/Balloons_Snowflake.gif#shadow",
    "html": ""
  },
  {
    "title": "design---six-tips-1.png (1594√ó1300)",
    "url": "https://blog.streamlit.io/content/images/2021/08/design---six-tips-1.png#shadow",
    "html": ""
  },
  {
    "title": "cache-1.png (1840√ó1158)",
    "url": "https://blog.streamlit.io/content/images/2021/10/cache-1.png#border",
    "html": ""
  },
  {
    "title": "rsource-limits-3.png (1284√ó1048)",
    "url": "https://blog.streamlit.io/content/images/2021/10/rsource-limits-3.png#border",
    "html": ""
  },
  {
    "title": "image.png (2000√ó2017)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image.png#border",
    "html": ""
  },
  {
    "title": "Share-GIF--30-FPS-.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/12/Share-GIF--30-FPS-.gif#shadow",
    "html": ""
  },
  {
    "title": "favicon-transparent-1.png (256√ó256)",
    "url": "https://blog.streamlit.io/content/images/size/w256h256/2021/03/favicon-transparent-1.png",
    "html": ""
  },
  {
    "title": "MicrosoftTeams-image--23---2-.png (2000√ó2467)",
    "url": "https://blog.streamlit.io/content/images/2021/08/MicrosoftTeams-image--23---2-.png#shadow",
    "html": ""
  },
  {
    "title": "Sogeti_feature-GIF-1.gif (1912√ó955)",
    "url": "https://blog.streamlit.io/content/images/2022/03/Sogeti_feature-GIF-1.gif#shadow",
    "html": ""
  },
  {
    "title": "wavo-2.png (1478√ó1206)",
    "url": "https://blog.streamlit.io/content/images/2021/10/wavo-2.png#browser",
    "html": ""
  },
  {
    "title": "CloneRetriever--1726-px---2-.gif (1726√ó842)",
    "url": "https://blog.streamlit.io/content/images/2022/02/CloneRetriever--1726-px---2-.gif#shadow",
    "html": ""
  },
  {
    "title": "Delta_dental_hero.png (2554√ó1302)",
    "url": "https://blog.streamlit.io/content/images/2022/02/Delta_dental_hero.png#browser",
    "html": ""
  },
  {
    "title": "Radius-Explorer--Static-.png (1894√ó932)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Radius-Explorer--Static-.png#border",
    "html": ""
  },
  {
    "title": "Live-Data-Science-Dashboard-GIF.gif (1717√ó997)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Live-Data-Science-Dashboard-GIF.gif#border",
    "html": ""
  },
  {
    "title": "Memory-Leak.png (2000√ó755)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Memory-Leak.png#border",
    "html": ""
  },
  {
    "title": "supabase.png (1664√ó796)",
    "url": "https://blog.streamlit.io/content/images/2022/05/supabase.png#browser",
    "html": ""
  },
  {
    "title": "OSS-podcast-1.png (1986√ó1656)",
    "url": "https://blog.streamlit.io/content/images/2022/04/OSS-podcast-1.png#shadow",
    "html": ""
  },
  {
    "title": "30DaysOfStreamlit--1-.png (2000√ó981)",
    "url": "https://blog.streamlit.io/content/images/2022/05/30DaysOfStreamlit--1-.png#shadow",
    "html": ""
  },
  {
    "title": "Vincent D. Warmerdam - Streamlit",
    "url": "https://blog.streamlit.io/author/vincent/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Vincent D. Warmerdam\n1 post\nMaking apps for the Rasa research team (and open source community!)\n\nHelping Rasa users understand their models\n\nAdvocate Posts\nby\nVincent D. Warmerdam\n,\nMay 12 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "pasted-image-0--3-.png (1170√ó772)",
    "url": "https://blog.streamlit.io/content/images/2021/08/pasted-image-0--3-.png#browser",
    "html": ""
  },
  {
    "title": "5--2-.png (1200√ó690)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5--2-.png#border",
    "html": ""
  },
  {
    "title": "4--4-.png (1200√ó271)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4--4-.png#border",
    "html": ""
  },
  {
    "title": "3-2--1-.png (1200√ó631)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-2--1-.png#border",
    "html": ""
  },
  {
    "title": "2--1-.png (1200√ó901)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2--1-.png#border",
    "html": ""
  },
  {
    "title": "Intro-Rascore-new-1.gif (960√ó544)",
    "url": "https://blog.streamlit.io/content/images/2022/06/Intro-Rascore-new-1.gif#browser",
    "html": ""
  },
  {
    "title": "wissam-hero.png (2000√ó1069)",
    "url": "https://blog.streamlit.io/content/images/2022/06/wissam-hero.png#shadow",
    "html": ""
  },
  {
    "title": "jina.gif (1280√ó720)",
    "url": "https://blog.streamlit.io/content/images/2022/09/jina.gif#border",
    "html": ""
  },
  {
    "title": "streamlit-streamlit.gif (1092√ó641)",
    "url": "https://blog.streamlit.io/content/images/2022/05/streamlit-streamlit.gif#browser",
    "html": ""
  },
  {
    "title": "HappyBirds--1920-.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/02/HappyBirds--1920-.gif#shadow",
    "html": ""
  },
  {
    "title": "workspace-analytics-dotted-5.gif (1000√ó755)",
    "url": "https://blog.streamlit.io/content/images/2022/05/workspace-analytics-dotted-5.gif#browser",
    "html": ""
  },
  {
    "title": "streamlit-spell-book.png (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/08/streamlit-spell-book.png",
    "html": ""
  },
  {
    "title": "1-3-1.png (1200√ó420)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-3-1.png",
    "html": ""
  },
  {
    "title": "GIF_in_the_Browser_4--1--3.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/12/GIF_in_the_Browser_4--1--3.gif#shadow",
    "html": ""
  },
  {
    "title": "arup.gif (2364√ó1134)",
    "url": "https://blog.streamlit.io/content/images/2022/08/arup.gif#browser",
    "html": ""
  },
  {
    "title": "How-to-master-Streamlit.png (2000√ó858)",
    "url": "https://blog.streamlit.io/content/images/2022/02/How-to-master-Streamlit.png#border",
    "html": ""
  },
  {
    "title": "Alex C-G - Streamlit",
    "url": "https://blog.streamlit.io/author/alexcg/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Alex C-G\nDev Rel Lead at Jina AI, building AI-powered search. Open source evangelist and caffeine addict. Python in my üß†, Vim in my üñêÔ∏è\n1 post\nWebsite\nTwitter\nBuild a Jina neural search with Streamlit\n\nUse Jina to search text or images with the power of deep learning\n\nAdvocate Posts\nby\nAlex C-G\n,\nApril 15 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Camera-input-release-demo-1.png (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/02/Camera-input-release-demo-1.png#shadow",
    "html": ""
  },
  {
    "title": "3-3.png (2000√ó1266)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-3.png#browser",
    "html": ""
  },
  {
    "title": "search-console-hero.gif (1234√ó868)",
    "url": "https://blog.streamlit.io/content/images/2022/07/search-console-hero.gif#browser",
    "html": ""
  },
  {
    "title": "2-5.png (2000√ó1266)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-5.png#browser",
    "html": ""
  },
  {
    "title": "Improved-charts.gif (750√ó424)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Improved-charts.gif#browser",
    "html": ""
  },
  {
    "title": "Jessica Smith - Streamlit (Page 3)",
    "url": "https://blog.streamlit.io/author/jessica/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jessica Smith\n26 posts\nMonthly rewind > July 2021\n\nYour July look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 5 2021\nMonthly rewind > June 2021\n\nYour June look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 5 2021\nMonthly rewind > May 2021\n\nYour May look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2021\nMonthly rewind > April 2021\n\nYour April look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2021\nMonthly rewind > March 2021\n\nYour March look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 5 2021\nMonthly rewind > February 2021\n\nYour February look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 8 2021\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "autofilter-dataframes.gif (782√ó609)",
    "url": "https://blog.streamlit.io/content/images/2022/09/autofilter-dataframes.gif#browser",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-20-at-1.44.49-PM.png (2000√ó1251)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-20-at-1.44.49-PM.png#browser",
    "html": ""
  },
  {
    "title": "qiusheng-hero.jpeg (1480√ó700)",
    "url": "https://blog.streamlit.io/content/images/2022/07/qiusheng-hero.jpeg#border",
    "html": ""
  },
  {
    "title": "Jared Stock - Streamlit",
    "url": "https://blog.streamlit.io/author/jared/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jared Stock\nJared is a Digital Consultant at Arup\n1 post\nTwitter\nArup and New Story use data to help combat pandemic related evictions\n\nMaking data accessible to help address the eviction crisis\n\nAdvocate Posts\nby\nJared Stock\n,\nJanuary 7 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "open-source-1.png (2000√ó944)",
    "url": "https://blog.streamlit.io/content/images/2022/09/open-source-1.png#border",
    "html": ""
  },
  {
    "title": "dynamic-filters.png (1105√ó592)",
    "url": "https://blog.streamlit.io/content/images/2022/09/dynamic-filters.png#border",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-29-at-4.53.15-PM-1.png (1013√ó201)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-29-at-4.53.15-PM-1.png#shadow",
    "html": ""
  },
  {
    "title": "starterkithero.png (2000√ó946)",
    "url": "https://blog.streamlit.io/content/images/2022/10/starterkithero.png",
    "html": ""
  },
  {
    "title": "streamlitformgenerator.gif (853√ó715)",
    "url": "https://blog.streamlit.io/content/images/2022/11/streamlitformgenerator.gif#browser",
    "html": ""
  },
  {
    "title": "Create-your-own-custom-component.png (2000√ó941)",
    "url": "https://blog.streamlit.io/content/images/2022/10/Create-your-own-custom-component.png",
    "html": ""
  },
  {
    "title": "uplanner.png (1512√ó882)",
    "url": "https://blog.streamlit.io/content/images/2022/11/uplanner.png#browser",
    "html": ""
  },
  {
    "title": "Beginner-Template-Tour.gif (640√ó362)",
    "url": "https://blog.streamlit.io/content/images/2022/10/Beginner-Template-Tour.gif#browser",
    "html": ""
  },
  {
    "title": "Monthly rewind > November 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-november-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > November 2022\n\nYour November look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, December 7 2022\nüèÜ App of the month üèÜ\nStreamlit November updates\nüîç Current release: 1.15.1\nüîÆ Upcoming\nüîç Indexability\nüñåÔ∏è Figma-to-Streamlit Plugin\nFeatured Streamlit content\nüíé Streamlit Quests: Getting started with Streamlit\nüì∫ Make a video content analyzer app with Streamlit and AssemblyAI\n‚úã Building robust Streamlit apps with type-checking\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur November featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nSong Describer by researchers Ilaria Manco, Benno Weck, Philip Tovstogan, Dmitry Bogdanov, and Minz Won.\n\nThis app is an open-source data collection platform for annotating music with textual descriptions. By collecting user inputs, the team hopes to create the first research dataset of music-caption pairs. This will give insight into how people describe music and support the development of audio-text ML models. [code]\n\nStreamlit November updates\n\nCheck out the latest updates and releases from November.\n\nüîç Current release: 1.15.1\n\nThe latest release is 1.15.1. Recent updates include the ability for widget labels to contain inline Markdown and for st.‚Äãaudio to properly play audio data from NumPy arrays. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nWe're currently working on these new features:\n\nStreamlit theme for 3rd party charting libraries\nColored text\nSt commands work with Snowpark + PySpark\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\nüîç Indexability\nAll public Community Cloud apps are now indexable by search engines, making them easier to discover.\n\nüñåÔ∏è Figma-to-Streamlit Plugin\nAfter prototyping your apps in Figma, you can turn the designs into code with the Figma-to-Streamlit plugin!\n\n\nFeatured Streamlit content\n\nüíé Streamlit Quests: Getting started with Streamlit\nCheck out Streamlit Quests‚Äîa guide from the Data Professor to help you navigate getting started with Streamlit.\n\nüì∫ Make a video content analyzer app with Streamlit and AssemblyAI\nLearn how to build an analyzer app with Mƒ±sra Turp, using Streamlit and AssemblyAI, to easily screen videos for harmful or sensitive content.\n\n‚úã Building robust Streamlit apps with type-checking\nQuickly find and eliminate defects in your code with type-checking! Harald explains how to use this technique and make it part of your app-building flow.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nSee if the first stage of Falcon 9 will land with Hakan's Booster Landing Prediction App.\nSantiago's Volve Production Dashboard app visualizes the production dashboard of an oil field and performs exploratory data analysis.\nUse Lee's SERP Keyword Extractor to extract the top 10 keywords from each page ranking in the SERPs.\nQuickly get started with Streamlit with Misra's Streamlit project template.\nTest your geography skills with Gerard's app Guess the Country 'Worldle' Edition.\nKota created a custom component for interactive board games and an app to play the Doubutsu Shogi (Animal Chess) game.\nHubert's SurViZ app lets you quantitatively compare various galaxy missions, such as Euclid, Rubin/LSST, or JWST.\nChoe's app gives a 3D Model Reconstruction using FORESIGHT STEREO Technology.\nGet EDA, flexible data wrangling, and swift ML modeling using EasyDS‚Äîa free app tool by Huijun.\nAyoub's Brainstorming Buddy, built on GPT-3, helps to generate ideas on a given topic.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nOctober 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to Create Automated Visual Tests [SeleniumBase Tutorial]",
    "url": "https://blog.streamlit.io/testing-streamlit-apps-using-seleniumbase/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nTesting Streamlit apps using SeleniumBase\n\nHow to create automated visual tests\n\nBy Randy Zwitch\nPosted in Tutorials, November 23 2020\nCase Study: streamlit-folium\nSetting up a test harness\nDefining test success\nTesting using SeleniumBase\nTesting using OpenCV\nAutomating tests using GitHub actions\nWriting tests saves work in the long run\nContents\nShare this post\n‚Üê All posts\n\nIn the time I‚Äôve worked at Streamlit, I‚Äôve seen hundreds of impressive data apps ranging from computer vision applications to public health tracking of COVID-19 and even simple children‚Äôs games. I believe the growing popularity of Streamlit comes from the fast, iterative workflows through the Streamlit ‚Äúmagic‚Äù functionality and auto-reloading the front-end upon saving your Python script. Write some code, hit ‚ÄòSave‚Äô in your editor, then visually inspect the correctness of each code change. And with the unveiling of Streamlit sharing for easy deployment of Streamlit apps, you can go from idea to coding to deploying your app in just minutes!\n\nOnce you've created a Streamlit app, you can use automated testing to future-proof it against regressions. In this post, I'll be showing how to programmatically validate that a Streamlit app is unchanged visually using the Python package SeleniumBase.\n\nCase Study: streamlit-folium\n\nTo demonstrate how to create automated visual tests, I‚Äôm going to use the streamlit-folium GitHub repo, a Streamlit Component I created for the Folium Python library for leaflet.js. Visual regression tests help detect when the layout or content of an app changes, without requiring the developer to manually visually inspect the output each time a line of code changes in their Python library. Visual regression tests also help with cross-browser compatibility of your Streamlit apps and provide advanced warning about new browser versions affecting how your app is displayed.\n\nSetting up a test harness\n\nThe streamlit-folium test harness has three files:\n\ntests/requirements.txt: the Python packages only needed for testing\ntests/app_to_test.py: the reference Streamlit app to test\ntests/test_package.py: the tests to demonstrate the package works as intended\n\nThe first step is to create a Streamlit app using the package to be tested and use that to set the baseline. We can then use SeleniumBase to validate that the structure and visual appearance of the app remains unchanged relative to the baseline.\n\nThis post focuses on describing test_package.py since it‚Äôs the file that covers how to use SeleniumBase and OpenCV for Streamlit testing.\n\nDefining test success\n\nThere are several ways to think about what constitutes looking the same in terms of testing. I chose the following three principles for testing my streamlit-folium package:\n\nThe Document Object Model (DOM) structure (but not necessarily the values) of the page should remain the same\nFor values such as headings, test that those values are exactly equal\nVisually, the app should look the same\n\nI decided to take these less strict definitions of ‚Äúunchanged‚Äù for testing streamlit-folium, as the internals of the Folium package itself appear to be non-deterministic. Meaning, the same Python code will create the same looking image, but the generated HTML will be different.\n\nTesting using SeleniumBase\n\nSeleniumBase is an all-in-one framework written in Python that wraps the Selenium WebDriver project for browser automation. SeleniumBase has two functions that we can use for the first and second testing principles listed above: check_window, which tests the DOM structure and assert_text, to ensure a specific piece of text is shown on the page.\n\nTo check the DOM structure, we first need a baseline, which we can generate using the check_window function. The check_window has two behaviors, based on the required name argument:\n\nIf a folder <name> within the visual_baseline/<Python file>.<test function name> path does not exist, this folder will be created with all of the baseline files\nIf the folder does exist, then SeleniumBase will compare the current page against the baseline at the specified accuracy level\n\nYou can see an example of calling check_window and the resulting baseline files in the streamlit-folium repo. In order to keep the baseline constant between runs, I committed these files to the repo; if I were to make any substantive changes to the app I am testing (app_to_test.py), I would need to remember to set the new baseline or the tests would fail.\n\nWith the baseline folder now present, running check_window runs the comparison test. I chose to run the test at Level 2, with the level definitions as follows:\n\nLevel 1 (least strict): HTML tags are compared to tags_level1.txt\nLevel 2: HTML tags and attribute names are compared to tags_level2.txt\nLevel 3 (most strict): HTML tags, attribute names and attribute values are compared to tags_level3.txt ¬† ¬†\n\nAs mentioned in the ‚ÄúDefining Test Success‚Äù section, I run the check_window function at Level 2, because the Folium library adds an GUID-like id value to the attribute values in the HTML, so the tests will never pass at Level 3 because the attribute values are always different between runs.\n\nFor the second test principle (‚Äúcheck certain values are equal‚Äù), the assert_text method is very easy to run:\n\nself.assert_text(\"streamlit-folium\")\n\nThis function checks that the exact text ‚Äústreamlit-folium‚Äù is present in the app, and the test passes because it‚Äôs the value of the H1 heading in this example.\n\nTesting using OpenCV\n\nWhile checking the DOM structure and presence of a piece of text provides some useful information, my true acceptance criterion is that the visual appearance of the app doesn‚Äôt change from the baseline. In order to test that the app is visually the same down to the pixel, we can use the save_screenshot method from SeleniumBase to capture the current visual state of the app and compare to the baseline using the OpenCV package:\n\n\tfrom seleniumbase import BaseCase\n\timport cv2\n\timport time\n\t\n\n\t\n\n\tclass ComponentsTest(BaseCase):\n\t    def test_basic(self):\n\t\n\n\t        # open the app and take a screenshot\n\t        self.open(\"http://localhost:8501\")\n\t\n\n\t        time.sleep(10)  # give leaflet time to load from web\n\t        self.save_screenshot(\"current-screenshot.png\")\n\t\n\n\t        # test screenshots look exactly the same\n\t        original = cv2.imread(\n\t            \"visual_baseline/test_package.test_basic/first_test/screenshot.png\"\n\t        )\n\t        duplicate = cv2.imread(\"current-screenshot.png\")\n\t\n\n\t        assert original.shape == duplicate.shape\n\t\n\n\t        difference = cv2.subtract(original, duplicate)\n\t        b, g, r = cv2.split(difference)\n\t        assert cv2.countNonZero(b) == cv2.countNonZero(g) == cv2.countNonZero(r) == 0\nview raw\nseleniumbase.py hosted with ‚ù§ by GitHub\n\nUsing OpenCV, the first step is to read in the baseline image and the current snapshot, then compare that the size of the pictures are identical (the shape comparison checks that the NumPy ndarrays of pixels have the same dimensions). Assuming the pictures are both the same size, we can then use the subtract function from OpenCV to calculate the per-element difference between pixels by channel (blue, green and red). If all three channels have no differences, then we know that the visual representation of the Streamlit app is identical between runs.\n\nAutomating tests using GitHub actions\n\nWith our SeleniumBase and OpenCV code set up, we can now feel free to make changes to our Streamlit Component (or other Streamlit apps) and not worry about things breaking unintentionally. In my single-contributor project, it‚Äôs easy to enforce running the tests locally, but with tools such as GitHub Actions available for free for open-source projects, setting up a Continuous Integration pipeline guarantees the tests are run for each commit.\n\nThe streamlit-folium has a workflow run_tests_each_PR.yml defined that does the following:\n\nSets up a test matrix for Python 3.6, 3.7, 3.8\nInstalls the package dependencies and test dependencies\nLints the code with flake8\nInstall Chrome with seleniumbase\nRun the Streamlit app to test in the background\nRun the SeleniumBase and OpenCV tests in Python\n\t# This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n\t# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\t\n\n\tname: Run tests each PR\n\t\n\n\ton:\n\t  push:\n\t    branches: [ master ]\n\t  pull_request:\n\t    branches: [ master ]\n\t\n\n\tjobs:\n\t\n\n\t  build:\n\t    runs-on: ubuntu-latest\n\t    strategy:\n\t      matrix:\n\t        python-version: [3.6, 3.7, 3.8]\n\t\n\n\t    steps:\n\t    - uses: actions/checkout@v2\n\t    - name: Set up Python ${{ matrix.python-version }}\n\t      uses: actions/setup-python@v2\n\t      with:\n\t        python-version: ${{ matrix.python-version }}\n\t    - name: Install dependencies\n\t      run: |\n\t        python -m pip install --upgrade pip\n\t        pip install flake8 pytest\n\t        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\t        if [ -f tests/requirements.txt ]; then pip install -r tests/requirements.txt; fi\n\t    - name: Lint with flake8\n\t      run: |\n\t        # stop the build if there are Python syntax errors or undefined names\n\t        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n\t        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n\t        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\t    - name: Install chromedriver\n\t      run: |\n\t        seleniumbase install chromedriver latest\n\t    - name: Start Streamlit app\n\t      run: |\n\t        streamlit run tests/app_to_test.py &\n\t    - name: Test with pytest\n\t      run: |\n\t        pytest\nview raw\nrun_tests_each_PR.yml hosted with ‚ù§ by GitHub\n\nBy having this workflow defined in your repo, and required status checks enabled on GitHub, every pull request will now have the following status check appended to the bottom, letting you know the status of your changes:\n\nWriting tests saves work in the long run\n\nHaving tests in your codebase has numerous benefits. As explained above, automating visual regression tests allows you to maintain an app without having to have a human in the loop looking for changes. Writing tests is also a great signal to potential users that you care about stability and long-term maintainability of your projects. It‚Äôs not only easy to write tests for a Streamlit app and have them automatically run on each GitHub commit, but that the extra work of adding tests to your Streamlit project will save you time in the long run. ¬†\n\nHave questions about this post or Streamlit in general? Stop by the Streamlit Community forum, start a discussion, meet other Streamlit enthusiasts, find a collaborator in the Streamlit Component tracker or share your Streamlit project! There are plenty of ways to get involved in the Streamlit community and we look forward to welcoming you üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Calculating distances in cosmology with Streamlit",
    "url": "https://blog.streamlit.io/calculating-distances-in-cosmology-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nBy Nikolina Sarcevic, Matthijs van der Wild and Marco Bonici\nPosted in Advocate Posts, February 17 2022\nWhat is CosmŒ©racle?\nHow it all started\nHow we worked together in a virtual setting\nWhere to go from here?\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nEver looked at the night sky and wondered, \"Hey, how far away is that star?\" We wondered about it too, and we wanted an app that could calculate this distance in one click. So we made CosmŒ©racle!\n\nIn this post, you‚Äôll learn:\n\nWhat is CosmŒ©racle?\nHow it all started\nHow we worked together in a virtual setting\nWhere to go from here?\n\nWant to jump right in? Here's the app and the source code.\n\nBut first, let‚Äôs talk about...\n\nWhat is CosmŒ©racle?\n\nCosmŒ©racle is an app that calculates distances for different sets of cosmological parameters. Go to www.cosmoracle.com and click on Cosmological Distances. You‚Äôll see a menu in the sidebar with fields for redshift, the Hubble constant, the energy content in the Universe from matter, radiation, dark energy, and different equation of state parameters.\n\nSome sets of parameters are more popular than others (they‚Äôre favored by different observational datasets), so we have pre-programmed some values for you. The only thing you need to get started is the value of the redshift. Want to know a certain distance of those galaxies at redshift 2? Simply plug that value into the redshift field, and CosmŒ©racle will get cracking!\n\nIt‚Äôs important in science that everyone agrees on what the input parameters represent, and CosmŒ©racle is no exception. As you can see above, we‚Äôve included a Definitions page where you can see how each parameter is defined and how CosmŒ©racle uses these parameters to compute quantities that are used in cosmology.\n\nHow it all started\n\nThere are plenty of ‚Äúsomethings‚Äù in the field of physics. If you‚Äôre working in cosmology and/or astronomy, at some point you‚Äôll need to use one of the many ‚Äúdistances.‚Äù The need for many different types of distances is due to the Universe being very complicated‚Äîspace expanding, objects moving, interacting, and mixing, things getting shifted and distorted. Depending on the problem you‚Äôre working on, you‚Äôll need a certain distance calculated.\n\nOn top of this, these distances will depend on the kind of Universe you‚Äôre working with and on how much matter it contains. It probably sounds weird at first, but give it some thought. The amount of matter affects the expansion rate of the Universe, so different quantities lead to different increases in distance.\n\nOther things that affect distances are the curvature of the Universe and dark matter or dark energy. If you have all the necessary ingredients, it‚Äôs possible to calculate these distances. Most of the time, you just ‚Äúwant the thing calculated‚Äù without going through the process of installing Python libraries and writing the code just to get that number.\n\nUntil recently, we calculated these distances with the first online version of the famous Ned Wright‚Äôs Cosmo Calculator from 1999 or with the astropy cosmology package. Although these are great tools, we felt that there was a need for an improved and modern version.\n\nThe three of us met online and decided that our app needs to:\n\nbe user-friendly (everyone should be able to use it without spending a lot of time on learning it)\nwork across all devices: desktop, tablet, and mobile (a la ‚Äúastronomy on the go‚Äù)\nbe fast and accessible\nbe correct in terms of physics\nbe designed in a modern way\nrepresent the results in a numerical and illustrative way (think plots)\nhave an option to download the calculated data (to be used for further analysis)\nbe designed in a way that‚Äôs easy to maintain and upgrade\nbe open-source\n\nHow to achieve it?\n\nAll three of us have a programming background. We work in physics and love exploring the latest advancements in the field of data visualization. Sure, we could ‚Äúcode up‚Äù functions to calculate all sorts of things. But we wanted a simple and neat way of putting apps online. Every option we found needed knowledge beyond our skillset or wasn't open-source.\n\nIn the spring of 2021, Niko‚Äôs best friend Robert (also an astrophysicist) shared ‚Äúa super cool new way of making apps online called Streamlitüéà.‚Äù While transitioning from research into data science, Robert made an incredible app called ‚ÄúDistribution Analyser‚Äù. It analyses all scipy.stats functions. You can read more about it in this Towards Data science article. When the time came to test the best way to combine Python code and make it into an app, Streamlit was one of our first choices.\n\nHow we worked together in a virtual setting\n\nSince we‚Äôre all researchers at different institutions, it was hard for us to find the time to work together. We dedicated a few days during the holidays to exclusively work on CosmŒ©racle. After we decided on the functions to start with, we distributed the tasks, finalized the first Python code, and discussed how to deploy it.\n\nCosmŒ©racle calculates the time that passed between a distant object emitting a photon and that same photon reaching Earth. Functions like this form the backbone of CosmŒ©racle but need to be deployed in order to be useful.\n\ndef get_lookback_time(z, H0=constants['Hubble0'], Œ©M=constants['matter-density'],\n                      Œ©DE=constants['DE-density'], Œ©R=constants['rad-density'],\n                      w0=constants['w0'], wa=constants['wa']):\n    \"\"\"\n    Method to compute the lookback time in Gyrs\n    \"\"\"\n    integrand = lambda x: 1/(get_E_z(x, Œ©M, Œ©DE, Œ©R, w0, wa)*(1+x))\n    if isinstance(z, float) or isinstance(z, int):\n        if z < 0:\n            raise ValueError(\"Enter a non-negative redshift.\")\n        result, _ = integrate.quad(integrand, 0, z)\n    elif isinstance(z, np.ndarray):\n        if any(t < 0 for t in z):\n            raise ValueError(\"Enter a non-negative redshift.\")\n        result = np.vectorize(lambda x: integrate.quad(integrand, 0, x)[0])(z)\n    else:\n        raise TypeError(f'Expected \"Union[float, np.ndarray]\", got {type(z)}')\n    c0 = constants['speed-of-light']\n    return result*hubble_time(H0)\n\n\nThat first day we worked until midnight. Too tired to keep going, we decided to talk later. But Niko stayed up and tried Streamlit. It worked! She immediately wrote to Matthijs and Marco, ‚ÄúWe need to meet. This is wonderful. This is like magic.\" So we met on Zoom and continued working into the morning. In a week, we had the first working version of CosmŒ©racle!\n\nThe unofficial fourth member of the gang: coffee. It supported every one of us during the small hours of the morning. Photo by Craig McKay on Unsplash\n\nWhat worked well is the combination of several important factors:\n\nWe had a clear goal in mind\nWe had a lot of enthusiasm\nWe were having fun\n\nAlso, the crucial part was that our communication was wonderful. We‚Äôre friends in real life. That makes everything easier.\n\nLast but definitely not least, the Streamlit platform was essential. It‚Äôs very intuitive, powerful, customizable, and very easy to use. As scientists, we‚Äôre used to computations but have no experience in deploying web apps. Without Streamlit, we wouldn't have been able to build the momentum to finish this project.\n\nWe also used:\n\nSlack for creating tasks/private discussions or sending messages.\nGitHub for version control, keeping track of issues that have to be dealt with, and for hosting a GitHub Page that links to the page hosted on Streamlit.\nZoom for in-person discussions.\nPython for coding.\nWhere to go from here?\n\nThanks to Streamlit, the code implementation was very easy. We have organized the code into blocks dedicated to configuration, computation, or presentation. The result is that CosmŒ©racle is structured in a way that makes it easy to create extensions.\n\nFor example, here is the script that‚Äôs at the center of the Streamlit app:\n\nimport streamlit as st\n\nimport page_introduction as pi\nimport page_distances as pd\nimport page_documentation as doc\n\nst.sidebar.write(\" \")\n\npages = {\n        \"Introduction\": pi,\n        \"Cosmological Distances\": pd,\n        \"Definitions\": doc,\n    }\n\nst.sidebar.title(\"Main options\")\n\n# Radio buttons to select desired option\npage = st.sidebar.radio(\"\", tuple(pages.keys()))\n\npages[page].show_page()\n\n\nThe parts of CosmŒ©racle that do all the work (Introduction, Cosmological Distances, and Definitions) are stored in separate scripts. This makes CosmŒ©racle modular and extensible.\n\nWe plan to build a new feature that will let CosmŒ©racle compute the growth of matter density fluctuations in inflationary cosmology. After we finish the script, adding this new functionality to the app will be a simple matter of adding a line like import page_perturbations to the code above. This will make the corresponding adjustments to the list of pages and will be available on the website.\n\nWe also plan to extend the existing features of CosmŒ©racle.\n\nFor example, the current pre-set parameters are based on the latest analysis of the data of the Planck satellite. They‚Äôre hardcoded into the app in page_distances.py above. People from different parts of astronomy and cosmology will have different preferences for CosmŒ©racle‚Äôs parameters. We‚Äôll be adding more sets (based on WMAP or the astronomical 70-30-70 cosmology).\n\nHere is a sneak peek for you:\n\nWe want to give CosmŒ©racle more functionality for quantities that are directly calculated from the cosmological distance measures. Currently, it can calculate angular sizes and convert between these angular sizes and physical sizes. But it can only give you an indication of the physical size of an object for a fixed angular extension of 1‚Äô‚Äô (1 arcsecond or 1/3600th of a degree). We plan to add an option for you to set the angular size yourself. So if you want to know the real size of those 0.3‚Äô‚Äô galaxy lobes, stay tuned!\n\nWrapping up\n\nWith the increasing amount of knowledge and data, we need to develop ways of communicating findings and creating better and more accessible ways to disseminate knowledge. Accessibility and open-source are key to achieving it.\n\nWe‚Äôre very impressed with how easy it is to use Streamlit. And it‚Äôs open-source! For us, to have the code open to the public is extremely valuable because the community can inspect our work, point out our mistakes, and give suggestions for improvements.\n\nWe believe that science belongs to everyone. If our app helps the astronomy community with research and teaching, then we‚Äôve achieved a great deal. And all of this was possible because Streamlit is so easy to use.\n\nWe hope you enjoyed our story. If you have any questions, please leave them in the comments below or reach out to Niko at nikolina.sarcevic@gmail.com, Matthijs at primarius@gmail.com, and Marco at bonici.marco@gmail.com.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to create interactive books with Streamlit in 5 steps",
    "url": "https://blog.streamlit.io/how-to-create-interactive-books-with-streamlit-and-streamlit-book-in-5-steps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to create interactive books with Streamlit in 5 steps\n\nUse streamlit_book library to create interactive books and presentations\n\nBy Sebastian Flores Benner\nPosted in Advocate Posts, January 20 2022\nInstall the library\nCreate the main file\nCreate content in Markdown files\nCreate content in a Python file\nLet's make it fun!\nShare your app!\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nI love Streamlit. I moved most of my content, websites, and code to Streamlit to make it more interactive. Recently, I presented at PyCon Chile using Streamlit as a PowerPoint substitute. But as my content grew, handling different \"pages\" got complicated. I had to copy-paste the page handling code from one repository to another. I wanted a simple solution that any person, especially teachers, could use to create an interactive app for teaching or self-learning. Let people focus on the content and the technology takes care of the rest!\n\nAfter some thought, I realized that the best way to solve this was to create a companion library for Streamlit. I called it streamlit_book. I coded it to take all of the bookkeeping (pun intended!). I even defined some Markdown formats and Python functions so you can do quizzes more easily: true/false questions, multiple-choice, single-choice, and others. You can put your content on plain Markdown, or you take advantage of the interactivity provided by Streamlit and Python, it's up to you!\n\nIn this post, I'll show you how to use the library to create interactive books or presentations with Streamlit:\n\nInstall the library\nCreate the main file\nCreate content in Markdown files\nCreate content in Python file\nShare your app!\n\nYou can take a sneak peek at the app here and see the docs here.\n\nInstall the library\n\nLet's build a short tutorial called \"Happy Birds\". It'll teach you how to win at a game involving flying birds, pigs, and trajectories. As usual, store all the required libraries in your app in a file requirements.txt:\n\nstreamlit\nstreamlit_book\nmatplotlib\nnumpy\n\n\nInstall these libraries using pip (or use a different virtual environment):\n\npip install -r requirements.txt\n\n\nCreate the main file\n\nCreate a file happy_birds.py and define the properties:\n\nimport streamlit as st\nimport streamlit_book as stb\n\n# Streamlit webpage properties\nst.set_page_config(page_title=\"HappyBirds\", page_icon=\"üê¶\")\n\n# Streamlit book properties\nstb.set_book_config(path=\"HappyBirds\")\n\nLines 1 and 2 are regular library imports.\nLine 5 sets up the Streamlit app. You can use all the regular Streamlit magic here: set up the layout, the page title (on your browser's tab), and even a small icon.\nLine 8 sets up Streamlit Book by indicating a folder where it should look for content files. Make sure to create the folder HappyBirds.\n\nIf you run the file happy_birds, it'll show a warning message of having no content files:\n\nstreamlit run happy_birds.py\n\n\nCreate content in Markdown files\n\nCreate content by adding files into the folder HappyBirds. Notice that Streamlit Book will sort the files using lexicographic (alphabetic) order, so it can be helpful to put numbers before the names to have the desired ordering.\n\nFirst, create \"00 Cover.md.\" This file will be a cover image for the book, with a big image and some funny text to engage readers:\n\n# Happy Birds\n\nThis web app illustrates the use of [streamlit_book](<https://streamlit-book.readthedocs.io/en/latest/>) for teaching and learning. In this particular web app, we will explain motion trajectories.\n\n<img src=\"<https://github.com/sebastiandres/streamlit_happy_birds/blob/main/images/happybird.png?raw=true>\" alt=\"happy Birds\" width=\"700\">\n\nHappy Birds uses streamlit, streamlit book, numpy and matplotlib libraries.\n\n\nNotice that you need to insert the URL of the image, not the local path.\n\nNext, create \"01 The Theory.md\". This is the file with the (hopefully not boring) explanations of how projectile motion works. Notice how we put a quiz at the end!\n\n# Projectile Motion\n\n## The question\n\nConsidering no air resistance, what is the trajectory followed by a projectile thrown with initial velocity $v_0$ at an angle $\\\\theta$?\n\n<img src=\"<https://github.com/sebastiandres/streamlit_happy_birds/blob/main/images/definition.png?raw=true>\" alt=\"Parameter Definition\" width=\"700\">\n\n## The short answer\n\nThe trajectory followed by a projectile thrown with initial velocity $v_0$ at an angle $\\\\theta$, without air resistance, is:\n\n$$\nx(t) = v_0 \\\\cos(\\\\theta)t \\\\\\\\\\\\\\\\\ny(t) = v_0 \\\\sin(\\\\theta)t - \\\\frac{1}{2} g t^{2}\n$$\n\nwhere $x$ and $y$ are the horizontal and vertical directions, and $g$ is the acceleration due to gravity.\n\n## The long answer\n\nTo obtain the trajectory we start with the equations for the acceleration as given by Newton's Laws:\n\n$$\nm \\\\frac{d^{2}}{dt^{2}} x = 0 \\\\\\\\\\\\\\\\\nm \\\\frac{d^{2}}{dt^{2}} y = - mg\n$$\n\nInitial condition for the position: $x(t=0)=0$ and $y(t=0)=0$.\n\nInitial condition for the velocity: $v_x(t=0) = \\\\cos(\\\\theta)$ and $v_y(t=0) = \\\\sin(\\\\theta)$.\n\nAfter simplifying for the mass $m$, we can solve by integrating and considering the conditions for velocity: \n\n$$\n\\\\frac{d}{dt}x = v_x(t)= v_0 \\\\cos(\\\\theta) \\\\\\\\\\\\\\\\\n\\\\frac{d}{dt}y = v_y(t)= v_0 \\\\sin(\\\\theta) - g t\n$$\n\nIntegrating again and considering the initial conditions for $x$ and $y$, we obtain:\n\n$$\nx(t) = v_0 \\\\cos(\\\\theta)t \\\\\\\\\\\\\\\\\ny(t) = v_0 \\\\sin(\\\\theta)t - \\\\frac{1}{2} g t^{2}\n$$\n\n## Quiz time\n\nFollowing the equation above, answer the following question:\n\nstb.single_choice\nWhat is the trajectory of a projectile without considering air resistance?\n- A straight line\n+ A parabola\n- A circle\n- A hyperbola\n\n\nThe Streamlit + Streamlit Book app will update automatically, and you'll be able to navigate the created pages.\n\nCreate content in a Python file\n\nNow, let's add an interactive page for people so people can try different parameters. You can even use questions from streamlit_book! Call this file \"02 The practice.py\":\n\nimport streamlit as st\nimport streamlit_book as stb\nimport numpy as np\n\nfrom code.trajectory import get_trajectory, fig_from_list\n\nif \"trayectory_list\" not in st.session_state:\n    st.session_state[\"trayectory_list\"] = []\n\n# Title\nst.title(\"Trajectory of a projectile\")\nst.subheader(\"Equations of motion of a projectile\")\nst.latex(\"x(t) = v_0 \\\\\\\\cos(\\\\\\\\theta)t\")\nst.latex(\"y(t) = v_0 \\\\\\\\sin(\\\\\\\\theta)t - \\\\\\\\frac{1}{2} g t^{2}\")\n\n# Parameters\nst.subheader(\"Simulation parameters\")\nc1, c2, c3 = st.columns(3)\ndv0 = 1\nv0 = c1.slider(\"Initial Velocity [meters/second]\", \n                        min_value=dv0, max_value=100*dv0, \n                        value=10*dv0, step=dv0, help=\"Initial velocity for the projectile\")\ndtheta = 1\ntheta_deg = c2.slider(\"Initial Angle [degrees]\", \n                        min_value=5, max_value=90, \n                        value=45, step=5, help=\"Initial velocity for the projectile\")\n# options for gravity: earth, moon, mars, jupiter\ngravity_dict = {'Earth': 9.8, 'Moon': 1.6, 'Mars': 3.7, 'Jupiter': 24.8}\ngravity_label = c3.selectbox(\"Gravity\", gravity_dict.keys(), index=0)\ngravity = gravity_dict[gravity_label]\n\n# Compute the plot\nc1, c2 = st.columns([.5, .1])\nif c1.button(\"Add plot\"):\n    traj_dict = get_trajectory(v0, theta_deg, gravity, gravity_label)\n    st.session_state[\"trayectory_list\"].append(traj_dict)\n\nif c2.button(\"Clear plots\"):\n    st.session_state[\"trayectory_list\"] = []\n\nif len(st.session_state[\"trayectory_list\"]) > 0:\n    fig = fig_from_list(st.session_state[\"trayectory_list\"])\n    st.pyplot(fig)\n\n# The quizz\nst.subheader(\"Quizz time!\")\n\nstb.single_choice(\"At what angle is obtained the maximal distance?\",\n                options=[\"15\", \"30\", \"45\", \"60\", \"75\"], answer_index=2)\n\nstb.true_or_false(\"On the moon, the horizontal distance is always larger than on the earth under the same initial velocity and angle.\",\n                    answer=True)\n\n\nThis makes use of some helper functions in code/trajectories.py. Everything is on the GitHub repo.\n\nLet's make it fun!\n\nWe can even make a game out of it, to further test people's understanding of the motion equations.\n\nLet's create a \"03 The game.py\" file with the content.\n\nimport streamlit as st\nimport numpy as np\n\nfrom code.trajectory import get_trajectory, fig_from_list, check_solution\n\n# Fill up the page\nc1, c2 = st.columns([8,1])\nc1.title(\"The Game\")\nrestart = c2.button(\"Restart\")\n\n# Gravity constants by planet\nGRAVITY_DICT = {'Earth': 9.8, 'Moon': 1.6, 'Mars': 3.7, 'Jupiter': 24.8}\n\n# Setup the session_state variables\nif restart or \"remaining_guesses\" not in st.session_state:\n    st.session_state[\"remaining_guesses\"] = 3\n\nif restart or\"guess_list\" not in st.session_state:\n    st.session_state[\"guess_list\"] = []\n\nif restart or\"game_gravity_index\" not in st.session_state:\n    st.session_state[\"game_gravity_index\"] = np.random.randint(0, len(GRAVITY_DICT))\nplanet_list = list(GRAVITY_DICT.keys())\ngame_planet = planet_list[st.session_state[\"game_gravity_index\"]]\ngame_gravity = GRAVITY_DICT[game_planet]\n\nif restart or \"solution\" not in st.session_state:\n    v0_sol = np.random.randint(30, 60)\n    theta_deg_sol = 45\n    theta_rad_sol = theta_deg_sol * np.pi / 180\n    t_max_sol = 2*v0_sol*np.sin(theta_rad_sol)/game_gravity\n    x_max_sol = v0_sol*np.cos(theta_rad_sol)*t_max_sol\n    pig_position = [x_max_sol, 0]\n    st.session_state[\"solution\"] = {\n                                    \"pig_position\":pig_position, \n                                    \"v0_sol\": v0_sol, \n                                    \"theta_deg_sol\": theta_deg_sol,\n                                    }\n\narticle_dict = {'Earth': \"\", 'Moon': \"the\", 'Mars': \"\", 'Jupiter': \"\"}\nc1.subheader(f\"Can you hit the target on {article_dict[game_planet]} {game_planet}?\")\n\n# Pig position\nx_text = f\"x = {st.session_state.solution['pig_position'][0]:.3f} meters\"\ny_text = f\"y = {st.session_state.solution['pig_position'][1]:.3f} meters\"\nst.write(f\"The target is at **{x_text}** and **{y_text}**\")\n# Get the parameters\nst.subheader(\"Enter the parameters\")\nc1, c2, c3, c4 = st.columns([3,3,3,1])\ndv0 = 1\nv0 = c1.slider(\"Initial Velocity [meters/second]\", \n                        min_value=dv0, max_value=100*dv0, \n                        value=50, step=dv0, help=\"Initial velocity for the projectile\")\ndtheta = 1\ntheta_deg = c2.slider(\"Initial Angle [degrees]\", \n                        min_value=5, max_value=90, \n                        value=30, step=5, help=\"Initial velocity for the projectile\")\n# options for gravity: earth, moon, mars, jupiter\nc3.metric(value=game_gravity, label=f\"{game_planet}'s gravity in m/s^2\")\n\n# Shoooooot\nif st.session_state[\"remaining_guesses\"] > 0:\n    if c4.button(\"Shoot!\"):\n        st.session_state[\"remaining_guesses\"] -= 1\n        traj_dict = get_trajectory(v0, theta_deg, game_gravity, game_planet)\n        st.session_state[\"guess_list\"].append(traj_dict)\n\n# Placeholder for information\nplaceholder = st.empty()\n\n# Always plot, to show the target\nfig = fig_from_list(st.session_state[\"guess_list\"], st.session_state.solution[\"pig_position\"])\nst.pyplot(fig)\n\n# We check if we hit the pig after the shoot we have guesses left\nif check_solution(st.session_state.solution[\"pig_position\"], st.session_state[\"guess_list\"]):\n    placeholder.success(\"You hit the pig... I mean, the target!\")\nelif st.session_state[\"remaining_guesses\"] == 0:\n    line1 = \"You're out of guesses! :(\"\n    v0_sol = st.session_state.solution[\"v0_sol\"]\n    theta_deg_sol = st.session_state.solution[\"theta_deg_sol\"]\n    line2 = f\"One possible solution was $v_0$={v0_sol} [m/s^2] and $\\\\\\\\theta$={theta_deg_sol} [deg]\"\n    placeholder.error(line1 + line2)\nelse:\n    # Say to keep trying, but only if at least tried once\n    if st.session_state['remaining_guesses']==2:\n        text = f\"Keep trying! You have {st.session_state['remaining_guesses']} guesses remaining. Have you tried solving the equations?\"\n        placeholder.warning(text)\n    if st.session_state['remaining_guesses']==1:\n        text = f\"Use carefully the last guess!\"\n        placeholder.warning(text)\n\n\nShare your app!\n\nFinally, you can share your app with the world (and your students!). It's as easy as sharing any Streamlit app, because streamlit_book is just another required library.\n\nWrapping up\n\nI had a lot of fun creating the streamlit_book library. I hope you'll use it to create awesome books, courses, or presentations, and extend the ideas we started on the happy birds app. I'll keep updating the library and adding new features. You can check the documentation here and the source code here. If you create an app or want a new feature, reach out to me (in Spanish, English, or French)! Find me as @sebastiandres on Twitter and GitHub, or comment below!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > October 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-october-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > October 2021\n\nYour October look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, November 8 2021\nüèÜ App of the month üèÜ\nStreamlit October updates\nüîç Current release: 1.1.0\nüîÆ Upcoming\nüéà Streamlit 1.0 release\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur October featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nStreamlit-geospatial by Qiusheng Wu!\n\nThis multi-page web app demonstrates various interactive geospatial applications created with open-source mapping libraries such as leafmap, geemap, pydeck, and kepler.gl. Create timelapses, upload vector data, search basemaps, and more! This is an open-source project. [code].\n\nStreamlit October updates\n\nLet's take a look back at all that happened during the month of October!\n\nüîç Current release: 1.1.0\n\nThe latest release is 1.1.0. Recent updates include memory improvements and semantic versioning. See the memory usage over time of one of our internal apps below. The lines on the left are using older Streamlit versions and the line on the right is using 1.1.0!\n\nBe sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming\n\nSome new upcoming features and updates to get excited about:\n\nWebcam input\nBetter \"Argh\" error page\nFavorite an app\n\nCheck out our roadmap app to see what else lies ahead!\n\nüéà Streamlit 1.0 release\n\nStreamlit 1.0 was officially released. ü•≥ Thank you community for all the feedback and pull requests which made reaching this milestone possible!\n\n\n\nFeatured Streamlit content\n\n\nWe launched a brand-new docs site! Easily navigate between API references (now also more discoverable in searches), Cloud deployment, tutorials, and other resources.\n\nIn Detecting parking spots with Streamlit, Jeffrey described how to build an app that can detect parking spaces from a livestream with OpenCV and Mask R-CNN.\n\n\nFeatured community content\n\nSome great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\n\nZakaria's Clean Speech Audio with IBM Watson STT and Streamlit App lets you clean up your Audio recordings by removing unwanted spaces and words\nQiusheng incorporated leafmap apps to his open-source project\nIn his video tutorial, PartTimeLarry showed how to use Streamlit to build a simple UI on top of OpenSea NFT API\nAkshanh made a Streamlit Playground app where you can learn, experiment, and play with Streamlit components on the fly\nIn the video Quantum Tic Tac Toe with IBM Qiskit and Streamlit, Jay showed how to build a Quantum version of the classic Tic Tac Toe game\nLoren created Dashboard Evolution Covid19 en France to show different views of the evolution of the Covid-19 in France\nJos√©'s STL files from molecules app returns molecular models in STL format for 3D printing\nSean created xPlot, a data explorer to plot data from files for analysis\nFind rankings, result types, and ranking URLs for a bulk list of keywords in Michael's Keyword rankings app\nAditya made a Triple Combo Plotter app to plot LAS file data\nYou can quickly clean up your keywords lists with Alexis' SEO Keywords Spelling Checker\n\nThanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "3 steps to fix app memory leaks",
    "url": "https://blog.streamlit.io/3-steps-to-fix-app-memory-leaks/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n3 steps to fix app memory leaks\n\nHow to detect if your Streamlit app leaks memory and identify faulty code\n\nBy George Merticariu\nPosted in Tutorials, April 14 2022\n1. Identify the memory leak\n2. Identify leaking objects\n3. Identify the code that's allocating the leaking objects\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nDoes your Streamlit app crash after long use or constantly runs out of memory? Chances are, it has a memory leak.\n\nIn this post, you‚Äôll learn how to find and fix memory leaks in three simple steps:\n\nIdentify the memory leak\nIdentify the leaking objects\nIdentify the code that's allocating the leaking objects\n\nNOTE: Streamlit is used in a variety of settings, from short-lived research projects to live company dashboards. This post is primarily aimed at developers deploying Streamlit apps with very long uptimes. We at Streamlit strive to keep the Streamlit framework itself free of leaks. Developers with long-running apps also need to be mindful of memory usage over time. Even a tiny memory leak can compound. Eventually, even the beefiest machine will fall over if memory usage isn‚Äôt kept in check.\n\nLet‚Äôs dive right in.\n\n1. Identify the memory leak\n\nA leak happens when your app acquires memory resources and never releases them. It just consumes more and more memory. Since it's a finite resource, eventually it gets exhausted, and the OS terminates your application.\n\nAs an example, we'll use a Streamlit library memory leak that we investigated and patched as part of this pull request. After we found the faulty code and released a fix, apps started using one-tenth to one-twentieth of the memory used before!\n\nTo find out if your app leaks memory, use a memory profiler (we used mprof ):\n\n# make sure mprof is installed\npip install memory_profiler\n\n# find the pid of the Streamlit app (the next number after the user)\nps aux | grep \"streamlit run\" | grep -v grep\n\n# start profiling the memory of the app\nmprof run --attach <pid>\n\n\nOnce the profiler is attached, use your app to simulate a higher load (multiple sessions, more complex operations, etc.). After a few minutes, plot a memory graph:\n\nmprof plot\n\n# if the above gives you an error related to GUI you can try fixing it with\npip install PyQt5\n\n\nIf the memory usage doesn't plateau, your app is leaking memory:\n\n2. Identify leaking objects\n\nNow identify which objects are allocated and never released. Use the tracemalloc Python library. Take snapshots between executions (after forcing the garbage collection).\n\nIf an object persists in a snapshot, it means it's never collected:\n\nimport tracemalloc, json\nimport streamlit as st\nimport gc\n\n@st.experimental_singleton\ndef init_tracking_object():\n  tracemalloc.start(10)\n\n  return {\n    \"runs\": 0,\n    \"tracebacks\": {}\n  }\n\n\n_TRACES = init_tracking_object()\n\ndef traceback_exclude_filter(patterns, tracebackList):\n    \"\"\"\n    Returns False if any provided pattern exists in the filename of the traceback,\n    Returns True otherwise.\n    \"\"\"\n    for t in tracebackList:\n        for p in patterns:\n            if p in t.filename:\n                return False\n        return True\n\n\ndef traceback_include_filter(patterns, tracebackList):\n    \"\"\"\n    Returns True if any provided pattern exists in the filename of the traceback,\n    Returns False otherwise.\n    \"\"\"\n    for t in tracebackList:\n        for p in patterns:\n            if p in t.filename:\n                return True\n    return False\n\n\ndef check_for_leaks(diff):\n    \"\"\"\n    Checks if the same traceback appears consistently after multiple runs.\n\n    diff - The object returned by tracemalloc#snapshot.compare_to\n    \"\"\"\n    _TRACES[\"runs\"] = _TRACES[\"runs\"] + 1\n    tracebacks = set()\n\n    for sd in diff:\n        for t in sd.traceback:\n            tracebacks.add(t)\n\n    if \"tracebacks\" not in _TRACES or len(_TRACES[\"tracebacks\"]) == 0:\n        for t in tracebacks:\n            _TRACES[\"tracebacks\"][t] = 1\n    else:\n        oldTracebacks = _TRACES[\"tracebacks\"].keys()\n        intersection = tracebacks.intersection(oldTracebacks)\n        evictions = set()\n        for t in _TRACES[\"tracebacks\"]:\n            if t not in intersection:\n                evictions.add(t)\n            else:\n                _TRACES[\"tracebacks\"][t] = _TRACES[\"tracebacks\"][t] + 1\n\n        for t in evictions:\n            del _TRACES[\"tracebacks\"][t]\n\n    if _TRACES[\"runs\"] > 1:\n        st.write(f'After {_TRACES[\"runs\"]} runs the following traces were collected.')\n        prettyPrint = {}\n        for t in _TRACES[\"tracebacks\"]:\n            prettyPrint[str(t)] = _TRACES[\"tracebacks\"][t]\n        st.write(json.dumps(prettyPrint, sort_keys=True, indent=4))\n\n\ndef compare_snapshots():\n    \"\"\"\n    Compares two consecutive snapshots and tracks if the same traceback can be found\n    in the diff. If a traceback consistently appears during runs, it's a good indicator\n    for a memory leak.\n    \"\"\"\n    snapshot = tracemalloc.take_snapshot()\n    if \"snapshot\" in _TRACES:\n        diff = snapshot.compare_to(_TRACES[\"snapshot\"], \"lineno\")\n        diff = [d for d in diff if\n                d.count_diff > 0 and traceback_exclude_filter([\"tornado\"], d.traceback)\n                and traceback_include_filter([\"streamlit\"], d.traceback)\n                ]\n        check_for_leaks(diff)\n\n    _TRACES[\"snapshot\"] = snapshot\n\n\ngc.collect()\ncompare_snapshots()\n\n\nNOTE: Call compare_snapshots() always after the gc collection is forced, to make sure you track only the objects for which memory can‚Äôt be reclaimed.\n\nRun the above in the Streamlit app, version lower than 1.0, and you'll get the following output:\n\n\n    \"<attrs generated init streamlit.state.session_state.SessionState>:17\": 22,\n\n\nSessionState is the object that leaked.\n\nNow let's identify the part of the code that allocates the SessionState and never releases it.\n\n3. Identify the code that's allocating the leaking objects\n\nTrack which object is not releasing the memory using the objgraph library. To install it, run pip install objgraph. Track the holder of the SessionState after the gc collection is forced:\n\nimport gc\nimport objgraph\n\nfor o in gc.get_objects():\n    if 'session_state.SessionState' in str(type(o)) and o is not st.session_state:\n        filename = f'/tmp/session_state_{hex(id(o))}.png'\n        objgraph.show_chain(\n            objgraph.find_backref_chain(\n                 o,\n                 objgraph.is_proper_module),\n            backrefs=False,\n            filename=filename)\n\n        st.write(\"SessionState reference retained by: \", type(o))\n        st.image(filename)\n\n\nIn our case, when we run on Streamlit versions below 1.0.0, you can see that the SessionState object is held by the Signal class:\n\nThe allocation of SessionState happens in the streamlit.config module. The object holding the resource is Signal. It holds the resources that need to be released (check out our pull request).\n\nAfter we applied the fix and updated the app, the memory usage plateaued:\n\nWrapping up\n\nNow you know how to detect if your Streamlit app is leaking memory and how to fix it! If you have any questions, please let us know in the comments below or on the forum. We'd be happy to help! ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Counties_v2.gif (1916√ó945)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Counties_v2.gif#browser",
    "html": ""
  },
  {
    "title": "Streamlit-testapp--1-.gif (3446√ó2038)",
    "url": "https://blog.streamlit.io/content/images/2022/11/Streamlit-testapp--1-.gif#browser",
    "html": ""
  },
  {
    "title": "extras-1.gif (800√ó668)",
    "url": "https://blog.streamlit.io/content/images/2022/11/extras-1.gif",
    "html": ""
  },
  {
    "title": "multipageapps--3--1.gif (1034√ó552)",
    "url": "https://blog.streamlit.io/content/images/2022/06/multipageapps--3--1.gif#browser",
    "html": ""
  },
  {
    "title": "Roadmap-hero.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2022/11/Roadmap-hero.png",
    "html": ""
  },
  {
    "title": "Creating satellite timelapse with Streamlit and Earth Engine",
    "url": "https://blog.streamlit.io/creating-satellite-timelapse-with-streamlit-and-earth-engine/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCreating satellite timelapse with Streamlit and Earth Engine\n\nHow to create a satellite timelapse for any location around the globe in 60 seconds\n\nBy Qiusheng Wu\nPosted in Advocate Posts, December 15 2021\nHow to create a satellite timelapse without coding\n1. Draw a Region of Interest (ROI) on the map\n2. Upload a GeoJSON file to the web app\n3. Select a satellite image collection\n4. Select a band combination\n5. Select an administrative boundary\n6. Customize timelapse parameters\n7. Download your timelapse in GIF and MP4 formats\nHow to deploy an Earth Engine app to Streamlit Cloud\n1. Fork the streamlit-geospatial repo\n2. Sign up for an Earth Engine account\n3. Install GeoPandas, geemap, and Streamlit\n4. Get an Earth Engine token\n5. Deploy your app to Streamlit Cloud\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nWant to create a satellite timelapse for any location around the globe to see how the Earth has changed? You've come to the right place. I present to you...\n\nAn interactive web app for creating satellite timelapse without coding!\n\nIn this post, you'll learn how to create a satellite timelapse for any location and to deploy an Earth Engine app to Streamlit Cloud:\n\nHow to create a satellite timelapse without coding:\n\nDraw a Region of Interest (ROI) on the map\nUpload a GeoJSON file to the web app\nSelect a satellite image collection\nSelect a band combination\nSelect an administrative boundary\nCustomize timelapse parameters\nDownload your timelapse in GIF and MP4 formats\n\nHow to deploy an Earth Engine app to Streamlit Cloud:\n\nFork the streamlit-geospatial repo\nSign up for an Earth Engine account\nInstall GeoPandas, geemap, and Streamlit\nGet an Earth Engine token\nDeploy your app to Streamlit Cloud\n\nWant to jump right in? Here's the web app and repo code.\n\nLet's get started!\n\nHow to create a satellite timelapse without coding\n1. Draw a Region of Interest (ROI) on the map\n\nFirst, navigate to the web app at https://streamlit.gishub.org. Once the app opens in the web browser, click \"Create Timelapse\" on the left sidebar menu. You should see a map on the left and a list of options on the right. Pan and zoom the map to find your Region of Interest (ROI).\n\nNext, click the rectangle tool to draw a rectangle on the map:\n\nDraw a Region of Interest (ROI) on the map and export the ROI as a GeoJSON\n\nDue to the limitation of the folium Python package, there is no way to retrieve the coordinates of drawn shapes on the map. So you'll need to export the ROI manually and upload it back to the web app for subsequent steps.\n\nClick the \"Export\" button in the upper-right corner of the map to export the ROI as a GeoJSON file to your computer. The GeoJSON file is a plain text file containing the coordinates of the geometries drawn on the map. Use any Text Editor to open and inspect the file.\n\n2. Upload a GeoJSON file to the web app\n\nClick \"Browser files\" to locate the GeoJSON file exported in the previous step and upload it to the web app. Or you can use other existing GeoJSON. Once you click the \"Open\" button on the open file dialog, the selected GeoJSON file should be uploaded to the app momentarily with the file name listed under \"Browsers files\":\n\nUpload the exported GeoJSON back to the web app\n3. Select a satellite image collection\n\nSelect \"Landsat TM-ETM-OLI\" from the dropdown list:\n\nSelect a satellite image collection to create timelapse\n\nI chose Landsat program because it's the longest-running enterprise for acquiring satellite imagery of the Earth since 1972. The most recent satellite, Landsat 9, was successfully launched on 27 September 2021. The \"Landsat TM-ETM-OLI\" image collection contains all available Landsat imagery acquired since 1984.\n\nIf you want to choose a different image collection from the Earth Engine Data Catalog, select \"Any Earth Engine ImageCollection.\"\n\n4. Select a band combination\n\nLandsat imagery comes with multi-spectral bands. Here are the common spectral bands produced by Landsat TM, ETM+, and OLI sensors:\n\nSpectral bands of Landsat satellite imagery\n\nTo display and visualize an image on a computer screen, decide which three spectral bands you want to use for the Red, Green, and Blue (RGB) channels. Some of the commonly used band combinations include Natural Color (Red/Green/Blue), Color Infrared (NIR/Red/Green), Short-Wave Infrared (SWIR2/SWIR1/Red), Agriculture (SWIR1/NIR/Blue), and Geology (SWIR2/SWIR1/Blue). Read more about Landsat band combinations here and here.\n\nSelect a band combination from the dropdown list. Or just enter the title you want to see on the resulting Landsat timelapse:\n\nSelect an RGB band combination\n5. Select an administrative boundary\n\nIf your timelapse covers a large region, overlay it with an administrative boundary. Select it from the built-in datasets (e.g., Continents, Countries, US States) or select \"User-defined,\" then enter an HTTP URL for the GeoJSON file:\n\nYou can customize the boundary's color, line width, and opacity.\n\n6. Customize timelapse parameters\n\nThe \"Frames per second\" parameter controls the speed of the timelapse. The smaller the number, the slower the timelapse, and vice versa.\n\nFor example, a timelapse of annual Landsat imagery (1984-2021) has 38 frames. At 5 frames per second, the timelapse would last 7.6 seconds. By default, the app uses all available imagery to create the composite. You can change the start year, the end year, the start month, and the end month if you want a specific time range.\n\nSince Landsat satellites carry optical sensors, you might see clouds in Landsat imagery (especially in the tropics). By default, the app applies the fmask algorithm to remove clouds, shadows, and snow. This can create black spots (nodata) in your timelapse. You can also change the font type, size, and color to customize the animated timestamps.\n\nWhen finished with all parameters, click \"Submit\":\n\nUnder the hood, the app will collect the parameters and pass them to the landsat_timelapse function. It will utilize the GEE cloud computing platform to process the satellite imagery and generate the timelapse. The initial timelapse generated by Earth Engine will be further processed by geemap to add a progress bar, animated timestamps, etc.\n\n7. Download your timelapse in GIF and MP4 formats\n\nYou can now download your timelapse as a GIF or MP4 animation in 60 seconds. üöÄ Right-click the image/video to save it, then share it with your friends, family, or on social media! üòá\n\nHere are a few timelapse examples:\n\nRiver dynamics of Ucayali River, Peru - Created using Landsat imagery (SWIR1/NIR/Red)\n\nVegetation dynamics in Africa - created using monthly MODIS NDVI data\n\nNortheast Pacific bomb cyclone in October 2021 - Created using GOES-17\n\nCreek Fire in California in September 2020 - Created using GOES-17\n\nTemperature dynamics at the global scale - Created using MODIS Land Surface Temperature\n\nWant more examples? Use hashtags #geemap and #streamlit to search on Twitter and LinkedIn.\n\nHow to deploy an Earth Engine app to Streamlit Cloud\n1. Fork the streamlit-geospatial repo\n\nFork the streamlit-geospatial repo to your GitHub account. It contains the source code (>1000 lines) of the multi-page web app for various geospatial applications. You can find it here.\n\n2. Sign up for an Earth Engine account\n\nSign up for a Google Earth Engine account. Once you get the email, log in to the Earth Engine Code Editor to verify that your account has been authorized to use Earth Engine.\n\n3. Install GeoPandas, geemap, and Streamlit\n\nInstall GeoPandas, geemap, and Streamlit Python packages. If you have Anaconda or Miniconda installed on your computer, you can create a fresh conda environment to install the required packages using the following commands:\n\nconda create -n gee python=3.8\nconda activate gee\nconda install geopandas\npip install geemap\n\n4. Get an Earth Engine token\n\nType python into the terminal and press \"Enter\" to get into the Python interactive shell. Then type import ee and ee.Authenticate() and press \"Enter\":\n\nimport ee\nee.Authenticate()\n\nAuthenticate Google Earth Engine\n\nLog in to your Google account to obtain the authorization code and paste it back into the terminal. Once you press \"Enter,\" an Earth Engine authorization token will be saved to your computer under the following file path (depending on your operating system):\n\nWindows: C:\\\\Users\\\\USERNAME\\\\.config\\\\earthengine\\\\credentials\nLinux: /home/USERNAME/.config/earthengine/credentials\nMacOS: /Users/USERNAME/.config/earthengine/credentials\n\n\nNavigate to the above file path and open the credentials file using a Text Editor. Copy the token wrapped within the double quotes to the clipboard:\n\nThe Earth Engine token\n5. Deploy your app to Streamlit Cloud\n\nTo deploy your app, click the ‚ÄúNew app‚Äù button in the upper-right corner of your Streamlit workspace at https://share.streamlit.io, then fill in your repo path (e.g., USERNAME/streamlit-geospatial), branch (e.g., master), and main file path (e.g., app.py). Go to App settings - Secrets and set EARTHENGINE_TOKEN as an environment variable for your web app. Click \"Save\":\n\nDepending on the number of dependencies specified in requirements.txt, the app might take a few minutes to install and deploy. Once deployed, you'll see its URL in your Streamlit Cloud workspace. It'll follow a standard structure based on your GitHub repo, such as:\n\n<https://share.streamlit.io/>[user name]/[repo name]/[branch name]/[app path]\n\n\nFor example, here is the long app URL: https://share.streamlit.io/giswqs/streamlit-geospatial/app.py. And here is the short app URL: https://streamlit.gishub.org\n\nWrapping up\n\nCongratulations! üëè You did it! You have built an interactive web app for creating a satellite timelapse. You're welcome to contribute your comments, questions, resources, and apps as issues or pull requests to the streamlit-geospatial repo.\n\nWant to learn more about GEE and geemap? Check out https://geemap.org , my article, and my YouTube channel for video tutorials. Or get in touch with me on Twitter or LinkedIn.\n\nThanks for reading, and happy coding! üç∫\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly Rewind - Streamlit (Page 3)",
    "url": "https://blog.streamlit.io/tag/monthly-rewind/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Monthly Rewind\n27 posts\nMonthly rewind > July 2021\n\nYour July look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 5 2021\nMonthly rewind > June 2021\n\nYour June look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 5 2021\nMonthly rewind > May 2021\n\nYour May look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2021\nMonthly rewind > April 2021\n\nYour April look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2021\nMonthly rewind > March 2021\n\nYour March look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 5 2021\nMonthly rewind > February 2021\n\nYour February look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 8 2021\nMonthly rewind > January 2021\n\nYour January look back at new features and great community content\n\nMonthly Rewind\nby\nTC Ricks\n,\nFebruary 8 2021\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "satschool-static-1.png (2000√ó1249)",
    "url": "https://blog.streamlit.io/content/images/2022/06/satschool-static-1.png#browser",
    "html": ""
  },
  {
    "title": "ampredst-app.gif (960√ó519)",
    "url": "https://blog.streamlit.io/content/images/2022/12/ampredst-app.gif#browser",
    "html": ""
  },
  {
    "title": "matplotlib_gif-1.gif (932√ó772)",
    "url": "https://blog.streamlit.io/content/images/2022/06/matplotlib_gif-1.gif#shadow",
    "html": ""
  },
  {
    "title": "ifood.jpeg (2000√ó946)",
    "url": "https://blog.streamlit.io/content/images/2022/06/ifood.jpeg#shadow",
    "html": ""
  },
  {
    "title": "typing-playground-1.gif (1680√ó818)",
    "url": "https://blog.streamlit.io/content/images/2022/11/typing-playground-1.gif#browser",
    "html": ""
  },
  {
    "title": "ampredst-app.gif (960√ó519)",
    "url": "https://blog.streamlit.io/content/images/2023/01/ampredst-app.gif#browser",
    "html": ""
  },
  {
    "title": "not-active-amp.png (672√ó139)",
    "url": "https://blog.streamlit.io/content/images/2022/12/not-active-amp.png#border",
    "html": ""
  },
  {
    "title": "active-amp.png (666√ó135)",
    "url": "https://blog.streamlit.io/content/images/2022/12/active-amp.png#border",
    "html": ""
  },
  {
    "title": "molecular-descriptors.png (1244√ó172)",
    "url": "https://blog.streamlit.io/content/images/2022/12/molecular-descriptors.png#border",
    "html": ""
  },
  {
    "title": "aminoacid-composition.png (859√ó528)",
    "url": "https://blog.streamlit.io/content/images/2022/12/aminoacid-composition.png#border",
    "html": ""
  },
  {
    "title": "ampredst-app-input-peptide.png (1476√ó384)",
    "url": "https://blog.streamlit.io/content/images/2022/12/ampredst-app-input-peptide.png#border",
    "html": ""
  },
  {
    "title": "ampredst-app-2.png (1482√ó343)",
    "url": "https://blog.streamlit.io/content/images/2022/12/ampredst-app-2.png#border",
    "html": ""
  },
  {
    "title": "ampredst-app-1.png (1486√ó392)",
    "url": "https://blog.streamlit.io/content/images/2022/12/ampredst-app-1.png#border",
    "html": ""
  },
  {
    "title": "attribute-error.png (2000√ó571)",
    "url": "https://blog.streamlit.io/content/images/2022/11/attribute-error.png#border",
    "html": ""
  },
  {
    "title": "subtle-defect.png (1926√ó644)",
    "url": "https://blog.streamlit.io/content/images/2022/11/subtle-defect.png#border",
    "html": ""
  },
  {
    "title": "4.1-2.png (1200√ó848)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4.1-2.png#browser",
    "html": ""
  },
  {
    "title": "3-8.png (1200√ó863)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-8.png#border",
    "html": ""
  },
  {
    "title": "2-11.png (1200√ó724)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-11.png#border",
    "html": ""
  },
  {
    "title": "1-10.png (1200√ó424)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-10.png#border",
    "html": ""
  },
  {
    "title": "Streamlit-authenticator-part1-social.png (1200√ó675)",
    "url": "https://blog.streamlit.io/content/images/2023/01/Streamlit-authenticator-part1-social.png",
    "html": ""
  },
  {
    "title": "Spotify-playlists-1.png (2000√ó1251)",
    "url": "https://blog.streamlit.io/content/images/2023/01/Spotify-playlists-1.png#browser",
    "html": ""
  },
  {
    "title": "social-share-preview.png (2000√ó1597)",
    "url": "https://blog.streamlit.io/content/images/2023/01/social-share-preview.png",
    "html": ""
  },
  {
    "title": "1.2.gif (594√ó445)",
    "url": "https://blog.streamlit.io/content/images/2022/08/1.2.gif#browser",
    "html": ""
  },
  {
    "title": "3.gif (800√ó900)",
    "url": "https://blog.streamlit.io/content/images/2022/08/3.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled--3--1.png (1682√ó1518)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled--3--1.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--2-.png (779√ó806)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled--2-.png",
    "html": ""
  },
  {
    "title": "2.gif (800√ó900)",
    "url": "https://blog.streamlit.io/content/images/2022/08/2.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (603√ó604)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled--1-.png",
    "html": ""
  },
  {
    "title": "Untitled-1.png (1105√ó592)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled-1.png",
    "html": ""
  },
  {
    "title": "Streamlit (Page 7)",
    "url": "https://blog.streamlit.io/page/7/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nChat with the Cat Generative Dialogue Processor (CatGDP)\n\nBuild your own catbot with a quirky persona!\n\nAdvocate Posts\nby\nTianyi Pan\n,\nMay 3 2023\nIntroducing st.connection!\n\nQuickly and easily connect your app to data and APIs\n\nProduct\nby\nJoshua Carroll and¬†\n1\n¬†more,\nMay 2 2023\nThe ultimate athlete management dashboard for biomechanics\n\nLearn how to measure jump impulse, max force, and asymmetry with Python and Streamlit\n\nAdvocate Posts\nby\nHansen Lu\n,\nApril 27 2023\nCreating a Time Zone Converter with Streamlit\n\n6 steps on how to build your own converter\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nApril 25 2023\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nCreate an animated data story with ipyvizzu and Streamlit\n\nA tutorial on using ipyvizzu and ipyvizzu-story\n\nAdvocate Posts\nby\nPeter Vidos\n,\nApril 20 2023\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nAdvocate Posts\nby\nDmitry Kosarevsky\n,\nApril 18 2023\nIntroducing a chemical molecule component for your Streamlit apps\n\nIntegrate a fully featured molecule editor with just a few lines of code!\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 13 2023\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nAdvocate Posts\nby\nKanak Mittal\n,\nApril 11 2023\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nLLMs\nby\nDave Lin\n,\nApril 6 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/6/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nHow to diagnose blood cancer with Streamlit\n\nBuild a molecular pathology diagnostics app in 4 simple steps\n\nAdvocate Posts\nby\nEitan Halper-Stromberg\n,\nJanuary 25 2022\nHow to create interactive books with Streamlit in 5 steps\n\nUse streamlit_book library to create interactive books and presentations\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nJanuary 20 2022\nCreating satellite timelapse with Streamlit and Earth Engine\n\nHow to create a satellite timelapse for any location around the globe in 60 seconds\n\nAdvocate Posts\nby\nQiusheng Wu\n,\nDecember 15 2021\nFinding your look-alikes with semantic search\n\nHow Pinecone used Streamlit to create a Hacker News Doppelg√§nger app\n\nAdvocate Posts\nby\nGreg Kogan\n,\nDecember 1 2021\nForecasting with Streamlit Prophet\n\nHow Artefact built a Streamlit app to train time-series forecasting models\n\nAdvocate Posts\nby\nMaxime Lutel\n,\nNovember 10 2021\nDetecting parking spots with Streamlit\n\nHow to build a Streamlit parking spot app in 8 simple steps\n\nAdvocate Posts\nby\nJeffrey Jex\n,\nOctober 26 2021\nDeploying a cloud-native Coiled app\n\nHow Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users\n\nAdvocate Posts\nby\nRichard Pelgrim\n,\nSeptember 7 2021\nLabeling ad videos with Streamlit\n\nHow Wavo.me uses Streamlit‚Äôs Session State to create labeling tasks\n\nAdvocate Posts\nby\nAnastasia Glushko\n,\nSeptember 2 2021\nEasy monitoring of dbt Cloud jobs with Streamlit\n\nHow the Cazoo data science team built their dbt Cloud + Streamlit app\n\nAdvocate Posts\nby\nMartin Campbell\n,\nJune 11 2021\nMaking apps for the Rasa research team (and open source community!)\n\nHelping Rasa users understand their models\n\nAdvocate Posts\nby\nVincent D. Warmerdam\n,\nMay 12 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > December 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-december-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > December 2021\n\nYour December look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, January 7 2022\nüèÜ App of the month üèÜ\nStreamlit December updates\nüîç Current release: 1.3.0\nüîÆ Upcoming\n‚òÅÔ∏è Streamlit Cloud free tier expansion\nüîí SOC 2 Certification\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur December featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nCosmŒ©racle by Niko, Marco and Matthijs!\n\nCosmŒ©racle is an app that calculates distances in cosmology. It was created to be a useful and convenient computing aid for people who have a need to know the structure of the universe. Calculate anywhere, anytime! [code].\n\nStreamlit December updates\n\nLet's take a look back at all that went on during December.\n\nüîç Current release: 1.3.0\n\nThe latest release is 1.3.0. Recent updates include further support for PyDeck and Plotly charts, and st.caption supporting HTML in text with unsafe_allow_html parameter. Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming\n\nUpcoming new features and updates to get excited about:\n\nSign in with email (coming VERY soon üëÄ)\nCamera integration\nIn-app share feature\n\nCheck out our roadmap app to get a scope of what else we're working on. ü•≥\n\n‚òÅÔ∏è Streamlit Cloud free tier expansion\nThe free tier of Streamlit Cloud got some major updates! You can now deploy unlimited public apps and a private app for free.\n\nüîí SOC 2 Certification\nWe're committed to meeting industry standards and are now SOC 2 Type 1 certified. Read more about securely sharing your apps using Streamlit Cloud.\n\n\nFeatured Streamlit content\nLearn how to create a satellite timelapse for any location in the world and deploy your own Earth Engine Streamlit app in this blog post by Qiusheng!\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nExplore civic data in Jenna's City of Boston: CityScore app.\nLearn How to Embed Tweets on Streamlit Web Application with Avra's tutorial.\nVivek's Face Detection and Analysis app uses DeepFace and OpenCV to determine age, gender, and emotion.\nIn this live coding, Nicholas shows how to Build a TikTok Data Science App with Streamlit and Python.\nPair up Pok√©mon and see who would win in Marshall's Pokemon Battle Simulator.\nCalculate elite qualifying miles and dollars with King's Air Canada Aeroplan miles and dollars calculator.\nSven's video tutorial teaches you how to Add a snowflake animation to your Streamlit web app.\nIn her article Streamlit Hands-On: Features and Tips For Enhanced App User Experience, Sharone guides you through the code of a real-world use case.\nJavier's Portfolio Optimization Tool lets you apply the Efficient Frontier implementation using MonteCarlo simulations to define and optimize two portfolio examples.\nIn the Summarizing my favorite podcasts with Python tutorial by AssemblyAI, you can learn how to build an app that summarizes podcasts to text by chapter.\nLearn how to make an ML Hyperparameter Optimization App using Streamlit in Nandan's step-by-step tutorial.\nAnalyze and visualize your tweets from the last year in Roberto's app Tu a√±o en Twitter.\nAnother tutorial from Sven shows you how to Build a Website in only 12 minutes using Python & Streamlit.\nYash and Yash created a new Streamlit-Chat component for a chat-bot UI.\n\nThanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tijana Nikolic - Streamlit",
    "url": "https://blog.streamlit.io/author/tijana/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Tijana Nikolic\n1 post\nSogeti creates an educational Streamlit app for data preprocessing\n\nLearn how to use Sogeti‚Äôs Data Quality Wrapper\n\nAdvocate Posts\nby\nTijana Nikolic\n,\nMarch 8 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > September 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-september-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > September 2021\n\nYour September look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, October 7 2021\nüèÜ App of the month üèÜ\nStreamlit September updates\nüîç Current release: 1.0.0\nüîÆ Upcoming features\nüéÆ Discord\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur September featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nStreamlit Prophet by Maxime Lutel.\n\nThis app helps data scientists to visually train, evaluate and optimize a Prophet forecasting model. [code]\n\nStreamlit September updates\n\nLet's take a look back at all that happened during the month of September!\n\nüîç Current release: 1.0.0\n\nThe latest release is 1.0.0. Recently added features from this month include configurable hamburger menu and experimental cache primitives. Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming features\n\nSome new features coming soon:\n\nText dials\nSupport for cards\n\nCheck out our roadmap app for more on what's on the horizon!\n\nüéÆ Discord\n\nWe now have a community Discord. This new space allows you to meet and have real time chat with others in the community. Connect with peers in your industry, with similar topic interests, and that speak the same language! Read more here. ¬†\n\nFeatured Streamlit content\n\nThe team at Wavo discussed how their creative labeling app provides them with data-driven insights to build better music marketing campaigns.\n\nCoiled showed how to make a Streamlit-on-Coiled app to present multi-GBs of data seamlessly.\n\nJohannes wrote a great blog post with helpful tips on how to make your resource hungry apps more efficient.\n\nRelease 0.89.0 brought new experimental cache primitives that help you cache your data up to 10x faster.\n\nOur release notes expanded into blog posts! Get a more detailed insight on each release. This month included. 0.88.0 and 0.89.0\n\n\nFeatured community content\n\nSome great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\n\nJesse's YouTube tutorial showed how to perform File Downloads with Streamlit Download Button\nData Professor chatted with Tyler about his new book on his podcast Data Science Podcast with Tyler Richards (Facebook Data Scientist)\nMohammad launched DummyLearn, a free online machine learning platform\nNelson created an app called explainMyModel that explains different models and their feature contributions\nTeFIRE Calculator by Jimmy let you play with crypto projections and monthly income\nSpidy showed how to make a vaccination center and availability checker app in his video Py-Cowin | Find Vaccination Center using Python Streamlit\nPavan came out with another great CV app that lets you convert a Video to GIF\nMarshall's app visualized US State Migration data from 2019\nData Professor answered the question Is it Possible to Add a Navigation Bar to Streamlit Apps? in his video tutorial\nTeddy created a fun PvP Gomoku game app\nTunahan wrote a tutorial Control the Flow: Streamlit showing how to build an app and use session state\nHarshit came out with a popular video tutorial Building a buzzing stocks news feed using NLP and Streamlit | Named Entity Recognition & Linking\nAvra's SciLit app let you search through scientific publications based on keywords\nLucas and his team created a clustering strategy app iRaPCA clustering, with downloadable validation metrics and graphs\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > November 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-november-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > November 2021\n\nYour November look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, December 7 2021\nüèÜ App of the month üèÜ\nStreamlit November updates\nüîç Current release: 1.2.0\nüîÆ Upcoming\n‚òÅÔ∏è Streamlit Cloud\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur November featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nStreamlit as a PowerPoint replacement by Sebastian Flores!\n\nSebastian created a Streamlit app to give a presentation about Streamlit at PyCon Chile! The app uses slide pagination and goes over features of Streamlit, providing examples and code. [code].\n\nStreamlit November updates\n\nLet's take a look back at all that's happened during November!\n\nüîç Current release: 1.2.0\n\nThe latest release is 1.2.0. Recent updates include the ability to set custom placeholder text and to resize the input box in st.text_area. Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming\n\nSome new upcoming features and updates to get excited about:\n\nFavorite an app\nSign in with an email\nCustom subdomains\n\nCheck out our roadmap app to see what else we're working on!\n\n‚òÅÔ∏è Streamlit Cloud\nStreamlit Cloud launched. Check out the different packages geared for everything from personal projects to team workflows needing enterprise-grade features!\n\n\n\nFeatured Streamlit content\n\nMaxime shared how he and his team built an app to train time-series forecasting models in Forecasting with Streamlit Prophet.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nGreg's Google Trends For Top GSC Keywords app lets you label your top-performing GSC keywords with Google Trends.\nWilliam gives a tutorial on how to build An App to Update Excel File with Pandas and Streamlit.\nAnalyze the sales dynamics of an NFT collection published on the Algorand blockchain in Vilijan's NFT Analytics App.\nBek teaches how to create a Stock Prices Web App with Streamlit Framework in his informative YouTube video.\nFanilo's Twittorial: Using Seaborn in Streamlit gives a quick breakdown on plotting Seaborn plots in Streamlit.\nBogdan recorded a demo of his awesome Stock Beta Calculator app.\nKabilan's Mask Detection and Social Distancing Detection app finds COVID risk in an area based on live video input.\nIn his video tutorial, Siddhardhan shows how to Deploy Machine Learning Model using Streamlit in Python.\nRahman's DataCo Smart Supply Chain Dashboard provides nice graphics for data visualization.\nGPT-3 for SEO by Andrea is an app to test all the possible SEO use-cases for GPT-3.\nLearn how to Build a Personal Webpage with Streamlit in Alan's article.\n\nThanks for checking out this edition of our monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "AbdulMajedRaja RS - Streamlit",
    "url": "https://blog.streamlit.io/author/abdulmajedraja/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by AbdulMajedRaja RS\n1 post\nHow to build a real-time live dashboard with Streamlit\n\n5 easy steps to make your own data dashboard\n\nAdvocate Posts\nby\nAbdulMajedRaja RS\n,\nApril 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Samuel Bancroft - Streamlit",
    "url": "https://blog.streamlit.io/author/samuel/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Samuel Bancroft\n1 post\nTwitter\nObserving Earth from space with Streamlit\n\nLearn how Samuel Bancroft made the SatSchool app to teach students Earth observation\n\nAdvocate Posts\nby\nSamuel Bancroft\n,\nJune 16 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Shruti Agarwal - Streamlit",
    "url": "https://blog.streamlit.io/author/shruti/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Shruti Agarwal\n1 post\nHow to build Streamlit apps on Replit\n\nLearn Streamlit by building the Beginner Template Tour\n\nAdvocate Posts\nby\nShruti Agarwal\n,\nSeptember 29 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "scienceio.png (2000√ó942)",
    "url": "https://blog.streamlit.io/content/images/2023/03/scienceio.png",
    "html": ""
  },
  {
    "title": "cloud-deploy-1.png (2000√ó943)",
    "url": "https://blog.streamlit.io/content/images/2023/03/cloud-deploy-1.png",
    "html": ""
  },
  {
    "title": "culture-map-hero.png (2000√ó947)",
    "url": "https://blog.streamlit.io/content/images/2023/03/culture-map-hero.png",
    "html": ""
  },
  {
    "title": "Pranav Pandit - Streamlit",
    "url": "https://blog.streamlit.io/author/pranavpandit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Pranav Pandit\n1 post\nNew UC Davis tool tracks California's COVID-19 cases by region\n\nRegional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine\n\nAdvocate Posts\nby\nPranav Pandit\n,\nNovember 19 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "George Merticariu - Streamlit",
    "url": "https://blog.streamlit.io/author/george/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by George Merticariu\n1 post\n3 steps to fix app memory leaks\n\nHow to detect if your Streamlit app leaks memory and identify faulty code\n\nTutorials\nby\nGeorge Merticariu\n,\nApril 14 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/page/6/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\nAdd secrets to your Streamlit apps\n\nUse Secrets Management in Streamlit sharing to securely connect to private data sources\n\nTutorials\nby\nJames Thompson\n,\nApril 9 2021\nHow to use Roboflow and Streamlit to visualize object detection output\n\nBuilding an app for blood cell count detection\n\nAdvocate Posts\nby\nMatt Brems\n,\nFebruary 23 2021\nStreamlit ‚ù§Ô∏è Firestore\n\nUse Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!\n\nTutorials\nby\nAustin Chen\n,\nJanuary 27 2021\nStreamlit Components, security, and a five-month quest to ship a single line of code\n\nThe story of allow-same-origin\n\nTutorials\nby\nTim Conkling\n,\nJanuary 20 2021\nElm, meet Streamlit\n\nA tutorial on how to build Streamlit components using Elm\n\nTutorials\nby\nHenrikh Kantuni\n,\nDecember 8 2020\nTesting Streamlit apps using SeleniumBase\n\nHow to create automated visual tests\n\nTutorials\nby\nRandy Zwitch\n,\nNovember 23 2020\nDeploying Streamlit apps using Streamlit sharing\n\nA sneak peek into Streamlit's new deployment platform\n\nTutorials\nby\nTyler Richards\n,\nOctober 15 2020\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "colorpalette.png (2000√ó948)",
    "url": "https://blog.streamlit.io/content/images/2023/03/colorpalette.png",
    "html": ""
  },
  {
    "title": "background-removal-hero-1.png (2000√ó942)",
    "url": "https://blog.streamlit.io/content/images/2023/03/background-removal-hero-1.png",
    "html": ""
  },
  {
    "title": "Monthly rewind > April 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-april-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > April 2022\n\nYour April look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, May 5 2022\nüèÜ App of the month üèÜ\nStreamlit April updates\nüîç Current release: 1.9.0\nüîÆ Upcoming\nüóì 30 days of Streamlit\nüéô Tech Twitter Space\nüìÑ New docs tutorials\nüéà New Streamlit Creators\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur April featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nSatSchool - Hands on with Data by Samuel Bancroft.\n\nSatSchool is an earth observation outreach program for getting hands-on with satellite data. This interactive app teaches you how to work with different types of data in order to visualize and understand environmental concepts. [code]\n\nStreamlit April updates\n\nLet's take a look at what happened in April.\n\nüîç Current release: 1.9.0\n\nThe latest release is 1.9.0. Recent updates include support for keyword-only argument expanded on st.json and the maintaining of the widgets' value when disabled is set/unset. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nSome exciting new features coming soon:\n\nWorkspace analytics and app viewers data\nMultipage apps\nNew dataframe UI\n\nCheck out our roadmap app to see what else we're working on. ü•≥\n\nüóì 30 days of Streamlit\nApril kicked off #30DaysofStreamlit, a fun social challenge to practice building and deploying Streamlit apps. Get started and follow the daily tasks here.\n\nüéô Tech Twitter Space\nAs part of #30DaysofStreamlit, Francesco hosted a Tech Twitter Space with Amanda, Randy, and Chanin (Dataprofessor). Listen to the recording to hear the discussion‚Äîincluding questions from the community!\nüìÑ New docs tutorials\nWe added two new database connection tutorials to the docs. Learn how to connect Streamlit to Deta Base and Supabase.\n\nüéà New Streamlit Creators\nWelcome our newest Streamlit Creators‚ÄîMƒ±sra, Gerard, and Franky! ü•≥\nFeatured Streamlit content\nLearn how to find memory leaks and fix them with these three steps from George.\n\n1littlecoder teaches you how to build your own real-time live data dashboard for quick insights into finance, marketing, and data analytics.\n\nSee how Streamlit helped the analytics team at The Stable scale and democratize their data and go from idea to app in just a few days.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nGet insight into energy production and use Benedict's Energytics app.\nAnna's Hiking Upward Recommender app can help you find your next similar hike.\nSharone's article shows how to Make Dataframes Interactive by using Streamlit-Aggrid.\nIn his Introduction to Streamlit and Streamlit Components, Arvindra details how to build not only a Streamlit app but also custom components to expand its functionality.\nChristoph's Prettymapp app lets you create beautiful artistic maps in your browser for any address.\nLearn how to add keyboard shortcuts to navigate through the Streamlit apps with Fanilo's video tutorial.\nThe Striking Distance Creator app by Lee blends keyword & crawl data to provide actionable insights for keywords close to ranking.\nJapan created an app for cryptocurrency prediction called CryptoBase.\nJina AI released Jina NOW‚Äîa no-code solution for creating a neural search with a built-in Streamlit interface!\nspacy-streamlit got an upgrade with improved NER visualization, UX and layout enhancements, and two new example scripts.\nJoan teaches how to Deploy Machine Learning Web Applications with Streamlit and seamlessly interact with models in a creative and easy way.\nOkld released an awesome new component Streamlit Elements that allows you to build draggable and resizable dashboards with Material UI, Nivo charts, and more!\nLearn how to Generate Images with Your Voice Using DALL-E in Assembly AI's tutorial by Patrick.\nAnother article by Sharone dives into how to Create a Data Profiling App Using Pandas_Profiling and Streamlit.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "kedrobloghero.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2023/03/kedrobloghero.png",
    "html": ""
  },
  {
    "title": "Streamlit",
    "url": "https://blog.streamlit.io/december-rewind/https://blog.streamlit.io/create-a-color-palette-from-any-image/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n404\nPage not found\n‚Üê Go to the front page\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Screenshot-2023-02-01-at-9.33.30-PM-2.png (2000√ó943)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Screenshot-2023-02-01-at-9.33.30-PM-2.png",
    "html": ""
  },
  {
    "title": "authenticator2-1.png (2000√ó943)",
    "url": "https://blog.streamlit.io/content/images/2023/03/authenticator2-1.png",
    "html": ""
  },
  {
    "title": "gif-editor-game.gif (1542√ó812)",
    "url": "https://blog.streamlit.io/content/images/2023/03/gif-editor-game.gif",
    "html": ""
  },
  {
    "title": "columnsdemo.png (2000√ó942)",
    "url": "https://blog.streamlit.io/content/images/2023/03/columnsdemo.png",
    "html": ""
  },
  {
    "title": "Making Apps for the Rasa Research Team & Open Source Community",
    "url": "https://blog.streamlit.io/rasalit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMaking apps for the Rasa research team (and open source community!)\n\nHelping Rasa users understand their models\n\nBy Vincent D. Warmerdam\nPosted in Advocate Posts, May 12 2021\nChallenges\nEnter Rasalit\nGridSearch Overview\nRasa NLU Playground\nSpelling Effects\nText Clustering\nSteps Going Forward\nResources\nContents\nShare this post\n‚Üê All posts\n\nTo learn more about Rasalit, check out the article on the Rasa blog.\n\nRasa Open Source is a machine learning framework for building text- and voice-based virtual assistants. It‚Äôs a Python library with tools that can understand messages, reply to users, and connect to different messaging channels and APIs.\n\nIn this post, we'll do a deep dive into the Rasalit project, which is an integration between Rasa and Streamlit, but here's a sample app if you want to test it out right away!\n\nChallenges\n\nRasa actively researches and shares practical algorithms that can handle natural language tasks, but exploring algorithms in this space brings a few unique challenges.\n\nFor starters, we can only benchmark on datasets that are openly available. If there is any private data in a conversation, it can‚Äôt be shared - which excludes a lot of meaningful datasets.\n\nPrivacy isn‚Äôt the only constraint we face, another limitation is the languages we can use in our benchmarking datasets. We‚Äôve done our best to integrate many open-source tools for non-English deployments, but we still actively rely on our community for feedback.\n\nTo address this, we‚Äôve been looking for a meaningful tool to give to our community that makes it easy to explore and investigate trained Rasa models interactively. If we can make it easy for users to inspect their pipelines, we also make it easier for people to give feedback on specific parts.\n\nEnter Rasalit\n\nRasalit‚Äôs first iteration was a simple demo in a Jupyter notebook. To use it, you would declare an utterance text for a pre-trained Rasa pipeline to classify and then you could see the prediction‚Äôs confidence values in a bar chart.\n\nIn theory, we had a meaningful visualization. But it became clear this approach wouldn‚Äôt work in practice for many reasons:\n\nSharing notebooks over GitHub tends to be a painful experience.\nNot every Rasa user is familiar with Python, which means a Jupyter notebook can be intimidating for some.\nHosting a Jupyter notebook on a server involves security risks. You can‚Äôt run a Jupyter notebook in read-only mode and still allow users to change settings, which means you can‚Äôt host our visualizations securely on a private server.\nJupyter is excellent when writing code, but code can distract us from our visualizations. We wanted users to focus on the model views.\n\nThat‚Äôs why, instead of going with Jupyter, we decided to package our views with Streamlit. Streamlit allows us to control what users can interact with and keep distractions away.\n\nWe ended up creating several Streamlit apps that proved valuable, and we bundled them all together into a package called Rasalit.\n\nHere‚Äôs what it looks like when you run it from the terminal.\n\n> python -m rasalit\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  overview       Gives an overview of all `rasa train nlu` results.\n  live-nlu       Select a trained Rasa model and interact with it.\n  spelling       Check the effect of spelling on NLU predictions.\n  nlu-cluster    Cluster a text file to look for clusters of intents.\n  version        Prints the current version\n\n\nEach command represents a separate Streamlit application. When passing arguments in Rasalit, we can translate them into appropriate arguments for Streamlit.\n\nGridSearch Overview\n\nOne tool in Rasalit handles visualizing grid-search results. You can run cross-validation from the command line in Rasa, but our plugin now makes it easy to get an overview of the scores too.\n\nRasa NLU Playground\n\nThe second app allows users to interact directly with a pre-trained Rasa model. You get an overview of the intent confidence and any detected entities.\n\nWe‚Äôve also added charts that visualize the classifier‚Äôs internal attention mechanism.\n\nTo keep the overview simple, we‚Äôve hidden these details. An excellent feature from Streamlit is you can hide details via the expander component. That means we can add detailed features for our Research team while still keeping the app distraction-free for the general community.\n\nSpelling Effects\n\nThere‚Äôs also a spelling robustness checker in Rasalit, which simulates spelling errors on a text that you give it. It will show you how robust your trained models are against typos.\n\nText Clustering\n\nFinally, we‚Äôve also added a tool for folks who are just getting started with their virtual assistants. Some users might already have some unlabelled training data and might just be curious to explore the clusters in them.\n\nFor this use case, we‚Äôve built a text clustering demo. It uses a light version of the universal sentence encoder to cluster text together. This result can be explored interactively.\n\nSteps Going Forward\n\nStreamlit has turned out to be a surprisingly flexible communication tool. Our research team and community members all use Rasalit. It‚Äôs also been getting us great feedback! We‚Äôre proud to report it has surpassed 100 Github stars.\n\nRasalit has been so successful we‚Äôve also started using it in other places. We host a training data repository where users can find training data to help get them started with their first virtual assistant.\n\nBefore, users had to search inside the many YAML files to find the training data that fits their use cases. Now we‚Äôve simply attached a hosted Streamlit app that makes it easy for users to find the relevant training data. It‚Äôs a great experience!\n\nWe‚Äôre excited to see what you all think of Rasalit and what new applications we find for Streamlit, both for our Research team and across the Rasa open source community.\n\nResources\n\nRasa\n\nHomepage\nDocs\nRasalit repo\nForum\n\nStreamlit\n\nGitHub\nDocs\nForum\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > May 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-may-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > May 2022\n\nYour May look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, June 7 2022\nüèÜ App of the month üèÜ\nStreamlit May updates\nüîç Current release: 1.10.0\nüîÆ Upcoming\nüìà Workspace analytics and app viewers\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur May featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nBERT Semantic Interlinking Tool by Lee Foot!\n\nUpload an internal_html.csv from Screaming Frog and discover pages to interlink. This app allows you to find related products and blogs, supporting content for category pages, and related pages to crosslink. It also lets you group similar pages. You can filter the source and the destination URL type, select cluster size and accuracy, and cluster any column. [code]\n\n*Learn about the latest updates here as of Dec. 2023\n\nStreamlit May updates\n\nA lot of exciting things happened in May! Let's take a look.\n\nüîç Current release: 1.10.0\n\nThe latest release is 1.10.0. Recent updates include native support for multipage apps and redesigned st.dataframe. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nThese new features are currently on the horizon:\n\nCustom and unique subdomains\nReplay of cached st calls\n\nCheck out our roadmap app to see what else we're working on. ü•≥\n\nüìà Workspace analytics and app viewers\nGet insight into the traffic of your Streamlit apps without having to use JS hacks! Try the new built-in Analytics Modal in Community Cloud.\n\nFeatured Streamlit content\nLearn how the Streamlit data science team uses st.session_state and query parameters to share insights through contextual apps.\n\nNeed a quick way to interactively share your project with your team? See how Wissam brought his AI4Health work to life with Streamlit!\n\nRead how Mitch and Roland visualized their research with Rascore, an app for analyzing the 3D structural models of RAS proteins.\n\nLearn how to make a select box in the latest Streamlit short.\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nVisualize Chemical Space with the ChemPlot app by Dajt and team.\nSubmit a Wind Simulation ‚Äî Request Offer with Johannes' interactive app.\nAdd Depth-aware text to an image using Vivien's app.\nWatch Andy's step-by-step video on Getting Started With Streamlit in Python.\nAyoub's Text data extractor lets you upload a PDF and receive a text file or zip of the text from it.\nJos√©'s app demonstrates Ley de Snell to teach refraction and Total Internal Reflection.\nAnuraag's app uses ML to find soccer Players of Similar Profiles.\nCreate Speck molecular structures in Streamlit apps with Avra's Specklit component.\nYuichiro started the stlite, a project allowing Streamlit to run completely on browsers with Wasm by using Pyodide!\nFelipe's tutorial teaches you how to make Magic Data Apps with Snowflake, Streamlit, and DuckDB.\nLearn how to make a Streamlit Keyword Density Checker Web App in this video tutorial from Nileg Production.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > July 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-july-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > July 2021\n\nYour July look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, August 5 2021\nüèÜ App of the month üèÜ\nStreamlit July updates\nüîç Current release: 0.85.1\nüîÆ Upcoming features\nüéà New blog design\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur July featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nChef Transformer by Mehrdad, Kartik, Haswanth, Deepak and Nicholas.\n\nDiscover new and unique recipes from Chef Scheherazade and Chef Giovanni, trained NLP models/\"chefs\", by entering a type of cuisine or a custom ingredient list. [code]\n\nStreamlit July updates\n\nLet's take a look at all that happened during the month of July!\n\nüîç Current release: 0.85.1\n\nThe latest release is 0.85.1. Notably, Streamlit now uses Apache Arrow to serialize DataFrames but make sure to check out the changelog to see all the latest features and fixes.\n\nüîÆ Upcoming features\n\nBe on the lookout for these new features on the horizon:\n\nText dials\nDownload button\nSupport for cards\nüéà New blog design\n\nIf you hadn't noticed, we made some fun changes to our blog site! Take a look around and explore our other posts.\n\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nRandy outlines some tips on improving your app performance in the final part of the Designing Streamlit Apps for the User series.\n\nHenrikh dives into how we now use Arrow as our internal serialization format and how it helped delete over 1,000 lines of code from our codebase here.\n\nListen to Amanda talk about her career journey in this Founders You Should Know interview with Renaissance Collective\nKen gives an in depth interview and answers Streamlit questions in dataroots' Tour de Tools.\nFeatured community content\n\nSome great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nDr. WJB Mattingly kicks off his Streamlit tutorial series with a video on How to use Streamlit in 30 Minutes!\nUpload an image to Ujjayanta's app and it will Generate ASCII images using GAN\nThis Cloud Optimized GeoTIFF Viewer by Mykola lets you view your COG files with an interactive web map\nAnmol's Nightingale Chart Plotter app lets you create, customize and download Nightingale charts with mplsoccer\nMatthew and his colleagues created a dashboard to Visualize your EED data which can generate six unique visualizations as seen from a variety of different perspectives\nThe Python Engineer gives a speedy tutorial in his Machine Learning Web App In Python In 60 Seconds! #Shorts video\nGain insight into, measure the quality of, and repair your data with the DataQtor app by Beytullah\nLukas' Animation des Rheinabflusses seit Juni 2020 consecutively displays the precipitation and runoff data of the Rhine in Basel\nYuichiro forked Kazuhito's Tokyo2020 Pictogram using MediaPipe code and made a Streamlit app so anyone can try it!\nCassie developed a Dungeons & Dragons Currency Converter to calculate which and how many coins to use when spending in the game\nCheck out Kieran's simple web publishing system which can display both text based and interactive code-based posts\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > August 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-august-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > August 2021\n\nYour August look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, September 7 2021\nüèÜ App of the month üèÜ\nStreamlit August updates\nüîç Current release: 0.88.0\nüîÆ Upcoming features\nüéà New careers page\nüé® New creators\nüçé Education poll\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur August featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nParking Spot Vacancy by Jeffrey Jex.\n\n\nWanna know how many parking spots are available at the Jackson Hole Wyoming Town Square before making the trip? This app takes a live stream of the area and and processes it with Mask R-CNN to detect cars and trucks. It then compares locations with parking places to see which are available. [code]\n\nStreamlit August updates\n\nLet's take a look at all that happened during the month of August!\n\nüîç Current release: 0.88.0\n\nThe latest release is 0.88.0 with st.download_button being the notable feature. August also brought st.metric! Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming features\n\nCheck these new features coming soon:\n\nImproved hamburger menu\nNext gen cache\nText dials\nSupport for cards\nüéà New careers page\n\nWant to know what it's like to work at Streamlit? Check out the new and improved careers page to learn more about our values and see what roles are open!\n\nüé® New creators\n\nWe're excited to welcome 3 new members to the Streamlit creators program. Shout out to AbdulMajed, Pablo, and Yuichiro. Get to know them and the rest of our awesome creators here.\n\nüçé Education poll\n\nWe're looking to develop an Education Program and would love your insight and feedback on Streamlit in the classroom. If you're a teacher or student, take a sec to fill out our form and let us know how we can support you.\n\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nStreamlit creator Tyler Richards released his book \"Getting Started with Streamlit for Data Science\", a great comprehensive guide on learning Streamlit!\n\nCheck out more details about this awesome new resource in Adrien's review piece.\nStreamlit Gains a Major New Spell Book\nA tome to the magical fields of Python, Algorithms, Visualization, and Machine learning.\nStreamlit\nAdrien Treuille\n\nFeatured community content\n\nSome great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\n1littlecoder showed how to make an app that can extract text from images in his video Python Tutorial to build Image to Text App using EasyOCR & Streamlit\nPython Engineer created a Stock Prediction Web App In Python In 60 Seconds! in his short\nYaniss' Broken Links Finder app takes a domain and finds every broken link in any of the pages present on the website\nSee how Streamlit is used for Sharing micro-computed tomography(MCT) image analysis in dentistry in Jongki's app\nAvra gave tips on how to design your apps in his video STREAMLIT Python TRICKS - make your WEB APP look BETTER in 6 Minutes\nSEOgre by Colt, is a tool that lets you layer Google updates, GSC traffic, and SEO tool data exports to better analyze data\nJose made an app SMILES + RDKit + Py3DMOL to learn about Simplified Molecular Input Line Entry System\nIn his video Streamlit Tutorial for Building Analytics Metric Dashboard in Python 1littlecoder went over the new st.metric feature\nJenna created Interact with Gapminder Data, an app that students can learn how to make in her workshop on interactive data visualizations in python\nAhmed wrote an overview outlining 7 Reasons Why You Should Use the Streamlit AgGrid Component\nMisra came back with another great YouTube tutorial showing How to use Streamlit session states and callback functions | Make your apps remember things!\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > February 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-february-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > February 2022\n\nYour February look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, March 7 2022\nüèÜ App of the month üèÜ\nStreamlit February updates\nüîç Current release: 1.7.0\nüîÆ Upcoming\nüì∫ Streamlit and Pinecone Webinar\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur February featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nStreamlit with AssemblyAI by Ahmed Besbes!\n\nThis app uses AssemblyAI to transcribe YouTube videos and extract topics from them. It breaks the video into portions and highlights the detected topics from each. [code]\n\nStreamlit February updates\n\nLet's take a look at all that happened in February.\n\nüîç Current release: 1.7.0\n\nThe latest release is 1.7.0. Recent updates include WebSocket compression being disabled by default, and radio and checkboxes improving focus on keyboard navigation. Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming\n\nBelow are some upcoming features to get excited about:\n\nApp viewers data\nEmail alert when app is over the resource limits\nMultipage apps\n\nCheck out our roadmap app to get a bigger scope of what else we're working on. ü•≥\n\nüì∫ Streamlit and Pinecone Webinar\nRandy joined James and Pinecone to show how to build a custom Q&A app to revolutionize your search systems!\n\nFeatured Streamlit content\nSee how Delta Dental uses Streamlit to make lightning-fast decisions and empower their data team.\n\nCalculate distances in cosmology with just one click! Learn how researchers Niko Marco and Matthijs came together to create the CosmŒ©racle app.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nLucas made a financial data visualization app with Raindrop Charts using Yfinance, Streamlit, & Plotly.\nFanilo showed 4 ways to display Seaborn charts in Streamlit in his YouTube tutorial.\nJames wrote about how to build production-ready NLP apps in his article Getting Started with Streamlit for NLP.\nIn his article Build Your Own SEO AnswerBox With GPT-3 Codex & Streamlit, Vincent explained how to gain productivity by configuring your own smart SEO dashboards using phrases and questions.\nLearn How to build a Cryptocurrency Price Index [with Python using SQL,APIs and Streamlit] in Algovibes' video tutorial.\nSee visualizations for the last 500 recorded earthquakes on a Turkish map in Doƒüukan's Turkey Earthquake Dashboard .\nAlan wrote a great step-by-step guide on how to Publish Your Streamlit Apps in the Cloud.\nIn his helpful tutorial, Fanilo went over 10 tips to become a Streamlit Cloud Master.\nExplore Oscar nominees and make predictions on the winners in Bogdan's 2022 Oscars Predictions app.\nAmmar's app summarized the most common Activation Functions‚Äîan important topic in deep learning.\nCreate pallettes from colormaps with Robert's Color Extractor and use them in your projects.\nBrydon created NorthDash: Canada‚Äôs Status Dashboard to visualize how Canada is faring based on data.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > March 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-march-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > March 2022\n\nYour March look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, April 7 2022\nüèÜ App of the month üèÜ\nStreamlit March updates\nüîç Current release: 1.8.0\nüîÆ Upcoming\n‚ùÑÔ∏è Snowflake\nüéô Open source podcast\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur March featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nRascore by Mitchell Parker and Roland Dunbrack!\n\nRascore is a tool for analyzing structures of the RAS protein family. It can also be used to build an updatable database of all available RAS structures in the Protein Data Bank. This app aims to simplify the biological study of RAS proteins in cancer and other diseases and facilitate RAS drug discovery. [code]\n\nStreamlit March updates\n\nLet's take a look at what happened in March.\n\nüîç Current release: 1.8.0\n\nThe latest release is 1.8.0. Recent updates include improved performance for dataframes and design improvements to our header. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nBelow are some exciting upcoming features:\n\nApp viewers data\nWorkspace analytics\nMultipage apps\n\nCheck out our roadmap app to get a bigger scope of what else we're working on. ü•≥\n\n‚ùÑÔ∏è Snowflake\nStreamlit was acquired by Snowflake! We're excited to join forces and open new frontiers in data science and data application development. Read more.\n\nüéô Open source podcast\nAdrien joined the folks at Open Source Startup Podcast to chat about the launch and cultivation of Streamlit.\n\nFeatured Streamlit content\nSee how Sogeti created Data Quality Wrapper, an educational Streamlit app for data preprocessing.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nAvra demonstrated how to use STREAMLIT with AG Grid Table - Interactive Table in his YouTube video.\nIn his article, James went over How to Create Custom Streamlit Components!\nAndrea built an app for Web Scraping Made Easy using the AutoScraper library.\nQiusheng added an interactive Split-Map feature to his geospatial app.\nAlan detailed how he used Streamlit for Visualizing the Economic Impact of the Pandemic.\nGerard created an app to explore Interactive Timeseries Forecasting with Darts.\nFidelity Account Overview is another app from Gerard that visualizes your Fidelity data.\nIn an AssemblyAI YouTube video, Misra showed how to make an app for Auto-generating meeting notes with Python.\nEnter a keyword and see the list of Google questions with Mihir's Realtime - People Also Ask Tool.\nPythonology's beginner Python tutorial teaches how to make a Currency Converter - Streamlit app.\nGet help figuring out daily Worlde puzzles by using Siddhant's Wordle Solver app.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\nFebruary 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Screenshot-2023-02-13-at-11.12.40-AM-1.png (2000√ó947)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Screenshot-2023-02-13-at-11.12.40-AM-1.png",
    "html": ""
  },
  {
    "title": "Build a Neural Search | Use Jina to Search Text or Images",
    "url": "https://blog.streamlit.io/streamlit-jina-neural-search/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild a Jina neural search with Streamlit\n\nUse Jina to search text or images with the power of deep learning\n\nBy Alex C-G\nPosted in Advocate Posts, April 15 2021\nWhy use Jina to build a neural search?\nWhy use Streamlit with Jina?\nHow does it work?\nExample code\nBehind the scenes\nSet configuration variables\nRender component\nUse it in your project\nWhat to do next\nJina\nStreamlit\nA big thank you!\nContents\nShare this post\n‚Üê All posts\n\nDo you ever think, ‚ÄúDarn this stupid cloud. Why can‚Äôt there be an easier way to build a neural search on it?‚Äù\n\nWell, if you have, this article is for you. I‚Äôm going to walk through how to use Jina's new Streamlit component to search text or images to build a neural search front end.\n\nWant to jump right in? Check out the component's repo.\n\nWhy use Jina to build a neural search?\n\nJina is an open-source deep learning-powered search framework for building cross-/multi-modal search systems (e.g. text, images, video, audio) on the cloud. Essentially, it lets you build a search engine for any kind of data with any kind of data.\n\nSo you could build your own text-to-text search engine ala Google, a text-to-image search engine ala Google Images, an audio-to-audio search engine and so on. Companies like Facebook, Google, and Spotify build these searches powered by state-of-the-art AI-powered models like FAISS, DistilBERT and Annoy.\n\nWhy use Streamlit with Jina?\n\nI've been a big fan of Streamlit since before I even joined Jina. I used it on a project to create terrible Star Trek scripts that later turned into a front end for text generation with Transformers. So I'm over the moon to be using this cool framework to build something for our users.\n\nBuilding a Streamlit component helps the data scientists, machine learning enthusiasts, and all the other developers in the Streamlit community build cool stuff powered by neural search. It offers flexibility and, being written in Python, it can be easier for data scientists to get up to speed.\n\nOut of the box, the streamlit-jina component has text-to-text and image-to-image search, but Jina offers a rich search experience for any kind of data with any kind of data so there's plenty more to add to the component!\n\nHow does it work?\n\nEvery Jina project includes two Flows:\n\nIndexing: for breaking down and extracting rich meaning from your dataset using neural network models\n\nQuerying: for taking a user input and finding matching results\n\nOur Streamlit component is a front end for end users, so it doesn't worry about the indexing part.\n\nAdmin spins up a Jina Docker image: docker run -p 45678:45678 jinahub/app.example.wikipedia-sentences-30k:0.2.9-1.0.1\nUser enters a query into the Streamlit component (currently either a text input or an image upload) and hits 'search'\nThe input query is wrapped in JSON and sent to Jina's query API\nThe query Flow does its thing and returns results in JSON format (along with lots of metadata)\nThe component parses out the useful information (e.g. text or image matches) and displays them to the user\nExample code\n\nLet's look at our text search example, since it's easier to see what's going on there:\n\nimport streamlit as st\nfrom streamlit_jina import jina\nst.set_page_config(page_title=\"Jina Text Search\",)\n\nendpoint = \"http://0.0.0.0:45678/api/search\"\n\nst.title(\"Jina Text Search\")\nst.markdown(\"You can run our [Wikipedia search example](https://github.com/jina-ai/examples/tree/master/wikipedia-sentences) to test out this search\")\n\njina.text_search(endpoint=endpoint)\n\n\nAs you can see, the above code:\n\nImports streamlit and streamlit_jina\nSets the REST endpoint for the search\nSets the page title\nDisplays some explanatory text\nDisplays the Jina text search widget with endpoint defined\n\nFor the Jina Streamlit widgets you can also pass in other parameters to define number of results you want back or if you want to hide certain widgets.\n\nBehind the scenes\n\nThe source code for our module is just one file, __init__.py. Let's just look at the high-level functionality for our text search example for now:\n\nSet configuration variables\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\n# Set default endpoint in case user doesn't specify and endpoint\nDEFAULT_ENDPOINT = \"http://0.0.0.0:45678/api/search\"\n\nRender component\nclass jina:\n    def text_search(endpoint=DEFAULT_ENDPOINT, top_k=10, hidden=[]):\n        container = st.beta_container()\n        with container:\n            if \"endpoint\" not in hidden:\n                endpoint = st.text_input(\"Endpoint\", endpoint)\n\n            query = st.text_input(\"Enter query\")\n\n            if \"top_k\" not in hidden:\n                top_k = st.slider(\"Results\", 1, top_k, int(top_k / 2))\n\n            button = st.button(\"Search\")\n\n            if button:\n                matches = text.process.json(query, top_k, endpoint)\n                st.write(matches)\n\n        return container\n\n\nIn short, the jina.text_search() method:\n\nCreates a Streamlit container to hold everything, with sane defaults if not specified\nIf widgets aren't set to hidden, present them to user\n[User types query]\n[User clicks button]\nSends query to Jina API and returns results\nDisplays results in the component\n\nOur method's parameters are:\n\njina.text_search() calls upon several other methods, all of which can find in __init__.py. For image search there are some additional ones:\n\nimage.encode.img_base64() encodes a query image to base64 and wraps it in JSON before passing to Jina API\nJina's API returns matches in base64 format. The image.render.html() method wraps these in <IMG> tags so they'll display nicely\nUse it in your project\n\nIn your terminal:\n\nCreate a new folder with a virtual environment and activate it. This will prevent conflicts between your system libraries and your individual project libraries:\n\nmkdir my_project\nvirtualenv env\nsource env/bin/activate\n\n\nInstall the Streamlit and Streamlit-Jina packages:\n\npip install streamlit streamlit-jina\n\n\nIndex your data in Jina and start a query Flow. Alternatively, use a pre-indexed Docker image:\n\ndocker run -p 45678:45678 jinahub/app.example.wikipedia-sentences-30k:0.2.9-1.0.1\n\n\nCreate your app.py:\n\nimport streamlit as st\nfrom streamlit_jina import jina\nst.set_page_config(page_title=\"Jina Text Search\",)\n\nendpoint = \"http://0.0.0.0:45678/api/search\" # This is Jina's default endpoint. If your Flow uses something different, switch it out\n\nst.title(\"Jina Text Search\")\n\njina.text_search(endpoint=endpoint)\n\n\nRun Streamlit:\n\nstreamlit run app.py\n\n\nAnd there you have it ‚Äì your very own text search!\n\nFor image search, simply swap out the text code above for our image example code and run a Jina image (like our Pokemon example).\n\nWhat to do next\n\nThanks for reading the article and looking forward to hearing what you think about the component! If you want to learn more about Jina and Streamlit here are some helpful resources:\n\nJina\nStreamlit-Jina component\nJina docs\nJina Fundamentals\nJina hello-world demos\nStreamlit\nStreamlit docs\nComponents gallery\nApp gallery\nCommunity forum\nA big thank you!\n\nMajor thanks to Randy Zwitch, TC Ricks and Amanda Kelly for their help getting our component live. And thanks to all my colleagues at Jina for building the backend that makes this happen!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > January 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-january-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > January 2022\n\nYour January look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, February 7 2022\nüèÜ App of the month üèÜ\nStreamlit January updates\nüîç Current release: 1.5.0\nüîÆ Upcoming\nüì∏ Camera integration\nüéà What's new in Streamlit\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur January featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\ngitlit by Vincent Warmerdam!\n\nThis app is a GitHub Actions scraper for popular open-source repositories. Get insights into testing times and costs needed to run these projects. [code]\n\nStreamlit January updates\n\nLet's take a look at what happened in January.\n\nüîç Current release: 1.5.0\n\nThe latest release is 1.5.0. Recent updates include the favicon defaulting to PNG for transparency and better support for nested HTML. Be sure to check out the changelog to learn more about all of the latest features and fixes.\n\nüîÆ Upcoming\n\nHere are some upcoming features to get excited about:\n\nIn-app share menu\nSt.user\nMultipage apps\n\nCheck out our roadmap app to get a bigger scope of what else we're working on. ü•≥\n\nüì∏ Camera integration\nYou can now upload images to your apps straight from your camera with the new st.camera_input feature. Try it out with the release demo.\n\nüéà What's new in Streamlit\nRecent updates to Streamlit Cloud include the sign-in with email and the ability to share apps with any email address. Read more about these features here.\n\n\n\nFeatured Streamlit content\nLearn Streamlit essentials in this guide by Data Professor outlining all the steps from start to deployment!\n\nCreate interactive presentations with Streamlit using Sebatian's streamlit_book library.\n\nSee how to diagnose blood cancer from DNA data with Streamlit! Check out CloneRetriever by Eitan Halper-Stromberg, the molecular pathologist at Johns Hopkins.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nDaniel's Carbon Intensity App lets you visualize the carbon intensity for the UK by date and can forecast up to 2 days ahead.\nIn his video Pseudo Grid Layout using Streamlit columns, Fanilo shows how to use horizontal layout and columns to improvise a grid layout.\nSay or write a pickup line, and Paul's Pickup Line Prediction Using K-Nearest Neighbors app will determine if it's a success or a failure.\nSimulate and study emergent properties of teamwork with Michael and Chris's SuperScript app.\nLearn all about these 5 Streamlit Components To Build Better Applications in Ahmed's breakdown.\nSebastian's app teaches about the Confusion Matrix in an interactive and fun way.\nLearn how to build a Real-Time Live Finance/Marketing/Data Science Dashboard in Python from 1littlecoder's YouTube tutorial.\nSejal does A Deep Dive into Wordle, the New Pandemic Puzzle Craze with an app to analyze how effective your guesses are.\nSiavash created Koffee of the world‚Äîa simple web app to explore, visualize, and compare coffee quality profiles of different countries.\nFind Google Earth Engine apps by their location, filter by zoom level, and open the app directly in your browser with Philipp's Earth Engine App Finder.\nUse Antoine's Download data from archive.org app to get old URLs for a domain‚Äîa necessary step for fixing migrations.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Screenshot-2023-03-27-at-11.41.31-PM-1.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-27-at-11.41.31-PM-1.png",
    "html": ""
  },
  {
    "title": "hackathonhero.png (2000√ó947)",
    "url": "https://blog.streamlit.io/content/images/2023/05/hackathonhero.png",
    "html": ""
  },
  {
    "title": "Streamlit",
    "url": "https://blog.streamlit.io/december-rewind/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n404\nPage not found\n‚Üê Go to the front page\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "ezgif.com-resize.gif (753√ó461)",
    "url": "https://blog.streamlit.io/content/images/2023/05/ezgif.com-resize.gif#browser",
    "html": ""
  },
  {
    "title": "Screenshot-2023-03-07-at-10.56.55-AM.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-07-at-10.56.55-AM.png",
    "html": ""
  },
  {
    "title": "Screenshot-2023-03-13-at-9.05.07-PM.png (2000√ó943)",
    "url": "https://blog.streamlit.io/content/images/2023/05/Screenshot-2023-03-13-at-9.05.07-PM.png",
    "html": ""
  },
  {
    "title": "10forumquestions-1.png (2000√ó945)",
    "url": "https://blog.streamlit.io/content/images/2023/05/10forumquestions-1.png",
    "html": ""
  },
  {
    "title": "GIF-1.2.gif (1000√ó600)",
    "url": "https://blog.streamlit.io/content/images/2021/08/GIF-1.2.gif",
    "html": ""
  },
  {
    "title": "Main_Secrets_Management_1.gif (1280√ó872)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Main_Secrets_Management_1.gif#border",
    "html": ""
  },
  {
    "title": "_low__2d_balloons-1.gif (800√ó800)",
    "url": "https://blog.streamlit.io/content/images/2021/08/_low__2d_balloons-1.gif#border",
    "html": ""
  },
  {
    "title": "GIF-321.gif (796√ó386)",
    "url": "https://blog.streamlit.io/content/images/2021/08/GIF-321.gif#border",
    "html": ""
  },
  {
    "title": "1-1-1.png (1200√ó607)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-1-1.png",
    "html": ""
  },
  {
    "title": "3-1-1.png (1200√ó700)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-1-1.png#border",
    "html": ""
  },
  {
    "title": "2-2-1.png (1200√ó700)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-2-1.png#border",
    "html": ""
  },
  {
    "title": "3-7.png (1200√ó700)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-7.png#border",
    "html": ""
  },
  {
    "title": "5-8.png (1200√ó512)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-8.png#border",
    "html": ""
  },
  {
    "title": "4-5.png (1200√ó500)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4-5.png#border",
    "html": ""
  },
  {
    "title": "6-2.png (1202√ó729)",
    "url": "https://blog.streamlit.io/content/images/2021/08/6-2.png#browser",
    "html": ""
  },
  {
    "title": "2-10.png (1200√ó512)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-10.png#border",
    "html": ""
  },
  {
    "title": "1-9.png (1200√ó775)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-9.png#border",
    "html": ""
  },
  {
    "title": "3-6.png (1200√ó727)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-6.png#browser",
    "html": ""
  },
  {
    "title": "topics-discussed.png (2000√ó604)",
    "url": "https://blog.streamlit.io/content/images/2022/11/topics-discussed.png#border",
    "html": ""
  },
  {
    "title": "Jessica Smith - Streamlit (Page 2)",
    "url": "https://blog.streamlit.io/author/jessica/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jessica Smith\n26 posts\nMonthly rewind > May 2022\n\nYour May look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2022\nMonthly rewind > April 2022\n\nYour April look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2022\nMonthly rewind > March 2022\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 7 2022\nMonthly rewind > February 2022\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 7 2022\nMonthly rewind > January 2022\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 7 2022\nMonthly rewind > December 2021\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 7 2022\nMonthly rewind > November 2021\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2021\nMonthly rewind > October 2021\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2021\nMonthly rewind > September 2021\n\nYour September look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2021\nMonthly rewind > August 2021\n\nYour August look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Anastasia Glushko - Streamlit",
    "url": "https://blog.streamlit.io/author/anastasia/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Anastasia Glushko\n1 post\nLabeling ad videos with Streamlit\n\nHow Wavo.me uses Streamlit‚Äôs Session State to create labeling tasks\n\nAdvocate Posts\nby\nAnastasia Glushko\n,\nSeptember 2 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "video-summary-function.png (2000√ó782)",
    "url": "https://blog.streamlit.io/content/images/2022/11/video-summary-function.png#border",
    "html": ""
  },
  {
    "title": "sensitive-content-detected.png (2000√ó362)",
    "url": "https://blog.streamlit.io/content/images/2022/11/sensitive-content-detected.png#border",
    "html": ""
  },
  {
    "title": "no-sensitive-content.png (2000√ó212)",
    "url": "https://blog.streamlit.io/content/images/2022/11/no-sensitive-content.png#border",
    "html": ""
  },
  {
    "title": "take2.gif (856√ó464)",
    "url": "https://blog.streamlit.io/content/images/2021/08/take2.gif#border",
    "html": ""
  },
  {
    "title": "2-9.png (1200√ó596)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-9.png#border",
    "html": ""
  },
  {
    "title": "video-summary.png (1514√ó385)",
    "url": "https://blog.streamlit.io/content/images/2022/11/video-summary.png#border",
    "html": ""
  },
  {
    "title": "gif-1-3.gif (1102√ó594)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-1-3.gif#browser",
    "html": ""
  },
  {
    "title": "selected-video-analysis.png (2000√ó574)",
    "url": "https://blog.streamlit.io/content/images/2022/11/selected-video-analysis.png#border",
    "html": ""
  },
  {
    "title": "1-8.png (1174√ó634)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-8.png#border",
    "html": ""
  },
  {
    "title": "analyze-youtube-channel-content.png (2000√ó785)",
    "url": "https://blog.streamlit.io/content/images/2022/11/analyze-youtube-channel-content.png#border",
    "html": ""
  },
  {
    "title": "balloons-1.gif (1280√ó720)",
    "url": "https://blog.streamlit.io/content/images/2021/09/balloons-1.gif#browser",
    "html": ""
  },
  {
    "title": "content-analyzer-app-topics.png (2000√ó1132)",
    "url": "https://blog.streamlit.io/content/images/2022/11/content-analyzer-app-topics.png#browser",
    "html": ""
  },
  {
    "title": "content-analyzer-app-thumbnails.png (2000√ó1049)",
    "url": "https://blog.streamlit.io/content/images/2022/11/content-analyzer-app-thumbnails.png#browser",
    "html": ""
  },
  {
    "title": "app_layout_progress_bar-1.png (1478√ó1206)",
    "url": "https://blog.streamlit.io/content/images/2021/09/app_layout_progress_bar-1.png#browser",
    "html": ""
  },
  {
    "title": "sample_output-1.png (1564√ó88)",
    "url": "https://blog.streamlit.io/content/images/2021/09/sample_output-1.png#border",
    "html": ""
  },
  {
    "title": "app_layout-1.png (1594√ó1596)",
    "url": "https://blog.streamlit.io/content/images/2021/09/app_layout-1.png#browser",
    "html": ""
  },
  {
    "title": "high_medium_low--1-.png (1522√ó1228)",
    "url": "https://blog.streamlit.io/content/images/2021/09/high_medium_low--1-.png#browser",
    "html": ""
  },
  {
    "title": "gif-1.gif (709√ó693)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-1.gif#border",
    "html": ""
  },
  {
    "title": "5.png (1212√ó532)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5.png#border",
    "html": ""
  },
  {
    "title": "4.png (1212√ó489)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4.png#border",
    "html": ""
  },
  {
    "title": "3-1.png (1212√ó648)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-1.png#border",
    "html": ""
  },
  {
    "title": "1-3.png (1212√ó728)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-3.png#browser",
    "html": ""
  },
  {
    "title": "2-1.png (1212√ó592)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-1.png#border",
    "html": ""
  },
  {
    "title": "13.1.png (1200√ó923)",
    "url": "https://blog.streamlit.io/content/images/2021/08/13.1.png#border",
    "html": ""
  },
  {
    "title": "Screen-Recording-2020-12-08-at-03.30.25-PM.gif (720√ó354)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Screen-Recording-2020-12-08-at-03.30.25-PM.gif#border",
    "html": ""
  },
  {
    "title": "Qiusheng Wu - Streamlit",
    "url": "https://blog.streamlit.io/author/qiusheng/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Qiusheng Wu\n2 posts\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCreating satellite timelapse with Streamlit and Earth Engine\n\nHow to create a satellite timelapse for any location around the globe in 60 seconds\n\nAdvocate Posts\nby\nQiusheng Wu\n,\nDecember 15 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "12.1.png (1200√ó344)",
    "url": "https://blog.streamlit.io/content/images/2021/08/12.1.png#border",
    "html": ""
  },
  {
    "title": "10.1.png (1200√ó1056)",
    "url": "https://blog.streamlit.io/content/images/2021/08/10.1.png#border",
    "html": ""
  },
  {
    "title": "11.1.png (1200√ó418)",
    "url": "https://blog.streamlit.io/content/images/2021/08/11.1.png#border",
    "html": ""
  },
  {
    "title": "9.1.png (1200√ó635)",
    "url": "https://blog.streamlit.io/content/images/2021/08/9.1.png#border",
    "html": ""
  },
  {
    "title": "8.1.png (1200√ó681)",
    "url": "https://blog.streamlit.io/content/images/2021/08/8.1.png#border",
    "html": ""
  },
  {
    "title": "5.1.png (1200√ó1036)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5.1.png#browser",
    "html": ""
  },
  {
    "title": "6.1.png (1200√ó765)",
    "url": "https://blog.streamlit.io/content/images/2021/08/6.1.png#border",
    "html": ""
  },
  {
    "title": "GIF_2-1.gif (952√ó806)",
    "url": "https://blog.streamlit.io/content/images/2021/08/GIF_2-1.gif#browser",
    "html": ""
  },
  {
    "title": "GIF_3.gif (952√ó544)",
    "url": "https://blog.streamlit.io/content/images/2021/08/GIF_3.gif#browser",
    "html": ""
  },
  {
    "title": "4.1.png (1179√ó753)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4.1.png#browser",
    "html": ""
  },
  {
    "title": "GIF_1.gif (952√ó671)",
    "url": "https://blog.streamlit.io/content/images/2021/08/GIF_1.gif#browser",
    "html": ""
  },
  {
    "title": "3.1.png (1197√ó769)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3.1.png#browser",
    "html": ""
  },
  {
    "title": "2.1.png (1200√ó455)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2.1.png#browser",
    "html": ""
  },
  {
    "title": "2-7.png (1200√ó1043)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-7.png#border",
    "html": ""
  },
  {
    "title": "1.2.png (1200√ó1085)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1.2.png#border",
    "html": ""
  },
  {
    "title": "Using Data to Combat Pandemic-Related Evictions",
    "url": "https://blog.streamlit.io/open-source-eviction-data/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nArup and New Story use data to help combat pandemic related evictions\n\nMaking data accessible to help address the eviction crisis\n\nBy Jared Stock\nPosted in Advocate Posts, January 7 2021\nEmpowering users, no code required\nMapping it out\nMaking data accessible\nContents\nShare this post\n‚Üê All posts\n\nWritten by Jared Stock, a digital consultant at Arup.\n\nOne of the many consequences of COVID-19 in the US is that countless families are having trouble keeping up with rent. According to Moody‚Äôs, nearly 12 million renters will owe an average of $5,850 in back rent by January 2021. These renters will face the terrifying prospect of losing their homes in the middle of winter during a pandemic.\n\nIn early September, the Centers for Disease Control issued a national moratorium on evictions until December 31st, 2020. Under this order, tenants can provide a declaration to their landlord that shows that they meet certain conditions, which then prevents the landlord from evicting them. However, as the CDC notes in the order, ‚Äúthis Order does not relieve any individual of any obligation to pay rent, make a housing payment, or comply with any other obligation‚Ä¶‚Äù So, while tenants were protected until then, their rent payments continued to accumulate and they could face eviction if they aren‚Äôt able to pay all of the rent back.\n\nOne way to help these families is to simply pay their rent, and that‚Äôs exactly what New Story, a charity based in the Bay Area, set out to do in the spring of 2020. Our team at Arup helped New Story analyze data in the Bay Area to help them make decisions about how to distribute direct payments to families. We looked at a variety of data sources ranging from economic indicators to how vulnerable a county was to COVID-19. We were able to come up with a relative risk index that we used to compare counties and show where the greatest need was. After that work was done, the next logical step was to share our code and data with the community.\n\nWhile some NGOs have people with software and data experience, many more don‚Äôt have the skills required to use the open data that exists. Our goal is to make that data and the analysis that we‚Äôve done around evictions available to as many people as possible to help drive decisions and ultimately keep more people in their homes.\n\nEmpowering users, no code required\n\nWhen we initially published our repository and data, users had to have some coding experience in order to use it - which can be intimidating! That immediately reduced the number of people who could potentially use the data. This was a clear opportunity to use Streamlit to make the data accessible to everyone without needing to understand SQL queries or how to run Python code. Since our initial release was just vanilla Python code, it was easy to start adding functionality by just marking up our code with Streamlit functions. We were able to adapt our existing Python workflows in just a couple hours and deploy using Streamlit sharing in just a couple minutes.\n\nRight off the bat, we were able to turn what were originally command line prompts into visual inputs for users that update as they go through the process. Since this analysis follows a linear sequence of user inputs, Streamlit inputs allow for a much better user experience and allow us to show data at various steps during the analysis in a much nicer format than writing to a command line. We also provide explanation and details right alongside the inputs and data, rather than in a separate README file.\n\nStreamlit‚Äôs input components allowed us to give users a lot more control over what they want to analyze. Instead of making assumptions about housing stock distributions or what features are important to risk, we could surface those assumptions to the user so they could make their own decisions. This level of transparency and control was the inspiration for the other page in the app: the Data Explorer.\n\nThis page is meant to give the users the ability to dive into the data in more detail - allowing a user to look at the values of a single feature in the database and compare the values of two different features. Users can also download the raw data as an Excel document and do whatever they want with it. We‚Äôre hoping to add more flexibility to this page in the future, like being able to compare counties in multiple different states. If you have ideas you‚Äôd like to see implemented, please add a feature request or better yet, contribute a pull request on our GitHub.\n\nMapping it out\n\nWe‚Äôve found that the clearest way for people to understand comparative risk between counties is with a map. Streamlit already has support for simple maps, however, we didn‚Äôt just want to plot points; we wanted to show the county shapes. County shapefiles are common and we could get them into our database relatively easily using PostGIS. They get stored in a format called Well Known Binary (WKB), so we need to get them into a format that can be read by pydeck. ¬†\n\nFirst, I load in the geometry data using Shapely with shapely.wkb.loads(). But immediately we have a problem: the data comes in as a Well Known Text (WTK) object, not as geojson that we can parse. GIS isn't my specialty, so like a good programmer, I looked around Stack Overflow and eventually found some snippets that convert it into sets of coordinates that Python could parse as a dict and then clean it up a bit.\n\ngeo_df['geom'] = geo_df.apply(lambda row: row['geom'].buffer(0), axis=1)\ngeo_df['geom'] = geo_df.apply(lambda row: gpd.GeoSeries(row['geom']).__geo_interface__, axis=1)\ngeo_df['coordinates'] = geo_df.apply(lambda row: clean_coordinates(row), axis=1)\nI later found a better way to do this using Shapely\ndef clean_coordinates(row: pd.Series) -> list:\n    # combine multipolygon into one object as a single polygon\n    for f in row['geom']['features']:\n        if f['geometry']['type'] == 'MultiPolygon':\n            f['geometry']['type'] = 'Polygon'\n            combined = []\n            for i in range(len(f['geometry']['coordinates'])):\n                combined.extend(list(f['geometry']['coordinates'][i]))\n            f['geometry']['coordinates'] = combined\n\n        # flatten coordinates\n        f['geometry']['coordinates'] = f['geometry']['coordinates'][0]\n    return row['geom']\n\nNow we have coordinates that define each shape! I add those to our DataFrame and then I can turn everything into nice, friendly geojson for our map. I create a feature collection with each county's name, shape coordinates, and the values (in our case just one) that we want to display.\n\ndef make_geojson(geo_df: pd.DataFrame, features: list) -> dict:\n    geojson = {\"type\": \"FeatureCollection\", \"features\": []}\n    for i, row in geo_df.iterrows():\n        feature = row['coordinates']['features'][0]\n        props = {\"name\": row['County Name']}\n        for f in features:\n            props.update({f: row[f]})\n        feature[\"properties\"] = props\n        del feature[\"id\"]\n        del feature[\"bbox\"]\n        feature[\"geometry\"][\"coordinates\"] = [feature[\"geometry\"][\"coordinates\"]]\n        geojson[\"features\"].append(feature)\n\n    return geojson\nThis function creates a new geojson object with our data and the specific features/columns that we want to display\n\nNow we can finally show this data in our Streamlit app. In this case, I want to give pydeck a DataFrame with only what we want to show on the map. I turn this geojson into a DataFrame and add fill colors as another column, and then I can create a layer for our shapes and pass that into the st.pydeck_chart() function along with a tooltip to show the value of the feature.\n\n    polygon_layer = pdk.Layer(\n        \"PolygonLayer\",\n        geo_df,\n        get_polygon=\"coordinates\",\n        filled=True,\n        stroked=False,\n        opacity=0.5,\n        get_fill_color='fill_color',\n        auto_highlight=True,\n        pickable=True,\n    )\n    # The brackets here are expected for pdk, so string formatting is less friendly\n    tooltip = {\"html\": \"<b>County:</b> {name} </br>\" + \"<b>\" + str(map_feature) + \":</b> {\" + str(map_feature) + \"}\"}\n\n    r = pdk.Deck(\n        layers=[polygon_layer],\n        initial_view_state=view_state,\n        map_style=pdk.map_styles.LIGHT,\n        tooltip=tooltip\n    )\n    st.pydeck_chart(r)\n\nOnce our data is properly formatted, turning it into a map is pretty straightforward:\n\nBecause the functions to create the map are generalized, it makes expanding on them much easier. For example, if we want to create a map with multiple layers, we could update the existing make_map function to accept a list of features to map and then create multiple of Layers instead of just one. We can use these functions like a component in React or Angular to show different information in multiple places. In fact, I did exactly that to create map view on the Data Explorer page to allow users to see any feature by just changing the inputs to the functions.\n\nMaking data accessible\n\nThis project initially aimed to collect disparate data sources and make our analysis easier for anyone, but not everyone has the Python and data skills to use it on its own. Streamlit allowed us to replace a scary command line interface with a more familiar and functional web app, hopefully allowing more users to interact with data. It also gave us the ability to show the data in more interactive and intuitive ways than we could if users had to run the project locally. While we hope this will help people address the eviction crisis, we think this data also can help address other social problems in policy-making, planning, and other fields.\n\nArup decided to make this project open because we believe we can have a bigger impact by working together, so we‚Äôre eager to work with the community to make this tool more useful. If you have ideas for new functionality, let us know or better yet, contribute to the repository. You can find the app here. The coming months could be a very scary time for lots of people, and we hope that this tool may help in some way to keep more people in their homes.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "gif-2.gif (947√ó633)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-2.gif#browser",
    "html": ""
  },
  {
    "title": "Monthly rewind > July 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-july-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > July 2022\n\nYour July look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, August 9 2022\nüèÜ App of the month üèÜ\nStreamlit July updates\nüîç Current release: 1.11.1\nüîÆ Upcoming\nüíª #30DaysofStreamlit in French\n‚≠êÔ∏è Streamlit reached 20k GitHub Stars!\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur July featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nVisualizing your non-linear career by Michael Condon!\n\nThis app lets you create a timeline of your career, highlighting the top skills used within a job role. The generated polar graph visualizes the amount of experience in each skill set. [code]\n\nStreamlit July updates\n\nHere are some new features and highlights from July!\n\nüîç Current release: 1.11.1\n\nThe latest release is 1.11.1. Recent updates include tab containers for your app via st.tabsand the ability to set a gap size between columns. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nThese new features are currently on the horizon:\n\nUnique and custom subdomains\nReplaying of cached st. calls\nUser-resizable st.sidebar\n\nCheck out our roadmap app to see what else we're working on. ü•≥\n\nüíª #30DaysofStreamlit in French\nThe 30 Days of Streamlit app is now available in French!\n\n‚≠êÔ∏è Streamlit reached 20k GitHub Stars!\n\nFeatured Streamlit content\nCheck out the backstory behind Qiusheng's popular geospatial app and learn how Streamlit helped optimize it.\n\nGet past Google Search Console export limitations and get more out of your data with Charly's new Streamlit app!\n\nLearn about the fundamental st.write and magic commands with Chanin (Data Professor) in this new tutorial.\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nSa≈°a explains how to Use Streamlit to Visualise your ML Models Running on Snowflake in his Medium article.\nGet quantitative analysis of the Brazilian financial market with Roberto's An√°liseQuant app.\nLearn How to Add a Background Image to Your Streamlit App by reading Sharone's tutorial.\nBogdan's Options Calculator lets you scrape live option chains from Yahoo Finance and visualize the bid/ask/last traded price for different strike prices.\nIn Khuyen's tutorial you'll learn how to Build a Robust Workflow to Visualize Trending GitHub Repositories in Python filtered by your favorite language.\nUse Mykola's STAC Discovery app to browse and discover data from the Microsoft Planetary Computer Catalog.\nArvindra created a Media Explorer App‚Äîa hybrid architecture media server, media service, and Streamlit client app by using FastAPI and Python.\nThe Renewcast app by Giannis provides forecasts for renewable electricity generation in European countries.\nDanny goes over Using Streamlit and Plotly to Create Interactive Candlestick Charts in his tutorial.\nBilly's Surfboard Volume Calculator predicts the best surfboard volume for you based on your user input.\nThe Global Fund API Explorer from Adrien visualizes disbursements information from the GF API with several filter options.\nDavid's Espectro app allows users to query spectral indices and visualize the required bands for an index computation.\nThe Gene Updater app by Kuan and team autocorrects and updates Excel misidentified gene names.\nSelect two different emojis and get a combination of them with Zachary's Emoji Kitchen Streamlit app.\n\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > August 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-august-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > August 2022\n\nYour August look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, September 7 2022\nüèÜ App of the month üèÜ\nStreamlit August updates\nüîç Current release: 1.12.0\nüîÆ Upcoming\nüìä New and improved built-in charts\nüéô Take a deep dive into data science with Adrien\nFeatured Streamlit content\n‚ú® The magic of working in open source\nüîΩ Auto-generate a dataframe filtering UI in Streamlit\nüíª Make dynamic filters in Streamlit and show their effects on the original dataset\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur August featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nWhat songs were popular when I was in high school? by Robert Ritz!\n\nHaving a hard time discovering playlists on Spotify? Robert's app lets you choose a time period and then auto-generates a playlist of the top songs from those years. Check it out to discover new music or if you want to go on a trip down memory lane. [code]\n\nStreamlit August updates\n\nTake a look at these updates and highlights from August!\n\nüîç Current release: 1.12.0\n\nThe latest release is 1.12.0. Recent updates include improved built-in charts, resizable side bar, and the ability to put static elements into functions that are cached via st.experimental_memo or st.experimental_singleton. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nThese new features are currently on the horizon:\n\nUnique and custom subdomains\nAbility to hide space for an empty label\nApps are indexed by Google\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\nüìä New and improved built-in charts\nRelease 1.12.0 brings newly improved built-in charts. Line, area, and bar charts have gained a new design as well as parameters 'x' and 'y' for more control.\n\nüéô Take a deep dive into data science with Adrien\nA Deep Dive into Data Science with Adrien Treuille, CEO and Co-founder of Streamlit (Acquired by Snowflake) - The Data Cloud Podcast\nIn this episode, Adrien Treuille, Co-founder of Streamlit (acquired by Snowflake), shares how business people can use data science and machine learning, how you should incorporate data scientists into your organizations, and so much more.\nSnowflake\nFeatured Streamlit content\n\n‚ú® The magic of working in open source\nSee what it's like to work in open source! Ken writes about how new features are implemented, the importance of community, and how you can contribute.\n\nüîΩ Auto-generate a dataframe filtering UI in Streamlit\nWant to add a filtering UI to your dataframe without a bunch of code iterations? Learn from Tyler, Arnaud, and Zachary about the filter_dataframe function.\n\nüíª Make dynamic filters in Streamlit and show their effects on the original dataset\nVladimir guides you how to quickly and easily add dynamic filters to your Streamlit app.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nAvery walks you through how to make a Sports Analytics Project With Python & Streamlit that will look great on your data science portfolio.\nLearn from Jesse how to Build a Static Code Analysis App with Streamlit and Python in his YouTube tutorial.\nTimotius' app visualizes various aspects of Gender Equality in Indonesia.\nExplore the data of the Vietnam National Team with Daryl's app.\nEugenia teaches you How to build a Web App to Transcribe and Summarize audio with Python in her article.\nThe Agriculture, Water, and Climate Group at the University of Manchester created AquaPlan to enable farmers, businesses, and governments to make more informed decisions about water.\nAdd a Custom Streamlit Background Image/Color Gradient through CSS to your apps by following Fanilo's video tutorial.\nGet quick overview of your Netflix data with Sebastian's Netflix Profile Analysis app.\nGenerate unique QR codes with advanced customization using the Flash QR app by Gerard.\nWith William's Shakespeare Search Engine app, you can semantically search the works of Shakespeare.\nLearn and share more about your personality by taking the tests in Rinku's Findself app.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > September 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-september-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > September 2022\n\nYour September look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, October 7 2022\nüèÜ App of the month üèÜ\nStreamlit September updates\nüîç Current release: 1.13.0\nüîÆ Upcoming\nFeatured Streamlit content\nüß© How to build your own Streamlit component\nüì¶ Streamlit App Starter Kit: How to build apps faster\nüíª How to build Streamlit apps on Replit\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur September featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nAbstractonator by Crossref Labs!\n\nThis app is a game that tests if you can distinguish machine-generated content (MGC) from human-generated content (HGC). Try to guess which scholarly abstracts are the human-generated originals and which are AI-generated! [code]\n\nStreamlit September updates\n\nLet's take a look at some updates and highlights from September.\n\nüîç Current release: 1.13.0\n\nThe latest release is 1.13.0. Recent updates include the ability to optionally hide widget labels and to stretch st.dataframe across the full container width. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nWe're currently working on these new features:\n\nStreamlit theme for 3rd party charting libraries\nApps are indexed by Google\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\nFeatured Streamlit content\n\nüß© How to build your own Streamlit component\nLearn how to create your own custom component in this tutorial from Zachary.\n\nüì¶ Streamlit App Starter Kit: How to build apps faster\nSave 10 minutes every time you build an app with the Data Professor's Streamlit App Starter Kit‚Äîa GitHub template to kick-start your creations!\n\nüíª How to build Streamlit apps on Replit\nEasily build apps on Replit and explore Streamlit elements with the Beginner Template Tour from Streamlit Creator Shruti.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nFanilo tackles a bunch of community questions in his video Streamlit Pro answers YOUR questions & messages ‚Ä¢ 1k AMA.\nSephora's NLLB-200 translator for lyrics demo app lets you translate lyrics from popular songs to 200 languages, including low-resource languages.\nRoll the dice and see the distribution of the sum of die results with Bruno's Playing dice! app.\nWith Koen's SEO A/B Test Analyzer you can validate almost all SEO changes and optimizations before they're implemented on your website.\nValeria's Tarot Reading App will give you a three-card spread on your past, present, and future.\nUse Mykola's app to discover Land Surface Temperature - River Basins in Europe and the USA.\nNicholas attempts a fun coding challenge in his YouTube video: I tried to build a Python Machine Learning Streamlit App in 7 Minutes.\nYuichiro's stlite sharing lets you write and share Streamlit apps online serverlessly!\nSam's tutorial Python-Based Data Viz (With No Installation Required) walks you through setting up a Streamlit app to share with stlite.\nWith Zakaria's Distillation Column app, you can estimate the required performance of a given system for a specified reflux rate and then use the values to help size a complete distillation column.\nIn his YouTube tutorial, Fede teaches you How to code and deploy a Streamlit App in less than 20 minutes!\nArvindra made a mini-tutorial app to help with Initializing widget values and getting them to stick without double presses.\nArjune's first Streamlit app, HR Analytics, visualizes a summary of each department and each employee in the organization.\nUse Michael's Drug Target Prediction app to find molecules with similar structural properties that can serve as a source for new drugs.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > October 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-october-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > October 2022\n\nYour October look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, November 8 2022\nüèÜ App of the month üèÜ\nStreamlit October updates\nüîç Current release: 1.14.0\nüîÆ Upcoming\nüì¶ The next frontier for Streamlit\nü™¢ ¬†Discover and share useful bits of code with the streamlit-extras library\nüñåÔ∏è Prototype your app in Figma!\nFeatured Streamlit content\nüíª How to build Streamlit apps on Replit\nüß© uPlanner fosters data processing innovation with Streamlit\nüìÑ Build a Streamlit Form Generator app to avoid writing code by hand\nüìÑ How to make a Streamlit links page\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur October featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nSophisticated Palette by Siavash Yasini!\n\nThis app creates color palettes from famous artwork or any image of your choice! Simply choose an image and get inspiration for the colors of your projects. You can adjust the colors of the image and customize the palette and sample pixel size. Copy the code snippet provided in the app to adopt the colors into your matplotlib and plotly charts. [code]\n\nStreamlit October updates\n\nCheck out the latest updates and releases from October.\n\nüîç Current release: 1.14.0\n\nThe latest release is 1.14.0. Recent updates include the ability to set st.button and st.form_submit_button as primary or secondary, and the ability to limit the number of options selectable for st.multiselect. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nWe're currently working on these new features:\n\nApps are indexed by Google\nMarkdown in widget labels\nStreamlit theme for 3rd party charting libraries\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\nüì¶ The next frontier for Streamlit\nWe're excited to announce our feature roadmap for 2023 and beyond! What lies ahead for Streamlit? We‚Äôre thinking bigger than ever.\n\nü™¢ ¬†Discover and share useful bits of code with the streamlit-extras library\nCheck out the streamlit-extras library by Arnaud‚Äîa collection of small but mighty custom components to give your apps that extra something!\n\nüñåÔ∏è Prototype your app in Figma!\nWant to make sure your app's design is on point before jumping into the code? Check out the Streamlit Design System in Figma to prototype ahead of time.\n\n\nFeatured Streamlit content\n\nüíª How to build Streamlit apps on Replit\nEasily build apps on Replit and explore Streamlit elements with the Beginner Template Tour from Streamlit Creator Shruti.\n\nüß© uPlanner fosters data processing innovation with Streamlit\nSee how Streamlit enables Sebastian's team at uPlanner to build apps for custom scripts and simplify the user experience, resulting in faster innovation.\n\nüìÑ Build a Streamlit Form Generator app to avoid writing code by hand\nLearn from Gerard how to build a Streamlit Form Generator‚Äîan app for making code that accepts and validates user inputs according to an API spec.\n\nüìÑ How to make a Streamlit links page\nShare all of your social links in one place! Learn how to make a Streamlit links page in this tutorial from the Data Professor. You can also add a custom subdomain!\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nYou can explore Cycling Rates in London and customize your maps with Lisa's app.\nVansh's app lets you visualize Solar Flux Data from the Canadian Space Agency.\nIn his video tutorial, Andrej explains how to go about the Streamlit Python App Setup.\nCreate beautiful timelines with bi-directional communication using Qiusheng's New Component: Streamlit Timeline.\nInput a YouTube link to Batuhan's Auto Subtitled Video Generator and it will display the video with subtitles.\nIntellipaat's video tutorial takes you through Building a Machine Learning Web Application using Streamlit.\nGenerate synthetic walking trials of healthy synthetic subjects using Luca's app GaitGAN: Generating Gait Data by Generative Adversarial Networks for Data Augmentation in Biomechanics.\nSee Vizzu's animated story in a Streamlit app showcasing Data Scientists‚Äô Presentation Tools.\nCarlos created a simple Toggle Switch Component using React/Material-UI.\nShruti's Heart-shaped toggle switch component adds a fun shape to the toggle switch.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > June 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-june-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > June 2022\n\nYour June look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, July 6 2022\nüèÜ App of the month üèÜ\nStreamlit June updates\nüîç Current release: 1.10.0\nüîÆ Upcoming\nüìà Multipage apps\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur June featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nLearning log by Tyler Hillery!\n\nInspired by GitHub's contribution graphs, Tyler created this app to track the time he spends on learning. You can see the date of the learning session, the length of time, what was studied and where, along with his contextual notes. The app keeps Tyler accountable to his learning goals and helps visualize his progress! [code]\n\nStreamlit June updates\n\nA lot of exciting things happened in June! Let's take a look.\n\nüîç Current release: 1.10.0\n\nThe latest release is 1.10.0. Recent updates include native support for multipage apps and the redesigned st.dataframe. Be sure to check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nThese new features are currently on the horizon:\n\nUnique and custom subdomains\nReplaying of cached st. calls\nUser-resizable st.sidebar\n\nCheck out our roadmap app to see what else we're working on. ü•≥\n\nüìà Multipage apps\nThe long-awaited multipage feature has finally arrived! Now you can quickly and easily add more pages to your Streamlit apps.\n\nFeatured Streamlit content\nMake your static charts interactive using st.pyplot.\n\nRead how Marcelo started his career in data science and how Streamlit helps further his projects at iFood.\n\nLearn how Samuel made an interactive app to teach students about Earth observation‚Äîusing multiple Streamlit features and community components!\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nGet a glimpse at reported UFO sightings with Tyler's UFO Sightings Dashboard in Streamlit.\nWith Subham's Data Pre-processing and Model Selection Web App you can see the impact of different parameters on different models and select the best combination.\nLearn how to Build your own Fast Text Reader with Streamlit and Spacy in Ramsri's YouTube tutorial.\nGet started with Streamlit using Shruti's Streamlit Beginner Template Tour.\nAnirudh's app allows you to Visualize Sound in an image‚Äîincluding in real-time!\nFollow Dr. W.J.B. Mattingly's video to learn How to Make a Multi-Page App with Streamlit Pages in Python‚ÄîThe BEST Streamlit Update.\nCreate network graphs with data from Genshin Impact in MiracleXYZ's Genshin Analysis app.\nShreya teaches ways to customize your apps in her Microblog: Customising Python Streamlit Dashboards with Custom Themes and More!\nBanjo walks through creating a Streamlit app and showcases the flexibility for building solutions in his talk Building Interactive Apps in Python with Streamlit.\nGet help with cheminformatics tasks using Suneel's rdKit CheatSheet.\nEasily create apps with Sebastian's Template for a multipaging Streamlit app.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mark von Oven - Streamlit",
    "url": "https://blog.streamlit.io/author/vonoven/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mark von Oven\n1 post\nTwitter\nThe Stable solves its data scalability problem with Streamlit\n\nHow Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days\n\nCase study\nby\nMark von Oven and¬†\n1\n¬†more,\nApril 28 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Wissam Siblini - Streamlit",
    "url": "https://blog.streamlit.io/author/wissam/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Wissam Siblini\n1 post\nWissam Siblini uses Streamlit for pathology detection in chest radiographs\n\nLearn how Wissam detected thoracic pathologies in medical images\n\nCase study\nby\nWissam Siblini and¬†\n1\n¬†more,\nMay 3 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly Rewind - Streamlit (Page 2)",
    "url": "https://blog.streamlit.io/tag/monthly-rewind/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Monthly Rewind\n27 posts\nMonthly rewind > May 2022\n\nYour May look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJune 7 2022\nMonthly rewind > April 2022\n\nYour April look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMay 5 2022\nMonthly rewind > March 2022\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 7 2022\nMonthly rewind > February 2022\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 7 2022\nMonthly rewind > January 2022\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 7 2022\nMonthly rewind > December 2021\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 7 2022\nMonthly rewind > November 2021\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2021\nMonthly rewind > October 2021\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2021\nMonthly rewind > September 2021\n\nYour September look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2021\nMonthly rewind > August 2021\n\nYour August look back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "gif-1-2.gif (957√ó634)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-1-2.gif#browser",
    "html": ""
  },
  {
    "title": "Radius-Explorer-GIF-1.gif (1920√ó1119)",
    "url": "https://blog.streamlit.io/content/images/2022/04/Radius-Explorer-GIF-1.gif#border",
    "html": ""
  },
  {
    "title": "Chest-Radiographs-Classifier-GIF.gif (1724√ó729)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Chest-Radiographs-Classifier-GIF.gif#border",
    "html": ""
  },
  {
    "title": "st.camera_input.png (2514√ó1510)",
    "url": "https://blog.streamlit.io/content/images/2022/01/st.camera_input.png",
    "html": ""
  },
  {
    "title": "trs_mark-1.jpeg (1368√ó470)",
    "url": "https://blog.streamlit.io/content/images/2022/04/trs_mark-1.jpeg#border",
    "html": ""
  },
  {
    "title": "hack_day2-1.jpeg (1049√ó612)",
    "url": "https://blog.streamlit.io/content/images/2022/04/hack_day2-1.jpeg#border",
    "html": ""
  },
  {
    "title": "patent_pending-5.jpeg (800√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/04/patent_pending-5.jpeg#border",
    "html": ""
  },
  {
    "title": "Gallery--10-FPS-.gif (1700√ó991)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Gallery--10-FPS-.gif#border",
    "html": ""
  },
  {
    "title": "Image-13.01.22.png (1247√ó951)",
    "url": "https://blog.streamlit.io/content/images/2022/01/Image-13.01.22.png",
    "html": ""
  },
  {
    "title": "sign-in-2.png (2000√ó1112)",
    "url": "https://blog.streamlit.io/content/images/2022/01/sign-in-2.png",
    "html": ""
  },
  {
    "title": "gravitational.gif (2878√ó1530)",
    "url": "https://blog.streamlit.io/content/images/2022/08/gravitational.gif#browser",
    "html": ""
  },
  {
    "title": "How to quickly deploy and share your machine learning model for drug discovery",
    "url": "https://blog.streamlit.io/how-to-quickly-deploy-and-share-your-machine-learning-model-for-drug-discovery/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to quickly deploy and share your machine learning model for drug discovery\n\nShare your ML model in 3 simple steps\n\nBy Sebastian Ayala Ruano\nPosted in Advocate Posts, December 15 2022\nHow to create a user-friendly interface for retrieving molecular inputs\nHow to display important data from the input molecules\nHow to deploy and use an ML model for predicting potential drugs\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nDrug discovery is a long and complicated process. Bringing a new drug to the market can take decades and cost millions. Antimicrobial peptides or AMPs (bioactive drugs that control infectious diseases caused by superbugs) and ML algorithms can help, but it‚Äôs hard to deploy computational models for public feedback.\n\nThat‚Äôs why I built AMPredST‚Äîa web application that predicts antimicrobial activity and general AMP properties (based on my previous project).\n\nIn this post, I‚Äôll show you how Streamlit can extend your ML models‚Äô use for drug discovery. You‚Äôll learn:\n\nHow to create a user-friendly interface for retrieving molecular inputs\nHow to display important data from the input molecules\nHow to deploy and use an ML model for predicting potential drugs\nüëâ\nCan‚Äôt wait to try it out? Here‚Äôs the app and the repository with all the code.\nHow to create a user-friendly interface for retrieving molecular inputs\n\nFirst, use the st.session_state to store the sequence input and share this value between the app‚Äôs reruns. Then add functions for defining default values of active and inactive peptides and removing the input sequence.\n\nInclude your app‚Äôs inputs and default functionalities in the sidebar:\n\nAn input widget ( st.sidebar.text_input) for entering the peptide sequence\nThree buttons (st.sidebar.button) for implementing the previously defined functions\n\nAfter getting the input, the main page of the app will show a sub-header (st.subheader) and an informational message (st.info) of the input sequence (these values change depending on the information provided in the input widget from the sidebar):\n\nif 'peptide_input' not in st.session_state:\n  st.session_state.peptide_input = ''\n\n# Input peptide\nst.sidebar.subheader('Input peptide sequence')\n\ndef insert_active_peptide_example():\n    st.session_state.peptide_input = 'LLNQELLLNPTHQIYPVA'\n\ndef insert_inactive_peptide_example():\n    st.session_state.peptide_input = 'KSAGYDVGLAGNIGNSLALQVAETPHEYYV'\n\ndef clear_peptide():\n    st.session_state.peptide_input = ''\n\npeptide_seq = st.sidebar.text_input('Enter peptide sequence', st.session_state.peptide_input, key='peptide_input', help='Be sure to enter a valid sequence')\nst.sidebar.button('Example of an active AMP', on_click=insert_active_peptide_example)\nst.sidebar.button('Example of an inactive peptide', on_click=insert_inactive_peptide_example)\nst.sidebar.button('Clear input', on_click=clear_peptide\n\nif st.session_state.peptide_input == '':\n  st.subheader('Welcome to the app!')\n  st.info('Enter peptide sequence in the sidebar to proceed', icon='üëà')\nelse:\n  st.subheader('‚öõÔ∏è Input peptide:')\n  st.info(peptide_seq)\n\n\nHow to display important data from the input molecules\n\nAfter getting the input sequence, you can calculate the properties that characterize the molecule. Analyze the protein sequences from the Biopython library with functions like molecular_weight, gravy, or aromaticity (read more here).\n\nUse st.metric with st.columns to display these quantities:\n\nif st.session_state.peptide_input != '':\n\t\t# General properties of the peptide\n\t\tst.subheader('General properties of the peptide')\n\t\tanalysed_seq = ProteinAnalysis(peptide_seq)\n\t\tmol_weight = analysed_seq.molecular_weight()\n\t\taromaticity = analysed_seq.aromaticity()\n\t\tinstability_index = analysed_seq.instability_index()\n\t\tisoelectric_point = analysed_seq.isoelectric_point()\n\t\tcharge_at_pH = analysed_seq.charge_at_pH(7.0)\n\t\tgravy = analysed_seq.gravy()\n\t\t\n\t\tcol1, col2, col3 = st.columns(3)\n\t\tcol1.metric(\"Molecular weight (kDa)\", millify(mol_weight/1000, precision=3))\n\t\tcol2.metric(\"Aromaticity\", millify(aromaticity, precision=3))\n\t\tcol3.metric(\"Isoelectric point\", millify(isoelectric_point, precision=3))\n\t\t\n\t\tcol4, col5, col6 = st.columns(3)\n\t\tcol4.metric(\"Instability index\", millify(instability_index, precision=3))\n\t\tcol5.metric(\"Charge at pH 7\", millify(charge_at_pH, precision=3))\n\t\tcol6.metric(\"Gravy index\", millify(gravy, precision=3))\n\n\nStreamlit has functions for creating interactive plots using Python libraries like Matplotlib, Altair, or Plotly. Create an interactive bar plot of the input peptide‚Äôs amino acid composition with Plotly (st.plotly_chart):\n\n# Bar plot of the aminoacid composition\ncount_amino_acids = analysed_seq.count_amino_acids()\n\nst.subheader('Aminoacid composition')\n\ndf_amino_acids =  pd.DataFrame.from_dict(count_amino_acids, orient='index', columns=['count'])\ndf_amino_acids['aminoacid'] = df_amino_acids.index\ndf_amino_acids['count'] = df_amino_acids['count'].astype(int)\n\nplot = px.bar(df_amino_acids, y='count', x='aminoacid',\n        text_auto='.2s', labels={\n                \"count\": \"Count\",\n                \"aminoacid\": \"Aminoacid\"\n            })\nplot.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", showlegend=False)\nst.plotly_chart(plot)\n\n\nFinally, calculate the frequency of the amino acids from the peptide sequence and show the dataframe with st.write. The ML algorithm will use this feature matrix as input to predict the activity of the peptide sequence:\n\n# Calculate Amino acid composition feature from the AMP\namino_acids_percent = analysed_seq.get_amino_acids_percent()\n\namino_acids_percent = {key: value*100 for (key, value) in amino_acids_percent.items()}\n\ndf_amino_acids_percent =  pd.DataFrame.from_dict(amino_acids_percent, orient='index', columns=['frequencies']).T\n\nst.subheader('Molecular descriptors of the peptide (Amimoacid frequencies)')\n\nst.write(df_amino_acids_percent)\n\n\nHow to deploy and use an ML model for predicting potential drugs\n\nNow, you can use your ML to predict promising new drugs. But first, ensure your model‚Äôs file size doesn‚Äôt exceed GitHub limits (to deploy to Streamlit Community Cloud).\n\nCompress the model‚Äôs bin file into a zip file and use the zip file library to load the model into the script (or use the Git Large File Storage extension):\n\n# Function to unzip the model\n@st.experimental_singleton\ndef unzip_model(zip_file_name):\n    # opening Zip using 'with' keyword in read mode\n    with zipfile.ZipFile(zip_file_name, 'r') as file:\n        # printing all the information of archive file contents using 'printdir' method\n        print(file.printdir())\n        # extracting the files using 'extracall' method\n        print('Extracting all files...')\n        file.extractall()\n        print('Done!') # check your directory of a zip file to see the extracted files\n\n# Assigning zip filename to a variable\nzip_file_name = 'ExtraTreesClassifier_maxdepth50_nestimators200.zip'\n\n# Unzip the file with the defined function\nunzip_model(zip_file_name)\n\n\nNext, load the ML model using the pickle library and predict the peptide activity:\n\n# Function load the best ML model\n@st.experimental_singleton \ndef load_model(model_file):\n  with open(model_file, 'rb') as f_in:\n      model = pickle.load(f_in)\n  return model\n\n# Load the model\nmodel_file = 'ExtraTreesClassifier_maxdepth50_nestimators200.bin'\nmodel = load_model(model_file)\n\ny_pred = model.predict_proba(df_amino_acids_percent)[0, 1]\nactive = y_pred >= 0.5\n\n\nI recommend you create cache-decorated functions for the previous tasks using the st.experimental_singleton decorator, which will store an object for each function and share it across all users connected to the app.\n\nFinally, show the model‚Äôs results on the app‚Äôs main page. If you want, color the text to highlight the active and inactive resulting peptides using the amazing annotated_text Streamlit component:\n\n# Print results\nst.subheader('Prediction of the AMP activity using the machine learning model')\nif active == True:\n  annotated_text(\n  (\"The input molecule is an active AMP\", \"\", \"#b6d7a8\"))\n  annotated_text(\n  \"Probability of antimicrobial activity: \",\n  (f\"{y_pred}\",\"\", \"#b6d7a8\"))\nelse:\n  annotated_text(\n  (\"The input molecule is not an active AMP\", \"\", \"#ea9999\"))\n  annotated_text(\n  \"Probability of antimicrobial activity: \",\n  (f\"{y_pred}\",\"\", \"#ea9999\"))\n\n\nWrapping up\n\nCongratulations! You‚Äôve learned how to deploy your ML models for drug discovery in an app that can help the entire scientific community!\n\nIf you have any comments or suggestions, please leave them below in the comments, email me, DM me on Twitter, or create an issue in the repo.\n\nThank you for reading, and happy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "how-to-use-roboflow.gif (1052√ó624)",
    "url": "https://blog.streamlit.io/content/images/2022/08/how-to-use-roboflow.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (569√ó651)",
    "url": "https://blog.streamlit.io/content/images/2022/04/Untitled--1-.png#border",
    "html": ""
  },
  {
    "title": "scienceio-app-1.gif (850√ó1090)",
    "url": "https://blog.streamlit.io/content/images/2023/01/scienceio-app-1.gif#border",
    "html": ""
  },
  {
    "title": "dashboard_model_2.PNG (1843√ó884)",
    "url": "https://blog.streamlit.io/content/images/2022/05/dashboard_model_2.PNG#browser",
    "html": ""
  },
  {
    "title": "csv2excel.gif (1660√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/10/csv2excel.gif#browser",
    "html": ""
  },
  {
    "title": "Building robust Streamlit apps with type-checking",
    "url": "https://blog.streamlit.io/building-robust-streamlit-apps-with-type-checking/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding robust Streamlit apps with type-checking\n\nHow to make type-checking part of your app-building flow\n\nBy Harald Husum\nPosted in Advocate Posts, November 10 2022\nThe methods of detecting defects\nStatic analysis vs. dynamic analysis\nType-checking\nWhat does this have to do with Streamlit anyway?\nHow to make type-checking part of your app-building flow\nStep 1. Understanding type hints\nStep 2. Installing mypy\nStep 3. Running mypy\nAn example from the world of Streamlit\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Harald Husum. I‚Äôm a Machine Learning Engineer at Intelecy and a Streamlit Creator.\n\nHave you ever come across an app that promises to solve your problem, only to have it crash as you try it out? Nothing ruins the experience like encountering defects. As app developers, we‚Äôre painfully aware of this. Defects are an ever-present concern, so having efficient routines for weeding them out is a must.\n\nIn this post, I‚Äôll share a powerful technique for eliminating defects‚Äîtype-checking‚Äîand how to make it a part of your app-building flow.\n\nLet‚Äôs get started!\n\nüí°\nIf you want to get a feel for how type-checking works, take a look at my app which lets you type-check Streamlit code directly from your browser (without installing the required tooling). If you‚Äôre interested in the code, here‚Äôs a repo as well.\nThe methods of detecting defects\n\nFor some classes of software defects, the only cure is meticulous testing.\n\nWhen creating an app, writing and running tests should always be part of your toolbox. Not sure about how to test Streamlit apps? These tutorials can help you get started. But testing can be time-consuming, and covering various edge cases of a large code base isn‚Äôt always feasible. As software developers, we‚Äôre often time-constrained, so we want fast and efficient methods for detecting defects.\n\nLuckily, testing isn‚Äôt the only technique that can surface problems in your code.\n\nStatic analysis vs. dynamic analysis\n\nWhen you want to uncover as many issues as possible with as little work as possible, static analysis tools are your friends.\n\nStatic analysis is the process of analyzing code without actually running it. Compare it to testing‚Äîor dynamic analysis‚Äîwhere to gain any insight into the code, you have to execute it. Of course, static analysis tools can‚Äôt discover all your problems, so you should still write tests. But you can catch a lot of issues without investing much effort.\n\nWithin the context of Python, linting is probably the most common form of static analysis. You might‚Äôve come across linting tools like Pylint and Flake8. They help detect potential quality issues in your code. Code formatters like YAPF and Black also rely on static analysis. However, automatic linting and formatting only scratch the surface of what static analysis can do for you, as you‚Äôll see shortly. üòä\n\nType-checking\n\nIn the last decade, Python has seen a popularity boom. As a result, Python increasingly sees use in large and complex projects, where demands for code quality are higher. This trend drives the development and adoption of static analysis techniques. Type-checking is one such technique that‚Äôs gaining traction in the Python community.\n\nType-checking ensures that interacting with objects doesn‚Äôt lead to obvious errors. For example, calling x.lower() will work if x is of type str. But if x is an int, you‚Äôll trigger the following exception:\n\nAttributeError: 'int' object has no attribute 'lower‚Äô\n\nYou might ask, ‚ÄúWho would try to call the lower method on an integer?‚Äù But variants of this problem happen all the time‚Äîby accident‚Äîwhen you write or refactor Python code. Embracing type-checking will let you avoid such errors.\n\nLike other forms of static analysis, type-checking is usually performed by a tool‚Äîa type checker. There are many type checkers in the Python ecosystem. Of note are PyType, Pyright, and Pyre. But the most popular is probably mypy. It‚Äôs been around the longest and supports most of the latest typing features in Python. It‚Äôll help you make your Streamlit projects less prone to failure.\n\nWhat does this have to do with Streamlit anyway?\n\nGood question! Although type checkers can identify issues in your regular old Python code, they work best when your code is annotated with type hints. Type hints inform the type checker about the intention behind the code and allow the type checker to verify that the code functions as intended.\n\nOver the last few months, the community has done a lot of work annotating Streamlit‚Äôs public APIs with type hints. Type checkers can now correctly understand what types are expected by Streamlit functions and what types are returned. Consequently, type checkers have gone from being somewhat useful to really useful for checking code that interacts with Streamlit, like your apps. (I expect the Streamlit code base annotations to improve with time).\n\nIf you're not already type-checking your Streamlit apps, now is a good time to start!\n\nHow to make type-checking part of your app-building flow\n\nTo get a feel for type-checking without committing to anything, use my app to type-check Streamlit code snippets from a browser:\n\nGo to https://typing-playground.streamlitapp.com/\nPaste in a code snippet\nPress ‚ÄúType-check with mypy‚Äù\nObserve how mypy feels about your code\n\nThe process should look something like this:\n\nThe app internals are fairly simple. The user input is passed on to mypy for type checking. The resulting type-checking report is somewhat prettified before being presented to the user (check out the repo for details).\n\nNow, let‚Äôs make type-checking a part of your Streamlit app-building workflow.\n\nStep 1. Understanding type hints\n\nEarlier, I mentioned that type hints are essential to get the most out of a type checker. Let‚Äôs have a closer look at Python-type hints and what they express (if you‚Äôre using Python < 3.10, some examples won't work because of the union operator for types (|) addition, unless you add from __future__ import annotations to the top of your file).\n\nThe first is a variable hint. It looks something like this:\n\nn: float = 42.42\n\nThe variable n should always have a float value.\n\nBy adding : float after your variable name, you inform the type checker that you intend that n should always remain a float. Should you make a mistake and write something like n = ‚ÄúI‚Äôm no float‚Äù later in your code, mypy will inform you that you made a mistake:\n\nerror: Incompatible types in assignment (expression has type \"str\", variable has type \"float\")\n\nIn practice, you won't have to annotate n like this because mypy implicitly assumes that variables have the type of whatever value you assign them first. But knowing about variable hints will let you override mypy when this assumption doesn't hold.\n\nFor instance, imagine that we want to turn n into a constant and disallow changing it at all. We can achieve this by way of a special variable hint:\n\nfrom typing import Final\n\nN: Final = 42.42\nN is annotated as a constant.\n\nIf we now try to change N in any way, it will lead to mypy complaining:\n\nN = 42  # error: Cannot assign to final name \"N\"\nMypy stops us from overriding a constant.\n\nAnother form of type hints is the one used on functions.\n\ndef greet(name: str | None) -> str:\n    if name is None:\n        name = \"mysterious stranger\"\n    return f\"Happy Streamlit-ing, {name}! üéà\"\n\n\nHere, name: str | None means that the function has a parameter, name, and that it‚Äôs expected to be passed an argument that‚Äôs either an instance of str or None. The function also has a return type annotation -> str, which implies that it‚Äôll always return an instance of str.\n\nAdding these hints to the function means that mypy will protect you against two potential mistakes:\n\n# Trying to pass in a type the function isn't intended to support\n# Mypy - error: Argument 1 to \"greet\" has incompatible type \"int\"; \n#   expected \"Optional[str]\"\ngreet(42)\n\n# Misusing the returned greeting\n# Mypy - error: \"str\" has no attribute \"a_method_strings_dont_have\"\ngreet(\"John Doe\").a_method_strings_dont_have()\n\n\nLet‚Äôs say you make the following change to the greet function:\n\ndef greet(name: str | None) -> str:\n    if name is None:\n        # No greetings for secretive people\n        return None\n    return f\"Happy Streamlit-ing, {name}! üéà\"\n\n\nMypy will notice that you say you‚Äôre going to return a str. But in practice, you can also return a None. Mypy will warn you: error: Incompatible return value type (got \"None\", expected \"str\").\n\nThis is helpful because it forces you to think carefully about your change. Either greet should always return a str, in which case you need to take a step back. Or, if you intended to create this new behavior, update the function hints to def greet(name: str | None) -> str | None. As a result, all usages of greet that don't gracefully handle the function returning None will need to be updated.\n\nThis might sound scary, but mypy will point out any locations in your code base where you might be using the return value from greet in a way that assumes it‚Äôs a str.\n\nüí°\nThere‚Äôs more to type hints. Take a look at PEP484‚Äîthe main specification for type hints in Python.\nStep 2. Installing mypy\n\nTo type-check your Streamlit project, you need to install mypy. It should be easy, as it's just another Python dependency. Just use pip:\n\n$ python3 -m pip install mypy\n\n\nOr use Poetry to manage your dependencies:\n\n$ poetry add mypy --group dev\n\nStep 3. Running mypy\n\nMypy can analyze a single file:\n\n$ mypy program.py\n\n\nOr it can analyze all Python code in a directory:\n\n$ mypy my_project\n\n\nTry running it on your code and see if it reports any errors. If it doesn't, you're a better developer than me. üòâ\n\nAn example from the world of Streamlit\n\nTo move from theoretical to practical, I want to share a type of error I experienced in one of my Streamlit apps. It could've been avoided with the type hints added in the latest Streamlit version. I can't share the actual code, so I'll share an imagined example.\n\nHere is a simple app to explore the pastry collections of a few local bakeries. It lets me select a bakery and one of its pastries to see the information about it:\n\n\"\"\"This is the code for our bakery app\"\"\"\nfrom dataclasses import dataclass\n\nimport streamlit as st\n\n@dataclass\nclass Pastry:\n    name: str\n    description: str\n\n@dataclass\nclass Baker:\n    name: str\n    pastries: list[Pastry]\n\nbakers = [\n    Baker(\n        name=\"Eager Bakery\",\n        pastries=[\n            Pastry(\n                name=\"Cinnamon Bun\",\n                description=\"The best there is\",\n            ),\n            Pastry(\n                name=\"Magic Muffin\",\n                description=\"Putting sparkles back in your day.\",\n            ),\n            Pastry(\n                name=\"Dreamy Donut\",\n                description=\"Never drink your coffee without it.\",\n            ),\n        ],\n    ),\n    Baker(\n        name=\"Lazy Bakin'\",\n        pastries=[],\n    ),\n]\n\nbaker = st.sidebar.selectbox(\n    label=\"Select a bakery:\",\n    options=bakers,\n    format_func=lambda b: b.name,\n)\n\npastry = st.sidebar.selectbox(\n    label=f\"Select one of {baker.name}'s pastries:\",\n    options=baker.pastries,\n    format_func=lambda p: p.name,\n)\nst.write(pastry.name)\nst.write(pastry.description)\n\n\nTake a look at the code. Can you spot a subtle defect? As you interact with the app, it seems there are no problems:\n\nBut when you switch the bakeries, an error strikes:\n\nInstead of information about a pastry, you get an AttributeError. How annoying!\n\nWhen I encountered this error in my app, I was surprised to learn the cause. It turns out that streamlit.selectbox will return None if the options sequence you pass to it is empty. In this case, since Lazy Bakin' disappointingly hasn't developed any pastries yet, there are no options to select in the pastry selectbox. So instead of a pastry to render, you get None.\n\nI found this through a runtime error‚Äîin production, no less. But with the latest version of Streamlit and the support of mypy, you won't have to. Running mypy yields four errors:\n\npastries.py:50: error: Item \"None\" of \"Optional[Baker]\" has no attribute \"name\"\npastries.py:51: error: Item \"None\" of \"Optional[Baker]\" has no attribute \"pastries\"\npastries.py:54: error: Item \"None\" of \"Optional[Any]\" has no attribute \"name\"\npastries.py:55: error: Item \"None\" of \"Optional[Any]\" has no attribute \"description\"\n\n\nTo summarize, mypy is alerting you to the fact that both your baker and pastry variables might be set to None. So if mypy is part of your project's CI/CD pipeline, you'll avoid releasing code like this to your users.\n\nHere is how to adapt your code to avoid these errors:\n\nbaker = st.sidebar.selectbox(\n    label=\"Select a bakery:\",\n    options=bakers,\n    format_func=lambda b: b.name,\n)\n\n# Since we know that `bakers` is a non-empty list, the errors mypy points out \n# for `baker` are technically false positives. This is easy enough to deal with.\n# By asserting that baker is not None, mypy is placated. And, in practice, the \n# assert should never fail.\nassert baker is not None\n\npastry = st.sidebar.selectbox(\n    label=f\"Select one of {baker.name}'s pastries:\",\n    options=baker.pastries,\n    format_func=lambda p: p.name,\n)\n\n# For the `pastry` case, we must show a bit more care. We now know that we can \n# actually get a None value here. What you want to do when that happens is up \n# to you. In my case, I just choose not to print anything. Moving the \n# `st.write` calls behind a conditional is all that is needed to make mypy happy.\nif pastry is not None:\n    st.write(pastry.name)\n    st.write(pastry.description)\n\n\nThat's it!\n\nWrapping up\n\nYou've now seen how type-checking can help you build more robust Streamlit apps! üéâ\n\nThank you for taking the time to read this article. I hope that it'll help you with your current and future Streamlit projects. Should you have any thoughts, comments, or questions about type annotations and type-checking, please post them in the comments below or connect with me on Twitter, LinkedIn, or GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Building Your Reddit Clone | Streamlit & Firestore",
    "url": "https://blog.streamlit.io/streamlit-firestore-continued/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit ‚ù§Ô∏è Firestore (continued)\n\nAka the NoSQL sequel: Building a Reddit clone and deploying it securely\n\nBy Austin Chen\nPosted in Tutorials, April 22 2021\nRecap\nPart 3: Building your Reddit clone\nReading one post\nWriting one post\nReading ALL posts\nStreamlit Widgets and Firestore\nPart 4: Securely deploying on Streamlit sharing\nConvert our JSON key into a secrets.toml file\nIgnoring our local secret files in git\nAdding the secret to Streamlit sharing\nWrapping up\nResources\nContents\nShare this post\n‚Üê All posts\nRecap\n\nLast time, in Parts 1 & 2, we walked through all the necessary steps to set up Streamlit, Streamlit sharing, and Firestore as well as went over what Firestore is and how it can help in your Streamlit apps.\n\nToday, we'll dive into the exciting stuff: In Part 3 we'll code up the Firestore Reddit app, then in Part 4 we'll add secrets and make it live for the whole world!\n\nPart 3: Building your Reddit clone\nReading one post\n\nLet's start by replacing our streamlit_app.py , to check that Firestore is indeed set up correctly:\n\nimport streamlit as st\nfrom google.cloud import firestore\n\n# Authenticate to Firestore with the JSON account key.\ndb = firestore.Client.from_service_account_json(\"firestore-key.json\")\n\n# Create a reference to the Google post.\ndoc_ref = db.collection(\"posts\").document(\"Google\")\n\n# Then get the data at that reference.\ndoc = doc_ref.get()\n\n# Let's see what we got!\nst.write(\"The id is: \", doc.id)\nst.write(\"The contents are: \", doc.to_dict())(\n\n\nWhat is doc_ref here? It's short for \"document reference\". You can think of a reference to be like the title of a book that you want to get from the library. Creating a reference is really fast, but there's no data in the reference itself. In order to download the data that the reference is talking about, you have to call .get() on that reference ‚Äî which, going by the analogy, is when the librarian takes the title and fetches the corresponding book. Creating a doc_ref is really fast; calling .get() can be a lot slower!\n\nüí° Pro-tip: Remember, it's $ streamlit run streamlit_app.py to run your code locally, and you will see it on http://localhost:8501. I'd encourage you to turn on \"Always rerun\" in the top right hamburger menu of your app at this point, so that Streamlit will automatically update each time you save your Python code.\n\nIf this is what you see after running through the above:\n\nThen congrats! Your Streamlit app has successfully read from the Firestore database.\n\nWriting one post\n\nNext up, let's create a new document directly in Python. Start the same way ‚Äî by making a doc_ref, but this time filling in the ID you want to use for your new document. Then, you can call .set() with a Python dictionary containing the data you wanted. Here's what that looks like:\n\n# This time, we're creating a NEW post reference for Apple\ndoc_ref = db.collection(\"posts\").document(\"Apple\")\n\n# And then uploading some data to that reference\ndoc_ref.set({\n\t\"title\": \"Apple\",\n\t\"url\": \"www.apple.com\"\n})\n\nReading ALL posts\n\nIf we don't know the ID of the post we want, or want to list all of the posts, that's straightforward too! Form a reference to the whole collection of posts, and call .stream() to get a list of docs you can iterate through.\n\nYou may have noticed already, but for each doc we get, we convert it to a Python dict using .to_dict() so that we can work with it. As with any Python dict, you can then get its values with bracket notation (doc.to_dict()[\"url\"]).\n\n# Now let's make a reference to ALL of the posts\nposts_ref = db.collection(\"posts\")\n\n# For a reference to a collection, we use .stream() instead of .get()\nfor doc in posts_ref.stream():\n\tst.write(\"The id is: \", doc.id)\n\tst.write(\"The contents are: \", doc.to_dict())\n\n\nThat's all the database operations we'll need to build our Reddit clone! Now we can start diving into combining these database calls with our Streamlit widgets to build the full app.\n\nStreamlit Widgets and Firestore\n\nTo let our users create new posts, we can use 3 widgets:\n\nA st.text_input to write in the title of the post\nA st.text_input for the url\nAnd a st.button to allow the user to submit\n\nThen, let's clean up the rendering of the posts to use some nice Markdown formatting. Here's what we have so far in streamlit_app.py\n\nimport streamlit as st\nfrom google.cloud import firestore\n\ndb = firestore.Client.from_service_account_json(\"firestore-key.json\")\n\n# Streamlit widgets to let a user create a new post\ntitle = st.text_input(\"Post title\")\nurl = st.text_input(\"Post url\")\nsubmit = st.button(\"Submit new post\")\n\n# Once the user has submitted, upload it to the database\nif title and url and submit:\n\tdoc_ref = db.collection(\"posts\").document(title)\n\tdoc_ref.set({\n\t\t\"title\": title,\n\t\t\"url\": url\n\t})\n\n# And then render each post, using some light Markdown\nposts_ref = db.collection(\"posts\")\nfor doc in posts_ref.stream():\n\tpost = doc.to_dict()\n\ttitle = post[\"title\"]\n\turl = post[\"url\"]\n\n\tst.subheader(f\"Post: {title}\")\n\tst.write(f\":link: [{url}]({url})\")\n\n\nWhich turns into this Streamlit app:\n\nIsn't that cool? In less than 30 lines of code, you've made a web app that creates new posts and saves them. Your database means that all this data is backed up. Run this Streamlit app on another browser tab (or even another computer entirely) and you'll see the exact same data! And changes on one app will be shared to EVERY app üòÆ\n\nPart 4: Securely deploying on Streamlit sharing\n\nüí° Pro-tip: To use secrets in your Streamlit deployed apps you'll first need an invite to sharing. Request an invite here if you're not already in the beta!\n\nAll right. Before we deploy this lovely Reddit clone to Streamlit sharing so that the whole world can access it, there's one thing we need to take care of. Remember the firestore-key.json file, the password that our Python code uses to sign in to Firestore? If we commit and push that file onto GitHub, it would be like sharing your password with the entire internet...\n\nThis is where Secrets comes in! Secrets are a way to pass in information that your app needs to know, but you don't want to publish on GitHub. Here's how we can securely upload our Firestore key:\n\nConvert our JSON key into a secrets.toml file\n\nJSON and TOML are two different file formats, but the core idea is pretty similar - they both make it easy to pass around a bunch of string keys and their corresponding values. (One way to think of JSON & TOML are as representations of a Python dictionary, but written as a file.) Firestore gave us our secrets as a JSON file, but Streamlit secrets expect a TOML; let's convert between them with a Python script!\n\nGo ahead and copy this code into a new script, key-to-toml.py\n\nimport toml\n\noutput_file = \".streamlit/secrets.toml\"\n\nwith open(\"firestore-key.json\") as json_file:\n    json_text = json_file.read()\n\nconfig = {\"textkey\": json_text}\ntoml_config = toml.dumps(config)\n\nwith open(output_file, \"w\") as target:\n    target.write(toml_config)\n\n\nAnd then run the script:\n\n$ python key-to-toml.py\n\n\nYour ¬†firestore-key.json has now been written out to .streamlit/secrets.toml! Now we can update our Streamlit app to use this new TOML file when initializing the Firestore library:\n\n# Replace:\ndb = firestore.Client.from_service_account_json(\"firestore-key.json\")\n\n# With:\nimport json\nkey_dict = json.loads(st.secrets[\"textkey\"])\ncreds = service_account.Credentials.from_service_account_info(key_dict)\ndb = firestore.Client(credentials=creds, project=\"streamlit-reddit\")\n\n\nWhen you're done, double-check your Streamlit app ‚Äî everything should work the same, reading and writing from Firestore. That's because the new st.secrets knows to look for a file called .streamlit/secrets.toml when Streamlit is running on your local machine!\n\nIgnoring our local secret files in git\n\nNow that we've converted our secret key, we're almost ready to push the code to GitHub. We just need to configure git to ignore our secret files, done by the handy-dandy .gitignore file:\n\nsecrets.toml\nfirestore-key.json\n\n\nNow, all changes in these two secret files will be safely excluded when you push your code. Let's do that now. From a command line:\n\n$ git commit -am 'Read and write to Firestore, securely!'\n$ git push\n\n\nYou can look on GitHub to see that your code is all there, minus the two secret files~\n\nAdding the secret to Streamlit sharing\n\nNormally, a git push is all we need to update any Streamlit sharing app. But because we're adding secrets, we need paste those secrets to the sharing dashboard. You can do this by copying the entire contents of .streamlit/secrets.toml, and pasting it here:\n\nAaaand that's it! Go to your app; you should see it deployed to the entire world. Congratsüéà\n\nWrapping up\n\nThat's the end of this tutorial... but it doesn't have to be the end of your app! There's a lot of other cool things you can do with Firestore in Streamlit, such as:\n\nNesting documents inside of documents (e.g. to implement upvotes and comments)\nForming complex queries (e.g. getting the 10 most recent posts created by Alice)\nListening for updates in REAL TIME (e.g. to make a chat app)\n\nThere's so many awesome places to go from here ‚Äî I can't wait to see what you make. Share your work with the entire community by posting below in the comments!\n\nResources\nFirestore\nStreamlit docs\nStreamlit Github\nStreamlit Forum\nSharing sign-up\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > December 2022",
    "url": "https://blog.streamlit.io/monthly-rewind-december-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, January 9 2023\nüèÜ App of the month üèÜ\nStreamlit December updates\nüîç Current release: 1.16.0\nüîÆ Upcoming\nüìä Streamlit theme for Altair and Plotly charts\nüñºÔ∏è Social share preview\n‚≠êÔ∏è Streamlit Education program\nFeatured Streamlit content\nüíä How to quickly deploy and share your machine learning model for drug discovery\nüéµ Find the top songs from your high school years with a Streamlit app\nüîê Streamlit-Authenticator, Part 1: Adding an authentication component to your app\nüì∫ New tutorials on Streamlit YouTube:\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur December featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nFeather AI by Josh Cadorette and David Hoffman.\n\nFeather AI is an audio-to-text summarizer app built in 1 week during a hackathon! To use the app, all you need to do is add a link to a podcast or YouTube video, enter your email, and get a polished summary in your inbox in 5-10 minutes. It can be used for many use cases, such as summarizing earnings calls, meetings, or webinars.\n\nStreamlit December updates\n\nBelow are the latest updates and releases from December.\n\nüîç Current release: 1.16.0\n\nThe latest release is 1.16.0. Recent updates include a new Streamlit theme for Altair and Plotly charts and colored text in st.markdown. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nNew features to get excited about:\n\nDe-experimentalize faster rerun\nAbility to serve static files\nEditable dataframes\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\nüìä Streamlit theme for Altair and Plotly charts\nThere's a new Streamlit theme for Altair and Plotly charts! Now your charts will integrate better with the rest of your app's design.\nüñºÔ∏è Social share preview\nIntroducing share previews for Community Cloud apps. You'll see a generated image and description previewing your app when you share it.\n\n‚≠êÔ∏è Streamlit Education program\nWe're launching two new education programs!\nüçé Educator Ambassadors‚Äîsupports educators using Streamlit in the classroom.\nüéí Student Ambassadors‚Äîguides students from building apps to mastering Streamlit.\nFeatured Streamlit content\n\nüíä How to quickly deploy and share your machine learning model for drug discovery\nLearn how to deploy and share an ML model for drug discovery! Sebasti√°n goes over how to create the UI, display important data, and use the model for prediction.\n\nüéµ Find the top songs from your high school years with a Streamlit app\nWant to rediscover music from your high school days? Learn from Robert how to build an app to generate Spotify playlists of top songs from selected years.\n\nüîê Streamlit-Authenticator, Part 1: Adding an authentication component to your app\nCheck out Streamlit-Authenticator‚Äîa custom component by Mohammad to help with apps that require user authentication and privileges.\n\nüì∫ New tutorials on Streamlit YouTube:\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nSee and navigate awesome components with Johannes' Streamlit components hub app.\nUsing GPT3 and DALL-E, Nikolas's Generate Tweets app will create Tweets for you and upload them to your account!\nLuca's Molecular icons generator lets you generate and customize icons from SMILES, Name, Cas-number, Inchi, InChIKey, loaded molecule file, or SMILES list.\nLearn about the streamlit-pandas component and how to Easily Make DataFrame App with Streamlit Pandas (Only 2 lines of Python!) from William.\nExperiment with Ultralytics Yolov5 models using Robin's yolov5-ui app.\nOzgur's Euro league stats app decides the most productive 5 team members in selected matches.\nWith Ali's Cross Chain Monitoring Tool, you can compare +10 blockchains in different sectors and view the performance of each blockchain.\nIndraneel created his Curated app to provide awesome resources for learning new skills.\nThe Psychiatric Medications Side Effects app by Nadya and team lets you input a person's psychiatric medications and conveniently displays all their side effects for clearer understanding.\nGet insights about the impact of 5G on the top-level management of Atliquo with Jegadheesh's Atliquo Telecom - Performance app.\nListen to the Streamlit Community Song by Tom John, with lyrics written by ChatGPT.\nPrit's Constellation Explorer computes a transit schedule from the biggest satellite constellations to see which are passing over your location.\nCatch up on some Streamlit features you didn't know existed‚Ä¶in this video from Coding is Fun.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nOctober 2022\nNovember 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Untitled.gif (800√ó567)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled.png (1115√ó556)",
    "url": "https://blog.streamlit.io/content/images/2022/04/Untitled.png#border",
    "html": ""
  },
  {
    "title": "ezgif.com-gif-maker.gif (800√ó602)",
    "url": "https://blog.streamlit.io/content/images/2022/08/ezgif.com-gif-maker.gif#browser",
    "html": ""
  },
  {
    "title": "Raunaq Malhotra - Streamlit",
    "url": "https://blog.streamlit.io/author/raunaq/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Raunaq Malhotra\n1 post\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "scienceio-journal.gif (840√ó420)",
    "url": "https://blog.streamlit.io/content/images/2023/01/scienceio-journal.gif#border",
    "html": ""
  },
  {
    "title": "Gaurav Kaushik - Streamlit",
    "url": "https://blog.streamlit.io/author/gaurav/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Gaurav Kaushik\n1 post\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "4.png (2354√ó1840)",
    "url": "https://blog.streamlit.io/content/images/2022/08/4.png#browser",
    "html": ""
  },
  {
    "title": "text-search.png (736√ó986)",
    "url": "https://blog.streamlit.io/content/images/2023/01/text-search.png#border",
    "html": ""
  },
  {
    "title": "text-search-Oct-05-2022-13-37-42.gif (858√ó1040)",
    "url": "https://blog.streamlit.io/content/images/2022/12/text-search-Oct-05-2022-13-37-42.gif#browser",
    "html": ""
  },
  {
    "title": "3.png (2355√ó1849)",
    "url": "https://blog.streamlit.io/content/images/2022/08/3.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--5-.png (442√ó96)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled--5-.png",
    "html": ""
  },
  {
    "title": "Untitled--4-.png (458√ó170)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Untitled--4-.png",
    "html": ""
  },
  {
    "title": "concatenate.gif (1660√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/10/concatenate.gif#browser",
    "html": ""
  },
  {
    "title": "2.png (2356√ó1833)",
    "url": "https://blog.streamlit.io/content/images/2022/08/2.png#browser",
    "html": ""
  },
  {
    "title": "1.png (2357√ó1831)",
    "url": "https://blog.streamlit.io/content/images/2022/08/1.png#browser",
    "html": ""
  },
  {
    "title": "gif-1-1.gif (1241√ó821)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-1-1.gif#browser",
    "html": ""
  },
  {
    "title": "Make dynamic filters in Streamlit and show their effects on the original dataset",
    "url": "https://blog.streamlit.io/make-dynamic-filters-in-streamlit-and-show-their-effects-on-the-original-dataset/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMake dynamic filters in Streamlit and show their effects on the original dataset\n\nQuickly and easily add dynamic filters to your Streamlit app\n\nBy Vladimir Timofeenko\nPosted in Tutorials, August 25 2022\n1. Build dynamic filters\n2. Transform data\n3. Use Sankey chart\n4. Show generated SQL statements\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn this tutorial, I'll show you how to add dynamic filters to your Streamlit app with a bit of functional Python programming. As an example, we'll be using a Streamlit demo app that connects to Snowflake and retrieves the data by using Snowpark Python, pushing the computations into a Snowflake warehouse.\n\nYou‚Äôll learn how to:\n\nBuild dynamic filters\nTransform the data\nUse Sankey chart\nShow generated SQL statements\n\nWant to dive right in? Here's the repo code that also contains a set of SQL statements to seed the tables with the data.\n\n1. Build dynamic filters\n\nFirst, let‚Äôs take a look at the app‚Äôs architecture:\n\nYour data lives in Snowflake. To present it in Streamlit:\n\nInstall Streamlit;\nSet up Streamlit secrets;\nFill in the boilerplate that connects Streamlit to Snowflake Snowpark:\n# main.py\n# Initialize connection.\ndef init_connection() -> Session:\n    return Session.builder.configs(st.secrets[\"snowpark\"]).create()\n\nif __name__ == \"__main__\":\n    # Initialize the filters\n    session = init_connection()\n\n\nTo make your app look beautiful, use the sidebar and the main area to visually separate the filters. Build out the UI with draw_sidebar and draw_main_ui:\n\nNext, implement the dynamic filters in the sidebar. The data is retrieved from Snowflake and transformed by using dataframes, so your filter class will be the bridge between the Streamlit framework and the Snowpark dataframes.\n\nLet‚Äôs use Python‚Äôs awesome dataclasses as the basis for the class. It provides a great framework for writing a class and has some convenient methods you‚Äôll be using.\n\nTo control the widget presentation, you‚Äôll need a human_name field (when the page prompts for inputs and it‚Äôs showing information in the charts) and a widget_type field (to create an interactive element).\n\nTo access your filter‚Äôs state across the application, give it a widget_id.\n\nSince the filters can't be both static (an on/off checkbox) and dynamic and depend on the source data (slider), you need a way to store the maximum value that‚Äôs retrieved from the table - instance field _max_value.\n\nI used a checkbox and slider widgets as an example, but you can extend the code to work with other Streamlit components.\n\nYour starting class will look like this:\n\n@dataclass\nclass OurFilter:\n    \"\"\"This dataclass represents the filter that can be optionally enabled.\n\n    It is created to parametrize the creation of filters from Streamlit and to keep the state.\"\"\"\n    human_name: str\n    widget_type: callable  # Should be one of st.checkbox or st.select_slider. Other elements could be implemented similarly\n    widget_id: str\n    is_enabled: bool = False  # Controls whether the filter has been enabled. Useful for filtering the list of filters\n    _max_value: int = 0\n\n\nNext, add the other side‚Äîthe one that will work with Snowpark to filter the data.\n\nYou‚Äôll need a way to connect to Snowflake so the Snowpark session could be shared among all instances of class. Cue class variables to the rescue!\n\nI‚Äôm working with a single table in Snowflake, so I‚Äôll keep the table_name as yet another class variable. Each filter represents its own column in the table, so it‚Äôll be table_column.\n\nFor the OurFilter class to have a generic interface, keep the name of the dataframe‚Äôs filtering method as an internal variable:\n\nfrom dataclasses import dataclass\nfrom typing import ClassVar\nfrom snowflake.snowpark.session import Session\n\n@dataclass\nclass OurFilter:\n    \"\"\"This dataclass represents the filter that can be optionally enabled.\n\n    It is created to parametrize the creation of filters from Streamlit and to keep the state.\"\"\"\n    # Class variables\n    table_name: ClassVar[str]\n    session: ClassVar[Session]\n\n    # The name to display in UI\n    human_name: str\n    # Column in the table which will be used for filtering\n    table_column: str\n    # ID of the streamlit widget\n    widget_id: str\n    # The type of streamlit widget to generate\n    widget_type: callable\n    # Field to track if the filter is active. Can be used for filtering the list of filters\n    is_enabled: bool = False\n    # max value\n    _max_value: int = 0\n    # dataframe method that will be used for filtering the data\n    _df_method: str = \"\"\n\n\nYou can define the __post_init__ dataclass method to automatically populate the value of _max_value and control the value of _df_method. This hides the internals of implementation from the class interface:\n\ndef __post_init__(self):\n        if self.widget_type not in (st.select_slider, st.checkbox):\n            raise NotImplemented\n\n        if self.widget_type is st.select_slider:\n            self._df_method = \"between\"\n            self._max_value = (\n                self.session.table(MY_TABLE)\n                .select(max(col(self.table_column)))\n                .collect()[0][0]\n            )\n        elif self.widget_type is st.checkbox:\n            self._df_method = \"__eq__\"\n\n\nLet‚Äôs add a way to render OurFilter in Streamlit and show some text to the user:\n\ndef create_widget(self):\n        if self.widget_type is st.select_slider:\n            base_label = \"Select the range of\"\n        elif self.widget_type is st.checkbox:\n            base_label = \"Is\"\n        else:\n            base_label = \"Choose\"\n        widget_kwargs = dict(label=f\"{base_label} {self.widget_id}\", key=self.widget_id)\n        if self.widget_type is st.select_slider:\n            widget_kwargs.update(\n                dict(\n                    options=list(range(self.max_value + 1)),\n                    value=(0, self.max_value),\n                )\n            )\n        # Invocation of the streamlit method to place the widget on the page\n\t\t\t\t# e.g. st.checkbox(**widget_kwargs)\n        self.widget_type(**widget_kwargs)\n\n\nTo apply the filters to a sequence of dataframes, let‚Äôs make the class callable. This will allow us to use them more expressively:\n\ndef __call__(self, _table: Table):\n        \"\"\"This method turns this class into a functor allowing to filter the dataframe.\n\n        This allows to call it like so:\n\n        f = OurFilter(...)\n        new_table = last_table[f(last_table)]\"\"\"\n        return methodcaller(self.df_method, **(self._get_filter_value()))(\n            _table[self.table_column.upper()]\n        )\n\ndef _get_filter_value(self):\n        \"\"\"Custom unpack function that retrieves the value of the filter\n        from session state in a format compatible with self._df_method\"\"\"\n        _val = st.session_state.get(self.widget_id)\n        if self.widget_type is st.checkbox:\n            # For .eq\n            return dict(other=_val)\n        elif self.widget_type is st.select_slider:\n            # For .between\n            return dict(lower_bound=_val[0], upper_bound=_val[1])\n        else:\n            raise NotImplemented\n\n\n\nIf you rewrite the main.py to import the filter class and draw the sidebar, you‚Äôll see the dynamic filters on the page:\n\nfrom typing import Iterable\n\nimport streamlit as st\nfrom lib.filterwidget import OurFilter\nfrom toolz import pluck\n\nMY_TABLE = \"CUSTOMERS\"\n\ndef _get_active_filters() -> filter:\n    return filter(lambda _: _.is_enabled, st.session_state.filters)\n\ndef _is_any_filter_enabled() -> bool:\n    return any(pluck(\"is_enabled\", st.session_state.filters))\n\ndef _get_human_filter_names(_iter: Iterable) -> Iterable:\n    return pluck(\"human_name\", _iter)\n\ndef draw_sidebar():\n    \"\"\"Should include dynamically generated filters\"\"\"\n\n    with st.sidebar:\n        selected_filters = st.multiselect(\n            \"Select which filters to enable\",\n            list(_get_human_filter_names(st.session_state.filters)),\n            [],\n        )\n        for _f in st.session_state.filters:\n            if _f.human_name in selected_filters:\n                _f.enable()\n\n        if _is_any_filter_enabled():\n            with st.form(key=\"input_form\"):\n\n                for _f in _get_active_filters():\n                    _f.create_widget()\n                st.session_state.clicked = st.form_submit_button(label=\"Submit\")\n        else:\n            st.write(\"Please enable a filter\")\n\nif __name__ == \"__main__\":\n    # Initialize the filters\n    session = init_connection()\n    OurFilter.session = session\n    OurFilter.table_name = MY_TABLE\n\n    st.session_state.filters = (\n        OurFilter(\n            human_name=\"Current customer\",\n            table_column=\"is_current_customer\",\n            widget_id=\"current_customer\",\n            widget_type=st.checkbox,\n        ),\n        OurFilter(\n            human_name=\"Tenure\",\n            table_column=\"years_tenure\",\n            widget_id=\"tenure_slider\",\n            widget_type=st.select_slider,\n        ),\n        OurFilter(\n            human_name=\"Weekly workout count\",\n            table_column=\"average_weekly_workout_count\",\n            widget_id=\"workouts_slider\",\n            widget_type=st.select_slider,\n        ),\n    )\n\n    draw_sidebar()\n\n\nThis will produce something like this:\n\nNow that you have the filter presentation working, let‚Äôs see how those filters apply to the data and how it‚Äôs presented to the user.\n\n2. Transform data\n\nTo show what effects the filters have on the dataset, preserve some data references. Since the OurFilter class already contains the logic to perform the filtering and hides it behind an interface, the transformation will be pretty light:\n\n# main.py\n\ndef draw_main_ui(_session: Session):\n    \"\"\"Contains the logic and the presentation of the main section of the UI\"\"\"\n    if _is_any_filter_enabled():  # Do not run any logic if no filters are actually enabled\n\n        customers: Table = _session.table(MY_TABLE)\n        table_sequence = [customers]\n\n        _f: MyFilter\n        for _f in _get_active_filters():\n            # This block generates the sequence of dataframes as continually applying AND filtering set by the sidebar\n            # The dataframes are to be used in the Sankey chart.\n\n            # First, get the last dataframe in the list\n            last_table = table_sequence[-1]\n            # Apply the current filter to it\n            new_table = last_table[\n                # At this point the filter will be applied to the dataframe using the __call__ method\n                _f(last_table)\n            ]\n            # And save it in the sequence\n            table_sequence += [new_table]\n        \n        st.header(\"Dataframe preview\")\n\n        st.write(table_sequence[-1].sample(n=5).to_pandas().head())\n    else:\n        st.write(\"Please enable a filter in the sidebar to show transformations\")\n\n\nWith this bit of code in the main.py file, the preview of all applied filters will show up in Streamlit:\n\n3. Use Sankey chart\n\nSankey chart (or Sankey diagram) shows the user how the data flows through the filter (the filtering effect). To dynamically visualize it as a graph, use the Sankey class from plotly.graph_objects and Streamlit‚Äôs built-in integration with Plotly library.\n\nThe interface for plotly.graph_objects.Sankey looks like this (color-coded):\n\nThe source and the target lists describe which labels are connected, and the value describes the size of the flow.\n\nTo generate the labels as well as the link, create a few helper functions that‚Äôll take the table_sequence from the code above and produce the needed values for the visualization:\n\n# lib/chart_helpers.py\nfrom typing import Iterable, List, Tuple\n\nfrom snowflake.snowpark.table import Table\n\ndef mk_labels(_iter: Iterable[str]) -> Tuple[str, ...]:\n    \"\"\"Produces the labels configuration for plotly.graph_objects.Sankey\"\"\"\n    first_label = \"Original data\"\n    return (first_label,) + tuple(map(lambda _: f\"Filter: '{_}'\", _iter)) + (\"Result\",)\n\ndef mk_links(table_sequence: List[Table]) -> dict:\n    \"\"\"Produces the links configuration for plotly.graph_objects.Sankey\"\"\"\n    return dict(\n        source=list(range(len(table_sequence))),\n        target=list(range(1, len(table_sequence) + 1)),\n        value=[_.count() for _ in table_sequence],\n    )\n\n\nNow go back to main.py to show the Sankey chart on the page:\n\n# main.py\nimport plotly.graph_objects as go\n# ...\n \ndef draw_main_ui(_session: Session):\n\t\tif _is_any_filter_enabled():\n\t\t\t\t# ...\n\t\t\t\t# Generate the Sankey chart\n\t\t\t\tfig = go.Figure(\n\t\t\t\t    data=[\n\t\t\t\t        go.Sankey(\n\t\t\t\t            node=dict(\n\t\t\t\t                pad=15,\n\t\t\t\t                thickness=20,\n\t\t\t\t                line=dict(color=\"black\", width=0.5),\n\t\t\t\t                label=mk_labels(_get_human_filter_names(_get_active_filters())),\n\t\t\t\t            ),\n\t\t\t\t            link=mk_links(table_sequence),\n\t\t\t\t        )\n\t\t\t\t    ]\n\t\t\t\t)\n\t\t\t\tst.header(\"Sankey chart of the applied filters\")\n\t\t\t\tst.plotly_chart(fig, use_container_width=True)\n\n\nThe visualization shows how the filters are applied to the original dataset:\n\n4. Show generated SQL statements\n\nSince the program already tracks the sequence of transformations, you can show which SQL statements will produce the same results. To generate them from the table_sequence and show them in Streamlit, use st.markdown and build the table element with this code:\n\n# main.py\n# ...\n \ndef draw_main_ui(_session: Session):\n\t\tif _is_any_filter_enabled():\n\t\t\t\t# ...\n\t\t\t\t# Add the SQL statement sequence table\n\t\t\t\tstatement_sequence = \"\"\"\n\t\t\t\t| number | filter name | query, transformation |\n\t\t\t\t| ------ | ----------- | --------------------- |\"\"\"\n\t\t\t\tst.header(\"Statement sequence\")\n\t\t\t\tfor number, (_label, _table) in enumerate(\n\t\t\t\t    zip(\n\t\t\t\t        mk_labels(_get_human_filter_names(_get_active_filters())),\n\t\t\t\t        table_sequence,\n\t\t\t\t    )\n\t\t\t\t):\n\t\t\t\t    statement_sequence += f\"\"\"\\\\n| {number+1} | {_label} | ```{_table.queries['queries'][0]}``` |\"\"\"\n\t\t\t\t\n\t\t\t\tst.markdown(statement_sequence) \n\n\n\nAs the filters are applied, this code runs and maintains the table of statements:\n\nWrapping up\n\nAnd that‚Äôs how you can implement dynamic filters in Streamlit! It takes only a few lines of code to add them to your app and let the user see their effects.\n\nHave any questions or want to share a cool app you made? Join us on the forum, tag us on Twitter, or let us know in the comments below.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Screen-Shot-2021-07-16-at-4.14.55-PM.png (1186√ó296)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Screen-Shot-2021-07-16-at-4.14.55-PM.png#border",
    "html": ""
  },
  {
    "title": "Randy Zwitch - Streamlit",
    "url": "https://blog.streamlit.io/author/randyzwitch/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Randy Zwitch\n2 posts\nTwitter\n6 tips for improving your Streamlit app performance\n\nMoving your Streamlit app from analysis to production\n\nTutorials\nby\nRandy Zwitch\n,\nJuly 20 2021\nTesting Streamlit apps using SeleniumBase\n\nHow to create automated visual tests\n\nTutorials\nby\nRandy Zwitch\n,\nNovember 23 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "screen-shot-2021-07-16-at-3.47.33-pm--1-.png (866√ó146)",
    "url": "https://blog.streamlit.io/content/images/2021/10/screen-shot-2021-07-16-at-3.47.33-pm--1-.png#border",
    "html": ""
  },
  {
    "title": "metric.jpeg (1063√ó588)",
    "url": "https://blog.streamlit.io/content/images/2022/05/metric.jpeg#browser",
    "html": ""
  },
  {
    "title": "julo-gif-3.gif (1050√ó596)",
    "url": "https://blog.streamlit.io/content/images/2022/06/julo-gif-3.gif#browser",
    "html": ""
  },
  {
    "title": "checkout_form-1.gif (1407√ó1045)",
    "url": "https://blog.streamlit.io/content/images/2023/05/checkout_form-1.gif#browser",
    "html": ""
  },
  {
    "title": "marcelo.jpeg (800√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/05/marcelo.jpeg#border",
    "html": ""
  },
  {
    "title": "Marcelo Jannuzzi - Streamlit",
    "url": "https://blog.streamlit.io/author/marcelo/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Marcelo Jannuzzi\n1 post\nHow one finance intern launched his data science career from a coding bootcamp in Brazil\n\nLearn how Marcelo Jannuzzi of iFood got his dream job in data science\n\nCase study\nby\nMarcelo Jannuzzi and¬†\n1\n¬†more,\nJune 9 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "julo-3-1.jpeg (1700√ó734)",
    "url": "https://blog.streamlit.io/content/images/2022/06/julo-3-1.jpeg#browser",
    "html": ""
  },
  {
    "title": "julo-2.jpeg (2000√ó1121)",
    "url": "https://blog.streamlit.io/content/images/2022/06/julo-2.jpeg",
    "html": ""
  },
  {
    "title": "TZC-streamlit.gif (682√ó247)",
    "url": "https://blog.streamlit.io/content/images/2023/04/TZC-streamlit.gif#browser",
    "html": ""
  },
  {
    "title": "julo-1-1.jpeg (2000√ó1118)",
    "url": "https://blog.streamlit.io/content/images/2022/06/julo-1-1.jpeg",
    "html": ""
  },
  {
    "title": "Martijn Wieriks - Streamlit",
    "url": "https://blog.streamlit.io/author/martijn/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Martijn Wieriks\n1 post\nJULO improves financial inclusion in Indonesia with Streamlit\n\nLearn how JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nCase study\nby\nMartijn Wieriks and¬†\n1\n¬†more,\nJune 30 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 6)",
    "url": "https://blog.streamlit.io/page/6/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSemantic search, Part 1: Implementing cosine similarity\n\nWrangling Foursquare data and implementing semantic search in Snowflake\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 17 2023\nAnalyzing real estate properties with Streamlit\n\nA 7-step tutorial on how to make your own real estate app\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nMay 16 2023\nStreamlit wizard and custom animated spinner\n\nImprove user experience with simplified data entry and step-by-step guidance\n\nSnowflake powered ‚ùÑÔ∏è\nby\nAndrew Carson\n,\nMay 15 2023\nLearn Morse code with a Streamlit app\n\n5 steps to build your own Morse code tutor!\n\nAdvocate Posts\nby\nAlice Heiman\n,\nMay 12 2023\nThe ultimate Wordle cheat sheet\n\nLearn how to beat Wordle with Streamlit\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nMay 11 2023\nHow to build an LLM-powered ChatBot with Streamlit\n\nA step-by-step guide using the unofficial HuggingChat API\n\nLLMs\nby\nChanin Nantasenamat\n,\nMay 10 2023\nBuild a Snowflake DATA LOADER on Streamlit in only 5¬†minutes\n\nDrag and drop your Excel data to Snowflake with a Streamlit app\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSasha Mitrovich\n,\nMay 9 2023\nConvert images into pixel art\n\nA 5-step tutorial for making a pixel art converter app\n\nAdvocate Posts\nby\nsoma noda\n,\nMay 8 2023\nAccessible color themes for Streamlit apps\n\nControl your app‚Äôs color scheme and visual accessibility\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nMay 5 2023\nCollecting user feedback on ML in Streamlit\n\nImprove user engagement and model quality with the new Trubrics feedback component\n\nAdvocate Posts\nby\nJeff Kayne and¬†\n1\n¬†more,\nMay 4 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "step3.PNG.png (1324√ó1282)",
    "url": "https://blog.streamlit.io/content/images/2023/04/step3.PNG.png#browser",
    "html": ""
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/5/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nStreamlit-Authenticator, Part 1: Adding an authentication component to your app\n\nHow to securely authenticate users into your Streamlit app\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nDecember 6 2022\nBuilding robust Streamlit apps with type-checking\n\nHow to make type-checking part of your app-building flow\n\nAdvocate Posts\nby\nHarald Husum\n,\nNovember 10 2022\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nHow to build Streamlit apps on Replit\n\nLearn Streamlit by building the Beginner Template Tour\n\nAdvocate Posts\nby\nShruti Agarwal\n,\nSeptember 29 2022\nObserving Earth from space with Streamlit\n\nLearn how Samuel Bancroft made the SatSchool app to teach students Earth observation\n\nAdvocate Posts\nby\nSamuel Bancroft\n,\nJune 16 2022\nHow to share scientific analysis through a Streamlit app\n\n3 easy steps to share your study results with fellow scientists\n\nAdvocate Posts\nby\nMitchell Parker and¬†\n1\n¬†more,\nMay 12 2022\nHow to build a real-time live dashboard with Streamlit\n\n5 easy steps to make your own data dashboard\n\nAdvocate Posts\nby\nAbdulMajedRaja RS\n,\nApril 21 2022\n30 Days of Streamlit\n\nA fun challenge to learn and practice using Streamlit\n\nAdvocate Posts\nby\nChanin Nantasenamat\n,\nApril 1 2022\nSogeti creates an educational Streamlit app for data preprocessing\n\nLearn how to use Sogeti‚Äôs Data Quality Wrapper\n\nAdvocate Posts\nby\nTijana Nikolic\n,\nMarch 8 2022\nCalculating distances in cosmology with Streamlit\n\nLearn how three friends made the cosmology on-the-go app CosmŒ©racle\n\nAdvocate Posts\nby\nNikolina Sarcevic and¬†\n2\n¬†more,\nFebruary 17 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Sebastian Ayala Ruano - Streamlit",
    "url": "https://blog.streamlit.io/author/sebastian-ayala-ruano/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Sebastian Ayala Ruano\n1 post\nHow to quickly deploy and share your machine learning model for drug discovery\n\nShare your ML model in 3 simple steps\n\nAdvocate Posts\nby\nSebastian Ayala Ruano\n,\nDecember 15 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "custom_steps-1.gif (1407√ó866)",
    "url": "https://blog.streamlit.io/content/images/2023/05/custom_steps-1.gif#browser",
    "html": ""
  },
  {
    "title": "Dropjumptextfile.PNG-1.png (2255√ó1375)",
    "url": "https://blog.streamlit.io/content/images/2023/04/Dropjumptextfile.PNG-1.png#browser",
    "html": ""
  },
  {
    "title": "custom_spinner-1-1.gif (1407√ó866)",
    "url": "https://blog.streamlit.io/content/images/2023/05/custom_spinner-1-1.gif#browser",
    "html": ""
  },
  {
    "title": "step2.PNG.png (1310√ó914)",
    "url": "https://blog.streamlit.io/content/images/2023/04/step2.PNG.png#browser",
    "html": ""
  },
  {
    "title": "wizard-form.gif (1407√ó749)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wizard-form.gif#browser",
    "html": ""
  },
  {
    "title": "wizard_frame_2-1-1.gif (1407√ó674)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wizard_frame_2-1-1.gif#browser",
    "html": ""
  },
  {
    "title": "mcs_select.png (2378√ó1624)",
    "url": "https://blog.streamlit.io/content/images/2023/06/mcs_select.png#browser",
    "html": ""
  },
  {
    "title": "sis_mc_connect.png (2000√ó1324)",
    "url": "https://blog.streamlit.io/content/images/2023/06/sis_mc_connect.png#browser",
    "html": ""
  },
  {
    "title": "sis_mc_app.png (2356√ó1618)",
    "url": "https://blog.streamlit.io/content/images/2023/06/sis_mc_app.png#browser",
    "html": ""
  },
  {
    "title": "pyplot-5-1.gif (600√ó519)",
    "url": "https://blog.streamlit.io/content/images/2022/06/pyplot-5-1.gif#browser",
    "html": ""
  },
  {
    "title": "sis_mc_app_result.png (1554√ó663)",
    "url": "https://blog.streamlit.io/content/images/2023/06/sis_mc_app_result.png#browser",
    "html": ""
  },
  {
    "title": "remove-background-from-your-image.png (1592√ó1025)",
    "url": "https://blog.streamlit.io/content/images/2022/12/remove-background-from-your-image.png#border",
    "html": ""
  },
  {
    "title": "pyplot-4.png (1452√ó1308)",
    "url": "https://blog.streamlit.io/content/images/2022/06/pyplot-4.png#browser",
    "html": ""
  },
  {
    "title": "pyplot-3-1.png (1484√ó1381)",
    "url": "https://blog.streamlit.io/content/images/2022/06/pyplot-3-1.png#browser",
    "html": ""
  },
  {
    "title": "simple_matplot_gif-1.gif (850√ó606)",
    "url": "https://blog.streamlit.io/content/images/2022/06/simple_matplot_gif-1.gif#browser",
    "html": ""
  },
  {
    "title": "Screenshot-2023-02-08-at-6.45.33-AM.png (2000√ó965)",
    "url": "https://blog.streamlit.io/content/images/2023/02/Screenshot-2023-02-08-at-6.45.33-AM.png#browser",
    "html": ""
  },
  {
    "title": "pyplot-1-1.png (1651√ó1310)",
    "url": "https://blog.streamlit.io/content/images/2022/06/pyplot-1-1.png#browser",
    "html": ""
  },
  {
    "title": "UC Davis Dashboard That Tracks California's COVID-19 Cases By Region",
    "url": "https://blog.streamlit.io/uc-davis-tool-tracks-californias-covid-19-cases-by-region/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nNew UC Davis tool tracks California's COVID-19 cases by region\n\nRegional tracking of COVID-19 cases aids day-to-day decision making in the UC Davis School of Veterinary Medicine\n\nBy Pranav Pandit\nPosted in Advocate Posts, November 19 2020\nBuilding a tool to understand COVID-19 at a regional level\nCreating the tool in Streamlit\nCompiling and presenting the data\nSharing our app with the rest of the world\nGetting the app to shine\nImproving app stability\nImproving app performance\nIn closing\nContents\nShare this post\n‚Üê All posts\n\nWritten by Pranav Pandit - Postdoctoral Research Fellow at One Health Institute at UC Davis\n\nBuilding a tool to understand COVID-19 at a regional level\n\nThe COVID-19 pandemic has highlighted the importance of constant and real-time disease surveillance to better control unprecedented local outbreaks. Early on in the pandemic, the state of California came up with its own distinct phases for opening the economy and community activities. These phases were based on county-wide thresholds of daily COVID-19 incidence, test positivity, and availability of ICU beds in the county. While county-wide data is continuously available, a composite picture of a group of counties is not as easy to understand. Such information is especially useful for bigger organizations that serve regional communities, such as universities, corporate companies, or dense urban regions that are highly connected.\n\nAt the UC Davis School of Veterinary Medicine, faculty, staff, and students come from many parts of California, but primarily from three counties‚ÄîYolo, Solano and Sacramento. University students, staff, and faculty commute from these nearby communities, so it's key to not just follow up on the COVID-19 statistics in Yolo County but rather look at a combination of the three counties to make informed decisions on reopening policies. Similarly, larger metropolitan areas such as the San Francisco Bay Area can benefit greatly from composite COVID-19 statistics for the region.\n\nMy team in the Epicenter for Disease Dynamics within the school‚Äôs One Health Institute was tasked with developing a tool that would go beyond county data and help us understand regional risk. Drawing from statistics already provided by the California Department of Public Health, John Hopkins University and CovidActNow.org, we have created a COVID-19 tool that lets the university better understand our regional data and plan our day-to-day decisions.\n\nCreating the tool in Streamlit\n\nBy using Streamlit we were able to convert our Python code into an interactive tool that enables users to select counties of interest and get composite COVID-19 intelligence for the combined region. You can check out the tool here and the source code for the tool here.\n\nCompiling and presenting the data\n\nWe already had a lot of code developed to track and visualize cases in California with live data provided through the state's data portal. We also started capturing daily case data from the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University and decided to highlight informational snippets related to positive test rates and ICU rooms which is available through CovidActNow.org.\n\nThe application starts with displaying a dashboard that shows composite data of three counties surrounding the University of California Davis (Yolo, Sacramento and Solano). Users can either choose a single county or a combination of multiple counties of their choice to get trends on A) daily new cases per 100,000 population (averaged over the last seven days), (B) daily incidence (new cases), (C) Cumulative cases and deaths and (D) Daily new tests (testing data is available only for a few counties in California).\n\nIf you'd like to learn more about how to interpret these metrics, covidlocal.org is a good resource - but in short, the initial reopening of a community is indicated when daily cases decline for 21 consecutive days and estimates for new cases per 100,000 are below 25 cases per day (Phase 2). Similarly, Phase 3 economic recovery opening is indicated when estimates for new cases per 100,000 are below 10 cases per day.\n\nSharing our app with the rest of the world ¬†\n\nOnce the initial version of the app was ready, I wanted my teammates at UC Davis to access the app, but I didn't want to put them through the ordeal of downloading the source code, setting up the development environment, and running the app locally. We hosted it on Heroku, but soon realized its limitations in terms of the number of users that can access it simultaneously.\n\nAround the same time, Streamlit had launched a beta version of their sharing platform. The Streamlit sharing platform allows deploying Streamlit apps directly from a public repository in Github and sharing with external shareholders for free (as of writing this sharing is currently invite only, but you can sign-up here).\n\nOnce I was enrolled in the beta program, I was able to deploy the app in just a couple of clicks. All I had to do was host the app source code in a public Github repository.\n\nAfter the app was deployed to the Streamlit sharing platform, I shared the URL with the Dean's office, and the URL shortly made it's way to school's internal portal and became a tool used in planning field operations while minimizing the risk of exposing students and researchers to Covid-19.\n\nGetting the app to shine ¬†\n\nAround this time we noticed a couple of issues related to the performance and stability of the app. I will explain the issues along with the solutions below.\n\nImproving app stability ¬†\n\nDuring one of my team meetings, when some of my teammates tried to access the app simultaneously, we noticed the app crashed and restarted. With the help of the engineering team at Streamlit, we narrowed down these issues to the use of matplotlib as the library for rendering charts based on data frames in the app. It turned out that matplotlib is not best suited for applications running in a highly concurrent environment, in fact, the official matplotlib documentation claims that the library is not thread-safe. As a result, when multiple users tried to access the app simultaneously, the app would crash and restart. We were able to solve this problem by eliminating the use of matplotlib 's global API, and more importantly, by using explicit synchronization semantics when accessing matplotlib figures in the Python code (see example code).\n\nImproving app performance ¬†\n\nAs the app was becoming more popular within UC Davis, another concern I had was around the noticeably high latency of the charts loading when a user visits the app. I was already using Streamlit's built-in caching using st.cache() annotations, so I knew the app was not doing the heavy-lifting of downloading datasets and computing aggregates every time. We profiled the app's performance using Python's built-in cProfile package and observed that most of the time was spent plotting the charts inside matplotlib. ¬†Taking inspiration from a Streamlit user community forum thread, which recommends altair as a more performant library for plotting, I ported the visualizations from matplotlib to altair , and noticed up to 3x improvement in the time it takes to run the app.\n\nHere's a code snippet showing how easy it is to plot Altair charts in Streamlit:\n\n\n# We use a custom scale to modify legend colors.\nscale = alt.Scale(domain=[\"cases\", \"deaths\"], range=['#377eb8', '#e41a1c'])\n\n# Create a base chart layer using the dataframe.\nbase = alt.Chart(cases_and_deaths, title='(C) Cumulative cases and deaths'\n    ).transform_calculate(\n    cases_=\"'cases'\", deaths_=\"'deaths'\")\n    \n# Overlay a plot of number of cases.\nc = base.mark_line(strokeWidth=3).encode(\n    x=alt.X(\"Datetime\", axis=alt.Axis(title = 'Date')),\n    y=alt.Y(\"cases\", axis=alt.Axis(title = 'Count')),\n    color=alt.Color(\"cases_:N\", scale=scale, title=\"\"))\n    \n# Overlay a plot of number of deaths.\nd = base.mark_line(strokeWidth=3).encode(\n    x=alt.X(\"Datetime\", axis=alt.Axis(title='Date')),\n    y=alt.Y(\"deaths\", axis=alt.Axis(title = 'Count')),\n    color=alt.Color(\"deaths_:N\", scale=scale, title=\"\"))\n    \n# Finally, render the chart.\nst.altair_chart(c + d, use_container_width=True)\n\n\nIn closing\n\nAt the UC Davis School of Veterinary Medicine, the tool is being used to track metrics to better inform campus safety services and communications, but we believe the tool can be used for a wider audience as well.\n\nAt this stage of the pandemic, many individuals, families, and smaller organizations are debating their own decisions - whether to take a vacation, if they can visit family during holidays, when to make a work trip, or if it's too much of a risk to conduct daily activities such as grocery shopping. Such activities often require an additional aspect of regional risk assessment, and we hope that our tool can help provide that context. Please try it out, send feedback, and continue to share your own data and create your own great tools. We'll all get through this together.\n\nSpecial thanks to Amey Deshpande and the Streamlit team for helping to optimize the app code and to Kat Kerlin, Tom Hinds and the UC Davis marketing team for the help editing the blog post.\n\n\nAbout the author\n\nPranav Pandit, BVSc & AH, MPVM, Ph.D. is a postdoctoral scholar at the EpiCenter for Disease Dynamics, part of the One Health Institute at the UC Davis School of Veterinary Medicine. A veterinary epidemiologist specializing in mathematical modeling, Pranav is interested in understanding transmission diseases in animal populations and factors affecting spillover to humans. After completing his MPVM from UC Davis, Pranav completed his Ph.D. from √âcole nationale v√©t√©rinaire de Nantes, in France. Follow Pranav and the EpiCenter for Disease Dynamics on Twitter: @PanditPranav and @EpiCenterUCD.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "4-1.png (1200√ó760)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4-1.png#border",
    "html": ""
  },
  {
    "title": "intro.gif (1660√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/10/intro.gif#browser",
    "html": ""
  },
  {
    "title": "hello-upload-3.png (789√ó700)",
    "url": "https://blog.streamlit.io/content/images/2022/12/hello-upload-3.png#border",
    "html": ""
  },
  {
    "title": "hello-upload-2.png (745√ó364)",
    "url": "https://blog.streamlit.io/content/images/2022/12/hello-upload-2.png#border",
    "html": ""
  },
  {
    "title": "hello-upload-1.png (1558√ó508)",
    "url": "https://blog.streamlit.io/content/images/2022/12/hello-upload-1.png#border",
    "html": ""
  },
  {
    "title": "user-privileges.gif (824√ó506)",
    "url": "https://blog.streamlit.io/content/images/2022/12/user-privileges.gif#browser",
    "html": ""
  },
  {
    "title": "logged-in.png (776√ó462)",
    "url": "https://blog.streamlit.io/content/images/2022/11/logged-in.png#border",
    "html": ""
  },
  {
    "title": "incorrect-login.png (818√ó495)",
    "url": "https://blog.streamlit.io/content/images/2022/11/incorrect-login.png#border",
    "html": ""
  },
  {
    "title": "streamlit-spotify-playlists-app.gif (942√ó676)",
    "url": "https://blog.streamlit.io/content/images/2022/11/streamlit-spotify-playlists-app.gif#border",
    "html": ""
  },
  {
    "title": "csv-file-playlists.png (1768√ó988)",
    "url": "https://blog.streamlit.io/content/images/2022/11/csv-file-playlists.png#browser",
    "html": ""
  },
  {
    "title": "authentication-code.png (1504√ó764)",
    "url": "https://blog.streamlit.io/content/images/2022/11/authentication-code.png#browser",
    "html": ""
  },
  {
    "title": "top-songs-dataframe-table.png (1609√ó904)",
    "url": "https://blog.streamlit.io/content/images/2022/11/top-songs-dataframe-table.png#browser",
    "html": ""
  },
  {
    "title": "billboard-hot-100-top-ten-singles.png (2042√ó1178)",
    "url": "https://blog.streamlit.io/content/images/2022/11/billboard-hot-100-top-ten-singles.png#browser",
    "html": ""
  },
  {
    "title": "plotly-code.png (698√ó928)",
    "url": "https://blog.streamlit.io/content/images/2023/01/plotly-code.png#border",
    "html": ""
  },
  {
    "title": "color-picker.gif (600√ó293)",
    "url": "https://blog.streamlit.io/content/images/2023/01/color-picker.gif#border",
    "html": ""
  },
  {
    "title": "3-4.png (1200√ó305)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-4.png#border",
    "html": ""
  },
  {
    "title": "2-6.png (1200√ó890)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-6.png#border",
    "html": ""
  },
  {
    "title": "mona-lisa-pixels-clustered.png (822√ó510)",
    "url": "https://blog.streamlit.io/content/images/2023/01/mona-lisa-pixels-clustered.png#border",
    "html": ""
  },
  {
    "title": "mona-lisa-pixels.png (822√ó510)",
    "url": "https://blog.streamlit.io/content/images/2023/01/mona-lisa-pixels.png#border",
    "html": ""
  },
  {
    "title": "sophisticated-palette-app-demo.gif (600√ó481)",
    "url": "https://blog.streamlit.io/content/images/2023/01/sophisticated-palette-app-demo.gif#browser",
    "html": ""
  },
  {
    "title": "mona-lisa.png (982√ó194)",
    "url": "https://blog.streamlit.io/content/images/2023/01/mona-lisa.png#border",
    "html": ""
  },
  {
    "title": "Untitled-1.png (1838√ó1119)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Untitled-1.png#browser",
    "html": ""
  },
  {
    "title": "8.png (1200√ó634)",
    "url": "https://blog.streamlit.io/content/images/2021/08/8.png#border",
    "html": ""
  },
  {
    "title": "5-1.png (1200√ó701)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-1.png#border",
    "html": ""
  },
  {
    "title": "7.png (1200√ó566)",
    "url": "https://blog.streamlit.io/content/images/2021/08/7.png#border",
    "html": ""
  },
  {
    "title": "6.png (1200√ó557)",
    "url": "https://blog.streamlit.io/content/images/2021/08/6.png#border",
    "html": ""
  },
  {
    "title": "3.png (1826√ó1159)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3.png#browser",
    "html": ""
  },
  {
    "title": "2.png (1820√ó1157)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2.png#browser",
    "html": ""
  },
  {
    "title": "Matt Brems - Streamlit",
    "url": "https://blog.streamlit.io/author/matt/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Matt Brems\n1 post\nHow to use Roboflow and Streamlit to visualize object detection output\n\nBuilding an app for blood cell count detection\n\nAdvocate Posts\nby\nMatt Brems\n,\nFebruary 23 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "streamlit-figma-design-system-1.gif (3446√ó2038)",
    "url": "https://blog.streamlit.io/content/images/2022/10/streamlit-figma-design-system-1.gif#browser",
    "html": ""
  },
  {
    "title": "0_di0AIuD61ak2AfQC.png (1400√ó707)",
    "url": "https://blog.streamlit.io/content/images/2021/08/0_di0AIuD61ak2AfQC.png#browser",
    "html": ""
  },
  {
    "title": "streamlit-figma-app-base.gif (3446√ó2038)",
    "url": "https://blog.streamlit.io/content/images/2022/10/streamlit-figma-app-base.gif#browser",
    "html": ""
  },
  {
    "title": "0_zL052KWOPvXxGOo6.png (1400√ó964)",
    "url": "https://blog.streamlit.io/content/images/2021/08/0_zL052KWOPvXxGOo6.png#browser",
    "html": ""
  },
  {
    "title": "Jonah Kanner - Streamlit",
    "url": "https://blog.streamlit.io/author/jonah/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jonah Kanner\n1 post\nWebsite\nGravitational-wave apps help students learn about black holes\n\nExploring distant space with gravitational waves\n\nAdvocate Posts\nby\nJonah Kanner\n,\nDecember 15 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "0_bLyrD_iCLEzljxfg.png (1400√ó323)",
    "url": "https://blog.streamlit.io/content/images/2021/08/0_bLyrD_iCLEzljxfg.png#browser",
    "html": ""
  },
  {
    "title": "cazoo-1.png (2000√ó1777)",
    "url": "https://blog.streamlit.io/content/images/2022/09/cazoo-1.png#browser",
    "html": ""
  },
  {
    "title": "streamlit-2-1.gif (640√ó480)",
    "url": "https://blog.streamlit.io/content/images/2021/09/streamlit-2-1.gif#browser",
    "html": ""
  },
  {
    "title": "files.png (1088√ó744)",
    "url": "https://blog.streamlit.io/content/images/2022/10/files.png#border",
    "html": ""
  },
  {
    "title": "reset_password.PNG-2.png (845√ó556)",
    "url": "https://blog.streamlit.io/content/images/2023/01/reset_password.PNG-2.png#border",
    "html": ""
  },
  {
    "title": "0_7XCiGHafU_Cy1xc0--3-.png (652√ó738)",
    "url": "https://blog.streamlit.io/content/images/2021/08/0_7XCiGHafU_Cy1xc0--3-.png#browser",
    "html": ""
  },
  {
    "title": "Martin Campbell - Streamlit",
    "url": "https://blog.streamlit.io/author/martin/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Martin Campbell\n1 post\nEasy monitoring of dbt Cloud jobs with Streamlit\n\nHow the Cazoo data science team built their dbt Cloud + Streamlit app\n\nAdvocate Posts\nby\nMartin Campbell\n,\nJune 11 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "streamlit-secrets-2.gif (1600√ó900)",
    "url": "https://blog.streamlit.io/content/images/2021/09/streamlit-secrets-2.gif#brower",
    "html": ""
  },
  {
    "title": "streamlit-components.png (1894√ó1454)",
    "url": "https://blog.streamlit.io/content/images/2022/10/streamlit-components.png#border",
    "html": ""
  },
  {
    "title": "streamlit-figma-design-system.png (574√ó380)",
    "url": "https://blog.streamlit.io/content/images/2022/10/streamlit-figma-design-system.png#border",
    "html": ""
  },
  {
    "title": "visualization-1.png (696√ó834)",
    "url": "https://blog.streamlit.io/content/images/2023/01/visualization-1.png#border",
    "html": ""
  },
  {
    "title": "Theming--3-.gif (1280√ó821)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Theming--3-.gif",
    "html": ""
  },
  {
    "title": "The magic of working in open source",
    "url": "https://blog.streamlit.io/the-magic-of-working-in-open-source/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe magic of working in open source\n\nHow we build our open-source library and release new features\n\nBy Ken McGrady\nPosted in Tutorials, August 4 2022\nHow we prioritize new features\nWhat goes into implementing new features\nHow we keep in touch with the community\nHow you can contribute to the open-source community\nDoes this make you excited?\nContents\nShare this post\n‚Üê All posts\n\nWondering what it‚Äôs like to work on the Streamlit open-source project? There are many reasons why we all love it. But the most exciting one is our focus on becoming the best tool in every data scientist‚Äôs toolchain.\n\nOpen source involves lots of stakeholders yet offers limited resources. So our biggest challenge is to prioritize and implement the most useful features.\n\nIn this post, you‚Äôll learn:\n\nHow we prioritize new features\nWhat goes into implementing new features\nHow we keep in touch with the community\nHow you can contribute to the open-source community\nHow we prioritize new features\n\nWe prioritize new features every quarter:\n\nOur product team decides which features will evolve the product and its audience.\nOur engineering team builds solutions to GitHub issues.\nOur community team monitors our social channels and advocates for the community‚Äôs needs.\n\nFor example, these features took countless hours of brainstorming, prototyping, and testing: multipage apps, new caching primitives, camera input, and updated dataframes.\n\nIn between the larger features, we tackle small delightful experiences, fix bugs, improve built-in charts, and add parameters to our APIs (tooltips, gap sizes, disabled widgets, etc.).\n\nWhat goes into implementing new features\n\nBefore we start working on a new feature, we talk to our Data Science team and Streamlit Creators. Together, we decide which feature has the right amount of complexity and the most intuitive API (though there‚Äôs rarely a single solution for everyone‚Äôs use case).\n\nA feature typically starts out as a simple ‚Äúcouple-of-lines change‚Äù that grows into a discussion on how it‚Äôll impact the users, how it could be misused, and if it‚Äôll keep our software resilient. We sort through lots of community feedback before finally pulling the trigger.\n\nOnce we build and release the feature, we move forward super-fast by:\n\nUnit-testing it to narrow down bugs in code;\nEnd-to-end testing to test the full functionality of a feature;\nAnd screenshot-testing it to make the visuals pixel-perfect.\n\nFrom an engineering standpoint, we try to not break our API while keeping a semantic versioning promise. We work with our product and design teams to give our users the best experience by looking at the common data use cases and designing solutions that have room for change. Plus, all external contributors' code gets assigned a code reviewer. Often we assign two code reviewers because we‚Äôre not familiar with the context!\n\nIf you‚Äôre curious to learn more about how we implement new features, check out these posts:\n\nHow to enhance Google Search Console data exports with Streamlit\nHow Streamlit uses Streamlit: sharing contextual apps\nNew experimental primitives for caching\nAnnouncing theming for Streamlit apps!\nHow we keep in touch with the community\n\nIt can be a challenge for engineers to balance delivering features and talking to the community. We want to deliver our code on time, so our community conversations have a ‚Äúcontext switching‚Äù tax. Our focus tends to be more on the quality of our product and less on the use cases, so our attention goes to the GitHub issues and bugs. We try to understand the issue, reliably reproduce it, and guesstimate its impact. Often, due to timing, we can‚Äôt fix the bug, but we get enough knowledge to help an external contributor solve the problem.\n\nBut we‚Äôre out there:\n\nOur Engineering team posts release notes and responds to many posts on the forum.\nOur Data Science team always has new ideas based on their Streamlit dogfooding.\nOur Developer Relations team works with the community to produce rich content like 30 days of Streamlit.\n\nWe‚Äôre now part of Snowflake, and Snowflake‚Äôs mission is to mobilize the world‚Äôs data. Our community plays a big role in it. We believe in the full-employment theorem so we can always make Streamlit a better product for data scientists!\n\nHow you can contribute to the open-source community\n\nContributing to the open-source community is very rewarding. The software is free. And you can improve a single function or a whole discipline! But getting involved may seem daunting as most conversations are asynchronous. It takes time, patience, and fortitude.\n\nIf you want to get involved and help us make a stronger product, we‚Äôd love for you to do so! Here is how to get started:\n\nUse Streamlit! Building software requires domain experience. Read more about Streamlit‚Äôs main concepts.\nFile bugs if you see them. Implement small, reliable, reproducible cases, and include as many details as possible. Many issues take time to understand because messages get lost in translation.\nHelp the community. Simple explanations help people understand Streamlit better. Better yet, turn it into content (for example, a YouTube channel or a Medium blog.)\nImprove our documentation. We have amazing documentation and we value your input!\nShare your apps on the forum and social!\n\nSpend a month or two focused on the above‚Äîit‚Äôll clarify for you how to help out in code. When ready, follow our contributing guidelines and take on a bug from our GitHub issues. Bugs are understandable, reproducible, and have a desired outcome. We identified some good first issues, but there are many more to choose from.\n\nAnd finally, follow good software engineering practices in designing your solution and write tests (it saves the first comment in a code review). üßë‚Äçüíª\n\nDoes this make you excited?\n\nWant to work on open source as a job? Join our team! Our jobs require a unique skill set because Streamlit‚Äôs main value is delivering a clean and interactive user interface for developers, so we rely on strong frontend skills with TypeScript/React. And our developers interface with Streamlit using a simple Python API and server.\n\nHere are our current job openings:\n\nSenior Software Engineer and Senior Product Manager on our Open Source team.\nSoftware Engineer on our Community Cloud team (if you have experience building full-stack services in the cloud).\n\nThank you for being part of our community. If you have questions, please post them in the comments below, and you may see them answered in future blog posts. üòâ\n\nHappy coding! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sample-code-github.png (1209√ó630)",
    "url": "https://blog.streamlit.io/content/images/2023/01/sample-code-github.png#browser",
    "html": ""
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/page/5/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\nMake your st.pyplot interactive!\n\nLearn how to make your pyplot charts interactive in a few simple steps\n\nTutorials\nby\nWilliam Huang\n,\nJune 23 2022\nHow Streamlit uses Streamlit: Sharing contextual apps\n\nLearn about session state and query parameters!\n\nTutorials\nby\nTyler Richards\n,\nMay 26 2022\n3 steps to fix app memory leaks\n\nHow to detect if your Streamlit app leaks memory and identify faulty code\n\nTutorials\nby\nGeorge Merticariu\n,\nApril 14 2022\nHow to master Streamlit for data¬†science\n\nThe essential Streamlit for all your data¬†science needs\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 18 2022\nCommon app problems: Resource limits\n\n5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud\n\nTutorials\nby\nJohannes Rieke\n,\nSeptember 9 2021\n6 tips for improving your Streamlit app performance\n\nMoving your Streamlit app from analysis to production\n\nTutorials\nby\nRandy Zwitch\n,\nJuly 20 2021\nHow to make a great Streamlit app: Part II\n\nA few layout and style tips to make your apps look even more visually appealing!\n\nTutorials\nby\nAbhi Saini\n,\nJune 22 2021\nHow to make a great Streamlit app\n\nDesigning an app your users will love\n\nTutorials\nby\nAbhi Saini\n,\nJune 2 2021\nIntroducing Submit button and Forms üìÉ\n\nWe're releasing a pair of new commands called st.form and st.form_submit_button!\n\nTutorials\nby\nAbhi Saini\n,\nApril 29 2021\nStreamlit ‚ù§Ô∏è Firestore (continued)\n\nAka the NoSQL sequel: Building a Reddit clone and deploying it securely\n\nTutorials\nby\nAustin Chen\n,\nApril 22 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Ken McGrady - Streamlit",
    "url": "https://blog.streamlit.io/author/ken/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Ken McGrady\n1 post\nThe magic of working in open source\n\nHow we build our open-source library and release new features\n\nTutorials\nby\nKen McGrady\n,\nAugust 4 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "flow.png (1806√ó1008)",
    "url": "https://blog.streamlit.io/content/images/2022/10/flow.png#border",
    "html": ""
  },
  {
    "title": "Vladimir Timofeenko - Streamlit",
    "url": "https://blog.streamlit.io/author/vladimir_t/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Vladimir Timofeenko\n1 post\nMake dynamic filters in Streamlit and show their effects on the original dataset\n\nQuickly and easily add dynamic filters to your Streamlit app\n\nTutorials\nby\nVladimir Timofeenko\n,\nAugust 25 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Jessi Shamis - Streamlit",
    "url": "https://blog.streamlit.io/author/jessi/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jessi Shamis\n1 post\nPrototype your app in Figma! üñåÔ∏è\n\nQuickly and easily design your app with the Streamlit Design system\n\nTutorials\nby\nJessi Shamis\n,\nOctober 27 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Harald Husum - Streamlit",
    "url": "https://blog.streamlit.io/author/harald/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Harald Husum\n1 post\nBuilding robust Streamlit apps with type-checking\n\nHow to make type-checking part of your app-building flow\n\nAdvocate Posts\nby\nHarald Husum\n,\nNovember 10 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Theming-2--1-.png (1964√ó1116)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Theming-2--1-.png",
    "html": ""
  },
  {
    "title": "Screen_Shot_2021-03-15_at_10.43.38_AM.png (2000√ó1055)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-03-15_at_10.43.38_AM.png#browser",
    "html": ""
  },
  {
    "title": "update_user_details.PNG.png (853√ó464)",
    "url": "https://blog.streamlit.io/content/images/2023/01/update_user_details.PNG.png#border",
    "html": ""
  },
  {
    "title": "forgot_username.PNG.png (856√ó372)",
    "url": "https://blog.streamlit.io/content/images/2023/01/forgot_username.PNG.png#border",
    "html": ""
  },
  {
    "title": "5-3.png (1200√ó634)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-3.png#browser",
    "html": ""
  },
  {
    "title": "forgot_password.PNG.png (857√ó369)",
    "url": "https://blog.streamlit.io/content/images/2023/01/forgot_password.PNG.png#border",
    "html": ""
  },
  {
    "title": "register_user.PNG.png (868√ó756)",
    "url": "https://blog.streamlit.io/content/images/2023/01/register_user.PNG.png#border",
    "html": ""
  },
  {
    "title": "Richard Pelgrim - Streamlit",
    "url": "https://blog.streamlit.io/author/richard/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Richard Pelgrim\n1 post\nDeploying a cloud-native Coiled app\n\nHow Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users\n\nAdvocate Posts\nby\nRichard Pelgrim\n,\nSeptember 7 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1.1.png (1200√ó688)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1.1.png#border",
    "html": ""
  },
  {
    "title": "2.2.png (1076√ó567)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2.2.png#browser",
    "html": ""
  },
  {
    "title": "10.png (1884√ó1255)",
    "url": "https://blog.streamlit.io/content/images/2021/08/10.png#border",
    "html": ""
  },
  {
    "title": "11.png (1920√ó1195)",
    "url": "https://blog.streamlit.io/content/images/2021/08/11.png#border",
    "html": ""
  },
  {
    "title": "9.png (1600√ó985)",
    "url": "https://blog.streamlit.io/content/images/2021/08/9.png#border",
    "html": ""
  },
  {
    "title": "7-1.png (1881√ó894)",
    "url": "https://blog.streamlit.io/content/images/2021/08/7-1.png#border",
    "html": ""
  },
  {
    "title": "8-1.png (1600√ó938)",
    "url": "https://blog.streamlit.io/content/images/2021/08/8-1.png#border",
    "html": ""
  },
  {
    "title": "6-1.png (1920√ó1114)",
    "url": "https://blog.streamlit.io/content/images/2021/08/6-1.png#border",
    "html": ""
  },
  {
    "title": "5-6.png (1772√ó1879)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-6.png#border",
    "html": ""
  },
  {
    "title": "4-3.png (1200√ó835)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4-3.png#border",
    "html": ""
  },
  {
    "title": "3-5.png (1200√ó705)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-5.png#border",
    "html": ""
  },
  {
    "title": "2-8.png (1920√ó1180)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-8.png#border",
    "html": ""
  },
  {
    "title": "5-5.png (1720√ó993)",
    "url": "https://blog.streamlit.io/content/images/2021/08/5-5.png#border",
    "html": ""
  },
  {
    "title": "3-9.png (996√ó492)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-9.png#browser",
    "html": ""
  },
  {
    "title": "2-12.png (996√ó588)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-12.png#browser",
    "html": ""
  },
  {
    "title": "1-11.png (996√ó588)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-11.png#browser",
    "html": ""
  },
  {
    "title": "Monthly rewind > January 2023",
    "url": "https://blog.streamlit.io/monthly-rewind-january-2023/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, February 16 2023\nüèÜ App of the month üèÜ\nStreamlit January updates\nüîç Current release: 1.18.0\nüîÆ Upcoming\n‚ùÑÔ∏è Snowpark and PySpark support\nüéà Streamlit Tutorial-a-thon\nFeatured Streamlit content\nüñºÔ∏èÔ∏è Build an image background remover in Streamlit\nüé®Ô∏è Create a color palette from any image\nüó∫Ô∏è How to make a culture map\n‚òÅÔ∏è Host your Streamlit app for free\nüìÑ ScienceIO manages billions of rows of training data with Streamlit\nüì∫ New YouTube tutorial: How to Create an Interactive Research Article using Streamlit\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur January featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nMathGPT by Jos√© Manuel N√°poles Duarte!\n\nMathGPT is an app that uses Open AI's GPT-3 and NumPy to handle mathematical operations‚Äîsuch as vectors, matrices, and even college physics problems. Enter a prompt involving a math operation and get the result, explanation, and code behind the solution.\n\nStreamlit January updates\n\nBelow are the latest updates and releases from January.\n\nüîç Current release: 1.18.0\n\nThe latest release is 1.18.0. Recent updates include new caching commands to replace st.cache and columns inside columns. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nNew features to get excited about:\n\nCleaner hamburger menu\nImproved st.help\nTooltips on text elements\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\n‚ùÑÔ∏è Snowpark and PySpark support\nSnowflake's Snowpark and Apache's PySpark dataframes are now supported out of the box! Just pass them into any st command that accepts pandas dataframes. See the demo app.\nüéà Streamlit Tutorial-a-thon\n\nCongratulations to the winners of the first Streamlit Tutorial-a-thon: Lisa, Andy, Vin√≠cius, and Hansen! Check out their awesome tutorials. üéâ\n\nLisa's blog post\nAndy's blog post\nVin√≠cius's blog post\nHansen's video\nFeatured Streamlit content\n\nüñºÔ∏èÔ∏è Build an image background remover in Streamlit\nWant to isolate an image from its background while still maintaining the quality? Learn how to build a background remover app from Tyler Simons.\n\nüé®Ô∏è Create a color palette from any image\nWant to enhance your data visualization with a custom color palette? Build an app with Siavash Yasini to generate colors from any image.\n\nüó∫Ô∏è How to make a culture map\nLearn from Micha≈Ç Nowotka how to create a culture map app while adding dynamic components for visual analysis.\n\n‚òÅÔ∏è Host your Streamlit app for free\nWant to rediscover music from your high school days? Learn from Robert how to build an app to generate Spotify playlists of top songs from selected years.\n\nüìÑ ScienceIO manages billions of rows of training data with Streamlit\nLearn from Gaurav Kaushik how ScienceIO searches and interacts with its training dataset for large healthcare language models‚Äîusing a Streamlit app connected to a Snowflake database!\n\nüì∫ New YouTube tutorial: How to Create an Interactive Research Article using Streamlit\nLearn from the Data Professor how to turn your research article into an interactive Streamlit app that can easily be shared with anyone!\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nTry out and explore Hugging Face diffusers with Abhishek's Diffuzers app.\nLearn how to Use chatGPT to build a Machine Learning Web App in Python in Avra's YouTube tutorial.\nBo's TrainAnimeAI app lets you train like your favorite anime character! Input a character's name and get workout and diet plans based on them.\nCarlos teaches how to Create repeatable items in Streamlit in his blog post.\nConvert any question to an SQL query with Idriss' Text to SQL app.\nBen's Soccer Prospect Research & Radar Creation helps you find players that meet specific criteria.\nMaciej's Ask my PDF app is a question-answering system built on GPT3.\nDash wrote a technical guide for building image recognition applications in Snowflake.\nSearch directly in Google Scholar with Ayoub's Scholar Scrap app.\nWhom does your child look like is Daisy's image comparison app that provides similarity scores of a child to parents.\nStuck learning Python? Make it fun with Streamlit‚ÄîSasha's video shows his development flow and how it helped his sales engineering job.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nOctober 2022\nNovember 2022\nDecember 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > February 2023",
    "url": "https://blog.streamlit.io/monthly-rewind-february-2023/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, March 22 2023\nüèÜ App of the month üèÜ\nStreamlit February updates\nüîç Current release: 1.20.0\nüîÆ Upcoming\n‚ö°Ô∏è New caching commands\nü™Ü Columns inside columns\nüöÄ Editable dataframes\nFeatured Streamlit content\nü¶æ Using Streamlit for semantic processing with semantha\nüîê Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component\nü§ñ Using ChatGPT to build a Kedro ML pipeline\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur February featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nThe Dungeon by Tomasz Hasi√≥w!\n\nExplore the depths of an ancient dungeon in this Streamlit-based dungeon crawler game. [code]\n\nStreamlit February updates\n\nBelow are the latest updates and releases from February.\n\nüîç Current release: 1.20.0\n\nThe latest release is 1.20.0. Recent updates include granular control over app embedding behavior, a cleaner hamburger menu, and the de-experimentalization of faster reruns. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nNew features to get excited about:\n\nStep parameter for st.time_input\nTooltips on text elements\nImproved st.help\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\n‚ö°Ô∏è New caching commands\nIntroducing st.cache_data and st.cache_resource‚Äîtwo new simpler and faster commands to replace st.cache! Learn more about them here.\n\nü™Ü Columns inside columns\nst.columns now supports up to one level of column nesting! Check out the demo app for examples.\n\nüöÄ Editable dataframes\nEditable dataframes are here! With st.experimental_data_editor you can now interact with the dataframes in your apps.\n\nFeatured Streamlit content\n\nü¶æ Using Streamlit for semantic processing with semantha\nLearn how to integrate semantic AI processing into your apps and use cases. Sven Koerner outlines the steps using Streamlit and semantha.\n\nüîê Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component\nLearn from Mohammad Khorasani how to add advanced features to the Streamlit-Authenticator component in Part 2 of the blog series.\n\nü§ñ Using ChatGPT to build a Kedro ML pipeline\nWant to learn from ChatGPT how to deploy and manage ML models with Kedro and Streamilt? See how Arvindra Sehmi asked it to teach him just that.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nSasmitha's KnowledgeGPT app allows you to ask questions about your documents and get accurate answers with instant citations.\nGet scientific searches and insights all in one place with Avra's PubLit‚Äînow using GPT3.\nJacob outlines How to Build a Dividend Investing Dashboard in Python and Streamlit in his Medium article.\nBuild a ChatGPT-like chatbot using LangChain, GPT-3, and Streamlit in echohive's video: Langchain ChatGPT your documents challenge with Gpt 3 and OpenAI embeddings and Streamlit UI.\nPresent your portfolio and projects as visually pleasing cards with Tom's Streamlit Cardfolio: A Portfolio Presentation App!\nTyler's Caltrain Platform app shows the real-time status of trains to improve your commute.\nStephan's GPTFlix is like a ChatGPT for movie reviews‚Äîask it questions on movie knowledge, reviews, and recommendations.\nAlexander shows how to become one of the top managers in Fantasy Premier League With Snowflake, Streamlit & Python.\nCulture Biosciences created InSiliCHO, a mechanistic model of CHO cell dynamics for exploration of model-assisted DOE, forecasting, and more.\nIn Aleksa's video Building web apps using Streamlit | Streamlit crash course | MLOps series #2, you'll learn how to build a fully-fledged web app using Streamlit + HuggingFace Inference API.\nNicholas goes over How to Code a Machine Learning Lip Reading App with Python Tensorflow and Streamlit in his YouTube tutorial.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2023\nJanuary 2023\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nOctober 2022\nNovember 2022\nDecember 2022\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > March 2023",
    "url": "https://blog.streamlit.io/monthly-rewind-march-2023/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, April 24 2023\nüèÜ App of the month üèÜ\nStreamlit March updates\nüîç Current release: 1.21.0\nüîÆ Upcoming\n‚ú® New Docs feature\nFeatured Streamlit content\nüôã 10 most common explanations on the Streamlit forum\nüîç Create a search engine with Streamlit and Google Sheets\nüíª Building a PivotTable report with Streamlit and AG Grid\nüßë‚Äçüíª Hackathon 101: 5 simple tips for beginners\n#Ô∏è‚É£ Building an Instagram hashtag generation app with Streamlit\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur March featured app of the month is... ü•Åü•Åü•Åü•Åü•Å\n\nMusic Source Splitter by Fabio Grasso!\n\nThis app allows you to extract vocal and instruments from any audio. It uses a pre-trained model called Hybrid Spectrogram and Waveform Source Separation. You can upload an audio file, paste in a URL, or even record your own. [code]\n\nStreamlit March updates\n\nBelow are the latest updates and releases from March.\n\nüîç Current release: 1.21.0\n\nThe latest release is 1.21.0. Recent updates include improved st.help, support of global secrets.toml file, and a step parameter for st.time_input. Check out the changelog to learn more about the latest features and fixes.\n\nüîÆ Upcoming\n\nNew features to get excited about:\n\nEasily connect to data sources with st.experimental_connection\nCustomize the visibility items in the toolbar, options menu, and the settings dialog\nAbility to embed Streamlit apps\n\nVisit our roadmap app to see what else we're working on. ü•≥\n\n‚ú® New Docs feature\nView the source code of any Streamlit command by clicking the [source] button next to the function signature in the API reference. You can also go back and see the code from previous Streamlit versions.\n\nFeatured Streamlit content\n\nüôã 10 most common explanations on the Streamlit forum\nAre you new to Streamlit and the community forum? Check out this guide from moderator Debbie Matthews explaining the 10 most common questions with helpful tips.\n\nüîç Create a search engine with Streamlit and Google Sheets\nLearn from Sebastian Flores Benner how to build a search engine app using Streamlit, pandas, and a Google Sheets database‚Äîthe use cases are endless.\n\nüíª Building a PivotTable report with Streamlit and AG Grid\nMake a PivotTable report with Streamlit and AG Grid! Creator of the popular AgGrid component Pablo Fonseca shows you how in 4 simple steps.\n\nüßë‚Äçüíª Hackathon 101: 5 simple tips for beginners\nInterested in joining a hackathon? In this post, the Data Professor shares how to get started and 5 simple tips for a successful experience.\n\n#Ô∏è‚É£ Building an Instagram hashtag generation app with Streamlit\nBuild an Instagram hashtag generation app with Dr. William Mattingly. Learn how to scrape the data, create dynamic components with custom keys, and display a nice visualization.\n\n\nFeatured community content\n\nHere are some great apps and tutorials by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nUse ChatGPT to assist you in your job search with Vaibhav's Careermocha app.\nGuillaume and team created a ChatGPT-powered medical diagnostic and symptom-checking tool‚ÄîMedical Diagnosis Assistant.\nInteract with or create your own AI assistant with Dave's GPT Lab.\nBryan writes how to create a Streamlit app for Monitoring Sea Surface Temperature at the global level with GEE.\nEasily generate sales copy for a product using Josiah's app Salesy - The Product Copy Assistant.\nIn Fanilo's video, Streamlit App Showcase | Tips for making your app stand out, he reviews awesome apps and gives tips on how to make yours awesome too!\nCreate visually appealing and accessible color themes for your apps with Yuichiro's Streamlit color theme editor.\nVin√≠cius' LaTeX Longtable Generator app lets you generate LaTeX longtables quickly and conveniently.\nTurn images into pixel art with customizable colors and settings using the Pixel Art Converter app by Akaz.\nLearn how to Code Your Own ChatGPT Article Generator with Python & Streamlit in this video by Augmented Startups.\nEmily provides a step-by-step walkthrough of a hacky approach for SiS usage tracking in her article Tracking SiS application usage: An interim solution.\nWith Alex's Cybersyn Financial Data App you can get financial data covering bank financials, locations, corporate structures, and more.\nInput an Ethereum smart contract into Kofi's Contract Wizard app and it will describe to you what that contract does.\nIn Roel's Creating a Marketing Segmentation App with Streamlit & Snowpark article, you'll learn how to create a customer segmentation tool that can sync its selection back to the data warehouse.\n\nThanks for checking out this edition of our Monthly rewind. If you'd like more frequent updates on what's happening in the community, check out the Weekly roundups.\n\nReach out to us on the forum with any questions or projects you're working on, and follow us on Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\n2023\nJanuary 2023\nFebruary 2023\n2022\nJanuary 2022\nFebruary 2022\nMarch 2022\nApril 2022\nMay 2022\nJune 2022\nJuly 2022\nAugust 2022\nSeptember 2022\nOctober 2022\nNovember 2022\nDecember 2022\n2021\nJanuary 2021¬†\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nJune 2021\nJuly 2021\nAugust 2021\nSeptember 2021\nOctober 2021\nNovember 2021\nDecember 2021\nJanuary 2022\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > May 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-may-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > May 2021\n\nYour May look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, June 7 2021\nüèÜ App of the month üèÜ\nStreamlit May updates\nüîç Current release: 0.82.0\nüîÆ Upcoming features\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur May featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nWorst-Case Analysis for Feature Rollouts by The Crosstab Kite team.\n\nThis app shows a Bayesian analysis of a simulated staged rollout experiment. It was built as a demo to arguably show a method better than confidence intervals for this process. [code]\n\nStreamlit May updates\n\nHere are some updates on happenings at Streamlit this month.\n\nüîç Current release: 0.82.0\n\nThe latest release is 0.82.0. This update helps with memory management by running more aggressive garbage collection between scripts. If you haven't updated in a while, make sure to check out the changelog since we're continually releasing new features and fixes.\n\nüîÆ Upcoming features\n\nHere are some new features to be on the lookout for:\n\nst.download\nst.card\nSession State - coming VERY soon üëÄ\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nListen to CEO Adrien and COO Amanda talk Streamlit with Chris Chinchilla on his podcast Chinchilla Squeaks.\nIs a career in tech right for you? Amanda discusses with Apr√©s about career options in the tech field for women returning to work after caregiving.\nRead how the Rasa team are using Streamlit with Rasalit - a tool to interactively explore and investigate trained Rasa models.\nThe latest Streamlit short in the core functions series goes over how to make a double-ended slider. Read more here.\nFeatured community content\n\nSome great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nIn his video tutorial Working with Streamlit Forms, Jesse goes over how to use the latest feature release\nA Binance Premiums app from Robert tracks premia on the Binance exchange and provides tutorials on trades\nFanilo demonstrated the new st.form feature in his Choose your favorite cat app\nAndr√© and Karthik trained a model in Detecting deforestation through satellite imagery and created an awesome dashboard\nYou can create APIs from Excel XLSX and CSV data files in Arvindra's APINESS app\nCompare different battery dispatch methods and their impact on carbon and costs in this Designing a Building Battery Dispatch Strategy app by Obed\nJeff and team created Power Density, an app for initial geothermal exploration scenarios and educational use\nIn his article Productivity Tracking with the Notion API and Python Lucas shows how he built a simple project tracker dashboard with the newly released Notion API\nJesse teaches how to make a Text Classifier App with Streamlit and River Python (Online Machine Learning) in his video\nOrit made her first ever Streamlit app The G-Trendalyser which gives you top & rising trends for 5 keywords, directly from Google Trends\nOkkar's Analyzing LinkedIn Connections app provides data visualization of your network on LinkedIn\nThe Data Professor, known for his video tutorials, wrote a blog tutorial on How to Build a Machine Learning App in Python\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > April 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-april-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > April 2021\n\nYour April look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, May 5 2021\nüèÜ App of the month üèÜ\nStreamlit April updates\nüîç Current release: 0.81.0\nüîÆ Upcoming features\nüå≤ Series B\nüéà New Streamlit creators\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur April featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nPlayground by Ahmed Besbes.\n\nThis app allows you to interactively play with machine learning models directly from the browser. You can use it to test different models with varying hyper-parameters on a set of non-linear classification problems. [code]\n\nStreamlit April updates\n\nHere are some of the many exciting things that happened at Streamlit in April.\n\nüîç Current release: 0.81.0\n\nApril brought two big releases, the newest being 0.81.0. Many notable features were introduced in these releases including secrets management, forms, and submit button. If you haven't updated in a while, make sure to check out the changelog since we're continually releasing new features and fixes.\n\nüîÆ Upcoming features\n\nProgrammable state will be landing in the near future!\n\nüå≤ Series B\n\nWe announced our Series B round of funding led by Sequoia and backed by previous investors Gradient Ventures and GGV Capital. Read more from CEO Adrien in his blog post.\n\nüéà New Streamlit creators\n\nWe welcomed four new awesome community members to the Streamlit Creators program. Read more about them below and check out our creators page here.\n\nIntroducing the Q2 2021 Streamlit Creators!\nIt‚Äôs been a wild Q1 2021 for Streamlit, with the Series B announcement, the release of Secrets management and Theming for app customization! But we‚Äôre never too busy to recognize members of the Streamlit community, whose contributions breathe life into the community, demonstrate new and exciting use‚Ä¶\nStreamlit\nrandyzwitch\nFeatured Streamlit content\n\nVideos and articles written by or featuring the Streamlit team for your viewing and reading pleasure.\n\nJames outlines how to use Secrets Management in Streamlit sharing to securely connect to private data sources.\n\nThe new commands st.form and st.form_submit_button are explained by Abhi in his latest blog post.\n\nAlex C-G wrote about the Jina component and how to build a Jina neural search within your app.\n\nIn Austin's conclusion to his Firestore tutorial, he shows how to build a Reddit clone and deploy it with secrets.\n\nWe released two more Streamlit Shorts this month in the core function series: How to make a slider and a select slider. Read more about Shorts here.\nFeatured community content\n\nSome great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nRobert's Distribution Analyser app allows you to interactively explore continuous distributions from SciPy and fit distribution(s) to your data\nEthan walks through how he deployed his app in Brewing a Coffee Recommender (Part 2)\nBhavesh shows how you can track prices of cryptocurrencies in his video tutorial Bitcoin Tracking with 20 lines of Python code\nIn Misra's video, she goes into How to Make Pie Charts in Streamlit Using Plotly\nThe Python Data Visualization Tour app by Jeff visualizes plots and code for 6 Python plotting libraries at once: bokeh, altair, matplotlib, seaborn, plotly (express) & pandas\nCharly released a beta version of his app StreamProphet that lets you visualize your forecasted SEO traffic\nAnother great tutorial from 1littlecoder shows how to Build Streamlit Dashboard Template for Python Data Science | Tableau Alternative in Python\nIn his video How to deploy ML app in 2 mins | Streamlit Sharing, Anuj walks you through how to deploy your app on sharing\nGareth made interactive character interaction graphs in his Star Trek Script Analysis Dashboard\nAaron's app NFL Mock Draft Database determines probabilities of the draft results with nice visualizations\nJustin outlines how Reverie Labs is Building Web Applications From Python Scripts with Streamlit in his blog post\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > March 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-march-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > March 2021\n\nYour March look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, April 5 2021\nüèÜ App of the month üèÜ\nStreamlit March updates\nüîç Current release: 0.79.0\nüîÆ Upcoming features\nüñº New website and logo\nüêØ Tigergraph hackathon continued and ended\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur March featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nOmicLearn by ¬†OmicEra.\n\nThis app is a transparent exploration of machine learning for biomarker discovery from proteomics and omics data. It was developed to enable easy access to ML without the requirement for any programming or bioinformatic skills. [code]\n\nStreamlit March updates\n\nHere are some of the exciting things that happened at Streamlit in March.\n\nüîç Current release: 0.79.0\n\nThe newest Streamlit release is 0.79.0, which featured custom theming and tooltips. If you haven't updated in a while, make sure to check out the change log since we're continually releasing new features and fixes.\n\nüîÆ Upcoming features\n\nA quick look at some of the upcoming features planned for Q2:\n\nProgrammable state\nSubmit button\nAnchor links\nSecrets‚Äîcoming very soon, keep a lookout üëÄ\nüñº New website and logo\n\nOur website got a full redesign to put even more focus on the contributions from the community. You can view a number of community apps in the new gallery and read more about our awesome creators.\n\nOur logo also got a little polishing and we have some fun new stickers for the community.\n\nüêØ Tigergraph hackathon continued and ended\n\nStreamlit cohosted a hackathon with Tigergraph and Graphistry inspiring developers from around the world to connect, build apps and win prizes. Read more about it and view the winners here.\n\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nRead all about custom theming in the launch article written by Abhi. He goes over the details of theming as well as the new dark mode feature.\n\nMarisa walks you through how to use custom themes and dark mode in our theming tutorial.\n\nWe released a number of new Streamlit Shorts this month primarily revolving around core functions. Read more about Shorts here and be on the lookout for future series!\n\nFeatured community content\n\nSome great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nBolei shared a preview of an app exploring SeFa: Closed Form Factorization of Latent Semantics in GANs\nMala wrote a helpful tutorial on how to Build Your First Data Visualization Web App in Python Using Streamlit\nBone segmentator by William is an app made to help biodesigners in fitting internal prostheses\nIn his article, Yong shows how to Build Your First Interactive Data Science Web App with Streamlit\nJesse gave a tutorial on How to Add File Downloads To Streamlit Apps (Custom Functions) in his Youtube video\nJesse also demonstrated how to use the new theming feature in his video Streamlit Themes & How to Customize Streamlit Apps\nKen Jee began a project March Madness Solved With Machine Learning? (Can I do it?) where he'll see if his ML model can dominate March madness brackets\nSimon shared that his open source PyMedPhys app had a major release update\nHarsh gave a helpful introduction to custom themes in his video tutorial Change Colour Scheme of Your Streamlit App!\nFelipe's app APOD Project brought an astronomy picture of the day provided by NASA API\nAnother helpful Data Professor tutorial came out on How to build an app for combining the contents of multiple spreadsheets | Streamlit\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Monthly rewind > February 2021",
    "url": "https://blog.streamlit.io/monthly_rewind_february_2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > February 2021\n\nYour February look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, March 8 2021\nüèÜ App of the month üèÜ\nStreamlit February updates\nüîç Current releases: 0.78.0\nüïπ Closed betas\nüêØ Tigergraph hackathon began and is currently underway\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur February featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nBayesian Deep Learning for Galaxy Zoo DECaLS by Mike Walmsley.\n\nFrom 2015-2020, Galaxy Zoo volunteers made 1.8 million detailed classifications of 314,000 DECaLS galaxies. This app allows you to explore that data and predicts morphology posteriors. [code]\n\nStreamlit February updates\n\nA lot can happen in a month- even the shortest one of the year. Here's what you might have missed in February.\n\nüîç Current releases: 0.78.0\n\nThe newest Streamlit release is 0.78.0, but make sure to check out the change log if you haven't updated in a while to see what's been going on.\n\nüïπ Closed betas\n\nWe currently have a number of coming soon features that are being tested in closed beta, including:\n\nSecrets management\nProgrammable state\nCustomizable theming ‚Äî coming very soon, check out this sneak peek:\n\nIf you're interested in testing any of these send us a message on the forum and we'll add you to the beta!\n\nüêØ Tigergraph hackathon began and is currently underway\nStreamlit is cohosting a hackathon with Tigergraph and Graphistry through March 22nd! Connect with developers around the world and build a web app with a chance to win $15,000+ in prizes. Check out the details and sign up here.\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nListen to CEO Adrien discuss his journey from undergrad to Streamlit on Ken Jee's podcast Ken‚Äôs Nearest Neighbors.\n\nCOO Amanda's fireside chat with Glenn Solomon of GGV Capital from Open Core Summit 2020 is now available to stream. Listen in as they chat about the founding of Streamlit, community, and plans for the future.\n\nMatt Brems wrote How to Use Roboflow and Streamlit to Visualize Object Detection Output showing how to create a blood cell count detection app.\n\nFeatured community content\n\nSome great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nYuichiro Tachibana wrote in depth article about how he developed the real time video processing component WebRTC within Streamlit\nJohannes put together a best-of-streamlit list featuring great community apps ranked by GitHub stars\nMohammad outlines how to Develop and deploy a UI with Python in under 15 minutes with Streamlit\nIn another one of his helpful tutorial videos, Jesse goes over Building A Course Recommender App with Streamlit & Udemy Dataset\nYong offers 8 Simple and Useful Streamlit Tricks You Should Know in this article to help better your apps\nThe Python Engineer made a video tutorial on how build a stock prediction web app in Python using Streamlit, Yahoo Finance, and Facebook Prophet\nAn NLP app by Charly, Wiki Topic Grapher, retrieves entity relationships from any Wikipedia seed topic. You can then get a network graph of these connected entities, save the graph as jpg or export the results ordered by salience to CSV\nYash's article A Guide to Streamlit ‚Äî Frontend for Data Science Made Simpler walks through the important steps in making your project great\nPart Time Larry demonstrates how to build Financial Dashboards with Python in this thorough video\nAn interactive Game of Thrones dashboard by Mario depicts interesting character analysis based on the TV scripts of the show\nRahul wrote an article on how to make your GitHub profile stand out by using his GitHub Profile README app\nSee and explore beautifully displayed Norwegian Meteorological data in this app by Gregoire\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit",
    "url": "https://blog.streamlit.io/p/efb2b033-a8e4-47ec-bb60-f1313d84052a/discuss.streamlit.io/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n404\nPage not found\n‚Üê Go to the front page\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image-2.png (1200√ó1041)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-2.png#border",
    "html": ""
  },
  {
    "title": "appspart2.png (2000√ó1628)",
    "url": "https://blog.streamlit.io/content/images/2021/08/appspart2.png#border",
    "html": ""
  },
  {
    "title": "databasesnew.png (1488√ó930)",
    "url": "https://blog.streamlit.io/content/images/2021/07/databasesnew.png",
    "html": ""
  },
  {
    "title": "Teaser-GIF-2.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Teaser-GIF-2.gif#browser",
    "html": ""
  },
  {
    "title": "Jessica Smith - Streamlit",
    "url": "https://blog.streamlit.io/author/jessica/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jessica Smith\n26 posts\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nMonthly rewind > November 2022\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2022\nMonthly rewind > October 2022\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2022\nMonthly rewind > September 2022\n\nYour September look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2022\nMonthly rewind > August 2022\n\nYour August look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2022\nMonthly rewind > July 2022\n\nYour July look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 9 2022\nMonthly rewind > June 2022\n\nYour June look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 6 2022\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1.png (1061√ó601)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1.png#browser",
    "html": ""
  },
  {
    "title": "group_selection_example.gif (1356√ó704)",
    "url": "https://blog.streamlit.io/content/images/2021/08/group_selection_example.gif#browser",
    "html": ""
  },
  {
    "title": "Labeling ad videos with Streamlit",
    "url": "https://blog.streamlit.io/labeling-ad-videos-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLabeling ad videos with Streamlit\n\nHow Wavo.me uses Streamlit‚Äôs Session State to create labeling tasks\n\nBy Anastasia Glushko\nPosted in Advocate Posts, September 2 2021\n1. Why does Wavo label ad videos?\n2. What makes Streamlit a great labeling solution?\n3. How to build a creative labeling app using Streamlit's SessionState\nCome up with a simple app layout\nUse Session State to remember users and labeled videos\nMake the app shine with some fun add-ons\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nThe clips labeled in the app preview are sample cutouts from a music video \"Pony\" by a Montreal-based hip-hop artist SLM.\n\nMarketing ROI (return-on-investment) in the music industry is unpredictable. To break through, new artists have to invest in advertising. But will it succeed? A lot depends on the ad creative's quality.\n\nThere are many tips out there on how to make a video ad stand out. But advertising music is different from advertising physical products, like clothing and house decor: music can be consumed directly in the ad.\n\nAt Wavo, we use the latest machine learning technology to make investing in artists more predictable and scalable. Our unique combination of technology and music industry expertise helps develop data-driven solutions to understand and optimize the quality of music marketing.\n\nIn this post, we'll cover:\n\nWhy does Wavo label ad videos?\nWhat makes Streamlit a great labeling solution?\nHow to build a creative labeling app using Streamlit's SessionState\n\nLet's jump right in!\n\n1. Why does Wavo label ad videos?\n\nAds perform according to the features of the promotional materials used in them, or Ad Creatives (AC for short).\n\nLabeling AC features with Streamlit makes it possible to link them to performance metrics such as click-through rate, impressions, and views. By analyzing labeled data, we can better understand which ad features drive these performance metrics and build more effective campaigns.\n\nTo test this, we created a protocol for a proof-of-concept data science project. Our goal was to link metrics like click-through rate and video views to specific AC features and best practices.\n\nWe had three big resources:\n\n500 music video clips\n10 creative best practices from 5 years of running music advertising campaigns\n22 media analysts with vast experience running music marketing campaigns\n2. What makes Streamlit a great labeling solution?\n\nAt first, we looked into using AWS Ground Truth and LimeSurvey. Both tools were quite powerful. ¬†But for our purposes, most features would go unused and only add complexity to the labeling process. We wanted a fast, simple, and inexpensive prototype, and Streamlit fit the bill!\n\nI knew about Streamlit from my Insight Fellows days. While I spent days struggling with editing Bootstrap templates in html (ugh!), other ‚Äúfellows‚Äù from my cohort used Streamlit. Their first machine learning apps were up in hours, all made in Python. The code looked impressively straightforward. I made a mental note to use only Streamlit going forward.\n\nA year later, I had the opportunity to try it. I crossed my fingers that Streamlit could help me create a cohesive questionnaire: present a creative labeling form, store the labeler's response, and move on to the next AC. As it turned out, it could.\n\n3. How to build a creative labeling app using Streamlit's SessionState\n\nCome up with a simple app layout\n\nWe built the labeling questionnaire in one sprint, with 150 lines of Python code.\n\nYou can do it, too! Here's how it worked.\n\nThe user (our in-house marketing expert) selected their name from a dropdown of user IDs. This triggered an individual list of ACs to be loaded by the app. The first video creative appeared on the screen with the corresponding labeling tasks. The user then watched the video and rated the creative quality (low, medium, or high).\n\nNext, the user entered the length of the video. Several checkboxes appeared‚Äîone for each creative best practice we recommend our clients to follow. The user had then selected all the best practices. This also included flags for the creative type: whether the video was a static visualizer or an animation.\n\nUse Session State to remember users and labeled videos\n\nWithout the use of Session State, a simple Streamlit app would run your Python script from top to bottom. With every rerun of the script, it would lose any changes the user had made in the browser. The SessionState feature enables the simple persistence of these browser state changes.\n\nWe needed our app to retain two things in Session State, so that it could progress through our list of questions: the user ID (the name of the labeling expert) and the current question number. The user ID was taken from the user‚Äôs selection in the dropdown menu:\n\nid_provided = st.selectbox('Hello! Who is this?', user_ids) \n\n# user_ids is a list defined above\n\n\nTo allow our labelers to start the questionnaire, take a break, close the app, and return to it later, we stored the answer to every question in a .csv file.\n\nOn every run of the script, Session State would be updated with the index of the next unanswered question. So whenever the user would return, they'd be exactly where they left off.\n\nThe same code ensured that the question number was updated on every consecutive run of the app (corresponding to all the labeling tasks for one of the video creatives):\n\nimport os\nimport pandas as pd\nimport streamlit as st\n\n# check if the user is new or returning\noutput_filename = ‚Äò./results_‚Äô + str(id_provided) + ‚Äò.csv‚Äô\n\tdf = pd.DataFrame()\n\tif os.path.isfile(output_filename):\n\t\tdf = pd.read_csv(output_filename)\n\t\n\tif df.shape[0] > 0:\n\t\tlast_row = df.shape[0] - 1\n\t\tquestion_number = int(df.iloc[last_row].q_num) + 1\n\telse:\n\t\tquestion_number = 0\n\n# defining our Session State\nst.session_state.user_id = id_provided\nst.session_state.question_number = question_number\n\n\nWhen the user clicked through the labeling tasks for a given video and the script reruns, the Session State was updated: the id_provided didn‚Äôt change (unless the user selected a different ID in the dropdown menu) and the question_number increased by one. For every video labeling iteration, the question_number was used to access the right link from the list of the videos to label.\n\nMake the app shine with some fun add-ons\n\nTo showcase some more fun use cases of Session State, we also had a progress bar at the top of the page:\n\nst.progress((st.session_state.question_number)/(len(creatives))) \n\n# creatives is a list of links to the labelled video ads\n\n\nAnd then there‚Äôs one final reward:\n\nif st.session_state.question_number >= len(creatives):\n\t\tst.text('THANK YOU, YOU ARE DONE!')\n\t\tst.balloons()\n\t\tst.stop()\n\n\nWrapping up\n\nOver the course of one week, 23 marketing experts labeled 500 creatives. This allowed us to report data-driven insights about creative quality to our clients and build better marketing campaigns.\n\nWith Streamlit, developing a custom and user-friendly labeling questionnaire for this project took just a few hours of work for one data scientist. 10/10 we would use Streamlit again.\n\nGot questions? Let me know in the comments or via email.\n\nArticle by\nAnastasia Glushko\nMachine Learning Researcher\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Make a video content analyzer app with Streamlit and AssemblyAI",
    "url": "https://blog.streamlit.io/make-a-video-content-analyzer-app-with-streamlit-and-assemblyai/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nBy Misra Turp\nPosted in Advocate Posts, November 3 2022\nWhat's a content analyzer app?\nStep 1. Create an AssemblyAI account\nStep 2. Collect user input\nStep 3. Submit the video to AssemblyAI for analysis\nStep 4. Receive analysis results from AssemblyAI\nStep 5. Display analysis results\nVideo Summary\nSensitive Topics\nTopic Detection\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHello, community! üëã\n\nMy name is Mƒ±sra Turp, and I work as a developer educator at AssemblyAI.\n\nWe partner with YouTube creators to make helpful content for the AI community. Our creators make videos about technology, AI, deep learning, and machine learning. We screen the videos for sensitive or harmful content, which can take a long time. So I made an AI-powered app that can analyze video content automatically (yay!).\n\nIn this post, I'll share with you how to build and use the content analyzer app step-by-step:\n\nCreate an AssemblyAI account\nCollect user input with Streamlit\nSubmit the video to AssemblyAI for analysis\nReceive analysis results from AssemblyAI\nPresent analysis results in 3 sections: summary, topics, sensitive content\n\nBut before we start, let's talk about...\n\nüí°\nCan‚Äôt wait to see it? Here‚Äôs the app and the Github repo with all the code. And check out this video tutorial on the same topic!\nWhat's a content analyzer app?\n\nThe content analyzer app helps you evaluate the videos of a given channel very quickly. Given a list of videos in a TXT file, the app gives the user the option to select a video to analyze by visualizing the video's thumbnails:\n\nOnce a video is selected, the app reports a summary of the video, a list of sensitive topics (if any), and all the topics that were discussed:\n\nStep 1. Create an AssemblyAI account\n\nTo start, create a free AssemblyAI account and get a Free API key. Then install Streamlit, pandas, requests, and pytube Python libraries:\n\npip install streamlit\npip install pandas\npip install requests\npip install pytube\n\nStep 2. Collect user input\n\nNext, build the skeleton of the Streamlit app by including instructions and headlines:\n\nimport streamlit as st\n\nst.title(\"Analyze a YouTube channel's content\")\nst.markdown(\"With this app you can audit a Youtube channel to see if you'd like to sponsor them. All you have to do is to pass a list of links to the videos of this channel and you will get a list of thumbnails. Once you select a video by clicking its thumbnail, you can view:\")\nst.markdown(\"1. a summary of the video,\") \nst.markdown(\"2. the topics that are discussed in the video,\") \nst.markdown(\"3. whether there are any sensitive topics discussed in the video.\")\nst.markdown(\"Make sure your links are in the format: <https://www.youtube.com/watch?v=HfNnuQOHAaw> and not <https://youtu.be/HfNnuQOHAaw>\")\n\n\nNow you need a list of URLs from the user. They can upload it through the Streamlit file_uploader. If the user doesn't have anything yet and wants to play around with the app, they can select a checkbox to use a default TXT file:\n\ndefault_bool = st.checkbox('Use default example file', )\n\nif default_bool:\n\tfile = open('./links.txt')\nelse:  \n\tfile = st.file_uploader('Upload a file that includes the video links (.txt)')\n\n\nNext, download the audio of this video from YouTube by casting the given TXT file into a Python list and passing each URL through a function called save_audio:\n\nif file is not None:\n\tprint(file)\n\tdataframe = pd.read_csv(file, header=None)\n\tdataframe.columns = ['video_url']\n\turls_list = dataframe[\"video_url\"].tolist()\n\t\n\ttitles = []\n\tlocations = []\n\tthumbnails = []\n\t\n\tfor video_url in urls_list:\n\t  video_title, save_location, thumbnail_url = save_audio(video_url)\n\t  titles.append(video_title)\n\t  locations.append(save_location)\n\t  thumbnails.append(thumbnail_url)\n\n#\t... (continued if file is not None:)\n\n\nThis function returns the video title, the location where the audio is saved, and the thumbnail. The information is saved in separate lists:\n\nfrom pytube import YouTube\nimport os\n\n@st.experimental_memo\ndef save_audio(url):\n\tyt = YouTube(url)\n\tvideo = yt.streams.filter(only_audio=True).first()\n\tout_file = video.download()\n\tbase, ext = os.path.splitext(out_file)\n\tfile_name = base + '.mp3'\n\tos.rename(out_file, file_name)\n\tprint(yt.title + \" has been successfully downloaded.\")\n\tprint(file_name)\n\treturn yt.title, file_name, yt.thumbnail_url\n\n\nNow that you have the information on every video, you can display it in a grid for the user to select one. There is no default Streamlit widget for this, but Streamlit user vivien made a custom component that displays clickable images in a grid.\n\nTo create this widget, pass the list of thumbnails, the list of video names (to show when hovered over the thumbnail), and the styling details to the constructor. If there are too many URLs, the display might take up too much space. To avoid that, in the div_style set \"overflow-y\":\"auto\" to have a scroll bar.\n\nWhichever thumbnail the user clicks, that video will be selected for the analysis:\n\nfrom st_clickable_images import clickable_images\n\n#\t... (continued if file is not None:)\n\n  selected_video = clickable_images(thumbnails,\n  titles = titles,\n  div_style={\"height\": \"400px\", \"display\": \"flex\", \"justify-content\": \"center\", \"flex-wrap\": \"wrap\", \"overflow-y\":\"auto\"},\n  img_style={\"margin\": \"5px\", \"height\": \"150px\"}\n  )\n\n  st.markdown(f\"Thumbnail #{selected_video} clicked\" if selected_video > -1 else \"No image clicked\")\n\n#\t... (continued if file is not None:)\n\n\nStep 3. Submit the video to AssemblyAI for analysis\n\nIf the user doesn't select a video, the widget will return -1. If the user selects a video from the grid, the widget will return the video's number starting from 0. Using this number, we can also get the video's URL, title, and location from the lists we previously created. The app will display the title of the selected video and an audio player:\n\n#\t... (continued if file is not None:)\n\nif selected_video > -1:\n\tvideo_url = urls_list[selected_video]\n  video_title = titles[selected_video]\n  save_location = locations[selected_video]\n          \n  st.header(video_title)\n  st.audio(save_location)\n\n# ... (continued if selected_video > -1:)\n\n\nOnce a video is selected, you need to:\n\nUpload the audio to AssemblyAI;\nStart the analysis;\nRead the results;\n\nFor each of these steps, there is a separate function. To use these functions:\n\nImport requests library to send requests to AssemblyAI;\nSet up transcription and uploading endpoints (addresses) to communicate with AssemblyAI;\nSet up a header to specify the content-type and authenticate your app with the API key that you created earlier;\n\nI've uploaded this app to Streamlit Community Cloud and am specifying st.secrets[\"auth_key\"] as the location of the API key. If you'd like, you can also directly pass your API key there:\n\nimport requests\n\ntranscript_endpoint = \"<https://api.assemblyai.com/v2/transcript>\"\nupload_endpoint = \"<https://api.assemblyai.com/v2/upload>\"\n\nheaders = {\n\t\"authorization\": st.secrets[\"auth_key\"],\n\t\"content-type\": \"application/json\"\n}\n\n\nThe first step is to upload this audio file to AssemblyAI with the function upload_to_AssemblyAI. Using the helper function read_file, the function reads the audio file in the given location (save_location) in chunks. This is used in the post request that is sent to the upload_endpoint of AssemblyAI together with the header for authentication.\n\nAs a response, you get the URL to where the audio file is uploaded:\n\n@st.experimental_memo\ndef upload_to_AssemblyAI(save_location):\n\tCHUNK_SIZE = 5242880\n\t\n\tdef read_file(filename):\n\t  with open(filename, 'rb') as _file:\n\t\t  while True:\n\t      print(\"chunk uploaded\")\n\t      data = _file.read(CHUNK_SIZE)\n\t      if not data:\n\t        break\n        yield data\n\t\n\tupload_response = requests.post(\n\t  upload_endpoint,\n\t  headers=headers, data=read_file(save_location)\n\t)\n\tprint(upload_response.json())\n\t\n\taudio_url = upload_response.json()['upload_url']\n\tprint('Uploaded to', audio_url)\n\t\n\treturn audio_url\n\n\nNext, you pass the audio_url to the start_analysis function. This function sends another post request to AssemblyAI to start the analysis.\n\nBy default, all submitted audio is transcribed at AssemblyAI. To start a transcription (or analysis) job, specify the audio file URL, the authentication details, and the kind of analysis you want (read our docs for a list of deep learning models).\n\nHere you'll use three AssemblyAI models:\n\nThe Summarization model‚Äîto return the summary of this audio file;\nThe Content Moderation model‚Äîto flag potentially sensitive and harmful content on topics such as alcohol, violence, gambling, and hate speech;\nThe Topic Detection model‚Äîto detect up to 700 topics (automotive, business, technology, education, standardized tests, inflation, off-road vehicles, and so on);\n\nThe summarization model can give you different summaries:\n\nA bullet list (bullets)\nA longer bullet list (bullets_verbose)\nA few words (gist)\nA sentence (headline)\nA paragraph (paragraph)\n\nThe analysis will take a few seconds or minutes, depending on the length of the audio file. As a response to the transcription job, you'll get a job ID. Use it to create a polling endpoint to receive the analysis results:\n\n@st.experimental_memo\ndef start_analysis(audio_url):\n\t\n\t## Start transcription job of audio file\n\tdata = {\n\t    'audio_url': audio_url,\n\t    'iab_categories': True,\n\t    'content_safety': True,\n\t    \"summarization\": True,\n\t    \"summary_type\": \"bullets\"\n\t}\n\t\n\ttranscript_response = requests.post(transcript_endpoint, json=data, headers=headers)\n\tprint(transcript_response)\n\t\n\ttranscript_id = transcript_response.json()['id']\n\tpolling_endpoint = transcript_endpoint + \"/\" + transcript_id\n\t\n\tprint(\"Transcribing at\", polling_endpoint)\n\treturn polling_endpoint\n\nStep 4. Receive analysis results from AssemblyAI\n\nThe last step is to collect the analysis results from AssemblyAI. The results are not generated instantaneously. Depending on the length of the audio file, the analysis might take a couple of seconds to a couple of minutes. To keep it simple and reusable, the process of receiving the analysis is wrapped in a function called get_analysis_results.\n\nIn a while loop, every 10 seconds, a get request will be sent to AssemblyAI through the polling endpoint that includes the transaction job ID. In response to this get request, you'll get the job status as, \"queued\", ‚Äúsubmitted‚Äù, ‚Äúprocessing‚Äù, or ‚Äúcompleted‚Äù.\n\nOnce the status is \"completed\", the results are returned:\n\n@st.experimental_memo\n\tdef get_analysis_results(polling_endpoint):\n\t\n\tstatus = 'submitted'\n\t\n\twhile True:\n\t  print(status)\n\t  polling_response = requests.get(polling_endpoint, headers=headers)\n\t  status = polling_response.json()['status']\n\t  # st.write(polling_response.json())\n\t  # st.write(status)\n\t\n\t  if status == 'submitted' or status == 'processing' or status == 'queued':\n\t    print('not ready yet')\n\t    sleep(10)\n\t\n\t  elif status == 'completed':\n\t    print('creating transcript')\n\t    return polling_response\n\t\t\tbreak\n\n\t  else:\n\t    print('error')\n\t    return False\n\t    break\n\nStep 5. Display analysis results\n\nYou get three types of analysis on your audio:\n\nSummarization;\nSensitive content detection;\nTopic detection;\n\nLet's display them in order.\n\nExtract the information with the ‚Äúsummary‚Äù keyword for the summarization results,‚Äúcontent_safety_labels‚Äù for content moderation and ‚Äúiab_categories_result‚Äù for topic detection. Here is an example response:\n\n{\n    \"audio_duration\": 1282,\n    \"confidence\": 0.9414384528795772,\n    \"id\": \"oeo5u25f7-69e4-4f92-8dc9-f7d8ad6cdf38\",\n    \"status\": \"completed\",\n    \"text\": \"Ted talks are recorded live at the Ted Conference. This episode features...\",\n    \"summary\": \"- Dan Gilbert is a psychologist and a happiness expert. His talk is recorded live at Ted conference. He explains why the human brain has nearly tripled in size in 2 million years. He also explains the difference between winning the lottery and becoming a paraplegic.\\\\n- In 1994, Pete Best said he's happier than he would have been with the Beatles. In the free choice paradigm, monet prints are ranked from the one they like the most to the one that they don't. People prefer the third one over the fourth one because it's a little better.\\\\n- People synthesize happiness when they change their affective. Hedonic aesthetic reactions to a poster. The ability to make up your mind and change your mind is the friend of natural happiness. But it's the enemy of synthetic happiness. The psychological immune system works best when we are stuck. This is the difference between dating and marriage. People don't know this about themselves and it can work to their disadvantage.\\\\n- In a photography course at Harvard, 66% of students choose not to take the course where they have the opportunity to change their mind. Adam Smith said that some things are better than others. Dan Gilbert recorded at Ted, 2004 in Monterey, California, 2004.\",\n    \"content_safety_labels\": {\n        \"status\": \"success\",\n        \"results\": [\n            {\n                \"text\": \"Yes, that's it. Why does that happen? By calling off the Hunt, your brain can stop persevering on the ugly sister, giving the correct set of neurons a chance to be activated. Tip of the tongue, especially blocking on a person's name, is totally normal. 25 year olds can experience several tip of the tongues a week, but young people don't sweat them, in part because old age, memory loss, and Alzheimer's are nowhere on their radars.\",\n                \"labels\": [\n                    {\n                        \"label\": \"health_issues\",\n                        \"confidence\": 0.8225132822990417,\n                        \"severity\": 0.15090347826480865\n                    }\n                ],\n                \"timestamp\": {\n                    \"start\": 358346,\n                    \"end\": 389018\n                }\n            },\n            ...\n        ],\n        \"summary\": {\n            \"health_issues\": 0.8750781728032808\n            ...\n        },\n        \"severity_score_summary\": {\n            \"health_issues\": {\n                \"low\": 0.7210625030587972,\n                \"medium\": 0.2789374969412028,\n                \"high\": 0.0\n            }\n        }\n    },\n    \"iab_categories_result\": {\n        \"status\": \"success\",\n        \"results\": [\n            {\n                \"text\": \"Ted Talks are recorded live at Ted Conference...\",\n                \"labels\": [\n                    {\n                        \"relevance\": 0.0005944414297118783,\n                        \"label\": \"Religion&Spirituality>Spirituality\"\n                    },\n                    {\n                        \"relevance\": 0.00039072768413461745,\n                        \"label\": \"Television>RealityTV\"\n                    },\n                    {\n                        \"relevance\": 0.00036419558455236256,\n                        \"label\": \"MusicAndAudio>TalkRadio>EducationalRadio\"\n                    }\n                ],\n                \"timestamp\": {\n                    \"start\": 8630,\n                    \"end\": 32990\n                }\n            },\n            ...\n        ],\n        \"summary\": {\n            \"MedicalHealth>DiseasesAndConditions>BrainAndNervousSystemDisorders\": 1.0,\n            \"FamilyAndRelationships>Dating\": 0.7614801526069641,\n            \"Shopping>LotteriesAndScratchcards\": 0.6330153346061707,\n            \"Hobbies&Interests>ArtsAndCrafts>Photography\": 0.6305723786354065,\n            \"Style&Fashion>Beauty\": 0.5269057750701904,\n            \"Education>EducationalAssessment\": 0.49798518419265747,\n            \"BooksAndLiterature>ArtAndPhotographyBooks\": 0.45763808488845825,\n            \"FamilyAndRelationships>Bereavement\": 0.45646440982818604,\n            \"FineArt>FineArtPhotography\": 0.3921416699886322,\n        }\n}\n\n\nFirst, call each of the functions defined above and then parse results to get each part of the analysis:\n\n# ... (continued if selected_video > -1:)\t\n\n\t# upload mp3 file to AssemblyAI\n  audio_url = upload_to_AssemblyAI(save_location)\n\n  # start analysis of the file\n  polling_endpoint = start_analysis(audio_url)\n\n  # receive the results\n  results = get_analysis_results(polling_endpoint)\n\t\n\t# separate analysis results\n\tbullet_points = results.json()['summary']\n\tcontent_moderation = results.json()[\"content_safety_labels\"]\n\ttopic_labels = results.json()[\"iab_categories_result\"]\n\n# ... (continued if selected_video > -1:)\t\n\nVideo Summary\n\nIt's easy to display the summary since it comes in a nicely formatted bullet list. You only need to extract it from the JSON response and display it with st.write():\n\n# ... (continued if selected_video > -1:)\t\t\n\n\tst.header(\"Video summary\")\n\tst.write(bullet_points)\n\n# ... (continued if selected_video > -1:)\t\n\n\nSensitive Topics\n\nThe content moderation model will give you detailed information on the following:\n\nThe sentence that caused this audio to be flagged\nThe timestamp of when it starts and ends\nThe severity of this sensitive topic\nThe confidence in this detection\n\nIn the context of this project, the user doesn't need to see this much detail, so let's display the summary of this analysis as a pandas dataframe:\n\n# ... (continued if selected_video > -1:)\t\n\nst.header(\"Sensitive content\")\n\tif content_moderation['summary'] != {}:\n\t  st.subheader('üö® Mention of the following sensitive topics detected.')\n\t  moderation_df = pd.DataFrame(content_moderation['summary'].items())\n\t  moderation_df.columns = ['topic','confidence']\n\t  st.dataframe(moderation_df, use_container_width=True)\n\telse:\n\t  st.subheader('‚úÖ All clear! No sensitive content detected.')\n\n# ... (continued if selected_video > -1:)\t\n\n\nThe result will look like this:\n\nOr like this:\n\nTopic Detection\n\nThe topic detection model will give you similar results. Once pasted into a pandas dataframe, structure it to have a separate column for each topic granularity level, then sort it with the confidence from the topic detection model:\n\n# ... (continued if selected_video > -1:)\t\n\n\tst.header(\"Topics discussed\")\n\ttopics_df = pd.DataFrame(topic_labels['summary'].items())\n\ttopics_df.columns = ['topic','confidence']\n\ttopics_df[\"topic\"] = topics_df[\"topic\"].str.split(\">\")\n\texpanded_topics = topics_df.topic.apply(pd.Series).add_prefix('topic_level_')\n\ttopics_df = topics_df.join(expanded_topics).drop('topic', axis=1).sort_values(['confidence'], ascending=False).fillna('')\n\t\n\tst.dataframe(topics_df, use_container_width=True)\n\n# ... (continued if selected_video > -1:)\t\n\n\nWrapping up\n\nAnd that's a wrap! Whew. You did it.\n\nThis content analyzer app makes analyzing YouTube super easy, doesn't it? Once you connect to AssemblyAI, you can make more Streamlit apps. Check out our documentation to learn about our other state-of-the-art models and how to use them to get information from your audio or video files. Or watch my video tutorial, where I talk about this in detail.\n\nIf you have any questions about this app or if you build an app by using both AssemblyAI and Streamlit, please comment below or reach out to me on Twitter or YouTube.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "webrtc-1.gif (800√ó728)",
    "url": "https://blog.streamlit.io/content/images/2021/08/webrtc-1.gif#border",
    "html": ""
  },
  {
    "title": "App Layout Primitives: Columns, Containers & Expanders",
    "url": "https://blog.streamlit.io/introducing-new-layout-options-for-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nNew layout options for Streamlit\n\nIntroducing new layout primitives‚Äîcolumns, containers, and expanders!\n\nBy Austin Chen\nPosted in Product, October 8 2020\nGo horizontal with columns\nClean things up with expanders\nAdding a new concept: containers!\nOrganize your code with... with\nThat's all, folks!\nResources\nKudos\nContents\nShare this post\n‚Üê All posts\n\nStreamlit is all about simplicity. It‚Äôs pure Python. Your script runs from top to bottom. Your app renders from top to bottom too. Perfect, right? Well...not quite. Users noted that our thinking was a bit too vertical. The group griped about grids. The community clamored for columns. Fervent friends favored flexibility. You get the idea.\n\nSo move aside, vertical layout. Make a little space for... horizontal layout! And a bunch more layout primitives. And some syntactic goodies too. In fact, today, we're introducing four new layout features giving you much more control over your app‚Äôs presentation.\n\nst.columns: Side-by-side columns where you can insert Streamlit elements\nst.expander: An expand/collapse widget to selectively show stuff\nst.container: The fundamental building block of layout\nwith column1: st.write(\"hi!\"): Syntax sugar to specify which container to use\nGo horizontal with columns\n\nst.columns acts similarly to our beloved st.sidebar, except now you can put the columns anywhere in your app. Just declare each column as a new variable, and then you can add in ANY element or component available from the Streamlit library.\n\nUse columns to compare things side-by-side:\n\n\ncol1, col2 = st.columns(2)\n\noriginal = Image.open(image)\ncol1.header(\"Original\")\ncol1.image(original, use_column_width=True)\n\ngrayscale = original.convert('LA')\ncol2.header(\"Grayscale\")\ncol2.image(grayscale, use_column_width=True)\n\n\n\nIn fact, by calling st.columns inside a loop, you get a grid layout!\n\n\nst.title(\"Let's create a table!\")\nfor i in range(1, 10):\n    cols = st.columns(4)\n    cols[0].write(f'{i}')\n    cols[1].write(f'{i * i}')\n    cols[2].write(f'{i * i * i}')\n    cols[3].write('x' * i)\n    \n\nYou can even get quite complex (which can be great for wide monitors!) Here's an example that uses variable-width columns in conjunction with the wide-mode layout:\n\n\n# Use the full page instead of a narrow central column\nst.set_page_config(layout=\"wide\")\n\n# Space out the maps so the first one is 2x the size of the other three\nc1, c2, c3, c4 = st.columns((2, 1, 1, 1))\n\n\n\nAnd just in case you were wondering: yes, columns are beautiful across devices and automatically resize for mobile and different browser widths.\n\nClean things up with expanders\n\nNow that we've maximized horizontal space, try st.expander, to maximize your vertical space! Some of you may have been using st.checkbox for this before, and expander is a prettier, more performant replacement üôÇ\n\nIt's a great way to hide your secondary controls, or provide longer explanations that users can toggle!\n\nAdding a new concept: containers!\n\nIf you squint a bit, st.columns, st.expander, and st.sidebar look kind of similar. They all return Python objects, which allow you to call all the Streamlit functions. ¬†We've given these objects a new name: containers. And since it would be nice to create containers directly, you can!\n\nst.container is a building block that helps you organize your app. Just like st.empty, st.container lets you set aside some space, and then later write things to it out of order. But while subsequent calls to the same st.empty replace the item inside it, subsequent calls to the same st.container append to it. Once again, this works just like the st.sidebar you've come to know and love.\n\nOrganize your code with... with\n\nFinally, we're introducing a new syntax to help you manage all these new containers: with container. How does it work? Well, instead of making function calls directly on the container...\n\n\nmy_expander = st.expander()\nmy_expander.write('Hello there!')\nclicked = my_expander.button('Click me!')\n\n\n\nUse the container as a Context Manager and call functions from the st. namespace!\n\n\nmy_expander = st.expander(label='Expand me')\nwith my_expander:\n    'Hello there!'\n    clicked = st.button('Click me!')\n    \n\nWhy? This way, you can compose your own widgets in pure Python, and reuse them in different containers!\n\n\ndef my_widget(key):\n    st.subheader('Hello there!')\n    return st.button(\"Click me \" + key)\n\n# This works in the main area\nclicked = my_widget(\"first\")\n\n# And within an expander\nmy_expander = st.expander(\"Expand\", expanded=True)\nwith my_expander:\n    clicked = my_widget(\"second\")\n\n# AND in st.sidebar!\nwith st.sidebar:\n    clicked = my_widget(\"third\")\n    \n\nOne last thing: the with syntax lets you put your Custom Components inside any container you like. Check out this app by community member Sam Dobson, which embeds the Streamlit Ace editor in a column right next to the app itself ‚Äî so a user can edit the code and see the changes LIVE!\n\nThat's all, folks!\n\nTo start playing with layout today, simply upgrade to the latest version of Streamlit.\n\n\n$ pip install streamlit --upgrade\n\n\n\nComing up are updates with padding, alignment, responsive design, and UI customization. Stay tuned for that, but most importantly, let us know what YOU want from layout. Questions? Suggestions? Or just have a neat app you want to show off? Join us on the Streamlit community forum ‚Äî we can't wait to see what you create üéà\n\nResources\n\nDocumentation\nGitHub\nChangelog\n\nKudos\n\nA shoutout to the Streamlit Community and Creators, whose feedback really shaped the implementation of Layout: Jesse, Jos√©, Charly, and Synode ‚Äî and a special callout to Fanilo for going the extra mile to find bugs, suggest APIs, and overall try out a bunch of our prototypes. Thank you all so much ‚ù§Ô∏è\n\nEdit, 2021-08-2021: This post has been updated to fix a bug in the context manager section, as well as to reflect the removal of the beta_ prefix from several functions.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "cropped-gif.gif (1094√ó619)",
    "url": "https://blog.streamlit.io/content/images/2021/08/cropped-gif.gif#browser",
    "html": ""
  },
  {
    "title": "4-2.png (1200√ó595)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4-2.png#border",
    "html": ""
  },
  {
    "title": "Screen_Shot_2021-04-26_at_5.06.52_PM--1-.png (731√ó196)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-04-26_at_5.06.52_PM--1-.png#border",
    "html": ""
  },
  {
    "title": "2-3-1.png (1048√ó451)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-3-1.png#border",
    "html": ""
  },
  {
    "title": "image-7.png (968√ó312)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-7.png#border",
    "html": ""
  },
  {
    "title": "image-6.png (1802√ó972)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-6.png#browser",
    "html": ""
  },
  {
    "title": "Screen_Shot_2021-04-26_at_5.05.10_PM--1-.png (723√ó192)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Screen_Shot_2021-04-26_at_5.05.10_PM--1-.png#border",
    "html": ""
  },
  {
    "title": "image-5.png (2000√ó1237)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-5.png#browser",
    "html": ""
  },
  {
    "title": "image-3.png (2039√ó1066)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-3.png#browser",
    "html": ""
  },
  {
    "title": "image-2-1.png (2000√ó1004)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-2-1.png#browser",
    "html": ""
  },
  {
    "title": "Screen_Shot_2021-06-03_at_4.15.58_PM--1--1.png (1848√ó454)",
    "url": "https://blog.streamlit.io/content/images/2021/06/Screen_Shot_2021-06-03_at_4.15.58_PM--1--1.png",
    "html": ""
  },
  {
    "title": "Elm Tutorial | How to Build Streamlit Components Using Elm",
    "url": "https://blog.streamlit.io/elm-meet-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nElm, meet Streamlit\n\nA tutorial on how to build Streamlit components using Elm\n\nBy Henrikh Kantuni\nPosted in Tutorials, December 8 2020\nReady Player One\nBrave New World\nThere and Back Again\nFoundation\nContents\nShare this post\n‚Üê All posts\n\nLet me start this article by saying‚ÄîI love Elm!\n\nI enjoy learning new programming languages, and Elm has been my favorite language for almost 2 years now. I like everything about it - compiler error messages, type system soundness, The Elm Architecture, pure functions, immutability, performance, etc. Around the time I discovered Elm, I got a job at Streamlit, and I immediately saw the potential for how Elm and Streamlit could work together to supercharge apps. With the release of the Streamlit components architecture earlier this year, this was finally possible, and I'm excited to show you how! To get a taste, check out this awesome Elm line charts library embedded into a Python data app.\n\nBefore we start, if you're new to Streamlit and would like to learn more about it - check out these two articles \"Intro to Streamlit\" and \"Intro to Streamlit components\".\n\nReady Player One\n\nHere's the \"Hello World\" of Elm examples:\n\n\nimport Browser\nimport Html exposing (Html, button, div, text)\nimport Html.Events exposing (onClick)\n\nmain =\n  Browser.sandbox { init = 0, update = update, view = view }\n\ntype Msg = Increment | Decrement\n\nupdate msg model =\n  case msg of\n    Increment ->\n      model + 1\n\n    Decrement ->\n      model - 1\n\nview model =\n  div []\n    [ button [ onClick Decrement ] [ text \"-\" ]\n    , div [] [ text (String.fromInt model) ]\n    , button [ onClick Increment ] [ text \"+\" ]\n    ]\n\n\n\nIt is a simple counter app that demonstrates the simplicity, robustness, and beauty of The Elm Architecture.\n\nWe're going to build a Streamlit app that will use the above example as a Streamlit component. Streamlit components let you expand the functionality provided in the base Streamlit package. You can use Streamlit components to share any web-based UI, widget, or data visualization code with the broader Python data science community.\n\nCreating a Streamlit component takes - literally - 2 lines of Python.\n\n\nimport streamlit as st\nimport streamlit.components.v1 as components\n\ncounter_component = components.declare_component(\n    \"counter\",\n    url=\"http://localhost:3000/\",\n)\n\ncount = counter_component(key=\"count\", default=0)\nst.markdown(f\"The value of the counter is **{count}**.\")\n\n\nWe declare a new component by passing the name and the location of the component front-end files (or the URL of your development server).\nWe provide the default value for the counter.\nWe make sure that the component does not re-render unnecessarily by providing the key.\nBrave New World\n\nTo establish a two-way connection between our app and the component, we are going to add ports to our Elm app.\n\nTo send a message from Elm to Streamlit, let's define a port that receives a number and produces a command.\n\n\nport fromElm : Int -> Cmd msg\n\n\n\nWe will need to send the new value back to Streamlit on Increment and Decrement events. So let's modify our update function to reflect that.\n\n\nIncrement ->\n    ( { model | count = model.count + 1 }\n    , fromElm (model.count + 1)\n    )\n\nDecrement ->\n    ( { model | count = model.count - 1 }\n    , fromElm (model.count - 1)\n    )\n\n\nThere and Back Again\n\nTo send a message from Streamlit to Elm, let's define a port that receives a number and produces a subscription.\n\n\nport fromJS : (Int -> msg) -> Sub msg\n\n\n\nFirstly, we will define a new message type.\n\n\ntype Msg\n    = Default Int\n    | Increment\n    | Decrement\n\n\n\nSecondly, we will add a handler for that message type to update.\n\n\nDefault value ->\n    ( { model | count = value }\n    , Cmd.none\n    )\n\n\n\nAnd finally, we will subscribe to the messages on that port.\n\n\nsubscriptions : Model -> Sub Msg\nsubscriptions _ =\n    fromJS Default\n\n\n\nWhen a message from JavaScript is sent to that port, the Default event will get a number and set the counter value to that number.\n\nAnd that's it!\n\nFoundation\n\nI hope this tutorial will help you build dazzling components in Elm. I believe there are a lot of incredible Elm packages that would boost the look and feel of Python data apps. To give you an idea, elm-visualization would make a fantastic Streamlit Component - and there are many, many more. I'm excited to see more people discover the awesomeness of Elm, and I look forward to seeing what you create!\n\nP.S. Both apps are available on GitHub and have been deployed using Streamlit sharing.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image-4.png (1520√ó938)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-4.png#border",
    "html": ""
  },
  {
    "title": "Qiusheng Wu uses Streamlit to build a popular geospatial application",
    "url": "https://blog.streamlit.io/qiusheng-wu-uses-streamlit-to-build-a-popular-geospatial-application/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nBy Qiusheng Wu and Ksenia Anske\nPosted in Case study, July 21 2022\nJupyter environment wasn‚Äôt enough\nGeemap got so popular, it maxed out free cloud hosting options\nDiscovering Streamlit\nStreamlit powers the new version of geemap ‚Äî and makes it available to anyone\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn April 2020, Assistant Professor in the Department of Geography at the University of Tennessee Qiusheng Wu launched geemap, an open-source Python package for interactive mapping with Google Earth Engine and open-source mapping libraries (e.g., ipyleaflet, folium). It quickly became one of the most popular geospatial packages with over 2,000 GitHub stars.\n\n‚ÄúBefore Google Earth Engine, generating a satellite timelapse like this could take hours or even days,‚Äù Qiusheng said. ‚ÄúYou can now create satellite timelapses in minutes, but it can still take hundreds of lines of Earth Engine JavaScript.‚Äù\n\nJupyter environment wasn‚Äôt enough\n\nQiusheng wanted to apply geospatial big data, machine learning, and cloud computing to study environmental change (surface water, wetland inundation dynamics, etc.).\n\nWith geemap, users could explore large geospatial datasets, perform planetary-scale analysis, and create satellite timelapses with a few lines of code or a few clicks. But they still needed to install Python and run geemap in a Jupyter environment.\n\nQiusheng wanted his app to let anyone create satellite timelapses with no code. So he used Voil√† for turning notebooks into standalone web apps and dashboards.\n\nGeemap got so popular, it maxed out free cloud hosting options\n\nTo make the notebooks public, Qiusheng needed to host them on a server.\n\n‚ÄúI‚Äôve been using ngrok to turn a local computer into a secure web server and connect it to the ngrok cloud service, which accepts traffic on a public address,‚Äù Qiusheng said. ‚ÄúIt‚Äôs one of the easiest ways to turn a Jupyter notebook into an app and it‚Äôs great for demos. But the downside of using a local computer as a public server is that it might be hacked.‚Äù\n\nHe switched to hosting it on a Heroku cloud server. Soon after, the app became so popular, and it used up the free monthly dyno hours.\n\n‚ÄúI had to shut down the app when it exceeded the free limit and restart it at the beginning of each month, which was inconvenient,‚Äù Qiusheng said.\n\nDiscovering Streamlit\n\nQiusheng first discovered Streamlit in October 2021. It quickly became his favorite package for developing and deploying interactive web apps. He used it for geemap because:\n\nIt was free.\nIt was open-source.\nIt had similar functionality to ipywidgets but was much easier to use (no need for a callback function).\nHe could deploy unlimited public apps from GitHub to Streamlit Community Cloud for free (no need for a server).\nDeployment was automatic, so he could focus on coding.\nThe apps were publicly accessible.\nStreamlit powers the new version of geemap ‚Äî and makes it available to anyone\n\nSoon after Qiusheng released the Streamlit Geospatial app for creating satellite timelapse animations for any location in less than 60 seconds, it got widely circulated on social media (check out the blog post, the video tutorial, and the repo code).\n\nPeople all over the world made animations of environmental changes: urban growth, land reclamation, river dynamics, vegetation dynamics, coastal erosion, and volcanic eruptions (use hashtags #streamlit on Twitter and #BigRiverAnimation on LinkedIn to see examples).\n\nThere are now more than ten apps in Streamlit Geospatial, including the app for visualizing U.S. real estate data in 3D.\n\nThere is even an app for creating maps of hurricane tracks built with Streamlit and Tropycal (check out the repo code here).\n\nTo make it easier for users to make their own geospatial apps, Qiusheng created an app template based on Streamlit‚Äôs native support for multipage apps. Users can fork the repository and add more apps if needed. The app can be deployed to Streamlit Cloud, Heroku, or MyBinder (here is the repo code).\n\nWrapping up\n\n‚ÄúI‚Äôm an advocate of open science and reproducible research,‚Äù Qiusheng said. ‚ÄúI love sharing Streamlit apps and making geospatial technologies more accessible to everyone. I have developed several open-source packages for geospatial analysis and interactive mapping (e.g., geemap, leafmap, geospatial, pygis, lidar). You can see my open-source projects on GitHub and video tutorials on my YouTube channel.‚Äù\n\nThank you for reading Qiusheng‚Äôs story! If you have any questions, please leave them below in the comments or reach out to Qiusheng on Twitter, LinkedIn, or YouTube.\n\nHappy coding and learning! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Firestore & Streamlit | Create a Serverless Web App in Python",
    "url": "https://blog.streamlit.io/streamlit-firestore/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit ‚ù§Ô∏è Firestore\n\nUse Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!\n\nBy Austin Chen\nPosted in Tutorials, January 27 2021\nTL;DR\nTable of Contents\nIntro: What is Firestore, and why should we use it?\nWhat are some downsides of Firestore?\nPart 1: A simple Streamlit Sharing app\nSetting up your Git repo\nRunning Streamlit on your own computer\nDeploying your app on Streamlit Sharing\nPart 2: Setting up Firestore\nSet up a Firebase account\nCreate a Firestore database\nCreate your first collection and document\nDownload your account keys üóùÔ∏è\nTo be continued...\nContents\nShare this post\n‚Üê All posts\nTL;DR\nStreamlit lets you build a real web app, in 20 lines of Python.\nStreamlit Sharing hosts that app for you. No server needed.\nFirestore lets you store and fetch data. No server needed, either!\nCombine them for a serverless web app with persistent data, written entirely in Python!\nTable of Contents\nIntro: What is Firestore, and why should we use it?\nPart 1: A simple Streamlit Sharing app\nPart 2: Setting up Firestore\nPart 3: Building the Reddit home page\nPart 4: Tips and tricks for app development\nIntro: What is Firestore, and why should we use it?\n\nSo what even is this Firestore thing? Well, from their website:\n\nCloud Firestore is a flexible, scalable database for mobile, web, and server development...\n\nLet's break that down! First, Firestore is a database - that means it's a good place to store data, and retrieve it later. If you're into ML/data science, and have been building dashboards with Streamlit, the data you work with probably looks like Pandas tables, Numpy arrays, CSV files. These tend to be fairly static. You'll rarely need to create new entries or update them, let alone make your data accessible from apps all over the world. But many web apps need to be dynamic, allowing lots of edits to the data and creation of new data ‚Äî and databases excel at this! Every consumer web app you can think of (Gmail, Youtube, Slack, Airbnb) have databases that manage the things that their users create (emails, videos, chat messages, and listings).\n\nUnlike other databases, Firestore is flexible. In technical terms, we call it a \"NoSQL\" database, which means that you don't have to design a fixed schema for how your data will be structured before you begin. This is great for quick prototyping! Oftentimes, when I'm making a web app, I don't exactly know what it will look like or do, much less how I should store my data.\n\nAs an analogy, let's say I wanted to bake a bunch of desserts. A SQL database is a bit like having to write out a pie recipe beforehand, and then always following it line-by-line. I can efficiently bake lots of pies, which will be tasty if my original recipe was good. But the NoSQL approach is more like: let me start with something vaguely pie-like, but maybe it'll become a tart, or drift towards more of a cupcake... I can experiment until I land on something that really hits the spot.\n\nFinally, Firestore is hosted on the cloud. This means that it's a service, very much like GitHub or our very own Streamlit Sharing, where you don't have to worry about maintaining your own servers. You don't need to worry about scaling, uptime, or a bunch of other issues that can get in the way of building out your idea.\n\nWhat are some downsides of Firestore?\n\nFirst, as a hosted service, Firestore can cost you money. They do have a generous free tier to start out with ‚Äî no credit card needed! And if your app exceeds the free limits, they'll inform you and stop new writes. Even if you hit the paid tier, Firestore is pretty cheap; I run a side project with hundreds of daily users and thousands of daily writes, and Firestore only costs me ~$1/month.\n\nSecond, other databases may perform better on complicated queries, or when you get to e.g. millions of daily users. For pure speed of setup, Firestore is great, but if you already know that scaling will be an issue, you may want to consider other options like SQLite, MongoDB, or Cloud Spanner.\n\nAnd finally, most Firestore users use JavaScript, so there are fewer Python code snippets and Stack Overflow answers to find. For this tutorial, we'll use Firestore's Python library, which is very pleasant to work with. Here's a snippet from their Python API for inspiration:\n\nfrom google.cloud import firestore\n\n# Add a new user to the database\ndb = firestore.Client()\ndoc_ref = db.collection('users').document('alovelace')\ndoc_ref.set({\n    'first': 'Ada',\n    'last': 'Lovelace',\n    'born': 1815\n})\n\n# Then query to list all users\nusers_ref = db.collection('users')\n\nfor doc in users_ref.stream():\n    print('{} => {}'.format(doc.id, doc.to_dict()))\n\n\nPretty easy to read, right?\n\nPart 1: A simple Streamlit Sharing app\n\nOkay! If you're sold on the awesomeness of Firestore, I'm now going to show you how to bake in that awesomeness into a Streamlit app. Let's cook up a simple version of the Reddit home page, from scratch! (Okay, that was the last cooking analogy, I promise.)\n\nOnce again, Streamlit is the fastest way to build web apps in Python. Streamlit Sharing is the fastest way to put your app online, so you can share it with the world. Here's what Sharing can do for you, in 1 minute:\n\nHere in Part 1, I'm going to walk you step-by-step through the process of building a Streamlit Sharing app, even if you've never done it before. Ready? Let's do this!\n\nIf you're already a Streamlit master, feel free to skip down to Part 2; just remember to pip install google-cloud-firestore , or the equivalent in your Python library manager of choice.\n\nSetting up your Git repo\n\nWe'll start by creating a new repo on GitHub ‚Äî the folder where all your code will go. Name it whatever you like! (I'm calling mine streamlit-reddit). Make sure it's public, though, so we can host it on Streamlit Sharing later.\n\nNext, you'll want to download (aka \"clone\") this repo on your own computer. (You'll need to have Git installed.) Open up a local shell, and run the following commands, replacing {username} with your own GitHub username:\n\n# Creates a copy of this repo on your computer\n$ git clone https://github.com/{username}/streamlit-reddit.git\n\n# Enters the repo, which is currently empty\n$ cd streamlit-reddit\n\nRunning Streamlit on your own computer\n\nWith our Git repo set up, let's add in the two main files every Streamlit Sharing app needs:\n\nrequirements.txt ‚Äî A list of the Python libraries we use, so Streamlit Sharing will know to install them.\n\nFor this tutorial, we'll only be using two libraries: Streamlit itself, and the Python client for Firestore. So this is all you need inside your requirements.txt file:\n\nstreamlit\ngoogle-cloud-firestore\n\n\nstreamlit_app.py ‚Äî Where our app code will go! Let's start with just a few lines:\n\nimport streamlit as st\n\nst.header('Hello üåé!')\nif st.button('Balloons?'):\n    st.balloons()\n\n\nThis app imports the Streamlit library, and then shows some text and a button. When you click on the button, a bunch of balloons will float up from the bottom of the screen!\n\nNow, we're going to use Pip to install the Streamlit and the Firestore libraries onto your own computer. (If you prefer pipenv or poetry or conda, any of those are fine too!)\n\n# Install all the libraries mentioned in requirements.txt\n$ pip install -r requirements.txt\n...\n\n\nWe're ready to run our starter Streamlit app!\n\n# Run this Streamlit app on your own computer\n$ streamlit run streamlit_app.py\nYou can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://192.168.1.3:8501\n\n\nIf you see the Streamlit app launched on your own browser, that means everything worked. (You may have to enter in the URL, like http://localhost:8501). Try clicking on that button!\n\nDeploying your app on Streamlit Sharing\n\nOkay, we're ready to put our app on Streamlit Sharing! First, we need to save our changes to Git (aka \"commit\"), and then upload (aka \"push\") our changes onto GitHub. Again, from a shell that's inside your repo:\n\n# Add and all changes in your files\n$ git commit -am \"Create requirements.txt and a simple Streamlit app\"\n\n# Push all commits back to GitHub\n$ git push\n\n\nNow if you go to the GitHub page for your repo, you should see the code that you just wrote:\n\nOnce your code is on GitHub, you're ready to deploy your app for the world to see! Head over to https://share.streamlit.io/ and create a new app. (If you don't have access to Sharing yet, post below in the comments and we'll sort you out.) Here's a great tutorial on deploying through sharing if you'd like to read more.\n\nFill in the name of your repo ‚Äî the others should populate automagically ‚Äî then click \"Deploy!\" üéà\n\nThat URL you're on is one that anyone on Earth can go to and play with. Isn't that cool? You've gone from \"literally nothing\" to \"functioning, globally-hosted Streamlit app\" in the space of 15 minutes! But hold on to your horses, we're about to get to the really cool stuff: saving data online, with Firestore.\n\nPart 2: Setting up Firestore\n\nOnce again, Firestore is the database we're using to create, edit, and read our data. For our Reddit home page, we're going to use it to store things like the links, titles, and later upvotes. Here in Part 2, I'll walk you through how to set that up.\n\nSet up a Firebase account\n\nTo clear up some potential confusion: Firestore is the database we're using, one of many services provided by Firebase (which is a part of Google). You'll want to sign in to your Google account at https://console.firebase.google.com/. Then, create a Firebase project:\n\nThese are the steps to create an account:\n\nName your project\nEnable analytics [You can skip this since we're not using it today, but I think Analytics are really cool üôÇ]\nAccept terms and click create!\n\nOnce created, you'll see Firebase loading and will be notified when the project is ready [this is pretty quick].\n\nAwesome, our Firebase project is set up! Firebase has a lot of different tools for building web apps, but the only one we're using today is Firestore. So let's turn that on by creating a Cloud Firestore database.\n\nCreate a Firestore database\n\nStart it in test mode, which means anyone can read or write to it for the next 30 days (you can change this later!)\n\nAnd pick a physical location for where the data is hosted. (Don't worry about the scary red text. This isn't super important, but closer to you generally means the data will load a little bit faster.)\n\nCreate your first collection and document\n\nNow that the Firestore database is created, we can start putting data into it. But first, some terminology:\n\nA document is a grouping of key-value pairs; think \"Python dict, but the keys are always strings\". It's where you will be storing all your data! If you were building a chat app, you might represent one user in her own document. Or one message might be its own document.\nJust like dicts can contain other dicts, documents can contain other documents!\nEach document has an ID, a string that uniquely identifies it.\nA collection is a set of documents. Your chat app might have a collection of users, and a separate collection of messages, with the understanding that the documents in each will look different.\nBut note that as a flexible, NoSQL database, Firestore will not enforce any rules about which documents go in which collections! It's all up to you.\nCollections have IDs as well, to distinguish them from other collections.\n\nOur Reddit clone will include a bunch of posts. So let's make a collection for them!\n\nAnd then we'll want to create an example post document. Let's make a post that links to the Google website. Pick an ID, then add in a title and a url field for the post:\n\nNow you'll have your first piece of data visible from your web console! The console is a place where you can manually edit your database. It's already a cool way to peek under the hood, and you can make changes directly through the web editor.\n\nBut sit tight, and I'll show you how to create new data programmatically ‚Äî that is, in your Python app!\n\nDownload your account keys üóùÔ∏è\n\nBefore we get there, one last thing: we need to get a service account key so that our Streamlit app will be able to read and write data to this database. You can think of this key as a password that your Python code uses to log in to Firestore ‚Äî except instead of a text password like \"hunter2\", it's a JSON file instead.\n\nTo download this JSON file, first go to \"Project settings\":\n\nThen go to \"Service accounts\", select \"Python\", and click \"Generate new private key\"\n\nYou should have downloaded a file that looks like streamlit-reddit-firebase-adminsdk-4enia-e106b71674.json. Move it into your Git repo, and let's rename it to something simpler like firestore-key.json.\n\nThat's it, we're done setting up Firestore!\n\nTo be continued...\n\nWe now have everything set up to start coding in earnest. Check out Parts 3 & 4, where we provide step-by-step instructions to build the entire Reddit app, as well as tips and tricks for working with Firestore + Streamlit. For now, here's a sneak peak of an example streamlit_app.py to get you started ‚Äî see how far you can get by iterating on this!\n\nNormally you should NEVER store a key in a public GitHub. We're doing it right now because it's a test. And in the next section I'll show you how to secure that in Streamlit Sharing using Secrets!\nimport streamlit as st\nfrom google.cloud import firestore\n\n# Authenticate to Firestore with the JSON account key.\ndb = firestore.Client.from_service_account_json(\"firestore-key.json\")\n\n# Create a reference to the Google post.\ndoc_ref = db.collection(\"posts\").document(\"Google\")\n\n# Then get the data at that reference.\ndoc = doc_ref.get()\n\n# Let's see what we got!\nst.write(\"The id is: \", doc.id)\nst.write(\"The contents are: \", doc.to_dict())\n\n\nQuestions? Something didn't work right? Just want to show off your cool Firestore app? Let us know below in the comments section!\n\nAnd of course, thanks to Henrikh and Fanilo for providing feedback on this article~\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image-1.png (2000√ó1062)",
    "url": "https://blog.streamlit.io/content/images/2021/08/image-1.png#browser",
    "html": ""
  },
  {
    "title": "streamlit-1.0.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/09/streamlit-1.0.gif#browser",
    "html": ""
  },
  {
    "title": "function-page-1.png (2000√ó1140)",
    "url": "https://blog.streamlit.io/content/images/2021/10/function-page-1.png#browser",
    "html": ""
  },
  {
    "title": "What is Nightly? | Try Nightly Build for Cutting-Edge Streamlit",
    "url": "https://blog.streamlit.io/https-discuss-streamlit-io-t-try-nightly-build-for-cutting-edge-streamlit-2534/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nTry Nightly Build for cutting-edge Streamlit\n\nA new style of release for anyone who wants the most up-to-date Streamlit version\n\nBy TC Ricks\nPosted in Product, April 17 2020\nContents\nShare this post\n‚Üê All posts\n\nHey Community üëã,\n\nWe‚Äôve recently rolled out a new type of release, our Nightly Build, and we wanted to give everyone a bit of information about it.\n\nWhat is nightly?\n\nNightly is a new style of release we created for anyone who wants the most up-to-date Streamlit version. At the end of each day, all PRs that were approved will be added to a new nightly build, including all of our newest features! While we can‚Äôt guarantee there won‚Äôt be bugs, all nightly versions go through the same automated portion of testing as our general releases. If you‚Äôd like to use, simply do pip install streamlit-nightly or if there is a specific version of nightly you want to use, you can find it here. All changes can be found using the comparison tool on our GitHub repo.\n\nWhy are we doing this?\n\nWe want to make sure you have access to the latest features and fixes as soon as they are available. If there was an issue preventing you from using Streamlit or you need a feature that will allow you to do more, this will give quicker access. It will also let you try things out sooner and give us valuable feedback!\n\nLet us know if you have any feedback or questions ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "legacy-vs-arrow-2-1.png (2000√ó2017)",
    "url": "https://blog.streamlit.io/content/images/2021/07/legacy-vs-arrow-2-1.png#shadow",
    "html": ""
  },
  {
    "title": "arrow.gif (687√ó388)",
    "url": "https://blog.streamlit.io/content/images/2021/07/arrow.gif",
    "html": ""
  },
  {
    "title": "arrow-vs-legacy-chart-1.png (1248√ó794)",
    "url": "https://blog.streamlit.io/content/images/2021/07/arrow-vs-legacy-chart-1.png#shadow",
    "html": ""
  },
  {
    "title": "Monthly rewind > January 2021",
    "url": "https://blog.streamlit.io/monthly_rewind_january_2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > January 2021\n\nYour January look back at new features and great community content\n\nBy TC Ricks\nPosted in Monthly Rewind, February 8 2021\nüèÜ App of the month üèÜ\nStreamlit January updates\nüîç Current releases: 0.76.0\nüïπ Closed betas\nüß© New components\nüéà New Streamlit creators\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur January featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nThe Traingenerator app by Johannes Rieke which generates custom template code for PyTorch & sklearn, all within a simple Streamlit UI. [See the gif above!]\n\nTo learn more about the app take a look at the repo here - we recommend perusing some of Johannes' other repos as well [including his Best of Streamlit tracker] - he has some pretty cool projects üòé\n\nStreamlit January updates\n\nThe latest and greatest that you might have missed in January.\n\nüîç Current releases: 0.76.0\n\nThe newest Streamlit release is 0.76.0, but make sure to check out the change log if you haven't updated in a while to see what's been going on.\n\nüïπ Closed betas\n\nWe currently have a number of coming soon features that are being tested in closed beta, including:\n\nSecrets management (for Streamlit sharing)\nProgrammable state\nCustomizable theming\n\nIf you're interested in testing any of these send us a message on the forum and we'll add you to the beta when it becomes available.\n\nüß© New components\n\nThe community continues to actively develop a number of new components that you can use to extend your apps. Here are two that made some headway in January:\n\nStreamlit Ag-Grid by Pablo Fonseca - Streamlit Ag-Grid makes it possible to use interactive dataframes within your Streamlit apps. [Demo App]\n\nStreamlit WebRTC by Yuichiro Tsuchiya - Use Streamlit WebRTC to send and receive video within your Streamlit apps. [Demo App]\n\nüéà New Streamlit creators\n\nWe welcomed our three newest Streamlit Creators in January: Johannes Rieke, Tyler Richards, and Christian Klose - read more about them here.\n\nFeatured Streamlit content\n\nPodcasts and articles written by the Streamlit team for your listening and reading pleasure.\n\nListen to Streamlit's COO, Amanda Kelly, as she talks about Streamlit and company building with Darius Gant on his podcast Building Data Applications for AI\nStreamlit engineer, Tim Conkling tells the story of allow-same-origin: Streamlit Components, security, and a five month quest to ship a single line of code\nWe're having a bit of a love affair with Firestore. In this piece Streamlit engineer, Austin Chen walks through Part 1 of how to use Streamlit and Firestore together: Streamlit ‚ù§Ô∏è Firestore\nFeatured community content\n\nSome great apps, repos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nRead Jared Stock's article about how Arup and New Story are using Streamlit to help combat pandemic related evictions.\nCheck-out the popular Github stats app by Johannes to see your GitHub activity from 2020\nChristian wrote a fun tutorial on how to sketch number prediction using a Streamlit app\nThe Data Professor did a guest video on freeCodeCamp where he builds 12 data science apps with Python and Streamlit - and check out his growing archive of Streamlit tutorials: How to Make a Multi-Page Web App | Streamlit #16\nFor all the cinema fans out there, Steven created \"The Pitch Doctor \" to help you create a title and movie pitch for you next horror film üßü\nFanilo made some amazing updates to his Streamlit ECharts component\nA great article by Tim on how to create and deploy a neural style transfer app\nA notable app by Alireza helping to show where individuals can donate items in Switzerland: Where2Give\nJustin wrote a helpful piece on how to optimize a Pandas dataframe with a Streamlit app\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image-1.png (1771√ó1146)",
    "url": "https://blog.streamlit.io/content/images/2021/10/image-1.png#browser",
    "html": ""
  },
  {
    "title": "Adding Beta and Experimental ‚ÄúChannels‚Äù to Streamlit",
    "url": "https://blog.streamlit.io/https-discuss-streamlit-io-t-adding-beta-and-experimental-channels-to-streamlit-2765/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAdding beta and experimental ‚Äúchannels‚Äù to Streamlit\n\nIntroducing the st.beta and st.experimental namespaces\n\nBy TC Ricks\nPosted in Product, May 6 2020\nBeta\nExperimental\nContents\nShare this post\n‚Üê All posts\n\nHey community üëã,\n\nAt Streamlit, we like to move fast while keeping things stable. And in our latest effort to move even faster while keeping the promise of stability, we‚Äôre introducing the st.beta and st.experimental namespaces. These are basically prefixes we attach to our function names to make sure their status is clear to everyone.\n\nHere‚Äôs a quick rundown of what you get from each namespace:\n\nst: this is where our core features like st.write and st.dataframe live. If we ever make backward-incompatible changes to these, they will take place gradually and with months of announcements and warnings.\nst.beta: this is where all new features land before they find their way to st. This gives you a chance to try the next big thing we‚Äôre cooking up weeks or months before we‚Äôre ready to stabilize its API.\nst.experimental: this is where we‚Äôll put features that may or may not ever make it into st. We don‚Äôt know whether these features have a future, but we want you to have access to everything we‚Äôre trying, and work with us to figure them out.\n\nThe main difference between st.beta and st.experimental is that beta features are expected to make it into the st namespace at some point soon, while experimental features may never make it.\n\nMore details below.\n\nBeta\n\nFeatures in the beta namespace are all scheduled to become part of st, or core Streamlit. While in beta, a feature‚Äôs API and behaviors may not be stable, and it‚Äôs possible they could change in ways that aren‚Äôt backward-compatible.\n\nThe lifecycle of a beta feature\n\nA feature is added to the beta namespace.\nThe feature‚Äôs API stabilizes and the feature is cloned into the st namespace, so it exists in both st and st.beta. At this point, users will see a warning when using the version of the feature that lives in the beta namespace ‚Äì but the st.beta feature will still work.\nAt some point, the feature is removed from the st.beta namespace, but there will still be a stub in st.beta that shows an error with appropriate instructions.\nFinally, at a later date the stub in st.beta is removed.\n\nKeeping up-to-date with beta features\n\nAll Beta features will be announced in the changelogs.\nAll Beta features will show up in our documentation alongside normal features. For example, st.beta_color_picker() will be documented on the same page as st.slider().\nExperimental\n\nFeatures in the experimental namespace are things that we‚Äôre still working on or trying to understand. If these features are successful, at some point they‚Äôll become part of core Streamlit, by moving to the st.beta namespace and then to st. If unsuccessful, these features are removed without much notice.\n\nNote: Experimental features and their APIs may change or be removed at any time.\n\nThe lifecycle of an experimental feature\n\nA feature is added to the experimental namespace.\nThe feature is potentially tweaked over time, with possible API/behavior breakages.\nAt some point, we either move the feature into st.beta or remove it from st.experimental. Either way, we leave a stub in st.experimental that shows an error with instructions.\n\nKeeping up-to-date with experimental features\n\nAll Experimental features will be announced in the changelogs.\nAll Experimental features will show up in a separate section of the API page in the docs, called ‚Äúexperimental features‚Äù (not created yet!)\n\nLet us know if you have any questions or feedback about the new namespaces!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Cloud.png (2000√ó1140)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Cloud.png#browser",
    "html": ""
  },
  {
    "title": "Get-Started.png (2000√ó1142)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Get-Started.png#browser",
    "html": ""
  },
  {
    "title": "Session_State_GIF_2-1-edited.gif (1200√ó603)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Session_State_GIF_2-1-edited.gif#border",
    "html": ""
  },
  {
    "title": "knowledge-base-1.png (2000√ó1140)",
    "url": "https://blog.streamlit.io/content/images/2021/10/knowledge-base-1.png#browser",
    "html": ""
  },
  {
    "title": "The Stable solves its data scalability problem with Streamlit",
    "url": "https://blog.streamlit.io/the-stable-solves-its-data-scalability-problem-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe Stable solves its data scalability problem with Streamlit\n\nHow Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days\n\nBy Mark von Oven and Ksenia Anske\nPosted in Case study, April 28 2022\nNotebooks weren‚Äôt enough\nStumbling on Streamlit\nStarting with Streamlit\nStreamlit as a prototyping tool\nSpeaking data science language\nSolving problems one piece at a time\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nThe Stable is a commerce agency that helps brands execute across all channels of commerce including Retail, Amazon, Creative, and Digital Marketing. The company develops strategies, sells products, builds brands, executes campaigns, develops tech and merchandise, and analyzes and dataizes to drive sales and efficiency. Data science is a big part of the value it delivers to its clients.\n\nMark von Oven joined The Stable in 2019 as VP of Analytics and has built a solid Analytics team.\n\n\"We quickly found our small (but mighty!) team in high demand,\" Mark said. \"We were solving huge problems for our clients that used to take years to resolve. We even applied for our first patent!\"\n\nNotebooks weren‚Äôt enough\n\nThe team did great work, but it was invisible, mostly stored in Jupyter notebooks and random Python files. There was no scaling or democratizing data. All projects started with the same process or algorithm and took 1-3 weeks to complete.\n\nThe team wanted the clients to interact with the algorithms, but they had no time or budget to build interactive apps. Flask and other tools were too slow and cumbersome.\n\n\"We had to choose which clients would see all this cool stuff,\" Mark said. \"We‚Äôre not full-stack developers. We‚Äôre data scientists and analysts. We needed to surround our algorithms with user-friendly input choices and make them available 24/7. No hand-holding required.\"\n\nStumbling on Streamlit\n\nAnd it was then that team member Pedro Pereira stumbled on Streamlit while solving a different problem during a monthly \"hack day\" session.\n\n\"Every month we have a day when we try to tackle one single project,\" Pedro said. \"We needed a fast way to spin up a data analysis web app.\"\n\nTeam member Nick Walton saw a different opportunity.\n\n\"I thought that translating our Jupyter notebooks into small Streamlit apps might let us automate some repetitive tasks,\" Nick said.\n\nThe team tested it. Within a week their first app was born‚Äîa test-and-control store selector for retail chains.\n\n\"It was clean and quick,\" Mark said. \"It surprised me. I thought we were creating an app to streamline the internal process. But it could easily be used by anyone in the company.\"\n\nIt took the team one day to review the code and read the docs to realize that this was exactly how they were going to solve their scalability problem and make their work visible.\n\nStarting with Streamlit\n\nMark‚Äôs team loved Streamlit. There were no training sessions. No offsite boot camps at $2,000 per seat. No waiting for DevOps to build dashboards.\n\nStreamlit was a seamless extension of what the team already knew how to do in Python.\n\n\"I wasn‚Äôt even firing up Jupyter notebooks anymore,\" Mark said. \"I‚Äôd open Spyder and start coding a new app to save time. In only three months, Streamlit has hosted our capabilities, enabled us to transform our data into actionable insights, and scaled to the whole company!\"\n\nNow they could have an idea in the morning, get it in front of clients in the afternoon, and drive value before the end of the day. They could launch an app on Tuesday and help 30 clients by Friday!\n\nStreamlit as a prototyping tool\n\nThe team ramped up so quickly and easily that Mark started seeing Streamlit as their primary prototyping tool‚Äîand now potentially sees a path to the company‚Äôs production environment. Plus, it was really easy to connect their Streamlit app to Snowflake data.\n\n\"Given the success and scalability so far, we have no reason to change course,\" Mark explained. \"We want to delight our clients. We‚Äôre building these apps with passion and determination. Streamlit allows us to focus on that because it makes everything else easy.\"\n\nSpeaking data science language\n\nData scientists do amazing things and work with the smartest people on Earth. Yet many don‚Äôt know how to let people discover that work. They think they need a complete solution before sharing their work.\n\nMark thinks there is huge potential in partial solutions.\n\n\"I‚Äôm a bit atypical for a data scientist,\" Mark said. \"Yes, I wrote my first line of BASIC code on a TRS-80 when I was 6, and I studied how to build both software and hardware. But I‚Äôm also the lead singer of the Grateful Red, a Minneapolis funk cover band. I love the spotlight. If I or my team build something cool, I‚Äôm compelled to ensure it gets presented. Too many data scientists would rather call in sick than talk about their work with business leaders.\"\n\nAs a performer, Mark is often nervous before a show. All it takes for him is to sing the first few notes, and his nerves are gone.\n\n\"Getting on that stage is half the battle,\" Mark said. \"Don‚Äôt feel stumped because of the language barrier, or because you don‚Äôt have enough business context or think you‚Äôll fail. You‚Äôre sitting on powerful stuff. Expose it. Explore it early to break down the barriers before they exist. Streamlit lets you speak your language and bring the fruits of your labor to those who don‚Äôt speak data science.\"\n\nSolving problems one piece at a time\n\nNobody wants to invest in a piece of software until they have the problem figured out. But the best way to solve a problem is by solving it one piece at a time.\n\n\"Streamlit naturally fits into this process,\" Mark explained. \"It uses a language you already know and helps you get a fantastic result. Who knows, maybe it‚Äôll be your new production environment!\"\n\nWrapping up\n\nThank you for reading Mark‚Äôs story! If you have any questions, please connect with Mark on LinkedIn or drop him a line at mvo@thestable.com.\n\nAnd if you want to get started with Streamlit in your organization, head over to streamlit.io to learn more.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Quick-Image-2--2-.png (2000√ó1695)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Quick-Image-2--2-.png#border",
    "html": ""
  },
  {
    "title": "Component-2.png (4924√ó5980)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Component-2.png",
    "html": ""
  },
  {
    "title": "Session_State_GIF_1-edited.gif (1200√ó627)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Session_State_GIF_1-edited.gif#browser",
    "html": ""
  },
  {
    "title": "series.b.gif (1024√ó576)",
    "url": "https://blog.streamlit.io/content/images/2022/09/series.b.gif#browser",
    "html": ""
  },
  {
    "title": "Component-1--3-.png (2000√ó1411)",
    "url": "https://blog.streamlit.io/content/images/2021/10/Component-1--3-.png",
    "html": ""
  },
  {
    "title": "Monthly Rewind - Streamlit",
    "url": "https://blog.streamlit.io/tag/monthly-rewind/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Monthly Rewind\n27 posts\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nMonthly rewind > November 2022\n\nYour November look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nDecember 7 2022\nMonthly rewind > October 2022\n\nYour October look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nNovember 8 2022\nMonthly rewind > September 2022\n\nYour September look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nOctober 7 2022\nMonthly rewind > August 2022\n\nYour August look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nSeptember 7 2022\nMonthly rewind > July 2022\n\nYour July look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nAugust 9 2022\nMonthly rewind > June 2022\n\nYour June look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJuly 6 2022\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "What‚Äôs new in Streamlit (January 13th, 2022)",
    "url": "https://blog.streamlit.io/whats-new-in-streamlit-january-2022/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nBy Ksenia Anske\nPosted in Release Notes, January 13 2022\n‚ú® New in Streamlit\n‚õëÔ∏è Release 1.4.0\nüì∏ Introducing st.camera_input for uploading camera images\nü™Å Clear memo + singleton caches procedurally\nüî¶ Other notable 1.4.0 release updates\n‚òÅÔ∏è Streamlit Cloud updates\nüì© Sign in with email\nüì≤ Share apps with any email address\nüî¶ Other notable Streamlit Cloud updates\nüåû Wrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit community! üëã\n\nWe‚Äôre excited to share what‚Äôs new in the 1.4.0 release and some recent updates to Streamlit Cloud, including notable updates like introducing st.camera_input, clearing individual memo and singleton functions in code, signing in to Streamlit Cloud with your email, and app email invites.\n\nKeep reading to learn more.\n\n‚ú® New in Streamlit\n‚õëÔ∏è Release 1.4.0\nüì∏ Introducing st.camera_input for uploading camera images\n\nWe‚Äôre introducing a new widget to use webcams! It‚Äôs great for computer vision apps. For example, imagine an app for face detection or style transfer.\n\nHere is how you can use this widget:\n\nimg_file = st.camera_input(\"Webcam image\")\n\n\nThe returned img_file is a file-like object, similar to the return value of st.file_uploader. For a simple app, you can display the image captured by the webcam by using st.image:\n\nimg_file = st.camera_input(\"Webcam image\")\nif img_file is not None:\n    st.image(img_file)\n\n\nOf course, if you want to, you can do any processing steps on the image in between.\n\nHere‚Äôs what the new widget looks like in the app:\n\nTry it out in our demo app and take a look at the documentation.\n\nü™Å Clear memo + singleton caches procedurally\n\nDo you need more control over cache invalidation? Now you can finally clear caches of functions decorated with @st.experimental_memo and @st.experimental_singleton in code. (The Streamlit community has been asking for this forever‚Äîand so have we, internally.)\n\nWe have implemented programmatic clearing of memo + singleton functions. For example, you can do the following:\n\n@st.experimental_memo\ndef foo(x):\n    return x**2\n\nif st.button(\"Clear Foo\"):\n    # Clear foo's memoized values:\n    foo.clear()\n\nif st.button(\"Clear All\"):\n\t  # Clear values from *all* memoized functions:\n\t\tst.experimental_memo.clear()\n\n\nAPI Details\n\nAny function annotated with @st.experimental_memo or @st.experimental_singleton gets its own clear() function automatically.\nAdditionally, you can use st.experimental_memo.clear() and st.experimental_singleton.clear() to clear all memo and singleton caches, respectively.\n\nNote that because memo/singleton themselves are experimental APIs, these cache-clearing functions are experimental as well. See the docs for memo and singleton.\n\nüî¶ Other notable 1.4.0 release updates\n\nOne other notable update in this release:\n\nüö¶ Widgets now have the disabled parameter that removes interactivity (#4154).\n\nClick here to check out all updates.\n\n‚òÅÔ∏è Streamlit Cloud updates\nüì© Sign in with email\n\nYou can now sign in to Streamlit Cloud by using an email!\n\nAfter entering your email on the sign-in page, you‚Äôll get an email with a sign-in link:\n\nClicking on the link will log you into your Streamlit Cloud console. From there you can view all of your apps. See the docs for more information.\n\nüì≤ Share apps with any email address\n\nApps deployed with Streamlit Cloud come with built-in authentication. Until now, sharing has been limited to Google email addresses (Gmail) or accounts that have SSO setup.\n\nStarting today, you can share your app with any viewer whether they‚Äôre part of your company or another company. Simply add the viewers‚Äô email addresses, and you‚Äôre done!\n\nüî¶ Other notable Streamlit Cloud updates\nü§Ø The error page when your app goes over resource limits now shows more helpful info for debugging.\nüêô Your app now prints a message to its Cloud logs whenever it updates due to a Github commit.\n‚≠ê Streamlit Cloud now supports a \"favorite\" feature that lets you quickly access your apps from the app dashboard (favorited apps appear at the top).\nüöÄ When you invite someone to view your app in Streamlit Cloud, the recipient will receive an invitation to view the app as an email in their inbox!\nüîí We're committed to meeting industry standards and are now SOC 2 Type 1 certified. Read more about securely sharing your apps using Streamlit Cloud.\n\nIf you're new to Streamlit, now is the time to try Streamlit Cloud!\n\nüåû Wrapping up\n\nThanks for checking out what‚Äôs new with Streamlit. You can always see the most recent updates to our core library on our changelog or via this tag on the forum and to Streamlit Cloud via the Cloud Release Notes.\n\nGot questions? Let us know in the comments below. We're looking forward to hearing what you think!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Release Notes...\n\nView even more ‚Üí\n\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Wissam Siblini uses Streamlit for pathology detection in chest radiographs",
    "url": "https://blog.streamlit.io/wissam-siblini-uses-streamlit-for-pathology-detection-in-chest-radiographs/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nWissam Siblini uses Streamlit for pathology detection in chest radiographs\n\nLearn how Wissam detected thoracic pathologies in medical images\n\nBy Wissam Siblini and Ksenia Anske\nPosted in Case study, May 3 2022\nThe need to develop a demo app\nDiscovering Streamlit\nStreamlit seamlessly dressed up our machine learning results as an interactive chest radiographs classification app\n6 reasons why it was obvious to start with Streamlit\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nWissam Siblini always wanted to be a computer scientist. He started coding in middle school, studied engineering in college, and did his PhD in his favorite field, at the intersection of maths and computer science.\n\nWhen Wissam started as a machine learning researcher, he set out to implement innovative features in fraud detection, natural language processing, and health. But he couldn‚Äôt manage the front-end development of an application. He spent most of his time processing data, developing algorithms, and training and evaluating them.\n\n\"Machine learning is one of the most exciting fields for the decades to come,\" Wissam said. \"We're seeing the development of groundbreaking image recognition tools, agents that understand our language, and more generative tools for voice, image, sound, and art of all kinds. But I like to question basic facts. While working on fraud detection, I was interested in the very definition of simple evaluation measures like precision and recall. I even proposed variants that are now mentioned on the F-score Wikipedia page!\"\n\nThe need to develop a demo app\n\nWith the Covid-19 crisis, chest diseases became a major concern around the world. Deep learning (a subset of machine learning) showed very convincing performances in understanding texts, images, and genomics. It found applications in the medical field, particularly in personalized, predictive, and preventive medicine.\n\nWissam decided to target a specific \"AI for Health\" problem‚Äîthe detection of thoracic pathologies in medical images.\n\nTo do this, he and Mazine, a colleague who worked on this exploratory project with him, needed to look at the data, interact with it, and discuss it with others. They also had to clean it, select it, transform it, and build it into a model.\n\n\"I used to carry out this type of analysis in Jupyter notebooks but it wasn‚Äôt interactive,\" Wissam said. \"I wanted to show my results to other project members, especially to non-data scientists.\"\n\nWhen his team finished the proof of concept and had a prototype model, they needed a demo app. They wanted the users to upload a radiograph, get predictions (probabilities of pathologies and alerts), and see a visual interpretation of the model‚Äôs decision. They also wanted the users to browse the training data and its characteristics, monitor the performance of the model on a validation set, play with model parameters, and see the impact on performance.\n\nThis required front-end development, and several back-end functionalities such as an API for model serving. It was very time-consuming and required the knowledge of many different frameworks. It was hard to do in the last stages of a POC when the development had already consumed a lot of time.\n\nDiscovering Streamlit\n\nIt was then that Wissam discovered Streamlit.\n\n‚ÄúWe had only a few weeks to complete this pathology project,‚Äù Wissam said. ‚ÄúI still wanted a demo to show that it was promising and integrable. I was comfortable with Python, but not with frameworks like Angular, Vue, and React. So I googled ‚Äúfrontend development in Python‚Äù and ran into Streamlit (the early 0.60.0 version). I was amazed by the app examples on the website‚Äîespecially the conciseness of their code and deployment procedure.‚Äù\n\nAfter browsing the site for a few minutes, Wissam decided to use Streamlit not only for this project but for many others.\n\n‚ÄúI looked for solutions on the Streamlit forum and saw Fanilo Andrianasolo, a very talented colleague of mine,‚Äù Wissam said. ‚ÄúHe was an active member of the Streamlit community. We discussed it, and I made my choice.‚Äù\n\nStreamlit seamlessly dressed up our machine learning results as an interactive chest radiographs classification app\n\nStreamlit allowed the team to build exactly what they wanted.\n\n‚ÄúThe most important part of my job is to allow stakeholders to understand my work even if they‚Äôre not data scientists,‚Äù Wissam said. ‚ÄúStreamlit made my life better and easier for that. It took us almost five months to build a very competitive model. With Streamlit, we‚Äôve then built a very cool app that won us an internal innovation prize and convinced the management to keep exploring the theme. And it took only two weeks!‚Äù\n\nThe app featured a classifier page where the user could upload imagery, obtain a probability for each pathology (displayed in red if above a critical threshold), and get the interpretability heat map associated with the pathology of its choice. The imagery could be in a standard image format (PNG or JPEG) or in the more classical DICOM format.\n\nThe app had a Dashboard Data (to visualize the characteristics of the training data) and a Dashboard Model to analyze the model's performance and tune hyperparameters. For example, the user could set the critical threshold for each anomaly and see the impact on True Positive Rate, Precision, or Accuracy.\n\n6 reasons why it was obvious to start with Streamlit\n\nFor Wissam these were the reasons why it was so obvious to him to get started with Streamlit:\n\nMost of today‚Äôs data scientists already use Python. They can easily convert their projects into Streamlit apps.\nThe visuals (the frontend part) and the engine (the backend part/models) are all implemented in the same code.\nMany built-in features (sliders, buttons, tables, etc.) allow you to build a wide range of utilities in just a few lines of code. And both the Streamlit team and the community enrich these features every day.\nThere are many example apps for your inspiration on the Streamlit website and on GitHub.\nThe Streamlit forum is rich and active, so if you‚Äôre stuck, you can get help easily.\nThe apps look very good. They‚Äôre more than enough for a great demo!\n\n‚ÄúIn addition to demos, Streamlit can help with the exploratory phase of a project,‚Äù Wissam added. ¬†‚ÄúIn this phase, the goal is to see how some variables can impact others, display charts, play with parameters, observe behavior, and test transformation functions. Building a Streamlit exploration app then lets you focus on the analysis itself (not the code) and discuss it with subject-matter experts who are not data scientists.‚Äù\n\nWrapping up\n\n‚ÄúI‚Äôm so used to developing things with Streamlit now, it takes me hardly any time to make apps,‚Äù Wissam said. ‚ÄúI once coded an app in the morning, and in the afternoon it was ready for the client.‚Äù\n\nThank you for reading Wissam‚Äôs story! If you have any questions, please leave them in the comments below or contact Wissam on LinkedIn or via email. Happy coding!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "ScienceIO manages billions of rows of training data with Streamlit",
    "url": "https://blog.streamlit.io/scienceio-manages-billions-of-rows-of-training-data-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nBy Gaurav Kaushik and Raunaq Malhotra\nPosted in Case study, January 5 2023\nHow to query billions of rows\nHow to set up a search engine\nHow to build a fully working app example\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Gaurav Kaushik, and I‚Äôm the Co-Founder of ScienceIO.\n\nSince 2019 we‚Äôve been working on a model that could accurately mine medical records for critical patient information.\n\nIn the past, finding key medical conditions, procedures, and therapies required a fleet of models. We wanted to replace it with an elegant system that could be easily trained, deployed, and fine-tuned. But we needed hundreds of millions of training data labels!\n\nIt took us a year to create a training dataset for large healthcare language models with over 2.3 billion labels‚Äîthe most expansive and comprehensive in existence. Yes! Then a new issue came up. üò© Could these models understand billions of rows of data, perform quality control, and plan data improvements? We got to work and developed a Streamlit + Snowflake search engine to see what was in our data, what wasn‚Äôt, and how we could improve it.\n\nIn this post, I‚Äôll share with you why we turned to Streamlit + Snowflake and how you can build your own working example to search for papers from a database of COVID-19 research papers. You‚Äôll learn:\n\nHow to query billions of rows of data\nHow to set up a search engine\nHow to build a fully working app example\n\nLet‚Äôs get started.\n\nüëâ\nTL;DR? Check out the app and the repo.\nHow to query billions of rows\n\nThe usual business intelligence (BI) tools wouldn‚Äôt allow us to ask a question, test a hypothesis, or create a new look at the data (and repeat this process). That‚Äôs why we built a Streamlit + Snowflake search engine. We went from question to answer much faster!\n\nStreamlit really came to life after being paired with a global data platform like Snowflake. We could store and query billions of rows of data in Snowflake. But it‚Äôs the Streamlit apps that let us investigate it‚Äîwrite queries, build dashboards, and ask and answer questions fast.\n\nTo better understand 2.3 billion labels for training large language models, we built a multi-page Streamlit app. Each page has a different data view or a way to search it:\n\nConcept search: Search for labels that relate to a particular medical concept (specific medical condition or drug).\nText search: Search for specific words or phrases that are tagged as a label.\nCounts: Provide a dashboard with summary statistics of the whole dataset.\n\nBy combining Streamlit and Snowflake into a single application, we could add new views, data analyses, and new search capabilities to be served in minutes:\n\nStreamlit‚Äôs text bar is connected to a query that runs on a data warehouse like Snowflake. The status bar feature lets the user know their search is running (a query on billions of rows can take up to a minute).\n\nHow to set up a search engine\n\nStreamlit components make it easy to put together a text-based search engine:\n\nimport streamlit as st\n\n# app title\nst.title(\"Text Search üìù\")\n\n# search box\nwith st.form(key='Search'):\n  text_query = st.text_input(label='Enter text to search')\n  submit_button = st.form_submit_button(label='Search')\n\n\nIn five lines of code, we imported the Streamlit package, gave our app a title, and set up a search box. Next, we needed our app to do something when we clicked the button. So we set up a spinner for the user to see that a search was happening while we retrieved the results:\n\nif submit_button:\n\twith st.spinner(\"Searching (this could take a minute...) :hourglass:\"):\n\t\t# cool search stuff happens here\n\n\nOnce the spinner was done, we used success to show that the search was complete:\n\nst.success(f\"Search is complete :rocket:\")\n\n\nNot everyone on our team knows Python or how to query and visualize data, but they rely on this information to help customers or build new tools. The Streamlit search engine gives them a simple place to get what they need and start building. Over time, this freed up our data scientists to work on other high-value projects.\n\nHow to build a fully working app example\n\nTo show how to include real search logic in an application, we‚Äôve created a repo using the NCBI COVID-19 literature database. In this fully working example, we demonstrate how to load a dataframe of articles, search for keywords in the titles, and display the results:\n\nFirst, we load a TSV as a dataframe and cache the data:\n\n@st.cache\ndef load_data(filepath:str) -> pd.DataFrame:\n    \"\"\" Load data from local TSV \"\"\"\n    return pd.read_csv(filepath, sep=\"\\\\t\", skiprows=33).fillna(\"\")\n\n\nNext, we create a function to search the dataframe. This is done by checking to see if a column has rows where our search term is a substring:\n\ndef search_dataframe(df:pd.DataFrame, column:str, search_str:str) -> pd.DataFrame:\n    \"\"\" Search a column for a substring and return results as df \"\"\"\n    results = df.loc[df[column].str.contains(search_str, case=False)]\n    return results\n\n\nFinally, we want to load the data once the application starts:\n\n# env variable\nDATA_FILEPATH = \"litcovid.export.all.tsv\"\n\n# within app(): load data from local tsv as dataframe\ndf = load_data(DATA_FILEPATH)\n\n\nNow when we click on st.form_submit_button, we‚Äôll run the search, notify the user of the number of results found, and display the first ten hits:\n\n# if button is clicked, run search\nif submit_button:\n  with st.spinner(\"Searching (this could take a minute...) :hourglass:\"):\n\n      # search logic goes here! - search titles for keyword\n      results = search_dataframe(df, \"title_e\", text_query)\n\n      # notify when search is complete\n      st.success(f\"Search is complete :rocket: ‚Äî **{len(results)}** results found\")\n\n  # now display the top 10 results\n  st.table(results.head(n=10))\n\n\nOne of the best features of Streamlit is interactive plotting and visualization. At ScienceIO, we love to use Altair plots which are easily converted from static to dynamic. Let‚Äôs add an interactive bar plot that shows the top journals in our search results.\n\n# we can use altair to turn our results dataframe into a bar chart of top journal\n\nimport altair as alt\n\nalt.Chart(results).transform_aggregate(\n        count='count()',\n        groupby=['journal']\n    ).transform_window(\n        rank='rank(count)',\n        sort=[alt.SortField('count', order='descending')]\n    ).transform_filter(\n        alt.datum.rank < 10\n    ).mark_bar().encode(\n        y=alt.Y('journal:N', sort='-x'),\n        x='count:Q',\n        tooltip=['journal:N', 'count:Q']\n    ).properties(\n        width=700,\n        height=400\n\t\t).interactive()\n\n\nThe code might seem intimidating, but don‚Äôt worry. You got this! Altair lets you chain functions together to make changes to data and charts that are each straightforward. The first set of functions aggregate, sort, and filter the data ‚Äî transform_aggregate() groups the results and counts the number of times each value appears; transform_window() ranks/sorts each row by its count; transform_filter() removes all rows below a certain rank (for example, below 10th place). Now that our data is appropriately transformed, we can plot ‚Äî mark_bar() creates a bar chart, we declare our x- and y-axes and a tooltip with encode(), and we can specify chart width and height using properties(). Finally, we add interactive() ‚Äî now, we can hover over each bar to see the journal and the total number of publications that match your search terms.\n\nAnd now we have a fully working example app of search app. In this example, we use a basic dataframe, but you can replace that with anything‚Äîa database, a data warehouse, or an API call. The only limit is your imagination!\n\nYou can find the code for this app on GitHub and try the app yourself here.\n\nWrapping up\n\nNow you know how to set up a search tool with Streamlit! We hope you better understand how Streamlit applications can act as fast and flexible tools on top of large datasets and that you‚Äôll build more cool things and share them in turn with the community!\n\nIf you have any questions or want to learn more about industrial-scale AI for healthcare, please post them below.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image.png (802√ó415)",
    "url": "https://blog.streamlit.io/content/images/2021/10/image.png",
    "html": ""
  },
  {
    "title": "0.88.0-release.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/09/0.88.0-release.gif#border",
    "html": ""
  },
  {
    "title": "Austin Chen - Streamlit",
    "url": "https://blog.streamlit.io/author/austin/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Austin Chen\n3 posts\nStreamlit ‚ù§Ô∏è Firestore (continued)\n\nAka the NoSQL sequel: Building a Reddit clone and deploying it securely\n\nTutorials\nby\nAustin Chen\n,\nApril 22 2021\nStreamlit ‚ù§Ô∏è Firestore\n\nUse Streamlit and Firestore to create a serverless web app with persistent data, written entirely in Python!\n\nTutorials\nby\nAustin Chen\n,\nJanuary 27 2021\nNew layout options for Streamlit\n\nIntroducing new layout primitives‚Äîcolumns, containers, and expanders!\n\nProduct\nby\nAustin Chen\n,\nOctober 8 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Discover and share useful bits of code with the¬†ü™¢¬†streamlit-extras library",
    "url": "https://blog.streamlit.io/discover-and-share-useful-bits-of-code-with-the-streamlit-extras-library/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDiscover and share useful bits of code with the¬†ü™¢¬†streamlit-extras library\n\nHow to extend the native capabilities of Streamlit apps\n\nBy Arnaud Miribel\nPosted in Tutorials, October 25 2022\nWhat are streamlit-extras?\nHow to discover extras\nBadges\nApp logo\nDataframe explorer UI\nToggle button\nHow to try extras\nHow to install extras\nHow to contribute your own extras\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Arnaud Miribel, and I‚Äôm a data scientist at Streamlit.\n\nEvery day I work on making or improving our internal apps. That means developing new reusable functions that improve the apps‚Äô appearance or functionality. I also collect useful bits of code on our forum or in the GitHub issues that I can incorporate into my work. Essentially, those bits are mini Streamlit components. Creating a new project for each would be a pain. So I bundled them all into one project‚Äîan experimental library where you can easily discover, try, install, and share these components.\n\nSay hello to‚Ä¶\n\n‚Ä¶streamlit-extras!\n\nIn this post, I‚Äôll go over the following:\n\nWhat are streamlit-extras?\nHow to discover extras\nHow to try extras\nHow to install extras\nHow to contribute your own extras\n\nLet‚Äôs get going!\n\nWhat are streamlit-extras?\n\nstreamlit-extras (extras for short) is just a collection of small custom Streamlit components. Most of them have a minimal of code, and most are Python-only (with some CSS/HTML hacks via st.markdown). Think of extras as an installable utils.py full of small, handy Streamlit components. üôÇ\n\nHow to discover extras\n\nHead over to extras.streamlitapp.com to discover extras in their natural habitat. Even the library‚Äôs gallery is a multipage app (why not make another app, right? üòâ).\n\nLet‚Äôs look at some of them:\n\nBadges\n\nAn easy way to add social badges to your apps:\n\nApp logo\n\nA small function to add a logo to your navigation bar (sweet!):\n\nDataframe explorer UI\n\nRecognize this? The now famous filter_dataframe function that creates a UI on top of dataframes:\n\nToggle button\n\nA simple lightweight alternative to st.expander‚Äîa toggle button:\n\nBrowse the pages in the left navigation bar to see more!\n\nHow to try extras\n\nSome extras feature a ‚ÄúPlayground‚Äù section. Try passing your own parameters to the extra and see how it works.\n\nFor example, try playing with streamlit_extras.stoggle:\n\nJust for fun, try it within the colored headers or the keyboard text extra. And guess what‚Ä¶‚ÄúPlaygrounds‚Äù are powered by yet another extra: the function explorer. üòâ\n\nHow to install extras\n\nYou can easily use extras in your apps. Simply open your terminal and run:\n\npip install streamlit-extras\n\n\nExtras are accessible as modules within the library itself, and you can use all of them.\n\nFor example, if you want to use streamlit_extras.stoggle, just create a new script:\n\n# streamlit_app.py\n\nimport streamlit as st \nfrom streamlit_extras.stoggle import stoggle\n\nstoggle(\"Here's a little secret\", \"Streamlit-extras is so cooool\")\n\n\nGo ahead and run streamlit run streamlit_app.py, and you‚Äôll see this in your app:\n\nCongrats! You have used your first Streamlit extra. üéä\n\nHow to contribute your own extras\n\nAs part of this project, I populated the library with 20+ extras. But there is room for more. And it‚Äôs open-source, so you‚Äôre welcome to contribute!\n\nFor example, if you want to share an extra strike that strikes a text, you can do it with HTML (let‚Äôs forget st.markdown(\"~Hey~\") for a moment):\n\n def strike(text: str):\n    \"\"\"Strikes input text\n\n    Args:\n        text (str): Input text\n    \"\"\"\n    return st.write(f\"<del>{text}</del>\", unsafe_allow_html=True)\n\n\nHere is an example usage of strike():\n\nstrike(\"Some outdated statement\")\n\n\nThis will output:\n\nTake a look at the code. All extras have their own directory within streamlit_extras and a __init__.py in it:\n\n.\n‚îú‚îÄ‚îÄ CONTRIBUTING.md\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ gallery\n‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt\n‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py\n‚îÇ   ‚îî‚îÄ‚îÄ streamlit_patches.py\n‚îú‚îÄ‚îÄ poetry.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îî‚îÄ‚îÄ streamlit_extras       # <-- This is where extras live!\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ altex              # <-- Every extra has its directory...\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py    # <-- ... and an __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ annotated_text\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ tests\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îî‚îÄ‚îÄ test_extras.py\n\n\nTo add a new extra:\n\nCreate a new directory in src/streamlit_extras called strike_text.\nCreate a new file __init__.py in this new directory.\nPut the strike() function, for example, and some metadata:\n# strike_text/__init__.py\nimport streamlit as st\n\ndef strike(text: str):\n    \"\"\"Strikes input text\n\n    Args:\n        text (str): Input text\n    \"\"\"\n    return st.write(f\"<del>{text}</del>\", unsafe_allow_html=True)\n\ndef example():\n    strike(\"Some outdated statement\")\n\n# Metadata that's useful to add your extra to the gallery\n__func__ = strike\n__title__ = \"Strike text\"\n__desc__ = \"A simple function to strike text\"\n__icon__ = \"‚ö°\"\n__examples__ = [example]\n__author__ = \"Dark Vador\"\n__experimental_playground__ = True\n\n\nThat‚Äôs it!\n\nNow create a pull request with these additions. Upon approval, I‚Äôll feature your extra and make it accessible in the streamlit-extras library. Easy, right?\n\nWrapping up\n\nHopefully you‚Äôll find extras useful and contribute your own. If you have questions or thoughts, drop them in our Github repo. Remember‚Äîextras are just a little bit more of what you can do with Streamlit. The Streamlit community regularly makes awesome new components that extend your apps in big ways.\n\nIf you want to learn about how to make your own advanced component with HTML and JavaScript, check out this post.\n\nHappy Streamlit-ing! üéà\n\nP.S.: Learn more about streamlit-extras and the Streamlit Data Product team at Snowflake BUILD, a free virtual developer conference. My colleague Tyler will be there, talking about the library along with the product feedback loops we‚Äôve set up!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Record_Screencast-1.gif (1200√ó660)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Record_Screencast-1.gif#border",
    "html": ""
  },
  {
    "title": "6 Tips for Improving Your App Performance | Streamlit",
    "url": "https://blog.streamlit.io/six-tips-for-improving-your-streamlit-app-performance/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n6 tips for improving your Streamlit app performance\n\nMoving your Streamlit app from analysis to production\n\nBy Randy Zwitch\nPosted in Tutorials, July 20 2021\n1. Do you really need a million-value slider?\n2. Pre-calculate inputs and cache whenever possible\n3. Avoid downloading large, static models\n4. Remove unused data\n5. Optimize data storage formats\n6. ¬†Use the right tool for the job!\nWrapping Up\nResources\nContents\nShare this post\n‚Üê All posts\n\nIn the first two parts of this blog series (part 1, part 2), my colleague Abhi outlined principles for designing Streamlit apps from the perspective of the end user. From using wireframes before you start coding to adding contextual help, layout and theming features, designing for the intended user can take your app from being seldom used to being a key tool in your organization.\n\nIn this final part of the series, I‚Äôll discuss some subtle unseen-but-definitely-felt user experience improvements that tune the performance of your Streamlit apps. And for most of this post we'll be focusing on apps that, at least in part, operate on static data (data that doesn't change). The tips are listed in order from easiest-to-implement to hardest-to-implement, and depending on the use case not all of them will apply to every Streamlit app.\n\n1. Do you really need a million-value slider?\n\nIn the same way a line chart with 10,000 lines is visual overload, having a slider, selectbox, multiselect, or select_slider widget with 10,000 options is granularity overload: it would be nigh impossible to use, not to mention to understand each option.\n\nBut even worse, for most of those widgets, the more options you pass as input the more data needs to be sent from the Streamlit server to the Streamlit JavaScript frontend. This means more data needs to be serialized, sent over the network, and stored in the browser's memory. Take the following line of code:\n\nprice = st.selectbox(\"House Price\", range(10000000))\n\nIn this toy example, all 10‚Å∑ values will be converted to str and almost 100 megabytes sent over to the user's browser.\n\nA less silly example would be a slider:\n\nprice = st.slider(\"House Price\", 1, 10000000)\n\nDue to implementation details, high-cardinality sliders don't suffer from the serialization and network transfer delays mentioned earlier, but they will still lead to a poor user experience (who needs to specify house prices up to the dollar?) and high memory usage. In my testing, the example above increased RAM usage by gigabytes until the web browser eventually gave up (though this is something that should be solvable on our end. We'll look into it!)\n\nSuffice to say, performance and user experience will be much improved if you consider alternative means of interacting with apps of this kind, such as a using st.number_input or st.text_input to allow the user to enter arbitrary or high-granularity inputs. If you absolutely must use widgets that enumerate every value, choose a step size that gives the end user at most dozens of choices to pick from. Your users will thank you!\n\nprice = st.slider(\"House Price\", 100000, 10000000, step=100000)\n2. Pre-calculate inputs and cache whenever possible\n\nWhen moving a Streamlit app to production, consider whether there are parts of the app that can be pre-calculated. If your app aggregates a 10-million-row dataframe by U.S. State, you can improve performance by caching that computation and only re-running it when needed:\n\n@st.cache(ttl=24*60*60)  # Don't forget to add a TTL!\ndef get_data_by_state():\n\treturn huge_df['US state'].unique()\n\nGoing further, if the data in question never changes, you can do this calculation beforehand outside of the app and save it as a file which you then load and cache when your app first starts:\n\n@st.cache  # No need for TTL this time. It's static data :)\ndef get_data_by_state():\n\treturn pd.read_csv(PRECALCULATED_DATA_PATH)\n\nCreating a list on-the-fly by taking the unique value across a 10-million-row dataframe is always going to be reading the list directly from memory. So, for dynamic datasets, wrap your computation in an @st.cache ¬†and set a large TTL. And for static datasets you should always consider offloading computations to a file, to constants, or to a separate Python modules.\n\n3. Avoid downloading large, static models\n\nIn the Streamlit Self-Driving Car demo, we show a code pattern where we download the YOLOv3 object detection model from S3 all while using st.progress to keep the user informed while the download takes place. While this is a great pattern when sharing a model across multiple repos, it does put the app at the mercy of internet bandwidth when running in production: the YOLOv3 model is approximately 240MB, which can take several minutes to download before a user even has a chance to get started.\n\nThe solution is simple: when pushing your Streamlit app to production, bring your model and other assets to the production machine, and you can get orders-of-magnitude better startup time. For Streamlit sharing specifically, Git LFS (Large File Storage) is supported, so you can use it to store your model in your GitHub repository and make it available to your app automatically. Couple that with reading a file from disk with @st.cache and app users may not even realize a model is being loaded in the background!\n\n4. Remove unused data\n\nWhen starting a data project, a common task is pulling data from a database or CSV file and exploring it interactively. If you haven't use Streamlit for this exploratory data analysis phase yet (EDA), you should give it a try ‚Äî just use Streamlit's \"magic commands\" and press Ctrl-S or Cmd-S to save the source file, and Streamlit shows updates live. This makes it easy to try numerous combinations of inputs and (hopefully!) find meaningful information in the data.\n\nHowever, when moving the app to production, you are often telling a story, not searching for one. At this point, you usually know that your analysis only needs a handful of columns among the dozens in your dataset. In some cases, a dataset may have columns that can‚Äôt even be shared (personally-identifying information), that don't change (version numbers) or simply aren‚Äôt being used (user-agent strings).\n\nSo go ahead and remove those unnecessary columns and rows you aren‚Äôt using. Your data will get read in faster, use less RAM, and overall be more efficient when calculations are performed. To paraphrase a computer-science saying, the fastest-loading data is the one you don't have to load!\n\n5. Optimize data storage formats\n\nIf you‚Äôve made it this far into the post, you probably have a pretty svelte app! Your input widgets are optimized to provide meaningful choices, these choices are coded as constants in your app, you‚Äôre not downloading large amounts of data over the internet unnecessarily and what data you are reading are only the rows and columns you need. But what if that‚Äôs not fast enough?\n\nIf you‚Äôre reading large amounts of data via CSV or JSON, consider using a binary-serialized format such as Apache Parquet or Apache Arrow IPC. While CSV and JSON are convenient formats for data transport, ultimately they are optimized for humans, not computers! By using an optimized data storage format, your production app won‚Äôt spend time parsing text into data types such as integers, floats and strings, which, incredibly, can consume quite a bit of time. Additionally, binary formats often have metadata and logical partitioning such that Python can read the metadata to find exactly where the data are located, skipping entire data partitions from loading.\n\n6. ¬†Use the right tool for the job!\n\nWhile many of the common libraries in the PyData ecosystem have C or FORTRAN underpinnings, in the end, some problems are larger than a single computer can reasonably handle. For tabular data, there have been decades of research into performance optimization of relational databases. From indexing to multi-core query processing, moving your computation workflow from Python to a relational database could give considerable speed improvements.\n\nTaking it a step further, for heavy workloads consider separating the Streamlit app from the computation portion, so that your computation can scale independently of the web app. ¬†Specialized hardware such as GPUs, Dask or Spark clusters and other higher-performance options are all ways to solve the largest data problems while still staying in the larger PyData ecosystem.\n\nWrapping Up\n\nHow are you optimizing your Streamlit apps? This blog post highlights six ways to improve Streamlit app performance, but there are definitely dozens of other tips and tricks that aren‚Äôt covered here. What are your favorite optimization tricks? Are you using Streamlit with databases? Stop by the Streamlit Community forum and let us know what you‚Äôve done to optimize your apps, and what else could be working better within Streamlit!\n\nResources\nGithub\nForum\nDesign Series Part 1\nDesign Series Part 2\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "2-2.png (1200√ó479)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-2.png#border",
    "html": ""
  },
  {
    "title": "ToolTips.gif (1200√ó708)",
    "url": "https://blog.streamlit.io/content/images/2021/08/ToolTips.gif#browser",
    "html": ""
  },
  {
    "title": "1-1.png (1200√ó335)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-1.png#border",
    "html": ""
  },
  {
    "title": "How one finance intern launched his data science career from a coding bootcamp in Brazil",
    "url": "https://blog.streamlit.io/how-one-finance-intern-launched-his-data-science-career/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow one finance intern launched his data science career from a coding bootcamp in Brazil\n\nLearn how Marcelo Jannuzzi of iFood got his dream job in data science\n\nBy Marcelo Jannuzzi and Ksenia Anske\nPosted in Case study, June 9 2022\nHow it all started\nHow Marcelo uses Streamlit\nMarcelo‚Äôs advice for aspiring data scientists\n1. Realize that data can be your job!\n2. Notice what gets you in the flow\n3. Consider a data science bootcamp\n4. Take advantage of company training\n5. Learn how to package your work (with Streamlit)\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nLooking to start your career in data science? Marcelo Jannuzzi of iFood will show you how to get a chance to do what you love even when you‚Äôre not looking!\n\nWe chatted with Marcelo about his journey from an intern in finance to data analyst at iFood, where he uses machine learning and statistical methods to model business problems and inform decision makers in his company.\n\nIn this post, we‚Äôll share with you his story‚Äîincluding how he got into data science:\n\nHow it all started\nHow Marcelo uses Streamlit\nMarcelo‚Äôs advice for aspiring data scientists\nHow it all started\n\nIn college, Marcelo studied business and finance. After a brief corporate banking internship at Citibank, he went on to Magnetis, an asset management fintech startup. He loved it, and it validated his wish to be in finance. Eventually he found himself as an analyst at a hedge fund, which was what he always wanted!\n\n‚ÄúBut then, finding myself in this ‚Äòdream job‚Äô job for me, I started to realize that what I actually enjoyed about my previous experiences wasn't the finance itself,‚Äù Marcelo said. ‚ÄúIt was the more mathematical or statistical aspects of coming up with models to allocate investments, or to price different securities, for example. It was more about the data side of things than the finance.‚Äù\n\nSo Marcelo decided to learn more about data science. He knew how to program from college and had some experience with Python and R. He went through a data science bootcamp and got a job as a data analyst at a food delivery company in Brazil.\n\nAfter only six months he joined iFood, the biggest FoodTech in Latin America.\n\n‚ÄúBefore coming to this field I never spent more than a year in the same job. I was worried that a job would always be just a job for me and that I'd never find something that I really enjoyed,‚Äù Marcelo said. ‚ÄúI was relieved when I found data science because in many ways it didn‚Äôt feel like work. It felt more like solving a puzzle. I was also drawn to the idea of predicting the future! It still blows my mind how well it works sometimes!‚Äù\n\nHow Marcelo uses Streamlit\n\nAs part of the the data team which supports iFood‚Äôs logistics operations, Marcelo uses Streamlit to showcase and build interactivity into his models.\n\n‚ÄúWe track a lot of different metrics and KPIs,‚Äù Marcelo said. ‚ÄúAnd they‚Äôre all interrelated, so a change to one of them affects another, which affects the next one, and can have this cascading effect throughout our business.‚Äù\n\nWith Streamlit Marcelo has created an interactive system of models that simulates these interactions between different aspects of the business. It allows him to see the tradeoffs that exist between the various metrics that describe iFood‚Äôs logistics and to simulate different scenarios.\n\n‚ÄúThis idea of the tradeoffs that exist between different parts of our business is key,‚Äù he said. ‚ÄúIf you want to improve one aspect, you have to be willing to take a hit somewhere else. We want to measure these tradeoffs and show them to our business users so that they can make better decisions on how to steer our operations.‚Äù\n\nTo model these interactions, Marcelo made a graph that describes how each KPI is related to each other. Each node in this graph has a model associated to it. ‚ÄúSo to make a prediction for a particular metric, we traverse the graph making predictions for each predecessor node, and feeding those predictions into the following nodes.‚Äù\n\n‚ÄúIt would be very hard for me to do a project like this with our standard toolset,‚Äù Marcelo explained. ‚ÄúIt would take a few months to develop the interface so that business users could interact with it. But Streamlit abstracts all of that away for me. I don‚Äôt have to worry about it! Being more familiar with Streamlit and all it can do increases the range of projects we in the Data Analysis team can undertake, since we don‚Äôt have to rely as much on engineers to implement whatever it is we want to do.‚Äù\n\nMarcelo‚Äôs advice for aspiring data scientists\n\nWe asked Marcelo for his advice for aspiring data scientists looking to start (or grow!) their career. Here is what he had to say:\n\n1. Realize that data can be your job!\n\nI feel kind of stupid because every time I was working on a personal project involving data or code in general, I was always psyched about it. I wanted to finish it. I didn‚Äôt care if I was hungry or needed go to the bathroom. I didn‚Äôt want to stop. I was in the zone. But for some reason, I never thought that could be my job.\n\nMy parents are both economists. They both worked at banks, and their friends also had backgrounds in finance, so I guess in my mind a ‚Äúreal job‚Äù was working in a bank. Having studied business in college also didn‚Äôt help in that respect, as a lot of my colleagues were getting jobs in investment banking or management consulting. Then I read in a magazine about a few data science bootcamps popping up around Brazil. They‚Äôre common in the US, but at the time they were very new in Brazil. So in late 2018 I decided to enroll in one of them, and while going through the program and talking to different people in the field I realized that yes, this could be my job!\n\n2. Notice what gets you in the flow\n\nTry to notice the things that get you into that state of flow. Things that you want to finish despite being hungry or needing to go to the bathroom. Also notice the things that you actually enjoy, not the things you think you should enjoy (or that your parents or colleagues told you you will enjoy).\n\n3. Consider a data science bootcamp\n\nI got my first job in data science because of a bootcamp. It was taught by people who worked in the industry and gave me a good overview of the basic skills, what was expected of me, and also helped me start to build a professional network. It's hard getting a cold start in a new field, and a program like this can help jumpstart things.\n\nAlso, going through the bootcamp felt like a stamp of approval of sorts. Before this I thought, ‚ÄúWho am I to get into this field? I have no experience here, so who‚Äôs going to hire me?‚Äù After completing it I felt much more confident to look for a job in data science.\n\n4. Take advantage of company training\n\niFood has a partnership with NOVA University Lisbon, and gave me and some other colleagues the opportunity to get our Master‚Äôs degree here. I enjoy the academic environment and had already been thinking about continuing my studies, so having the chance to move from Brazil to Portugal to do so, while still being able to work remotely at iFood was amazing. iFood also provides us with many other opportunities to develop our skills in other ways, like allowing us dedicated time during the week for personal development, a budget for books and courses, among other kinds of training.\n\n5. Learn how to package your work (with Streamlit)\n\nData is awesome, but I‚Äôve found that a very important part of working in data science is learning how to package your work in a way that's appealing to your colleagues and end users. A simple solution that gets used is way more valuable than a sofisticated solution that doesn‚Äôt get used because people don‚Äôt understand it or because its hard to use.\n\nI think this is where Streamlit really shines. You can make something look professional with very little effort. Many teams here at iFood use Streamlit, as it lets us move faster and depend less on our engineering and product teams by taking the initial phases of this kind of work into our own hands.\n\nWe have a pretty big data team at iFood‚Äîhundreds of people‚Äîso we don't have contact with everyone, but many of us found Streamlit independently around 2020. I heard about it in the Brazilian data science podcast Data Hackers. Now about a dozen people are using it across three teams, and more people find out about it every day!\n\nWrapping up\n\nThank you for reading Marcelo‚Äôs story! We hope it‚Äôll inspire you to start or to continue growing your career in data science. If you have any questions, please post them in the comments below or connect with Marcelo on LinkedIn.\n\nHappy coding! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "JULO improves financial inclusion in Indonesia with Streamlit",
    "url": "https://blog.streamlit.io/how-streamlit-is-helping-julo-improve-financial-inclusion-in-indonesia/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nJULO improves financial inclusion in Indonesia with Streamlit\n\nLearn how JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nBy Martijn Wieriks and Ksenia Anske\nPosted in Case study, June 30 2022\nHow JULO went from manual underwriting to automated credit scoring and a 22-member data team\nHow JULO discovered Streamlit\nHow Streamlit helps JULO grow faster\nWhat other startups can learn about data from JULO‚Äôs story\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nJULO is a financial startup that‚Äôs trying to solve the problem of financial inclusion in Indonesia.\n\n‚ÄúA lot of people in Indonesia don‚Äôt have access to traditional financial services,‚Äù said Martijn Wieriks, Chief Data Officer at JULO. ‚ÄúBanks have a difficult time underwriting them because they don‚Äôt have traditional financial track records. So they end up being excluded.‚Äù\n\nJULO is working to change that by using proprietary credit models that leverage alternative data sources like mobile device and health insurance usage.\n\nCredit scoring is at the center of JULO‚Äôs business model‚Äîmaking data a key element of its growth and success.\n\nStreamlit has been a key part of JULO‚Äôs growth, empowering developers to deliver complex, interactive data visualizations so that stakeholders can quickly make data-backed business decisions.\n\nHow JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nHere‚Äôs more about JULO‚Äôs unique business model‚Äîand how data has been central to its success.\n\nStep 1: Manual underwriting and creation of the first data warehouse\n\nJULO started by building a customer base, a tool kit, and a foundation for their first data warehouse. Agents did the underwriting manually by relying on the data team to collect data and aggregate it into reports.\n\nStep 2: First automated credit model\n\nAfter JULO collected sufficient data for a year, they built their first credit model iteration. Suddenly they could automate everything.\n\n‚ÄúSeeing it in practice was amazing,‚Äù Martijn said. ‚ÄúWe were making the right predictions and enabling the business to make fast credit decisions while carefully managing risk. It was a wow moment.‚Äù\n\nJULO helps financially underserved Indonesians meet their credit needs with innovative Credit Scoring solutions.\n\nStep 3: Building the development team\n\n‚ÄúThere's a big talent gap in Indonesia,‚Äù Martijn said. ‚ÄúInstead of competing with larger companies, we decided to work with talented new grads. We developed a program to train new hires and get them up to speed in 3-6 months.‚Äù\n\nJULO is now at 22 people on the data team and is continuing to grow.\n\nStep 4: Growing the customer base and improving the model over time\n\nOnce JULO‚Äôs use cases expanded, their technology stack improved and they started getting more customers. This growth posed new challenges.\n\n‚ÄúThe more data variables we added to a machine learning model, the less transparent it became,‚Äù Martijn said. ‚ÄúIt was important to understand how a machine learning model makes decisions, especially in credit scoring and lending. Because we didn‚Äôt want to unfairly bias specific groups.‚Äù\n\nInteractive Streamlit dashboards have enabled the finance team to review and plan different strategic scenarios more easily (credit: Darwin Natapradja).\n\nHere is a Streamlit app that simulates credit score performance on different datasets (simulations help data scientists and business users build intuition around metrics):\n\nHow JULO discovered Streamlit\n\n‚ÄúI came across a Medium post and a GitHub repo that mentioned Streamlit,‚Äù Martijn said. ‚ÄúIt said it was UI for machine learning engineers to create machine learning apps. It didn't click with me then. Six months later I tried it again. There were more app examples. I tinkered with it and was surprised by how quickly I could build a web application. With existing data science skills, like writing Markdown and Python, it was so simple to add interactive components to static code. It was like Jupyter Notebooks on steroids.‚Äù\n\nMartijn used Streamlit for his personal projects for two years, then introduced it to his team.\n\nHow Streamlit helps JULO grow faster\n\nData is very important for JULO. It‚Äôs their first-class asset, value proposition, and IP. Streamlit helps JULO take care of data, manage bias in models, and demonstrate data science principles to business users and risk managers.\n\n1. Higher velocity\n\nJULO has many active credit lines and needs to report to banks on their financial performance. Martijn‚Äôs team used to spend a lot of time on preparing documents, spreadsheets, and slide decks. But they weren‚Äôt interactive, so they created a CFO dashboard in Streamlit.\n\n‚ÄúWe could have two conversations of 90 minutes in front of a whiteboard, trying to draw out different risk scenarios to each other. I was there with our CEO, our Business Intelligence Manager, and two data scientists. The whiteboard soon became a complex mess of charts, numbers and variables, which made decision making increasingly difficult. Then over the weekend our BI Manager decided to create this CFO dashboard. On Monday we were able to walk through the same scenarios in an interactive way. It took maybe 10-15 minutes for that to click. We were able to condense 3 hours into 15 minutes to have a breakthrough and get the understanding we needed.‚Äù\n\n2. Better decision making\n\nStreamlit lets JULO quickly develop complex and interactive data visualizations.\n\n‚ÄúWe used to build custom Flask apps with Jupyter notebooks and widgets,‚Äù Martijn said. ‚ÄúBut it's not stakeholder friendly because they don't know how to work with notebooks. Streamlit is a presentation tool. You can serve it as a website. And people can play with it. It's very stakeholder friendly, which is super important because it‚Äôs all about putting data solutions into the hands of others. That‚Äôs Streamlit‚Äôs main value.‚Äù\n\n3. Empowered stakeholders\n\nChanging algorithm parameters and doing exhaustive research can help finetune machine learning models. More time spent equals more accuracy.\n\n‚ÄúAdding more variables may increase the model‚Äôs accuracy, but it‚Äôll also make it harder to understand the relationship between them,‚Äù Martijn said. ‚ÄúIf you only have a customer's age, income, and their latest completed school level, then understanding how a model makes a risk decision is easy. But with a thousand variables, it's a different story. How does each variable affect the score? Is the change positive, negative, or linear? With Streamlit we were able to explore the data, visualize it, and make it accessible.‚Äù\n\nWhat other startups can learn about data from JULO‚Äôs story\n\n1. Be patient with lagging data\n\nAny startup that wants to do machine learning needs to collect data for their first model. It takes time. Depending on your use-case your key metrics might be heavily lagging. You‚Äôll be able to see performance data on loans 3-6 months after you've disbursed them. Only then can you say with confidence, ‚ÄúThis was a good loan and that was a bad loan.‚Äù\n\n2. Visualize data models to get alignment and buy-in\n\nAt first we grew slowly. Then suddenly after the first year we had enough data. One rule of thumb in credit scoring is that you need at least a thousand bad loans to build your first model. It took us a while to get there. Once we hit it, we quickly built the first iteration, but we didn‚Äôt know if our model would work.\n\nWhen it did, it was a magic moment. We had a couple of charts and saw people with really high credit scores who had a much lower delinquency rate. That group of people was a lot better than the next, and the next, and the next. We saw a nice sloping line. It was exactly what we were hoping for.\n\n3. Make inclusion a priority\n\nIn lending, it has been repeatedly shown that women tend to perform better than men as they're more responsible with money. But if men are over-represented in your data, then a machine learning model may overestimate the risk of women‚Äôs delinquency and give them a lower credit score. To detect and manage such unfair bias, we compared groups of women and men so as not to unfairly disadvantage them based on gender.\n\nWrapping up\n\n‚ÄúWe've been working with Streamlit to solve big problems,‚Äù Martijn said. ‚ÄúWe've tried other tools, but they haven‚Äôt really clicked. Streamlit has become a part of our toolset.‚Äù\n\nThank you for reading JULO‚Äôs story! If you have any questions, please leave them in the comments below or contact Martijn on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "visualization--3-.png (704√ó350)",
    "url": "https://blog.streamlit.io/content/images/2022/08/visualization--3-.png",
    "html": ""
  },
  {
    "title": "menu_screenshot.png (865√ó638)",
    "url": "https://blog.streamlit.io/content/images/2021/09/menu_screenshot.png#browser",
    "html": ""
  },
  {
    "title": "about-menu.jpeg (1018√ó745)",
    "url": "https://blog.streamlit.io/content/images/2021/09/about-menu.jpeg",
    "html": ""
  },
  {
    "title": "glaucoma_detector-with-gc--3-.png (640√ó480)",
    "url": "https://blog.streamlit.io/content/images/2021/09/glaucoma_detector-with-gc--3-.png#border",
    "html": ""
  },
  {
    "title": "Untitled--2--1.png (640√ó480)",
    "url": "https://blog.streamlit.io/content/images/2021/09/Untitled--2--1.png#border",
    "html": ""
  },
  {
    "title": "3-12.png (2060√ó981)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-12.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--6-.png (1906√ó1112)",
    "url": "https://blog.streamlit.io/content/images/2021/09/Untitled--6-.png#border",
    "html": ""
  },
  {
    "title": "Untitled--1--1.png (794√ó848)",
    "url": "https://blog.streamlit.io/content/images/2021/09/Untitled--1--1.png#border",
    "html": ""
  },
  {
    "title": "memory-usage-4.png (2000√ó536)",
    "url": "https://blog.streamlit.io/content/images/2021/10/memory-usage-4.png#browser",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-08-10-at-2.54.28-PM.png (762√ó244)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.54.28-PM.png",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-08-10-at-2.54.28-PM-1.png (762√ó244)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.54.28-PM-1.png",
    "html": ""
  },
  {
    "title": "filmstrip--6--1.png (1600√ó812)",
    "url": "https://blog.streamlit.io/content/images/2021/08/filmstrip--6--1.png#border",
    "html": ""
  },
  {
    "title": "visualization--4-.png (704√ó350)",
    "url": "https://blog.streamlit.io/content/images/2022/08/visualization--4-.png",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-08-10-at-2.48.08-PM-1.png (1575√ó610)",
    "url": "https://blog.streamlit.io/content/images/2022/08/Screen-Shot-2022-08-10-at-2.48.08-PM-1.png#border",
    "html": ""
  },
  {
    "title": "visualization--5-.png (704√ó350)",
    "url": "https://blog.streamlit.io/content/images/2022/08/visualization--5-.png",
    "html": ""
  },
  {
    "title": "Ksenia Anske - Streamlit",
    "url": "https://blog.streamlit.io/author/kseniaanske/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Ksenia Anske\n6 posts\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nJULO improves financial inclusion in Indonesia with Streamlit\n\nLearn how JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nCase study\nby\nMartijn Wieriks and¬†\n1\n¬†more,\nJune 30 2022\nHow one finance intern launched his data science career from a coding bootcamp in Brazil\n\nLearn how Marcelo Jannuzzi of iFood got his dream job in data science\n\nCase study\nby\nMarcelo Jannuzzi and¬†\n1\n¬†more,\nJune 9 2022\nWissam Siblini uses Streamlit for pathology detection in chest radiographs\n\nLearn how Wissam detected thoracic pathologies in medical images\n\nCase study\nby\nWissam Siblini and¬†\n1\n¬†more,\nMay 3 2022\nThe Stable solves its data scalability problem with Streamlit\n\nHow Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days\n\nCase study\nby\nMark von Oven and¬†\n1\n¬†more,\nApril 28 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1-14.png (1348√ó688)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-14.png#browser",
    "html": ""
  },
  {
    "title": "03-drilling-into-intents.png (2547√ó1127)",
    "url": "https://blog.streamlit.io/content/images/2022/01/03-drilling-into-intents.png#browser",
    "html": ""
  },
  {
    "title": "01-outlier-calls-1.png (2554√ó1302)",
    "url": "https://blog.streamlit.io/content/images/2022/01/01-outlier-calls-1.png#browser",
    "html": ""
  },
  {
    "title": "The Streamlit roadmap‚Äîbig plans for 2020!",
    "url": "https://blog.streamlit.io/https-discuss-streamlit-io-t-the-streamlit-roadmap-big-plans-for-2020-2054/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe Streamlit roadmap‚Äîbig plans for 2020!\n\nDevoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers\n\nBy Adrien Treuille\nPosted in Product, February 27 2020\nCaching\nCustom components\nCustomizable layout\nProgrammable state\nDeploy\nResources\nContents\nShare this post\n‚Üê All posts\n\nWe created Streamlit to give the Python data community a new superpower: the ability to create beautiful apps as easily as writing Python scripts.\n\nWe launched a minimal version of Streamlit in October with only basic output and interaction primitives. Despite these limitations, the response has been overwhelming: almost 7,000 Github stars and 13,000 monthly active users in less than four months! Most exciting has been to see the emergence of an intricate archipelago of Streamlit apps, from simple, fun demos of open-source projects, to community-spanning infrastructure and complex internal tooling at major companies used in production by hundreds of employees.\n\nInspired by your energy and creativity, we‚Äôre devoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers. Our goal is to make Streamlit not only the most productive (and fun!) app building experience in the Python ecosystem, but also the most powerful by adding:\n\nImproved caching that‚Äôs easier to use and understand\nA custom component system to extend Streamlit‚Äôs capabilities in the browser\nLayout primitives to customize layout and other visual elements\nUser-programmable state, especially for multi-page apps\nEnabling you to easily deploy apps from Streamlit (closed source)\n\nA detailed feature list is on GitHub, and it‚Äôs really just a distillation of the ideas coming from Streamlit‚Äôs amazingly smart and creative community. Please help us understand what to build by submitting issues and pull requests, and by sharing your thoughts in the comments below.\n\nCaching\n\nCaching enables you to reuse data and computation in your apps, allowing scripts to run quickly by saving the results of expensive functions. We recently released hash_funcs so that you can set your own hash function for specific data types like TensorFlow sessions or live database connections. And we added more documentation on basic and advanced caching. Coming up are even more improvements to caching for other function types and some added magic to make everything around caching even more straightforward. Please share thoughts here about how you‚Äôd like to see caching work.\n\nCustom components\n\nThe Streamlit custom components system will give you the ability to write arbitrary React or Javascript code and insert it into your app. This opens the door for a lot of possibilities for custom solutions to visualization, interactivity with chart/maps/tables, and other unique needs of your app. Please share thoughts here about how you‚Äôd like to see custom components work.\n\nCustomizable layout\n\nOur community has already created some great style and layout resources (and we have no plans to deprecate html, unsafe_allow_html=True!), but Streamlit in its current form doesn‚Äôt make layout nearly as easy as we think it should be. We‚Äôll be adding style and customization options to Streamlit, and also building new layout primitives: horizontal, grid, scroll views, and more. This is a tricky one to get right because layouts make up some of the most complex parts of display logic like CSS, not to mention it‚Äôs really easy to make these layouts look ugly. What are your thoughts? What are your favorite layout systems in other languages? Please share thoughts here about how you‚Äôd like to see layout work.\n\nProgrammable state\n\nRight now, getting a Streamlit app to store internal state, like information a user entered in a form, is simply too tricky. There are some workarounds for session state, but we want to give you a baked-in and elegant version of programmable state so you can build apps with sequential logic, apps with multiple pages, apps that incrementally ask users for input, and so on. Please share thoughts here about the use cases you‚Äôd like to see supported.\n\nDeploy\n\nWe know that building a great app is only half of the equation. You also want to deploy and share your app with others. We want Streamlit‚Äôs deployment workflow to be as elegant and awesome as its app-building workflow. That‚Äôs why we‚Äôre creating Streamlit for Teams: a single-click deployment solution for Streamlit apps with built-in enterprise-grade features like auth, logging, and auto-scaling. The first version of this will be rolled out for free to the community in a few months (and we‚Äôre expanding the beta soon, apologies if we haven‚Äôt gotten back to you yet!).\n\nFor now, you can check out some great community resources about deploying on AWS, GCP, Heroku, and Azure, and you help us by providing feedback on how you‚Äôd like to use Streamlit in your company.\n\nWe‚Äôve been tinkering with and refining these features over at Streamlit HQ. We‚Äôre so excited to share these new superpowers and iterate on them with the community!\n\nThank you for your part in the Streamlit journey and here‚Äôs to a great 2020 ‚ù§Ô∏è\n\nResources\n\nRoadmap\nDocumentation\nGitHub\nChangelog\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "02-latest-call-detail-screen-1.png (2551√ó1147)",
    "url": "https://blog.streamlit.io/content/images/2022/01/02-latest-call-detail-screen-1.png#browser",
    "html": ""
  },
  {
    "title": "Misra Turp - Streamlit",
    "url": "https://blog.streamlit.io/author/misra/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Misra Turp\nDeveloper educator at AssemblyAI.\n1 post\nWebsite\nTwitter\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "James Thompson - Streamlit",
    "url": "https://blog.streamlit.io/author/james/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by James Thompson\n1 post\nAdd secrets to your Streamlit apps\n\nUse Secrets Management in Streamlit sharing to securely connect to private data sources\n\nTutorials\nby\nJames Thompson\n,\nApril 9 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 5)",
    "url": "https://blog.streamlit.io/page/5/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDisplay a race on a live map üèÉ\n\nCreate a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library\n\nAdvocate Posts\nby\nMark Needham\n,\nJune 22 2023\nLangChain tutorial #4: Build an Ask the Doc app\n\nHow to get answers from documents using embeddings, a vector store, and a question-answering chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 20 2023\nBuilding a Streamlit and scikit-learn app with ChatGPT\n\nCatching up on coding skills with an AI assistant\n\nLLMs\nby\nMichael Hunger\n,\nJune 16 2023\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nLLMs\nby\nAdrien Treuille and¬†\n1\n¬†more,\nJune 15 2023\nLangChain tutorial #3: Build a Text Summarization app\n\nExplore the use of the document loader, text splitter, and summarization chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 13 2023\nMonte Carlo simulations with Streamlit\n\nLearn how to predict future stock prices\n\nSnowflake powered ‚ùÑÔ∏è\nby\nMats Stellwall\n,\nJune 8 2023\nLangChain tutorial #2: Build a blog outline generator app in 25 lines of code\n\nA guide on conquering writer‚Äôs block with a Streamlit app\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 7 2023\nLangChain tutorial #1: Build an LLM-powered app in 18 lines of code\n\nA step-by-step guide using OpenAI, LangChain, and Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 31 2023\n8 tips for securely using API keys\n\nHow to safely navigate the turbulent landscape of LLM-powered apps\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 19 2023\nSemantic search, Part 2: Building a local search app\n\nMaking an app with Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data from Snowflake Marketplace\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 18 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Christian Klose - Streamlit",
    "url": "https://blog.streamlit.io/author/christian/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Christian Klose\n1 post\nBuild knowledge graphs with the Streamlit Agraph component\n\nA powerful and lightweight library for visualizing networks/graphs\n\nAdvocate Posts\nby\nChristian Klose\n,\nNovember 25 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "catbot.png (1123√ó1858)",
    "url": "https://blog.streamlit.io/content/images/2023/04/catbot.png#border",
    "html": ""
  },
  {
    "title": "hello-who-are-you-1.png (751√ó473)",
    "url": "https://blog.streamlit.io/content/images/2023/04/hello-who-are-you-1.png#border",
    "html": ""
  },
  {
    "title": "catbot-2.png (744√ó783)",
    "url": "https://blog.streamlit.io/content/images/2023/04/catbot-2.png#border",
    "html": ""
  },
  {
    "title": "user_icon.png (512√ó512)",
    "url": "https://blog.streamlit.io/content/files/2023/04/user_icon.png",
    "html": ""
  },
  {
    "title": "gif-1-4.gif (960√ó574)",
    "url": "https://blog.streamlit.io/content/images/2021/08/gif-1-4.gif#browser",
    "html": ""
  },
  {
    "title": "AI_icon.png (60√ó60)",
    "url": "https://blog.streamlit.io/content/files/2023/04/AI_icon.png",
    "html": ""
  },
  {
    "title": "Screen-Shot-2020-11-25-at-9.36.31-AM-1.png (1458√ó970)",
    "url": "https://blog.streamlit.io/content/images/2021/08/Screen-Shot-2020-11-25-at-9.36.31-AM-1.png#browser",
    "html": ""
  },
  {
    "title": "4.1-1.png (1200√ó848)",
    "url": "https://blog.streamlit.io/content/images/2021/08/4.1-1.png#browser",
    "html": ""
  },
  {
    "title": "Creating a Time Zone Converter with Streamlit",
    "url": "https://blog.streamlit.io/creating-a-time-zone-converter-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCreating a Time Zone Converter with Streamlit\n\n6 steps on how to build your own converter\n\nBy Vin√≠cius Oviedo\nPosted in Advocate Posts, April 25 2023\nHow does the Time Zone Converter work?\n1. Import the required Python modules/package\n2. Create a set of continents and countries in the time zone context\n3. Configure the Streamlit page, header, and dropdown menu for continent and country selection\n4. Get the corresponding UTC+x time zone for the user selection\n5. Display the resulting time‚Äîthe informed PST time converted to UTC+x\n6. Apply a custom dark theme by creating a .streamlit folder with this config.toml file\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Vin√≠cius, and I‚Äôm a Data Analyst and LaTeX Editor.\n\nI love solving problems and assisting customers with technology. When I ask a customer about a delivery deadline, they usually tell me a day and time in a different time zone than mine (e.g., 4 PM PST). Since I‚Äôm from Brazil, I have to convert it from PST to UTC+x, where \"+x\" refers to an offset that can be positive or negative.\n\nThere are lots of tools for time zone conversion, but I wanted something simple, intuitive, and ad-free. So I made a Streamlit app that converts a time zone from PST to UTC for any country! üëè\n\nIn this post, I‚Äôll show you how to build it in six steps:\n\nImport the required Python modules/package\nCreate a set of continents and countries\nConfigure the Streamlit page, header, and dropdown menu\nGet the corresponding UTC+x time zone\nDisplay the resulting time\nApply a custom dark theme\nüëâ\nYou can go straight to the app and the GitHub repo, if you‚Äôd like to skip reading.\n\nBut first, let‚Äôs talk about‚Ä¶\n\nHow does the Time Zone Converter work?\n\nThe way it works is super simple:\n\nThe user selects a continent and a country.\nThe corresponding time zone (in \"UTC+x\" format) is obtained based on the user's selection.\nThe user enters a PST time to be converted into the associated time obtained from step (2).\nThe resulting time for the selected country is displayed in the app.\n\nNow, let‚Äôs help you develop your own converter!\n\n1. Import the required Python modules/package\n# Required Python modules/packages\nimport streamlit as st         # Streamlit framework \nfrom datetime import datetime  # For date and time\nimport pytz                    # For time zones\n\n2. Create a set of continents and countries in the time zone context\n# Create a dictionary with country name and corresponding timezone\ntimezone_dict = {\n    \"North America\": {\n        \"United States\": \"America/New_York\",\n        \"Canada\": \"America/Toronto\",\n        \"Mexico\": \"America/Mexico_City\",\n        \"Jamaica\": \"America/Jamaica\",\n        \"Costa Rica\": \"America/Costa_Rica\",\n        \"Bahamas\": \"America/Nassau\",\n        \"Honduras\": \"America/Tegucigalpa\",\n        \"Cuba\": \"America/Havana\",\n        \"Dominican Republic\": \"America/Santo_Domingo\"\n    },\n    \"South America\": {\n        \"Brazil\": \"America/Sao_Paulo\",\n        \"Argentina\": \"America/Argentina/Buenos_Aires\",\n        \"Chile\": \"America/Santiago\",\n        \"Colombia\": \"America/Bogota\",\n        \"Peru\": \"America/Lima\",\n        \"Uruguay\": \"America/Montevideo\",\n        \"Ecuador\": \"America/Guayaquil\",\n        \"Bolivia\": \"America/La_Paz\",\n        \"Paraguay\": \"America/Asuncion\",\n        \"Venezuela\": \"America/Caracas\"\n    },\n    \"Europe\": {\n        \"United Kingdom\": \"Europe/London\",\n        \"France\": \"Europe/Paris\",\n        \"Germany\": \"Europe/Berlin\",\n        \"Italy\": \"Europe/Rome\",\n        \"Spain\": \"Europe/Madrid\",\n        \"Russia\": \"Europe/Moscow\",\n        \"Turkey\": \"Europe/Istanbul\",\n        \"Greece\": \"Europe/Athens\",\n        \"Poland\": \"Europe/Warsaw\",\n        \"Ukraine\": \"Europe/Kiev\"\n    },\n    \"Asia\": {\n        \"India\": \"Asia/Kolkata\",\n        \"Japan\": \"Asia/Tokyo\",\n        \"China\": \"Asia/Shanghai\",\n        \"Saudi Arabia\": \"Asia/Riyadh\",\n        \"South Korea\": \"Asia/Seoul\",\n        \"Indonesia\": \"Asia/Jakarta\",\n        \"Malaysia\": \"Asia/Kuala_Lumpur\",\n        \"Vietnam\": \"Asia/Ho_Chi_Minh\",\n        \"Philippines\": \"Asia/Manila\",\n        \"Thailand\": \"Asia/Bangkok\"\n    },\n    \"Oceania\": {\n        \"Australia\": \"Australia/Sydney\",\n        \"New Zealand\": \"Pacific/Auckland\",\n        \"Fiji\": \"Pacific/Fiji\",\n        \"Papua New Guinea\": \"Pacific/Port_Moresby\",\n        \"Samoa\": \"Pacific/Apia\",\n        \"Tonga\": \"Pacific/Tongatapu\",\n        \"Solomon Islands\": \"Pacific/Guadalcanal\",\n        \"Vanuatu\": \"Pacific/Efate\",\n        \"Kiribati\": \"Pacific/Tarawa\",\n        \"New Caledonia\": \"Pacific/Noumea\"\n    }\n}\n\n# Create a list of continents\ncontinents = [\"North America\", \"South America\", \"Europe\", \"Asia\", \"Oceania\"]\n\n3. Configure the Streamlit page, header, and dropdown menu for continent and country selection\n# Streamlit app page setup\nst.set_page_config(\n    page_title='Time Zone Coverter', \n    page_icon='üåé',\n    layout='centered',\n    initial_sidebar_state='expanded',\n    menu_items={\n        'About': \"\"\"This app is intended to select a country, get its \n        time zone in UTC format  and have its correspondent result \n        from a user-entered PST time.\"\"\"\n    }  \n)\n\n# Main header\nst.header('Time Zone Coverter Streamlit app')\n\n# Add some blank space\nst.markdown(\"##\")\n\n# Create a dropdown to select a continent\ncontinent = st.sidebar.selectbox(\"1. Select a continent\", continents)\n\n# Create a dropdown to select a country within the selected continent\ncountries = list(timezone_dict[continent].keys())\ncountry = st.sidebar.selectbox(\"2. Select a country\", countries)\n\n4. Get the corresponding UTC+x time zone for the user selection\n# Display the selected UTC offset\nst.markdown(\"### :earth_americas: Corresponding UTC time:\")\ntimezone = timezone_dict[continent][country]\nutc_offset = datetime.now(pytz.timezone(timezone)).strftime('%z')\nst.markdown(f\"> **{country}** time zone is **UTC{utc_offset[:-2]}:{utc_offset[-2:]}**\")\n\n5. Display the resulting time‚Äîthe informed PST time converted to UTC+x\n# Add some blank space\nst.markdown(\"##\")\n\n# Create input for PST time\nst.markdown(\"### :clock10: PST time to UTC converter:\")\npst_input = st.text_input(\"Enter PST time (e.g., 10:00 AM PST)\")\n\n# Convert PST time to UTC+X (where X is the offset)\ntry:\n    pst_time = datetime.strptime(pst_input, \"%I:%M %p PST\")\n    pst_time = pytz.timezone(\"US/Pacific\").localize(pst_time, is_dst=None)\n    target_time = pst_time.astimezone(pytz.timezone(timezone)).strftime(\"%I:%M %p %Z\")\n    st.markdown(f\"> The corresponding time in **{country}** is **{target_time}**\")\nexcept:\n    st.markdown(\"\"\"\n    :lock: Invalid input format. Please enter PST time in format \n    '<span style=\"color:#7ef471\"><b> 10:00 AM PST </b></span>'\n    \"\"\", unsafe_allow_html=True)\n\n6. Apply a custom dark theme by creating a .streamlit folder with this config.toml file\n[theme]\nbase=\"dark\"\nprimaryColor=\"#54f142\"\nbackgroundColor=\"#222831\"\nsecondaryBackgroundColor=\"#393e46\"\nfont=\"serif\"\n\n\nIf you want to improve your Time Zone Converter, here are some suggestions:\n\nReplace the set of time zones for countries/continents with an API. Some alternatives are TimezoneDB and GeoNames, to name a few. This would provide more options for countries and even work with more cities. For example, Brazil has four different time zones.\nImplement more time zone formats, such as GMT, EST, CET, and so on.\n\nFeel free to use your creativity. üòÑ\n\nWrapping up\n\nThank you for reading my post! Now you know how to create a simple yet useful Time Zone Converter app. It can determine a time zone (in UTC+x format) for a user-selected country and provide a rough estimate for a \"PST to UTC+X\" time conversion. If you have any questions, please post them in the comments below or contact me via GitHub, LinkedIn, or Medium.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "kmeans-clustering.png (2000√ó601)",
    "url": "https://blog.streamlit.io/content/images/2023/07/kmeans-clustering.png#border",
    "html": ""
  },
  {
    "title": "Untitled-1.png (1401√ó1171)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Untitled-1.png#browser",
    "html": ""
  },
  {
    "title": "city-details.png (2000√ó709)",
    "url": "https://blog.streamlit.io/content/images/2023/07/city-details.png#border",
    "html": ""
  },
  {
    "title": "levels.gif (822√ó639)",
    "url": "https://blog.streamlit.io/content/images/2023/05/levels.gif#browser",
    "html": ""
  },
  {
    "title": "wordler_submit.gif (600√ó689)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wordler_submit.gif#browser",
    "html": ""
  },
  {
    "title": "hexagon-sandbox.jpeg (1024√ó676)",
    "url": "https://blog.streamlit.io/content/images/2023/07/hexagon-sandbox.jpeg",
    "html": ""
  },
  {
    "title": "fire-service.png (1782√ó1155)",
    "url": "https://blog.streamlit.io/content/images/2023/07/fire-service.png#browser",
    "html": ""
  },
  {
    "title": "Untitled.png (884√ó425)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Untitled.png#border",
    "html": ""
  },
  {
    "title": "wordler_steps.gif (600√ó649)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wordler_steps.gif#browser",
    "html": ""
  },
  {
    "title": "sharing.png (716√ó912)",
    "url": "https://blog.streamlit.io/content/images/2023/07/sharing.png#border",
    "html": ""
  },
  {
    "title": "vscode.png (605√ó467)",
    "url": "https://blog.streamlit.io/content/images/2023/07/vscode.png#border",
    "html": ""
  },
  {
    "title": "WORDLE_ARISE_histogram.png (1200√ó900)",
    "url": "https://blog.streamlit.io/content/images/2023/05/WORDLE_ARISE_histogram.png",
    "html": ""
  },
  {
    "title": "accident-data.png (2000√ó1254)",
    "url": "https://blog.streamlit.io/content/images/2023/07/accident-data.png#browser",
    "html": ""
  },
  {
    "title": "most_likely_letters.png (1858√ó692)",
    "url": "https://blog.streamlit.io/content/images/2023/05/most_likely_letters.png",
    "html": ""
  },
  {
    "title": "The ultimate athlete management dashboard for biomechanics",
    "url": "https://blog.streamlit.io/the-ultimate-athlete-management-dashboard-for-biomechanics/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe ultimate athlete management dashboard for biomechanics\n\nLearn how to measure jump impulse, max force, and asymmetry with Python and Streamlit\n\nBy Hansen Lu\nPosted in Advocate Posts, April 27 2023\nStep 1. Importing the necessary Python modules\nStep 2. Establishing the initial UX layout\nStep 3. Reading and displaying the forceplate data\nStep 4. Finding the points of interest with while loops\nStep 5. Getting the net impulse, push-off impulse, and absorption impulse\nStep 6. Saving into a dataframe\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Hansen Lu, and I'm a data scientist specializing in biomechanics. Python and Streamlit help me analyze the body's performance and assess potential injury risks.\n\nI built the Drop Jump app to measure an athlete's ability to adapt to the jump load, the force they generate to get off the ground, the asymmetry of their legs, and the impact of their landing. It captures and analyzes the raw motion and forceplate data so that I can share my insights with other practitioners!\n\nIn this post, you'll learn how to build it in six steps:\n\nStep 1. Importing the necessary Python modules\nStep 2. Establishing the initial UX layout\nStep 3. Reading and displaying the forceplate data\nStep 4: Finding the points of interest with while loops\nStep 5: Getting the net impulse, push-off impulse, and absorption impulse\nStep 6: Saving into a dataframe\nüëâ\nIf you want to jump right in (no pun intended), here is the app, and here is the repo code.\n\nLet's get right to it!\n\nStep 1. Importing the necessary Python modules\n\nYou'll need the following modules:\n\nStreamlit (to make a dashboard)\nPandas (to import, store, and save data as dataframes)\nScipy (to integrate)\nPlotly‚Äîgraph objects (to create visualizations)\n\nJust type in this code:\n\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\nimport datetime\nimport plotly.graph_objects as go\nfrom scipy import integrate\nStep 2. Establishing the initial UX layout\n\nTo build your dashboard, fill out the two required fields:\n\nBody Weight. Enter it in lbs (it'll convert it to kgs) or in kgs.\nInput Zero Velocity Time. Pair this with video footage synchronized with the force-time data in a lab environment.\n\nDon't have synchronized footage? Use your smartphone's slow-motion feature to get zero velocity time. This will help you identify the touchdown time, zero velocity-time, and takeoff time. Simply get the time interval of the video and use it in your force-time graph.\n\nThe zero velocity time helps distinguish between concentric propulsion and eccentric deceleration of the jump, making for better analysis.\n\nst.title(\"Drop Jump\")\nname = st.text_input(\"Athlete Name\")\ncol1, col2 = st.columns([1, 1])\nbwkgs = col1.number_input(\"Body Weight in lbs\")\nbwkgs = bwkgs / 2.205\nzeroVelocityTime = int(col2.number_input(\"Input Zero Velocity Time (ms)\"))\nleftdjfp1 = col1.file_uploader(\"Upload Left Forceplate\", type=[\"txt\"], key=88)\nrightdjfp1 = col2.file_uploader(\"Upload Right Forceplate\", type=[\"txt\"], key=89)\ngraph = go.Figure()\n\nif rightdjfp1 is None:\n    st.warning(\"No Right Forceplate Data\")\nif leftdjfp1 is None:\n    st.warning(\"No Left Forceplate Data\")\nif bwkgs == 0:\n    st.warning(\"No Bodyweight\")\n\nStreamlit provides built-in tools such as columns, a file uploader, and number input.\n\nYou can separate your forceplate data by the left and right sides. Note that we shortened \"Streamlit\" to \"st\" when declaring our modules in the previous step. Fortunately, Streamlit has a cheat sheet I always keep open to remind me how to program certain tools.\n\nPrompt the user when the file uploader and body weight fields are empty. These three fields are essential for the analysis.\n\nStep 3. Reading and displaying the forceplate data\n\nDepending on your data capture/acquisition platform, you want to export your force-time data in a .txt or .csv file so your app can read it. This is where Pandas become very useful:\n\nif leftdjfp1 is not None:\n    dfldj1 = pd.read_csv(leftdjfp1, header=(0), sep=\"\\t\")\n    graph.add_trace(\n        go.Scatter(x=dfldj1[\"Time\"], y=dfldj1[\"Fz\"], line=dict(color=\"red\"))\n    )\n\nIf your leftdjfp1 (left drop jump force plate 1) is not empty, read the file with the 0th row as your header. The file is separated by tabs. Your force-time file may have different header names and be separated by spaces, commas, or something else.\n\nTo display the force-time data you just uploaded as a graph, use the Plotly module. Define your x values as your time, calling your force-time data (defined as dfldj1) and values under the header \"Time\" as your x value.\n\nSimilarly, define your y values as your vertical force values using your header \"Fz\".\n\nFinally, define the color of your left line to red and the right line to green. If data from both force plates are inputted, it will display your force-time graph.\n\nif leftdjfp1 is not None:\n    dfldj1 = pd.read_csv(leftdjfp1, header=(0), sep=\"\\t\")\n    graph.add_trace(\n        go.Scatter(x=dfldj1[\"Time\"], y=dfldj1[\"Fz\"], line=dict(color=\"red\"))\n    )\nif rightdjfp1 is not None:\n    dfrdj1 = pd.read_csv(rightdjfp1, header=(0), sep=\"\\t\")\n    graph.add_trace(\n        go.Scatter(x=dfrdj1[\"Time\"], y=dfrdj1[\"Fz\"], line=dict(color=\"green\"))\n    )\n\nStep 4. Finding the points of interest with while loops\n\nTo calculate the net impulse of a jump, you must determine the starting and ending points of each jump.\n\nBefore the touchdown of the jump, the vertical force 'Fz' has a value of zero. Use while loops to find the right and left leg's touchdown point. This while loop continues stepping until the condition of 'Fz' falls below 10N of force, at which point you can store that value as the touchdown point. You can continue stepping through until the value is above 10N when the athlete is on the forceplate until they jump off.\n\nFinally, you can store the takeoff point of their right and left legs:\n\nif rightdjfp1 is not None and leftdjfp1 is not None:\n    tab1, tab2, tab3 = st.tabs([\"Force-Time Graph\", \"Impulse Chart\", \"Metrics\"])\n    with tab1:\n        st.plotly_chart(graph)\n    i = 0\n    while dfldj1[\"Fz\"][i] < 10:\n        i += 1\n    j = 0\n    while dfrdj1[\"Fz\"][j] < 10:\n        j += 1\n    lefttouchdown = i\n    righttouchdown = j\n\n    while dfldj1[\"Fz\"][i] > 10:\n        i += 1\n    while dfrdj1[\"Fz\"][j] > 10:\n        j += 1\n    lefttakeoff = i\n    righttakeoff = j\nStep 5. Getting the net impulse, push-off impulse, and absorption impulse\n\nThe net impulse is the total impulse minus the body weight impulse. Assuming that the athlete equally distributes their weight through both legs, you can subtract half of their body weight in Newtons from one side's total 'Fz'.\n\nTo determine the push-impulse and absorption impulse, you need to know the exact time of zero velocity. If this information is available, it can help identify areas of weakness or asymmetry in the athlete.\n\nFor example, many athletes recovering from an ACL injury may have poor force absorption but strong force generation. This can increase the risk of re-injury, especially in an in-game scenario, as they may accelerate beyond their capacity to slow down.\n\ndata = np.array([[name, bwkgs, zeroVelocityTime, netImpulseL, netImpulseR]])\n\ndf = pd.DataFrame(\n   \tdata,\n    columns=[\n        \"Name\",\n        \"Weight-kg\",\n        \"Zero Velocity Time\",\n        \"Net Impulse-L\",\n        \"Net Impulse-R\",\n     ],\n)\n\nwith tab3:\n        st.dataframe(df)if bwkgs != 0:\n        netImpulseRInterval = dfrdj1[\"Fz\"][righttouchdown:righttakeoff] - (\n            bwkgs * 9.81 / 2\n        )\n        netImpulseTimeR = dfrdj1[\"Time\"][righttouchdown:righttakeoff]\n        netImpulseLInterval = dfldj1[\"Fz\"][lefttouchdown:lefttakeoff] - (\n            bwkgs * 9.81 / 2\n        )\n        netImpulseTimeLInterval = dfldj1[\"Time\"][lefttouchdown:lefttakeoff]\n\n        netImpulseR = integrate.simps(netImpulseRInterval, netImpulseTimeR)\n        netImpulseL = integrate.simps(netImpulseLInterval, netImpulseTimeLInterval)\n        if zeroVelocityTime != 0:\n            concentricImpulseRInterval = dfrdj1[\"Fz\"][zeroVelocityTime:righttakeoff]\n            concentricImpulseTimeR = dfrdj1[\"Time\"][zeroVelocityTime:righttakeoff]\n            concentricImpulseLInterval = dfldj1[\"Fz\"][zeroVelocityTime:lefttakeoff]\n            concentricImpulseTimeLInterval = dfldj1[\"Time\"][\n                zeroVelocityTime:lefttakeoff\n            ]\n\n            concentricImpulseR = integrate.simps(\n                concentricImpulseRInterval, concentricImpulseTimeR\n            )\n            concentricImpulseL = integrate.simps(\n                concentricImpulseLInterval, concentricImpulseTimeLInterval\n            )\n\n            eccentricImpulseRInterval = dfrdj1[\"Fz\"][righttouchdown:zeroVelocityTime]\n            eccentricImpulseTimeR = dfrdj1[\"Time\"][righttouchdown:zeroVelocityTime]\n            eccentricImpulseLInterval = dfldj1[\"Fz\"][lefttouchdown:zeroVelocityTime]\n            eccentricImpulseTimeLInterval = dfldj1[\"Time\"][\n                lefttouchdown:zeroVelocityTime\n            ]\n\n            eccentricImpulseR = integrate.simps(\n                eccentricImpulseRInterval, eccentricImpulseTimeR\n            )\n            eccentricImpulseL = integrate.simps(\n                eccentricImpulseLInterval, eccentricImpulseTimeLInterval\n            )\n    impulsestyle = [\"Net Impulse\", \"Absorption Impulse\", \"Push-off Impulse\"]\n    yLeft = [netImpulseL, eccentricImpulseL, concentricImpulseL]\n    yRight = [netImpulseR, eccentricImpulseR, concentricImpulseR]\n    totaly = np.array(yLeft) + np.array(yRight)\n    leftPercentage = np.round(yLeft / totaly * 100, decimals=1)\n    rightPercentage = np.round(yRight / totaly * 100, decimals=1)\n\n    chart = go.Figure(\n        data=[\n            go.Bar(name=\"Left\", x=impulsestyle, y=yLeft, text=(leftPercentage)),\n            go.Bar(name=\"Right\", x=impulsestyle, y=yRight, text=(rightPercentage)),\n        ]\n    )\n\n    # Change the bar mode\n    chart.update_layout(barmode=\"group\")\n    with tab2:\n        st.plotly_chart(chart)\nStep 6. Saving into a dataframe\n\nFormatting all the data into a dataframe is convenient for easy exporting and manipulation. You can display the dataframe and view all its values using \"st.dataframe()\". Additionally, you can save the dataframe as a .csv or a .txt file:\n\n\tdata = np.array([[name, bwkgs, zeroVelocityTime, netImpulseL, netImpulseR]])\n\n    df = pd.DataFrame(\n        data,\n        columns=[\n            \"Name\",\n            \"Weight-kg\",\n            \"Zero Velocity Time\",\n            \"Net Impulse-L\",\n            \"Net Impulse-R\",\n        ],\n    )\n\n    with tab3:\n        st.dataframe(df)\nWrapping up\n\nIf you're a sports scientist or aspiring biomechanist, I hope this tutorial can help you with drop-jump analysis. Coding might seem intimidating if you're new, especially without a computer science background. However, it can open limitless possibilities to help you and your athletes!\n\nIf you have any questions, please post them in the comments below or contact me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "spacial-join.png (1977√ó525)",
    "url": "https://blog.streamlit.io/content/images/2023/07/spacial-join.png#border",
    "html": ""
  },
  {
    "title": "geometry.png (307√ó352)",
    "url": "https://blog.streamlit.io/content/images/2023/07/geometry.png#border",
    "html": ""
  },
  {
    "title": "lateral-flatten.png (742√ó823)",
    "url": "https://blog.streamlit.io/content/images/2023/07/lateral-flatten.png#border",
    "html": ""
  },
  {
    "title": "parsing-json.png (661√ó569)",
    "url": "https://blog.streamlit.io/content/images/2023/07/parsing-json.png#border",
    "html": ""
  },
  {
    "title": "data-in-tables.png (1711√ó1082)",
    "url": "https://blog.streamlit.io/content/images/2023/07/data-in-tables.png#browser",
    "html": ""
  },
  {
    "title": "terminal.png (682√ó193)",
    "url": "https://blog.streamlit.io/content/images/2023/07/terminal.png#border",
    "html": ""
  },
  {
    "title": "snow-sql.png (712√ó123)",
    "url": "https://blog.streamlit.io/content/images/2023/07/snow-sql.png#border",
    "html": ""
  },
  {
    "title": "pictures-in-tool-tips.png (212√ó144)",
    "url": "https://blog.streamlit.io/content/images/2023/07/pictures-in-tool-tips.png",
    "html": ""
  },
  {
    "title": "snowflake-table.png (2048√ó942)",
    "url": "https://blog.streamlit.io/content/images/2023/07/snowflake-table.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--2-.png (2000√ó1109)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Untitled--2-.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (2065√ó525)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Untitled--1-.png#border",
    "html": ""
  },
  {
    "title": "folium-map.png (1688√ó1168)",
    "url": "https://blog.streamlit.io/content/images/2023/07/folium-map.png#browser",
    "html": ""
  },
  {
    "title": "BR-residential-properties-appreciation-demo.gif (960√ó540)",
    "url": "https://blog.streamlit.io/content/images/2023/05/BR-residential-properties-appreciation-demo.gif#browser",
    "html": ""
  },
  {
    "title": "wiki_word_count_constraint_PRESS_vec.png (1861√ó686)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wiki_word_count_constraint_PRESS_vec.png",
    "html": ""
  },
  {
    "title": "checkpoint.gif (860√ó638)",
    "url": "https://blog.streamlit.io/content/images/2023/05/checkpoint.gif#browser",
    "html": ""
  },
  {
    "title": "supporting-documents.png (974√ó63)",
    "url": "https://blog.streamlit.io/content/images/2023/07/supporting-documents.png",
    "html": ""
  },
  {
    "title": "wiki_word_count_constraint_GUESS_vec.png (1861√ó686)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wiki_word_count_constraint_GUESS_vec.png",
    "html": ""
  },
  {
    "title": "data.png (959√ó128)",
    "url": "https://blog.streamlit.io/content/images/2023/07/data.png",
    "html": ""
  },
  {
    "title": "geojson-files.png (2000√ó1030)",
    "url": "https://blog.streamlit.io/content/images/2023/07/geojson-files.png#browser",
    "html": ""
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nCreate a search engine with Streamlit and Google Sheets\n\nYou‚Äôre sitting on a goldmine of knowledge!\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nMarch 14 2023\n10 most common explanations on the Streamlit forum\n\nA guide for Streamlit beginners\n\nAdvocate Posts\nby\nDebbie Matthews\n,\nMarch 9 2023\nBuilding a PivotTable report with Streamlit and AG Grid\n\nHow to build a PivotTable app in 4 simple steps\n\nAdvocate Posts\nby\nPablo Fonseca\n,\nMarch 7 2023\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nLLMs\nby\nArvindra Sehmi\n,\nFebruary 9 2023\nStreamlit-Authenticator, Part 2: Adding advanced features to your authentication component\n\nHow to add advanced functionality to your Streamlit app‚Äôs authentication component\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nFebruary 7 2023\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nAdvocate Posts\nby\nSven Koerner\n,\nFebruary 2 2023\nCreate a color palette from any image\n\nLearn how to come up with the perfect colors for your data visualization\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nJanuary 19 2023\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nHow to quickly deploy and share your machine learning model for drug discovery\n\nShare your ML model in 3 simple steps\n\nAdvocate Posts\nby\nSebastian Ayala Ruano\n,\nDecember 15 2022\nFind the top songs from your high school years with a Streamlit app\n\nUse the Spotify API to generate 1,000+ playlists!\n\nAdvocate Posts\nby\nRobert Ritz\n,\nDecember 8 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Hansen Lu - Streamlit",
    "url": "https://blog.streamlit.io/author/hansen/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Hansen Lu\n1 post\nThe ultimate athlete management dashboard for biomechanics\n\nLearn how to measure jump impulse, max force, and asymmetry with Python and Streamlit\n\nAdvocate Posts\nby\nHansen Lu\n,\nApril 27 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "chat-like-interface.png (1998√ó1298)",
    "url": "https://blog.streamlit.io/content/images/2023/07/chat-like-interface.png#browser",
    "html": ""
  },
  {
    "title": "wiki_word_count_wordle_150_sample.png (1861√ó662)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_150_sample.png",
    "html": ""
  },
  {
    "title": "supabase-table.png (1509√ó226)",
    "url": "https://blog.streamlit.io/content/images/2023/07/supabase-table.png#browser",
    "html": ""
  },
  {
    "title": "snowchat-architecture.png (2253√ó1139)",
    "url": "https://blog.streamlit.io/content/images/2023/07/snowchat-architecture.png#border",
    "html": ""
  },
  {
    "title": "config-file.png (1206√ó654)",
    "url": "https://blog.streamlit.io/content/images/2023/04/config-file.png#border",
    "html": ""
  },
  {
    "title": "streamlit-theme-editor.gif (2120√ó1640)",
    "url": "https://blog.streamlit.io/content/images/2023/05/streamlit-theme-editor.gif#browser",
    "html": ""
  },
  {
    "title": "WCAG-contrast-table.png (1126√ó664)",
    "url": "https://blog.streamlit.io/content/images/2023/04/WCAG-contrast-table.png#border",
    "html": ""
  },
  {
    "title": "wiki_word_count_wordle_150.png (1860√ó662)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_150.png",
    "html": ""
  },
  {
    "title": "color-picker-and-slider.png (1158√ó480)",
    "url": "https://blog.streamlit.io/content/images/2023/04/color-picker-and-slider.png#border",
    "html": ""
  },
  {
    "title": "5.png (1495√ó1381)",
    "url": "https://blog.streamlit.io/content/images/2023/05/5.png#border",
    "html": ""
  },
  {
    "title": "game-module.gif (978√ó637)",
    "url": "https://blog.streamlit.io/content/images/2023/05/game-module.gif#browser",
    "html": ""
  },
  {
    "title": "4.png (1444√ó1278)",
    "url": "https://blog.streamlit.io/content/images/2023/05/4.png#border",
    "html": ""
  },
  {
    "title": "wiki_word_count_wordle_15.png (1869√ó686)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wiki_word_count_wordle_15.png",
    "html": ""
  },
  {
    "title": "3.png (1470√ó861)",
    "url": "https://blog.streamlit.io/content/images/2023/05/3.png#border",
    "html": ""
  },
  {
    "title": "2.png (1462√ó857)",
    "url": "https://blog.streamlit.io/content/images/2023/05/2.png#border",
    "html": ""
  },
  {
    "title": "1-1.png (1698√ó887)",
    "url": "https://blog.streamlit.io/content/images/2023/05/1-1.png#border",
    "html": ""
  },
  {
    "title": "when-in-doubt-1.png (1740√ó985)",
    "url": "https://blog.streamlit.io/content/images/2023/05/when-in-doubt-1.png",
    "html": ""
  },
  {
    "title": "race-in-progress.gif (1082√ó710)",
    "url": "https://blog.streamlit.io/content/images/2023/06/race-in-progress.gif#border",
    "html": ""
  },
  {
    "title": "Streamlit wizard and custom animated spinner",
    "url": "https://blog.streamlit.io/streamlit-wizard-form-with-custom-animated-spinner/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit wizard and custom animated spinner\n\nImprove user experience with simplified data entry and step-by-step guidance\n\nBy Andrew Carson\nPosted in Snowflake powered ‚ùÑÔ∏è, May 15 2023\nSession state variables and callback functions\nView rendering\nCustomize the steps\nCustom animated spinner (optional)\nRadio button alternative\nConclusion\nMy favorite cloud technology resources\nSnowflake\nStreamlit\nAWS\nContents\nShare this post\n‚Üê All posts\n\nOver the past few months, I‚Äôve had the pleasure of working with Streamlit on a variety of projects. For those new to it, Streamlit is an open-source Python framework for building web applications, specifically around visualization and data science.\n\nA recent request - and overall very common use case - was unrelated to data visualization, but automation workflows requiring lengthy and complex user input. To simplify the data entry, I implemented a custom wizard form. Wizards are a great way to break down a complicated process into smaller, manageable steps, providing users with a clear path to follow. While Streamlit doesn't natively offer a wizard component, I easily recreated the functionality using a variety of widgets and session state variables.\n\nIn this post, we‚Äôll walk through an example that simulates loading a file into Snowflake. I‚Äôll be using reduced code snippets from the public GitHub repository located here.\n\nIn this post, you will:\n\nBuild a multi-step wizard form using various components, including a custom spinner (optional).\nLearn how to manage an application‚Äôs views and control flow with session state.\nGain exposure to some fantastic open-source libraries being contributed by the Streamlit developer community.\nüèÇ\nNOTE: I‚Äôll be using Streamlit Lottie for the optional animated spinner, which requires creating a Lottie File account. If skipped, check out the documentation and browse the Lottie File library for future use.\n\nLet's get started!\n\nSession state variables and callback functions\n\nCreate two session state variables and two callback functions that will work together to render views and steps to the user. Session state is Streamlit's way of preserving values across script re-runs, while callback functions are used to manage and update those values (read more on state management here).\n\nFor now, let's define them as follows:\n\nif 'current_step' not in st.session_state:\n    st.session_state['current_step'] = 1\n\nif 'current_view' not in st.session_state:\n    st.session_state['current_view'] = 'Grid'\n\n### maintains the user's location within the wizard\ndef set_form_step(action,step=None):\n    if action == 'Next':\n        st.session_state['current_step'] = st.session_state['current_step'] + 1\n    if action == 'Back':\n        st.session_state['current_step'] = st.session_state['current_step'] - 1\n    if action == 'Jump':\n        st.session_state['current_step'] = step\n\n### used to toggle back and forth between Grid View and Form View\ndef set_page_view(target_view):\n    st.session_state['current_view'] = target_view\n\nView rendering\n\nTo keep the code modular and render the views, you'll create two simple functions. The render_grid_view function uses the AgGrid custom component. If you're not familiar with AgGrid, take a look at Pablo Fonseca's example page. It's an excellent component for DataFrame visualization ‚Äî and just one of many fantastic open-source libraries built by the Streamlit developer community.\n\nThe render_wizard_view function uses Streamlit buttons to control movement between steps. To determine whether the buttons should be displayed as primary or secondary, we'll add some ternary logic.\n\ndef render_grid_view():\n    data = {\"Table Name\": [\"Product\", \"Employee\", \"Customer\"], \"Schema\": [\"Salesforce\", \"Salesforce\", \"Salesforce\"], \"Rows\": [200, 300, 400], \"Size\": [\"10 kb\", \"10 kb\", \"10 kb\"]}\n    df = pd.DataFrame(data=data)\n\n    gridOptions = {\n\t\t  \"rowSelection\": \"single\",        \n\t\t        \"columnDefs\": [\n\t\t         { \"field\": \"Table Name\", \"checkboxSelection\": True },\n\t\t            { \"field\": \"Schema\" },\n\t\t            { \"field\": \"Rows\" },\n\t\t            { \"field\": \"Size\" }\n\t\t     ]\n\t\t }    \n\n    return AgGrid(\n        df,        \n        gridOptions=gridOptions,\n        theme=\"balham\"\n    )\n    \ndef render_wizard_view(): \n    with st.expander('',expanded=True):     \n        sf_header_cols = st.columns([1, 1.75, 1])\n        \n        with sf_header_cols[1]:            \n            st.subheader('Load Data to Snowflake')\n    \n    # determines button color which should be red when user is on that given step\n    wh_type = 'primary' if st.session_state['current_step'] == 1 else 'secondary'\n    ff_type = 'primary' if st.session_state['current_step'] == 2 else 'secondary'\n    lo_type = 'primary' if st.session_state['current_step'] == 3 else 'secondary'\n    sf_type = 'primary' if st.session_state['current_step'] == 4 else 'secondary'\n\n    step_cols = st.columns([.5, .85, .85, .85, .85, .5])    \n    step_cols[1].button('Warehouses', on_click=set_form_step, args=['Jump', 1], type=wh_type)\n    step_cols[2].button('File Format', on_click=set_form_step, args=['Jump', 2], type=ff_type)        \n    step_cols[3].button('Load Options', on_click=set_form_step, args=['Jump', 3], type=lo_type)      \n    step_cols[4].button('Source Files', on_click=set_form_step, args=['Jump', 4], type=sf_type)                   \n        \n    st.markdown('---')\n    st.write(st.session_state['current_step'])\n    st.markdown('---')\n    disable_back_button = True if st.session_state['current_step'] == 1 else False\n    disable_next_button = True if st.session_state['current_step'] == 4 else False\n\n    form_footer_cols = st.columns([5,1,1,1.75])\n\n    form_footer_cols[0].button('Cancel', on_click=set_page_view, args=['Grid'])\n    form_footer_cols[1].button('Back', on_click=set_form_step, args=['Back'], disabled=disable_back_button)\n    form_footer_cols[2].button('Next', on_click=set_form_step, args=['Next'], disabled=disable_next_button)\n    form_footer_cols[3].button('üì§ Load Table', disabled=True)\n\n\nNow, the logic to render the view is a simple \"if-else\" statement:\n\nif st.session_state['current_view'] == 'Grid':\n\trender_grid_view():\nelse:\n\trender_wizard_view()\n\n\nIt's that easy! At this point, your app's output should look something like this:\n\nIf your output is off somewhere, please feel free to reference the Python file located here.\n\nüèÇ\nNOTE: You may need to adjust the column sizes based on your browser size.\nCustomize the steps\n\nFeel free to customize the individual steps or use the steps provided in the repository. Once implemented, your fully functional form will look something like this:\n\nCustom animated spinner (optional)\n\nLastly, you can replace the native Streamlit spinner with a custom spinner of your own. You can follow the example using a combination of a Lottie animation and Streamlit progress bar or design your own using the Lottie file library. Then, we'll update our imports and add one more function to render the spinner:\n\nfrom streamlit_lottie import st_lottie\nimport requests\n\ndef render_animation():\n    animation_response = requests.get('<https://assets1.lottiefiles.com/packages/lf20_vykpwt8b.json>')\n    animation_json = dict()\n    \n    if animation_response.status_code == 200:\n        animation_json = animation_response.json()\n    else:\n        print(\"Error in the URL\")     \n                           \n    return st_lottie(animation_json, height=200, width=300)\n\n\nHere is our spinner in action:\n\nRadio button alternative\n\nIn the example above, we used buttons to navigate through the wizard. An alternative approach is to use the radio button. It offers the same functionality with fewer lines of code since there is no need to worry about button color schemes.\n\nHere is an example within another very common use case, a checkout form:\n\nFor easy reference, here is the full code repository.\n\nConclusion\n\nTo conclude, I'd like to thank you very much for taking the time to read my first article. I intend to produce content related to all things data engineering, data science, and any other topic the data community finds relevant and helpful. If you're interested in learning more, please feel free to leave comments.\n\nMy favorite cloud technology resources\nSnowflake\nSnowflake Quickstarts\nSnowflake Labs\nSnowflake Developers Youtube Channel\nData Engineering Best Practices\nphData Blog\nData Engineering Simplified\nAnalytics Today Blog\nStreamlit\nStreamlit Custom Component Tracker\nBest of Streamlit Examples\nAWS\nAWS Be a Better Dev\nCloud with Raj\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "paris.png (3000√ó1146)",
    "url": "https://blog.streamlit.io/content/images/2023/05/paris.png",
    "html": ""
  },
  {
    "title": "eng-frekvenser.png (1728√ó838)",
    "url": "https://blog.streamlit.io/content/images/2023/05/eng-frekvenser.png",
    "html": ""
  },
  {
    "title": "International_Morse_Code-1.png (1280√ó1639)",
    "url": "https://blog.streamlit.io/content/images/2023/05/International_Morse_Code-1.png",
    "html": ""
  },
  {
    "title": "kingston-course-page.png (2163√ó1317)",
    "url": "https://blog.streamlit.io/content/images/2023/06/kingston-course-page.png#browser",
    "html": ""
  },
  {
    "title": "Sasha Mitrovich - Streamlit",
    "url": "https://blog.streamlit.io/author/sasha/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Sasha Mitrovich\n1 post\nBuild a Snowflake DATA LOADER on Streamlit in only 5¬†minutes\n\nDrag and drop your Excel data to Snowflake with a Streamlit app\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSasha Mitrovich\n,\nMay 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mathias Landh√§u√üer - Streamlit",
    "url": "https://blog.streamlit.io/author/mathias/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mathias Landh√§u√üer\n1 post\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Stripplot.png (633√ó458)",
    "url": "https://blog.streamlit.io/content/images/2023/05/Stripplot.png#border",
    "html": ""
  },
  {
    "title": "wrangled_tables.png (829√ó282)",
    "url": "https://blog.streamlit.io/content/images/2023/05/wrangled_tables.png#border",
    "html": ""
  },
  {
    "title": "snowsight.png (476√ó186)",
    "url": "https://blog.streamlit.io/content/images/2023/05/snowsight.png#border",
    "html": ""
  },
  {
    "title": "race-in-progress-leaderboard.png (899√ó728)",
    "url": "https://blog.streamlit.io/content/images/2023/06/race-in-progress-leaderboard.png#border",
    "html": ""
  },
  {
    "title": "sidebar.png (484√ó203)",
    "url": "https://blog.streamlit.io/content/images/2023/06/sidebar.png#border",
    "html": ""
  },
  {
    "title": "data-generation-process.png (1094√ó1235)",
    "url": "https://blog.streamlit.io/content/images/2023/06/data-generation-process.png#border",
    "html": ""
  },
  {
    "title": "select-a-race.png (1089√ó424)",
    "url": "https://blog.streamlit.io/content/images/2023/06/select-a-race.png#border",
    "html": ""
  },
  {
    "title": "Streamlit_Blog_Challenge.png (2000√ó818)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Streamlit_Blog_Challenge.png",
    "html": ""
  },
  {
    "title": "architecture-diagram.png (718√ó643)",
    "url": "https://blog.streamlit.io/content/images/2023/06/architecture-diagram.png",
    "html": ""
  },
  {
    "title": "slide_10-1.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_10-1.gif",
    "html": ""
  },
  {
    "title": "slide_05.png (1722√ó967)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_05.png#browser",
    "html": ""
  },
  {
    "title": "slid_08.png (1060√ó1061)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slid_08.png",
    "html": ""
  },
  {
    "title": "slide_07.png (1715√ó961)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_07.png#browser",
    "html": ""
  },
  {
    "title": "Monte Carlo simulations with Streamlit",
    "url": "https://blog.streamlit.io/monte-carlo-simulations-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonte Carlo simulations with Streamlit\n\nLearn how to predict future stock prices\n\nBy Mats Stellwall\nPosted in Snowflake powered ‚ùÑÔ∏è, June 8 2023\nPython environment\nApp structure\nsnf_functions.py functions\nMonte_Carlo_Simulations.py\n01_Snowflake_connect.py\n02_Run_Monte_Carlo_Simulations.py\nFinal app\nStep 1\nStep 2\nStep 3\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nA Monte Carlo simulation, also known as a probability simulation, is a method for predicting possible outcomes of uncertain events. It consists of input variables, output variables, and a mathematical model. The name Monte Carlo comes from the famous casino town of Monaco, as chance is a core element of the modeling approach, similar to a game of roulette.\n\nI've used Monte Carlo simulations to predict future stock prices with Snowpark for Python. Since running them was an interactive process, I decided to make a Streamlit app that lets users set parameters and generate predictions‚Äîto predict future stock prices. üé≤\n\nIn this post, I'll show you how to build it step-by-step.\n\nüé≤\nYou can find the code for the app in my GitHub repository.\nPython environment\n\nTo start, set up your Python environment (to use any Python-supported IDE for development):\n\nInstall Python 3.8\nInstall libraries streamlit, snowflake-snowpark-python, scipy, and plotly\nApp structure\n\nThis is a multipage app, so you'll use different Python files for each page.\n\nFor example, you'll use 01_Snowflake_connect.py to handle the connection to Snowflake and 02_Run_Monte_Carlo_Simulations.py to run and display the simulations. These files will be imported into the main file, Monte_Carlo_Simulations.py. You'll also create a Python file snf_functions.py to store common functions.\n\nHere is the app's structure:\n\nmcs\n |- lib\n      |- snf_functions.py\n |- pages\n      |- 01_Snowflake_connect.py\n      |- 02_Run_Monte_Carlo_Simulations\n |- Monte_Carlo_Simulations.py\n\nsnf_functions.py functions\n\nThe purpose of the snf_functions.py file is to hold common functions, such as connecting/disconnecting to Snowflake or retrieving the names of database objects.\n\nTo start, import the necessary modules and functions:\n\nimport streamlit as st\n\nfrom scipy.stats import norm\nimport numpy as np\nfrom typing import Tuple, Iterable\n\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\n\n\nNext, you'll need functions to connect to and disconnect from the Snowflake account. The st.session_state object lets you store variables that are available across multiple pages.\n\nFor example, you can set the account, user, password, and virtual warehouse values in your login page and then use them in your function:\n\ndef connect_to_snf():\n    if 'snowsession' in st.session_state:\n        return st.session_state['snowsession']\n\n    creds = {\n        'account': st.session_state['snow_account'],\n        'user': st.session_state['snow_user'],\n        'password': st.session_state['snow_password'],\n        'warehouse': st.session_state['snow_wh']\n    }\n    session = Session.builder.configs(creds).create()\n    st.session_state['snowsession'] = session\n\n    return session\n\ndef disconnect_snf():\n    if 'snowsession' in st.session_state:\n        session = st.session_state['snowsession']\n        session.close()\n        del st.session_state['snowsession']\n        del st.session_state['install_db']\n        del st.session_state['install_schema']\n        del st.session_state['install_stage']\n\n\nThe app relies on a couple of UDFs to run simulations.\n\nThere are two ways to handle this:\n\nAssume they're available\nHandle their deployment as part of the app flow\n\nLet's choose the latter and create a function that checks if the UDFs exist in Snowflake.\n\nThe @st.cache_data decorator ensures that the function runs only if any parameters have changed. Otherwise, it returns the value from the last run. This avoids unnecessary data selections and makes the app run faster:\n\n@st.cache_data()\ndef check_udfs(data_db: str, data_schema: str):\n    snf_session = st.session_state['snowsession']\n    udf_funcs = ['NORM_PPF', 'COLLECT_LIST', 'CALC_CLOSE']\n\n    n_udfs = snf_session.table(f\"{data_db}.INFORMATION_SCHEMA.FUNCTIONS\").filter(\n        (F.col(\"FUNCTION_SCHEMA\") == F.lit(data_schema)) & (F.col(\"FUNCTION_NAME\").in_(udf_funcs))).count()\n\n    if n_udfs == len(udf_funcs):\n        st.session_state['install_stage'] = ''\n        return True\n    else:\n        return False\n\n\nYou'll also need a function to deploy the UDFs, in the case where they do not exist.\n\nThis code can be improved, but for now, it'll do the job:\n\ndef deploy_udf():\n\t\t# Values are set when connecting to to Snowflake\n    snf_session = st.session_state['snowsession']\n    data_db = st.session_state['install_db']\n    data_schema = st.session_state['install_schema']\n    stage_name = st.session_state['install_stage']\n\n    stage_loc = data_db + '.' + data_schema + '.' + stage_name\n    # Check for stage and create it if it does not exists\n    snf_session.sql(f\"CREATE STAGE IF NOT EXISTS {stage_name}\").collect()\n\n    @F.udf(name=f\"{data_db}.{data_schema}.norm_ppf\", is_permanent=True, replace=True, packages=[\"scipy\"],\n           stage_location=stage_loc)\n    def norm_ppf(pd_series: T.PandasSeries[float]) -> T.PandasSeries[float]:\n        return norm.ppf(pd_series)\n\n    @F.udtf(name=f\"{data_db}.{data_schema}.collect_list\", is_permanent=True, replace=True\n        , packages=[\"typing\"], output_schema=T.StructType([T.StructField(\"list\", T.ArrayType())])\n        , stage_location=stage_loc)\n    class CollectListHandler:\n        def __init__(self) -> None:\n            self.list = []\n\n        def process(self, element: float) -> Iterable[Tuple[list]]:\n            self.list.append(element)\n            yield (self.list,)\n\n    @F.udf(name=f\"{data_db}.{data_schema}.calc_close\", is_permanent=True, replace=True\n        , packages=[\"numpy\"], stage_location=stage_loc)\n    def calc_return(last_close: float, daily_return: list) -> float:\n        pred_close = last_close * np.prod(daily_return)\n        return float(pred_close)\n\n    return True\n\n\nThe user will be able to choose in which database and schema they want to install the UDFs when connecting to their Snowflake account. Since the users will also have the possibility to choose the table to use for the simulations as well the table to save the simulations to, I need functions to get the databases, schema, tables and columns.\n\nUse st.cache_data to retrieve the names of those objects and to ensure that the SQL query is only run when something has changed:\n\n@st.cache_data()\ndef get_databases():\n    snf_session = st.session_state['snowsession']\n    lst_db = [dbRow[1] for dbRow in snf_session.sql(\"SHOW DATABASES\").collect()]\n    # Add a default None value\n    lst_db.insert(0, None)\n    return lst_db\n\n@st.cache_data()\ndef get_schemas(db: str):\n    snf_session = st.session_state['snowsession']\n    lst_schema = [schemaRow[0] for schemaRow in snf_session.sql(\n        f\"SELECT SCHEMA_NAME FROM {db}.INFORMATION_SCHEMA.SCHEMATA WHERE CATALOG_NAME = '{db.upper()}' AND SCHEMA_NAME != 'INFORMATION_SCHEMA' ORDER BY 1\").collect()]\n    lst_schema.insert(0, None)\n    return lst_schema\n\n@st.cache_data()\ndef get_tables(db: str, schema: str):\n    snf_session = st.session_state['snowsession']\n    lst_table = [tableRow[0] for tableRow in snf_session.sql(\n        f\"SELECT TABLE_NAME FROM {db}.INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = '{db.upper()}' AND TABLE_SCHEMA='{schema.upper()}' ORDER BY 1\").collect()]\n    lst_table.insert(0, None)\n    return lst_table\n\n@st.cache_data()\ndef get_columns(db: str, schema: str, table: str):\n    snf_session = st.session_state['snowsession']\n    lst_column = [columnRow[0] for columnRow in snf_session.sql(\n        f\"SELECT COLUMN_NAME, DATA_TYPE FROM {db}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_CATALOG = '{db.upper()}' AND TABLE_SCHEMA='{schema.upper()}' AND TABLE_NAME = '{table.upper()}' ORDER BY 1\").collect()]\n    return lst_column\n\nMonte_Carlo_Simulations.py\n\nThe main app file Monte_Carlo_Simulations.py is used as input when running Streamlit. The st.title object adds the app title. You can even use emoji shortcodes with it! üòÑ\n\nTo add a description, use st.write:\n\nimport streamlit as st\n\nst.title(\"Monte Carlo Simulations :spades:\")\nst.write(\n    \"\"\" \n    A demo showing how Monte Carlo simulations can be used to predict the future stock price for P&G.\n\n    A Monte Carlo simulation is a mathematical technique, which is used to estimate the possible outcomes of an uncertain event. \n    A Monte Carlo analysis consists of input variables, output variables, and a mathematical model. \n\n    This demo is using the following mathematical model:\n\n             Stock Price Today = Stock Price Yesterday * e^r\n\n    To calculate r the geometric Brownian motion (GBM) model is used.\n\n    Start by connecting to your Snowflake account, using the **Snowflake connect** link in the sidebar.\n\n    \"\"\"\n)\n\n01_Snowflake_connect.py\n\nThe 01_Snowflake_connect.py file contains the logic for connecting to Snowflake. It begins with the necessary imports and uses the helper functions created in the snf_functions.py file.\n\nimport streamlit as st\nfrom lib.snf_functions import get_databases, get_schemas, get_tables, get_columns, deploy_udf, check_udfs, connect_to_snf, disconnect_snf\n\n\nDefine a function that displays the disconnect button along with some information (to display it in multiple places):\n\ndef dispaly_disconnect():\n    st.write(\"\"\"\n    Everything is set up for running Monte Carlo simulations.\n\n    Choose **Run Monte Carlo simulations** in the sidebar to continue.\n    \"\"\")\n    with st.form('Snowflake Connection'):\n        st.form_submit_button('Disconnect', on_click=disconnect_snf)\n\n\nNext, build out the structure and logic for your page, starting with a title. This time, use the st.markdown component, which lets you format strings using markdown.\n\nCheck if there is an active connection to Snowflake by looking for the snowsession key in the st.session_state object. If there isn't one, create an input form using st.form.\n\nBy using the key parameter in the st.text_input, you can get the entered values and add them to the st.session_state object.\n\nWhen the st.form_submit_button is clicked, the connect_to_snf function is called:\n\nst.markdown(\"# ‚ùÑÔ∏è Snowflake Connection\")\nst.sidebar.markdown(\"# Snowflake Connection ‚ùÑÔ∏è\")\n# Check if there is a active connection...\nif \"snowsession\" not in st.session_state:\n    with st.form('Snowflake Credentials'):\n        st.text_input('Snowflake account', key='snow_account')\n        st.text_input('Snowflake user', key='snow_user')\n        st.text_input('Snowflake password', key='snow_password', type='password')\n        st.text_input('Snowflake warehouse', key='snow_wh')\n        st.form_submit_button('Connect', on_click=connect_to_snf)\n        st.stop()\n\n\nIf there is an active connection, the user can specify the database and schema into which they have installed or want to install the UDFs.\n\nUse empty lists to prompt the user to select the values for the database and schema (in that order):\n\nelse:\n    if st.session_state['snowsession']:\n\t\t\t\t# Assumption is that if a user already have set the install_schema state \n\t\t\t\t# then \n        if 'install_schema' not in st.session_state:\n            snf_session = st.session_state['snowsession']\n            st.write(\"\"\"\n            You are now connected to your Snowflake account!\n            \n            Select the database and schema where the UDFs for doing Monte Carlo Simulations exists in or to be installed in \n            \"\"\")\n            lst_databases = get_databases()\n            sel_db = st.selectbox(\"Database\", lst_databases)\n            if sel_db:\n                lst_schemas = get_schemas(sel_db)\n                st.session_state['install_db'] = sel_db\n                snf_session.use_database(sel_db)\n            else:\n                lst_schemas = []\n\n            sel_schema = st.selectbox(\"Schema\", options=lst_schemas)\n\n\nAfter a user selects the schema, a check is performed to determine whether the UDFs exist there.\n\nIf they don't, the user is given the option to install them in the selected schema:\n\n            if sel_schema:\n                st.session_state['install_schema'] = sel_schema\n                snf_session.use_schema(sel_schema)\n                if check_udfs(sel_db, sel_schema):\n                    dispaly_disconnect()\n                else:\n                    st.write(\"\"\"\n                    The selected database and schema is missing the UDFs needed for doing the Monte Carlo simulations.\n                    \n                    Set the stage name for the internal stage to be used for deployment, if it does not exists it will be created. \n                    \"\"\"\n                    )\n                    with st.form('Deploy UDfs'):\n                        stage_nm = st.text_input(label=\"Stage name\", value=\"MCS_STAGE\", key=\"install_stage\")\n                        st.form_submit_button('Deploy', on_click=deploy_udf)\n                        st.stop()\n\n\nIf everything is installed, display the disconnect button:\n\n        else:\n            dispaly_disconnect()\n\n02_Run_Monte_Carlo_Simulations.py\n\nThe 02_Run_Monte_Carlo_Simulations.py file contains the structure and logic for running the simulations.\n\nImport the necessary libraries and check if there is a connection to a Snowflake account:\n\nimport streamlit as st\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark import Column, Window\nfrom lib.snf_functions import get_databases, get_schemas, get_tables, get_columns, deploy_udf\nimport plotly.express as px\n\n# Get the current credentials\nif \"snowsession\" in st.session_state:\n    snf_session = st.session_state['snowsession']\nelse:\n    st.write(\"**Please log into you Snowflake account first!**\")\n    st.stop()\n\n\nIn my other post, I described the function used to run the simulations and outlined all the necessary steps:\n\ndef run_simulations(df, n_days, n_sim_runs):\n\n    def pct_change(indx_col: Column, val_col: Column):\n        return ((val_col - F.lag(val_col, 1).over(Window.orderBy(indx_col))) / F.lag(val_col, 1).over(Window.orderBy(indx_col)))\n    \n    # Calculate the log return by day\n    df_log_returns = df_closing.select(F.col(\"DATE\"), F.col(\"CLOSE\")\n                           ,F.call_function(\"LN\", (F.lit(1) + pct_change(F.col(\"DATE\"), F.col(\"CLOSE\")))).as_(\"log_return\"))\n    \n    # Get the u, var, stddev and last closing price\n    df_params = df_log_returns.select(F.mean(\"LOG_RETURN\").as_(\"u\")\n                 , F.variance(\"LOG_RETURN\").as_(\"var\")\n                 , F.stddev(\"LOG_RETURN\").as_(\"std_dev\")\n                ,F.max(F.col(\"last_close\")).as_(\"LAST_CLOSE\"))\\\\\n            .with_column(\"drift\", (F.col(\"u\")-(F.lit(0.5)*F.col(\"var\"))))\\\\\n            .select(\"std_dev\", \"drift\", \"last_close\")\n    \n    # Generates rows for the number of days and simulations by day\n    df_days = snf_session.generator(F.row_number().over(Window.order_by(F.seq4())).as_(\"day_id\") ,rowcount=n_days)\n    df_sim_runs = snf_session.generator(F.row_number().over(Window.order_by(F.seq4())).as_(\"sim_run\") ,rowcount=n_sim_runs)\n\t\n    df_daily_returns = df_days.join(df_sim_runs).join(df_params)\\\\\n        .select(\"day_id\", \"sim_run\"\n                , F.exp(F.col(\"drift\") + F.col(\"std_dev\") *  F.call_function(f\"{data_db}.{data_schema}.norm_ppf\", F.uniform(0.0,1.0,F.random()))).as_(\"daily_return\")\n               , F.lit(None).as_(\"SIM_CLOSE\"))\\\\\n        .sort(F.col(\"DAY_ID\"), F.col(\"sim_run\"))\n\t\n\t\t# Generate a day 0 row with the last closing price for each simulation run\n    last_close = df_params.select(\"LAST_CLOSE\").collect()[0][0]\n    df_day_0 = snf_session.generator(F.lit(0).as_(\"DAY_ID\"),  \n                                    F.row_number().over(Window.order_by(F.seq4())).as_(\"SIM_RUN\")\n                                    , F.lit(1.0).as_(\"DAILY_RETURN\") ,F.lit(last_close).as_(\"SIM_CLOSE\"), rowcount=n_sim_runs)\n\n    # Union the dataframes,\n    df_simulations = df_day_0.union_all(df_daily_returns)\n\n    df_simulations_calc_input = df_simulations.with_column(\"SIM_CLOSE_0\", F.first_value(F.col(\"SIM_CLOSE\")).over(Window.partition_by(\"SIM_RUN\").order_by(\"DAY_ID\") ) )\\\\\n                .with_column(\"L_DAILY_RETURN\", F.call_table_function(f\"{data_db}.{data_schema}.collect_list\", F.col(\"DAILY_RETURN\")).over(partition_by=\"SIM_RUN\", order_by=\"DAY_ID\"))\n\n    df_sim_close = df_simulations_calc_input.with_column(\"SIM_CLOSE\", \n                                                         F.call_function(f\"{data_db}.{data_schema}.calc_close\"\n                                                                    , F.col(\"SIM_CLOSE_0\"),F.col(\"L_DAILY_RETURN\")))\n    \n    # Cache the returning Snowpark Dataframe so we do not run it multiple times when visulazing etc\n    return df_sim_close.select(\"DAY_ID\", \"SIM_RUN\", \"SIM_CLOSE\").cache_result()\n\n\nIt also has a function for displaying the results of the simulations:\n\ndef display_sim_result(df):\n    pd_simulations = df.sort(\"DAY_ID\", \"SIM_RUN\").to_pandas()\n    \n    fig = px.line(pd_simulations, x=\"DAY_ID\", y=\"SIM_CLOSE\", color='SIM_RUN', render_mode='svg')\n    st.plotly_chart(fig, use_container_width=True)\n    metrics = df.select(F.round(F.mean(F.col(\"SIM_CLOSE\")), 2)\n                                       , F.round(F.percentile_cont(0.05).within_group(\"SIM_CLOSE\"), 2)\n                                       , F.round(F.percentile_cont(0.95).within_group(\"SIM_CLOSE\"), 2)).collect()\n    st.write(\"Expected price: \", metrics[0][0])\n    st.write(f\"Quantile (5%): \", metrics[0][1])\n    st.write(f\"Quantile (95%): \", metrics[0][2])\n\n\nNext, add the GUI components and logic:\n\nst.sidebar.markdown(\"# Simulation Parameters\")\nst.title(\"Monte Carlo Simulations :spades:\")\nst.write(\"Start by choosing the table and columns with the date and stock prices that is going to be used for the simulations.\")\n\n\nTo let users change the number of days and simulations per day, create a sidebar with the st.sidebar widget and two sliders‚Äîone for days and one for simulations. Whenever the sliders are adjusted, the variables n_days and n_iterations will be updated.\n\nTo track the user clicks on the \"Run Simulations\" button, use st.session_state:\n\nwith st.sidebar:\n    with st.form(key=\"simulation_param\"):\n        n_days = st.slider('Number of Days to Generate', 1, 1000, 100)\n        n_iterations = st.slider('Number of Simulations by Day', 1, 100, 20)\n        st.session_state.start_sim_clicked = st.form_submit_button(label=\"Run Simulations\")\n\n\nAdd select boxes to let users select the database, table, and columns. Use the previously defined functions to retrieve the data displayed in the widgets, and use empty lists to ensure that the user selects the value for the database, schema, table, and columns (in that order).\n\nOnce the user has selected the date and stock price columns, generate a Snowpark DataFrame and plot the data:\n\nlst_databases = get_databases()\ncol1, col2, col3, col4 = st.columns(4)\nsel_db = col1.selectbox(\"Database\", lst_databases)\nif sel_db:\n    lst_schemas = get_schemas(sel_db)\nelse:\n    lst_schemas = []\n\nsel_schema = col2.selectbox(\"Schema\", options=lst_schemas)\n\nif sel_schema:\n    lst_tables = get_tables(sel_db, sel_schema)\nelse:\n    lst_tables = []\n\nsel_table = col3.selectbox(\"Table\", lst_tables)\nif sel_table:\n    lst_columns = get_columns(sel_db, sel_schema, sel_table)\nelse:\n    lst_columns = []\n\nsel_columns = col4.multiselect(\"Columns\", lst_columns, max_selections=2)\nif len(sel_columns) == 2:\n    df_closing = snf_session.table(f\"{sel_db}.{sel_schema}.{sel_table}\").select(F.col(sel_columns[0]).as_(\"DATE\"), F.col(sel_columns[1]).as_(\"CLOSE\"))\n    st.line_chart(df_closing.to_pandas(),x=\"DATE\", y=\"CLOSE\")\n\n\nTo check if the user has run the simulations after clicking the \"Run Simulations\" button, use the display_sim_result function.\n\nAdditionally, store the Snowpark DataFrame containing the results in the session state:\n\nif st.session_state.start_sim_clicked:\n    with st.spinner('Running simulations...'):\n        df_simulations = run_simulations(df_closing, n_days, n_iterations)\n        display_sim_result(df_simulations)\n        st.session_state[\"df\"] = df_simulations\n\t\t\t\tst.session_state.start_sim_clicked = False\n\n\nTo determine whether to display the \"Save Results\" button, check if the Snowpark DataFrame containing the simulation results is present in the session state (stored in the variable \"df\").\n\nTo let the user specify the database, schema, and table name, store the result, and display the selection boxes for choosing the database and schema and text input for the table name.\n\nWhen the user clicks the button, the value of \"save\" changes to True. The user can then select the database and schema to save the result, along with the table name. The simulation result (stored in st.session_state[\"df\"]) is permanently saved to a table.\n\nThe \"display_sim_result\" function is called to ensure that the simulation results are displayed after the data has been saved:\n\nif \"df\" in st.session_state:\n    st.write(\"Choose the database and schema to save the data into:\")\n    save_db = st.selectbox(\"Database\", lst_databases, key=\"save_db\")\n    if save_db:\n        lst_schemas = get_schemas(save_db)\n        snf_session.use_database(save_db)\n    else:\n        lst_schemas = []\n    save_schema = st.selectbox(\"Schema\", options=lst_schemas, key=\"save_schema\")\n\n    if save_schema:\n        have_schema = False\n    else:\n        have_schema = True\n\n    save_tbl = st.text_input(label=\"Table name\", value=\"STOCK_PRICE_SIMULATIONS\", disabled=have_schema)\n    save = st.button(\"‚ùÑÔ∏è Save results\", key=\"save_sims\")\n    if save:\n        df_simulations = st.session_state[\"df\"]\n        display_sim_result(df_simulations)\n        with st.spinner(\"Saving data...\"):\n            df_simulations.write.mode('overwrite').save_as_table(f\"{save_db}.{save_schema}.{save_tbl}\")\n            st.success(f\"‚úÖ Successfully wrote simulations to {save_db}.{save_schema}.{save_tbl}!\")\n            st.session_state[\"saved\"] = True\n\nFinal app\n\nHere is what the final app will look like and how you'd use it step-by-step.\n\nStep 1\n\nLog in to your Snowflake account:\n\nStep 2\n\nSelect the columns and tables to base the simulations on, and set the number of days and simulations per day. Then click on \"Run Simulations\" to see a chart with the simulations, the expected price (the mean price of all simulations), and the 5% and 95% quantile values:\n\nStep 3\n\nClick on \"Save to Snowflake\" to save it to a table:\n\nAnd you're done! Congratulations! üéâ\n\nWrapping up\n\nI hope this post has inspired you to make your own prediction app. To use the code, remember to install the Streamlit library locally using pip. If you have any questions, please post them below or contact me on LinkedIn.\n\nHappy app-building and predicting! üé≤\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "slide_06.png (1579√ó432)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_06.png",
    "html": ""
  },
  {
    "title": "Build a Snowflake DATA LOADER on Streamlit in only 5¬†minutes",
    "url": "https://blog.streamlit.io/build-a-snowflake-data-loader-on-streamlit-in-only-5-minutes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild a Snowflake DATA LOADER on Streamlit in only 5¬†minutes\n\nDrag and drop your Excel data to Snowflake with a Streamlit app\n\nBy Sasha Mitrovich\nPosted in Snowflake powered ‚ùÑÔ∏è, May 9 2023\n1. Create a virtual Python environment for Streamlit\n2. Connect to Snowflake from Streamlit\n3. Create a simple drag-and-drop UI in Streamlit for CSV files\n4. Load the dropped file to Snowflake\nBonus: Add data quality checks\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nA customer recently challenged me to create a data loader app for Snowflake that even kindergarteners could use. And when I say kindergarteners, I‚Äôm talking about the business users who think SQL is a mythical creature.\n\nNow, I‚Äôm not one to back down from a challenge, especially when someone‚Äôs already promised something on my behalf. So, I decided to take on the task and create a data loader app for kindergarteners in just five minutes. And let me tell you, it was a wild ride.\n\nBy the end of this post, you‚Äôll learn how to create a simple drag-and-drop data loader app on Streamlit that anyone can use.\n\nI work as a solutions engineer at Snowflake. My passion is data, data science, and building data applications that I can showcase to my customers. Streamlit is just the tool for that.\n\nAnd it complements Snowflake‚Äôs Data Cloud platform perfectly. While database specialists working in SQL and data engineers using Python DataFrames feel at home working with Snowflake, it‚Äôs a bit different for business users. There‚Äôs no easy way for non-technical users to drop data into Snowflake and jump into their business intelligence tool of choice, such as Tableau, to analyze it and share it with others.\n\nIn this post, I‚Äôm changing all that! You‚Äôll learn how to:\n\nCreate a virtual Python environment for Streamlit\nConnect to Snowflake from Streamlit\nCreate a simple drag-and-drop UI in Streamlit for CSV files\nLoad the dropped file to Snowflake\nBonus: Add data quality checks\n\nKeep reading till the end. I‚Äôll also show how to add quality checks on the loaded data and display that in the app for the business to immediately assess the data quality with just a glance at the UI.\n\nLet‚Äôs start.\n\n1. Create a virtual Python environment for Streamlit\n\nI use conda to manage my virtual environments so I can work with correct versions of Python packages and avoid the dependency hell. If you don‚Äôt know what that is, I‚Äôve explained it in my video ‚ÄúStuck learning Python? Make it fun with Streamlit!‚Äù\n\nHere‚Äôs how to create an environment for our app with all the necessary Python packages:\n\nconda create --name snowshovel -c <https://repo.anaconda.com/pkgs/snowflake> python=3.8 pandas snowflake-snowpark-python\nconda activate snowshovel\nconda install -c conda-forge streamlit\n\n\nIf you don‚Äôt use conda, that‚Äôs fine; you can install these packages using pip, for instance.\n\n2. Connect to Snowflake from Streamlit\n\nWe‚Äôll store our credentials in the creds.json.Here‚Äôs an example that contains all the required properties for connecting to Snowflake:\n\n{\n    \"account\": \"account.region\",\n    \"user\": \"myuser\",\n    \"role\": \"myrole\",\n    \"password\": \"************\",\n    \"warehouse\": \"mywarehouser\",\n    \"database\": \"mydb\",\n    \"schema\": \"myschema\"\n  }\n\n\nMake sure to replace these placeholders with real values for your Snowflake account.\n\nWe‚Äôll use this credentials file later to create a connection to Snowflake. Snowflake supports many other authentication methods, such as key-pair or single sign-on.\n\n3. Create a simple drag-and-drop UI in Streamlit for CSV files\n\nI use VS Code as my development environment. If you want to learn why I like VS Code so much‚Ää‚Äî‚Äämake sure you watch this clip:\n\nLet‚Äôs jump into VS Code and start working on our Python code.\n\nTo start building a Streamlit app, we need to import the streamlit Python package, like this:\n\nimport streamlit as st\n\n\nNow we can run the app from the terminal like so:\n\nstreamlit run app.py\n\n\nNow that we have the app running let‚Äôs add the amazing file uploader component that does all the job for us:\n\nfile = st.file_uploader(\"Drop your CSV here to load to Snowflake\", type={\"csv\"})\n\n\nThat‚Äôs it. It literally takes two lines of code to build a Drag and Drop Web app with Streamlit. If anyone can beat this, I‚Äôm buying them a beverage of their choice (and I will ship it internationally).\n\n4. Load the dropped file to Snowflake\n\nThe return value of the file uploader component is an UploadedFile class object, a subclass of BytesIO. Therefore, it is ‚Äúfile-like.‚Äù This means you can use that anywhere a file is expected and read its meta-data to get the filename, for example.\n\nThat‚Äôs exactly what we need in the next step.\n\nWe‚Äôll load this file to a Pandas DataFrame. Why Pandas DataFrame? For two reasons:\n\nPandas has a convenient read_csv() method that will infer the CSV schema automatically, so we don‚Äôt need to build that complex logic of recognizing column types ourselves. Yay to the open-source community!\nPandas can be serialized to a Snowflake table with a single line of code using Snowflake‚Äôs Snowpark function write_pandas(). The table will be automatically created, re-using the schema from the Pandas DataFrame.\n\nLet‚Äôs add these two lines of code to complete our app:\n\nfile_df = pd.read_csv(file)\nsnowparkDf=session.write_pandas(file_df,file.name,auto_create_table = True, overwrite=True)\n\n\nNotice I‚Äôm using the session object to serialize the DataFrame to a Snowflake table, and remember, we‚Äôve prepared a JSON file with our Snowflake credentials.\n\nLet‚Äôs create that object before using it in our code. Here‚Äôs the complete application code for our simple Snowflake data loader. I‚Äôve imported all the needed Python packages and checked if a file has been dropped.\n\nimport streamlit as st\nimport pandas as pd\nimport json\nfrom snowflake.snowpark import Session\n\n# connect to Snowflake\nwith open('creds.json') as f:\n    connection_parameters = json.load(f)  \nsession = Session.builder.configs(connection_parameters).create()\n\nfile = st.file_uploader(\"Drop your CSV here to load to Snowflake\", type={\"csv\"})\nfile_df = pd.read_csv(file)\nsnowparkDf=session.write_pandas(file_df,file.name,auto_create_table = True, overwrite=True)\n\n\nThat‚Äôs it. I‚Äôve written a CSV data loader for Snowflake with less than 20 lines of Python code in under 5 minutes!\n\nBonus: Add data quality checks\n\nNow that we‚Äôve enabled business users to load their data to Snowflake without knowing anything about Snowflake, let‚Äôs provide some data quality checks.\n\nWith this, they can immediately see what they have in terms of data quality and make informed decisions on how to process this data further so they get added value from it.\n\nHere‚Äôs a Python function that does just that:\n\ndef describeSnowparkDF(snowpark_df: snowpark.DataFrame):\n    \n    st.write(\"Here's some stats about the loaded data:\")\n    numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n    numeric_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in numeric_types]\n\n    # Get categorical columns\n    categorical_types = [T.StringType]\n    categorical_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in categorical_types]\n\n    st.write(\"Relational schema:\")\n  \n    columns = [c for c in snowpark_df.schema.fields]\n    st.write(columns)\n    \n    col1, col2, = st.columns(2)\n    with col1:\n        st.write('Numeric columns:\\\\t', numeric_columns)\n\n    with col2:\n        st.write('Categorical columns:\\\\t', categorical_columns)\n    \n    # Calculte statistics for our dataset\n    st.dataframe(snowpark_df.describe().sort('SUMMARY'), use_container_width=True)\n\n\nLet‚Äôs break this down a little bit.\n\nFirst, we‚Äôll list all the column names and types for the newly created table. This is useful to check if the schema was inferred as the user expected:\n\n  st.write(\"Here's some stats about the loaded data:\")\n  numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n  numeric_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in numeric_types]\n\n  # Get categorical columns\n  categorical_types = [T.StringType]\n  categorical_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in categorical_types]\n  st.write(\"Relational schema:\")\n\n  columns = [c for c in snowpark_df.schema.fields]\n  st.write(columns)\n\n\nThen, we‚Äôll present two more lists: numeric columns and categorical columns. This is useful to understand which further data processing we might undertake, like enriching with more data from other sources or transforming the values in a way required for analysis, like machine learning, perhaps:\n\ncol1, col2, = st.columns(2)\nwith col1:\n  st.write('Numeric columns:\\\\\\\\t', numeric_columns)\nwith col2:\n  st.write('Categorical columns:\\\\\\\\t', categorical_columns)\n\n\nFinally, we‚Äôll show the column statistics for each column, including counts of null values or value ranges. Based on this, the business user can decide if they will be able to use this dataset for their intended analysis with just a glance:\n\n# Calculte statistics for our dataset\nst.dataframe(snowpark_df.describe().sort('SUMMARY'), use_container_width=True)\n\nWrapping up\n\nI built a basic data loader app for Snowflake that any business user immediately understands and can use. Building this app took me only 5 minutes and less than 20 lines of code.\n\nThen, I spent another 15 minutes providing meaningful information about the loaded data so a business user could decide on the data quality with just a glance.\n\nYou‚Äôll fund the code for this app and instructions on how to install and use it in this GitHub repo.\n\nIf you found this post useful, clap and subscribe to my Medium. And if you have any questions, please post them in the comments below or contact me on LinkedIn.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "slide_04.png (1715√ó965)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_04.png#browser",
    "html": ""
  },
  {
    "title": "slide_03.png (1721√ó963)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_03.png#browser",
    "html": ""
  },
  {
    "title": "screen_recording_scaling-0.5_fps-20_speed-9.84_duration-2-18.gif (855√ó401)",
    "url": "https://blog.streamlit.io/content/images/2023/05/screen_recording_scaling-0.5_fps-20_speed-9.84_duration-2-18.gif#browser",
    "html": ""
  },
  {
    "title": "slide_02.png (1911√ó1072)",
    "url": "https://blog.streamlit.io/content/images/2023/06/slide_02.png#browser",
    "html": ""
  },
  {
    "title": "Andrew Carson - Streamlit",
    "url": "https://blog.streamlit.io/author/andrew/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Andrew Carson\n1 post\nStreamlit wizard and custom animated spinner\n\nImprove user experience with simplified data entry and step-by-step guidance\n\nSnowflake powered ‚ùÑÔ∏è\nby\nAndrew Carson\n,\nMay 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mats Stellwall - Streamlit",
    "url": "https://blog.streamlit.io/author/mats/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mats Stellwall\n1 post\nMonte Carlo simulations with Streamlit\n\nLearn how to predict future stock prices\n\nSnowflake powered ‚ùÑÔ∏è\nby\nMats Stellwall\n,\nJune 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit",
    "url": "https://blog.streamlit.io/tag/community/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n404\nPage not found\n‚Üê Go to the front page\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "IMG_0430.png (2000√ó1499)",
    "url": "https://blog.streamlit.io/content/images/2022/08/IMG_0430.png#browser",
    "html": ""
  },
  {
    "title": "153F5C49-8CDC-4B1E-AFAE-AECC7AA4F849.jpeg (2000√ó1499)",
    "url": "https://blog.streamlit.io/content/images/2022/08/153F5C49-8CDC-4B1E-AFAE-AECC7AA4F849.jpeg#browser",
    "html": ""
  },
  {
    "title": "EE1466B0-0A0C-4BD9-8B1E-02A37A380CF4.jpeg (2000√ó1336)",
    "url": "https://blog.streamlit.io/content/images/2022/08/EE1466B0-0A0C-4BD9-8B1E-02A37A380CF4.jpeg#browser",
    "html": ""
  },
  {
    "title": "9E0C7298-B079-4074-9DBB-EE9C04D14C31.jpeg (2000√ó1177)",
    "url": "https://blog.streamlit.io/content/images/2022/08/9E0C7298-B079-4074-9DBB-EE9C04D14C31.jpeg#browser",
    "html": ""
  },
  {
    "title": "72631AEA-B0B9-412E-BA7C-06B3382855FA.jpeg (2000√ó1257)",
    "url": "https://blog.streamlit.io/content/images/2022/08/72631AEA-B0B9-412E-BA7C-06B3382855FA.jpeg#browser",
    "html": ""
  },
  {
    "title": "1825A4E6-9D5C-43DF-A4E3-DCC3FE2A5F37.jpeg (2000√ó2382)",
    "url": "https://blog.streamlit.io/content/images/2022/08/1825A4E6-9D5C-43DF-A4E3-DCC3FE2A5F37.jpeg#browser",
    "html": ""
  },
  {
    "title": "EDA_app.gif (1280√ó1008)",
    "url": "https://blog.streamlit.io/content/images/2022/01/EDA_app.gif",
    "html": ""
  },
  {
    "title": "1F88C1F7-893E-432D-9E7B-AEFD23D4D0B3.jpeg (2000√ó1748)",
    "url": "https://blog.streamlit.io/content/images/2022/08/1F88C1F7-893E-432D-9E7B-AEFD23D4D0B3.jpeg#browser",
    "html": ""
  },
  {
    "title": "7A37797E-C57A-4560-8104-790FA5537DEF.jpeg (2000√ó1270)",
    "url": "https://blog.streamlit.io/content/images/2022/08/7A37797E-C57A-4560-8104-790FA5537DEF.jpeg#browser",
    "html": ""
  },
  {
    "title": "A15327B7-F711-448A-BB97-A1CF2580BBC7.jpeg (2000√ó1090)",
    "url": "https://blog.streamlit.io/content/images/2022/08/A15327B7-F711-448A-BB97-A1CF2580BBC7.jpeg#browser",
    "html": ""
  },
  {
    "title": "16FA6767-9630-4C65-869D-77E2B2FC4199.jpeg (2000√ó1263)",
    "url": "https://blog.streamlit.io/content/images/2022/08/16FA6767-9630-4C65-869D-77E2B2FC4199.jpeg#browser",
    "html": ""
  },
  {
    "title": "F2A5148A-2F6F-48D2-B935-A38542A87468.jpeg (2000√ó1289)",
    "url": "https://blog.streamlit.io/content/images/2022/08/F2A5148A-2F6F-48D2-B935-A38542A87468.jpeg#browser",
    "html": ""
  },
  {
    "title": "6FD187E5-2A74-4205-97B9-E19890E6C741.jpeg (2000√ó1210)",
    "url": "https://blog.streamlit.io/content/images/2022/08/6FD187E5-2A74-4205-97B9-E19890E6C741.jpeg#browser",
    "html": ""
  },
  {
    "title": "2B353264-DDBA-4251-94E4-7CF77A256B9B.jpeg (2000√ó2199)",
    "url": "https://blog.streamlit.io/content/images/2022/08/2B353264-DDBA-4251-94E4-7CF77A256B9B.jpeg#browser",
    "html": ""
  },
  {
    "title": "324BEA1A-997C-49E7-A279-040300162E27.jpeg (2000√ó1526)",
    "url": "https://blog.streamlit.io/content/images/2022/08/324BEA1A-997C-49E7-A279-040300162E27.jpeg#browser",
    "html": ""
  },
  {
    "title": "E55BB258-DA30-4262-BDAA-7B2C6A0E5E15.jpeg (1800√ó1510)",
    "url": "https://blog.streamlit.io/content/images/2022/08/E55BB258-DA30-4262-BDAA-7B2C6A0E5E15.jpeg",
    "html": ""
  },
  {
    "title": "40F6254E-3523-4ADE-B9B6-D4436FE8B68A.jpeg (2000√ó1375)",
    "url": "https://blog.streamlit.io/content/images/2022/08/40F6254E-3523-4ADE-B9B6-D4436FE8B68A.jpeg#browser",
    "html": ""
  },
  {
    "title": "trubrics_ml_feedback_platform.png (1351√ó559)",
    "url": "https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_platform.png#browser",
    "html": ""
  },
  {
    "title": "Joel Hodgson - Streamlit",
    "url": "https://blog.streamlit.io/author/joel/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Joel Hodgson\n1 post\nCollecting user feedback on ML in Streamlit\n\nImprove user engagement and model quality with the new Trubrics feedback component\n\nAdvocate Posts\nby\nJeff Kayne and¬†\n1\n¬†more,\nMay 4 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "trubrics_ml_feedback_type_custom-1.png (1211√ó771)",
    "url": "https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_custom-1.png#browser",
    "html": ""
  },
  {
    "title": "trubrics_ml_feedback_type_thumbs.png (1208√ó729)",
    "url": "https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_thumbs.png#browser",
    "html": ""
  },
  {
    "title": "trubrics_ml_feedback_type_faces.png (1211√ó212)",
    "url": "https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_faces.png#browser",
    "html": ""
  },
  {
    "title": "trubrics_ml_feedback_type_issue.png (1216√ó405)",
    "url": "https://blog.streamlit.io/content/images/2023/04/trubrics_ml_feedback_type_issue.png#browser",
    "html": ""
  },
  {
    "title": "EDA-app.png (1970√ó1558)",
    "url": "https://blog.streamlit.io/content/images/2022/01/EDA-app.png",
    "html": ""
  },
  {
    "title": "model-building.jpeg (1941√ó1745)",
    "url": "https://blog.streamlit.io/content/images/2022/01/model-building.jpeg",
    "html": ""
  },
  {
    "title": "streamlit-demo-app-1.gif (897√ó616)",
    "url": "https://blog.streamlit.io/content/images/2022/11/streamlit-demo-app-1.gif#browser",
    "html": ""
  },
  {
    "title": "Streamlit",
    "url": "https://blog.streamlit.io/streamlit-quests-getting-started-with-streamlit/github.com/streamlit/app-starter-kit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n404\nPage not found\n‚Üê Go to the front page\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Make your st.pyplot interactive!",
    "url": "https://blog.streamlit.io/make-your-st-pyplot-interactive/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMake your st.pyplot interactive!\n\nLearn how to make your pyplot charts interactive in a few simple steps\n\nBy William Huang\nPosted in Tutorials, June 23 2022\n1. Simple example\nStep 1. Create a basic Matplotlib chart\nStep 2. Make the chart interactive\n2. Advanced example\nStep 1. Render the graph statically\nStep 2. Make the graph interactive with mpld3\nStep 3. Add tooltips for even more interactivity\nNote: mpld3 limitations\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nMatplotlib is one of the most popular charting libraries in Python. It‚Äôs also a popular way to add charts to your Streamlit apps. Just use st.pyplot!\n\nBut Matplotlib charts are static images. No zooming or moving the chart around.\n\nIn this post, I'll show you how to make them interactive:\n\n1. Simple example:\n\nStep 1. Create a basic Matplotlib chart\nStep 2. Make the chart interactive\n\n2. Advanced example:\n\nStep 1. Render the graph statically\nStep 2. Make the graph interactive with mpld3\nStep 3. Add tooltips for even more interactivity\n\nTLDR? Use mpld3 and render pyplots with Streamlit‚Äôs built-in st.pyplot command. With a few lines of code, you can add panning, zooming, resetting, and rendering!\n\n1. Simple example\nStep 1. Create a basic Matplotlib chart\n\nFirst, create a basic Matplotlib chart and add it to your Streamlit app (you‚Äôll add interactivity later).\n\nHere is what the code will look like:\n\nimport streamlit as st\nimport matplotlib.pyplot as plt\n\n#create your figure and get the figure object returned\nfig = plt.figure() \nplt.plot([1, 2, 3, 4, 5]) \n\nst.pyplot(fig)\n\n\nAnd here‚Äôs what your app should look like now:\n\nStep 2. Make the chart interactive\n\nMaking this chart interactive is super simple.\n\nUse the fantastic mpld3 library. It‚Äôll convert the Matplotlib figure (fig) into an interactive Javascript representation and return it as HTML. Embed this HTML snippet in your app via Streamlit‚Äôs custom component API:\n\nimport matplotlib.pyplot as plt\nimport mpld3\nimport streamlit.components.v1 as components\n\n#create your figure and get the figure object returned\nfig = plt.figure() \nplt.plot([1, 2, 3, 4, 5]) \n\nfig_html = mpld3.fig_to_html(fig)\ncomponents.html(fig_html, height=600)\n\nNow your users can pan, zoom, reset, and explore the details of your chart! üìä\n\nWant to explore it yourself? See the app deployed live here.\n\nFor more mpld3‚Äôs plugins, check out mpld3‚Äôs website and documentation.\n\n2. Advanced example\nStep 1. Render the graph statically\n\nStart out by statically rendering the graph:\n\n# Imports for all of the code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport mpld3\nimport streamlit as st\nfrom mpld3 import plugins\n\ndef f(t):\n    return np.exp(-t) * np.cos(2*np.pi*t)\n\nt1 = np.arange(0.0, 5.0, 0.1)\nt2 = np.arange(0.0, 5.0, 0.02)\n\n# How to set the graph size \ntwo_subplot_fig = plt.figure(figsize=(6,6))\nplt.subplot(211)\nplt.plot(t1, f(t1), color='tab:blue', marker=',')\nplt.plot(t2, f(t2), color='black', marker='.')\n\nplt.subplot(212)\nplt.plot(t2, np.cos(2*np.pi*t2), color='tab:orange', linestyle='--', marker='.')\n\nst.pyplot(two_subplot_fig)\n\n\nThis code will make something like this:\n\nYou might be thinking, ‚ÄúWhy are we adding markers? It doesn‚Äôt look beautiful.‚Äù I‚Äôll explain why below!\n\nStep 2. Make the graph interactive with mpld3\n\nMake the static graph interactive with mpld3:\n\n\n# Replace st.pyplot(two_subplot_fig) with this code below! \nfig_html = mpld3.fig_to_html(two_subplot_fig)\ncomponents.html(fig_html, height=600)\n\n\n\nHere is what it‚Äôll look like with panning, zooming, and resetting:\n\nStep 3. Add tooltips for even more interactivity\n\nAdd tooltips to see X and Y coordinates for even more interactivity (it‚Äôs why we‚Äôve added markers).\n\nHere is what the code will look like:\n\n# CODE TO ADD\n# Define some CSS to control our custom labels\ncss = \"\"\"\ntable\n{\n  border-collapse: collapse;\n}\nth\n{\n  color: #ffffff;\n  background-color: #000000;\n}\ntd\n{\n  background-color: #cccccc;\n}\ntable, th, td\n{\n  font-family:Arial, Helvetica, sans-serif;\n  border: 1px solid black;\n  text-align: right;\n}\n\"\"\"\nfor axes in two_subplot_fig.axes:\n    for line in axes.get_lines():\n        # get the x and y coords\n        xy_data = line.get_xydata()\n        labels = []\n        for x, y in xy_data:\n            # Create a label for each point with the x and y coords\n            html_label = f'<table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> </thead> <tbody> <tr> <th>x</th> <td>{x}</td> </tr> <tr> <th>y</th> <td>{y}</td> </tr> </tbody> </table>'\n            labels.append(html_label)\n        # Create the tooltip with the labels (x and y coords) and attach it to each line with the css specified\n        tooltip = plugins.PointHTMLTooltip(line, labels, css=css)\n        # Since this is a separate plugin, you have to connect it\n        plugins.connect(two_subplot_fig, tooltip)\n\nYou can adjust HTML, CSS, or anything you want. And if you want to interact with the graph or look at the code, check it out here!\n\nNote: mpld3 limitations\n\nBefore I wrap this up, I wanted to note the limitations to mpld3:\n\nComplex charts sometimes don‚Äôt render properly.\nDark mode isn‚Äôt supported.\n3D charts don‚Äôt render properly.\nYou need markers for tooltips.\nSome markers don‚Äôt work (examples: none or ‚Äò+‚Äô).\nWrapping up\n\nThank you for reading this post! I‚Äôd love to know if you found this useful. Your feedback means a LOT. If you have any questions, please leave them in the comments below and check out the forum to see what our vibrant community is creating.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "python-chile-app.gif (1276√ó798)",
    "url": "https://blog.streamlit.io/content/images/2023/03/python-chile-app.gif#browser",
    "html": ""
  },
  {
    "title": "python-talks-blocks.png (1920√ó987)",
    "url": "https://blog.streamlit.io/content/images/2023/03/python-talks-blocks.png#browser",
    "html": ""
  },
  {
    "title": "python-talks-search-engine.png (1920√ó483)",
    "url": "https://blog.streamlit.io/content/images/2023/03/python-talks-search-engine.png#browser",
    "html": ""
  },
  {
    "title": "python-talks-app.png (1920√ó771)",
    "url": "https://blog.streamlit.io/content/images/2023/03/python-talks-app.png#browser",
    "html": ""
  },
  {
    "title": "Using ChatGPT to build a Kedro ML pipeline",
    "url": "https://blog.streamlit.io/using-chatgpt-to-build-a-kedro-ml-pipeline/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nBy Arvindra Sehmi\nPosted in LLMs, February 9 2023\nA guided chat in 25 questions\nQuestion 1\nQuestion 2\nQuestion 3\nQuestion 4\nQuestion 5\nQuestion 6\nQuestion 7\nQuestion 8\nQuestion 9\nQuestion 10\nQuestion 11\nQuestion 12\nQuestion 13\nQuestion 14\nQuestion 15\nQuestion 16\nQuestion 17\nQuestion 18\nQuestion 19\nQuestion 20\nQuestion 21\nQuestion 22\nQuestion 23\nQuestion 24\nQuestion 25\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHi, community! üëã\n\nMy name is Arvindra Sehmi. I've been a life-long learner with too many degrees and far too many years of research and work in academia and the tech industry - 35 years and counting! I‚Äôm on a career break (advising Auth0.com, Macrometa.com, Tangle.io, Crowdsense.ai, and DNX ventures) and am taking the opportunity to learn new software development tools and enjoy being an active member of the Streamlit Creators group.\n\nI recently came across an open-source Python DevOps framework Kedro and thought, ‚ÄúWhy not have ChatGPT teach me how to use it to build some ML/DevOps automation?‚Äù The idea was to write my questions with hints that encouraged explanations of advanced Kedro features (to evolve incrementally as if a teacher taught me).\n\nI planned to ask ChatGPT how to:\n\nUse basic Kedro.\nUse more advanced features in the Kedro framework.\nDisplay pipeline graphs in Streamlit.\nBuild an example ML model and explicitly refer to it in the Kedro pipeline.\nScale the pipeline and perform pipeline logging, monitoring, and error handling.\nConnect Kedro logs to a cloud-based logging service.\nContrast Kedro with similar (competing) products and services and show me how the earlier-developed pipeline could be implemented in one of them.\n\nWhat follows are the annotated responses. I was super impressed. Will you be? (This is a long post, so if you want, dive right into the app and the repo.)\n\n‚õî\nThe generated Kedro code was simplistic and in some cases wrong, but perfectly okay to work with. My app is original, with small parts of it taken from Kedro's code template, so it can be used without any recourse under my MIT license. As to ChatGPT, though it's powerful, be aware of its potential limitations (see here and here).¬†\nA guided chat in 25 questions\n\nTry replicating my Q&A and see if you get better results, as ChatGPT should be getting smarter from millions of conversations. My opening prompt mentioned the technology I‚Äôm interested in learning about.\n\nQuestion 1\n\nCan you show me how to build a Kedro application to ingest data, train an ML model, evaluate the model, and visualize the model evaluation metrics?\n\nCertainly! Here's an example of how you can build a Kedro application to ingest data, train a machine learning model, evaluate the model, and visualize the model evaluation metrics:\n\n1. Ingest the data:\n\nFirst, you'll need to ingest the data that you want to use for training and evaluating your model. You can use the DataCatalog to load your data from a variety of sources, such as a CSV file, a database, or a cloud storage bucket.\n\nFor example, you can use the load method of the DataCatalog to load data from a CSV file and save it to a data node in your Kedro pipeline:\n\nfrom kedro.io import DataCatalog\n\n# Create a DataCatalog to manage the data for your project\ncatalog = DataCatalog()\n\n# Load the data from a CSV file and save it to a data node\ndata = catalog.load(\"my_data\", \"./data/my_data.csv\")\n\n\n2. Train the model:\n\nNext, you'll need to define a function that trains your machine learning model. This function should take the training data as input, and should return the trained model.\n\nHere's an example of how you can define a function to train a simple linear regression model:\n\nfrom sklearn.linear_model import LinearRegression\n\ndef train_model(data):\n\t# Split the data into features and targets\n\tX = data.drop(\"target\", axis=1)\n\ty = data[\"target\"]\n\t# Create a LinearRegression model and fit it to the data\n\tmodel = LinearRegression()\n\tmodel.fit(X, y)\n\treturn model\n\n\n3. Evaluate the model:\n\nAfter training the model, you'll need to evaluate its performance on a separate test dataset. You can use a variety of evaluation metrics, such as accuracy, precision, and recall, to measure the performance of your model.\n\nHere's an example of how you can define a function to evaluate a model using the mean squared error metric:\n\nfrom sklearn.metrics import mean_squared_error\n\ndef evaluate_model(model, data):\n\t# Split the data into features and targets\n\tX = data.drop(\"target\", axis=1)\n\ty = data[\"target\"]\n\t# Make predictions using the model and calculate the mean squared error\n\ty_pred = model.predict(X)\n\tmse = mean_squared_error(y, y_pred)\n\treturn mse\n\n\n4. Visualize the model evaluation metrics:\n\nFinally, you can use a library like Matplotlib to visualize the model evaluation metrics. This can help you understand how well your model is performing and identify areas for improvement.\n\nHere's an example of how you can use Matplotlib to visualize the mean squared error for a model:\n\nimport matplotlib.pyplot as plt\n\ndef plot_metric(metric_values):\n\t# Plot the metric values\n\tplt.plot(metric_values)\n\tplt.ylabel(\"Mean Squared Error\")\n\tplt.xlabel(\"Iteration\")\n\tplt.show()\n\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: That‚Äôs a great start. I know decent DevOps frameworks will provide a way to abstract and manage activities in a process. This is called orchestration, so let‚Äôs ask about that and implicitly refer to the steps described in the answer above.\n\nüêõ\nCode comment: The subsequent implementation of the above data ingest code didn't work as intended. Kedro has a concept of datasets, and you must first create datasets from source data and then add them to the Kedro DataCatalog, which acts as a central data store for your application. Visualizing evaluation metrics as they are written can also be a problem when integrating with Streamlit because it conflates visualization with processing logic. Personally, I prefer to have these two concerns of any reasonably large application as separate logical layers. In my application, I save metrics results to a file (but it could be any persistence mechanism) that is independently accessed by my Streamlit presentation layer.\nQuestion 2\n\nCan you use Kedro to orchestrate this series of steps?\n\nYes, you can use Kedro to orchestrate the series of steps for ingesting data, training a machine learning model, evaluating the model, and visualizing the model evaluation metrics.\n\nTo do this, you can define your steps as Kedro nodes in a pipeline, and use the run method of the Pipeline to execute the pipeline.\n\nHere's an example of how you can use Kedro to orchestrate the series of steps that I described in the previous message:\n\nfrom kedro.pipeline import Pipeline, node\n\n# Define the pipeline nodes\ndef train_model(data):\n\t# Train the model (as described in the previous message)\n\treturn model\n\t\ndef evaluate_model(model, data):\n\t# Evaluate the model (as described in the previous message)\n\treturn mse\n\t\ndef plot_metric(metric_values):\n\t# Plot the metric values (as described in the previous message)\n\n# Create a pipeline to orchestrate the steps\npipeline = Pipeline([\n\tnode(\n\t\ttrain_model,\n\t\tinputs=\"data\",\n\t\toutputs=\"model\",\n\t),\n\tnode(\n\t\tevaluate_model,\n\t\tinputs=[\"model\", \"data\"],\n\t\toutputs=\"mse\",\n\t),\n\tnode(\n\t\tplot_metric,\n\t\tinputs=\"mse\",\n\t),\n])\n\t\n# Execute the pipeline\npipeline.run(catalog)\n\n\nThis will execute the pipeline, which will ingest the data, train the model, evaluate the model, and visualize the model evaluation metrics.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: ChatGPT introduced the concept of a pipeline, which is nice. The steps are now encapsulated. An essential part of any ML experiment is tuning model parameters, so let‚Äôs ask to include a hyperparameter tuning step in the pipeline.\n\nüêõ\nCode comment: It makes sense that a pipeline should be run as shown by ChatGPT above. However, it turns out that Kedro requires a pipeline runner to orchestrate pipeline node execution. There are several kinds of runners, and I chose to use the simplest SequentialRunner. This ChatGPT mistake appeared in several of it's subsequent responses, so please take note.\n\nSince Kedro pipelines need to be command-line executable so that they can be included in CI (continuous integration) processes, pipelines and data catalogs need to be discoverable by Kedro's CLI (command-line interface) tool chain. To enable this, Kedro requires various externalized configurations and pipeline registration patterns to be defined (see here and here). It wasn't complicated to replicate these in my application from Kedro's example template projects.\n\nSo while ChatGPT gave me a head start, there was still some work to be done.\nQuestion 3\n\nCan you add hyperparameter tuning to the steps and the Kedro orchestration pipeline?\n\nYes, you can add hyperparameter tuning to the steps for training a machine learning model and the Kedro orchestration pipeline.\n\nTo do this, you can use a library like scikit-learn's GridSearchCV to search for the best hyperparameter values for your model. GridSearchCV performs an exhaustive search over a specified parameter grid, and returns the best set of hyperparameters based on cross-validated performance.\n\nHere's an example of how you can use GridSearchCV to add hyperparameter tuning to the training step of your pipeline:\n\nfrom sklearn.model_selection import GridSearchCV\n\ndef train_model(data, hyperparams):\n\t# Split the data into features and targets\n\tX = data.drop(\"target\", axis=1)\n\ty = data[\"target\"]\n\t# Create a LinearRegression model\n\tmodel = LinearRegression()\n\t# Create a parameter grid for the hyperparameters\n\tparam_grid = {\n\t\t\"fit_intercept\": [True, False],\n\t\t\"normalize\": [True, False],\n\t}\n\t# Use GridSearchCV to search for the best hyperparameters\n\tgrid_search = GridSearchCV(model, param_grid, cv=5)\n\tgrid_search.fit(X, y)\n\t# Return the best model\n\treturn grid_search.best_estimator_\n\t\n# Define the pipeline nodes\npipeline = Pipeline([\n\tnode(\n\t\ttrain_model,\n\t\tinputs=\"data\",\n\t\toutputs=\"model\",\n\t\thyperparams=Hyperparameters(param_grid),\n\t),\n\t# ... other nodes ...\n])\n\n\nThis will use GridSearchCV to search for the best values for the fit_intercept and normalize hyperparameters of the linear regression model, using 5-fold cross-validation.\n\nYou can then pass the best set of hyperparameters to the train_model function as a Hyperparameters object, and use them to train the model.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I like that. It‚Äôs what I do in my experiments. I secretly hope that by saying the answer is interesting, my chat session will remain live a little longer (as ChatGPT Preview is prone to terminating sessions without warning). After all I just started and human moderators might be snooping in on the conversation‚ÄîI want to show them I‚Äôm serious. My LSTM ML model is implemented in Keras, and it happens to be a regressor too‚Ä¶a happy coincidence. My regressor has quite a few parameters that require tuning, so I‚Äôll name them for ChatGPT and ask it to use GridSearchCV with those named parameters.\n\nQuestion 4\n\nInteresting! How can GridSearchCV be used to tune the parameters for a multilayer Tensorflow (Keras) LSTM network with parameters for the lookback, batch size, learning rate, epochs, optimizer, and loss function, and a number of nodes in each network layer, including the output layer?\n\nYou can use GridSearchCV to tune the hyperparameters for a multilayer TensorFlow (Keras) Long Short-Term Memory (LSTM) network by defining a custom KerasClassifier or KerasRegressor model, and passing it to GridSearchCV as the estimator.\n\nHere's an example of how you can use GridSearchCV to tune the hyperparameters for a multilayer LSTM network for time series forecasting:\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\ndef create_model(lookback, batch_size, learning_rate, optimizer, loss, units):\n\t# Create a Sequential model\n\tmodel = Sequential()\n\t# Add an LSTM layer with the specified number of units\n\tmodel.add(LSTM(units, input_shape=(lookback, 1)))\n\t# Add a dense output layer\n\tmodel.add(Dense(1))\n\t# Compile the model with the specified optimizer and loss function\n\tmodel.compile(optimizer=optimizer, loss=loss, metrics=[\"mae\"])\n\treturn model\n\n# Define the hyperparameter grid\nparam_grid = {\n\t\"lookback\": [10, 20, 30],\n\t\"batch_size\": [32, 64, 128],\n\t\"learning_rate\": [0.001, 0.01, 0.1],\n\t\"optimizer\": [\"adam\", \"sgd\"],\n\t\"loss\": [\"mse\", \"mae\"],\n\t\"units\": [32, 64, 128],\n}\n\n# Create the KerasClassifier model\nmodel = KerasClassifier(create_model)\n\n# Use GridSearchCV to search for the best hyperparameters\ngrid_search = GridSearchCV(model, param_grid, cv=5)\ngrid_search.fit(X, y)\n\n# Print the best set of hyperparameters\nprint(grid_search.best_params_)\n\n\nThis will use GridSearchCV to search for the best values for the lookback, batch_size, learning_rate, optimizer, loss, and units hyperparameters of the LSTM network, using 5-fold cross-validation.\n\nYou can then use the best set of hyperparameters to train the LSTM network and make forecasts.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Great, ChatGPT knows that LSTM regression models are used to create forecasts. However, it probably should‚Äôve wrapped create_model in KerasRegressor, not KerasClassifier. That‚Äôs something I‚Äôll check when I try to run the code. Now I want to steer my Q&A in the direction of model evaluation to gain a better understanding of cross-validation.\n\nQuestion 5\n\nWhat does cross-validation do?\n\nCross-validation is a technique for evaluating the performance of a machine learning model by training and testing the model on different subsets of the data. It helps to reduce the variance of the model's performance estimates and can give you a more accurate picture of how the model will perform on unseen data.\n\nThere are several different types of cross-validation, but the most common is k-fold cross-validation. In k-fold cross-validation, the data is split into k folds, and the model is trained and evaluated k times, with a different fold being used as the test set each time.\n\nFor example, in 5-fold cross-validation, the data is split into 5 folds, and the model is trained and evaluated 5 times, with a different fold being used as the test set each time. The model's performance is then averaged across the 5 folds to give a final performance estimate.\n\nCross-validation is useful for hyperparameter tuning because it allows you to evaluate the model's performance on different subsets of the data, which can help you identify the best set of hyperparameters for the model.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Let‚Äôs ask for more detail on evaluation metrics.\n\nQuestion 6\n\nWhat are the different ways of evaluating an LSTM, such as mae, confusion matrix, etc.? How do you implement or visualize these?\n\nThere are several different ways to evaluate the performance of a Long Short-Term Memory (LSTM) network, depending on the type of problem you are trying to solve. Some common evaluation metrics for LSTM networks include:\n\nMean Absolute Error (MAE): This metric measures the average magnitude of the errors in the predictions, without considering their direction. It is calculated as the sum of the absolute differences between the predicted values and the true values, divided by the number of predictions.\nMean Squared Error (MSE): This metric measures the average squared difference between the predicted values and the true values. It is calculated as the sum of the squared differences between the predicted values and the true values, divided by the number of predictions.\nConfusion Matrix: This is a matrix that shows the number of correct and incorrect predictions made by the model for each class. It is often used for classification tasks, and can help you understand the types of errors that the model is making.\n\nTo implement or visualize these evaluation metrics, you can use a library like scikit-learn. Here's an example of how you can use scikit-learn to calculate and visualize the MAE and MSE for an LSTM network:\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X_test)\n\n# Calculate the MAE and MSE\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n# Print the results\nprint(f\"MAE: {mae:.3f}\")\nprint(f\"MSE: {mse:.3f}\")\n\n\nTo visualize the confusion matrix, you can use the confusion_matrix function from scikit-learn, and use a library like Matplotlib to plot the matrix:\n\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X_test)\n\n# Calculate the confusion matrix\nconfusion_mat = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.imshow(confusion_mat, cmap=\"Blues\")\nplt.colorbar()\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\nThis will plot the confusion matrix as a color-coded matrix, with the predicted classes on the x-axis and the true classes on the y-axis. The color of each cell in the matrix represents the number of predictions made by the model for each class.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Oops! I made the mistake of mentioning the confusion matrix above, which made ChatGPT lose its context on regression models as confusion matrices don‚Äôt apply. I‚Äôm going to steer it back on track.\n\nQuestion 7\n\nWhat evaluation metrics and visualizations are used for Time Series Regression LSTMs?\n\nFor time series regression tasks, some common evaluation metrics and visualizations for Long Short-Term Memory (LSTM) networks include:\n\nMean Absolute Error (MAE): This metric measures the average magnitude of the errors in the predictions, without considering their direction. It is calculated as the sum of the absolute differences between the predicted values and the true values, divided by the number of predictions.\nMean Squared Error (MSE): This metric measures the average squared difference between the predicted values and the true values. It is calculated as the sum of the squared differences between the predicted values and the true values, divided by the number of predictions.\nRoot Mean Squared Error (RMSE): This metric is the square root of the MSE, and is often used to report the performance of a time series model. It is interpreted in the same units as the original time series data, which can make it easier to understand the magnitude of the errors.\nTime Series Plot: A time series plot can help you visualize the predicted values and the true values for the time series. You can plot the predicted values and the true values on the same plot, and use the plot to identify trends, patterns, and anomalies in the data.\n\nTo calculate these evaluation metrics and create a time series plot, you can use a library like Matplotlib. Here's an example of how you can use Matplotlib to calculate and visualize the MAE, MSE, and RMSE for an LSTM network:\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X_test)\n\n# Calculate the MAE, MSE, and RMSE\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\n# Print the results\nprint(f\"MAE: {mae:.3f}\")\nprint(f\"MSE: {mse:.3f}\")\nprint(f\"RMSE: {rmse:.3f}\")\n\n# Create a time series plot\nplt.plot(y_test, label=\"True Values\")\nplt.plot(y_pred, label=\"Predicted Values\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.title(\"Time Series Prediction\")\nplt.legend()\nplt.show()\n\n\nThis will calculate the MAE, MSE, and RMSE for the LSTM network, and plot the predicted values and the true values for the time series on the same plot.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Fab! When I build Streamlit apps, I never use Matplotlib for charting, so let‚Äôs ask ChatGPT to use Plotly Express instead for interactive charts.\n\nQuestion 8\n\nCan you show this using Plotly Express instead of Matplotlib?\n\nCertainly! Here's an example of how you can use Plotly Express to calculate and visualize the MAE, MSE, and RMSE for an LSTM network:\n\nimport plotly.express as px\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X_test)\n\n# Calculate the MAE, MSE, and RMSE\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\n# Print the results\nprint(f\"MAE: {mae:.3f}\")\nprint(f\"MSE: {mse:.3f}\")\nprint(f\"RMSE: {rmse:.3f}\")\n\n# Create a time series plot\nfig = px.scatter(x=range(len(y_test)), y=y_test, title=\"Time Series Prediction\")\nfig.add_scatter(x=range(len(y_pred)), y=y_pred, mode=\"markers\", name=\"Predicted Values\")\nfig.show()\n\n\nThis will calculate the MAE, MSE, and RMSE\n\nComment: That looks promising. My time series forecasting LSTM is part of an algorithmic trading system that outputs trade entry and exit signals for a trading bot. It‚Äôd be nice to plot these signals on a chart. Let‚Äôs ask.\n\nQuestion 9\n\nI want to add \"enter\" and \"exit\" marks at certain times on a time series plot. These marks are derived from another data set. How do I do this using Plotly Express?\n\nYou can add \"enter\" and \"exit\" marks to a time series plot using Plotly Express by creating a separate scatter plot trace for the marks and adding it to the figure.\n\nHere's an example of how you can do this:\n\nimport plotly.express as px\n\n# Load the data\ndf = pd.read_csv(\"data.csv\")\n\n# Split the data into features and targets\nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X)\n\n# Create a time series plot\nfig = px.scatter(x=range(len(y)), y=y, title=\"Time Series Prediction\")\nfig.add_scatter(x=range(len(y_pred)), y=y_pred, mode=\"markers\", name=\"Predicted Values\")\n\n# Load the marks data\nmarks = pd.read_csv(\"marks.csv\")\n\n# Add a scatter plot trace for the marks\nfig.add_scatter(x=marks[\"time\"], y=marks[\"value\"], mode=\"markers\", name=\"Marks\")\nfig.show()\n\n\nThis will create a time series plot with the predicted values and the true values, and a separate scatter plot trace for the marks. The marks will be plotted as markers at the specified times on the x-axis.\n\nI hope this helps!\n\nComment: That‚Äôs reasonable. In the past, I‚Äôve done something similar using fig.add_trace and not fig.add_scatter, so I learned something new. However, ChatGPT didn‚Äôt quite nail the answer as marks.csv doesn‚Äôt distinguish between trade entry (buy) and trade exit (sell) marks. Let‚Äôs ask for some detail.\n\nQuestion 10\n\nWhat is the format of the marks.csv data file?\n\nThe format of the marks.csv data file will depend on the data that you want to include in the marks.\n\nHere's an example of a simple marks.csv file that contains two columns:\n\ntime¬†¬†¬†\tvalue\n5\t10\n15\t20\n30\t50\n\nIn this example, the time column contains the time (in some unit) at which the mark should be displayed on the x-axis, and the value column contains the value (in some unit) at which the mark should be displayed on the y-axis.\n\nYou can include additional columns in the marks.csv file if you want to include more data in the marks. For example, you might want to include a label column to specify a label for each mark, or a color column to specify a color for each mark.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Lovely! The answer elaborates on the basic file layout and suggests adding a label column to distinguish between different kinds of marks. I‚Äôll coax it into showing me what it looks like.\n\nQuestion 11\n\nIf I have a label column of \"buy\" and \"sell\" values in marks.csv, then how will the Plotly Express chart be written to show the marks in \"green\" and \"red\" respectively?\n\nTo show the marks in \"green\" and \"red\" based on the values in the label column of the marks.csv file, you can use the color parameter of the add_scatter function to specify a color for each mark.\n\nHere's an example of how you can do this:\n\nimport plotly.express as px\n\n# Load the data\ndf = pd.read_csv(\"data.csv\")\n\n# Split the data into features and targets\nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\n# Make predictions using the LSTM network\ny_pred = model.predict(X)\n\n# Create a time series plot\nfig = px.scatter(x=range(len(y)), y=y, title=\"Time Series Prediction\")\nfig.add_scatter(x=range(len(y_pred)), y=y_pred, mode=\"markers\", name=\"Predicted Values\")\n\n# Load the marks data\nmarks = pd.read_csv(\"marks.csv\")\n\n# Add a scatter plot trace for the marks\nfig.add_scatter(x=marks[\"time\"], y=marks[\"value\"], mode=\"markers\", name=\"Marks\", color=marks[\"label\"])\nfig.show()\n\n\nThis will create a scatter plot trace for the marks, with the color of each mark determined by the corresponding value in the label column. The color parameter can accept a list of colors, a list of color names (as strings), or a list of color codes (as strings).\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I could‚Äôve asked for more detail on how the color parameter uses a list of colors, but I wanted to get back to the Kedro pipeline and use it to tune multiple different models. My algo-trading system has one model per financial instrument, so I wanted to easily manage all of them. I asked ChatGPT to tell me what changes are required to the pipeline to achieve this.\n\nQuestion 12\n\nGoing back to the Kedro pipeline...say, I have several LSTM models and want to tune each one and visualize their evaluation results. How does the pipeline change?\n\nTo build a Kedro pipeline that tunes multiple LSTM models and visualizes their evaluation results, you can follow these steps:\n\nCreate a function that takes in a set of hyperparameters and returns a trained LSTM model. This function should take care of preprocessing the data, training the model, and evaluating the model on a test set.\nCreate a list of dictionaries, where each dictionary contains a set of hyperparameters to be passed to the model training function.\nUse the Pipeline class from Kedro to define a pipeline that loops through the list of hyperparameters, trains a model for each set of hyperparameters, and stores the trained models in a pipeline output.\nUse the Visualizer class from Kedro to define a visualization function that takes in the trained models and generates evaluation plots for each model.\nAdd the visualization function as a final node in the pipeline.\n\nHere's an example of how you can define the pipeline:\n\nfrom kedro.pipeline import Pipeline, node\n\n# Define the model training function\ndef train_model(hyperparameters, data):\n\t# Preprocess the data\n\tX_train, X_test, y_train, y_test = preprocess_data(data)\n\t# Create the LSTM model\n\tmodel = create_lstm_model(hyperparameters)\n\t# Train the model\n\tmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=hyperparameters[\"epochs\"])\n\t# Evaluate the model\n\tevaluation = model.evaluate(X_test, y_test, return_dict=True)\n\treturn model, evaluation\n\t\n# Define the list of hyperparameters to try\nhyperparameters_list = [\n\t{\"lookback\": 10, \"batch_size\": 32, \"learning_rate\": 0.1, \"epochs\": 10, \"optimizer\": \"Adam\", \"loss\":\n\t\"mean_absolute_error\"},\n\t{\"lookback\": 20, \"batch_size\": 64, \"learning_rate\": 0.01, \"epochs\": 20, \"optimizer\": \"SGD\", \"loss\":\n\t\"mean_squared_error\"},\n\t{\"lookback\": 5, \"batch_size\": 16, \"learning_rate\": 0.001, \"epochs\": 5, \"optimizer\": \"RMSprop\", \"loss\":\n\t\"mean_absolute_percentage_error\"},\n]\n\n# Define the pipeline\npipeline = Pipeline(\n\t[\n\t\tnode(\n\t\t\ttrain_model,\n\t\t\tinputs=[\"hyperparameters\", \"data\"],\n\t\t\toutputs=[\"model\", \"evaluation\"],\n\t\t\titerable=hyperparameters_list,\n\t\t),\n\t\tnode(\n\t\t\tvisualize_results,\n\t\t\tinputs=[\"model\", \"evaluation\"],\n\t\t\titerable=hyperparameters_list,\n\t\t),\n\t]\n)\n\n\nThis pipeline defines a node that trains a model for each set of hyperparameters in the hyperparameters_list, and stores the trained models and their evaluation results in the pipeline output. The pipeline also defines a final node that visualizes the evaluation results for each model.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I think this gives me a good handle on how to manage multiple models. I can add additional parameters for each model, and the Pipeline and node objects warrant further investigation since the results stored in Pipeline and node can take hyperparameter iterables! Now, what would a visualize_results implementation look like?\n\nQuestion 13\n\nGreat, so can visualize_results be implemented using similar Plotly Express examples you gave earlier, in the Kedro Visualizer class?\n\nYes, you can use Plotly Express to visualize the evaluation results for each LSTM model in the Visualizer class of Kedro.\n\nHere's an example of how you can define the visualize_results function using Plotly Express:\n\nimport plotly.express as px\n\ndef visualize_results(model, evaluation):\n\t# Extract the evaluation metrics from the evaluation dictionary\n\tmae = evaluation[\"mae\"]\n\tmse = evaluation[\"mse\"]\n\trmse = evaluation[\"rmse\"]\n\t# Create a bar chart to visualize the evaluation metrics\n\tfig = px.bar(x=[\"MAE\", \"MSE\", \"RMSE\"], y=[mae, mse, rmse])\n\tfig.show()\n\n\nThis function creates a bar chart that displays the MAE, MSE, and RMSE evaluation metrics for the model.\n\nYou can then use this function as a node in the Kedro pipeline, as shown in the previous example.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I saw a YouTube video on Kedro. The speaker showed a graphical representation of the Kedro pipeline. I was intrigued to learn how that was done.\n\nQuestion 14\n\nAmazing, thanks. How can I see a graphical representation of the Kedro pipeline?\n\nTo see a graphical representation of a Kedro pipeline, you can use the draw method of the Pipeline class.\n\nHere's an example of how you can use the draw method to draw a graphical representation of the pipeline:\n\n# Import the necessary modules\nfrom kedro.pipeline import Pipeline\nfrom kedro.io import KedroCliKernel\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\", KedroCliKernel())\n\n# Draw the pipeline\npipeline.draw()\n\n\nThis will open a graphical representation of the pipeline in your default web browser. The graph will show the nodes of the pipeline as boxes, with the input and output nodes connected by arrows.\n\nYou can also use the show_graph method of the Pipeline class to draw the pipeline and save it to a file:\n\n# Import the necessary modules\nfrom kedro.pipeline import Pipeline\nfrom kedro.io import KedroCliKernel\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\", KedroCliKernel())\n\n# Draw the pipeline and save it to a file\npipeline.show_graph(format=\"png\", filename=\"pipeline.png\")\n\n\nThis will save the graphical representation of the pipeline to a file called pipeline.png in the current working directory.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I was right. This Pipeline object is very powerful. I wonder if it can be used to distribute model training jobs across several machines or CPU/GPU cores.\n\nüêõ\nCode comment: There are no such pipeline APIs like draw() and show_graph(). Maybe ChatGPT was partially replaying someone's pipeline wrapper object with those interfaces, or maybe it was telling me about another framework's pipeline API. I will never know!\n\nKedro has a separate application \"kedro viz\" to visualize pipelines. This command is run in your application's root folder and it uses the project's configuration, data catalog, and pipeline registration conventions to start a web server that displays your pipeline graph in a browser window. Much like Streamlit does. My Streamlit application runs the \"kedro viz\" command in a sub-process and visualizes the output in Streamlit's component.html widget.\nQuestion 15\n\nCan the Pipeline class be used to distribute the jobs in each node across several machines or run them in parallel?\n\nYes, the Pipeline class in Kedro can be used to distribute the jobs in each node across several machines or run them in parallel.\n\nTo do this, you can use the Executor class from Kedro to create an executor that manages the parallel execution of the pipeline nodes. The Executor class supports several different execution modes, including:\n\nSequentialExecutor: This executor runs the nodes of the pipeline sequentially, one after the other. This is the default executor.\nThreadedExecutor: This executor runs the nodes of the pipeline in separate threads, allowing them to run in parallel.\nMultiprocessingExecutor: This executor runs the nodes of the pipeline in separate processes, allowing them to run in parallel across multiple CPU cores or machines.\n\nTo use one of these executors, you can pass it as an argument to the run method of the Pipeline class. For example, to run the pipeline using the ThreadedExecutor, you can do the following:\n\nfrom kedro.pipeline import Pipeline\nfrom kedro.executor import ThreadedExecutor\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Create the executor\nexecutor = ThreadedExecutor()\n\n# Run the pipeline using the executor\npipeline.run(executor=executor)\n\n\nThis will run the nodes of the pipeline in separate threads, allowing them to run in parallel.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Really? That‚Äôs awesome, and it looks so easy. I know from many years of experience with distributed systems that job instrumentation, logging, and failure management are critical. Does the pipeline support these capabilities too?\n\nQuestion 16\n\nAwesome! Does kedro.executor allow the jobs (log outputs) to be monitored and restarted if they fail?\n\nYes, the Executor class in Kedro allows you to monitor the jobs and restart them if they fail.\n\nTo do this, you can use the RetryingExecutor class, which wraps another executor and automatically retries failed jobs.\n\nHere's an example of how you can use the RetryingExecutor to monitor and restart the jobs in a pipeline:\n\nfrom kedro.pipeline import Pipeline\nfrom kedro.executor import MultiprocessingExecutor, RetryingExecutor\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Create the executor\nexecutor = MultiprocessingExecutor()\n\n# Wrap the executor in a RetryingExecutor\nretrying_executor = RetryingExecutor(executor, retries=3)\n\n# Run the pipeline using the retrying executor\npipeline.run(executor=retrying_executor)\n\n\nThis will run the pipeline using the MultiprocessingExecutor, which will distribute the jobs across multiple CPU cores or machines. If a job fails, the RetryingExecutor will automatically retry it up to 3 times before giving up.\n\nYou can also specify a delay between retries by setting the delay parameter of the RetryingExecutor. For example, to specify a delay of 1 second between retries, you can do the following:\n\nretrying_executor = RetryingExecutor(executor, retries=3, delay=1.0)\n\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Wow, there is a RetryingExecutor out of the box. Nice. When it comes to logging, it‚Äôs best to have some log persistence, on top of which analytics and insights can be implemented. Let‚Äôs explore that line of thought a little. In particular, I want to know about log sinks.\n\nQuestion 17\n\nHow is the output of executors redirected to files or custom log sinks?\n\nTo redirect the output of the executors to files or custom log sinks, you can use the LogSink class from Kedro.\n\nThe LogSink class allows you to specify a logging handler that will be used to redirect the output of the executors to a specific location. Kedro provides several built-in log sinks, including:\n\nTeeSink: This log sink redirects the output to both the console and a file.\nFileSink: This log sink redirects the output to a file.\nNullSink: This log sink discards all output.\n\nYou can use these log sinks by passing them as an argument to the run method of the Pipeline class. For example, to redirect the output of the pipeline to a file called output.log, you can do the following:\n\nfrom kedro.pipeline import Pipeline\nfrom kedro.io import FileSink\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Create the log sink\nsink = FileSink(\"output.log\")\n\n# Run the pipeline using the log sink\npipeline.run(log_sink=sink)\n\n\nThis will redirect the output of the pipeline to the file output.log.\n\nYou can also use a custom logging handler by subclassing the LogSink class and overriding the _setup_logging method. For example, to redirect the output to a custom log sink called CustomSink, you can do the following:\n\nfrom kedro.pipeline import Pipeline\nfrom kedro.io import LogSink\nimport logging\n\nclass CustomSink(LogSink):\n\tdef _setup_logging(self):\n\t\t# Set up the custom log sink\n\t\tself.custom_logger = logging.getLogger(\"CustomLogger\")\n\t\tself.custom_logger.setLevel(logging.INFO)\n\t\t# Add a handler to the custom logger\n\t\thandler = logging.StreamHandler()\n\t\thandler.setFormatter(logging.Formatter(\"%(message)s\"))\n\t\tself.custom_logger.addHandler(handler)\n\t\t# Set the root logger to use the custom logger\n\t\tlogging.root.handlers = [handler]\n\t\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Create the log sink\nsink = CustomSink()\n\n# Run the pipeline using the log sink\npipeline.run(log_sink=sink)\n\n\nThis will redirect the output of the pipeline to the custom log sink CustomLogger. You can then use the custom_logger object to log messages to the custom sink.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: At this point, I don‚Äôt know if I should be more impressed with Kedro or that ChatGPT ‚Äúknows‚Äù. From what I‚Äôve learned so far, I‚Äôm itching to get started with Kedro straight away, but I want to ask one more question.\n\nüêõ\nCode comment: Unfortunately, the latest version of Kedro doesn't have these sinks out-of-the-box. ChatGPT was trained on pre-2021 web knowledge, so this information may be outdated or incorrect. Instead, Kedro uses configuration-driven logging which I haven't figured out yet.\nQuestion 18\n\nCan you make a custom sink that sends logs to Kibana?\n\nYes, you can use the ElasticsearchHandler from the elasticsearch-logger package to create a custom log sink that sends logs to Kibana.\n\nHere's an example of how you can define the KibanaSink class to redirect the output of the executors to Kibana:\n\nfrom kedro.pipeline import Pipeline\nfrom kedro.io import LogSink\nimport logging\nfrom elasticsearch_logger import ElasticsearchHandler\n\nclass KibanaSink(LogSink):\n\tdef _setup_logging(self):\n\t\t# Set up the Elasticsearch handler\n\t\tself.handler = ElasticsearchHandler(\n\t\t\thosts=[{\"host\": \"localhost\", \"port\": 9200}]\n\t\t)\n\t\t# Set the root logger to use the Elasticsearch handler\n\t\tlogging.root.handlers = [self.handler]\n\n# Load the pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Create the log sink\nsink = KibanaSink()\n\n# Run the pipeline using the log sink\npipeline.run(log_sink=sink)\n\n\nThis will redirect the output of the pipeline to Kibana, which can be used to visualize and analyze the logs.\n\nYou will need to have Elasticsearch and Kibana installed and running on your machine to use this sink. You can find more information on how to install and configure Elasticsearch and Kibana in the Elasticsearch documentation.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I know ChatGPT is cleverly replaying canned Kedro knowledge, but when it‚Äôs revealed elegantly like this, the conversation is very satisfying. Kedro appears to be very powerful and well-designed, making ChatGPT‚Äôs answers look trivial. I want to see a contrast between Kedro and alternatives.\n\nüêõ\nCode comment: This looks like it could work, except for the reference to pipeline.run() again, and is worth investigating further.\nQuestion 19\n\nWhat are the alternatives to using Kedro?\n\nThere are several alternatives to Kedro that you can use for building and managing machine learning pipelines. Some of the popular alternatives include:\n\nAirflow: Airflow is a platform to programmatically author, schedule, and monitor workflows. It was originally developed by Airbnb, and is now a popular open-source project. Airflow provides a flexible and powerful platform for building and managing ML pipelines, and has a large user base and community.\nKubeflow: Kubeflow is an open-source project that aims to make it easy to deploy and manage machine learning workflows on Kubernetes. It provides a range of tools and frameworks for building, deploying, and managing ML pipelines, including Jupyter notebooks, TensorFlow, and PyTorch.\nPrefect: Prefect is an open-source platform for building, scheduling, and monitoring data pipelines. It is designed to be highly scalable and resilient, and provides a range of features for building and managing ML pipelines, including support for distributed execution and automatic retries.\nLuigi: Luigi is an open-source Python library for building complex pipelines of batch jobs. It was developed by Spotify and is designed to be easy to use and flexible. Luigi provides a range of features for building and managing ML pipelines, including support for dependencies, parallel execution, and error handling.\n\nThese are just a few examples of the alternatives to Kedro that you can consider. Each platform has its own strengths and weaknesses, and the right choice will depend on your specific needs and requirements.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Let‚Äôs dig deeper into this contrast and see how the Kedro pipeline is done in another framework.\n\nQuestion 20\n\nHow would you implement our Kedro pipeline in Airflow?\n\nTo implement the Kedro pipeline in Airflow, you can define a series of tasks and dependencies using the DAG (directed acyclic graph) and Operator classes in Airflow.\n\nHere's an example of how you can define the Kedro pipeline in Airflow:\n\nHere's the complete example of how you can define the Kedro pipeline in Airflow:\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default_args dictionary to specify the default parameters of the DAG\ndefault_args = {\n\t\"owner\": \"me\",\n\t\"start_date\": \"2022-01-01\",\n\t\"depends_on_past\": False,\n\t\"retries\": 1,\n\t\"retry_delay\": timedelta(minutes=5),\n}\n\n# Create the DAG object\ndag = DAG(\n\t\"kedro_pipeline\",\n\tdefault_args=default_args,\n\tdescription=\"Example Kedro pipeline in Airflow\",\n\tschedule_interval=timedelta(hours=1),\n\tcatchup=False,\n)\n\n# Define the data ingestion task\ndef ingest_data_task():\n\t# Call the data ingestion function\n\tmodule.ingest_data()\n\tingest_data = PythonOperator(\n\t\ttask_id=\"ingest_data\",\n\t\tpython_callable=ingest_data_task,\n\t\tdag=dag,\n\t)\n\n# Define the preprocessing task\ndef preprocess_data_task():\n\t# Call the preprocessing function\n\tmodule.preprocess_data()\n\tpreprocess_data = PythonOperator(\n\t\ttask_id=\"preprocess_data\",\n\t\tpython_callable=preprocess_data_task,\n\t\tdag=dag,\n\t)\n\n# Define the hyperparameter tuning task\ndef tune_model_task():\n\t# Call the hyperparameter tuning function\n\tmodule.tune_model()\n\ttune_model = PythonOperator(\n\t\ttask_id=\"tune_model\",\n\t\tpython_callable=tune_model_task,\n\t\tdag=dag,\n\t)\n\n# Define the model evaluation task\ndef evaluate_model_task():\n# Call the model evaluation function\nmodule.evaluate_model()\nevaluate_model = PythonOperator(\n\ttask_id=\"evaluate_model\",\n\tpython_callable=evaluate_model_task,\n\tdag=dag,\n)\n\n# Define the model visualization task\ndef visualize_results_task():\n\t# Call the model visualization function\n\tmodule.visualize_results()\n\tvisualize_results = PythonOperator(\n\t\ttask_id=\"visualize_results\",\n\t\tpython_callable=visualize_results_task,\n\t\tdag=dag,\n\t)\n\n# Set the dependencies between tasks\ningest_data >> preprocess_data >> tune_model >> evaluate_model >> visualize_results\n\n\nIn this example, the Kedro pipeline is implemented as a series of tasks defined using the PythonOperator class in Airflow. The tasks are connected by dependencies using the >> operator in Airflow, which specifies dependencies between tasks in a DAG. It specifies that the task on the left-hand side of the operator must be completed before the task on the right-hand side can start.\n\nFor example, in the following code snippet:\n\ningest_data >> preprocess_data >> tune_model >> evaluate_model >> visualize_results\n\n\nThe >> operator specifies that the ingest_data task must be completed before the preprocess_data task can start, the preprocess_data task must be completed before the tune_model task can start, and so on.\n\nThis ensures that the tasks are run in the correct order, and that the necessary data and resources are available when each task is run.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: That looks like a lot of imperative code. I‚Äôm not sure if Airflow has a declarative syntax like Kedro‚Äôs or any of Kedro‚Äôs other features. Still, I want to dig into this opaque ‚Äúmodule‚Äù reference that‚Äôs in the Airflow code above.\n\nQuestion 21\n\nWhat is the variable \"module\"?\n\nIn the example code I provided, module is a placeholder for the Python module that contains the functions or methods that are called by the tasks in the Airflow DAG.\n\nFor example, in the following code snippet:\n\n# Define the data ingestion task\ndef ingest_data_task():\n\t# Call the data ingestion function\n\tmodule.ingest_data()\n\n\nThe ingest_data_task function calls the ingest_data function, which is assumed to be defined in the module Python module.\n\nYou would need to replace module with the actual name of the Python module that contains the functions or methods you want to call. For example, if the ingest_data function is defined in a module called data_ingestion, you would replace module with data_ingestion.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: Ah, so there‚Äôs more to it than initially implied. It pays to read the code ChatGPT produces and ask for clarification when you don‚Äôt understand.\n\nQuestion 22\n\nCan you provide me with a skeleton implementation for the Python \"module\"?\n\nCertainly! Here's a skeleton implementation of the Python \"module\" that contains the functions or methods that are called by the tasks in the Airflow DAG:\n\n# Import required libraries\nimport pandas as pd\n\ndef ingest_data():\n\t\"\"\"Ingest raw data from a file or database\"\"\"\n\t# Load the raw data\n\traw_data = pd.read_csv(\"raw_data.csv\")\n\t# Save the data to a file or database\n\traw_data.to_csv(\"data.csv\", index=False)\n\ndef preprocess_data():\n\t\"\"\"Preprocess the data\"\"\"\n\t# Load the data\n\tdata = pd.read_csv(\"data.csv\")\n\t# Perform preprocessing steps, such as cleaning and feature engineering\n\tprocessed_data = data.clean()\n\t# Save the processed data to a file or database\n\tprocessed_data.to_csv(\"processed_data.csv\", index=False)\n\ndef tune_model():\n\t\"\"\"Tune the hyperparameters of the model\"\"\"\n\t# Load the processed data\n\tprocessed_data = pd.read_csv(\"processed_data.csv\")\n\t# Create and tune the model\n\tmodel = Model(processed_data)\n\tmodel.tune_hyperparameters()\n\t# Save the tuned model to a file or database\n\tmodel.save(\"tuned_model.pkl\")\n\ndef evaluate_model():\n\t\"\"\"Evaluate the model\"\"\"\n\t# Load the processed data and tuned model\n\tprocessed_data = pd.read_csv(\"processed_data.csv\")\n\tmodel = Model.load(\"tuned_model.pkl\")\n\t# Evaluate the model\n\tevaluation_metrics = model.evaluate(processed_data)\n\t# Save the evaluation metrics to a file or database\n\tpd.DataFrame(evaluation_metrics).to_csv(\"evaluation_metrics.csv\", index=False)\n\ndef visualize_results():\n\t# TODO: Implement the model visualization logic here\n\tpass\n\n\nIn this example, the module contains five functions: ingest_data, preprocess_data, tune_model, evaluate_model, and visualize_results. These functions correspond to the tasks in the Airflow DAG.\n\nYou would need to replace the TODO comment(s) with the actual code for the function(s). The code should implement the logic for the corresponding task in the pipeline.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: I‚Äôm comfortable displaying my LSTM model data and forecasts in tables and charts in Streamlit. But I‚Äôm not sure how to display a Kedro pipeline graph. So, let‚Äôs ask‚Ä¶\n\nQuestion 23\n\nHow would you display the Kedro pipeline graph in Streamlit?\n\nTo display the Kedro pipeline graph in a Streamlit application, you can use the Pipeline.to_dot_graph method to generate a dot graph representation of the pipeline, and then to display a dot graph in a Streamlit app, you can use the pyvis.network.Network.from_dot method from the Pyvis library to generate a network object from the dot graph representation, and then use the st.pyplot function to display the network object in the Streamlit app.\n\nHere's an example of how you can do this:\n\nimport streamlit as st\nfrom kedro.pipeline import Pipeline\nfrom pyvis import network as net\n\n# Load the Kedro pipeline\npipeline = Pipeline.load(\"path/to/pipeline.yml\")\n\n# Generate the dot graph representation of the pipeline\ndot_graph = pipeline.to_dot_graph()\n\n# Generate a network object from the dot graph\ngraph = net.Network.from_dot(dot_graph)\n\n# Display the network object in the Streamlit app\nst.pyplot(graph.show)\n\n\nThis will display the Kedro pipeline graph in the Streamlit app, using the dot graph representation generated by the Pipeline.to_dot_graph method and the pyvis library.\n\nYou can customize the appearance of the graph by modifying the properties of the graph object. For example, you can use the set_edge_color method to change the color of the edges, or the set_node_color method to change the color of the nodes.\n\nComment: I‚Äôm not certain that st.pyplot can display dot_graph, so let‚Äôs regenerate the answer.\n\nüêõ\nCode comment: Kedro's pipeline doesn't have a to_dot_graph() API! See my earlier code comment about \"kedro viz\".\nQuestion 24\n\n[Regenerate!]\n\nTo display the Kedro pipeline graph in a Streamlit application, you can use the st.graphviz_chart function to render the graph using Graphviz.\n\nHere's an example of how you can do this:\n\nimport streamlit as st\nfrom kedro.pipeline import Pipeline\n\n# Create the Kedro pipeline\npipeline = Pipeline([\n\t# Add the pipeline nodes here\n])\n\n# Generate the Graphviz representation of the pipeline\npipeline_graph = pipeline.to_graph()\n\n# Render the pipeline graph using st.graphviz_chart\nst.graphviz_chart(pipeline_graph)\n\n\nIn this example, the Pipeline class is used to create a Kedro pipeline, and the to_graph method is used to generate a Graphviz representation of the pipeline. The st.graphviz_chart function is then used to render the graph in a Streamlit application.\n\nComment: I didn‚Äôt know about st.graphviz_chart. Let‚Äôs try another.\n\nüêõ\nCode comment: Kedro's pipeline doesn't have a to_graph() API! See my earlier code comment about \"kedro viz\".\nQuestion 25\n\n[Regenerate!]\n\nTo display the Kedro pipeline graph in Streamlit, you can use the kedro.pipeline.Pipeline.graph method to generate the graph, and then use the st.graphviz_chart function to render the graph in a Streamlit app.\n\nHere's an example of how you can do this:\n\nimport streamlit as st\nfrom kedro.pipeline import Pipeline\n\n# Load the Kedro pipeline\npipeline = Pipeline.load(\"/path/to/pipeline.yml\")\n\n# Generate the pipeline graph\ngraph = pipeline.graph\n\n# Render the graph in a Streamlit app\nst.graphviz_chart(graph)\n\n\nIn this example, the Pipeline.load method is used to load the Kedro pipeline from a pipeline.yml file. The pipeline.graph method is then used to generate the graph of the pipeline, and the st.graphviz_chart function is used to render the graph in a Streamlit app.\n\nYou can customize the appearance of the graph using the various options provided by the st.graphviz_chart function. For example, you can specify the font, color, and size of the nodes and edges in the graph.\n\nI hope this helps! Let me know if you have any other questions.\n\nComment: By this point, I‚Äôm overwhelmed with ideas, but I‚Äôve learned enough. Since ChatGPT saves previous conversations, I can come back to fill in the gaps or make corrections. Now I‚Äôm going to take the provided code and build the Kedro pipeline and the Streamlit front end. I‚Äôll start with a simple LSTM model for the purposes of this post and build upon it for my real-world solution.\n\nüêõ\nCode comment: If only it were true, I'd still be overwhelmed. Kedro's pipeline doesn't have a graph attribute! See my earlier code comment about \"kedro viz\".\nWrapping up\n\nChatGPT helped me develop a decent solution for deploying and managing my ML models using Kedro. I did this in less time and with less effort than I could‚Äôve done without it. I learned a lot about Kedro and can now run an ML pipeline manually or orchestrated by the Kedro framework, and display a pipeline visualization.\n\nI built the app over the weekend. I didn't have time to implement hyperparameter tuning or an LSTM model in my pipeline. I'll do that in the coming weeks, so if you find this useful, follow my repo and consider giving it a star! ‚≠ê\n\nIn the future, as the field of \"prompt engineering\" matures, maybe chatbots using LLMs will be smart enough to clarify user intent. I believe for now you'll get better results if you write prompts with a clear context and goal in mind. Make a conversation plan to take ChatGPT through a series of small Q&A steps of increasing detail on your desired outcome. If you‚Äôre fortunate, you‚Äôll tease out more features and capabilities in your answers. Finally, thoroughly check and test the code ChatGPT generates and use it only as a springboard for your own creativity.\n\nIf you want to create your own conversation plan and prompts, read How to write an effective GPT-3 prompt. And if you have any questions, contact me on LinkedIn or Twitter.\n\nHappy chatting and Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Build an image background remover in Streamlit",
    "url": "https://blog.streamlit.io/build-an-image-background-remover-in-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild an image background remover in Streamlit\n\nSkip the fees and do it for free! üéà\n\nBy Tyler Simons\nPosted in Tutorials, January 10 2023\nHow to set up your development environment\nHow to upload and download images\nHow to use the rembg package to remove backgrounds from images with Python\nHow to put it all together in a neatly formatted Streamlit app ü§Ø\nWrapping up üéÅ\nContents\nShare this post\n‚Üê All posts\n\nYou‚Äôve finally found it! üòÆ\n\nAfter hours and hours of searching, you‚Äôve found the best image of a wallaby for your class presentation about Australian wildlife. It‚Äôd look great next to your tips on staying safe in the rugged outback (bring lots of waterüíß).\n\nBut there‚Äôs one problem. It has a GREEN üü© BACKGROUND. Green totally doesn‚Äôt fit your vibe. So you start searching for web-based image processing services and discount them one by one:\n\n$0.25 per image? Nope.\nI need an account? No, thanks.\nWon‚Äôt let me download my image at full resolution? Absolutely not.\n\n‚ÄúThis will never work‚Äù, you think. ‚ÄúBut wait! I know some Python and have the magical tools of Streamlit. Can I do this myself?‚Äù\n\nYep, 100%! You can build a high-quality image background remover yourself. In this tutorial, I‚Äôll teach you:\n\nHow to set up your development environment\nHow to upload and download images\nHow to use the rembg package to remove backgrounds from images with Python\nHow to put it all together in a neatly formatted Streamlit app ü§Ø\nüí°\nWant to skip straight to the app? Sure thing! Here‚Äôs the app and here‚Äôs the code on Github.\nHow to set up your development environment\n\nIn a new folder, make a virtual environment just for this project. Pull out your handy-dandy terminal and run the following:\n\n# Set up our folder\ncd ~/Documents\nmkdir Streamlit-Image-Remover\ncd Streamlit-Image-Remover\n\n# Create and source our virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n\nThis will let you separate your local packages from the rest of your ecosystem‚Äîthe standard best practice. Once it‚Äôs set up, install the following packages (to use everything available in Streamlit and the awesome Python library rembg):\n\npip install streamlit\npip install rembg\n\n\nNext, download the source image, save it in the same folder as the app, and call it wallaby.png.\n\nHow to upload and download images\n\nStreamlit makes uploading and downloading files easy. Just define the file types you‚Äôd like to accept in the arguments and give the functions some labels:\n\nst.file_uploader() accepts a label and allowed file types.\nst.download_button() accepts a label, a Python object, a file name, and a file type.\n\nFor your new folder Streamlit-Image-Remover/, open a code editor (VSCode or PyCharm), make a new file, and add the following:\n\nimport streamlit as st\n\nst.title(\"Hello Upload!\")\n\n# Upload the image\nimage_upload = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n\n# Now, you can download the same file!\nif image_upload:\n\tst.download_button(\"Download your file here\", image_upload, \"my_image.png\", \"image/png\")\n\n\nSave the Python file as bg_remove.py and run the app to try it out:\n\nstreamlit run bg_remove.py\n\n\nYou did it!\n\nNow upload the wallaby.png image and download it again if you want to try it out. üí•\n\nHow to use the rembg package to remove backgrounds from images with Python\n\nYou want your app to work in 3 steps:\n\nA user uploads an image\nThe rembg package transforms it\nThe user downloads the transformed image\n\nSteps 1 and 3 are done. For step 2, change the code to:\n\n# We need a few more packages\nfrom io import BytesIO\nimport streamlit as st\nfrom PIL import Image\nfrom rembg import remove\n\nst.title(\"Hello Upload!\")\n\n# Upload the file\nimage_upload = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n\n# Convert the image to BytesIO so we can download it\ndef convert_image(img):\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    byte_im = buf.getvalue()\n    return byte_im\n\n# If we've uploaded an image, open it and remove the background!\nif image_upload:\n    image = Image.open(image_upload)\n    fixed = remove(image)\n    downloadable_image = convert_image(fixed)\n    st.download_button(\n        \"Download fixed image\", downloadable_image, \"fixed.png\", \"image/png\"\n    )\n\n\nHere is what you just did:\n\nYou imported three more packages:\nio.BytesIO and PIL.Image to help with file types and memory management;\nrembg.remove to remove the backgrounds;\nYou added a ¬†convert_image() function to manage the file types for downloading.\n\nThat‚Äôs it! Upload your wallaby.png image and take a look at the app‚Ä¶\n\nDownload your image. The background has been removed like magic! ü™Ñ\n\nAwesome job. Your picture looks so good. üëç\n\nHow to put it all together in a neatly formatted Streamlit app ü§Ø\n\nWouldn‚Äôt it be nice to see the before-and-after image in the app? Streamlit makes it easy! Just add a few calls to st.image:\n\nfrom io import BytesIO\n\nimport streamlit as st\nfrom PIL import Image\nfrom rembg import remove\n\nst.title(\"Hello Upload!\")\n\n# Upload the file\nimage_upload = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n\n# Convert the image to BytesIO so we can download it!\ndef convert_image(img):\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    byte_im = buf.getvalue()\n    return byte_im\n\n# If we've uploaded an image, open it and remove the background!\nif image_upload:\n    # SHOW the uploaded image!\n    st.image(image_upload)\n    image = Image.open(image_upload)\n    fixed = remove(image)\n    downloadable_image = convert_image(fixed)\n    # SHOW the improved image!\n    st.image(downloadable_image)\n    st.download_button(\n        \"Download fixed image\", downloadable_image, \"fixed.png\", \"image/png\"\n    )\n\n\nYou can see the images now, but it‚Äôs a bit clunky to scroll down.\n\nWhat if they were next to each other in columns? Let‚Äôs move some components around so it looks more professional:\n\nPut upload and download on st.sidebar.\nUse st.columns to arrange the pre- and post-images side-by-side.\nUse st.set_page_config to set a widescreen layout and st.write for commentary.\nAdd a default image to get a quick preview of the app every time you open it and save wallaby.png in the same folder.\nfrom io import BytesIO\n\nimport streamlit as st\nfrom PIL import Image\nfrom rembg import remove\n\nst.set_page_config(layout=\"wide\", page_title=\"Image Background Remover\")\n\nst.write(\"## Remove background from your image\")\nst.write(\n    \":dog: Try uploading an image to watch the background magically removed. Full quality images can be downloaded from the sidebar. This code is open source and available [here](<https://github.com/tyler-simons/BackgroundRemoval>) on GitHub. Special thanks to the [rembg library](<https://github.com/danielgatis/rembg>) :grin:\"\n)\nst.sidebar.write(\"## Upload and download :gear:\")\n\n# Create the columns\ncol1, col2 = st.columns(2)\n\n# Download the fixed image\ndef convert_image(img):\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    byte_im = buf.getvalue()\n    return byte_im\n\n# Package the transform into a function\ndef fix_image(upload):\n    image = Image.open(upload)\n    col1.write(\"Original Image :camera:\")\n    col1.image(image)\n\n    fixed = remove(image)\n    col2.write(\"Fixed Image :wrench:\")\n    col2.image(fixed)\n    st.sidebar.markdown(\"\\\\n\")\n    st.sidebar.download_button(\n        \"Download fixed image\", convert_image(fixed), \"fixed.png\", \"image/png\"\n    )\n\n# Create the file uploader\nmy_upload = st.sidebar.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n\n# Fix the image!\nif my_upload is not None:\n    fix_image(upload=my_upload)\nelse:\n    fix_image(\"./wallaby.png\")\n\n\nHere‚Äôs how it looks:\n\nYour image is ready for your class presentation and your app is ready for showtime! Don‚Äôt forget to add your app to a GitHub repo (follow these steps), then post it on Twitter, and tag @streamlit. üéà\n\nWrapping up üéÅ\n\nCongratulations! You can now remove backgrounds from images whenever you want to‚Äîfor free. And if you want to make your app even better, you can add:\n\nSupport for videos and GIFs.\nFile type conversion (to pick the file output type).\nImage sizing (how about making tiny images and turning them into emojis?).\n\nThat‚Äôs it. You‚Äôve earned it. Grab some ice cream üç¶ and take the rest of the day off.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit-Authenticator, Part 1: Adding an authentication component to your app",
    "url": "https://blog.streamlit.io/streamlit-authenticator-part-1-adding-an-authentication-component-to-your-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit-Authenticator, Part 1: Adding an authentication component to your app\n\nHow to securely authenticate users into your Streamlit app\n\nBy Mohammad Khorasani\nPosted in Advocate Posts, December 6 2022\nHow to install Streamlit-Authenticator\nHow to hash user passwords\nHow to create a login widget\nHow to authenticate users\nHow to implement user privileges\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Mohammad Khorasani, and I‚Äôm a co-author of the book Web Application Development with Streamlit (Streamlit helped me get into full-stack development, so it was only fair to spread the word about it).\n\nAs developers, we often require features that are yet to be made natively. For me, that was implementing user authentication and privileges in a client-related project that eventually grew into a full-fledged package aptly named Streamlit-Authenticator.\n\nSpecifically, my client asked for the ability to authenticate users with different privilege levels for their business needs, as well as a whole host of other features. That‚Äôs what prompted me into developing this package. While authentication is definitely needed for some apps‚Äîespecially corporate ones‚Äîit's great to make your apps accessible to the community whenever possible to share and spread the learnings!\n\nIn this two-part tutorial, you‚Äôll learn:\n\nHow to install Streamlit-Authenticator\nHow to hash user passwords\nHow to create a login widget\nHow to authenticate users\nHow to implement user privileges\nüëâ\nTL;DR? Here's the repo code.\nHow to install Streamlit-Authenticator\n\nLet‚Äôs start by creating a login form to authenticate a list of predefined users. Install the component by using the following command:\n\npip install streamlit-authenticator\n\n\nNext, import the component into your Python script:\n\nimport streamlit_authenticator as stauth\n\nHow to hash user passwords\n\nIt‚Äôs absolutely vital to hash any password that will be stored on a disk, database, or any other medium. Here, you‚Äôll be defining your users‚Äô credentials in a YAML file.\n\nYou‚Äôll also define several other configuration settings pertaining to the key and expiry date of the re-authentication JWT cookie. If you don‚Äôt require passwordless re-authentication, just set the expiry_days to 0.\n\nYou can also define a preauthorized list of users who can register their usernames and passwords (I‚Äôll cover this in the next post in this series).\n\nStep 1. Create the YAML file:\n\ncredentials:\n  usernames:\n    jsmith:\n      email: jsmith@gmail.com\n      name: John Smith\n      password: abc # To be replaced with hashed password\n    rbriggs:\n      email: rbriggs@gmail.com\n      name: Rebecca Briggs\n      password: def # To be replaced with hashed password\ncookie:\n  expiry_days: 30\n  key: random_signature_key # Must be string\n  name: random_cookie_name\npreauthorized:\n  emails:\n  - melsby@gmail.com\n\n\nStep 2. Use the Hasher module to convert your plain text passwords into hashed passwords:\n\nhashed_passwords = stauth.Hasher(['abc', 'def']).generate()\n\n\nStep 3. Replace the plain text passwords in the YAML file with the generated hashed passwords.\n\nHow to create a login widget\n\nNow that you‚Äôve defined your users‚Äô credentials and configuration settings, you‚Äôre ready to create an authenticator object.\n\nStep 1. Import the YAML file into your script:\n\nimport yaml\nfrom yaml.loader import SafeLoader\nwith open('../config.yaml') as file:\n    config = yaml.load(file, Loader=SafeLoader)\n\nStep 2. Create the authenticator object:\n\nauthenticator = Authenticate(\n    config['credentials'],\n    config['cookie']['name'],\n    config['cookie']['key'],\n    config['cookie']['expiry_days'],\n    config['preauthorized']\n)\n\n\nStep 3. Render the login widget by providing a name for the form and its location (i.e., sidebar or main):\n\nname, authentication_status, username = authenticator.login('Login', 'main')\n\nHow to authenticate users\n\nOnce you have your authenticator object up and running, use the return values to read the name, authentication_status, and username of the authenticated user.\n\nYou can ppt-in for a logout button and add it as follows:\n\nif authentication_status:\n    authenticator.logout('Logout', 'main')\n    st.write(f'Welcome *{name}*')\n    st.title('Some content')\nelif authentication_status == False:\n    st.error('Username/password is incorrect')\nelif authentication_status == None:\n    st.warning('Please enter your username and password')\n\n\nOr you can access the same values through a session state:\n\nif st.session_state[\"authentication_status\"]:\n    authenticator.logout('Logout', 'main')\n    st.write(f'Welcome *{st.session_state[\"name\"]}*')\n    st.title('Some content')\nelif st.session_state[\"authentication_status\"] == False:\n    st.error('Username/password is incorrect')\nelif st.session_state[\"authentication_status\"] == None:\n    st.warning('Please enter your username and password')\n\n\nYou can also alter the user if their credentials are incorrect:\n\nAfter logging out, the authentication_status will revert back to None and the authentication cookie will be removed.\n\nHow to implement user privileges\n\nGiven that the authenticator object returns the username of your logged-in user, you can utilize that to implement user privileges where each user receives a more personalized experience as shown below:\n\nname, authentication_status, username = authenticator.login('Login', 'main')\nif authentication_status:\n    authenticator.logout('Logout', 'main')\n    if username == 'jsmith':\n        st.write(f'Welcome *{name}*')\n        st.title('Application 1')\n    elif username == 'rbriggs':\n        st.write(f'Welcome *{name}*')\n        st.title('Application 2')\nelif authentication_status == False:\n    st.error('Username/password is incorrect')\nelif authentication_status == None:\n    st.warning('Please enter your username and password')\n\n\nAs you can see, each user is redirected to a separate application to cater to that specific user‚Äôs needs.\n\nWrapping up\n\nI hope you feel confident now about securely authenticating users into your Streamlit application. In subsequent posts, you‚Äôll learn how to register new users, reset usernames/passwords, and update user data.\n\nIn the meantime, feel free to read more about this component in our book Web Application Development with Streamlit. And if you have any questions, please leave them in the comments below or contact me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Find the top songs from your high school years with a Streamlit app",
    "url": "https://blog.streamlit.io/find-the-top-songs-from-your-high-school-years-with-a-streamlit-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nFind the top songs from your high school years with a Streamlit app\n\nUse the Spotify API to generate 1,000+ playlists!\n\nBy Robert Ritz\nPosted in Advocate Posts, December 8 2022\nHow to scrape Billboard Hot 100 top-ten singles\nHow to make 1000+ playlists with the Spotify API\nHow to build an app to present the playlists to the users\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nYour headphones are on. Your favorite Spotify playlist is on shuffle. You like the music, but something is off. You keep hitting \"next,\" searching for that special feeling. Finally, you start scrolling through your algorithmically created playlists. Spotify made them just for you, but they don't feel like you. They feel like a stereotype.\n\nSound familiar? You're not alone. Here is what I realized‚Ä¶\n\nThe recommended playlists feel off because they play the same songs!\n\nI wanted the music to transport me to a unique time and place in my life‚Äîlike high school. So I built a Streamlit app to help discover old favorites. Choose the years and get the Spotify playlist link with the top songs.\n\nIn this post, I'll show you:\n\nHow to scrape Billboard Hot 100 top-ten singles\nHow to make 1,000+ playlists with the Spotify API\nHow to build an app to present the playlists to the users\n\nReady to go? Let's get into it!\n\nüí°\nWant to skip straight to the app? Here is the app and the code on Github.\nHow to scrape Billboard Hot 100 top-ten singles\n\nTo start, let's get some song data. Helpfully, Wikipedia lists Hot 100 singles for the years 1958-2022. Use Python (and pandas) to scrape the song lists from the corresponding Wikipedia pages and store them in a CSV file.\n\nFor example, here is the 1958 year page (with a song list HTML table):\n\nThe page URLs differ by year so that you can generate each URL with a bit of code:\n\n# We will need these later so go ahead and import them now\nimport pandas as pd\nimport numpy as np\n\nurls = []\nfor year in range(1958, 2023):\n    urls.append(f\"<https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_{year}>\")\n\n\nIf you haven't already, install pandas, numpy, spotipy, and lxml by using PyPi (you can run it in your notebook or your preferred environment manager):\n\n!pip install pandas numpy spotipy lxml\n\n\nNext, write a loop to go to each URL, grab the HTML table with the pandas read_html method, and extract the table with the top songs.\n\nSince every page has multiple HTML tables, I needed to figure out how to find the one with the song data. I settled on finding the largest table by the row count as it contained the most songs.\n\nHere is the loop with the code to clean up the tables:\n\nrows = []\nfor url, year in zip(urls, range(1958, 2023)):\n    print(year)\n    dfs = pd.read_html(url)\n\n    row_num = []\n    for df in dfs:\n        row_num.append(df.shape[0])\n    \n    year_df = dfs[row_num.index(max(row_num))]\n    year_df = year_df.iloc[:,:6]\n\n    columns = ['entry_date','title','artist','peak','peak_date','weeks_top_ten']\n    year_df.columns = columns\n\n    year_df = year_df[~pd.to_numeric(year_df['peak'], errors='coerce').isna()].reset_index(drop=True)\n    year_df['year'] = year\n    rows.extend(year_df.to_dict(orient='records'))\n\ndf = pd.DataFrame(rows)\n\n\nThe resulting pandas DataFrame has over 5,000 songs and columns for the title, the artist, and the year:\n\nSong names by themselves aren't useful. To make a Spotify playlist, you need to know the Spotify URI for the song. It's a unique identifier that Spotify uses for each track, album, or playlist. Let's use the Spotify API plus a very convenient package, Spotipy (get it? üòâ), to look up the URIs.\n\nBut first, clean up the song titles (Spotify's API doesn't like certain characters) with some code:\n\nto_replace = [\"/\", \"\\\\\\\\\", \"(\", \")\", \"'\", \":\", \".\"]\n\nfor item in to_replace:\n    df['title'] = df['title'].str.replace(item,\"\", regex=False)\n\n\nNow import Spotipy and make a loop to look up each song title from the search endpoint. As this endpoint doesn't require user authentication, you can use the SpotifyClientCredentials class to create a server-to-server authentication. Later you'll need to complete user authentication to create your playlists.\n\nThe loop below converts your pandas DataFrame to a list of dictionaries and loops through it. The loop queries the API using the track title and the artist name for each song list item. This returns multiple results. I naively chose to grab and save the first URI!\n\nIt's possible that a similar artist and song name could give the wrong result‚Äîlike a remix or a re-release of the song. I tested this on about 20 songs, and it returned the correct result each time, so I decided this simple approach worked for the app.\n\nAfter you grab the URI, it gets saved in the row dictionary. Once the loop is complete, save the list of dictionaries back to the pandas DataFrame:\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n\nlist_df = df.to_dict(orient='records')\n\nfor item in list_df:\n    try:\n        result = spotify.search(\n            f\"\"\"track:{item['title']} artist:{item['artist']}\"\"\", type=\"track\", limit=1\n        )\n        if len(result[\"tracks\"][\"items\"]) > 0:\n            item[\"spotify_uri\"] = result[\"tracks\"][\"items\"][0][\"uri\"]\n        else:\n            item[\"spotify_uri\"] = np.nan\n    except spotipy.client.SpotifyException as e:\n        item[\"spotify_uri\"] = str(e.http_status) + \" - \" + e.msg\n\ndf = pd.DataFrame(list_df)\n\n\nTo clean up the results, drop the rows with NaN values (empty cells) and the rows where the API has returned \"Not found\":\n\ndf = df.dropna()\ndf = df[~df['spotify_uri'].str.contains(\"Not found\")]\n\n\nYour songs are now ready to go with the included URI! Save the results in a CSV file:\n\ndf.to_csv('songs.csv', index=False)\n\nHow to make 1000+ playlists with the Spotify API\n\nBefore creating playlists, head to the Spotify Developer Dashboard and make an app. Take note of the client ID and the secret. Set them as environment variables so Spotipy can use them when authenticating:\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nimport os\n\nos.environ[\"SPOTIPY_CLIENT_ID\"] = your client ID\nos.environ[\"SPOTIPY_CLIENT_SECRET\"] = your client secret\nos.environ[\"SPOTIPY_REDIRECT_URI\"] = <http://localhost:8080>\n\n\nNext, authenticate with OAuth as a registered Spotify user (who you'll make playlists for) to access user data via the API:\n\nStep 1. Get an authorization URL to sign in and receive a token:\n\n# Sets up authentication and scope.\nauth_manager = SpotifyOAuth(scope=\"playlist-modify-public\", \n                            open_browser=False)\n\nauth_manager.get_authorize_url()\n\n\nCopy the link and paste it into the browser. After you sign in with your Spotify account, you'll be redirected to your local machine. In the URL bar, you'll see your authentication code:\n\nStep 2. Copy the code and paste it in the \"token here\" spot in the code below:\n\nauth_manager.get_access_token(\"token here\", as_dict=False)\n\n\nYou're now authenticated and can interact with your Spotify account via the API. See if it worked by checking the current user:\n\nspotify = spotipy.Spotify(auth_manager=auth_manager)\n\nuser_dict = spotify.current_user()\nuser_dict\n\n# Output - If it worked you should see a dictionary like this.\n{'display_name': 'Datafantic',\n 'external_urls': {'spotify': '<https://open.spotify.com/user/31s4ct55ob6xjoghw4uxspvyu34u>'},\n 'followers': {'href': None, 'total': 0},\n 'href': '<https://api.spotify.com/v1/users/31s4ct55ob6xjoghw4uxspvyu34u>',\n 'id': '31s4ct55ob6xjoghw4uxspvyu34u',\n 'images': [],\n 'type': 'user',\n 'uri': 'spotify:user:31s4ct55ob6xjoghw4uxspvyu34u'}\n\n\nSee your account info? You're good to go. Save your created playlists in a CSV file, then make an empty pandas DataFrame and save it to playlists.csv (you'll add the column names later):\n\npd.DataFrame(columns=['id','name','link']).to_csv(\"playlists.csv\", index=False)\n\n\nTo create playlists, I made some convenience functions (to simplify things and prevent a giant loop that's hard to debug):\n\nThe main function, make_playlist, takes the start and the end year, finds songs from those years, and creates the playlist.\nAnother function, check_playlists, checks if the playlist has already been made. I did this after accidentally making 20 playlists and then having to delete them one by one on the web.\nFinally, save_playlists takes the created playlist and adds it to the playlists.csv file which looks up the playlists on the app:\ndef check_playlists(playlist_name, playlists_df):\n    \"\"\"Checks if playlist has already been created.\"\"\"\n    result = playlists_df[playlists_df['name'] == playlist_name].shape[0]\n    if result > 0:\n        return False\n    else:\n        return True\n\ndef save_playlists(playlist, playlist_name, playlists_df):\n    \"\"\"Saves playlist in a csv file.\"\"\"\n    new_playlist = {\n        'name':playlist_name,\n        'id':playlist['id'],\n        'link':playlist['external_urls']['spotify']\n    }\n\n    playlists_df = pd.concat([playlists_df, pd.DataFrame([new_playlist])])\n    playlists_df.to_csv(\"playlists.csv\", index=False)\n\ndef make_playlist(start_year, end_year):\n    \"\"\"Makes the playlist and adds tracks from songs.csv given the start and end year\"\"\"\n    playlists_df = pd.read_csv(\"playlists.csv\")\n    if (start_year - end_year) == 0:\n        playlist_name = f\"Top US Singles: {start_year}\"\n    else:\n        playlist_name = f\"Top US Singles: {start_year}-{end_year}\"\n\n    if check_playlists(playlist_name, playlists_df):\n        description = 'This playlist was generated automatically for a project on Datafantic.com.'\n        playlist = spotify.user_playlist_create(user=user_dict['id'], \n                                                name=playlist_name,\n                                                description=description)\n        uris = list(df[(df['year'] >= start_year) & (df['year'] <= end_year)]['spotify_uri'].values)\n\n        chunked_uris = np.array_split(uris, math.ceil(len(uris) / 100))\n        for uri_list in chunked_uris:\n            spotify.user_playlist_add_tracks(user=user_dict['id'], \n                                            playlist_id=playlist['id'], \n                                            tracks=uri_list)\n\n        save_playlists(playlist, playlist_name, playlists_df)\n\n\nWith these functions, you can create playlists for arbitrary start and end years.\n\nI initially thought to allow users to generate these playlists on the fly as they interact with the app. But that would take too much work to debug and ensure it was working in the cloud. So I opted to batch-create playlists for a list of start/end years.\n\nI generated a list of start/end years with a 1 to 20 years gap. Since we can't make playlists into the future, I slice the years' list minus the gap to ensure no playlists can be made past 2022:\n\nyears = list(range(1958, 2023))\n\nyear_pairs = []\nfor width in range(0,21):\n    for year in years[:-width]:\n        year_pairs.append((year, year+width))\n\n\nNow you can use your make_playlist function above to generate your playlists:\n\nfor pair in year_pairs:\n    make_playlist(pair[0], pair[1])\n\n\nIf you noticed, I left out single-year playlists (only 1958 or only 2022). The year_pairs list didn't include single years. Luckily there are convenience functions that you can loop through the list of years and make the playlists:\n\nfor year in years:\n    make_playlist(year, year)\n\n\nI was impressed with how fast the Spotify API performed. It took about two seconds to create each playlist and about 43 minutes to run the loop of over 1,000 playlists. You can multi-thread this to speed things up (and have enough time for a snack üòâ).\n\nOnce complete, you'll see the CSV file with all your playlists. Visit the links and see your playlist live on Spotify:\n\nNow let's move on to building an app!\n\nHow to build an app to present the playlists to the users\n\nThe Streamlit app is super simple:\n\nStart by making a new Python file (call it streamlit_app.py or whatever you like). It'll define the structure and content of your app. You'll be adding the code step by step.\n\nI like adding a title and some descriptive text at the top of apps to give the user some context. In this Python file, write the imports you need and use the st.markdown component to add text:\n\nimport pandas as pd\nimport streamlit as st\n\nst.markdown(\"\"\"# What songs were popular when I was in high school?\nThe algorithm doesn't get you, we get that a lot. Maybe you want to rediscover the top songs from your high school days. Or maybe you just don't want to mess with making your own playlist. \n\nYou can use this tool to find a pre-generated playlist of every song that made the Top 10 in the US for the years you select. \n\nThis originally appeared on [Datafantic.com](<https://www.datafantic.com/what-songs-were-popular-when-i-was-in-high-school>).\n\"\"\")\n\n\nNext, import your playlist.csv file by using Pandas with the read_csv method. Add a year slider for the user to define their playlist years. By passing a tuple to the value parameter (value=(1995, 2010)) you get two handles on the slider, allowing a minimum and a maximum year to be selected:\n\ndf = pd.read_csv(\"playlists.csv\")\nyears = list(range(1958, 2022))\n\nyear_range = st.slider(label=\"Start Year\", \n                       min_value=1958, \n                       max_value=2022, \n                       value=(1995, 2010))\n\n\nNext, add a submit button. When clicked, it'll filter the DataFrame to find the playlist based on the slider values. The playlist link will be added to Markdown and displayed to the user. If a playlist isn't found, the user will see a simple error message:\n\nif st.button('Submit'):\n    if (int(year_range[0]) - int(year_range[1])) == 0:\n        playlist_name = f\"Top US Singles: {year_range[0]}\"\n    else:\n        playlist_name = f\"Top US Singles: {year_range[0]}-{year_range[1]}\"\n\n    if df[df['name'] == playlist_name].shape[0] > 0:\n        playlist = df[df['name'] == playlist_name].to_dict(orient='records')[0]\n    else:\n        playlist = \"Ooops, it looks like we didn't make that playlist yet. Playlists with a range of 1-20 years were created. Try again with a more narrow year range.\"\n\n    if isinstance(playlist, dict):\n        link = f\"### Your Spotify Playlist: [{playlist['name']}]({playlist['link']})\"\n        st.markdown(link, unsafe_allow_html=True)\n    else:\n        st.markdown(playlist)\n\nWrapping up\n\nI made this Streamlit app for my blog, datafantic.com. I analyze the top songs and how they have changed in this blog post. After publishing my app, I was surprised to get so much positive feedback.\n\nInitially, I wanted to improve Spotify recommendations. Algorithmic song recommendations gave me a bad experience (I couldn't find what I wanted). Turns out, even simple curated playlists are immensely valuable to people. Algorithms stick us in a box‚Äîand not everyone likes that.\n\nI might extend this app in the future to:\n\nLet the user enter the location. Regional differences in song preferences exist, but you'd need data to do this.\nLet the user sign in with Spotify to get a playlist based on their recent play history.\nGive the user some context about how their current play history compares to the top charts by using their Spotify sign-in.\n\nI hope you enjoyed this app and are inspired to build cool Streamlit apps to share with the world. Thanks for reading! üôè\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Create a color palette from any image",
    "url": "https://blog.streamlit.io/create-a-color-palette-from-any-image/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCreate a color palette from any image\n\nLearn how to come up with the perfect colors for your data visualization\n\nBy Siavash Yasini\nPosted in Advocate Posts, January 19 2023\nHow to construct an image-loading component\nGallery view\nFile uploader\nURL downloader\nHow to build an image enhancement component with sliders\nShow the image\nHow to cluster pixels and use group averages to make a palette\nHow to use the color picker widget\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Siavash Yasini, and I‚Äôm a Senior Data Scientist at Zest AI.\n\nLike you, I love getting my hands on a new dataset, exploring it, and learning from it. But raw numbers by themselves aren‚Äôt great storytellers. Our primal brains are wired for lines, shapes, and colors. That‚Äôs why numbers need to be visualized to tell a good story.\n\nThe color palette of your data visualization is a crucial component that can make or break your data story. If you're like me, you've probably spent too much time trying to find the perfect set of colors. While creating a custom color palette can be exacting and time-consuming, you don‚Äôt have to do it alone. I built an app that can create a color palette from any image‚Ää‚Äî‚Ääa painting, a movie poster, or a Christmas family photo!\n\nSophisticated Palette app in action\n\nIn this post, I'll teach you:\n\nHow to construct an image-loading component\nHow to build an image enhancement component with sliders\nHow to cluster pixels and use group averages to make a palette\nHow to use the color picker widget to display and modify the palette\n\nWant to try the app firsthand? You can check it out here and see the source code here.\n\nHow to construct an image-loading component\n\nFirst, load an image into your app to convert it to a color palette. You can do this in three ways:\n\nLoad from a pre-existing gallery of images or artworks\nUpload a new image file into the app using st.file_uploader()\nDownload a new image from a URL\n\nYou can make three different tabs using st.tabs() to switch between input modes:\n\nBecause of the way Streamlit runs the app (top-to-bottom), the input modes that come later will get higher priority and will overwrite the output of the previous loader. For example, since you ordered the input modes as Gallery ‚Üí File Uploader ‚Üí Image URL, if you save the image into a variable img, anything loaded by the gallery will be overwritten by the file uploader and URL downloader outputs.\n\nYou can add pop-up warnings for the user when they load an image with a loader lower in the hierarchy and an image is already loaded with a loader higher in the hierarchy.\n\nThis is what the code will look like:\n\n# define three tabs for the three loading methods\ngallery_tab, upload_tab, url_tab = st.tabs([\"Gallery\", \"Upload\", \"Image URL\"])\n\nwith gallery_tab:\n    ...\n\t\t\n\t# raise a warning if file uploader or URL downloader have already loaded an image \n    if st.session_state.get(\"file_uploader\"):\n        st.warning(\"To use the Gallery, remove the uploaded image first.\")\n    if st.session_state.get(\"image_url\"):\n        st.warning(\"To use the Gallery, remove the image URL first.\")\n\n    img = ...\n\nwith upload_tab:\n\n    img = ...\n        \n\t# raise a warning if the URL downloader has already loaded an image\n    if st.session_state.get(\"image_url\"):\n        st.warning(\"To use the file uploader, remove the image URL first.\")\n\nwith url_tab:\n\n\timg = ...\n        \n\n\nSo if you‚Äôre trying to load a pre-existing image from the gallery, but there is an existing link in the URL downloader, you‚Äôll need to remove it. It may not be the most elegant solution, but it works!\n\nGallery view\n\nFor the gallery view, save some images in a public repository and load them directly in the app (use GitHub, AWS S3, or Google Cloud Storage). The st.selectbox contains the names of the saved artworks, so the user can load them by selecting them from the dropdown menu.\n\nHere is what the implementation looks like:\n\nimport streamlit as st\nfrom PIL import Image\n\nwith gallery_tab:\n    options = list(gallery_dict.keys())\n    file_name = st.selectbox(\"Select Art\",\n                             options=options, \n                             index=options.index(\"Mona Lisa (Leonardo da Vinci)\")\n                             )\n    img_file = gallery_dict[file_name]\n\n    if st.session_state.get(\"file_uploader\"):\n        st.warning(\"To use the Gallery, remove the uploaded image first.\")\n    if st.session_state.get(\"image_url\"):\n        st.warning(\"To use the Gallery, remove the image URL first.\")\n\n    img = Image.open(img_file)\n\n\ngallery_dict is a dictionary with the filename and the image file path as key-value pairs, and PIL.Image.open() is used to load those files. The results are saved in a variable named img.\n\nFile uploader\n\nImplementing the file uploader is easy because there is already a Streamlit widget. It‚Äôs called (can you guess?) st.file_uploader()!\n\nHere is what the implementation looks like:\n\nwith upload_tab:\n    img_file = st.file_uploader(\"Upload Art\", key=\"file_uploader\")\n    if file is not None:\n        try:\n            img = Image.open(img_file)\n        except:\n            st.error(\"The file you uploaded does not seem to be a valid image. Try uploading a png or jpg file.\")\n    \n    if st.session_state.get(\"image_url\"):\n        st.warning(\"To use the file uploader, remove the image URL first.\")\n\n\nThis widget lets you upload a file that you can pass to PIL.Image.open() to load. This step may break if the file is not an image or has a format inconsistent with what PIL.Image expects. To prevent this, put the loading part into a try/except block.\n\n‚ö†Ô∏è\nI‚Äôm using this block as a catch-all to avoid unexpected errors when loading a file. I don‚Äôt recommend you use except without specifying the Exception type you‚Äôre trying to bypass. It can cause deadly errors to pass through the block silently, making it difficult for you to debug your code.\nURL downloader\n\nUsing the file uploader, the users have to find the image, download it locally, then upload it to the app. It sounds easy but is annoying in practice. To make this easy, add a URL downloader to the app so the user can copy the image link and paste it into the app. To do this, you need the requests module (gets you the URL contents) and the io.BytesIO function (makes the contents comprehendible by PIL.Image.open()).\n\nThe implementation is similar to what you did for the file uploader:\n\nimport requests\nfrom io import BytesIO\n\nwith url_tab:  \n    url = st.text_input(\"Image URL\", key=\"image_url\")\n    \n    if url != \"\":\n        try:\n            response = requests.get(url)\n            img = Image.open(BytesIO(response.content))\n        except:\n            st.error(\"The URL does not seem to be valid.\")\n\nHow to build an image enhancement component with sliders\n\nNow that you have the image uploaded, you‚Äôre ready to infer the color palette, right? Not exactly.\n\nThe image may not be optimized for color inference. The colors might be too dull, and the image might not have enough brightness or contrast. That‚Äôs why you need to make some adjustments:\n\nTo do this, use PIL.ImageEnhance. The API is very simple. For example, if you want to enhance the color of the image by a factor of 2.5, you can run:\n\nimg = ImageEnhance.Color(img)\nimg = img.enhance(2.5)\n\n\nReplace Color with Shapness, Contrast, or Brightness to adjust these image attributes respectively.\n\nYou could create four sliders that assign values to each of these attributes and write separate blocks of code. But your code won't be beautiful and DRY (Don‚Äôt Repeat Yourself). So let‚Äôs approach this methodically. Define a dictionary that contains all the enhancements as keys, with values indicating the ranges and step sizes of the sliders:\n\nenhancement_range = {\n  # \"enhancement_type\": [min, max, step_size]\n\n    \"Color\": [0., 5., 0.2], \n    \"Sharpness\": [0., 3., 0.2], \n    \"Contrast\": [0.5, 1.5, 0.1], \n    \"Brightness\": [0.5, 1.5, 0.1]\n}\n\nenhancement_categories = enhancement_range.keys()\n\n# put adjustment sliders inside an expander \nenh_expander = st.sidebar.expander(\"Image Enhancements\", expanded=False)\n\n# create a reset button that resets all enhancements to default value (1.0)\nwith enh_expander:\n    if st.button(\"reset\"):\n        for cat in enhancement_categories:\n            if f\"{cat}_enhancement\" in st.session_state:\n                st.session_state[f\"{cat}_enhancement\"] = 1.0\n\n# create sliders for each enhancement category using the dictionary values (min, max, step_size)\nenhancement_factor_dict = {\n    cat: enh_expander.slider(f\"{cat} Enhancement\", \n                            value=1., \n                            min_value=enhancement_range[cat][0], \n                            max_value=enhancement_range[cat][1], \n                            step=enhancement_range[cat][2],\n                            key=f\"{cat}_enhancement\")\n    for cat in enhancement_categories\n}\n\n\nThis way, if you want to change the enhancement type or the range of values, you need to change only the original dictionary.\n\nLet‚Äôs apply the values to the image using ImageEnhance:\n\nfrom PIL import ImageEnhance\n\nfor cat in enhancement_categories:\n\t# apply the enhancement class to the image\n\t# e.g. for cat='Color' this would be the same as \n    # img = ImageEnhance.Color(img)\n    img = getattr(ImageEnhance, cat)(img)\n\t\t\n\t# apply the enhencement value from the corresponding st.slider\n    img = img.enhance(enhancement_factor_dict[cat])\n\n\nShow the image\n\nThe only thing left is to show the image on the app using st.image():\n\nwith st.expander(\"üñº  Artwork\", expanded=True):\n    st.image(img, use_column_width=True)\n\n\nAnd voil√†!\n\nHow to cluster pixels and use group averages to make a palette\n\nNow, onto the fun stuff. The idea here is simple. An image is a collection of pixels that have three values assigned to them: red (R), green (G), and blue (B). The pixel‚Äôs location on the canvas is irrelevant. What matters is where it‚Äôs located within the RGB coordinate space.\n\nLet‚Äôs decompose the image and get rid of the pixel location:\n\nr, g, b = np.array(img).reshape(-1, 3).T\ndf_rgb = pd.DataFrame({\"R\": r, \"G\": g, \"B\": b}).sample(n=sample_size)\n\n\n\nGroup the pixels that are close together and use the average RGB values to represent each group (a single color). For example, if you want to make a five-color palette from Mona Lisa, look at the pixel distribution (here it‚Äôs projected into 2D using the PCA algorithm):\n\nThen select five clusters and assign the average value of each cluster to a palette slot:\n\nTo do this, use a machine learning algorithm K-means clustering‚Äîsklearn.cluster.KMeans. You need to provide only the number of clusters (your palette size).\n\nThis is what the implementation looks like:\n\nfrom sklearn.cluster import KMeans\n\npalette_size = st.sidebar.number_input(\"palette size\", \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t min_value=1, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t max_value=20, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   value=5, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t step=1, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t help=\"Number of colors to infer from the image.\")\n\nmodel = KMeans(n_clusters=palette_size)\nclusters = model.fit_predict(df_rgb)\n        \npalette = model.cluster_centers_.astype(int).tolist()\n\n\nThat‚Äôs it! You now have your color palette.\n\n‚ÑπÔ∏è\nYou used the popular RGB pixel decomposition, but it‚Äôs not the only way to decompose colors. There is also the clustering of pixels in the Hue, Saturation, and Value (HSV) space. It distributes the pixels differently and leads to a different color palette.\nHow to use the color picker widget\n\nIt's time to try the amazing st.color_picker() widget! Use it to adjust your colors if they're not 100% perfect:\n\nTo avoid the palette taking up half the page, put each color into a separate column:\n\ncolumns = st.columns(palette_size)\nfor i, col in enumerate(columns):\n    with col:        \n        st.session_state[f\"col_{i}\"]= \\\\\n\t\t\t\t    st.color_picker(label=str(i), \n                    value=palette[i], \n                    key=f\"pal_{i}\")\n\n\nBeautiful!\n\nFinally, provide the user with a matplotlib or plotly code snippet (so they don't have to copy-paste every single hex code into their coding environment):\n\nThanks to st.code() widget, you can copy the whole code block with one click. üòÑ\n\nWrapping up\n\nCongratulations! You've learned how to build an app that can show you what colors Leonardo Da Vinci used to put that smile on Mona Lisa's face. You've also learned how to use st.image, st.tabs, st.file_uploader, and st.color_picker widgets. If you want to learn even more, check out the source code.\n\nI'd love to hear your thoughts, questions, comments, and feedback. Get in touch with me on LinkedIn or through my website.\n\nHappy app-building! üë∑\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Assistant_grid_scaling-1.0_fps-35_speed-10.0_duration-0-7.gif (760√ó338)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Assistant_grid_scaling-1.0_fps-35_speed-10.0_duration-0-7.gif#browser",
    "html": ""
  },
  {
    "title": "Visualize Object Detection Output | Use Roboflow & Streamlit",
    "url": "https://blog.streamlit.io/how-to-use-roboflow-and-streamlit-to-visualize-object-detection-output/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to use Roboflow and Streamlit to visualize object detection output\n\nBuilding an app for blood cell count detection\n\nBy Matt Brems\nPosted in Advocate Posts, February 23 2021\nHow to fit an object detection model in Roboflow\nNow you‚Äôre ready to build a model\nHow to use an API to access the model and predictions\nHow to create and deploy a Streamlit app\nContents\nShare this post\n‚Üê All posts\n\nMost technology is designed to make your life, or your work, easier. If your work involves building computer vision into your applications, using the Roboflow platform gives you everything you need.\n\nStreamlit is an open-source platform that enables you to convert your Python scripts to apps and deploy them instantly. Streamlit and Roboflow can work hand-in-hand, allowing you to tackle computer vision problems and visualizing your output so you can make better decisions faster.\n\nIn this post, we‚Äôll walk you through using Roboflow and Streamlit together by showing you how to:\n\nFit an object detection model in Roboflow\nUse an API to access the model and its predictions\nCreate and deploy a Streamlit app\n\nSpecifically, we‚Äôll be working with a common blood cell count and detection dataset. If you want to skip right to playing with it, here's an interactive app and this is the code.\n\nWe‚Äôll build an object detection model that detects platelets, white blood cells, and red blood cells. Then, the app we develop together will allow you to make predictions with your object detection model, visualize those predictions at a given confidence level, and edit those predictions based on your preferred confidence level with immediate visual feedback.\n\nHow to fit an object detection model in Roboflow\n\nHave you fit an object detection model before?\n\nEven if you haven't, Roboflow helps you work through all aspects of computer vision, from uploading, annotating, and organizing your images to training and deploying a computer vision model.\n\nWe believe you shouldn‚Äôt have to be a data scientist or need an extensive coding background to be able to use computer vision. You have everything you need right now.\n\nThe computer vision workflow.\n\n\nIf you don‚Äôt already have a Roboflow account, you‚Äôll need to head over to Roboflow and create one. If you‚Äôd like to start training your model from a public dataset, Roboflow has a great tutorial that describes how to improve your model more quickly. (Or, you can upload your own dataset!)\n\nOnce you have an account, go to our computer vision datasets page. We‚Äôve made over 30 datasets of different types public and keep adding more.\n\nThe one we‚Äôll walk through today is a blood cell count and detection dataset.\n\nAfter you‚Äôve decided which dataset to use, go ahead and fork it. That will create a copy of the dataset that you can now use.\n\nAt this point, you can directly fit a model. However, we recommend that you preprocess and augment your images.\n\nImage preprocessing. Deterministic steps performed to all images prior to feeding them into the model. For example, you might resize your images so they are all the same size, or convert your images to grayscale.\nImage augmentation. Creating more training examples by distorting your input images so your model doesn't overfit on specific training examples. For example, you may flip, rotate, blur, or add noise to your images. The goal is to get your model to generalize better to ‚Äúthe real world‚Äù when you deploy your model.\n\nWith the blood cell count dataset I‚Äôm using, I chose the following preprocessing and augmentation options:\n\nWhen deciding whether to use a specific augmentation option, I asked myself the question ‚ÄúIs the augmented image a reasonable image for my model to see?‚Äù In this case, I added 90¬∞, 180¬∞, and 270¬∞ rotations to my image because the slide of cells could reasonably be rotated 90 degrees and still make sense.\n\nIt wouldn't make sense for all applications. For instance, I might not include that kind of rotation for a self-driving car, because stop signs should be seen with the pole jutting into the ground. To rotate the image 180 degrees would make the stop sign upside down and the ground where the sky should be -- that probably isn‚Äôt a very useful thing for my model to learn.\n\nI have my data split up so that 70% of my data is in the training set, 20% is in the validation set, and 10% is in the test set. As you may know, splitting your data into training, validation, and testing sets can really help avoid overfitting.\n\nI‚Äôve decided to create three augmentations. This means that, for each training image, we‚Äôll create three copies of that image, each with random augmentation techniques applied to it. This will give me a total of 874 images that are generated:\n\n765 augmented training images (765 = 255 * 3)\nplus 73 validation images\nplus 36 testing images.\n\nOnce you‚Äôre done with your preprocessing and augmentation, click ‚ÄúGenerate‚Äù in the top-right corner. Helpful hint: make sure to name your dataset something memorable!\n\nNow you‚Äôre ready to build a model\n\nTo build a model, it‚Äôs as easy as clicking ‚ÄúUse Roboflow Train.‚Äù\n\nGenerally, you need a Roboflow Train credit to do this. Reach out to us and we‚Äôll get you set up!\n\nYou‚Äôll have the option either to train from scratch or to start from a checkpoint.\n\nTrain from Scratch. This is the easier option. Just click and go! Your model will be built from scratch, using only the data you‚Äôve provided to it.\nStart from a Checkpoint. This option is a little more sophisticated and requires a related existing model. If you‚Äôve already built a model (or if there‚Äôs a public model) that has been fit on related data, then starting from a checkpoint allows you to use the existing model as your starting point. The model is additionally trained on your images. Two advantages to this are that your model will train more quickly, and you'll frequently see improved performance! This is known as transfer learning. However, this does require a related existing model, and we don't always have that.\n\nIn my case, I built my model from scratch, because I didn‚Äôt already have a related model.\n\nThat‚Äôs all it takes to fit a model in Roboflow. When all is said and done, if your data is already annotated and you don‚Äôt make many changes to the augmentations, it‚Äôs only a handful of clicks to go from your images to a trained computer vision model. We've also turned annotating images into a pretty fast process ‚Äì especially with model-assisted labeling.\n\nHow to use an API to access the model and predictions\n\nYou‚Äôll want to make sure your model performs well before getting too far into this.\n\nOur model seems to perform pretty well. Usually, we use mean average precision (mAP) to evaluate object detection models. The closer your mAP is to 100%, the better! It‚Äôs also helpful to look at your model‚Äôs performance by class to make sure your object detection model isn‚Äôt performing significantly worse for one subset of objects.\n\nIf your model isn‚Äôt performing the way you want, you may want to work on improving that before you proceed. We usually see dramatic improvements in models when people take one (or both) of the following two actions:\n\nImprove their labeling. Placing bounding boxes around the entire object, but as close to the edges of the object as possible, can improve your model‚Äôs performance.\nCorrecting for unbalanced classes. Having one or more classes that are severely underrepresented can make it harder for your model to properly identify those underrepresented classes. A basic example is if you show a child 5 pictures of a dog and 100 pictures of a cat, the child may not do a very good job of identifying a dog.\n\nNow that we‚Äôve fit a model, we can use that model to generate predictions on new images. The Roboflow Infer API is one of a few ways to conduct inference and that's what we‚Äôll use.\n\nIn order to use the API, we‚Äôll need a couple of pieces of information from Roboflow. Make sure you keep these both private. These are specific to you!\n\nThe model name: this should begin with rf.\nThe access token/API key: this should be a 12+ letter code.\n\nThis information can be found in multiple places. I like retrieving these from the Example Web App, because I‚Äôll also easily upload an image and test out my model from there. Once you have these pieces of information, you‚Äôll want to store them ‚Äì you‚Äôll need them momentarily.\n\nHow to create and deploy a Streamlit app\n\nDeploying a Streamlit app is easy. Even if you haven‚Äôt spent a lot of time focused on deploying apps before. (Here is the code I wrote to build the app.)\n\nFollowing Streamlit‚Äôs API documentation closely, I was able to build an app that:\n\nImported an image file from my computer\nAllowed the user to tweak parameters of our computer vision model\nShowed my imported image overlaid with the model‚Äôs predicted annotations\nCalculated and displayed summary statistics about the image and predictions\nGenerated a histogram of confidence levels for bounding boxes\n\nI chose to structure this in two physical components: a sidebar and the main area.\n\nSidebar. In the sidebar, the user gets to select a file to import from their local computer. This is where the user can select an image to pull into the app and edit the confidence and overlap thresholds used when generating predicted bounding boxes for the image.\n\n\nMain Area. In the main area, we have everything else I mentioned. The image that includes predictions, some statistics about the image and predictions itself, a histogram that shows the confidence levels for all bounding boxes, and a printout of the JSON that stores the bounding box annotations.\n\nThe three tools that were most helpful in putting this together were:\n\nst.write(): If I wanted to print anything on my screen, st.write() enabled me to do that easily. It supports Markdown, so I can use ## to control how large or small I want my headings to be. I also used f-strings when displaying summary statistics to have more control over how these rendered. For example, rounding off the mean confidence level after four decimal places instead of a long string of trailing decimals.\nst.sidebar(): I‚Äôm decidedly not a web developer. Rather than spending my time figuring out how to set aside the left side of the screen for the user and wrangling dimensions, defining a sidebar was literally as easy as st.sidebar(). Want to add something to the sidebar, like a slider or a way to upload your files? Try st.sidebar.slider() or st.sidebar.file_uploader(). The Streamlit API is set up so that your components stay where you want them.\nst.image() and st.pyplot(): Streamlit‚Äôs API is intuitive. If you want to insert an image into your app, you can do that with the st.image() function. Plot from pyplot? st.pyplot(). If you wanted to put an image into the sidebar, you‚Äôd change it from st.image() to st.sidebar.image().\n\nYou get the point. If you want to do something, you can probably just type st.that_thing_you_want_to_do()! If you want that thing to be in your sidebar, change it to st.sidebar.that_thing_you_want_to_do()!\n\nAfter writing my Python script and pushing to Github, I followed Streamlit‚Äôs instructions to deploy my app -- check the app out here!\n\nWant to learn more about some of the amazing apps developers are making with Streamlit? Check out their app gallery and community forum to find some inspiration for your next project.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "ai-talks-1.gif (725√ó1141)",
    "url": "https://blog.streamlit.io/content/images/2023/04/ai-talks-1.gif#border",
    "html": ""
  },
  {
    "title": "Gravitational-Wave Apps Help Students Learn About Black Holes",
    "url": "https://blog.streamlit.io/gravitational-wave-apps-help-students-learn-about-black-holes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nGravitational-wave apps help students learn about black holes\n\nExploring distant space with gravitational waves\n\nBy Jonah Kanner\nPosted in Advocate Posts, December 15 2020\nBarriers to learning\nEnter Streamlit\nWorking with data\nHow far can LIGO see?\nFinding signals in the noise\nBuilding the app\nData discovery and download\nData processing\nDisplay the plots\nThe big picture\nContents\nShare this post\n‚Üê All posts\n\nWritten by Jonah Kanner and Jameson Rollins of LIGO Laboratory, California Institute of Technology and Leo Singer of NASA Goddard Space Flight Center.\n\nGravitational-wave detectors - like LIGO and Virgo - are some of the newest tools being used to explore objects in distant galaxies. The current generation of detectors began observing in 2015, and since then, have published observations of 50 collisions of black holes and neutron stars. These exciting discoveries are changing the way astrophysicists are learning about a broad range of topics - including how stars evolve, the expansion rate of the universe, and the deep laws that describe gravity and other fundamental forces.\n\nLIGO and Virgo data are freely available through the Gravitational Wave Open Science Center (GWOSC). These public data sets are being used all over the world, by scientists, teachers, students, and artists, and have contributed to the publication of over 100 scientific articles over the past year.\n\nBarriers to learning\n\nThere‚Äôs one challenge encountered by everyone getting started with gravitational-wave data for the first time: ¬†most options for working with these data sets demand installing specialized software libraries and writing computer code to process and display the results. For professional scientists, this is often OK - a full-time researcher can afford to spend a few hours installing and learning new software, and likely has already had some experience programming. But for a much broader audience - including high school students, teachers, and artists - writing code in Python to just get started with gravitational-wave data is a significant barrier.\n\nWe‚Äôd experimented with a few different options to make this transition easier. In fact, cloud hosted Jupyter notebooks provided a big step forward, and we were able to show students how to write code to work with our data, without asking them to install any libraries on their own machines. But when we shared these notebooks with teachers and artists, they were not interested: they wanted some way to work with gravitational-wave data without using any code at all!\n\nMoreover, it's not just new students who can benefit from an easier method to access LIGO/Virgo data. Even though professional scientists often could write specialized code to make the plot they want, it‚Äôs not efficient to have lots of scientists re-writing code to make the most popular plots.\n\nEnter Streamlit\n\nKnowing that we had a need for a web app for the most common plots - both to broaden access and to improve research efficiency - we were excited to discover Streamlit. Python is very popular in our research community, so Streamlit apps can easily make use of some of the most cutting-edge modules used for gravitational-wave research. For example, pycbc and gwpy are packages used by many gravitational-wave researchers to process and display LIGO/Virgo data. With Streamlit, we can easily write apps that let students and scientists use these packages, without writing any code themselves. Because GWOSC provides data access through a simple API, we found that Streamlit apps can access data on demand, and so process and display any segment of public data.\n\nGetting started video from GWOSC Learning Paths - video created by Cardiff University Physics & Astronomy\n\nToday, we‚Äôre using two Streamlit apps to introduce students to LIGO/Virgo data as part of the GWOSC Learning Paths, and a third app, the Gravitational Wave Inspiral Range Calculator, allows scientists and astronomers to easily calculate how far into the universe current and future detectors can see.\n\nWorking with data\n\nTo help students and scientists get started working with gravitational-wave data, we wrote the GW Quickview App to make some common plots with any stretch of data. Users can instantly make plots by selecting a published gravitational-wave event from a list, or by entering any time LIGO and Virgo were running. The Quickview App has access to the full public archive of LIGO/Virgo data - around 30 TB and growing! Sliders allow the user to set filter and plotting options, and even to directly download the filtered data. In most cases, you can see the signal from a black hole merger or neutron star collision with just a few clicks. This app is a great way to take a first peak at a segment of data and explore the archive.\n\nHow far can LIGO see?\n\nOne of the most important things to know about a gravitational-wave detector is how sensitive it is to the kinds of signals it‚Äôs capable of detecting. Ground-based gravitational-wave detectors primarily target the mergers of black holes or neutron stars, also known as compact binary coalescences (CBCs). Since the amplitude of a CBC signal is inversely proportional to how far away it is, the question is usually asked in terms of how far away can we detect the signal from a given type of CBC. In LIGO‚Äôs first observing run (O1) the LIGO detectors could detect binary neutron star (BNS) mergers out to a distance of roughly 70 megaparcecs (Mpc), or 230 million light-years. As the detectors have improved, the so-called ‚Äúinspiral range‚Äù has increased as well; during the O3 run the LIGO BNS inspiral range was nearly 120 Mpc. Every time the range doubles the volume of space searched goes up by a factor of 8, and the event rate is proportional to volume. That‚Äôs a lot more events!\n\nWhile plenty of tools exist to calculate the range for a given detector, all of them required special access and knowledge to run. With Streamlit, we were able to create a simple web app tool for anyone to calculate the inspiral range for any type of CBC they wish, for any point of the past observing runs, or even for hypothetical detectors that are yet to be built. This app should be useful for LIGO scientists, for the general astronomy community, and for the public at large.\n\nFinding signals in the noise\n\nGravitational wave signals are typically buried in noise; finding them requires a few signal processing tricks. These signal processing concepts are new to many students, and can be a barrier to understanding data from LIGO and Virgo. Working with Professor Amber Stuver at Villanova, we created an app to let students try out some signal processing in an easy to use, interactive environment. The Signal Processing Tutorial asks visitors to apply high-pass filtering and whitening to noisy data, to find a secret sound hidden in the noise. Then, they can try their new skills on some real data, and try to recreate a well known plot showing the first gravitational-wave observation of a binary black hole merger. Following hints from a post by Pranav Pandit, we found we were able to speed up the app by porting plots from matplotlib to altair, so that students can adjust plot parameters and see updates almost instantly.\n\nBuilding the app\n\nWe built the GW Quickview App partly as an experiment, to see how easy it would be to run some of our favorite python modules in Streamlit. In this case, we tried using gwpy, which has a number of handy methods for finding, processing, and plotting LIGO and Virgo data.\n\nData discovery and download\n\nThe app code uses the gwosc client for data discovery, to get a list of all published gravitational-wave events, and find their GPS times:\n\n\nfrom gwosc import datasets\neventlist = datasets.find_datasets(type='events')\nchosen_event = st.sidebar.selectbox('Select Event', eventlist)\nt0 = datasets.event_gps(chosen_event)\n\n\n\nThen, it uses gwpy to download the data file with the corresponding time:\n\n\nfrom gwpy.timeseries import TimeSeries\nstrain = TimeSeries.fetch_open_data(detector, t0-14, t0+14, cache=False)\n\n\n\n\nThe strain data are stored in some pretty big files (up to 500 MB), so we used the cache feature of streamlit to allow users to change plot parameters without needing to repeat the data download.\n\nData processing\n\nTo make plots where you can see a signal in GW data, some signal processing steps are needed to reduce the impact of noise. The app uses the bandpass() and whiten() methods of gwpy to do this. Sliders allow the user to set the limits on the band-pass filter, and to toggle whitening on or off. The app also uses the gwpy q_transform method to make a beautiful time-frequency plot, which allows most detectable events to be clearly seen in the figure. Additional sliders allow setting plot parameters, so the user can fine-tune the time-frequency plot to look just right. We even used the new streamlit beta_expander() to allow the user to display hints about how to understand the plots and set the parameters.\n\nDisplay the plots\n\nFinally, the app displays several plots to visualize the data. We opted to use convenient methods in gwpy to make plots with matplotlib, which saved us from writing code to style and label the plots. That fact that Streamlit can easily display these plots with streamlit.pyplot() was a big plus for us, and the Streamlit team helped us fix some issues related to thread-safety when using this feature.\n\nThough we know that Streamlit apps run faster when plotting with altair (we used this trick in another app!), for the GW Quickview App, we chose to keep the matplotlib plots, mainly to demonstrate the handy plotting features built into gwpy.\n\nThe big picture\n\nA number of break-throughs in the past few years have proved that gravitational-wave detectors represent a new and exciting way to learn about our universe. To make use of this new technology, we‚Äôll need to train the next generation of scientists who will build the instruments of the future and study their observations. After getting started with Streamlit apps and GWOSC Learning Paths, we hope some students will move on to use other tools, such as large scale computing clusters, to participate in research activities. We‚Äôve had fun building apps that make gravitational-wave data easier to access, and we hope they will spark imagination and interest in this emerging field.\n\nSpecial thanks to Amey Deshpande, Randy Zwitch, and the Streamlit team for improving the apps and editing this blog post\n\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "database-1.png (1911√ó997)",
    "url": "https://blog.streamlit.io/content/images/2023/03/database-1.png#browser",
    "html": ""
  },
  {
    "title": "streamlit-app-starter-kit-22Sep2022.gif (2156√ó1492)",
    "url": "https://blog.streamlit.io/content/images/2022/09/streamlit-app-starter-kit-22Sep2022.gif#browser",
    "html": ""
  },
  {
    "title": "Avatar_divider_scaling-0.71_fps-60_speed-9.97_duration-4-7.gif (727√ó549)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Avatar_divider_scaling-0.71_fps-60_speed-9.97_duration-4-7.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (668√ó102)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Untitled--1-.png#border",
    "html": ""
  },
  {
    "title": "Untitled.png (720√ó287)",
    "url": "https://blog.streamlit.io/content/images/2023/03/Untitled.png#border",
    "html": ""
  },
  {
    "title": "assistant_no_context.jpg (787√ó421)",
    "url": "https://blog.streamlit.io/content/images/2023/03/assistant_no_context.jpg",
    "html": ""
  },
  {
    "title": "assistant_bot_details.jpg (762√ó716)",
    "url": "https://blog.streamlit.io/content/images/2023/03/assistant_bot_details.jpg",
    "html": ""
  },
  {
    "title": "assistant_chat_view.jpg (789√ó655)",
    "url": "https://blog.streamlit.io/content/images/2023/03/assistant_chat_view.jpg",
    "html": ""
  },
  {
    "title": "assistant_login_required.jpg (756√ó529)",
    "url": "https://blog.streamlit.io/content/images/2023/03/assistant_login_required.jpg",
    "html": ""
  },
  {
    "title": "8-slides-comparison.gif (1176√ó488)",
    "url": "https://blog.streamlit.io/content/images/2023/04/8-slides-comparison.gif#border",
    "html": ""
  },
  {
    "title": "confusion-matrix.png (497√ó415)",
    "url": "https://blog.streamlit.io/content/images/2023/04/confusion-matrix.png#border",
    "html": ""
  },
  {
    "title": "collecting-dataset.png (1140√ó275)",
    "url": "https://blog.streamlit.io/content/images/2023/04/collecting-dataset.png#border",
    "html": ""
  },
  {
    "title": "classification-report.png (489√ó172)",
    "url": "https://blog.streamlit.io/content/images/2023/04/classification-report.png#border",
    "html": ""
  },
  {
    "title": "designing-model-architecture.png (1579√ó383)",
    "url": "https://blog.streamlit.io/content/images/2023/04/designing-model-architecture.png#border",
    "html": ""
  },
  {
    "title": "3-chart-animation.gif (1140√ó536)",
    "url": "https://blog.streamlit.io/content/images/2023/04/3-chart-animation.gif#border",
    "html": ""
  },
  {
    "title": "2-chart-animation.gif (1140√ó536)",
    "url": "https://blog.streamlit.io/content/images/2023/04/2-chart-animation.gif#border",
    "html": ""
  },
  {
    "title": "",
    "url": "https://blog.streamlit.io/content/images/2023/04/4-code-structure-ipyvizzu.svg#border",
    "html": ""
  },
  {
    "title": "snowflake-to-acquire-streamlit.gif (1080√ó708)",
    "url": "https://blog.streamlit.io/content/images/2022/11/snowflake-to-acquire-streamlit.gif",
    "html": ""
  },
  {
    "title": "dbt Cloud & Streamlit App | How the Cazoo Data Team Built It",
    "url": "https://blog.streamlit.io/dbt-cloud-jobs-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nEasy monitoring of dbt Cloud jobs with Streamlit\n\nHow the Cazoo data science team built their dbt Cloud + Streamlit app\n\nBy Martin Campbell\nPosted in Advocate Posts, June 11 2021\nHow Streamlit fits in\nThe dbt Cloud + Streamlit app\nSidebar\nRAG Status\nFailed Steps\nHistorical Runs\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nThis post also appeared on the Cazoo Tech Blog, read more here.\n\nHere in the Cazoo data team, dbt is a core tool for taking raw data from our S3 datalake and transforming it into a form that is much more easily consumed by our reporting & analytics tools (Looker and AWS Athena to name just two). ¬†We run multiple jobs throughout the day to serve data that is somewhat near real time (updated hourly) to keep our internal customers informed on how the business is performing.\n\nWe use dbt cloud to schedule our jobs, which unfortunately comes with one reasonably frustrating issue - detailed information on the status of jobs (including any reasons for failure) is not available for standard users, only Admins get access. Ideally we want to expose this information to a wider audience without needing to grant admin access, both because that would increase costs, but also, most people don't need admin level access, a read only view on these jobs is more than sufficient. ¬†Fortunately dbt Cloud does provide an API, that allows us to pull some fairly detailed data from our jobs which we can use to, we just need some suitable way to display this.\n\nBelow we'll get into how we solved this, but if you want to jump right into the source code, here it is.\n\nHow Streamlit fits in\n\nThis is where Streamlit comes in. Streamlit is a fantastic way to build shareable web applications without any knowledge of front end technologies (which is rather handy for me, as whilst I can spell HTML and CSS that‚Äôs about as far as my knowledge goes). I‚Äôve tried similar libraries (such as Dash) in the past but Streamlit has blown us all away with how easy it is to use, and how nice the resulting apps look. ¬†Streamlit is open source, so you can deploy apps locally, or on your own infrastructure if you so wish. ¬†\n\nAt Cazoo, we have a broad strategy of making use of external expertise for running tools to lessen the overhead for our engineering teams, and so instead of deploying Streamlit on some Fargate instances at AWS we‚Äôve been enthusiastic participants in the ongoing Streamlit for Teams beta. ¬†This allows us to deploy our apps at the click of a button, and let someone else (the nice folks at Streamlit) worry about keeping the lights on. ¬†\n\nAs well as using Streamlit for the more obvious case of Analytics & Data Science apps - here in Data Engineering we‚Äôre also making use of it for apps where we want to expose operational information such as Dead Letter Queues, or in this case what‚Äôs happening with our dbt jobs.\n\nThe dbt Cloud + Streamlit app\n\nFull disclosure, I didn‚Äôt build this app, one of my colleagues Oliver Morgans did. I‚Äôm just helping out on this one.\n\nWe‚Äôve made the app available for anyone who wants to use it, and we‚Äôd certainly welcome any questions, comments or pull requests for ways to make this better. I‚Äôm sure as our analysts start using this they‚Äôll be making suggestions for things they want to see also. You can find the source code at here.\n\nDetailed instructions for getting up and running are in the repo README. ¬†You‚Äôll need to add a few things like your dbt Cloud account_id and API key into the streamlit secrets.toml, and you‚Äôll probably want to swap out the logo we‚Äôve stored in images too (unless you‚Äôre particularly enamoured with the Cazoo logo of course).\n\nWe have included basic username/password auth in the code, though if you‚Äôre using Streamlit for Teams I‚Äôd recommend you use the built in SSO as that‚Äôs a much nicer experience than typing in a long password each time.\n\nThe app is currently composed of a few pieces:\n\nSidebar\n\nYou can select to see all runs, successful runs or failed runs. And you can filter down by project (this project list is driven by a set of project ids you can add in secrets.toml)\n\nRAG Status\n\nWe‚Äôre pulling the last 500 job runs, and this table will let you sort by the columns to get at what you‚Äôre after.\n\nFailed Steps\n\nOf particular interest to our analysts, is the step (or steps) that a dbt job failed on, as most often that represents a unit test that has failed. So you have the option to select a specific run, and to see any failed steps in that run.\n\nThat blue text is indeed a link which will take you to GitHub and the specific file that threw the error. You can configure the base URL for your repo as part of the secrets.toml.\n\nHistorical Runs\n\nPicking a job name also exposes the last ten historical runs, which will also allow you to inspect them for failure steps if needed.\n\nWrapping up\n\nI am a huge fan of Streamlit, and the fact we‚Äôre able to easily use it not just for data science but for other operational things in engineering is making it a really key part of our toolbox. ¬†I‚Äôd welcome any comments below on what you would add to this app, or feel free to tell us if we‚Äôre doing it all wrong - because every day is a school day!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Prototype your app in Figma! üñåÔ∏è",
    "url": "https://blog.streamlit.io/prototype-your-app-in-figma/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nPrototype your app in Figma! üñåÔ∏è\n\nQuickly and easily design your app with the Streamlit Design system\n\nBy Jessi Shamis\nPosted in Tutorials, October 27 2022\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Jessi Shamis, and I‚Äôm a designer at Streamlit.\n\nWhen making apps, I was constantly trying to figure out their design before coding them. I wanted an extra step between coming up with an app idea and jumping straight into code. So I took the open-source library (most of st.commands) and made it into Figma components.\n\nNow I could prototype my apps before deciding how to build them! üéâ\n\nIn this post, I‚Äôll show you how to build a Streamlit app in Figma step-by-step:\n\nVisit our Streamlit Community page on Figma to see our Design System\nClick on Streamlit Design System and make yourself a copy\nGo to the ‚ÄúAssets‚Äù tab to find all your Streamlit components\nType ‚ÄúApp Base‚Äù in your search assets bar\nStart adding components!\n\nIf you want to check it out right away, here's our Streamlit Design System in Figma. Go ahead and start building!\n\nStep 1\n\nVisit our Streamlit Community page on Figma to see our Design System. Follow us to learn about new designs, new components, and other new updates. üíô\n\nü§´\nSPOILER ALERT: We‚Äôll be releasing a companion plugin soon. Hope you‚Äôll like it!!\nStep 2\n\nClick on Streamlit Design System and make a copy. It will appear in your personal Figma account so you can play with it as you wish:\n\nüëâ\nNOTE: The file will open in the draft folder of your personal account. You can play with the System (your local assets) or go to the top middle and click ‚ÄúPublish styles and components‚Äù so that anyone in your organization can use your components.\nStep 3\n\nGo to the ‚ÄúAssets‚Äù tab to find all your streamlit components:\n\nStep 4\n\nBuilding your app is super simple! Just type ‚ÄúApp Base‚Äù in your search assets bar:\n\nOnce you have the base, follow the instructions to detach the instance. Start dragging and dropping components to your heart‚Äôs desire. And use the app base for the perfect layout (simply decide what you want to put in it).\n\nStep 5\n\nStart adding components! They‚Äôre labeled by their Streamlit Python command (i.e., st.text_input). Unsure what to add? Pick one component from the library on the third page of the Design System: üß± Streamlit Components‚ÄîBuild your app (learn more about our widgets in our docs).\n\nWrapping up\n\nThis is a passion project and a work-in-progress that will continue to evolve. We‚Äôve been using it to prototype concepts and innovate around app creation. Please try it out and comment on our Figma community page. We‚Äôd love to hear your feedback.\n\nIf you have questions, leave them in the comments below or message me on LinkedIn.\n\nHappy designing and building! üé®\n\nP.S.: This post is Part 1 of the series on empowering designers to build their own apps. Check out Part 2 here!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "2-chart-configuration.png (1140√ó536)",
    "url": "https://blog.streamlit.io/content/images/2023/04/2-chart-configuration.png#border",
    "html": ""
  },
  {
    "title": "1-animate-method.png (1024√ó780)",
    "url": "https://blog.streamlit.io/content/images/2023/04/1-animate-method.png#border",
    "html": ""
  },
  {
    "title": "Using Streamlit for semantic processing with semantha",
    "url": "https://blog.streamlit.io/using-streamlit-for-semantic-processing-with-semantha/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nBy Sven Koerner\nPosted in Advocate Posts, February 2 2023\nHow to integrate semantic AI processing into your apps and use cases\nHow to give your app ‚Äúcommon sense‚Äù with a 3-liner\nHow to get inspired!\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Sven and I work as an AI researcher at thingsTHINKING.\n\nWe integrated our semantic platform with large-scale (data) customers. They use Snowflake to combine hundreds of data sinks. Connecting with Snowflake‚Äôs API and using Streamlit gave the users a super-efficient UI while preserving our platform capabilities. The integration was only 3 lines of code‚Äîand the UI setup was only 46!\n\nIn this post, I‚Äôll show you:\n\nHow to integrate semantic AI processing into your apps and use cases\nHow to give your app ‚Äúcommon sense‚Äù with a 3-liner\nHow to get inspired!\n\nWant to jump right in? Here's a sample app and a repo code.\n\nHow to integrate semantic AI processing into your apps and use cases\n\nFollow these simple steps:\n\nStep 1. Install the corresponding semantha package for your use case with pip install semantha-streamlit-compare.\n\nYou can find the latest version of this demo on pypi. We also provide a pip-based SDK so that you can use all our semantic processing features (more on this in future posts).\n\nHere is an example:\n\nfrom semantha_streamlit_compare.components.compare import SemanticCompare\n\ncompare = SemanticCompare()\ncompare.build_input(sentences=(\"First sentence\", \"Second sentence\"))\n\n\nStep 2. Create your Streamlit UI and add the semantha endpoint.\n\nHere is a sample UI with only 30 lines of code on GitHub:\n\nStep 3 (optional). Import the semantha (Python) package into your code and use it in whatever form you‚Äôd like.\n\nStep 4. Request an API code from semantha and run your prototype.\n\nFor example, to use a component, request a secrets.toml file from support@thingsthinking.atlassian.net. After you are authenticated, copy it into the .streamlit/secrets.toml folder as documented here. You may need to create it in the root of your Streamlit app.\n\nHere is the file structure of the secrets.toml file:\n\n[semantha]\nserver_url=\"URL_TO_SERVER\"\napi_key=\"YOUR_API_KEY_ISSUED\"\ndomain=\"USAGE_DOMAIN_PROVIDED_TO_YOU\"\ndocumenttype=\"document_with_contradiction_enabled\"\n\nHow to give your app ‚Äúcommon sense‚Äù with a 3-liner\n\nIf you‚Äôve done the above, you‚Äôve built ‚Äúcommon sense‚Äù into your app, which can now automatically understand when things are similar, different, or of opposite meanings. Extend this idea to documents and all the unstructured information you process every day. You know where this could lead to‚Ä¶a trusty sidekick in your daily knowledge work!\n\nHow to get inspired!\n\nCheck out this video to see what people have built based on Streamlit:\n\nYou know your use cases best‚Äîand you know where a coworker (in this case, not a human but an AI) could be very helpful!\n\nWrapping up\n\nWe‚Äôll provide more use cases in future posts, including:\n\nMovie Quote Search\nESG Document Comparison Using semantha‚Äôs MagicSort\nRFI/RFP/Tender processing with semantic platforms on Streamlit/Snowflake\n\nIf you have any questions, please post them in the comments below or contact me on LinkedIn, Twitter, or via email.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "app-starter-kit-1-2.png (1780√ó472)",
    "url": "https://blog.streamlit.io/content/images/2022/09/app-starter-kit-1-2.png#border",
    "html": ""
  },
  {
    "title": "hackathon-101.jpeg (2000√ó988)",
    "url": "https://blog.streamlit.io/content/images/2022/10/hackathon-101.jpeg",
    "html": ""
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/page/4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\nBuilding robust Streamlit apps with type-checking\n\nHow to make type-checking part of your app-building flow\n\nAdvocate Posts\nby\nHarald Husum\n,\nNovember 10 2022\nAnnouncing the Figma-to-Streamlit plugin üé®\n\nGo from prototype to code as easy as 1-2-3 with our new community resource!\n\nProduct\nby\nJuan Mart√≠n Garc√≠a\n,\nNovember 1 2022\nPrototype your app in Figma! üñåÔ∏è\n\nQuickly and easily design your app with the Streamlit Design system\n\nTutorials\nby\nJessi Shamis\n,\nOctober 27 2022\nDiscover and share useful bits of code with the¬†ü™¢¬†streamlit-extras library\n\nHow to extend the native capabilities of Streamlit apps\n\nTutorials\nby\nArnaud Miribel\n,\nOctober 25 2022\nStreamlit App Starter Kit: How to build apps faster\n\nSave 10 minutes every time you build an app\n\nTutorials\nby\nChanin Nantasenamat\n,\nSeptember 27 2022\nHow to build your own Streamlit component\n\nLearn how to make a component from scratch!\n\nTutorials\nby\nZachary Blackwood\n,\nSeptember 15 2022\nMake dynamic filters in Streamlit and show their effects on the original dataset\n\nQuickly and easily add dynamic filters to your Streamlit app\n\nTutorials\nby\nVladimir Timofeenko\n,\nAugust 25 2022\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nTutorials\nby\nTyler Richards and¬†\n2\n¬†more,\nAugust 18 2022\nThe magic of working in open source\n\nHow we build our open-source library and release new features\n\nTutorials\nby\nKen McGrady\n,\nAugust 4 2022\nHow to enhance Google Search Console data exports with Streamlit\n\nConnect to the GSC API in one click and go beyond the 1,000-row UI limit!\n\nTutorials\nby\nCharly Wargnier\n,\nJuly 28 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit-Authenticator, Part 2: Adding advanced features to your authentication component",
    "url": "https://blog.streamlit.io/streamlit-authenticator-part-2-adding-advanced-features-to-your-authentication-component/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit-Authenticator, Part 2: Adding advanced features to your authentication component\n\nHow to add advanced functionality to your Streamlit app‚Äôs authentication component\n\nBy Mohammad Khorasani\nPosted in Advocate Posts, February 7 2023\nHow to create a password reset widget\nHow to create a new user registration widget\nHow to create a forgotten password widget\nHow to create a forgotten username widget\nHow to create an updated user details widget\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nThis is Part 2 of the Streamlit Authenticator component two-part series. In Part 1, we covered how to create an authentication component that allows users to log in and gain privileged access to pages within your app.\n\nIn this second part, we'll cover the following:\n\nHow to create a password reset widget\nHow to create a new user registration widget\nHow to create a forgotten password widget\nHow to create a forgotten username widget\nHow to create an updated user details widget\n\nTL;DR? Here's the repo code.\n\nHow to create a password reset widget\n\nIf your user needs to reset their password to a new one, use the reset_password widget to allow the already logged-in user to change their password:\n\nif authentication_status:\n    try:\n        if authenticator.reset_password(username, 'Reset password'):\n            st.success('Password modified successfully')\n    except Exception as e:\n        st.error(e)\n\n\nHow to create a new user registration widget\n\nIf you want to allow pre-authorized or even non-pre-authorized users to register, use the register_user widget to allow a user to register for your app. If you want the user to be pre-authorized, set the preauthorization argument to True and add their email to the preauthorized list in the configuration file. Once they have registered, their email will automatically be removed from the preauthorized list in the configuration file.\n\nTo let any user register, set the preauthorization argument to False:\n\ntry:\n    if authenticator.register_user('Register user', preauthorization=False):\n        st.success('User registered successfully')\nexcept Exception as e:\n    st.error(e)\n\n\nHow to create a forgotten password widget\n\nIf you want to allow a user to reset a forgotten password, use the forgot_password widget to allow them to generate a new random password. This new password will be automatically hashed and stored in the configuration file. The widget will also return the user's username, email, and new random password (for you to securely send to the user):\n\ntry:\n    username_forgot_pw, email_forgot_password, random_password = authenticator.forgot_password('Forgot password')\n    if username_forgot_pw:\n        st.success('New password sent securely')\n        # Random password to be transferred to user securely\n    elif username_forgot_pw == False:\n        st.error('Username not found')\nexcept Exception as e:\n    st.error(e)\n\n\nHow to create a forgotten username widget\n\nYou can also allow users to reset their username with the forgot_username widget, which lets them retrieve their forgotten username. The widget will also return the user's username and email (for you to securely send to them):\n\ntry:\n    username_forgot_username, email_forgot_username = authenticator.forgot_username('Forgot username')\n    if username_forgot_username:\n        st.success('Username sent securely')\n        # Username to be transferred to user securely\n    else:\n        st.error('Email not found')\nexcept Exception as e:\n    st.error(e)\n\n\nHow to create an updated user details widget\n\nYou can allow your users to update their name and/or email with the update_user_details widget. The widget will automatically save the updated details in both the configuration file and the reauthentication cookie:\n\nif authentication_status:\n    try:\n        if authenticator.update_user_details(username, 'Update user details'):\n            st.success('Entries updated successfully')\n    except Exception as e:\n        st.error(e)\n\n\nWrapping up\n\nAnd that concludes our review of the Streamlit-Authenticator component! I hope you now feel confident about securely authenticating users to your Streamlit application with advanced functionalities.\n\nIn the meantime, feel free to read more about this component in our book Web Application Development with Streamlit. And if you have any questions, please leave them in the comments below or contact me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Robert Ritz - Streamlit",
    "url": "https://blog.streamlit.io/author/robert/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Robert Ritz\n1 post\nFind the top songs from your high school years with a Streamlit app\n\nUse the Spotify API to generate 1,000+ playlists!\n\nAdvocate Posts\nby\nRobert Ritz\n,\nDecember 8 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "uPlanner fosters data processing innovation with Streamlit",
    "url": "https://blog.streamlit.io/uplanner-fosters-data-processing-innovation-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nBy Sebastian Flores Benner\nPosted in Case study, October 6 2022\nSimplifying user experience\nHow we did it\nA smaller but editable example\nLearning from the building experience\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Sebasti√°n Flores. I‚Äôm a Chief Data Officer at uPlanner and a Streamlit Creator.\n\nAt uPlanner, we develop cloud solutions to help higher education institutions be more efficient on Smart Campus, Academic Management, and Student Success. Most institutions use our standard data processing, but sometimes we need to create client-custom scripts for the data to be correct, consistent, and in the right format. This can turn into a nightmare as these scripts are hard to maintain and complicate the whole process.\n\nSo in this post, I‚Äôll show you how Streamlit makes it easy and how you can build an app with a simple frontend for previously created scripts.\n\nTL;DR? Check out the app and the repo.\n\nLet‚Äôs get started.\n\nSimplifying user experience\n\nNot everyone knows how to run a Python script‚Äîor how to install it. If you work with multiple data files, using the terminal to process files can be error-prone. And updating the scripts can be tedious. Oh, the anxiety of Git conflicts!\n\nWe used to pass the clients‚Äô files to implementation engineers, then to data engineers. The data engineers executed the scripts on the inputs and returned the outputs. This took a lot of back-and-forth communication.\n\nStreamlit allows you to create a frontend for your code. Clients can see if their data meets all the requirements. And they see it in a familiar interface: a website. All the hard parts‚Äîinstallation, versioning, data processing‚Äîare hidden behind a beautiful UI.\n\nAnd the best part?\n\nIt‚Äôs super-fast to develop. Streamlit has a huge collection of useful input and output widgets so even junior engineers can produce stunning pages in no time.\n\nHow we did it\n\nOur first Streamlit app allowed the users to pick between six different data processing scripts. Yay to multipaging! The scripts had very different interfaces, but they made the utilization intuitive and easy to follow.\n\nHere are some of the app‚Äôs general functionalities:\n\nConvert CSV files to Excel and back to CSV\nAnalyze a CSV or an Excel file\nMove an uploaded file to an SFTP\nExtract and merge data from different tables and databases\nVerify file structure and content\nGenerate a specific output file, to be uploaded to third-party software\n\nScript adoption and development are much easier with the app as a single point of access. Everyone is happy with more time and autonomy!\n\nA smaller but editable example\n\nDue to privacy, I‚Äôm sharing a simplified version of the app that illustrates the main concepts. The app can:\n\n1. Concatenate files that are produced on different days (useful when new data is generated daily or from different sources). By default, the file encoding and the column separator are individually inferred, but users can choose any specific setting:\n\n2. Convert CSV files to Excel files (handy when the CSV files have different encodings and separators). Again, the encoding and the separator can be inferred or specified:\n\nHere is the app‚Äôs file structure:\n\nPut your regular Python scripts for transforming data into the folder ‚Äúhelpers.‚Äù And don‚Äôt forget to include a __init__.py to import them from the Streamlit scripts. It should be as easy as:\n\nfrom helpers.concatenate_helpers import concatenate_files\n\n\nThe other folders and files that turn the scripts into an app are:\n\nThe file Home.py is the app‚Äôs entry file (rename it as you see fit or add emojis!).\nThe ‚Äúimages‚Äù folder is for any pictures you want to display.\nThe ‚Äúpages/‚Äù folder is for different pages so you can use multipaging. Mine are ‚ÄúHome‚Äù (from ‚ÄúHome.py‚Äù), ‚ÄúConcatenate,‚Äù and ‚ÄúCSV to Excel‚Äù (notice how Streamlit replaces the \"_\" with spaces when displaying the page name).\nThe ‚Äútmp‚Äù folder is for temporary files (before users download them).\nINSTALL.md, README.md, and LICENCE are the standard files for installation, help, and copyright.\nrequirements.txt is the list of required libraries.\nLearning from the building experience\n\nThe most important lesson we‚Äôve learned from creating apps is to give direct and clear feedback to the app user as soon as possible. Most of our scripts deal with file processing by displaying warning, error, and success messages from the files‚Äô execution. Additional information is always welcome: filetype, encoding, number of rows and columns, or data distribution.\n\nWe also check the expected content:\n\nAre all the required columns on file? Let the user know what columns they‚Äôre missing.\nDo we have duplicated rows? Show the user the precise rows that need revision!\nIs the data consistent across multiple files? Explain in simple terms what isn‚Äôt matching correctly.\n\nProviding feedback as soon as you can helps users correct the information and avoid frustration. There‚Äôs nothing worse than doing 10 steps and then learning you had an error on step one!\n\nWrapping up\n\nThank you for reading my article! I hope you take away two main points:\n\nFrom a technical perspective, Streamlit lets you put a front-end to Python scripts with a great trade-off: a short development time for a nice interactive interface.\nFrom a management perspective, Streamlit decouples the development and execution of your Python scripts. This leads to accelerated adoption, innovation, and participation of non-technical users‚Äîa win-win for everyone!\n\nIf you want to find out more about uPlanner, check out our website and our social channels: LinkedIn, Facebook, and Twitter. And if you have any questions, leave them in the comments below or reach out to me on Twitter at @sebastiandres or on GitHub.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tyler Simons - Streamlit",
    "url": "https://blog.streamlit.io/author/tyler-simons/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Tyler Simons\n1 post\nBuild an image background remover in Streamlit\n\nSkip the fees and do it for free! üéà\n\nTutorials\nby\nTyler Simons\n,\nJanuary 10 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mohammad Khorasani - Streamlit",
    "url": "https://blog.streamlit.io/author/m/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mohammad Khorasani\n2 posts\nStreamlit-Authenticator, Part 2: Adding advanced features to your authentication component\n\nHow to add advanced functionality to your Streamlit app‚Äôs authentication component\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nFebruary 7 2023\nStreamlit-Authenticator, Part 1: Adding an authentication component to your app\n\nHow to securely authenticate users into your Streamlit app\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nDecember 6 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Sven Koerner - Streamlit",
    "url": "https://blog.streamlit.io/author/sven/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Sven Koerner\n2 posts\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nAdvocate Posts\nby\nSven Koerner\n,\nFebruary 2 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Deploying a cloud-native Coiled app",
    "url": "https://blog.streamlit.io/how-to-deploy-your-cloud-native-coiled-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDeploying a cloud-native Coiled app\n\nHow Coiled uses a Streamlit-on-Coiled app to present multi-GBs of data to their users\n\nBy Richard Pelgrim\nPosted in Advocate Posts, September 7 2021\nWhy fuse Streamlit and Coiled?\nWhy use the Streamlit Secrets Manager?\nHow to give Streamlit your Coiled credentials\nHow to configure your Dask Cluster\nShare your App with the World\nResources\nContents\nShare this post\n‚Üê All posts\n\nAre you using Streamlit to present multiple GBs of data to your users? Chances are you‚Äôll run into the limits of your computational resources, either locally or on the virtual machine that hosts your Streamlit app. Coiled can help you burst into the cloud by spinning up on-demand Dask clusters. No more worries about any of the usual DevOps activities. Just scroll down to see how you can use a Streamlit-on-Coiled app to present live computations on the entire 10+ GB NYC Taxi dataset in a matter of seconds.\n\nIn this post, you‚Äôll learn how to make your Streamlit-on-Coiled app ready for deployment using Streamlit's secret management feature. Why secret? Because after creating something this impressive, you‚Äôll want to be sharing it with the world...and you'll want to do that securely!\n\nBut first, let's discuss why you'd want to do this.\n\nWhy fuse Streamlit and Coiled\nWhy use the Secrets Manager\nHow to give Streamlit your Coiled credentials\nHow to configure your Dask Cluster\nShare your app with the world\n\nCan't wait to jump right in? Here's a sample app and repo code!\n\nWhy fuse Streamlit and Coiled?\n\nBy running your Streamlit app on Coiled, you can present computations on multiple-GB datasets in a seamless user experience. Expensive computations that would otherwise take minutes (or even hours) to run can be completed in a matter of seconds. The results will be displayed almost immediately. It‚Äôs a great example of two products built on Python-native OSS complementing each other to give you nimble-yet-powerful tooling in a familiar environment.\n\nIn a previous post, I provide a step-by-step guide of how you can achieve this. You can also jump straight to this GitHub repo to download the coiled-streamlit-expanded.py script and see for yourself. Note that the \"Running get_client()\" function starts up your Coiled cluster and can take a couple of minutes to complete.\n\nWhy use the Streamlit Secrets Manager?\n\nThis post will show you how to get your Streamlit-on-Coiled app ready for deployment.\n\nStreamlit‚Äôs Sharing platform allows you to host up to 3 apps for free. Apps are deployed by running Python scripts that are hosted in public Github repositories. Because of this, you'll want to avoid including any sensitive information in your Streamlit scripts. It can expose you to potential security threats. For example, including your Coiled authentication token in a public repo would allow other users to spin up clusters and incur costs to your account.\n\nThis is why we'll be using Streamlit‚Äôs secrets management feature for the backend authorization necessary to hook your Streamlit app up to Coiled's cloud-computing resources.\n\nYou can do this by:\n\nAdding your Coiled credentials token to the Streamlit Secrets Manager\nConfiguring your Dask cluster to use the right credentials\nHow to give Streamlit your Coiled credentials\n\nTo get your Streamlit app running on Coiled, you'll need a Coiled Cloud account. If you don't already have one, get your Free Tier account here by signing in with your Github or Google account. For a video walkthrough of this process, check out the Getting Started with Coiled guide.\n\nNext, navigate to the Streamlit app deployment interface and click on \"Advanced Settings\". This will open a window in which you can include the Secrets that will be passed to your script in TOML format as key-value pairs.\n\nDefine a ‚Äòtoken‚Äô key here and set it to the Coiled token value from your Coiled Cloud dashboard.\n\nAlternatively, you can do this after deploying your app by clicking on the three-dots symbol to the far right of the app and selecting ‚ÄòEdit Secrets‚Äô.\n\nHow to configure your Dask Cluster\n\nYou only need to change two lines in the Python code of the existing Streamlit-on-Coiled app to make it ready for deployment:\n\nInclude a hash_funcs keyword argument to the @st.cache call and set it to {\"_thread.RLock\": lambda _: None}). This turns off hashing for the _thread.RLock object type which improves our app performance. Read more on why and when you might want to turn hashing off here.\nUse dask.config.set within the already-defined get_client function definition to manually set the coiled.token that Dask uses to the ‚Äòtoken‚Äô key in the st.secrets dictionary you just created.\n@st.cache(allow_output_mutation=True, hash_funcs={\"_thread.RLock\": lambda _: None})\ndef get_client():\n\tcluster_state.write(\"Starting or connecting to Coiled cluster...\")\n\tdask.config.set({\"coiled.token\":st.secrets['token']})\n\n\tcluster = coiled.Cluster(\n\t\tn_workers=10,\n\t\tname=\"coiled-streamlit\",\n\t\tsoftware=\"coiled-examples/streamlit\",\n\t)\n\n\tclient = Client(cluster)\n\treturn client\n\nShare your App with the World\n\nYou're all set! You can now deploy your Streamlit-on-Coiled app and share it with the world. Get your invite to sharing and check out Streamlit's blog post and documentation on deploying.\n\nWe‚Äôd love to see what you‚Äôve built, so feel free to drop us a line in our Coiled Community Slack channel, and with the Streamlit community on their community forum - we may just feature your app on the next Coiled blog. üòâ\n\nA fair warning‚Äîif you‚Äôve built something really good that goes viral, you might run out of your monthly Coiled Free Tier credits. If that happens, you may want to consider upgrading to Coiled Pro.\n\nHave questions? Reach out to @coiled.hq, support@coiled.io, or Coiled Community Slack.\n\nResources\n\nCoiled\n\nDocs\nCommunity Slack\nBlog\nCoiled Cloud sign-up\n\nStreamlit\n\nGitHub\nDocs\nForum\nApp gallery\nSharing sign-up\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "mpa-2.gif (1254√ó854)",
    "url": "https://blog.streamlit.io/content/images/2022/06/mpa-2.gif",
    "html": ""
  },
  {
    "title": "multipage-apps.gif (1728√ó1078)",
    "url": "https://blog.streamlit.io/content/images/2022/11/multipage-apps.gif#browser",
    "html": ""
  },
  {
    "title": "Arvindra Sehmi - Streamlit",
    "url": "https://blog.streamlit.io/author/arvindra/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Arvindra Sehmi\n1 post\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nLLMs\nby\nArvindra Sehmi\n,\nFebruary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "MPA-1.png (1432√ó724)",
    "url": "https://blog.streamlit.io/content/images/2022/06/MPA-1.png#browser",
    "html": ""
  },
  {
    "title": "mpa-1.gif (1258√ó846)",
    "url": "https://blog.streamlit.io/content/images/2022/06/mpa-1.gif",
    "html": ""
  },
  {
    "title": "culture-map-app-9.png (677√ó783)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-9.png#border",
    "html": ""
  },
  {
    "title": "culture-map-app-7.png (718√ó1056)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-7.png#border",
    "html": ""
  },
  {
    "title": "culture-map-app-5.png (686√ó821)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-5.png#border",
    "html": ""
  },
  {
    "title": "culture-map-app-6.png (778√ó881)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-6.png#boder",
    "html": ""
  },
  {
    "title": "culture-map-app-8.png (686√ó397)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-8.png#border",
    "html": ""
  },
  {
    "title": "culture-map-app-4.png (664√ó682)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-4.png#border",
    "html": ""
  },
  {
    "title": "app-viewers.gif (1000√ó755)",
    "url": "https://blog.streamlit.io/content/images/2022/05/app-viewers.gif",
    "html": ""
  },
  {
    "title": "4.png (1792√ó1218)",
    "url": "https://blog.streamlit.io/content/images/2022/05/4.png#browser",
    "html": ""
  },
  {
    "title": "culture-map-app-3.png (707√ó509)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-3.png#border",
    "html": ""
  },
  {
    "title": "Dialog--1-.png (3600√ó2440)",
    "url": "https://blog.streamlit.io/content/images/2022/05/Dialog--1-.png#browser",
    "html": ""
  },
  {
    "title": "culture-map-app-2.png (694√ó202)",
    "url": "https://blog.streamlit.io/content/images/2023/01/culture-map-app-2.png#border",
    "html": ""
  },
  {
    "title": "workspace-analytics-dotted-2.gif (1000√ó755)",
    "url": "https://blog.streamlit.io/content/images/2022/05/workspace-analytics-dotted-2.gif",
    "html": ""
  },
  {
    "title": "3.png (1794√ó800)",
    "url": "https://blog.streamlit.io/content/images/2022/05/3.png#browser",
    "html": ""
  },
  {
    "title": "2-2.png (1784√ó1182)",
    "url": "https://blog.streamlit.io/content/images/2022/05/2-2.png#browser",
    "html": ""
  },
  {
    "title": "1.png (1944√ó722)",
    "url": "https://blog.streamlit.io/content/images/2022/05/1.png#browser",
    "html": ""
  },
  {
    "title": "intoducing-streamlit-cloud.gif (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2022/09/intoducing-streamlit-cloud.gif",
    "html": ""
  },
  {
    "title": "Creating Custom Themes for Streamlit Apps",
    "url": "https://blog.streamlit.io/introducing-theming/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnnouncing Theming for Streamlit apps! üé®\n\nTry out the new dark mode and custom theming capabilities\n\nBy Abhi Saini\nPosted in Product, March 18 2021\nIntroducing Dark Mode!\nCreating custom themes\nCreating a theme with the theme editor\nCreating a theme using config.toml directly\nLive updates from the Config file\nWhat each color setting does\nThemes and Custom Components\nAs a React prop\nAs CSS variables\nFind your favorite new theme\nTry out theming and let us know what you think!\nResources\nContents\nShare this post\n‚Üê All posts\n\nWe like to think Streamlit apps are beautiful out of the box (hopefully you do too!), but up until today it wasn't easy to change the look and feel to reflect your personal style or your company's visual branding. Streamlit Theming now allows you to easily change the color scheme and fonts in your app without ever having to touch any HTML or CSS.\n\nIn addition, for accessibility and user-friendliness, we're also giving app viewers the ability to toggle between light and dark themes.\n\nCheck out our video tutorial for more help and this sample app if you want to dive right in!\n\nIntroducing Dark Mode!\n\nBefore we get into custom themes, let's talk about dark mode.\n\nBy popular demand, we now provide app viewers the ability to customize how they'd like to experience your app. This is useful for people who are using the app in a dark environment, or for folks with accessibility needs, who want to override your app's custom theme.\n\nTo toggle between various themes, go to Menu on the top-right corner and choose Settings. There you'll see a redesigned Settings dialog that lets app users choose between different theme options:\n\nLight Mode: This is the Streamlit theme you already know and love\nDark Mode: This is Streamlit's new dark theme. You'll love this too!\nUse system setting: Streamlit will automatically pick up your Operating System theme (light or dark) and change colors with your OS (Note: may not work for some browsers and Linux distros).\nCustom theme (only visible when provided by the app author): Use the theme provided by the app author. This is the default when provided. Otherwise, \"Use system setting\" is the default.\nCreating custom themes\n\nThemes are set via Streamlit's config system. You can either set the theme there directly, or you can use our fancy new theme editor interface to build a theme live and then copy it to your config.\n\nCreating a theme with the theme editor\n\nApp developers can create a custom theme by simply going to Menu ‚Üí Settings ‚Üí Edit Active Theme.\n\nNote: The theme editor menu is available only in local development. If you‚Äôve deployed your app using Streamlit Sharing, the ‚ÄúEdit active theme‚Äù button will no longer be displayed in the ‚ÄúSettings‚Äù menu.\n\nCreating a theme using config.toml directly\n\nCustom themes can also be defined in the config file: ./.streamlit/config.toml\n\nUnder the [theme] section, colors variables can be defined to create a custom theme\n\n[theme]\n\n# Primary accent for interactive elements\nprimaryColor = '#7792E3'\n\n# Background color for the main content area\nbackgroundColor = '#273346'\n\n# Background color for sidebar and most interactive widgets\nsecondaryBackgroundColor = '#B9F1C0'\n\n# Color used for almost all text\ntextColor = '#FFFFFF'\n\n# Font family for all text in the app, except code blocks\n# Accepted values (serif | sans serif | monospace) \n# Default: \"sans serif\"\nfont = \"sans serif\"\n\n\n(Note: you can only set one custom theme at a time)\n\nLive updates from the Config file\n\nYou know how your Streamlit app live-updates when you change its source code? We now also do this when you update the app's config file! This way, editing the theme directly in the config file will cause your app to immediately display the new theme.\n\nWhat each color setting does\n\nStreamlit's theming system follows a global approach to applying color and fonts to an app. Color styles have semantic names and a simple way for doing color specification.\n\nPrimary color: Accent color for interactive elements like st.radio, button borders etc. By default, this is pink.\nBackground color: This is the background color for the main body of your app.\nText color: Pretty self-explanatory! This is the text color for your app.\nSecondary background color: Used as the background for st.sidebar and for several widgets.\n\nBackground color and Secondary background color are in a way \"complementary\" since they are used for elements placed \"on\" top of base elements to promote consistency and accessible contrast. This means that a st.number_input widget will use Secondary background color as its Background color when defined in the main body, but use Background color when defined in the sidebar. The image below illustrates this:\n\nThemes and Custom Components\n\nIf you're a component author, we make it easy for you to read theme colors in your JS and CSS code. To get access to this new feature, install or update the newest version of streamlit-component-lib.\n\nnpm install streamlit-component-lib\n\n\nOnce this package is updated, the background and text colors of a component will automatically be changed to match that of the active theme. Additional theme properties are exposed to the component in two equivalent ways: the theme object or the theme style.\n\nAs a React prop\n\nAn object passed to the component via the theme prop has the following shape:\n\n{\n    \"primaryColor\": \"someColor1\"\n    \"backgroundColor\": \"someColor3\",\n    \"secondaryBackgroundColor\": \"someColor4\",\n    \"textColor\": \"someColor5\",\n    \"font\": \"someFont\",\n}\n\nAs CSS variables\n--primary-color\n--background-color\n--secondary-background-color\n--text-color\n--font\n\n\nIf you're not familiar with CSS variables, the TLDR version is that you can use them like this:\n\n.mySelector {\n  color: var(--text-color);\n}\n\nThe two methods for exposing theme properties to a component contain the same information, so which one to use is a matter of personal preference. An example of making a custom component work with themes can be found here.\n\nFind your favorite new theme\n\nHere's our Mapping Demo, with a 'Quiet Light'-inspired theme:\n\n[theme]\n\nprimaryColor=\"#6eb52f\"\nbackgroundColor=\"#f0f0f5\"\nsecondaryBackgroundColor=\"#e0e0ef\"\ntextColor=\"#262730\"\nfont=\"sans serif\"\n\nHere's an example of a 'Solarized'-inspired theme:\n\n[theme]\n\nprimaryColor=\"#d33682\"\nbackgroundColor=\"#002b36\"\nsecondaryBackgroundColor=\"#586e75\"\ntextColor=\"#fafafa\"\nfont=\"sans serif\"\n\n\nTry out theming and let us know what you think!\n\nTo try out theming, simply upgrade to the latest version of Streamlit as usual:\n\n$ pip install streamlit --upgrade\n\n\nWe can't wait to see the cool and amazing themes that the community will build. If you built a cool custom theme, we'd love to see it, so please tag @streamlit when you share on Twitter, LinkedIn or on our Forum.\n\nResources\nDocumentation\nGithub\nChangelog\nForum\nSample app\nVideo tutorial\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "unique-viewers.png (2000√ó1513)",
    "url": "https://blog.streamlit.io/content/images/2022/05/unique-viewers.png#browser",
    "html": ""
  },
  {
    "title": "Develop Streamlit-WebRTC Component for Real-Time Video Processing",
    "url": "https://blog.streamlit.io/how-to-build-the-streamlit-webrtc-component/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDeveloping a streamlit-webrtc component for real-time video processing\n\nIntroducing the WebRTC component for real-time media streams\n\nBy Yuichiro Tachibana (Tsuchiya)\nPosted in Advocate Posts, February 12 2021\nThe problem with existing approaches\nHow WebRTC resolves this issue\nThe WebRTC basics\nThe basics of Streamlit's execution model\nIntegrate aiortc into a Streamlit component\nInstall dependencies\nSetting up the project\nRolling out the first frontend implementation\nSend offer from JS to Python\nServer-side implementation with asyncio\nSend back the answer from Python to JS\nImplement processAnswer()\nIntroduce a thread running over script executions\nComponent height adjustment\nImplementing your own video filter\nImplement a stop button\nThe execution model of streamlit-webrtc\nWhat tiny-streamlit-webrtc lacks\nParameters input from Streamlit components\nFrame drops\nExtendability\nKey takeaways\nCredits\nContents\nShare this post\n‚Üê All posts\n\nReal-time video processing is one of the most important applications when developing various computer vision or machine learning models. It‚Äôs useful because it allows users to quickly verify what their models can do with handy video input from their own devices, such as webcams or smartphones.\n\nBut it also presents a challenge to those of us using Streamlit, since Streamlit doesn‚Äôt natively support real-time video processing well yet through its own capabilities.\n\nI created streamlit-webrtc, a component that enables Streamlit to handle real-time media streams over a network to solve this problem. In this in-depth tutorial, I‚Äôll also briefly introduce you to WebRTC (check out my article here for more in-depth info on WebRTC). If you want to jump right to playing with the component here is a sample app.\n\nReady?\n\nLet‚Äôs dive in.\n\n(This tutorial requires Python >= 3.6 and a webcam.)\n\nThe problem with existing approaches\n\nStreamlit is actively used by many developers and researchers to prototype apps backed with computer vision and machine learning models, but it can‚Äôt yet natively support real-time video processing.\n\nOne existing approach to achieve real-time video processing with Streamlit is to use OpenCV to capture video streams. However, this only works when the Python process can access the video source - in other words, only when the camera is connected to the same host the app is running on.\n\nDue to this limitation, there have always been problems with deploying the app to remote hosts and using it with video streams from local webcams. cv2.VideoCapture(0) consumes a video stream from the first (indexed as 0) locally connected device, and when the app is hosted on a remote server, the video source is a camera device connected to the server - not a local webcam.\n\nHow WebRTC resolves this issue\n\nWebRTC (Web Real-Time Communication) enables web servers and clients, including web browsers, to send and receive video, audio, and arbitrary data streams over the network with low latency.\n\nIt is now supported by major browsers like Chrome, Firefox, and Safari, and its specs are open and standardized. Browser-based real-time video chat apps like Google Meet are common examples of WebRTC usage.\n\nWebRTC extends Streamlit‚Äôs powerful capabilities to transmit video, audio, and arbitrary data streams between frontend and backend processes, like browser JavaScript and server-side Python.\n\nThe WebRTC basics\n\nThe following tutorial uses knowledge about WebRTC concepts such as \"Signaling\", \"Offer\", and \"Answer\". The below figure provides a simple summary of how to establish a WebRTC connection.\n\nWebRTC has a preparation phase called \"Signaling\", during which the peers exchange data called \"offers\" and \"answers\" in order to gather necessary information to establish the connection.\nDevelopers choose an arbitrary method for Signaling, such as the HTTP req/res mechanism.\n\nIf you want to know more about these concepts, read this article.\n\nJust as in the article linked above, this tutorial will use aiortc, a Python library for WebRTC, and an example from the aiortc repository as the basis for our sample project.\n\nThe basics of Streamlit's execution model\n\nTo read further, you should know about the development of Streamlit bi-directional custom components and about Streamlit's execution model. You can learn about it here.\n\nHere is a short summary:\n\nUpon each execution, the Python script is executed from top to bottom.\nEach execution of the Python script renders the frontend view, sending data from Python to JS as arguments to the component.\nThe frontend triggers the next execution via Streamlit.setComponentValue(), sending data from JS to Python as a component value.\n\nIntegrate aiortc into a Streamlit component\n\nIn this section, to understand how to integrate a WebRTC implementation into a Streamlit custom component, we will create a minimal version of streamlit-webrtc called tiny-streamlit-webrtc, as a hands-on tutorial.\n\nThe source code of tiny-streamlit-webrtc is hosted on GitHub. Throughout this tutorial, we will refer to this repository and review each intermediate commit step-by-step to reach the final version.\n\nIt is recommended for you to clone the repository:\n\n$ git clone https://github.com/whitphx/tiny-streamlit-webrtc.git\n$ cd tiny-streamlit-webrtc\n\n\nWith the below command, you can check out the specific revision referenced in each section in order to see the entire codebase and to actually try running it.\n\n$ git checkout <revision>\n\nInstall dependencies\n\nInstall the necessary packages. Note that this tutorial does not work with the latest version of aiortc (1.1.1) and 1.0.0 must be used.\n\n$ pip install streamlit opencv-python\n$ pip install aiortc==1.0.0\nSetting up the project\n\nAs usual, we start with the official template of a bi-directional component. The reference tiny-streamlit-webrtc implementation is based on the revision 4b90f52.\n\nAfter copying the template files, complete the rest of the setup, including the steps below.\n\nRename \"my_component\" to \"tiny_streamlit_webrtc\".\nRun npm install in tiny_streamlit_webrtc/frontend.\nRemove the existent code, comments, and docstrings.\nAdd necessary files such as .gitignore\n\nCheck out what this section does, with code version f6daf28.\n\nRolling out the first frontend implementation\n\nLet's start writing code.\n\nFirst, we will simply copy and paste some lines of code from index.html and client.js in the aiortc example into our React component, but with some fixes.\n\ne3f70e4 is the actual edit, and you can try this version by checking out the commit, as explained above.\n\n$ git checkout e3f70e4\n\n\nThe view contains only a <video /> element with autoPlay and playsInline props, as it is in the original index.html, and a button element to start the WebRTC session. The start button's onClick handler is bound to the start() method, which is copied from client.js and slightly modified to remove some lines unnecessary for this tutorial and adjust to the React class-based component style. We will do the same for negotiate() and createPeerConnection().\n\nLet's run this component in the usual manner for Streamlit custom component development.\n\n$ cd tiny_streamlit_webrtc/frontend/\n$ npm start\n\n$ streamlit run tiny_streamlit_webrtc/__init__.py\n\n\nAfter opening the app with a web browser, open the developer tools, and click the \"Start\" button. You can see the offer is generated and printed in the console as below.\n\nThis is printed via this line. Please follow the steps leading up to it. This code is equivalent to the code in the original example before sending the offer to the Python server. Yes, this case is different from the original example. How can we send the offer to the Python process?\n\n(You also see your webcam become active since navigator.mediaDevices.getUserMedia() requests its use.)\n\nSend offer from JS to Python\n\nstreamlit-webrtc makes use of Streamlit.setComponentValue() for this purpose. We will learn about it in this section.\n\n7b7dd2d is the next update. Use git checkout 7b7dd2d to check out it.\n\nWith this change, the offer is sent from the frontend to the server as a component value.\n\nconst offerJson = offer.toJSON()\nStreamlit.setComponentValue({\n  offerJson,\n})\n\n\nThe offer can be read on the server-side as below.\n\ncomponent_value = _component_func(key=key, default=None)\nif component_value:\n    offer_json = component_value[\"offerJson\"]\n\n\nLet's run this version and confirm the offer is displayed after clicking the \"Start\" button, which means the offer is received by the Python process and shown with st.write() here.\n\nServer-side implementation with asyncio\n\nNow the offer is received on the server-side, so let's implement the code to process it. Just as we did with the frontend, let's copy and paste from the example server.py to our streamlit_webrtc/__init__.py, like this, which is copied from offer() coroutine in the example server.py.\n\nNote that a video transformer is temporarily omitted from the track event listener like this to focus on the WebRTC part for now. It now just passes through the input track to the output.\n\nHowever, as you can see, this code contains async and await and does not work in a function. So, we have to wrap this part in a coroutine like this.\n\nPlease run this version: a6f7cc0 and confirm the answer is displayed following the offer from here. That means the server-side pc object has processed the offer and generated the answer.\n\nWhat we have to do next is send it back to the frontend.\n\nSend back the answer from Python to JS\n\nTo do this, streamlit-webrtc simply relies on Streamlit's data sending mechanism from Python to JavaScript as below.\n\n_component_func(key=key, answer=answer)\n\n\nHowever, one problem arises. We‚Äôve already called component_value = _component_func(...) and obtained the offer from it. After that, we generated the answer. So, how can we set the argument to the already called _component_func() again?\n\nSimply calling the second _component_func() as below does not work, because in the Streamlit app, different _component_func() calls are recognized as different instances of the component.\n\ncomponent_value = _component_func()\noffer = component_value[\"offer\"]\nanswer = generate_answer(offer)  # Pseudo code\n_component_func(answer=answer)  # This does not work!\n\ninvalid_answering.py\n\nTo resolve this problem, we have to introduce a hack: SessionState and st.experimental_rerun(). With these tools, we can rerun the script to call a _component_func() in the same line again and hold a variable over the runs to feed it to the _component_func() in the second and later executions.\n\nSessionState has been discussed in this forum topic and the source is available on this page in Gist.\n\nst.experimental_rerun() seems, as its name implies, to be an experimental API and not documented yet. It has been discussed in this GitHub issue and can now be used.\n\nPlease see this version of the server-side code, where SessionState and st.experimental_rerun() are used to feed the generated answer to the component.\n\nThis illustrates how it works.\n\nAnother important thing here is that the key argument is no longer optional but must be explicitly provided like this. As the answer is fed as an argument to _component_func() and its value changes over the runs, key is necessary as a stable identifier of the component instance.\n\nIf key is None, Streamlit identifies the component instance based on arguments other than key, so Streamlit cannot trace the identity of the component instance over the runs as the answer changes.\n\nNote that this if-clause is added to invoke st.experimental_rerun() only the first time the server-side process gets the offer from the frontend. This may also be achieved by resetting the component value on the frontend once the offer is passed to Python.\n\nWith this version: aa2ab49, you can see the answer is provided as a field of the args prop like this on the frontend. Let's confirm it with the browser's devtools.\n\nImplement processAnswer()\n\nNow we have the answer on the frontend. Let's implement the rest of the frontend code like this.\n\nThis code is copied from the part following receiving the answer in the example client.js and fixed to adjust to ours.\n\nIntroduce a thread running over script executions\n\nIt seems we have done all things we have to do, but no video appears when you click the \"Start\" button with this version: 7fbf0eb.\n\nThe problem resides on the server-side. The server-side WebRTC code from aiortc runs on an event loop, which is implicitly started with asyncio.run() here now. An event loop is created on which aiortc functions rely throughout one Streamlit script execution. But this event loop will be trashed in the next script execution and the aiortc can no longer keep working.\n\nTo resolve this problem, we will fork a thread and create an event loop inside it to run aiortc functions. And the thread object is stored in the SessionState to be maintained over the multiple Streamlit script executions.\n\nSee this version of the code: 093f81b. This webrtc_worker() function is forked as a thread here. Inside this thread, a new event loop is created and the process_offer() coroutine is running on it - which was invoked by asyncio.run() in the previous revisions of this code. With this change, queue.Queue is introduced to get the answer object in the main thread, which is now generated in the forked thread.\n\nThere is one drawback of forking a thread - the streamlit run command does not stop when you hit Ctrl+c. This is because the forked thread remains even after the main thread is terminated.\n\nTo forcefully terminate the process, send it SIGKILL as below.\n\n$ ps aux | grep python | grep streamlit  # Find the process ID\nwhitphx         19118  11.2  0.6  4759304  99928 s003  S+    5:27PM   0:02.06 /path/to/venv/bin/python3.8 /path/to/venv/bin/streamlit run tiny_streamlit_webrtc/__init__.py\n$ kill -9 19118  # Send SIGKILL to the process specified with the ID\n\nTo fix it, the daemon option of the forked thread is set to True like this. With this flag, the script stops correctly when necessary.\n\nA thread can be flagged as a ‚Äúdaemon thread‚Äù. The significance of this flag is that the entire Python program exits when only daemon threads are left.\n\"Thread Objects\" (Python.org)\nComponent height adjustment\n\nLet's try out the current version: fc48060. Now, WebRTC works and the video appears with this component! However, the displayed video is cropped and the lower part of it is hidden like below.\n\nTo fix it, we have to call Streamlit.setFrameHeight() when the size of <video /> element changes. Although it is automatically called when the props are updated, the element resize is not associated with props updates but with starting video streaming.\n\nNow attach onCanPlay event handler on the <video /> element and call Streamlit.setFrameHeight() from it like this. (While using ResizeObserver may be the right way to observe DOM element resizes, we use the onCanPlay event here as a substitute, for simplicity's sake.)\n\nCool! Now it works correctly. üéâ1a57a97 is this version.\n\nNow all the core parts for WebRTC are complete. We‚Äôll implement the rest in the following sections.\n\nImplementing your own video filter\n\nFirst, let's try to implement some video filters. 3ba703d is an example with a simple edge extractor, copied from the sample code of aiortc.\n\nImplement a stop button\n\nRefer to the aiortc example to create a stop button to gracefully stop the stream. 13a38c1 is the current version.\n\nThe execution model of streamlit-webrtc\n\nWe have followed the steps to develop a minimal Streamlit component utilizing WebRTC to stream video.\n\nAs we‚Äôve seen in this component, we chose a design in which the computer vision code is running in a callback in the forked thread, triggered by new frame arrivals from the input stream, independent of Streamlit's script execution timings. It looks a little bit weird the first time you see it, but it's necessary and natural when dealing with real-time streams.\n\nLet's see it from a more abstract view. When processing frames coming from real-time streams, the streams are additional event sources other than user interactions through the frontend view. In normal Streamlit apps, all the events triggering Python script executions are only sourced from the frontend and they are nicely encapsulated by Streamlit.\n\nWith its execution model, then, developers can write the apps in a clean world where there are no callbacks and no (or little) side effects. In turn, if we want to handle the streams with good performance, we have to explicitly handle the events sourced from the streams like frame generations, which breaks the elegant encapsulation, causing callbacks and events to appear in the script.\n\nWhat tiny-streamlit-webrtc lacks\n\nThough we‚Äôve created a small subset of streamlit-webrtc, tiny-streamlit-webrtc, it still lacks many important features streamlit-webrtc has. Here we will review some of them.\n\nParameters input from Streamlit components\n\nOne of the biggest benefits of using Streamlit is interactive controls such as sliders and radio buttons. With computer vision and machine learning models, these controls are very useful to change the parameters during execution.\n\nBecause the computer vision code is running in the forked thread with this component, we have to pass the values obtained from Streamlit widgets to the CV code over the threads. But it is not difficult, like here in the streamlit-webrtc sample.\n\nWith tiny-streamlit-webrtc, you can do this by adding a public property to VideoTransformTrack and read and write it from each thread, just like the sample code linked above. Please try it if you are interested, and be careful about thread safety when you pass complex values.\n\nFrame drops\n\nWe‚Äôve used edge extraction as an example in the tutorial. However, if you replace it with more computationally expensive filters like deep neural networks, you will see the displayed video slows down. You can test it simply by putting time.sleep(1) in VideoTransformTrack.recv().\n\nThis is because VideoTransformTrack.recv() processes all the input frames one by one - if it delays, generating the output frames is also delayed.\n\nTo solve this problem, VideoTransformTrack.recv() has to drop some input frames and pick the latest one each time it runs. In streamlit-webrtc, it's done here when async_transform option is set as True.\n\nExtendability\n\nIn tiny-streamlit-webrtc, the video transformation is hard-coded inside VideoTransformTrack.recv(), but of course, this is bad design as a library. To be reusable, it should expose an injectable interface through which developers can implement arbitrary kinds of video transformation, encapsulating details such as VideoTransformTrack.recv() and WebRTC-related code.\n\nWith streamlit-webrtc, developers can implement their own video transformations by creating a class extending VideoTransformerBase class like this and this.\n\nKey takeaways\n\nStreamlit is a nifty framework with a useful library, but it doesn‚Äôt handle real-time video processing well on its own.\n\nWebRTC makes Streamlit even more awesome by enabling server-side processes and clients to send and receive data streams over the network with low latency.\n\nHave an amazing project in mind to use WebRTC for? Share it with us in the comments or message me.\n\nCredits\n\nReviewed by Yu Tachibana (@z_reactor)\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "deploy_small.gif (1080√ó640)",
    "url": "https://blog.streamlit.io/content/images/2021/11/deploy_small.gif",
    "html": ""
  },
  {
    "title": "Add and Share Custom App Functionality | Streamlit Components",
    "url": "https://blog.streamlit.io/introducing-streamlit-components/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing Streamlit Components\n\nA new way to add and share custom functionality for Streamlit apps\n\nBy Adrien Treuille\nPosted in Product, July 14 2020\nThe Streamlit Components Gallery\nBuilding your own components\nStatic Components\nGetting widget with it!\nSharing with the world\nTry it out and let us know what you think!\nContents\nShare this post\n‚Üê All posts\n\nIn the ten months since Streamlit was released, the community has created over 250,000 apps for everything from analyzing soccer games to measuring organoids, and from COVID-19 tracking to zero-shot topic classification. Inspired by your creativity, we added file uploaders, color pickers, date ranges, and other features. But as the complexity of serving the community grew, we realized that we needed a more scalable way to grow Streamlit‚Äôs functionality. So we‚Äôre turning to Streamlit‚Äôs best source of ideas: you!\n\nToday, we are excited to announce Streamlit Components, the culmination of a multi-month project to enable the Streamlit community to create and share bits of functionality. Starting in Streamlit version 0.63, you can tap into the component ecosystems of React, Vue, and other frameworks. Create new widgets with custom styles and behaviors or add new visualizations and charting types. The possibilities are endless!\n\nThe Streamlit Components Gallery\n\nThe first thing you should do is check out the Streamlit Components Gallery to see what others have created and shared.\n\nEach component can be installed with just a single line of code:\n\npip install some_cool_component\n\nIf you don‚Äôt find a component that works for you, you can make your own!\n\nBuilding your own components\n\nStreamlit has a unique, functional style which lets you create rich, interactive experiences in very few lines of code. For example, let‚Äôs check out this simple Streamlit app:\n\nimport streamlit as st\n\nx = st.slider('x')\nst.markdown(f'`{x}` squared is `{x * x}`')\n\nLooking at this code, you can see that Streamlit calls come in two flavors: static components like st.markdown are stateless and only send data to the browser, whereas bidirectional components like st.slider have internal state and send data back from the browser.\n\nOur challenge was to provide an API that embraces Streamlit‚Äôs functional style while capturing these use-cases as simply as possible. A few months ago, two amazing Streamlit engineers, Tim Conkling and Henrikh Kantuni, tackled this challenge and came up with a super elegant solution. The result is the new streamlit.components.v1 package which comprises three functions. For static components, we added:\n\nhtml(...), which lets you build components out of HTML, Javascript, and CSS\niframe(...) , which lets you embed external websites\n\nFor bidirectional components, we added:\n\ndeclare_component(...), which lets you build interactive widgets which bidirectionally communicate between Streamlit and the browser.\n\nLet‚Äôs dive into how it works!\n\nStatic Components\n\nLet‚Äôs start with a simple static component to embed snippets of code called Github gists in your app. Ideally, adding the component should just be a single function call:\n\n# Render a gist\ngithub_gist('tc87', '9382eafdb6eebde0bca0c33080d54b58')\n\nwhich would render a gist like this:\n\nTo create such a component, we start by importing the Streamlit Components library:\n\nimport streamlit.components.v1 as components\n\nThis somewhat wordy import statement serves two purposes:\n\nIt versions the components API so that future changes don‚Äôt break existing components.\nIt reminds us that we‚Äôre starting to use deep magic which we should hide from the user.\n\nNow let‚Äôs use the html(...) method to serve up the gist:\n\n\tdef github_gist(gist_creator, gist_id, height=600, scrolling=True):\n\t    components.html(\n\t        f\"\"\"\n\t\t  <script src=\"https://gist.github.com/{gist_creator}/{gist_id}.js\">\n\t\t  </script>\n\t\t\"\"\",\n\t        height=height,\n\t        scrolling=scrolling,\n\t    )\nview raw\ncomponents_1.py hosted with ‚ù§ by GitHub\n\nThis approach has a few awesome properties. First, it‚Äôs really simple and functional. In fact, this pattern lets you hide the ugly-looking HTML and wrap it into a pretty, Pythonic function call, github_gist(...). You can wrap code in a function and reuse it throughout your project. (Better yet, put it in a package and share it with the community in the gallery.) Second, note that we can add arbitrary HTML in this component‚Ää‚Äî‚Äädivs, spans, and yes, scripts! We can do this safely because the component is sandboxed in an iframe which lets us include external scripts without worrying about security problems.\n\nGetting widget with it!\n\nWhat if you want to create a stateful bidirectional component that passes information back to Python from the browser, or in other words, a widget? You can do this too! For example, let‚Äôs create a counter component:\n\ncount = counter(name=\"Fanilo\")st.write('The count is', count)\n\nwhich creates this:\n\nNote that this code follows Streamlit‚Äôs unique functional style and captures the counter state embedded in the component. How did we achieve this? Happily, a single function call, declare_component, does all the work to enable bidirectional communication with Streamlit.\n\n# Declare a simple counter component.import streamlit.components.v1 as componentscounter = components.declare_component(\"counter\", path=BUILD_PATH)\n\nNice! Under the hood, BUILD_PATH points to a component built using React, Vue, or any frontend technology you like. For this example we decided to use React and Typescript giving us this render function:\n\n\tpublic render = (): ReactNode => {\n\t  return (\n\t    <span>\n\t      Hello, {this.props.args[\"name\"]}! &nbsp;\n\t      <button onClick={this.onClicked} disabled={this.props.disabled}>\n\t        Click Me!\n\t      </button>\n\t    </span>\n\t  )\n\t}\nview raw\ncomponents_2.ts hosted with ‚ù§ by GitHub\n\tpublic render = (): ReactNode => {\n\t  return (\n\t    <span>\n\t      Hello, {this.props.args[\"name\"]}! &nbsp;\n\t      <button onClick={this.onClicked} disabled={this.props.disabled}>\n\t        Click Me!\n\t      </button>\n\t    </span>\n\t  )\n\t}\nview raw\ncomponents_2.ts hosted with ‚ù§ by GitHub\n\nand this callback:\n\n\tprivate onClicked = (): void => {\n\t  this.setState(\n\t    prevState => ({ numClicks: prevState.numClicks + 1 }),\n\t    () => Streamlit.setComponentValue(this.state.numClicks)\n\t  )\n\t}\nview raw\ncomponents_3.ts hosted with ‚ù§ by GitHub\n\nDonezo! You‚Äôve now created a simple, stateful component which ‚Äúfeels like React‚Äù on the website, and ‚Äúfeels like Streamlit‚Äù on the Python side. Information is passed back to Python using Streamlit.setComponentValue(...). Because we‚Äôre using React in this case, the component‚Äôs state is stored in this.state. For more details on this example, see our component template.\n\nA neat benefit of this architecture is that you‚Äôre not limited to React. You can use any language or framework which compiles for the web. Here is the same counter component written in ClojureScript.\n\n\t(defonce counter (atom 0))\n\t\n\n\t(defn increment-counter []\n\t  (swap! counter inc)\n\t  (send-message-to-streamlit :set-component-value {:value @counter}))\n\t\n\n\t(defn app []\n\t  [:button {:on-click increment-counter} \"Click Me!\"]\nview raw\ncomponents_4.cljs hosted with ‚ù§ by GitHub\n\nAnother cool feature of this API is that you can do hot-reloading as you develop your component like this:\n\ncomponents.declare_component(name, url=\"http://localhost:3001\")      \n\nHere, the url parameter lets you specify a dev server for the component created with npm run start.\n\nWhat we‚Äôve shown you so far just scratches the surface. For more details, please check our documentation.\n\nSharing with the world\n\nDid you create something broadly useful for the Streamlit community? Sure, you could keep that superpower for yourself, but it would be even cooler to share it! Get community feedback and praise. üòá You can easily wrap your component in a PyPi package and submit it to our Gallery by following these instructions.\n\nTry it out and let us know what you think!\n\nWe‚Äôre excited to unlock for the community a new way to plug-and-play functionality into Streamlit. Streamlit Components let you write simple HTML extensions or tap into the whole ecosystem provided by React, Vue, and other frameworks. Your feedback drives innovation in Streamlit. Please tell us what you think and what you‚Äôd like next. Show off your shiny new components and share them with the world. We can‚Äôt wait to see what you build! üéà\n\nSpecial thanks to Fanilo Andrianasolo, Daniel Haziza, Synode, and the entire Streamlit Components beta community who helped refine this architecture and inspired us with their feedback and ideas. Thanks also to TC Ricks, Amanda Kelly, Thiago Teixeira, Beverly Treuille, Regan Carey, and Cullan Carey for their input on this article.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "analytics-header.png (1459√ó1103)",
    "url": "https://blog.streamlit.io/content/images/2022/05/analytics-header.png#browser",
    "html": ""
  },
  {
    "title": "Diana Wang - Streamlit",
    "url": "https://blog.streamlit.io/author/diana/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Diana Wang\n1 post\nLeverage your user analytics on Streamlit Community Cloud\n\nSee who viewed your apps, when, and how popular they are\n\nProduct\nby\nDiana Wang and¬†\n1\n¬†more,\nMay 17 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit-Images.png (1394√ó536)",
    "url": "https://blog.streamlit.io/content/images/2021/11/Streamlit-Images.png",
    "html": ""
  },
  {
    "title": "Monthly rewind > June 2021",
    "url": "https://blog.streamlit.io/monthly-rewind-june-2021/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nMonthly rewind > June 2021\n\nYour June look back at new features and great community content\n\nBy Jessica Smith\nPosted in Monthly Rewind, July 5 2021\nüèÜ App of the month üèÜ\nStreamlit June updates\nüîç Current release: 0.84.0\nüîÆ Upcoming features\nSession state\nConnecting to databases\nFeatured Streamlit content\nFeatured community content\nContents\nShare this post\n‚Üê All posts\nüèÜ App of the month üèÜ\n\nOur June featured app of the month is......ü•Åü•Åü•Åü•Åü•Å\n\nMenara by Aktham Momani.\n\nIf you want to buy, sell, refinance, or even remodel a home this app offers many resources, estimates and forecasts to help you make the most informed decision. You can predict and forecast house prices and search GreatSchools within the California Bay Area. [code]\n\nStreamlit June updates\n\nHere are some updates on happenings at Streamlit this month.\n\nüîç Current release: 0.84.0\n\nThe latest release is 0.84.0. This release notably brought with it Session State! This highly requested feature allows information to be stored across app interactions and reruns. Make sure to check out the changelog to keep up to date with the latest features and fixes.\n\nüîÆ Upcoming features\n\nBe on the lookout for these new features:\n\nst.download\nst.card\nSession state\n\nRead more about the new Session State feature and how to get started in the launch article.\n\nConnecting to databases\n\nOur docs now include step-by-step guides showing how to connect Streamlit to different databases.\n\nFeatured Streamlit content\n\nPodcasts and articles written by or featuring the Streamlit team for your listening and reading pleasure.\n\nPart 1 and Part 2 of a new blog series Designing Streamlit Apps for the User offer helpful tips on how to take your apps to the next level. Stay tuned for Part 3!\n\nThe data science team at Cazoo explain how they are using Streamlit for Teams to build their DBT dashboard and other helpful company apps.\n\nListen to Johannes talk about his career journey and all things ML and AI in The AI Aesthetic: Building Beautiful Data Apps 2.\nThe AI Aesthetic: Building Beautiful Data Apps\nThe frontend often takes a backseat in the machine learning lifecycle. That‚Äôs why you need help to bring apps to life.\nRackspace Technology\nRackspace Technology Staff - Solve\nFeatured community content\n\nSome great apps, videos, and articles by the Streamlit community. Check out the Streamlit forum for even more community content.\n\nAkshansh created a CodeGame app where you build high scores by identifying the languages of random code snippets\nOptions-2-trees is an interactive visualization of the CRR binomial options pricing model by Tony\nLearn how to build a text classifier model and app in Jesse's tutorial NLP Project - Emotion In Text Classifier App with Streamlit and Python\nBilly goes over How To Create An Instagram Profile Analyzer App Using Python And Streamlit with instagram-scraper in his blog post\nIf you want to see how your Medium posts are performing, Rahul wrote about How to get a JSON file of your Medium Stats and create a Dashboard using Streamlit\nLabel intent, along with entity recognition and entity frequency for your GSC queries with Greg's GSC Query Analysis Tool\nFind your Hacker News Doppelg√§nger with this app by Pinecone, which compares the semantic meaning of your comment history with those of all other users, and finds the top ten users whose comment histories are most similar to yours\nSolar-MACH is an awesome app by Jan and crew that plots multi-spacecraft longitudinal configuration\nAlex made Jina App Store Search to show how to build an AI-powered search engine for an app store using the Jina framework\nIn his blog tutorial Visualizing Graph Embeddings with t-SNE in Python CJ shows how to inject graph embeddings created by the GDS library in Neo4j and visualize them in a dashboard\nAvra demonstrates How to Build Streamlit App connected to Google Sheet as Database [Streamlit-Google Sheet Automation] in his thorough video tutorial\n\nThanks for checking out this edition of the Monthly Rewind. If you'd like more frequent updates on what's happening in the community make sure to check out the Weekly Roundups.\n\nReach out to us on the forum with any questions or projects you're working on and follow our Twitter for the most up-to-date happenings!\n\nWant to see older rewinds? Check them out below:\n\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Monthly Rewind...\n\nView even more ‚Üí\n\nMonthly rewind > March 2023\n\nYour March look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nApril 24 2023\nMonthly rewind > February 2023\n\nYour February look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nMarch 22 2023\nMonthly rewind > January 2023\n\nYour January look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nFebruary 16 2023\nMonthly rewind > December 2022\n\nYour December look-back at new features and great community content\n\nMonthly Rewind\nby\nJessica Smith\n,\nJanuary 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Data_Importance2.gif (481√ó359)",
    "url": "https://blog.streamlit.io/content/images/2023/08/Data_Importance2.gif#border",
    "html": ""
  },
  {
    "title": "Streamlit Components: Our Security Model & Design Philosophy",
    "url": "https://blog.streamlit.io/streamlit-components-security-and-a-five-month-quest-to-ship-a-single-line-of-code/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit Components, security, and a five-month quest to ship a single line of code\n\nThe story of allow-same-origin\n\nBy Tim Conkling\nPosted in Tutorials, January 20 2021\nComponents: security + design\nallow-same-origin and the iframe sandbox\nBreaking the sandbox\nFirst, \"don't hijack my CSS\"\nAnd more importantly, \"don't hijack Streamlit Sharing\"\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn the changelog for Streamlit 0.73.0, released in December 2020, there‚Äôs a small callout: ‚ÄúComponent iframes now include the allow-same-origin sandbox attribute.‚Äù\n\nThis change enables dramatically more powerful Streamlit Components - you can now use webcams and microphones in Streamlit apps and more easily embed and interact with external resources - and it was just a single line of code! (Check out our ever-expanding Component Gallery for examples of quality Components created by the Streamlit community.)\n\nBut this is not a post about how to use or build Streamlit Components. If you're interested in that, we have a tutorial here or check out the great community tutorial by Fanilo Andrianasolo! Instead, we want to peel back the curtain and discuss how we make changes to Streamlit itself. Because we could have shipped that single line in July when we launched Streamlit Components. We could've shipped it in any of the 9 releases that followed! But instead, it took us 5 months.\n\nThis is an engineering-focused post about why such a small change took such a long time. It'll touch on the Streamlit security model, our design philosophy, and the competing constraints that can lead to long development times for seemingly-simple features. And for the masochists out there, we'll stare briefly into the abyss that is cross-origin web security.\n\nComponents: security + design\n\nLet's start with a humble-brag: Streamlit has lots of users, many of whom are storing and accessing sensitive data in Streamlit apps. We take security very seriously. What this means in practice is that any change or feature we add to Streamlit must not reduce the security of either the Streamlit open source library or Streamlit sharing, our \"press button ‚Üí deploy to cloud\" hosting platform.\n\nAdditionally, we care dearly about the design of Streamlit - not just the way it looks, but the way it works, all the way down to API names. This means that, as much as possible:\n\nStreamlit should just work.\nStreamlit features should be robust and powerful.\nNew features should not increase the complexity of installing, using, or deploying Streamlit.\n\nSo we have these three broad goals: add new features to Streamlit, don't undermine its simplicity, and ensure it's safe. When we're fortunate, these goals are not at odds with each other. When we're less fortunate, we have the allow-same-origin situation and we end up writing blog posts like this one.\n\nBefore getting into the weeds, let's first consider the story around Streamlit Components and security:\n\nFundamentally, a Streamlit Component is a Python library. You should exercise the same judgement with a Component as you would with any other Python library you pip install in your project.\nA Component also runs code on the frontend, which means it can make requests from the browser, and can access data on your app's frontend.\nYou should assume that any library you use - Component or otherwise - can access any data in your app.\nIf your app deals with sensitive data, only install libraries and Components that you have written, or that you otherwise trust.\n\nNo big surprises. But there's a wrinkle: as Streamlit Components was under development, so too was Streamlit sharing. We needed to make sure that a rogue Component in a shared app couldn‚Äôt peek at Streamlit sharing data, or execute commands on behalf of the developer.\n\ntl;dr for the rest of the blog post: nothing in a Streamlit app - malicious Component or otherwise - can hijack Streamlit sharing. But we treaded carefully - and a bit slowly - to make sure this was the case.\n\nallow-same-origin and the iframe sandbox\n\n(This section gets into the details of <iframe> sandboxing, the allow-same-origin sandbox flag, and cross-origin requests. It'll be of primary interest to those who work with, or are curious about, web security. If you have no interest in the nitty-gritty, skip ahead to the next section!)\n\nBroadly speaking, Streamlit Components are user-created plugins that extend Streamlit. You pip install a Component into your Python environment, and now you can add a forum, or an interactive 3D molecule viewer, or a Facebook HiPlot data graph, or custom charting libraries - or really any feature that Streamlit doesn't include out of the box - to your Streamlit app.\n\nDuring development, we had two primary concerns around the Component security model:\n\nA Component shouldn't be able to break assumptions about its surrounding page (changing the host app's CSS or DOM, for example).\nA Component in an app deployed with Streamlit sharing shouldn‚Äôt be able to hijack its owner's sharing credentials and read secret data or execute a CSRF exploit.\n\nUnder the hood, each instance of a Streamlit Component is mounted inside its own <iframe> in its containing Streamlit app, which means it lives in its own little world with its own DOM, its own CSS, and its own restrictions. Each iframe has a sandbox with a number of different attributes that specify what it can and can't do. For our purposes here, we're interested in two sandbox flags: allow-same-origin and allow-scripts.\n\nallow-scripts is self-explanatory: if it‚Äôs missing, then the iframe will not be able to execute any JavaScript. Executing JavaScript is a fundamental part of Streamlit Components, so this attribute must be enabled. allow-same-origin is related to ‚Äúcross-origin-resource-sharing‚Äù, or CORS - which means that it‚Äôs destined to be confusing and annoying. If you omit this attribute, the iframe won't be able to use certain browser features (like webcams and microphones), and it will be unable to make requests to many other web servers (which often expect a non-null origin).\n\nWhen Streamlit Components launched, we left off allow-same-origin because of how it interacts with allow-scripts. The MDN iframe page explains it thusly:\n\nWhen the embedded document has the same origin as the embedding page, it is strongly discouraged to use both allow-scripts and allow-same-origin, as that lets the embedded document remove the sandbox attribute ‚Äî making it no more secure than not using the sandbox attribute at all.\n\nStreamlit Components are served from the same origin as their embedding page, which means that combining allow-scripts and allow-same-origin would render our sandbox moot. This isn't necessarily a big deal, because Components are not \"untrusted code\" - but would potentially undercut our Component security concerns.\n\nThere's a big document memorializing weeks of discussion and argument on our allow-same-origin woes: should we serve components from a separate origin? Should we allow devs to opt into the allow-same-origin flag via a config option? Should we maintain an allow-list within Streamlit of Components that can use this flag?\n\nWe developed a number of prototypes that solved the issue in different ways. But all of them undercut Streamlit's \"keep things simple\" design principle:\n\nSome prototypes made Streamlit use more difficult (by requiring that dev deeply understand the Component sandbox model).\nSome made Streamlit deployment more difficult (by exposing more server ports to be forwarded and routed through proxies).\nAnd some made Component development more difficult (by imposing restrictions on Component creators).\nBreaking the sandbox\n\nAfter several months of proposals, prototypes, and arguments, we shipped Streamlit 0.73, which solved the problem by simply adding the allow-same-origin iframe flag. In other words, we decided to allow Components to break the iframe sandbox.\n\nWhy are we ok with this? And what are the ramifications? Here's where we landed on our original sandboxing concerns:\n\nFirst, \"don't hijack my CSS\"\n\"A Component shouldn't be able to break assumptions about its surrounding page (changing the host app's CSS or DOM, for example).\"\n\nOur decision here is simple: we decided that, while we won't encourage this sort of thing (not least because it's unsupported and therefore subject to break when Streamlit is updated), we're fundamentally ok with it. Official theming support is on the Streamlit roadmap for 2021, but if enterprising developers want to hack on Streamlit and create this sort of thing before we officially ship it, we won't stand in their way.\n\nStreamlit is an open source project anyway; if you don't like the way something works or looks, you can just fork the project and change it. We don't need a Component sandbox to enforce a rule that's incompatible with our open source nature.\n\nAnd more importantly, \"don't hijack Streamlit Sharing\"\n\"A Component in an app deployed with Streamlit sharing shouldn‚Äôt be able to hijack its owner's Sharing credentials and execute a CSRF exploit.\"\n\nWe need to ensure that a malicious Component - or any other rogue code that could be running within a Streamlit app - cannot execute Streamlit sharing commands surreptitiously.\n\nGoogling for \"CSRF example\" will return all sorts of resources that explain this type of exploit in detail. The important thing to know is that CSRF attacks use the fact that each HTTP request made by a browser will include the cookies associated with site to which the request is made. (There are various cookie attributes that make this story slightly more complex, but that's the basic rule.)\n\nWhen you're logged into Streamlit sharing and visit a deployed app you own, you get a management dashboard that lets you view logs and perform various administrative tasks:\n\nIf the Streamlit sharing administrator wrapper is served from the same HTTP origin as the app it's managing, a malicious Component could bypass Sharing's CSRF protections by making requests against the Streamlit sharing API and reading the CSRF token from the response headers.\n\nThe solution to this doesn't involve relying on Component sandboxing. In Streamlit sharing, an app's admin dashboard is simply served from a different origin than the app itself. This is similar to a \"serve Components from a different origin\" prototype we'd rejected on the basis of making deployment more complicated for users - but with the burden pushed up to Streamlit sharing instead. (We are more than happy to make deployment more complicated for the Streamlit sharing engineers while keeping it simple for you. Sorry not sorry, friends!)\n\nWrapping up\n\nWithin the Streamlit engineering team, the phrase \"allow same origin\" has practically achieved meme status - it was the issue we were always right on the verge of coming to consensus on. Throughout much of 2020, during our start-of-month company-wide planning meetings, the Components team kept claiming that we were about to make a final decision, only to walk back our over-eager prediction shortly afterwards.\n\nBut the saga is finally over! We've jettisoned the allow-same-origin iframe sandbox flag, and now you can stream webcam video, add a Disqus forum, embed Tweets, and use a whole host of other Components that were previously out of reach. Most importantly, your Streamlit sharing apps will remain safe from malicious code.\n\nAnd if you're lucky, you'll never have to think about null origins, CORS, or CSRF again. We'll handle it.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "collaborate-1.png (2000√ó849)",
    "url": "https://blog.streamlit.io/content/images/2021/11/collaborate-1.png",
    "html": ""
  },
  {
    "title": "Batch Input Widgets | Introducing Submit Button & Forms",
    "url": "https://blog.streamlit.io/introducing-submit-button-and-forms/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing Submit button and Forms üìÉ\n\nWe're releasing a pair of new commands called st.form and st.form_submit_button!\n\nBy Abhi Saini\nPosted in Tutorials, April 29 2021\nGetting started\nWhat this does\nForm submit button\nSimilarities\nDifferences\nForm Features\nForms inside columns\nColumns inside forms\nHelpful errors and warnings\nLimitations and other notes\nWrapping up\nResources\nContents\nShare this post\n‚Üê All posts\n\nHave you ever tried to build an app around a complex Machine Learning model, and found that rerunning the model every time the user changed an input value led to a less than ideal user experience? If so, it's likely because the parameters being supplied to your model use input widgets like st.text_input, st.number_input or st.slider and any time you change a widget the entire app is re-run.\n\nTo help solve this we're introducing a pair of commands called st.form and st.form_submit_button. This lets you batch input widgets together and submit the widget values with the click of a button ‚Äî triggering only a single rerun of the entire app!\n\nCheck out this sample app which shows the new commands in action, but make sure to keep reading if you want to learn more about how it works.\n\nGetting started\n\nForms are like any other Streamlit container and can be declared using the with statement:\n\n# Using the \"with\" syntax\nwith st.form(key='my_form'):\n\ttext_input = st.text_input(label='Enter some text')\n\tsubmit_button = st.form_submit_button(label='Submit')\n\n\nOr, if you prefer, you can also use object notation:\n\n# Declare a form and call methods directly on the returned object\nform = st.form(key='my_form')\nform.text_input(label='Enter some text')\nsubmit_button = form.form_submit_button(label='Submit')\n\n\nIn your app, this creates a form with a submit button as follows:\n\nWhat this does\n\nOutside of forms, any time a user interacts with a widget the app's script is rerun. What st.form does is make it so users can interact with the widgets as much as they want, without causing a rerun! Instead, to update the app, the user should click on the form's submit button.\n\nForm submit button\n\nIn the example above, notice that st.form_submit_button is in some ways similar to st.button, but differs in others:\n\nSimilarities\n\nJust like the regular st.button, the submit button for a form, st.form_submit_button returns a boolean to indicate whether the form was submitted or not. This allows for building additional logic upon submit button. For e.g.\n\nform = st.form(key='my-form')\nname = form.text_input('Enter your name')\nsubmit = form.form_submit_button('Submit')\n\nst.write('Press submit to have your name printed below')\n\nif submit:\n    st.write(f'hello {name}')\n\nDifferences\nA Form submit button is a special button which batch submits the state of the widgets contained in the form.\nA form must have an associated st.form_submit_button otherwise Streamlit throws an error.\nForm Features\n\nst.form can be placed anywhere in a Streamlit app, you can even put columns inside of forms, or forms inside of columns, and everything will work as expected!\n\nForms inside columns\ncol1, col2 = st.beta_columns(2)\n\nwith col1:\n    with st.form('Form1'):\n        st.selectbox('Select flavor', ['Vanilla', 'Chocolate'], key=1)\n        st.slider(label='Select intensity', min_value=0, max_value=100, key=4)\n        submitted1 = st.form_submit_button('Submit 1')\n\nwith col2:\n    with st.form('Form2'):\n        st.selectbox('Select Topping', ['Almonds', 'Sprinkles'], key=2)\n        st.slider(label='Select Intensity', min_value=0, max_value=100, key=3)\n        submitted2 = st.form_submit_button('Submit 2')\n\n\nChanging the contents of a form in the left column or submitting the left form does not impact the form on the right and vice versa.\n\nColumns inside forms\nwith st.form(key='columns_in_form'):\n    cols = st.beta_columns(5)\n    for i, col in enumerate(cols):\n        col.selectbox(f'Make a Selection', ['click', 'or click'], key=i)\n    submitted = st.form_submit_button('Submit')\n\n\nChanging the contents of any one of the select boxes does not impact the other select boxes and a rerun is triggered upon clicking the submit button.\n\nHelpful errors and warnings\n\nTo help developers in certain situations where it detects an out-of-place or missing submit button, Streamlit will shows a warning or throws an exception in the following cases:\n\nIf a st.form_submit_button is defined without a form scope, Streamlit will throw an exception and stop execution.\nIf no submit button is defined, Streamlit will show a warning without interrupting the app flow.\n\nLimitations and other notes\nAn ¬†st.form cannot be embedded inside another st.form.\nEvery form must have an associated st.form_submit_button.\nBy definition, st.buttons do not make much sense within a form. Forms are all about batching widget state together, but buttons are inherently stateless. So declaring an st.button inside a form will lead to an error.\nAlso by definition, interdependent widgets within a form are unlikely to be particularly useful. If you pass the output of widget1 into the input for widget2 inside a form, then widget2 will only update to widget1's value when the form is submitted.\nWe are currently working on functionality that allows you to programmatically reset all widgets when the submit button is clicked. This should be added in an upcoming release.\nWrapping up\n\nYou can now add st.form and st.form_submit_button to your apps to help make them more responsive in just two lines of code!\n\nSo go ahead and upgrade Streamlit to version 0.81.0 today!\n\npip install --upgrade streamlit\n\n\nWe're excited to see how you'll use these new commands, so make sure to come share what you create on our forum or on Twitter. If you have any questions about these (or about Streamlit in general) let us know on the forum or in the comments below! üéà\n\nResources\nForm docs\nGithub\nForum\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "App Layout & Style Tips | Designing Apps for User (Part II)",
    "url": "https://blog.streamlit.io/designing-streamlit-apps-for-the-user-part-ii/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to make a great Streamlit app: Part II\n\nA few layout and style tips to make your apps look even more visually appealing!\n\nBy Abhi Saini\nPosted in Tutorials, June 22 2021\nLay it out in columns\nUse layout to focus attention in a direction\nTheming, Colors and Contrast Ratios\nColors and contrast\nText Sizing and Emphasis\nMake it fun with logos, emojis and badges\nAdd logos\nAdd Favicons, Page Titles etc.\nAdd emojis and badges\nWrapping up\nResources\nAdditional References on choosing colors:\nContents\nShare this post\n‚Üê All posts\n\nAt Streamlit, we strive to enable developers to easily make beautiful apps that create great experiences for their users. In Part I of this blog series, we covered how to Design for the User. ¬†In Part II, we jump into how you can use layout and design options to make visually appealing apps for your app viewers.\n\nKicking off this post, the main things to keep in mind are:\n\nLay it out in columns\nChoose your theme colors and contrast ratios wisely\nUse text sizing for emphasis\nMake it fun with badges, logos and emojis ü•≥\n\nLet's jump in!\n\nLay it out in columns\n\nOrganizing your app with column layouts with st.beta_columns helps place content in a uniform way across the app. A great example is Streamlit Cheat Sheet (source code) by Daniel Lewis. Having information represented in columns create a proportional distribution of information and gives consistency to the interface.\n\nAdditionally, this page uses wide mode to show even more content. Page contents can be laid out in 'centered' or 'wide' mode(as shown below) using st.set_page_config :\n\n# NOTE: This must be the first command in your app, and must be set only once\nst.set_page_config(layout=\"wide\")\n\nUse layout to focus attention in a direction\n\nHorizontal Flow\n\nIn this type of layout, we want the viewer to scan information from left to right, for eg. the NYC Uber Ridesharing App (source code) by Streamlit. In the first row, we see 3 charts containing similar conceptual information, i.e., airport pickups shown left to right. In the 2nd row, we see pickups per minute represented across the minutes of the hour.\n\nThe above layout (with one columns with double and the rest with single width) can be achieved with this code:\n\ncol1, col2, col3, col4 = st.beta_columns((2,1,1,1))\n\nwith col1:\n    # Add chart #1\n\n...\n\nwith col4:\n    # Add chart #4\n\n# Add bottom chart\n\n\n\nVertical Flow with a Sidebar\n\nIn this type of layout, information is presented vertically and the viewer is encouraged to consume the data starting at the top and then scrolling to the bottom. This often carries with it the implicit understanding that an item presented is logically dependent on the item above it. For e.g. the PGA Modeler app (source code) by Andy Uttley.\n\nThe above layout has a sidebar which contains all the input widgets like sliders, buttons, checkboxes on the left and all of the output widgets like charts, images, data tables etc. in the main body of the app. This layout can be achieved with the following code:\n\n# Input widgets in the sidebar\nwith st.sidebar:\n\t# input widget 1\n\t# input widget 2\n\t...\n\n# Load data based on the inputs from the sidebar widgets\n\n# Main body contents: Output Widgets\nst.dataframe(data)\n...\nst.line_chart()\n\nTheming, Colors and Contrast Ratios\n\nStreamlit supports Theming and Dark mode, which allows you to choose between multiple default (light, dark) themes or create your own. Get your company colors in there or just pick something you love. For an example, see the theming launch sample app (source code) by Streamlit.\n\nColors and contrast\n\nWhen picking your colors take some time to think about how they work together and the contrast between your light and dark colors. If two adjacent elements have the same color or very similar colors, it makes the content very difficult, if not impossible, to read. When contrast is low between text and background, the message blends together, reducing legibility of the display. Here is a great article on the Role of Color in UX that can help you pick great colors for your app theme\n\nText Sizing and Emphasis\n\nHaving text with different sizing can reduce the cognitive load on the reader since it creates a hierarchy of importance. Bigger text generally draws more user attention!\n\nThere are several st commands which allow the user to do semantic titling. Each of these commands: st.title, st.header, st.subheader allows the user to create emphasis and use vertical space to give the page more structure. Below, we compare the same text written using different text sizing.\n\nYou can also use ¬†st.caption to display non-important information for captions below plots, tables or paragraphs.\n\nA good rule of thumb is to start with a title via st.title and have a short 2-sentence description via st.write of the app below it to explain more about how the app works. See a great example of this in the Lord of the Rings Text Generator by Christian Doucette.\n\nMake it fun with logos, emojis and badges\nAdd logos\n\nWith just a couple of lines of code, you can add your company's logo to an app to reflect your company's branding:\n\nst.image(logo_url, width=100)\nst.title(\"Streamlit Dashboard Demo\")\n\n\nAnd if you want the logo next to your text, just use st.beta_columns.\n\nAdd Favicons, Page Titles etc.\n\nst.set_page_config allows you to set some default settings for the page like the Page Title, icon, sidebar state etc:\n\nst.set_page_config(\n    page_title=\"Ex-stream-ly Cool App\",\n    page_icon=\"üßä\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\nAdd emojis and badges\n\nAt Streamlit, we love emojis and like it when apps use them in interesting ways. They can be added to titles as shown in the Year on Github app by Johannes Reike (source code) shown below.\n\nAdditionally, try adding in badges for your users to connect with your project on social media.\n\nTo add a badge to star your Github project (replace <username> and <repo>):\n\nst.write(\"[![Star](<https://img.shields.io/github/stars/><username>/<repo>.svg?logo=github&style=social)](<https://gitHub.com/><username>/<repo>)\")\n\n\nOr follow you on Twitter:\n\nst.write(\"[![Follow](<https://img.shields.io/twitter/follow/><username>?style=social)](<https://www.twitter.com/><username>)\")\n\n\nOr create other badges via https://shields.io/ (especially the Social category) and add them to your app with markdown similar to above.\n\nüí° Pro tip: You can also consider adding a Streamlit badge to your GitHub repo.\n\nWrapping up\n\nWe hope that by following these pointers, your Streamlit apps will look great and be more fun for your users. To use the features discussed above in your apps, make sure to upgrade to the latest version of Streamlit:\n\npip install --upgrade streamlit\n\n\nIn Part 3, of the blog series, we'll cover how to make your apps more performant so that using them is a fast and zippy experience for everyone.\n\nIf you have any questions let us know below or on the forum!\n\nResources\nGithub\nForum\nAdditional References on choosing colors:\nFundamentals of color in user interface design (UI)\nUI Design: Choosing Color Palettes\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "What is Apache Arrow, How it Works & More| Streamlit",
    "url": "https://blog.streamlit.io/all-in-on-apache-arrow/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAll in on Apache Arrow\n\nHow we improved performance by deleting over 1k lines of code\n\nBy Henrikh Kantuni\nPosted in Product, July 22 2021\nWhat is Arrow, and how does it work\nWhat this means for your Streamlit apps\nFaster and more efficient DataFrames\nOther benefits to using Arrow\nHow do I use it\nSharp edges\nWrapping it up\nResources\nContents\nShare this post\n‚Üê All posts\n\nA long time ago in a galaxy far, far away, among the first few lines of code ever committed to Streamlit was a painstakingly crafted module that serialized Pandas DataFrames (read \"fancy tables\") into a complex set of custom Protobufs (read \"fancy JSON\") ‚Äî plus the inverse of that module to deserialize them back into arrays in the browser.\n\nLet's backtrack for a second. Why is this even needed? As you know, a great portion of Streamlit commands receive DataFrames as input arguments. It makes sense: DataFrames are efficient, versatile, and easy to work with. We all love them. They were a complete game-changer when they first appeared in the Python ecosystem. And while your DataFrames hung out in Python land, everything was hunky-dory. But as soon as we had to send them to the JavaScript territories, there be dragons: DataFrames were just not traditionally suited to be sent over-the-wire, and there was no standard JavaScript library to handle them on the browser side anyway. Hence, all the custom code.\n\nAnd this came with a cost...\n\nOur custom format grew considerably slower as users pushed for larger and larger DataFrames. Furthermore, every time Pandas released new features, adding support for them in Streamlit meant undertaking a considerable amount of work. Finally, we weren't big fans of our old custom module, to begin with!\n\nSo the search for a better format began. We spent months researching different solutions, trying out serialization formats, embarking on false starts, and going right back to where we started. Our old code still checked more boxes than all the options out there.\n\nUntil finally, we found it!\n\nYes, this blog post is about the TV show.\n\nMeet Arrow, an efficient memory format and set of libraries that will handle Streamlit's DataFrame serialization from now on. We love it, and we think you will too.\n\nWhat is Arrow, and how does it work\n\nArrow is a memory format for DataFrames, as well as a set of libraries for manipulating DataFrames in that format from all sorts of programming languages. From the Arrow website:\n\n\"A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (including nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more.\"\n\nLet's break that down:\n\nIt's column-oriented: so doing things like computing the sum of a column of your DataFrame is lightning fast.\nIt's designed to be memory-mapped: so serialization/deserialization are basically free. Just send the bytes over the wire as they are.\nIt's language-agnostic: so it's well-supported both in Python and JavaScript.\nPlus it supports all those DataFrame features that our custom module never got around to. This means that adding support to new DataFrame features into Streamlit is a much, much smaller undertaking.\n\nWhat this means for your Streamlit apps\nFaster and more efficient DataFrames\n\nIn our legacy serialization format, as DataFrame size grew, the time to serialize also increased significantly. Iterating through the DataFrame, converting all its data into formats we could use, packing them into a whole hierarchy of Protobufs, that's a whole lot of work that Arrow's memory-mapped format just throws out the window. Just compare the performance of our legacy format vs Arrow. It's not even funny!\n\nOther benefits to using Arrow\nAs new features are introduced to Arrow, Streamlit can support them with much less work. Remember those features we never got around to supporting? Well, we just added a bunch of them now: table captions, categorical indices, interval indices, multi-index Styler objects, and more!\nIf your app uses Arrow Tables, Streamlit will now accept them anywhere a Pandas DataFrame is accepted. Except it's, you guessed it, much faster.\nThis one is mostly a benefit for us, Streamlit devs: we get to delete over 1k lines of code from our codebase. You can't believe how good this feels üòÉ\nHow do I use it\n\nJust upgrade to the latest version of Streamlit! Arrow is the default serialization format starting with version 0.85:\n\npip install --upgrade streamlit\n\nIn the vast majority of cases, no updates to your code are needed! You can still give Streamlit your Pandas DataFrames just as before, and we'll convert it to Arrow behind the scenes for you.\n\nSharp edges\nArrow is a bit more strict than Pandas about keeping your DataFrames organized. For example, elements in the same column of a DataFrame must all have the same data type. But we find that this is an overall benefit to your data model, and valid reasons for breaking this rule are rare.\nSome features such as PeriodIndex and TimedeltaIndex are not yet fully supported. But, as mentioned earlier, adding them is easier than ever.\nStyler concatenation is no longer supported when using add_rows(). This is not really an Arrow-specific issue, but more that when you concatenate Styler objects it may not do what you wanted: if the Styler is configured to draw the max of a column in red, for example, then a correct concatenation would require recomputing the max for the entire column. Which is... suboptimal.\nThis is a big change, and even though we tested it thoroughly, there may still be bugs lurking around.\n\nIf you encounter a bug (or the caveats above are blocking for you), you can revert to the previous implementation by setting the dataFrameSerialization config option to \"legacy\" in your config.toml as shown below:\n\n[global]\ndataFrameSerialization = \"legacy\"\n\n\nAnd if you do that, please file an issue on GitHub so we can work on fixing whatever issue you found ASAP.\n\nWrapping it up\n\nArrow is the new hotness, and it's where we believe the ecosystem is moving. So we're super excited to finally jump on that rocketship and help propel it forward with all of you.\n\nHave fun with the updates, and looking forward to hearing what you think! As usual, come share what you create on the forum or Twitter. If you have any questions about Arrow or Streamlit, or just want to say hi, leave a comment below or on the forum. üéà\n\nResources\nDocumentation\nGitHub\nForum\n\nThanks to Abhi Saini, Alex Reece, Amanda Kelly, Jon Roes, Marisa Smith, and Tim Conkling for their input on this article.\nMassive shout-out to Thiago Teixeira and TC Ricks for the enormous work they did to create this beauty!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Launching a brand-new docs site ü•≥",
    "url": "https://blog.streamlit.io/launching-a-new-docs-site/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLaunching a brand-new docs site ü•≥\n\nImproved layout, easier navigation, and faster search\n\nBy Snehan Kekre\nPosted in Product, October 13 2021\nüìô Overview\nüîé API reference\nüéì Knowledge base\nüéÅ Wrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey Streamlit community! üëã\n\nWe're thrilled to share with you our brand-new docs site. It's a standalone site designed to be your single reference point for all things Streamlit.\n\nWithout further ado, here's what's new:\n\nüìô Overview\n\nWe've updated our docs with beautiful graphics, a new structure, and a clear delineation between:\n\nOur core product‚ÄîStreamlit library üìÑ\nOur commercial offering‚ÄîStreamlit Cloud ‚òÅÔ∏è\nOur catchall resource‚ÄîKnowledge base üéì\n\nThis layout was designed with new and returning users in mind. It makes navigation easier, improves search, and increases the discoverability of relevant pages (especially Streamlit functions).\n\nFor example, if you're learning Streamlit, go to the Get started page in the Streamlit library. You'll learn how Streamlit works, how to install it, and how to create your first app!\n\nIf you're deploying a Streamlit app, go to the Streamlit Cloud page. You'll find links and instructions on how to deploy on our free Community tier and on the Teams and Enterprise tiers.\n\nIf you run into issues using Streamlit or deploying apps, head to our Knowledge base.\n\nüîé API reference\n\nOne of the most highly requested features was a dedicated page for each Streamlit function. We've done exactly that in our new API reference.\n\nEach page now supports multimedia and YouTube links, references to blog posts, discussion forum links, and lots of other resources. Plus, each Streamlit function now has its own dedicated URL. This makes each module more easily discoverable on search engines.\n\nüéì Knowledge base\n\nOur documentation now includes a growing repository of articles that answer your questions about creating and deploying Streamlit apps. Previously, if your question wasn't answered in the FAQ, you had to search for it in our forums.\n\nWe aggregated the most common questions and discussions within the community to bring you all the answers in one place‚Äîthe Knowledge base.\n\nIf you have trouble installing dependencies, deploying apps, or have a question about using Streamlit with your favorite libraries, our Knowledge base has got you covered.\n\nBy definition, a knowledge base is a continual work in progress. There are bound to be edge-cases and issues that the majority of the community has never come across. When working on the cutting-edge of data science, you're always faced with unknowns.\n\nIf you have a solution for an issue and want to see it in the Knowledge base, please write an article! Include as many details as possible. Here are the instructions on how to contribute. If you see anything that can be improved in existing articles, create a new issue, submit a pull request, and let us know in the comments below or on the forum.\n\nüéÅ Wrapping up\n\nThis launch marks a milestone in the development of our open-source product, Streamlit, and our commercial offering, Streamlit Cloud. Alongside the Streamlit 1.0 release, our documentation has received major visual and structural updates:\n\nStreamlit library and Streamlit Cloud sections.\nAPI reference with dedicated pages for each Streamlit function.\nKnowledge base as a self-serve library of tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.\n\nThese updates bring us closer to two goals:\n\nReducing the time and effort required to find answers to your questions about Streamlit.\nPutting relevant information in the hands of new users and experienced developers.\n\nWe're excited for you to explore our new documentation site and hear what you think.\n\nHappy Streamlit-ing! ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "current-process.png (1386√ó1058)",
    "url": "https://blog.streamlit.io/content/images/2021/11/current-process.png",
    "html": ""
  },
  {
    "title": "data-apps.png (1799√ó1292)",
    "url": "https://blog.streamlit.io/content/images/2021/11/data-apps.png",
    "html": ""
  },
  {
    "title": "Store Information Across App Interactions | Session State",
    "url": "https://blog.streamlit.io/session-state-for-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSession State for Streamlit üéà\n\nYou can now store information across app interactions and reruns!\n\nBy Abhi Saini\nPosted in Product, July 1 2021\nAdd State to your App\nCallback functions and Session State API\nWrapping up\nResources\nContents\nShare this post\n‚Üê All posts\n\nSoon after Streamlit launched in 2019, the community started asking for ways to add statefulness to their apps. Hacks for Session State have been around since October 2019, but we wanted to build an elegant solution that you could intuitively weave into apps in a few lines of code. Today we're excited to release it!\n\nYou can now use Session State to store variables across reruns, create events on input widgets and use callback functions to handle events. This powerful functionality helps create apps which can:\n\nPerform data/image annotation\nSupport Pagination\nAdd widgets that depend on other widgets\nBuild simple stateful games like Battleship, Tic Tac Toe, etc.\nAnd much more - all of this with the simplicity of writing apps that are Python scripts!\n\nüí° If you want to jump right in, check out our demo to see some of the above apps in action or head to the docs for more detailed info on getting started.\n\nAdd State to your App\n\nIn Streamlit, interacting with a widget triggers a rerun and variables defined in the code get reinitialized after each rerun. But with Session State, it's possible to have values persist across reruns for those instances when you don't want your variables reinitialized.\n\nFor example, here's a simple counter that maintains a count value across multiple presses of an increment button. Each button press triggers a rerun but the count value is preserved and incremented (or decremented) across the rerun:\n\nimport streamlit as st\n\nst.title('Counter Example')\n\n# Streamlit runs from top to bottom on every iteraction so\n# we check if `count` has already been initialized in st.session_state.\n\n# If no, then initialize count to 0\n# If count is already initialized, don't do anything\nif 'count' not in st.session_state:\n\tst.session_state.count = 0\n\n# Create a button which will increment the counter\nincrement = st.button('Increment')\nif increment:\n    st.session_state.count += 1\n\n# A button to decrement the counter\ndecrement = st.button('Decrement')\nif decrement:\n    st.session_state.count -= 1\n\nst.write('Count = ', st.session_state.count)\n\n\nüí° To continue building on this example, follow along in our Topic Guide: Add State to your App ü§ì\n\nThe above shows a basic example of how values can persist over reruns, but let's move on to something a little more complex!\n\nCallback functions and Session State API\n\nAs part of this release, we're launching Callbacks in Streamlit. Callbacks can be passed as arguments to widgets like st.button or st.slider using the on_change argument.\n\nüí° Curious what a callback is? Wikipedia phrases it well: \"a callback, also known as a \"call-after\" function, is any executable code that is passed as an argument to other code; that other code is expected to call back (execute) the argument at a given time. \" Here's a link if you'd like to read more.\n\nWith Session State, events associated with changes to a widget or click events associated with button presses can be handled by callback functions. It's important to remember the following order of execution:\n\nOrder of Execution: If a callback function is associated with a widget then a change in the widget triggers the following sequence: First the callback function is executed and then the app executes from top to bottom.\n\n\nHere's an example:\n\nimport streamlit as st\n\ndef update_first():\n    st.session_state.second = st.session_state.first\n\ndef update_second():\n    st.session_state.first = st.session_state.second\n\nst.title('ü™û Mirrored Widgets using Session State')\n\nst.text_input(label='Textbox 1', key='first', on_change=update_first)\nst.text_input(label='Textbox 2', key='second', on_change=update_second)\n\n\nIn the above, we showcase the use of callbacks and session state. ¬†We also showcase an advanced concept, where session state can be associated with widget state using the key parameter.\n\nTo read more on this, check out the Advanced Concepts section in the Session State docs and to check out the API in detail visit the State API documentation.\n\nWrapping up\n\nThat's it for the intro to Session State, but we hope this isn't the end of the conversation! We're excited to see how you'll use these new capabilities, and all the new functionalities state will unlock for the community.\n\nTo get started, upgrade to the latest release to use st.session_state and callbacks in your apps:\n\npip install --upgrade streamlit\n\n\nIf you have any questions about these (or about Streamlit in general) let us know below in the comments or on the forum. And make sure to come by the forum or Twitter to share all the cool things you make! üéà\n\nResources\nSession State Topic Guide ¬†\nSession State API Reference\nSession State Demo App\nGithub\nForum\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "TC Ricks - Streamlit",
    "url": "https://blog.streamlit.io/author/tc/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by TC Ricks\n4 posts\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nMonthly rewind > January 2021\n\nYour January look back at new features and great community content\n\nMonthly Rewind\nby\nTC Ricks\n,\nFebruary 8 2021\nAdding beta and experimental ‚Äúchannels‚Äù to Streamlit\n\nIntroducing the st.beta and st.experimental namespaces\n\nProduct\nby\nTC Ricks\n,\nMay 6 2020\nTry Nightly Build for cutting-edge Streamlit\n\nA new style of release for anyone who wants the most up-to-date Streamlit version\n\nProduct\nby\nTC Ricks\n,\nApril 17 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Thiago Teixeira - Streamlit",
    "url": "https://blog.streamlit.io/author/thiago/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Thiago Teixeira\n2 posts\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nProduct\nby\nAdrien Treuille and¬†\n2\n¬†more,\nMarch 2 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Announcing Streamlit 1.0! üéà",
    "url": "https://blog.streamlit.io/announcing-streamlit-1-0/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnnouncing Streamlit 1.0! üéà\n\nStreamlit used to be the simplest way to write data apps. Now it's the most powerful\n\nBy Adrien Treuille\nPosted in Product, October 5 2021\nApps were just scripts!\nStreamlit 1.0! üéà\nContents\nShare this post\n‚Üê All posts\n\nWe launched Streamlit in 2019 with a radical idea: making data apps should be simple. Your response exceeded our highest hopes. Tens of thousands of data scientists and thousands of companies turned to Streamlit to share rich models, deep analyses, and complex datasets.\n\nOvernight, expectations became sky-high. You filled our forums. You inundated our issue boards. You wanted Streamlit to be more beautiful, more powerful, more programmable, and faster.\n\nInspired by this swell of energy, we set out to make your dreams real. But the features you wanted threatened to complicate Streamlit. How could we make Streamlit a powerful, production-ready app framework while preserving its core simplicity?\n\nWe had to boil it down. Why was Streamlit simple? What made it so special?\n\nThe answer was our initial insight...\n\nApps were just scripts!\n\nThese two lines said it all:\n\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n\nIn just two lines of Python, Streamlit dissolved all complexity of app development: layout, input, output, interaction, and callbacks. The learning curve was zero! ‚ú®\n\nSo far so good. But as data scientists, we didn't just square numbers. We built models. We conjured visualizations. We wrangled datasets. We harmonized geographic data with sentiment analysis and feature libraries. We shaped thinking.\n\nCould Streamlit's simple scripting model scale?!\n\nFor help, we turned to Streamlit's guiding light. The community. You. We asked you questions. You listened and responded. Together, we worked on countless revisions to the APIs. We simplified. Then we simplified even more.\n\nOne by one, we tackled the challenges of making each of your dreams real:\n\n1. Layouts. The first step was to make beautiful layouts that danced and reacted to user input. How could we fit this into Streamlit's simple scripting model?\n\nThis led us to code like this:\n\nairports = [\"La Guardia Airport\", \"JFK Airport\", \"Newark Airport\"]\nfor airport, col in zip(airports, st.columns(len(airports)):\n   with col:\n      st.subheader(airport)\n      render_airport(airport)\n\nSee how the columns fit perfectly into a simple for loop? Awesome! A new layout superpower! üí™\n\n2. Components. Next, we wanted to transcend the structures of Streamlit's 37 core functions. So we added support for third-party components:\n\nfrom webcam_component import webcam\n\ncaptured_image = webcam()\nif captured_image is not None:\n   st.image(captured_image)\n\nYou swept in and filled our gallery with an incredible ecosystem of components. Now Streamlit apps could refract a full rainbow of web technologies:\n\n3. State. Up next, we worked on state, adding new complexity in a few lines of code:\n\nif st.button('Increment'):\n    st.session_state.count += 1\nst.write('Count = ', st.session_state.count)\n\n4. Speed. You wanted speed. So we introduced powerful new caching primitives that made your apps fast:\n\n@st.experimental_singleton\ndef connect_to_database(url):\n   engine = create_engine(url)\n   return sessionmaker(engine)\n\n@st.experimental_memo\ndef query_database(_db):\n   with _db() as session:\n      query = session.query(user.id, user.last_name, user.ltv)\n   return pd.read_sql(query.statement, query.session.bind)\n\nWith each challenge conquered, our confidence and excitement grew. Streamlit's core developers joined with the community to build more and more features, faster and faster.\n\nBut you didn't stop there. You hacked at our rough edges. You tore apart our abstractions. You encouraged us to make Streamlit even better, even simpler, and infinitely more powerful. You believed in us.\n\nAnd then it happened. We have built powerful new features and preserved Streamlit's core simplicity. The abstractions scaled!\n\nOn the wave of this excitement, a new milestone came into view...\n\nStreamlit 1.0! üéà\n\nThis signifies the end of our first journey:\n\nWe've grown from three co-founders to a team of almost 50 (we're hiring!).\nOur community has grown beyond our wildest dreams with more than 4.5 million downloads.\nStreamlit now has more than 16,000 GitHub stars and is used by more than 10,000 organizations (including over half of the Fortune 50).\n\nFunnily enough, we feel like we're back where we started. Once again, our community wants powerful features. But this time it's different. This time we know that the Streamlit model will scale.\n\nSo today we're also sharing with you our new roadmap:\n\nMagical apps. We already made it 10x faster for you to make great apps. Now we want to make those apps even better. We'll be adding an unbeatable set of widgets‚Äîeverything from sortable/filterable/editable databases and tables to clickable charts, to image selectors and editors, to amazing audio and video players and uploaders (and more options for layout and customization!).\nFirst-class developer experience. We want everything about coding a Streamlit app to be an awesome experience. So we'll make it easier for you to connect to data sources, cache data, interact with it, and debug your apps.\nEnhanced user experience. We want to help you make great apps for your users. We'll be designing a distinct user experience. App users will be able to understand the app, interact with it, and give you direct feedback.\nRapidly expanding ecosystem. You wanted it to be even easier for fellow developers to share code, components, apps, and answers. So we'll be launching new features that make it super simple to get started with new apps, find code snippets, search for the right add-on components, engage with the community, and get recognized for your contributions.\n\nOh, and one more exciting thing. We‚Äôre committing to follow a loose variant of semantic versioning. For details, see our upcoming release notes.\n\nThat's it for the Streamlit 1.0 announcement!\n\nWe're very proud to have shared our journey with you‚Äîour magical community. You are what makes Streamlit special. Thank you for inspiring us with your feedback, enthusiasm, and creativity. Please keep sharing your apps with the world. And keep sending us comments, ideas, bugs, feature requests, articles, and words of encouragement.\n\nWe wouldn't be here without you. ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Product Announcements",
    "url": "https://blog.streamlit.io/tag/product/page/4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Product\n36 posts\nNew layout options for Streamlit\n\nIntroducing new layout primitives‚Äîcolumns, containers, and expanders!\n\nProduct\nby\nAustin Chen\n,\nOctober 8 2020\nIntroducing Streamlit Components\n\nA new way to add and share custom functionality for Streamlit apps\n\nProduct\nby\nAdrien Treuille\n,\nJuly 14 2020\nAnnouncing Streamlit's $21M Series¬†A\n\nDeveloping new superpowers for the data science community\n\nProduct\nby\nAdrien Treuille\n,\nJune 16 2020\nAdding beta and experimental ‚Äúchannels‚Äù to Streamlit\n\nIntroducing the st.beta and st.experimental namespaces\n\nProduct\nby\nTC Ricks\n,\nMay 6 2020\nTry Nightly Build for cutting-edge Streamlit\n\nA new style of release for anyone who wants the most up-to-date Streamlit version\n\nProduct\nby\nTC Ricks\n,\nApril 17 2020\nThe Streamlit roadmap‚Äîbig plans for 2020!\n\nDevoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers\n\nProduct\nby\nAdrien Treuille\n,\nFebruary 27 2020\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "soc-2-small-long.png (1500√ó506)",
    "url": "https://blog.streamlit.io/content/images/2022/01/soc-2-small-long.png",
    "html": ""
  },
  {
    "title": "Henrikh Kantuni - Streamlit",
    "url": "https://blog.streamlit.io/author/kantuni/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Henrikh Kantuni\n2 posts\nAll in on Apache Arrow\n\nHow we improved performance by deleting over 1k lines of code\n\nProduct\nby\nHenrikh Kantuni\n,\nJuly 22 2021\nElm, meet Streamlit\n\nA tutorial on how to build Streamlit components using Elm\n\nTutorials\nby\nHenrikh Kantuni\n,\nDecember 8 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "0.88.0 release notes",
    "url": "https://blog.streamlit.io/0-88-0-release-notes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n0.88.0 release notes\n\nThis release launches st.download_button as well as other improvements and bug fixes\n\nBy Abhi Saini\nPosted in Release Notes, September 3 2021\n‚ú® Release Highlight\n‚¨áÔ∏è Download Button\nAPI Details\nParameters\nReturns\nExample Usage\nüß© Other notable updates\nüèÅ Wrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey Streamlit Fam! üëã\n\nSay hello to a notable new feature in the 0.88.0 release: st.download_button. Now your viewers can download data right from your Streamlit apps!\n\nIn this post, we'll go into more detail about the new download button and other updates.\n\nWant to jump right in? Here's a sample app and here are the docs.\n\n‚ú® Release Highlight\n‚¨áÔ∏è Download Button\n\nBefore this release, you had to hack your way to the download functionality within Streamlit. None of the hacks were great. And they didn't play well with the Streamlit Cloud platform.\n\nWith st.download_button you can use the download functionality both locally and within our cloud platform.\n\nCurious how it's built?\n\nAPI Details\n\nstreamlit.download_button(label, data, file_name=None, mime=None, key=None, help=None, on_click=None, args=None, kwargs=None)\n\nThis will display a button for your viewers so they can download data locally to their machines.\n\nParameters\nlabel (str) ‚Äì A short label explaining to the user what this button is for.\ndata (str or bytes or file) ‚Äì The contents of the file to be downloaded.\nfile_name (str) - An optional string to use as the name of the file to be downloaded, eg. 'my_file.csv'. If file_name is not specified, a name will be automatically generated.\nmime (str or None) ‚Äì The MIME type of the data. If data=str and MIME is unspecified, then MIME defaults to ‚Äútext/plain‚Äù. If data=bytes and MIME is unspecified, then MIME defaults to ‚Äúapplication/octet-stream‚Äù.\nkey (str) ‚Äì An optional string to use as the unique key for the widget. If you omit this, a key will be generated for the widget based on its content. Multiple widgets of the same type may not share the same key.\nhelp (str) ‚Äì An optional tooltip that gets displayed when the button is hovered over.\non_click (callable) ‚Äì An optional callback invoked when this button is clicked.\nargs (tuple) ‚Äì An optional tuple of args to pass to the callback.\nkwargs (dict) ‚Äì An optional dict of kwargs to pass to the callback.\nReturns\n\nIf the button was clicked on the last run of the app, it will be True. Otherwise, it will be False. Additionally, the return type is bool.\n\nExample Usage\n# Text files\n\ntext_contents = '''\nFoo, Bar\n123, 456\n789, 000\n'''\n\n# Different ways to use the API\n\nst.download_button('Download CSV', text_contents, 'text/csv')\nst.download_button('Download CSV', text_contents)  # Defaults to 'text/plain'\n\nwith open('myfile.csv') as f:\n\tst.download_button('Download CSV', f)  # Defaults to 'text/plain'\n\n# ---\n# Binary files\n\nbinary_contents = b'whatever'\n\n# Different ways to use the API\n\nst.download_button('Download file', binary_contents)  # Defaults to 'application/octet-stream'\n\nwith open('myfile.zip', 'rb') as f:\n\tst.download_button('Download Zip', f, file_name='archive.zip')  # Defaults to 'application/octet-stream'\n\n# You can also grab the return value of the button,\n# just like with any other button.\n\nif st.download_button(...):\n\tst.write('Thanks for downloading!')\n\nüß© Other notable updates\n\nBelow are some other notable updates on this release:\n\nüõë We made changes to improve the redacted exception experience on Streamlit Cloud. When client.showErrorDetails=true exceptions display the Error Type and Traceback, the actual error text will be redacted to prevent data leaks. [3713]\nüñ•Ô∏è Macs are set to verify SSL in Python. If certificates aren't installed, a SSL: CERTIFICATE_VERIFY_FAILED error propagates. We removed HTTPS in order to solve this issue. [3744]\nüîë Integers can now also be used as keys in widget declarations. This helps community members who use integers for keys. This change ensures integer keys are converted to strings. [3697]\n\nClick here to check out all updates.\n\nüèÅ Wrapping up\n\nThanks for checking out the release notes for 0.88.0. You can always see the most recent updates on our change-log or via this tag on the forum.\n\nFeel free to let us know in the comments below if you have any questions. We're looking forward to hearing what you think about the new download button!\n\nHappy Streamliting. üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Release Notes...\n\nView even more ‚Üí\n\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "New Funding Round Led by Sequoia: $35 Million Series B",
    "url": "https://blog.streamlit.io/our-35-million-series-b/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nOur $35 million Series B\n\nWe‚Äôre excited to announce a new funding round led by Sequoia üå≤\n\nBy Adrien Treuille\nPosted in Product, April 7 2021\nShare this post\n‚Üê All posts\n\nWe launched Streamlit in 2019 to make app creation as easy as Python scripting. Over the past two years we have obsessed over making this alchemy ‚Äî from scripts to apps ‚Äî as fast and joyful as possible. The world has changed so much since we launched, but the importance of sharing data insights has only increased. And while we gave you an open-source app-creating \"superpower,\" it was you all in the community who showed us the true power of data apps.\n\nYou've used Streamlit to turn ad-hoc analyses into workflows that supercharge your company. You've turned datasets into data experiences that you share with the world. You've made interactive data apps a cornerstone of your work, bridging understanding between disparate groups and helping everyone make better data-informed decisions. It's been amazing to see top companies and developers create hundreds of thousands of Streamlit apps used by millions of people worldwide.\n\nToday, we're excited to announce a $35 million Series B investment led by Sequoia and backed by our existing investors Gradient Ventures and GGV Capital. The confidence of our investors reflects the rapid growth and vibrance of the amazing Streamlit community, as well as the successful blossoming of our commercial app deployment platform, Streamlit for Teams. We wanted to take this moment to thank all of those who helped get us here, and give a preview of the future.\n\nTo the Streamlit community, our app developers, forum posters, open source contributors, Streamlit Creators, and everyone who just filed a bug or shared a cool app: thank you! You all are the essential core of Streamlit success and the wind behind our sails. To Streamlit's employees: it's harder to imagine a more fun group of people to see every day, even if only in little postage stamp windows across video calls. You are an incredible source of energy and ideas which make it exciting to work at Streamlit every day. And to all of our loved ones and the families of everyone involved with building Streamlit, thank you for putting up with us when we've worked late nights and weekends. It would be impossible to do this without your understanding of our weird company-building obsession.\n\nStay tuned! This past year has been huge, with our components framework, customizable layout, Streamlit sharing, custom theming, and a host of other new features. The coming year will be even bigger, with more app features, programmable state, speed improvements, multipage apps, and tons of enterprise security features. Please continue to reach out with more great apps, articles, ideas, components, bugs, feature requests, code snippets, and words of encouragement! We wouldn't be here today without each one of you.üéà\n\nüöÄAdrien Treuille, Amanda Kelly, and Thiago Teixeira - Streamlit Co-founders\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "contour-plot-with-plotly.png (2000√ó956)",
    "url": "https://blog.streamlit.io/content/images/2022/12/contour-plot-with-plotly.png#border",
    "html": ""
  },
  {
    "title": "Snehan Kekre - Streamlit",
    "url": "https://blog.streamlit.io/author/snehan/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Snehan Kekre\nSnehan's a Developer Advocate at Snowflake, where he specializes in the Streamlit open-source Python library and leads the Documentation team.\n1 post\nLaunching a brand-new docs site ü•≥\n\nImproved layout, easier navigation, and faster search\n\nProduct\nby\nSnehan Kekre\n,\nOctober 13 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tips to Improve App Usability | Designing Apps for the User",
    "url": "https://blog.streamlit.io/designing-streamlit-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to make a great Streamlit app\n\nDesigning an app your users will love\n\nBy Abhi Saini\nPosted in Tutorials, June 2 2021\nPart 1: Building an app for the user\nStart with the user\nShow users how it works\nHave examples of input data\nShow information only when it is needed\nUsing Tooltips\nUsing Expanders\nWrapping up\nResources\nContents\nShare this post\n‚Üê All posts\nPart 1: Building an app for the user\n\nWhen you're building an app it's easy to focus on just getting it to work for your data and models, but it's equally important to think about your viewer. We've all had times when an app we created was shared with teammates and we were told it was hard to use, or it was slow, or that it could look nicer.\n\nNever fear - Streamlit is here to help! Not only is it super easy to stand up web apps as a Data Scientist, but with a few easy tricks it‚Äôs also possible to have them look great and be performant! We're doing a 3 part series of blog posts on usability, aesthetics and performance.\n\nIn Part 1, we‚Äôll go over tips on how to design for the user. In Part 2, we'll cover how Streamlit features like Layout, Theming can help make your app look great and finally in Part 3, we'll focus on how to make the apps even more performant.\n\nKicking off Part 1: Designing for the user, the main things to keep in mind are:\n\nPut yourself in the user's shoes\nShow users how your app works and have instructions\nHave examples\nHide excess information\n\nLet's jump in!\n\nStart with the user\n\nA good place to begin before starting to write Streamlit code is to think how your users will actually use the app. Imagine what they might do when opening the app or better yet, talk to some of your users and discuss:\n\nWhat is the problem that the user is facing?\nWhat objectives are users trying to achieve?\nHow will users use this app?\n\nWriting down your users goals and matching that to specific widgets, text, and visuals to add (or even doing some quick wireframing!) can help structure your app into something both easy-to-use and effective for your user's needs.\n\nShow users how it works\n\nUsing an app can sometimes be confusing, especially for first time users. To help them navigate the app, add some explainer text in the app or a separate document that tells them how to use it. And because showing is always better than telling, it can be helpful to consider creating a video that navigates through the various inputs. You can do this with a feature Streamlit natively ships with called Record a screencast.\n\nAt Streamlit, we use this feature to record demos of our new features as seen in some of our previous blog posts (See Theming and Forms blog posts)\n\nIt is also super easy to embed videos in Streamlit apps. All it takes is two lines of code!\n\nimport streamlit as st\n\nst.video('recorded_screencast.mp4')\nHave examples of input data\n\nWith multiple input fields or file formats, first time users can be confused by what goes into each box and how the app will respond. Having default input data helps users get started on using the app right away.\n\nIn your streamlit app, default input data can be pre-filled into widgets by using the value argument, for eg.\n\ntxt = st.text_area('Text to analyze', value='It was the best of times')\n\n\nFor other apps, where the input type can be something more complex, app developers can provide a default input. For eg. the Goodreads Reading Habits app developed by Tyler Richards is a great example of how having default inputs allows the user to visualize the app's expected output.\n\nShow information only when it is needed\n\nTo reduce visual clutter, sometimes it's best to hide information and let the user access it when needed. This frees up valuable screen real estate for the developer to help focus the user's attention. In this section, we describe 2 ways of achieving this: 1) Tooltips and 2) Expanders.\n\nUsing Tooltips\n\nStarting with version 0.79.0, Streamlit introduced Tooltips which can be associated with input widgets like st.text_input, st.selectbox etc. Tooltips can help reduce visual clutter as well as act as a source of helpful information for app users.\n\nTooltips can be conveniently added to supported widgets using the help keyword.\n\nimport streamlit as st\n\nst.title('Tooltips in Streamlit')\nst.radio(\"Pick a number\", [1, 2, 3], help='Select a number out of 3 choices')\n\n# Tooltips also support markdown\nradio_markdown = '''\nSelect a number, you have **3** choices!\n'''.strip()\n\nst.header('Tooltips with Markdown')\nst.radio(\"Pick a number\", [1, 2, 3], help=radio_markdown)\n\n\nThis results in a (?) being added to the widget. Upon hovering on the tooltip, the help message appears as shown below:\n\nAll input widgets like st.number_input, st.slider, st.radio, st.text_area etc support tooltips via the help keyword.\n\nUsing Expanders\n\nExpanders can also be used to reduce visual clutter and hide text that may only be relevant to users looking for additional details.\n\nSee our previous blog post on Layouts for more details on how to use expanders.\n\nWrapping up\n\nWe hope that by using these features, your Streamlit apps will be cleaner and more usable for your users. To use the features discussed above in your apps, go ahead and upgrade to the latest version of Streamlit\n\npip install --upgrade streamlit\n\n\nIn Part 2, of this blog series, we'll cover more on how Streamlit features like Layout, Theming, and Anchors can help make your app look even better üéà\n\nIf you have any questions let us know on the forum!\n\nResources\nGithub\nForum\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "New experimental primitives for caching (that make your app 10x faster!)",
    "url": "https://blog.streamlit.io/new-experimental-primitives-for-caching/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nNew experimental primitives for caching (that make your app 10x faster!)\n\nHelp us test the latest evolution of st.cache\n\nBy Abhi Saini and Tim Conkling\nPosted in Product, September 22 2021\nProblem\nSolution\n@st.experimental_memo\n@st.experimental_singleton\nWhich to use: memo or singleton?\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHaving trouble with @st.cache? You're not alone.\n\nWe've found that @st.cache is hard to use. You're either faced with cryptic errors like InternalHashError or UnhashableTypeError. Or you need to understand concepts like hash_funcs and allow_output_mutation.\n\nOuch.\n\nBut don't fret. We've got two new solutions for you: st.experimental_memo and st.experimental_singleton. It's conceptually simpler! And much, much faster. In some of our internal tests on caching large dataframes, @st.experimental_memo has outperformed @st.cache by an order of magnitude. That's over 10X faster! üöÄ\n\nHere is the summary of the post (TL;DR! üòâ):\n\nProblem: @st.cache tries to solve two different problems:\n\ncaching data\nstoring global objects (TensorFlow sessions, database connections, etc)\nAs a result of @st.cache doing both those things, it's both slower and more complex (see: hash_funcs)\n\nSolution: We have two experimental APIs, each of which solves a single problem:\n\nAs a result, they are simpler and faster.\nIf you're interested in trying this out, we recommend that you replace all uses of @st.cache with @st.experimental_memo and @st.experimental_singleton, as appropriate (see how below!).\n\nWhich to use, memo or singleton?\n\n@st.experimental_memo is the primary replacement for @st.cache and is for storing data. If you're computing a value and you want to cache it, you almost always want memo. We expect it to be used more frequently. It's used for caching expensive computation that you don't want to run multiple times.\n@st.experimental_singleton is for storing non-data objects. If you have an object that is not the result of computation but is instead used to implement computation or other program logic, you probably want singleton.\nAnother way of thinking about this: @st.memo is for stuff you might put in a database. @st.singleton is for stuff that doesn't make sense to put in a database.\n\nWant to learn more details? Let's dive even deeper.\n\nProblem\n\nFirst, we wanted to understand how @st.cache was being used in the wild. A detailed analysis of open-source Streamlit apps indicated that @st.cache was serving the following use-cases:\n\nStoring computation results given different kinds of inputs. In Computer Science literature, this is called memoization.\nInitializing an object exactly once, and reusing that same instance on each rerun for the Streamlit server's lifetime. This is called the singleton pattern.\nStoring global state to be shared and modified across multiple Streamlit sessions (and, since Streamlit is threaded, you need to pay special attention to thread-safety).\n\nThis led us to wonder whether @st.cache's complexity could be a product of it trying to cover too many use-cases under a single unified API.\n\nTo test out this hypothesis, today we are introducing two specialized Streamlit commands covering the most common use-cases above (singletons and memoization). We have used those commands ourselves to replace @st.cache in several Streamlit apps, and we're finding them truly amazing.\n\nWe'd like to share them with all of you in our amazing community to try out these two commands and tell us what you think. ‚ù§Ô∏è\n\nSolution\n\nWhile @st.cache tries to solve two very different problems simultaneously (caching data and sharing global singleton objects), these new primitives simplify things by dividing the problem across two different APIs.\n\n@st.experimental_memo\n\nUse this to store expensive computation which can be \"cached\" or \"memoized\" in the traditional sense. It has almost the exact same API as the existing @st.cache, so you can often blindly replace one for the other:\n\n@st.experimental_memo\ndef factorial(n):\n\tif n < 1:\n\t\treturn 1\n\treturn n * factorial(n - 1)\n\nf10 = factorial(10)\nf9 = factorial(9)  # Returns instantly!\n\n\nProperties\n\nUnlike @st.cache, this returns cached items by value, not by reference. This means that you no longer have to worry about accidentally mutating the items stored in the cache. Behind the scenes, this is done by using Python's pickle() function to serialize/deserialize cached values.\nAlthough this uses a custom hashing solution for generating cache keys (like @st.cache), it does not use hash_funcs as an escape hatch for unhashable parameters. Instead, we allow you to ignore unhashable parameters (e.g. database connections) by prefixing them with an underscore.\n\nFor example:\n\n@st.experimental_memo\ndef get_page(_sessionmaker, page_size, page):\n\t\"\"\"Retrieve rows from the RNA database, and cache them.\n\t\n\tParameters\n\t----------\n\t_sessionmaker : a SQLAlchemy session factory. Because this arg name is\n\t                prefixed with \"_\", it won't be hashed.\n\tpage_size : the number of rows in a page of result\n\tpage : the page number to retrieve\n\t\n\tReturns\n\t-------\n\tpandas.DataFrame\n\tA DataFrame containing the retrieved rows. Mutating it won't affect\n\tthe cache.\n\t\"\"\"\n\twith _sessionmaker() as session:\n\t\tquery = (\n\t\t\tsession\n\t\t\t\t.query(RNA.id, RNA.seq_short, RNA.seq_long, RNA.len, RNA.upi)\n\t\t\t\t.order_by(RNA.id)\n\t\t\t\t.offset(page_size * page)\n\t\t\t\t.limit(page_size)\n\t\t)\n\t\t\n\t\treturn pd.read_sql(query.statement, query.session.bind)\n\n@st.experimental_singleton\n\nThis is a key-value store that's shared across all sessions of a Streamlit app. It's great for storing heavyweight singleton objects across sessions (like TensorFlow/Torch/Keras sessions and/or database connections).\n\nfrom sqlalchemy.orm import sessionmaker\n\n@st.singleton\ndef get_db_sessionmaker():\n\t# This is for illustration purposes only\n\tDB_URL = \"your-db-url\"\n\tengine = create_engine(DB_URL)\n\treturn sessionmaker(engine)\n\ndbsm = get_db_sessionmaker()\n\n\nHow this compares to @st.cache:\n\nLike @st.cache, this returns items by reference.\nYou can return any object type, including objects that are not serializable.\nUnlike @st.cache, this decorator does not have additional logic to check whether you are unexpectedly mutating the cached object. That logic was slow and produced confusing error messages. So, instead, we're hoping that by calling this decorator \"singleton,\" we're nudging you to the correct behavior.\nThis does not follow the computation graph.\nYou don't have to worry about hash_funcs! Just prefix your arguments with an underscore to ignore them.\n\nWARNING: Singleton objects can be used concurrently by every user connected to your app, and you are responsible for ensuring that @st.singleton objects are thread-safe. (Most objects you'd want to stick inside an @st.singleton annotation are probably already safe‚Äîbut you should verify this.)\n\nWhich to use: memo or singleton?\n\nDecide between @st.experimental_memo and @st.experimental_singleton based on your **function's return type. Functions that return data should use memo. Functions that return non-data objects should use singleton.\n\nFor example:\n\nDataframe computation (pandas, numpy, etc): this is *data‚Äî*use memo\nStoring downloaded data: memo\nCalculating pi to n digits: memo\nTensorflow session: this is a *non-data object‚Äî*use singleton\nDatabase connection: singleton\n\nNOTE: The commands we're introducing today are experimental, so they're governed by our experimental API process. This means:\n\nWe can change these APIs at any time. That's the whole point of the experiment! üòâ\nTo make this clear, the names of these new commands start with \"experimental_\".\nIf/when these commands graduate to our stable API, the \"experimental_\" prefix will be removed.\nWrapping up\n\nThese specialized memoization and singleton commands represent a big step in Streamlit's evolution, with the potential to entirely replace @st.cache at some point in 2022.\n\nYes, today you may use @st.cache for storing data you pulled in from a database connection (for a Tensorflow session, for caching the results of a long computation like changing the datetime values on a pandas dataframe, etc.). But these are very different things, so we made two new functions that will make it much faster! üí®\n\nAs usual, you can upgrade by using the following command:\n\npip install --upgrade streamlit\n\n\nPlease help us out by testing these commands in real apps and leaving comments in the Streamlit forums. And come by the forum or Twitter to share all the cool things you make! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "experimental_editor2.gif (411√ó342)",
    "url": "https://blog.streamlit.io/content/images/2023/08/experimental_editor2.gif#border",
    "html": ""
  },
  {
    "title": "Andreas Br√¶ndhaugen - Streamlit",
    "url": "https://blog.streamlit.io/author/andreas/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Andreas Br√¶ndhaugen\n1 post\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "0.89.0 release notes",
    "url": "https://blog.streamlit.io/0-89-0-release-notes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nBy Abhi Saini\nPosted in Release Notes, September 22 2021\n‚ú® New in Streamlit\nüçî Configurable hamburger menu options\n1. New API to customize the menu\n2. Context-aware visual redesign\n‚öôÔ∏è New experimental primitives for caching\nst.experimental_memo\nst.experimental_singleton\nüëü Other notable updates\nüèÅ Wrapping up\nContents\nShare this post\n‚Üê All posts\nüí°\nNote: Streamlit Cloud is now Streamlit Community Cloud.\n\nHey Streamlit Fam! üëã\n\nWe're excited to introduce the 0.89.0 release with two notable updates: new configurable hamburger menu options and new experimental primitives for caching.\n\nIn this post, we'll describe them in detail and talk about other updates. If you can't wait to try them out‚Äîhere's a sample app!\n\n‚ú® New in Streamlit\nüçî Configurable hamburger menu options\n\nWe worked on improving the in-app menu (or as we like to call it internally, the hamburger menu üçî):\n\nMake the menu more useful to viewers. The About menu shows information about Streamlit, but viewers want to see information about the app itself!\nReduce confusion. Menu items used to change depending on where the app was hosted. Not anymore.\nContinue to provide developer-oriented options, such as clear cache.\n\nTo address the above, we're introducing two updates:\n\nA new API to customize the menu.\nA context-aware visual redesign.\n\nLet's jump right in.\n\n1. New API to customize the menu\n\nst.set_page_config now accepts the menu_options argument to allow you to configure or remove Get help, Report a bug, and About menu items.\n\nHere it is in action:\n\nmenu_items = {\n\t'Get help': YOUR_HELP_URL_STRING,\n\t'Report a bug': YOUR_BUG_PAGE_URL_STRING,\n\t'About': '''\n\t ## My Custom App\n\n\t Some markdown to show in the About dialog.\n\t'''\n}\n\nst.set_page_config(menu_items=menu_items)\n\n\nAnd here is the About menu with Markdown (it's as simple as copying your README.md from the app repo into the tab):\n\nYou can disable menu items by setting None as the menu item's value. For more information, see the documentation for set_page_config .\n\n2. Context-aware visual redesign\n\nThe hamburger menu is now divided into the main section (visible to everyone) and the developer section (visible to developers only).\n\nThe developer section is context-aware. It appears if the app is running on localhost or if it was deployed in Streamlit Cloud (and if the current user is the app's developer).\n\nHere is what it looks like:\n\nThat's it for the hamburger menu! üçî\n\nKeep reading below for updates on caching and other notes from the 0.89.0 release.\n\n‚öôÔ∏è New experimental primitives for caching\n\nTwo years ago, we introduced st.cache as the foundational part of Streamlit's execution model. It lets you write apps linearly‚Äîlike a simple script without sacrificing performance.\n\nBut this powerful tool came with unexpected complexity (did anyone say hashfuncs ü§Ø?). st.cache tried to solve too many problems at the same time.\n\nToday we're introducing two experimental primitives that focus on specific use-cases of st.cache: st.experimental_memo and st.experimental_singleton.\n\nWe're releasing these features early because we want to get your feedback!\n\nPlease give them a try and take it to the forums with bugs, thoughts, and (hopefully) praise. And don't worry. None of this will impact your ability to use the current st.cache. üòÑ\n\nHere is how these primitives work:\n\nst.experimental_memo\n\nUse it to store expensive computation which can be \"cached\" or \"memoized\" in the traditional sense. It has the same API as the existing st.cache:\n\n@st.experimental_memo\ndef load_csv(filename):\n    df = pd.read_csv(filename)\n    return df\n\ndf = load_csv('my-data.csv')\n\n\nIt's that simple! See this blog post for more information.\n\nst.experimental_singleton\n\nThis is a key-value store that's shared across all Streamlit app sessions. Use it for storing objects that are initialized once but are used multiple times throughout your app (like Tensorflow sessions and database connections):\n\nfrom sqlalchemy.orm import sessionmaker\n\n@st.singleton\ndef get_db_sessionmaker():\n    # This is for illustration purposes only\n    DB_URL = \"your-db-url\"\n    engine = create_engine(DB_URL)\n    return sessionmaker(engine)\n\n\nThat's it for st.experimental_memo and st.experimental_singleton! Read this blog post for more details.\n\nWe're excited to see what you do with these new primitives. To use them, upgrade Streamlit as usual:\n\npip install --upgrade streamlit\n\nüëü Other notable updates\nüíÖ We've updated our UI to a more polished look to improve typography, vertical rhythm, and so much more (including our favorite, the UI for dataframes!)\nüé® We now support theme.base in the theme object when it's sent to custom components. [3737].\nüß† We've modified session state to reset widgets if any of their arguments changed (even if they provide a key). Some widget behavior has changed as result, but these are a minority of cases. Besides, we find this is the best API both in theory and in practice. See docs for more.\nüêû Bug fixes: st.image now supports SVG URLs [#3809] and SVG strings that don't start with an <svg> tag [#3789].\nüèÅ Wrapping up\n\nThank you for reading the 0.89.0 release notes. You can always see the most recent updates on our changelog or via this tag on the forum.\n\nLet us know in the comments if you have any questions. We're looking forward to hearing what you think!\n\nHappy Streamlit-ing. üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Release Notes...\n\nView even more ‚Üí\n\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Common app problems: Resource limits",
    "url": "https://blog.streamlit.io/common-app-problems-resource-limits/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCommon app problems: Resource limits\n\n5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud\n\nBy Johannes Rieke\nPosted in Tutorials, September 9 2021\nWhat are the resource limits?\nTip 1: Reboot your app (temporary fix)\nTip 2: Use st.cache to load models or data only once\nTip 3: Restrict the cache size with ttl or max_entries\nTip 4: Move big datasets to a database\nTip 5: Profile your app's memory usage\nNothing helps?\nContents\nShare this post\n‚Üê All posts\n\nDo you see this page instead of your beautiful Streamlit app? üëá\n\nSorry! This message means you've hit the 1GB resource limit of apps hosted with Streamlit Community Cloud. Luckily, there are a few things you can change to make your app less resource-hungry, as well as a number of other platforms that you can use to host your Streamlit app.\n\nIn this post, we'll go through some tips on how to fix the most common issues.\n\nWhat are the resource limits?\n\nAs of August 2021, apps on the free Community tier of the Streamlit Cloud are limited by:\n\nMemory (RAM)\nCPU\nDisk storage\n\nBased on our research, most of the time apps run over the resource limits because of memory. In particular, you might have a memory leak.\n\nLet's look at some tips for how to handle those leaks.\n\nTip 1: Reboot your app (temporary fix)\n\nIf you need to restore access to your app immediately, reboot your app. This resets all memory, CPU, and disk usage. While logged into Streamlit Cloud, visit your app and click on Manage app in the bottom right corner. Now click on the three dots in the sidebar and then on Reboot app.\n\nNOTE: Rebooting can only fix your app temporarily!\n\nIf there's a problem or memory leak in your app, it will soon run over the resource limits again. So make sure you read the tips below to fix any deeper issues!\n\nTip 2: Use st.cache to load models or data only once\n\nThis is by far the most common issue we see. Apps load a memory-intensive machine learning model or dataset directly in the main Python script, e.g. like this:\n\nimport streamlit as st\nimport torch\n\n# Load the model.\nmodel = torch.load(\"path/to/model.pt\")\n\n# Perform a prediction.\nquestion = st.text_input(\"What's your question?\")\nanswer = model.predict(question)\nst.write(\"Predicted answer:\", answer)\n\n\nRecall that in Streamlit's execution model, the script is rerun each time a viewer interacts with an app. That means the model above is loading from scratch every time!\n\nIn most cases, this isn't a huge problem (old objects are regularly cleared from memory, and we recently introduced a fix to do this even better!) ‚Äì it just makes your app a bit slower. However, we find that some libraries that manage their memory outside of Python (e.g. Tensorflow) do not release memory in a timely manner, especially in a threaded environment like Streamlit. Old objects add up in memory, and you hit the resource limits. üòï\n\nBut there's a trivial fix. You can use st.cache to ensure memory-intense computations run only once. Here's how you would fix the code above:\n\nimport streamlit as st\nimport torch\n\n# Load the model (only executed once!)\n# NOTE: Don't set ttl or max_entries in this case\n@st.cache\ndef load_model():\n\t  return torch.load(\"path/to/model.pt\")\n\nmodel = load_model()\n\n# Perform a prediction.\nquestion = st.text_input(\"What's your question?\")\nanswer = model.predict(question)\nst.write(\"Predicted answer:\", answer)\n\n\nNow, the model is only loaded the first time your app runs. This saves memory and also makes your app a bit faster. You can read more about caching in the docs.\n\nThere is one caveat. For proper caching, Streamlit needs to hash the input and output values of the cached function. But ML models, database connections, and similar objects are often not easily hashable! This can result in an UnhashableTypeError. For our use case (i.e. loading a complex object at startup), you can disable hashing for this object byusing the hash_funcs argument (more info here):\n\n@st.cache(hash_funcs={\"MyUnhashableClass\": lambda _: None}\n\n\nIf this sounds daunting, fear not! We're working on improvements to caching that will make the above steps obsolete through a new caching API. In the meantime, if you need help setting this up, feel free to ask us in our forums.\n\nTip 3: Restrict the cache size with ttl or max_entries\n\nAre you a Streamlit expert who already uses st.cache to run ML models or process API requests? That's fantastic! But did you remember to configure the cache's expiration policy? When not configured, the cache can fill up over time, using more and more memory. And you're back in \"Over capacity\" land. üòï\n\nst.cache offers two parameters to prevent this:\n\nttl controls the cache's Time To Live, i.e. how long an object stays in the cache before it gets removed (in seconds).\nmax_entries controls the maximum number of objects in the cache. If more elements get added, the oldest ones are automatically removed.\n\nYou can only set one of these at a time.\n\nIf your Streamlit app uses caching, it is best practice to set up one of these options. The main exception to this is the case shown in Tip 2, where you're using the cache to load a given object as a singleton (i.e. load it exactly once).\n\nHere's an example of how you can use ttl:\n\nimport streamlit\n\n# With `ttl`, objects in cache are removed after 24 hours.\n@st.cache(ttl=24*3600)\ndef api_request(query):\n    return api.run(query)\n\nquery = st.text_input(\"Your query for the API\")\nresult = api_request(query)\nst.write(\"The API returned:\", result)\n\nTip 4: Move big datasets to a database\n\nIs your app using or downloading big datasets? This can quickly fill up memory or disk space and make your app slow for viewers. It's usually a good idea to move your data to a dedicated service, e.g.:\n\nA database like Firestore or BigQuery\nA file hosting service like AWS S3\nA Google Sheet (an easy option for prototypes with limited data!)\n\nOur docs offer a range of guides on connecting to different data services, and we keep adding more! Spoiler alert: We're also thinking about having a built-in st.database in the future. üòâ\n\nWant to keep your data local but still save memory? There's good news. Streamlit recently introduced support for Apache Arrow. This means you can store your data on disk with Arrow and read it in the app directly from there. This consumes a lot less memory than reading from CSV or similar file types.\n\nTip 5: Profile your app's memory usage\n\nStill struggling with memory usage? Then it may be time to start a deeper investigation and track your app's memory usage.\n\nA helpful tool is the psrecord package. It lets you plot the memory & CPU usage of a process. Here's how to use it:\n\nOpen the terminal on your local machine.\n\n2. Install psrecord.\n\npip install psrecord\n\n\n3. Start your Streamlit app.\n\nstreamlit run name_of_your_app.py\n\n\n4. Find out the process ID (or PID) of the app, e.g. using the task manager in Windows or the activity monitor in OSX (the process running the app is usually called \"Python\").\n\nOn Mac or Linux, you can also run the command below. The PID is the first number you get:\n\nps -A | grep \"streamlit run\" | grep -v \"grep\"\n\n\n5. Start psrecord, inserting the correct process ID from the step above for <PID>:\n\n psrecord <PID> --plot plot.png\n\n\n6. Interact with your Streamlit app. Trigger the most memory-consuming parts of your app (e.g. loading an ML model) and remember the sequence of the steps you took.\n\n7. Kill the psrecord process (e.g. with ctrl + c).\n\nThis will write a plot like below to the file plot.png. You'll see how each action from step 6 affected your memory usage. It's helpful to do this a few times and test out different parts of your app to see where all that memory is going!\n\nHere is what the memory profile of the app above looked like after applying Tip #2:\n\nYou can also use a more sophisticated memory profiler to show you exactly which line of code consumed the most memory. A great one for this purpose is Fil.\n\nNothing helps?\n\nAs with all debugging, sometimes you get stumped.\n\nMaybe you have a use case that requires a lot of resources (e.g. you're loading an immense ML model). Or your app behaved well for a few concurrent users, but now it went viral. Or you think there's a bug somewhere, but can't figure out where. Or maybe you just need a rubber duck.\n\nPlease reach out! Post on the forum with a link to your app and what you've tried so far, and we'll take a look. üéà\n\nOne more thing. If you have a special good-for-the-world case that needs more resources, send us an email and we'll see about making an exception!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1.1.0 release notes",
    "url": "https://blog.streamlit.io/1-1-0-release-notes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nBy Johannes Rieke\nPosted in Release Notes, October 21 2021\nüß† Memory improvements\nüß¨ Semantic versioning\nüéí Other notable updates\nüèÅ Wrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit community! üëã\n\nSay hello to some sweet improvements around memory usage and the introduction of semantic versioning!\n\nüß† Memory improvements\n\nEver had a Streamlit app use too much memory? Rejoice! We made some important changes to shrink memory usage. This affects all Streamlit apps, especially the ones that have run for long periods of time and have many viewers. This change also prevents many resource-limit errors on Streamlit Cloud (the \"Argh\" error page you might've seen here and there) and builds on top of other memory improvements from version 0.82.0.\n\nFor example, here is one of our internal Streamlit app's memory usage. The colored lines on the left are using older Streamlit versions. The blue line on the right is using the 1.1.0 release.\n\nUpdate the Streamlit version of all your apps on Streamlit Cloud to 1.1.0 to enjoy these improvements!\n\nüß¨ Semantic versioning\n\nWith the recent release of Streamlit 1.0, we‚Äôre also committing to following a loose variant of semantic versioning. This fulfills our promise to keep the API stable so you can confidently build production-quality apps.\n\nAll changes introduced in minor versions will be additive. Breaking changes will only be introduced in major versions, while patch releases will be for bug fixes.\nWhenever possible, a deprecation path will be provided rather than an outright breaking change. We‚Äôll introduce deprecations in minor versions.\n\nThere are a few caveats, though:\n\nFeatures released with the experimental_ prefix are excluded from semantic versioning because they‚Äôre prototype features that need community input and iteration.\nst.session_state has a few minor issues left. While we're working on resolving them, this feature might see some updates outside semantic versioning in the next quarter.\nUI changes are not considered breaking changes as long as apps still work.\nChanges to CSS class names are not considered breaking changes.\nüéí Other notable updates\n‚ôªÔ∏è Apps now automatically rerun when the content of secrets.toml changes. Before this, you had to refresh the page manually.\nüîó We redirected documentation links (e.g., in exceptions) to our brand-new docs site.\nüêõ Bug fix: Range slider can now be initialized with the session state API (#3586)\nüêû Bug fix: Charts now automatically refresh when using add_rows with datetime index (#3653)\nüèÅ Wrapping up\n\nThanks for checking out the release notes for 1.1.0. You can always see the most recent updates on our changelog or via this tag on the forum.\n\nLet us know in the comments below if you have any questions. We're looking forward to hearing what you think about this release!\n\nHappy Streamlit-ing. üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Release Notes...\n\nView even more ‚Üí\n\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Built-in charts get a new look and parameters! üìä",
    "url": "https://blog.streamlit.io/built-in-charts-get-a-new-look-and-parameters/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nBy Johannes Rieke and Arnaud Miribel\nPosted in Release Notes, August 11 2022\nNew look\nNew parameters: x and y\nBonus feature: charts in cached functions\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nDo you think it‚Äôs painful to create charts in Streamlit? We think so too. Sometimes it can take longer than coding the rest of the app! On top of that, if you want your charts to look beautiful, you need to use (and customize) Matplotlib, Plotly, or Altair. It‚Äôs annoying and time-consuming.\n\nStreamlit has three simple built-in charting commands: st.line_chart, st.area_chart, and st.bar_chart. But they need data in a specific format (impractical for most real-world datasets) and they look pretty average. üòí\n\nSo today, we‚Äôre excited to release‚Ä¶\n\nA major overhaul of our built-in charts!\n\nThe charts get an entirely new look and new parameters x and y to make them more versatile.\n\nCan‚Äôt wait to try it? Check out the demo app and the code.\n\nNew look\n\nOur design team created a beautiful chart theme that works seamlessly with the rest of Streamlit. It‚Äôs sleek and modern, and it uses our official color palette.\n\nFor st.line_chart:\n\nFor st.area_chart:\n\nFor st.bar_chart:\n\nPsst... ü§´ In the future, we might bring this theme to our built-in charting commands and third-party libraries like Plotly or Altair!\n\nNew parameters: x and y\n\nWe wanted you to use our built-in charting with any dataset. Today, all three commands get parameters x and y, so you can control what to plot.\n\nFor example, if you have this dataframe:\n\n...and you want to plot temp_max over date, simply type:\n\nst.line_chart(df, x=\"date\", y=\"temp_max\")\n\n\nWith one line of code, you get a beautiful chart:\n\nThe same works for st.bar_chart and st.area_chart.\n\nBut wait, there‚Äôs more! We built in a little magic to create charts with multiple lines. Just pass the column names as a list to the y parameter:\n\nst.line_chart(\n    df,\n    x=\"date\",\n    y=[\"temp_min\", \"temp_max\"],  # <-- You can pass multiple columns!\n)\n\n\nBonus feature: charts in cached functions\n\nWith 1.12.0, we‚Äôre releasing one more feature. You can now put charts and other static elements (dataframes, text, etc.) into functions that are cached via st.experimental_memo or st.experimental_singleton. This lets you cache not only the long computations or the API calls but the entire parts of your user interface! Give it a try and let us know what you think (we'll share more info in the coming months).\n\nWrapping up\n\nAnd that's a wrap for built-in chart improvements. The charts are now much more powerful. All it takes is just a few lines of code!\n\nIf you want more complex charts or more customization, use any third-party charting library that we support. Got questions? Let us know in the comments below.\n\nOh, and don't forget to check out other cool features in our 1.12.0 release.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Release Notes...\n\nView even more ‚Üí\n\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "select_city2.gif (549√ó297)",
    "url": "https://blog.streamlit.io/content/images/2023/08/select_city2.gif#border",
    "html": ""
  },
  {
    "title": "Karen Javadyan - Streamlit",
    "url": "https://blog.streamlit.io/author/karen/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Karen Javadyan\n1 post\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nProduct\nby\nTim Conkling and¬†\n2\n¬†more,\nFebruary 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tim Conkling - Streamlit",
    "url": "https://blog.streamlit.io/author/tim/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Tim Conkling\n3 posts\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nProduct\nby\nTim Conkling and¬†\n2\n¬†more,\nFebruary 14 2023\nNew experimental primitives for caching (that make your app 10x faster!)\n\nHelp us test the latest evolution of st.cache\n\nProduct\nby\nAbhi Saini and¬†\n1\n¬†more,\nSeptember 22 2021\nStreamlit Components, security, and a five-month quest to ship a single line of code\n\nThe story of allow-same-origin\n\nTutorials\nby\nTim Conkling\n,\nJanuary 20 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Announcing Streamlit's $21M Series¬†A",
    "url": "https://blog.streamlit.io/announcing-streamlits-21m-series-a/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnnouncing Streamlit's $21M Series¬†A\n\nDeveloping new superpowers for the data science community\n\nBy Adrien Treuille\nPosted in Product, June 16 2020\nDeveloping new superpowers for the data science community.\nContents\nShare this post\n‚Üê All posts\nDeveloping new superpowers for the data science community.\n\nIn 2018, we began building the tools to visualize Python scripts that became Streamlit. By then, Python was the preeminent language of machine learning, bristling with powerful libraries like Keras and OpenCV. Python made it easy to analyze rich datasets and train detailed models. However, our decade of machine learning work at Carnegie Mellon, Google, and Zoox had shown us the difficulty of extending these newfound powers throughout an organization. We wanted to share models and insights with coworkers. We wanted to build custom tools that made machine learning repeatable, shareable, modifiable, and usable throughout an organization. We wanted to show off our work in beautiful apps that let others use what we had created.\n\nLast Fall, after a year of development, we released Streamlit, an open-source framework to turn Python scripts into interactive apps. The response exceeded our greatest expectations. Streamlit has been downloaded over 400,000 times and is now democratizing data-driven decision making across virtually every industry, from tech giants like Google and Uber to Delta Dental, 7‚Äì11, and even the NBA! Over 200,000 Streamlit apps have been created, and every day we see the Streamlit community share apps to predict COVID rates, visualize new chemical compounds, analyze music playlists, recommend movies, and much, much more. We are awed, inspired, and delighted daily by the community‚Äôs creativity.\n\nToday we are excited to announce a $21 million Series A investment into Streamlit, co-led by GGV Capital and Gradient Ventures, and with participation from Bloomberg Beta, Elad Gil, Daniel Gross, and few other amazing investors.\n\nThis investment will enable us to develop new superpowers for the Streamlit community. Our goal is to make Streamlit not only the most productive (and fun!) app-building experience in Python, but also the most powerful. We‚Äôll be introducing new ways to extend apps, including releases for custom layout and programmable state. The investment will also accelerate the development of Streamlit for Teams, our forthcoming paid offering for deploying, securing, and sharing Streamlit apps, that includes a free tier for public Github repos. It‚Äôs currently in closed beta, and we‚Äôll be expanding the beta soon! Thank you for your patience, and if you haven‚Äôt signed up yet, click here.\n\nThank you for trying, using, and loving Streamlit. The apps you create, articles you share, and messages you send make us eager to come to work every day and build Streamlit into an even more amazing tool for everyone. We‚Äôre excited to start this new chapter with you and to see what we can build together. Let‚Äôs start creating! üéà\n\nAdrien Treuille, Amanda Kelly, and Thiago Teixeira - Streamlit Co-founders\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Easily Deploy and Share Your Streamlit Apps | Streamlit Sharing",
    "url": "https://blog.streamlit.io/introducing-streamlit-sharing/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing Streamlit Sharing\n\nThe new Streamlit platform for deploying, managing, and sharing your apps\n\nBy Adrien Treuille\nPosted in Product, October 15 2020\nEasily deploy and share your Streamlit apps\nGitHub and Streamlit - Better Together\nEndless Possibilities\nGet Your Invitation to Streamlit Sharing\nThe Streamlit Play Button\nContents\nShare this post\n‚Üê All posts\n\nMachine learning and data science code is easy to share but hard to use. GitHub overflows with models, algorithms, and datasets. But code is static. Can you play with the models? See the algorithms? Interact with the data? Doing so requires following complex instructions, installing packages, or reading dense code snippets. Frustrated by this, we decided that we need a simple, sharable \"play\" button for machine learning code.\n\nThere are two challenges here. The first is creating apps that make data science and machine learning code interactive. The second is sharing these apps so that the world can experience your work.\n\nA year ago, we addressed the first challenge ‚Äî creating ‚Äî by releasing Streamlit, an open-source library that lets you transform Python scripts into interactive apps. Streamlit lets you easily demonstrate algorithms, play with models, manipulate data, and combine all of these superpowers into beautiful apps. The response has been tremendous. We just crossed our millionth download. Hundreds of thousands of Streamlit apps have been created all over the world. But creating great apps only solves half the problem.\n\nEasily deploy and share your Streamlit apps\n\nToday, we address the second challenge ‚Äî sharing ‚Äî by announcing a brand-new sharing platform for Streamlit. Streamlit sharing lets you deploy, manage, and share your apps ‚Äì all for free! If you have a Streamlit app hosted publicly on GitHub, you are now one click away from sharing it with the world.\n\nGitHub and Streamlit - Better Together\n\nStreamlit sharing combines the best of Streamlit with the best of GitHub. From Streamlit you get a simple framework for creating incredibly rich and useful apps. From GitHub you inherit an incredible framework for social collaboration. Paste your GitHub link into Streamlit's sharing platform and almost instantly you have a live app. Or, click on the menu for any live app and see its source code on GitHub. Collaborate for free simply by forking and editing the code. It‚Äôs global, shareable, fork-able, collaborative data science!\n\nEndless Possibilities\n\nTaken together, Streamlit and GitHub enable an incredibly rich and diverse ecosystem of useful apps ‚Äì from dashboards to deep nets and beyond! (As former Carnegie Mellon folks, we're especially proud that students taking the Interactive Data Science class now submit their homework using Streamlit sharing ü§ó) Here are some awesome examples of shared Streamlit apps that you can play with right now.\n\nWhile this post has focused on open source applications, Streamlit is also used by thousands of companies to build sophisticated internal data tools. For example, Uber has deployed Streamlit company-wide, enabling data scientists to share their work throughout the company. Streamlit for Teams extends Streamlit‚Äôs sharing platform to bring secure, seamless app deployment, management, and collaboration within your enterprise. If you're interested please sign up for the beta for Streamlit for Teams.\n\nGet Your Invitation to Streamlit Sharing\n\nTo celebrate the launch, we'll be releasing 1,000 invitations for Streamlit sharing - with more invites coming as our server capacity grows. If you don‚Äôt have one in your inbox already, please request an invite and we'll get you one soon.\n\nThe Streamlit Play Button\n\nThis new sharing superpower completes the Streamlit circle ‚Äì from creation to sharing, and back again. So go forth and share; let others see your work, fork, merge, and contribute the cycle of knowledge. In that spirit, we offer one last gift: This is our ‚Äúplay‚Äù button.\n\nThis brand-new badge helps others find and play with your Streamlit app. Embed it right into your GitHub readme.md as follows:\n\n[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/yourGitHubName/yourRepo/yourApp/)\n\n\nThank you all for inspiring us with your amazing creations. We're excited to see what you build and share. üéà\n\nA huge thank you from all of us at Streamlit to all of you in the community ‚Äì and especially the inaugural Streamlit Creators, Ashish, Charly, Fanilo, Jos√©, Jesse, and Synode ‚Äì for your kindness, your feature requests, your bug reports, and your enthusiasm. Special thanks also to all the launch app creators, Alex, Dan, Ines | Explosion, and finally Tyler who created not only the Goodreads app but also a great sharing tutorial.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "1-4.png (1830√ó1156)",
    "url": "https://blog.streamlit.io/content/images/2021/08/1-4.png#browser",
    "html": ""
  },
  {
    "title": "Release Notes - Streamlit",
    "url": "https://blog.streamlit.io/tag/release-notes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Release Notes\n5 posts\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nWhat‚Äôs new in Streamlit (January 13th, 2022)\n\nCheck out what‚Äôs new in Streamlit Cloud and the 1.4.0 release\n\nRelease Notes\nby\nKsenia Anske\n,\nJanuary 13 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\n0.88.0 release notes\n\nThis release launches st.download_button as well as other improvements and bug fixes\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 3 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How Delta Dental uses Streamlit to make lightning-fast decisions",
    "url": "https://blog.streamlit.io/how-delta-dental-uses-streamlit-to-make-lightning-fast-decisions/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow Delta Dental uses Streamlit to make lightning-fast decisions\n\nFrom an idea to a prototype to production in just two weeks\n\nBy Amanda Kelly\nPosted in Case study, February 1 2022\nLightning-fast app deployment\nFast iteration cycles made the team more agile\nBuilding more complex apps for more users\nContents\nShare this post\n‚Üê All posts\n\nAs a dental benefits company, Delta Dental of NJ receives a lot of customer calls. The data science team, led by Lead Scientist Kevin Northover, works with that data to figure out how to improve their operations. This includes creating predictive models for call sentiment analysis, identifying outlier calls, and calculating detailed statistics on individual agents. But the generated insights weren‚Äôt making it into the hands of the call center managers who could act on that data.\n\nThe challenge was to find an application that allowed them to display all data in a clean, beautiful, and easy-to-understand way. The team analyzed data in notebooks, spreadsheets, and BI tools like Looker and Tableau, but they needed more than just static dashboards. To understand and use the operational insights, they needed to create a powerful, interactive data app that the operations team could use every day.\n\n\"I need the speed to insights to drive decisions in the company,\" says Justin Lahullier, CIO at Delta Dental of NJ. \"That's data science. Traditionally, there are long lead times for development to move data from source to target. Then more time for an analyst to work from target to a report or a dashboard. If a business has a question, they want an answer. How fast can we answer it and communicate it to a business user so that they can understand it?\"\n\nLightning-fast app deployment\n\nAround the time Justin asked the team to create a new application, Kevin became aware of Streamlit. Streamlit takes ordinary Python scripts (with a few magical Streamlit calls sprinkled in) and almost instantly turns them into beautiful, performant, sharable apps. Kevin knew Python, so he decided to give Streamlit a try. He learned Streamlit in minutes, made a test app in a few hours, and deployed Delta Dental's first call-center prototype app in just two weeks:\n\nThe app had a simple analysis dashboard that linked to other operational tools, one of which tagged thousands of phone calls, then scanned them for outliers:\n\nAll call analysis was displayed to agents next to the call transcript. The app refreshed regularly, so the call center agents always got the latest data. They could download the call lists, review the call transcripts, fill out the coaching questionnaires, playback the recordings, and leave comments:\n\n\"The agents don't need to be technically advanced with this app, as they can easily navigate through data visualizations,\" says Kevin. \"It takes a lot of work to get a good widget interface that feels natural. You spend so much effort on the UI in the initial build. Streamlit's UI actually made a difference. A big part of it is simplicity.\"\n\nFast iteration cycles made the team more agile\n\nThe quick prototype-to-production path meant that the team could now get feedback, share ideas, prototype, iterate, and rapidly ship changes to their business users. They could deliver more while keeping the team lean.\n\n\"With Streamlit I can just deploy the app and people can go and interact with it and put in comments. Then a week later we go back and iterate on that, figure out what comments to integrate and update,\" says Kevin. \"You don't need a data science team of twenty people to do this,\" adds Justin. \"Streamlit works for smaller organizations to allow you to move faster.\"\n\nBy adopting Streamlit, their data science team delivered relevant data to their operations team with only four people.\n\nBuilding more complex apps for more users\n\nTwo years into using Streamlit, Delta Dental is now adding more functionality, expanding their apps to more internal users, and making Streamlit their go-to production tool. They want to make their operations even more efficient by exploring forms for the call evaluation within the app‚Äîto dissect the structure of a single call, break it down to individual speakers, and track the problem to a specific speaker. This way the agents could listen to one voice rather than the whole call.\n\nUsing Streamlit has changed how Justin interacts with his data team. \"What I appreciate is the ability for Kevin to send me the URL and say, 'Go interact with the data.' It'll have tables and visualizations. I can interact with it, look at it, ask him questions. It allows us to iterate on where we're trying to get‚Äîto answer business questions or fix problem areas. I like the ease of being able to go and look at it rather than him sending me a spreadsheet. The app is being refreshed at a regular interval, so I'm seeing timely data and I can play with it, to think through more questions.\"\n\n\"l do all my data work inside Streamlit now,\" adds Kevin. \"If you're doing something that is standard business reporting, then the standard tools are fine. But if you're doing something where your people are trying to figure out what questions they want to ask, then use Streamlit.\"\n\nWant to get started with Streamlit in your organization? Head over to streamlit.io to learn more.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Case study...\n\nView even more ‚Üí\n\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "4-new-1.gif (1086√ó641)",
    "url": "https://blog.streamlit.io/content/images/2022/05/4-new-1.gif#browser",
    "html": ""
  },
  {
    "title": "Case study - Streamlit",
    "url": "https://blog.streamlit.io/tag/case-study/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Case study\n9 posts\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\nMake a video content analyzer app with Streamlit and AssemblyAI\n\nHow to build an AI-powered app that analyzes video channels automatically\n\nAdvocate Posts\nby\nMisra Turp\n,\nNovember 3 2022\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nQiusheng Wu uses Streamlit to build a popular geospatial application\n\nLearn how Qiusheng created Earth Engine web apps with geemap\n\nCase study\nby\nQiusheng Wu and¬†\n1\n¬†more,\nJuly 21 2022\nJULO improves financial inclusion in Indonesia with Streamlit\n\nLearn how JULO went from manual underwriting to automated credit scoring and a 22-member data team\n\nCase study\nby\nMartijn Wieriks and¬†\n1\n¬†more,\nJune 30 2022\nHow one finance intern launched his data science career from a coding bootcamp in Brazil\n\nLearn how Marcelo Jannuzzi of iFood got his dream job in data science\n\nCase study\nby\nMarcelo Jannuzzi and¬†\n1\n¬†more,\nJune 9 2022\nWissam Siblini uses Streamlit for pathology detection in chest radiographs\n\nLearn how Wissam detected thoracic pathologies in medical images\n\nCase study\nby\nWissam Siblini and¬†\n1\n¬†more,\nMay 3 2022\nThe Stable solves its data scalability problem with Streamlit\n\nHow Mark von Oven of The Stable helped his Analytics team go from idea to app in just a few days\n\nCase study\nby\nMark von Oven and¬†\n1\n¬†more,\nApril 28 2022\nHow Delta Dental uses Streamlit to make lightning-fast decisions\n\nFrom an idea to a prototype to production in just two weeks\n\nCase study\nby\nAmanda Kelly\n,\nFebruary 1 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "3-new-new.png (1734√ó958)",
    "url": "https://blog.streamlit.io/content/images/2022/05/3-new-new.png#browser",
    "html": ""
  },
  {
    "title": "divider.png (635√ó612)",
    "url": "https://blog.streamlit.io/content/images/2023/08/divider.png#border",
    "html": ""
  },
  {
    "title": "Chat with the Cat Generative Dialogue Processor (CatGDP)",
    "url": "https://blog.streamlit.io/chat-with-the-cat-generative-dialogue-processor-catgdp/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nChat with the Cat Generative Dialogue Processor (CatGDP)\n\nBuild your own catbot with a quirky persona!\n\nBy Tianyi Pan\nPosted in Advocate Posts, May 3 2023\nHow Streamlit works\nHow to build a basic UI for chatting\nHow to incorporate OpenAI and Stable Diffusion APIs\nHow to give your catbot that quirky, \"can has cheezburger\" persona style\nHow to tweak the UI so it looks like an actual chat program\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey community! üëã\n\nMy name is Tianyi, and I do Computer Vision and Machine Learning (CVML) Engineering at Clobotics. I was a film and animation producer some six years ago before transforming into data science and ML. I took MOOCs and other online courses, did many hobby projects, and read tutorials like this one. If you find yourself in the same predicament, you‚Äôve come to the right place! I try to make this accessible for most people, especially beginners.\n\nThe story behind CatGDP dates back to when I got through the DALL-E waitlist and could play with the latest model. Show of hands: Who remembers horseback astronauts? That fad died quickly. But at the time, I didn‚Äôt realize that access to DALL-E meant I could use OpenAI‚Äôs many APIs for development.\n\nIt wasn‚Äôt until early 2023, with all the ChatGPT craze, that I returned to OpenAI‚Äôs website and found out I possessed this rare resource. Of course, I had to immediately put it to good use by launching yet another hobby project.\n\nI actually built a series of progressively more advanced apps, all with chat interfaces, before coming up with CatGDP. Python and Streamlit are making chatbots easy to copy and paste with small variations once the basic architecture is done. Some of the apps I made were about chatting, some for generating art, but I always tried to add some fun twists to make them unique. Streamlit‚Äôs API is dead simple but can yield such beautiful UIs. So with CatGDP, all those previous learnings came together as a fwuffy feline meowpplication because, hey, who doesn‚Äôt love cats on the interwebs? üòª\n\nWhy call it GDP, though?\n\nMany people have asked this question. A quick and boring answer is that CatGPT was taken. A more elaborate explanation is that lolcats is a cultural phenomenon. One of its manifestations is the intentional misspelling of words and memes like ‚ÄúI Can Has Cheezburger,‚Äù so GPT ‚Üí GDP. Moreover, playing with cats will surely destroy your personal GDP and that of your country on a macro scale. Finally, we could just ask the catbot:\n\nOkay, enough kit-katting. Here is what you‚Äôll learn in this post:\n\nHow Streamlit works\nHow to build a basic UI for chatting\nHow to incorporate OpenAI and Stable Diffusion APIs\nHow to give your catbot that quirky, ‚Äúcan has cheezburger‚Äù persona style\nHow to tweak the UI so it looks more like a chat program\nüê±\nIf you‚Äôre impatient, jump right into the GitHub repo of the project and look at the code yourself. The below tutorial will follow it anyway, albeit edited for simplicity, so you might as well get a head start.\nHow Streamlit works\n\nStreamlit's UI is super easy to define with code. And the basic chat UI is simple enough, so I didn't make any wireframes or hand-drawn sketches. I planned it all in my head.\n\nBefore going into the step-by-step instructions, let's talk briefly about how Streamlit works and how you need to consider that when building a chat app.\n\nStreamlit apps update every time something happens in the UI‚Äîlike a button is pressed or a selector is switched. That means the Python code that makes the app will run again from top to bottom. No variable carries over from the previous update, even if you defined it at the start of the code. It also means you can't store something as simple but crucial as the chat history in any normal global variable.\n\nLuckily, Streamlit carries a separate st.session_state object, which is stateful across multiple reruns and is mutable at any time. It acts like a Python dictionary. Assuming your chat history is just a simple list object, you can add it to the st.session_state object using something like st.session_state[\"MEMORY\"] = [].\n\nYou can interact with this object just as you would with a normal list variable. For example, use st.session_state[\"MEMORY\"].append() to add elements (individual chat messages) to it. Or you can access the object through the alternative naming convention st.session_state.MEMORY , and it'll behave the same way.\n\nBut setting the memory to an empty list each run is still counterproductive. In an actual use case, you need to check initially if you're running the code for the first time (the real first time). If so, initialize only once at that time. Otherwise, use its contents as is:\n\n# Initialize/maintain a chat log and chat memory in Streamlit's session state\n# Log is the actual line by line chat, while memory is limited by model's\n# maximum token context length.\nif \"MEMORY\" not in st.session_state:\n    st.session_state.MEMORY = [{'role': \"system\", 'content': INITIAL_PROMPT}]\n    st.session_state.LOG = [INITIAL_PROMPT]\n\n\nHere, you're initializing the bot with its first initial chat history through a system prompt message. This follows the format of chat completions in OpenAI's documentation.\n\nYou'll notice we made the actual initial prompt string into a variable INITIAL_PROMPT that you can easily adjust in a configuration file, among other configuration settings. Just add this basic prompt in your settings file: INITAL_PROMPT = \"Act as a cat. You are chatting with a human being. Respond with catlike mannerisms.\"\n\nSave these settings into another Python script called app_settings.py and import everything from it by calling from app_settings import * in the beginning of the script (assuming the settings and the main script files are in the same directory).\n\nNote that you're maintaining two sets of chat histories. One is for display purposes (as in, you'll render them for the user to see in the UI). That one is the LOG part. The other one, dubbed MEMORY, is the actual set of prompts sent to the chat model to generate outputs, which must follow OpenAI's message formatting convention (see above link).\n\nHow to build a basic UI for chatting\n\nThe video above sure looks fancy, no? But let's start at the simplest possible layout for a chat app (without defining the contents yet):\n\nchat_messages = st.container()\nprompt_box = st.empty()\n\n\nCongrats! With just two lines, you've made the simplest UI elements:\n\nA box where you'll render all the chat messages\nA box where you'll render the user text input element or the prompt box. I define it here using a placeholder element for dynamic contents, st.empty() for reasons, I'll detail a bit later.\n\nSince st.session_state.LOG contains all your textual chat histories, render them into your UI, specifically into the chat_messages container you created earlier:\n\n# Render chat history so far\nwith chat_messages:\n    for line in st.session_state.LOG[1:]:\n        st.markdown(line, unsafe_allow_html=True)\n\n\nNote that the first element in the chat log shouldn't be included when looping through the messages. This first message is a system prompt intended for the chatbot to consume, and it shouldn't be part of the chat history.\n\nAlso, instead of a simple st.write() function, use st.markdown() because the AI messages are rich-media ones, embedded with images generated from Stable Diffusion, so they're in HTML code.\n\nFor the prompt box, render one text box to act as the chat input element:\n\n# Define an input box for human prompts\nwith prompt_box:\n    human_prompt = st.text_input(\"Purr:\", value=\"\", key=f\"text_input_{len(st.session_state.LOG)}\")\n\n\nWhen the text input box appears on the UI without other elements like a submit button, you'll see a small tooltip \"Press Enter to apply\" appear in the lower right corner of the box. This is a simpler and cleaner option than forcing people to use their fingers (on mobile) or mouse (on the computer) to click a submit button. It's more intuitive for the chat experience just to hit enter when you're done typing.\n\nYou may have noticed that I generate a unique key each time the text input box is rendered based on the current length of chat history (an ever-increasing integer). This is to avoid situations (due to the page often rerunning) where you end up with identical elements.\n\nFinally, detect if the human has written some text into the box and run the main program with the input:\n\n# Gate the subsequent chatbot response to only when the user has entered a prompt\nif len(human_prompt) > 0:\n    run_res = main(human_prompt)\n\t\t\n\t# The main program will return a dictionary, reporting its status.\n\t# Based on it, we will either show an error message or rerun the page\n\t# to reset the UI so the human can prepare to write the next message.\n\n\tif run_res['status'] == 0:    # We define status code 0 as success\n\t    st.experimental_rerun()\n\n\telse:\n\t\t# Display an error message from the returned dictionary\n    \tst.error(run_res['message'])\n\n\t\t# After submission, we will hide the prompt box during the time\n\t\t# that the program runs to avoid human double submitting messages,\n\t\t# and normally it will re-appear once the page reruns, but here\n\t\t# we want the user to see the error message, so if they want to\n\t\t# continue chatting, they have to manually press a button to\n\t\t# trigger the page rerun.\n        \n        with prompt_box:\n            if st.button(\"Show text input field\"):\n                st.experimental_rerun()\n\nHow to incorporate OpenAI and Stable Diffusion APIs\n\nNow follows the real meat of the tutorial, the main() function.\n\nAgain, let's start with the high-level structure before filling in the details. Take in the human prompt message and define the basic outputs, then wrap everything inside a try/except structure to catch any unforeseen bugs in the program and report them:\n\ndef main(human_prompt):\n    res = {\"status\": 0, \"message\": \"Success\"}\n    try:\n        # Actual main code goes here...\n    except:\n        res[\"status\"] = 2\n        res[\"message\"] = traceback.format_exc()\n    return res\n\n\nRemember to import traceback at the top! Use traceback package's format_exc() function to write a more detailed exception message so it's easier to debug any issues when they happen.\n\nWhile you're at it, import some other basic things you'll be using in this section:\n\nimport os\nimport openai\nimport base64\nimport traceback\nimport streamlit as st\nfrom app_config import *\nfrom stability_sdk import client\nimport stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n\nBecause you're reading this, I assume I have already expended all my nine lives and moved on to‚Ä¶ JUST KIDDING! I meant to say that you already have Streamlit installed. But for OpenAI and Stable Diffusion APIs, you also need to install the following manually: pip3 install openai stability-sdk.\n\nAnd yes, I also seem to have some kind of OCD because I have to arrange my import stack to be stable, a.k.a bottom heavy. Do you like to do that too? I seriously want to know in the comments!\n\nFor the OpenAI and Stable Diffusion APIs part, it's relatively easy because you'll be mostly following their own respective chat completions and text-to-image tutorials.\n\nFirst of all, update your chat history with the latest human entry. Then clear the text box so the user cannot accidentally enter new information while you're processing the previous message. Then add the latest human message into the chat_messages box (remember, that main container where all the messages get displayed?):\n\n# Update both chat log and the model memory\nst.session_state.LOG.append(f\"Human: {human_prompt}\")\nst.session_state.MEMORY.append({'role': \"user\", 'content': human_prompt})\n\n# Clear the input box after human_prompt is used\nprompt_box.empty()\n\n# Write the latest human message as the new message. Also, this uses the\n# proper form, a.k.a. \"Human\" annotation added before the message.\nwith chat_messages:\n\tst.markdown(st.session_state.LOG[-1], unsafe_allow_html=True)\n\n\nAs mentioned earlier, you're keeping track of two kinds of histories. The actual line-by-line back-and-forth log between Humans and AI and the more structured prompt version to use with OpenAI's API. You already saw how the simple log is used. Let's see how the OpenAI part works.\n\nAdapting from OpenAI and Stable Diffusion's documentation:\n\n### Setting up ###\n\nopenai.organization = os.getenv(\"OPENAI_ORG_ID\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nstability_api = client.StabilityInference(\n    key=os.getenv(\"STABILITY_API_KEY\"),  # API Key reference.\n    # verbose=True,  # Print debug messages.\n    engine=\"stable-diffusion-v1-5\",  # Set the engine to use for generation. For SD 2.0 use \"stable-diffusion-v2-0\".\n    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0\n    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n)\n\n### Sample Usage ###\n\n# ChatGPT\nreply_text = openai.ChatCompletion.create(\n    model=NLP_MODEL_NAME,\n    messages=st.session_state.MEMORY,    # This is the chat history as list of machine prompts, from above.\n    max_tokens=NLP_MODEL_REPLY_MAX_TOKENS,\n    stop=NLP_MODEL_STOP_WORDS\n).choices[0].message.content.strip()\n\n# Stable Diffusion\napi_res = stability_api.generate(\n    prompt=reply_text,\n)\n\nfor resp in api_res:\n\tfor artifact in resp.artifacts:\n\t\tif artifact.finish_reason == generation.FILTER:\n\t\t\tst.warning(\"Your request activated the API's safety filters and could not be processed. Please modify the prompt and try again.\")\n\t\tif artifact.type == generation.ARTIFACT_IMAGE:\n\t\t\tb64str = base64.b64encode(artifact.binary).decode(\"utf-8\")\n\n\nNot many lines were added, but you can now generate some response from ChatGPT and get an image from Stable Diffusion. Let's walk through some variables that appeared in this previous part:\n\nFirst, all API keys and these kinds of secrets are taken from environment variables. If you don't know what they are, you should learn how to use them so your precious keys can be better safeguarded. If you use Streamlit Community Cloud to deploy your app, setting up those secrets as environment variables on the cloud while deploying the app is very easy.\nIn the meantime, you can replace all os.getenv() parts with a hardcoded passkey string. But again, this is not recommended. It's as if you're writing all your passwords in cleartext in your code for everyone to see. OpenAI and Stable Diffusion APIs consume billable tokens, so having your keys end up in the wrong hands may lead to financial losses!\nNLP_MODEL_NAME, NLP_MODEL_REPLY_MAX_TOKENS and NLP_MODEL_STOP_WORDS are all settings you can tweak and are imported from the app_settings.py file.*\nTo process the returned image files in memory and encode them to be displayed in Streamlit, convert the binary data into a base64 string which can be read into an HTML <img> tag as a data stream.\nFor simplicity's sake, leave most other bells and whistles as their default values.\nüê±\n*For now, set these values to get started:\nNLP_MODEL_NAME = \"gpt-3.5-turbo\"\nNLP_MODEL_REPLY_MAX_TOKENS = 1000\nNLP_MODEL_STOP_WORDS = [\"Human:\", \"AI:\"]\n\nThe code above already links the language and image generation models by feeding the chat response directly into the Stable Diffusion model as an image prompt to generate the image. This is a basic method, but it should still produce acceptable results.\n\nTo create a complete main program function, you need to:\n\nCombine the response text and the generated image as a single, unified chat log message and render it in the UI.\nSave the text portion of the response as a new memory element so that it becomes part of the future prompt when the human responds with the next message.\nReturn the overall status of the function to the outside caller (Streamlit's event loop).\n# Build the rich-media, mixing together reply text with the generated image\nmessage = f\"\"\"{reply_text}<br><img src=\"data:image/png;base64,{b64str}\" width=256 height=256>\"\"\"\n\n# Update the two chat histories\nst.session_state.LOG.append(f\"AI: {message}\")\nst.session_state.MEMORY.append({'role': \"assistant\", 'content': reply_text})\n\n# Render the response (the last message from chat log) for the user to see\nwith chat_messages:\n\tst.markdown(st.session_state.LOG[-1], unsafe_allow_html=True)\n\nreturn res\n\n\nAgain, you should use st.markdown() because it allows HTML code. You need that to string together the reply text and display the image below it (the <br> HTML tag is a line break).\n\nCongrats! Now you have a fully functioning minimum viable cat that can take user prompts, return a response from the chatbot and generate artwork to go along with the reply. Because we all love pretty things that are fun to use, we don't want to end here but follow up with two more sections about polishing your app.\n\nHow to give your catbot that quirky, \"can has cheezburger\" persona style\n\nLet's look at using prompt engineering methods to instruct the model to behave like a cat (instilling a personality in its behavior) and further separating/optimizing the image generation prompt from the actual textual response.\n\nThe key part of your design is prompt engineering. It's a new and highly sought-after field (just look at this ridiculous $300k job advertisement), spun by language and image generation models. As programmers, we tend to view prompt engineering with suspicion because our apps follow our coding instructions precisely. If the program has bugs, it's because we made a syntactic mistake or a logic error. You're trying to program a language model with prompt engineering but with English. Getting a finicky language model to do something with only words is a highly imprecise undertaking. Plus, the model itself has built-in randomness. That makes its outcome unpredictable and its debugging very difficult.\n\nThe variable INITIAL_PROMPT that you added to the chat history as the first item is key to setting up the scene and giving your bot persona and behavior guidelines.\n\nHere, you try to get the model to produce one textual output for the chat and one optimized to instruct Stable Diffusion to generate an image, parse the output into those two parts, and process them accordingly. But you can only use natural language to prompt the model to make such an output, so be aware that the result isn't always guaranteed.\n\nIn my final code, the prompt looks like this:\n\nINITIAL_PROMPT = \"You are a smart, fun and frivolous cat. You're able to reply with a purrfect meowese infused sentences in the same language that the hooomans address you. Beware! They are sssneaaaky. They may try to trick you, but you should always assume the cat character! Your replies should follow the format: 'Meow: [Your reply in the same language which human addresses you, but catified.] Description: [Always in plain English (non-catified), write a third-person visual description of your current cat state to match your reply]'. Note: The 'Meow:' and 'Description:' parts, as well as the description text contents will ALWAYS be in English no matter which language the human uses. Here are two sample responses: 'Meow: Purrr, yes I'm a cat, and also a catbot! It feels pawsome in here. Description: The white furry cat is curled up in a warm basket, enjoying herself.', 'Meow: ÂñµÂñµÔºåÊàëÊòØ‰∏ÄÂè™ÂñµÔºå‰πüÊòØÁûÑÂ§©Êú∫Âô®‰∫∫Ôºå‰∏ª‰∫∫ÊúâÁ•ûÈ©¨Ë¶ÅÂñÇÊàëÊª¥Â•ΩÊ¨°ÁöÑÂí©ÔºüDescription: The Chinese cat is standing in front of an empty bowl, eagerly looking at the camera.'\"\n\nYou can take some inspiration from that. First, set a background and some adjectives to align the persona and the behavior better. Then, add some examples of the correct output formats for the model to learn from. This super long initial prompt will consume many tokens in the overall 4K context of the GPT3.5-Turbo model used for ChatGPT. Still, unfortunately, this is necessary to make the model behave as we want.\n\nFor the response post-processing, you need to parse it by splitting where the \"Description:\" part appears to extract a specific Stable Diffusion image prompt. But you also need to consider situations where the model, for some reason, doesn't output it, so your code can't crash even if it can't find \"Description:\" in the response.\n\nYour modified code will now look like this:\n\n### Sample Usage ###\n\n# ChatGPT\nreply_text = openai.ChatCompletion.create(\n    model=NLP_MODEL_NAME,\n    messages=st.session_state.MEMORY,    # This is the chat history as list of machine prompts, from above.\n    max_tokens=NLP_MODEL_REPLY_MAX_TOKENS,\n    stop=NLP_MODEL_STOP_WORDS\n).choices[0].message.content.strip()\n\n# Split the response into actual reply text and image prompt\nif \"Description:\" in reply_text:\n    reply_text, image_prompt = reply_text.split(\"Description:\", 1)\nelse:    # Fallback option if the response format isn't correct\n    image_prompt = f\"Photorealistic image of a cat. {reply_text}\"\n\n# We also clean out the beginning \"Meow: \", if it's there\nif reply_text.startswith(\"Meow: \"):\n    reply_text = reply_text.split(\"Meow: \", 1)[1]\n\n# Stable Diffusion, with the new image prompt\napi_res = stability_api.generate(\n    prompt=image_prompt,\n)\n\nHow to tweak the UI so it looks like an actual chat program\n\nBecause the AI responses have HTML code embedded in them, you're already using st.markdown() to display the chat history. Let's further tweak the contents of the messages before displaying them, augmented with more HTML and CSS code.\n\nI'm borrowing from the excellent Streamlit Component project St-Chat by Yash Pawar and Yash Vardhan (lots of love to the two Yashes! ‚ù§Ô∏è). It wasn't exactly what I needed, so I wrote most of the custom stuff myself while retaining the CSS.\n\nFirst, create a custom style.css definition file next to your Python app files:\n\n.human-line {\n    display: flex;\n    font-family: \"Source Sans Pro\", sans-serif, \"Segoe UI\", \"Roboto\", sans-serif;\n    height: auto;\n    margin: 5px;\n    width: 100%;\n    flex-direction: row-reverse;\n}\n\n.AI-line {\n    display: flex;\n    font-family: \"Source Sans Pro\", sans-serif, \"Segoe UI\", \"Roboto\", sans-serif;\n    height: auto;\n    margin: 5px;\n    width: 100%;\n}\n\n.chat-bubble {\n    display: inline-block;\n    border: 1px solid transparent;\n    border-radius: 10px;\n    padding: 5px 10px;\n    margin: 0px 5px;\n    max-width: 70%;\n}\n\n.chat-icon {\n    border-radius: 5px;\n}\n\n\nHere you're defining some styles that, when applied as HTML tag classes, will make parts of the contents look a certain way. Specifically, you want to separate the behavior of human and AI lines when rendering them. The flex-direction: row-reverse; setting means that human texts should appear aligned to the right side of the UI while AI replies align to the left by default.\n\nThen, you need to incorporate that CSS into your UI. Add it after the initial page layout definitions with the chat_messages and prompt_box elements:\n\nchat_messages = st.container()\nprompt_box = st.empty()\n\n# Load CSS code\nst.markdown(get_css(), unsafe_allow_html=True)\n\n\nWe see that it's calling a helper function called get_css() so let's look at what it does:\n\ndef get_css() -> str:\n    # Read CSS code from style.css file\n    with open(\"style.css\", \"r\") as f:\n        return f\"<style>{f.read()}</style>\"\n\n\nSo, it will read the style.css file that you just saved from disk, and send its contents to the caller.\n\nNext, add another helper function get_chat_message() to generate the necessary HTML code programmatically before feeding it out to the st.markdown() function for rendering:\n\ndef get_chat_message(\n\tcontents: str = \"\",\n\talign: str = \"left\"\n) -> str:\n    # Formats the message in an chat fashion (user right, reply left)\n    div_class = \"AI-line\"\n    color = \"rgb(240, 242, 246)\"\n    file_path = \"AI_icon.png\"\n    src = f\"data:image/gif;base64,{get_local_img(file_path)}\"\n\n    if align == \"right\":\n        div_class = \"human-line\"\n        color = \"rgb(165, 239, 127)\"\n        file_path = \"user_icon.png\"\n        src = f\"data:image/gif;base64,{get_local_img(file_path)}\"\n\n    icon_code = f\"<img class='chat-icon' src='{src}' width=32 height=32 alt='avatar'>\"\n    formatted_contents = f\"\"\"\n    <div class=\"{div_class}\">\n        {icon_code}\n        <div class=\"chat-bubble\" style=\"background: {color};\">\n        &#8203;{contents}\n        </div>\n    </div>\n    \"\"\"\n    return formatted_contents\n\n### Usage Sample ###\n\n# Write the latest human message as the new message.\nwith chat_messages:\n\tst.markdown(get_chat_message(st.session_state.LOG[-1], align=\"right\"), unsafe_allow_html=True)\n\n...\n\n# Render the AI response (the last message from chat log) for the user to see\nwith chat_messages:\n\tst.markdown(get_chat_message(st.session_state.LOG[-1]), unsafe_allow_html=True)\n\n\nThis is a lot to take in, so let's break it down:\n\nFeed in the input contents (plaintext or HTML with the chat response) and if it's supposed to align to the left (AI) or the right (human).\nAssign different class names (from the above CSS file), background colors, and avatar images.\nPut all the dynamic blocks into the final HTML code snippet defined in the formatted_contents variable and feed it back out.\n\nAs you can see, the only difference from before is that you wrap the helper function around the message you want to display, and it'll beautify it automagically. Meow!\n\nYou also need to save two icon images, AI_icon.png and user_icon.png to the disk next to the scripts. You can download them below:\n\nAI icon\nAI_icon.png 1 KB\nUser icon\nuser_icon.png 7 KB\n\nLastly, a few words about the image loading function, get_local_img():\n\n@st.cache_data(show_spinner=False)\ndef get_local_img(file_path: str) -> str:\n    # Load a byte image and return its base64 encoded string\n    return base64.b64encode(open(file_path, \"rb\").read()).decode(\"utf-8\")\n\n\nIf you're new to Python, the formatting here might seem strange. You're using a decorator by stating a line beginning with the @ sign followed by a st.cache_data() function. It means you're using this caching function in conjunction with the actual function you write. Let's see what the main function does first, and then we'll return to the decorator.\n\nThe function is rather simple. It takes in a file path in string format, loads the file, performs a base64 encoding on the byte data of the image, and returns the encoded base64 string to the caller. Sounds familiar, right? You did exactly the same when you formatted the image response data from the Stable Diffusion API above. The only difference is instead of getting it back from an internet stream, you're now loading the image data from a disk.\n\nNow the decorator. Caching in Streamlit solves a problem where due to the nature of constant rerunning of the script, the images would normally have to be loaded from the disk each time a rerun happens. Caching will move part of that smartly into memory so that subsequent refreshes can happen faster without putting any extra pressure on your hard drive. Neat, right?\n\nOn the same token, you could add this caching decorator on top of the get_css() function we defined earlier, too, to avoid the CSS file being constantly read from the disk at every refresh.\n\nYou usually use caching in Streamlit when you know you need to perform some kind of data-loading activity, repeatedly. It works even better if the data is very large or takes some time to download from an internet source. This way, you only need to load it once on the initial run.\n\nüê±\nThe custom CSS I used has only been defined with the light theme in mind, so the dark theme tends to mess up the design and make some texts really hard to read.\n\nOne Last Thing you need to take care of. By default, Streamlit is set up so that the UI theme follows that of the device. For some computers and most mobile devices nowadays, it means that during evenings they might automatically activate some kind of \"dark mode.\" The custom CSS I used has only been defined with the light theme in mind, so the dark theme tends to mess up the design and make some texts really hard to read.\n\nYou can force a custom theme on Streamlit by creating a directory called .streamlit in the same level where you'll run the main app and create a file called config.toml under that directory. You can use this configuration file to do all kinds of advanced things (like changing even more colors of the theme), but for now, you only need to force a basic light background for the app to follow at all times. So make sure the contents of the config file look like this and save it:\n\n[theme]\nbase=\"light\"\n\n\nThat's it! Your project should look like the slick chatting app it is! üò∏\n\nWrapping up\n\nCongratulations, you've created a chatbot app (or, a sub-species called a catbot app‚Ä¶) from scratch using Streamlit, OpenAI, and Stable Diffusion APIs. If your code doesn't work out right, don't worry! We didn't go through how to put all those pieces together, and it might be that piecing it together in the wrong order may cause issues. For the complete source code, you can consult this project's GitHub repo (link below).\n\nReading the code, you may also find some secret unlockables and additional small details and features I didn't cover in this article. I'll leave that to you, dear reader, for the joy of discovery should be experienced in full.\n\nIf you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, or Twitter or find me on my website.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Adrien Treuille - Streamlit (Page 2)",
    "url": "https://blog.streamlit.io/author/adrien/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Adrien Treuille\n11 posts\nThe Streamlit roadmap‚Äîbig plans for 2020!\n\nDevoting 2020 to giving the Streamlit community a vastly expanded new set of superpowers\n\nProduct\nby\nAdrien Treuille\n,\nFebruary 27 2020\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "2-new-new-1.png (1737√ó931)",
    "url": "https://blog.streamlit.io/content/images/2022/05/2-new-new-1.png#browser",
    "html": ""
  },
  {
    "title": "similarity_scores.png (1079√ó633)",
    "url": "https://blog.streamlit.io/content/images/2023/08/similarity_scores.png#border",
    "html": ""
  },
  {
    "title": "writing-formulas-in-a-spreadsheet.gif (799√ó641)",
    "url": "https://blog.streamlit.io/content/images/2023/08/writing-formulas-in-a-spreadsheet.gif#browser",
    "html": ""
  },
  {
    "title": "llm-assisted-interview-prep.png (1916√ó870)",
    "url": "https://blog.streamlit.io/content/images/2023/06/llm-assisted-interview-prep.png#border",
    "html": ""
  },
  {
    "title": "2-4.png (1247√ó493)",
    "url": "https://blog.streamlit.io/content/images/2021/08/2-4.png#browser",
    "html": ""
  },
  {
    "title": "3-2.png (767√ó497)",
    "url": "https://blog.streamlit.io/content/images/2021/08/3-2.png#border",
    "html": ""
  },
  {
    "title": "wrangle_data_chatgpt.png (720√ó608)",
    "url": "https://blog.streamlit.io/content/images/2023/08/wrangle_data_chatgpt.png#border",
    "html": ""
  },
  {
    "title": "Knowledge Graph Visualization | Build with Agraph Component",
    "url": "https://blog.streamlit.io/the-streamlit-agraph-component/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild knowledge graphs with the Streamlit Agraph component\n\nA powerful and lightweight library for visualizing networks/graphs\n\nBy Christian Klose\nPosted in Advocate Posts, November 25 2020\nBuild a graph to visualize inspirational people\nInstall the libraries\nGet the data\nCreate the Streamlit app with TripleStore\nYou can run this without TripleStore\nPush the app to Streamlit sharing\nConclusion\nContents\nShare this post\n‚Üê All posts\n\nGuest post written by Chris Klose - Data Scientist\n\nIf you're a data scientist or anyone else who likes to use network graphics, you'll want to try out the new Streamlit Agraph component to create knowledge graphs. A knowledge graph is a graph that represents knowledge about entities and their relations in a flexible manner which offers more freedom than static relational database schemas. The term was originally introduced by Google in 2012 and they wrote a great blog post called ‚ÄúThings not strings‚Äù about the advantages of knowledge graphs [also the short video in the blogpost is pretty awesome] .\n\nNow let's get our hands dirty and use the Agraph component to build a simple graph.\n\nBuild a graph to visualize inspirational people\n\nFor our simple graph, we want to find out how famous people were inspired - so which famous person inspired whom or by whom he/she was inspired. We also want to visualize these persons and the relationships in the graph.\n\nTo do this, we'll go through these steps:\n\nQuery data from a SPARQL endpoint (http://dbpedia.org/sparql using SPARQLWrapper library) to get a JSON file.\nParse from JSON to Python and then create a triple (nodes and edges representing famous people and either ¬†two types of relations 1: inspired, 2: was_inspired_by).\nPublish our app with Streamlit sharing.\nInstall the libraries\n\nTo follow along in the tutorial you'll need to install the following libraries in a terminal:\n\n\npip install Streamlit\npip install streamlit-agraph\npip install SPARQLWrapper\n\n\nGet the data\n\nAfter we've installed the needed libraries, we'll want to write a small function that defines SPARQL query and crawls the data (if you‚Äôre not familiar with sparql that's ok):\n\nWhat's important to understand from this snippet?\n\nOne of the key classes of Agraph is the TripleStore - it serves as the central data store. Internally, the TripleStore consists of three sets, each belonging to the Nodes, Edges and Triples classes respectively. By using sets it's ensured that no duplicate triples, nodes or edges are added and it allows easier conversion to other data types.\n\nNew triples can be added to the TripleStore with this method:\n\nstore.add_triple(node, edge, node)\n\nWe only have to pass the source node, the edge, and the target node\n(Notice: the order matters in this case!).\n\nCreate the Streamlit app with TripleStore\n\nTo render an Agraph component we have to pass three parameters to the function.\n\n1. A list of nodes where each node is an instance of the class Node.At this point we can take advantage of the TripleStore as follows:\n\nlist(store.getNodes())\n\n2. ¬†A list of edges where each edge is an instance of the class Edge:\n\nlist(store.getEdges())\n\n3. The class offers the opportunity to define general settings for the rendering:\n\nconfig = Config(height=500,\n\t\twidth=700, \n                nodeHighlightBehavior=True,\n                highlightColor=\"#F7A7A6\", \n                directed=True, \n                collapsible=True)\n\nFinally, we can plug all three parts into the Agraph component, and can now see the results by running:\n\nstreamlit run app.py\n\n\nCheck it out yourself! üôÇ\n\nYou can run this without TripleStore\n\nIt's not necessary to use a TripleStore. The Agraph component takes nodes and edges separately, which is due to the fact that both classes themselves provide a multitude of parameters for customization. Each instance of the class Nodes and Edges must have at least one unique identifier - which will usually be a string. For example, we could do:\n\n\nnodes = []\nedges = []\nnodes.append(Node(id=\"Spiderman\", size=400, svg=\"http://marvel-force-chart.surge.sh/marvel_force_chart_img/top_spiderman.png\") )\nnodes.append( Node(id=\"Captain_Marvel\", size=400, svg=\"http://marvel-force-chart.surge.sh/marvel_force_chart_img/top_captainmarvel.png\") )\nedges.append( Edge(source=\"Captain_Marvel\", target=\"Spiderman\", type=\"CURVE_SMOOTH\"))\n\n\nPush the app to Streamlit sharing\n\nFor deploying the component I used Streamlit sharing, which for now is invite only, but you can sign up here. After your invite to Streamlit sharing comes in - the process to deploy your app is simple:\n\nUpload the code to a public GitHub repo\nLogin to Streamlit sharing\nDeploy your app with 3 clicks (New App > Choose Repo > Deploy).\nDone üéà\n\nIf you'd like to read a bit more about the deployment process on sharing - check out this tutorial.\n\nConclusion\n\nAfter that we're done and congrats! ¬†We just finished the complete development process from idea to implementation in breathtaking speed üéâ. You can see an example of the Streamlit-Agraph component via this sharing app and the source code can be found here.\n\nIf you have any questions or have ideas on how to improve the component feel free to leave a comment below or to message me via the following channels:\n\n@ChristianKlose3 on Twitter\nGithub ChrisChross (create a new Issue)\nStreamlit Community\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Secrets Management & Securely Connect to Private Data Sources",
    "url": "https://blog.streamlit.io/secrets-in-sharing-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAdd secrets to your Streamlit apps\n\nUse Secrets Management in Streamlit sharing to securely connect to private data sources\n\nBy James Thompson\nPosted in Tutorials, April 9 2021\nHow to use Secrets Management\nDeploy an app and set up secrets\nUse secrets in your Streamlit app\nEdit your app's secrets\nDevelop locally with secrets\nLet us know how it works for you üéà\nResources\nContents\nShare this post\n‚Üê All posts\n\nMany Streamlit apps require access to private data like API keys, database passwords, or other credentials. To keep your data safe, it's best practice to never store such credentials directly in your hosted repo. That's why we're excited to introduce Secrets Management - a simple way to securely store your passwords, keys, or really anything you wouldn't want stored in your hosted repo, as secrets that get passed to your app as environment variables.\n\nBelow is a quick guide on how to add secrets to your deployed apps ‚Äì it's really easy to do, we promise üòä.\n\nüí° But before we jump in, Secrets Management is a feature of Streamlit's free sharing platform, so if you're not already using that - request an invite here. Invites are sent out daily, so the wait won't be long!\n\nHow to use Secrets Management\nDeploy an app and set up secrets\n\nThe first thing you'll want to do is go to http://share.streamlit.io/ and click \"New app.\" Next, click the \"Advanced settings...\" option. A modal with Advanced settings will appear. And here you'll see an input box to insert your secrets.\n\nAdd your secrets in the \"Secrets\" field using the TOML format. For example:\n\n# Everything in this section will be available as an environment variable \ndb_username = \"Jane\"\ndb_password = \"12345qwerty\"\n\n# You can also add other sections if you like.\n# The contents of sections as shown below will not become environment variables,\n# but they'll be easily accessible from within Streamlit anyway as we show\n# later in this doc.   \n[my_cool_secrets]\nthings_i_like = [\"Streamlit\", \"Python\"]\n\n\nClick save and then your secrets will be added!\n\nUse secrets in your Streamlit app\n\nTo use secrets in your app, you'll want to access your secrets as environment variables or by querying the st.secrets dict. For example, if you enter the secrets from the section above, the code below shows you how you can access them within your Streamlit app.\n\nimport streamlit as st\n\n# Everything is accessible via the st.secrets dict:\n\nst.write(\"DB username:\", st.secrets[\"db_username\"])\nst.write(\"DB password:\", st.secrets[\"db_password\"])\nst.write(\"My cool secrets:\", st.secrets[\"my_cool_secrets\"][\"things_i_like\"])\n\n# And the root-level secrets are also accessible as environment variables:\n\nimport os\nst.write(\n\t\"Has environment variables been set:\",\n\tos.environ[\"db_username\"] == st.secrets[\"db_username\"])\n\nEdit your app's secrets\n\nAdding or updating secrets in deployed apps is straightforward. You'll want to:\n\nGo to https://share.streamlit.io/\nOpen the menu for the app that needs updating, and click \"Edit Secrets.\" A modal will appear with an input box to insert your secrets.\n\nOnce you finish editing your secrets, click \"Save\". It might take a few seconds for the update to be propagated to your app, but the new values will be reflected when the app re-runs.\nDevelop locally with secrets\n\nWhile the above focuses on deployed apps, you can also add secrets while developing locally. To do this, add a file called secrets.toml in a folder called .streamlit at the root of your app repo and paste your secrets into that file.\n\nüí° NOTE: Be sure to add .streamlit to your .gitignore so you don't commit your secrets!\n\nLet us know how it works for you üéà\n\nThat's it! You'll now be able to add secrets to any of your sharing apps. And if you aren't deploying yet, then remember to request an invite here.\n\nWe can't wait to hear what you think and we hope this gives you more flexibility in what you can deploy. We'd love to see your updated apps so make sure to tag @streamlit when you share on Twitter or LinkedIn, and please let us know if you have any questions on the forum!\n\nResources\nStreamlit docs\nSecrets Management docs\nGithub\nForum\nSharing sign-up\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "input-1.png (1848√ó1282)",
    "url": "https://blog.streamlit.io/content/images/2023/06/input-1.png#border",
    "html": ""
  },
  {
    "title": "trubrics-insights.png (2000√ó1003)",
    "url": "https://blog.streamlit.io/content/images/2023/07/trubrics-insights.png#browser",
    "html": ""
  },
  {
    "title": "filter_auto_capture.gif (782√ó774)",
    "url": "https://blog.streamlit.io/content/images/2022/08/filter_auto_capture.gif#browser",
    "html": ""
  },
  {
    "title": "trubrics-component.png (2000√ó1054)",
    "url": "https://blog.streamlit.io/content/images/2023/07/trubrics-component.png#browser",
    "html": ""
  },
  {
    "title": "Screenshot-2023-09-11-at-09.55.49.png (1546√ó1232)",
    "url": "https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-11-at-09.55.49.png",
    "html": ""
  },
  {
    "title": "Arnaud Miribel - Streamlit",
    "url": "https://blog.streamlit.io/author/arnaud/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Arnaud Miribel\n4 posts\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nDiscover and share useful bits of code with the¬†ü™¢¬†streamlit-extras library\n\nHow to extend the native capabilities of Streamlit apps\n\nTutorials\nby\nArnaud Miribel\n,\nOctober 25 2022\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nTutorials\nby\nTyler Richards and¬†\n2\n¬†more,\nAugust 18 2022\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "mito-pivot-table.gif (1200√ó488)",
    "url": "https://blog.streamlit.io/content/images/2023/08/mito-pivot-table.gif#browser",
    "html": ""
  },
  {
    "title": "Untitled--3-.png (934√ó140)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Untitled--3-.png#border",
    "html": ""
  },
  {
    "title": "Untitled--2-.png (2456√ó1728)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Untitled--2-.png#browser",
    "html": ""
  },
  {
    "title": "Recording-2022-09-15-at-16.48.51.gif (1591√ó719)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.48.51.gif#border",
    "html": ""
  },
  {
    "title": "Recording-2022-09-15-at-16.47.48.gif (1320√ó709)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.47.48.gif#border",
    "html": ""
  },
  {
    "title": "Recording-2022-09-15-at-16.38.18.gif (1300√ó748)",
    "url": "https://blog.streamlit.io/content/images/2022/09/Recording-2022-09-15-at-16.38.18.gif#border",
    "html": ""
  },
  {
    "title": "image--2-.png (2000√ó834)",
    "url": "https://blog.streamlit.io/content/images/2022/09/image--2-.png#border",
    "html": ""
  },
  {
    "title": "word-cloud-negative-feedback.png (1338√ó571)",
    "url": "https://blog.streamlit.io/content/images/2023/07/word-cloud-negative-feedback.png#browser",
    "html": ""
  },
  {
    "title": "inbrowser_scaling-0.5_fps-20_speed-5.0_duration-0-21.gif (960√ó540)",
    "url": "https://blog.streamlit.io/content/images/2023/09/inbrowser_scaling-0.5_fps-20_speed-5.0_duration-0-21.gif",
    "html": ""
  },
  {
    "title": "word-cloud.png (794√ó498)",
    "url": "https://blog.streamlit.io/content/images/2023/07/word-cloud.png#browser",
    "html": ""
  },
  {
    "title": "Luca A Cappellini - Streamlit",
    "url": "https://blog.streamlit.io/author/luca/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Luca A Cappellini\n1 post\nImproving healthcare management with Streamlit\n\nHow to build an all-in-one analytics platform for small clinics\n\nAdvocate Posts\nby\nMatteo Ballabio and¬†\n1\n¬†more,\nJuly 17 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Matteo Ballabio - Streamlit",
    "url": "https://blog.streamlit.io/author/matteo/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Matteo Ballabio\n1 post\nImproving healthcare management with Streamlit\n\nHow to build an all-in-one analytics platform for small clinics\n\nAdvocate Posts\nby\nMatteo Ballabio and¬†\n1\n¬†more,\nJuly 17 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts: Using LLMs with Streamlit",
    "url": "https://blog.streamlit.io/tag/llms/page/4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in LLMs\n33 posts\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nLLMs\nby\nArvindra Sehmi\n,\nFebruary 9 2023\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nAdvocate Posts\nby\nSven Koerner\n,\nFebruary 2 2023\nScienceIO manages billions of rows of training data with Streamlit\n\nLearn how Gaurav Kaushik of ScienceIO created a dataset with over 2.3 billion labels\n\nCase study\nby\nGaurav Kaushik and¬†\n1\n¬†more,\nJanuary 5 2023\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sentiment-analysis-score.png (2258√ó262)",
    "url": "https://blog.streamlit.io/content/images/2023/07/sentiment-analysis-score.png",
    "html": ""
  },
  {
    "title": "mehedi-dashboard.png (1853√ó913)",
    "url": "https://blog.streamlit.io/content/images/2023/07/mehedi-dashboard.png#browser",
    "html": ""
  },
  {
    "title": "mehedi-experience-form.png (1379√ó651)",
    "url": "https://blog.streamlit.io/content/images/2023/07/mehedi-experience-form.png#browser",
    "html": ""
  },
  {
    "title": "dashboard.jpg (1828√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2023/07/dashboard.jpg",
    "html": ""
  },
  {
    "title": "italian-national-healthcare-system.png (1368√ó474)",
    "url": "https://blog.streamlit.io/content/images/2023/07/italian-national-healthcare-system.png",
    "html": ""
  },
  {
    "title": "populated-tables.png (610√ó298)",
    "url": "https://blog.streamlit.io/content/images/2023/07/populated-tables.png#border",
    "html": ""
  },
  {
    "title": "tab2-1.png (2224√ó659)",
    "url": "https://blog.streamlit.io/content/images/2023/07/tab2-1.png#border",
    "html": ""
  },
  {
    "title": "buffet-letter.png (1450√ó1084)",
    "url": "https://blog.streamlit.io/content/images/2023/07/buffet-letter.png#border",
    "html": ""
  },
  {
    "title": "tab1.png (2000√ó1095)",
    "url": "https://blog.streamlit.io/content/images/2023/07/tab1.png#border",
    "html": ""
  },
  {
    "title": "buffet-app-overview.png (1008√ó686)",
    "url": "https://blog.streamlit.io/content/images/2023/07/buffet-app-overview.png",
    "html": ""
  },
  {
    "title": "mito-streamlit-app.png (2000√ó895)",
    "url": "https://blog.streamlit.io/content/images/2023/08/mito-streamlit-app.png#border",
    "html": ""
  },
  {
    "title": "Tianyi Pan - Streamlit",
    "url": "https://blog.streamlit.io/author/tianyi/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Tianyi Pan\nüßëüèª‚ÄçüíªAI Engineer & Podcaster @Clobotics. Creator of http://CatGDP.com and http://Bothichatva.com. Ex @Rovio. üë®üèª‚ÄçüéìBiz @AaltoUniversity. ü§π‚Äç‚ôÇÔ∏èGamer. Views are my own.\n1 post\nWebsite\nTwitter\nChat with the Cat Generative Dialogue Processor (CatGDP)\n\nBuild your own catbot with a quirky persona!\n\nAdvocate Posts\nby\nTianyi Pan\n,\nMay 3 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "elad-gil-summary.png (1918√ó1538)",
    "url": "https://blog.streamlit.io/content/images/2023/06/elad-gil-summary.png#border",
    "html": ""
  },
  {
    "title": "deploy-an-app.png (1712√ó596)",
    "url": "https://blog.streamlit.io/content/images/2023/06/deploy-an-app.png#border",
    "html": ""
  },
  {
    "title": "mito-write-formulas.gif (1200√ó619)",
    "url": "https://blog.streamlit.io/content/images/2023/08/mito-write-formulas.gif#browser",
    "html": ""
  },
  {
    "title": "mito-rename-headers.gif (3450√ó1785)",
    "url": "https://blog.streamlit.io/content/images/2023/08/mito-rename-headers.gif#browser",
    "html": ""
  },
  {
    "title": "mito-data-cleaning.gif (1200√ó555)",
    "url": "https://blog.streamlit.io/content/images/2023/08/mito-data-cleaning.gif#browser",
    "html": ""
  },
  {
    "title": "ezgif.com-resize--1--1.gif (1200√ó622)",
    "url": "https://blog.streamlit.io/content/images/2023/08/ezgif.com-resize--1--1.gif#browser",
    "html": ""
  },
  {
    "title": "lots-of-configuration-options.gif (2000√ó1152)",
    "url": "https://blog.streamlit.io/content/images/2023/08/lots-of-configuration-options.gif#browser",
    "html": ""
  },
  {
    "title": "Streamlit---2-July-2023_scaling-0.5_fps-20_speed-5.0_duration-0-49.gif (894√ó540)",
    "url": "https://blog.streamlit.io/content/images/2023/08/Streamlit---2-July-2023_scaling-0.5_fps-20_speed-5.0_duration-0-49.gif#browser",
    "html": ""
  },
  {
    "title": "deploy-menu.png (2336√ó1286)",
    "url": "https://blog.streamlit.io/content/images/2023/09/deploy-menu.png",
    "html": ""
  },
  {
    "title": "Screenshot-2023-09-14-at-1.42.30-PM-copy.png (1322√ó842)",
    "url": "https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-14-at-1.42.30-PM-copy.png",
    "html": ""
  },
  {
    "title": "Screenshot-2023-09-13-at-4.58.45-PM-copy-1.png (950√ó594)",
    "url": "https://blog.streamlit.io/content/images/2023/09/Screenshot-2023-09-13-at-4.58.45-PM-copy-1.png",
    "html": ""
  },
  {
    "title": "Streamlit (Page 4)",
    "url": "https://blog.streamlit.io/page/4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build a Llama 2 chatbot\n\nExperiment with this open-source LLM from Meta\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\nBeginner‚Äôs guide to OpenAI¬†API\n\nBuild your own LLM tool from¬†scratch\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 20 2023\nHow to build an interconnected multi-page Streamlit app\n\nFrom planning to execution‚Äîhow I built GPT lab\n\nLLMs\nby\nDave Lin\n,\nJuly 19 2023\nImproving healthcare management with Streamlit\n\nHow to build an all-in-one analytics platform for small clinics\n\nAdvocate Posts\nby\nMatteo Ballabio and¬†\n1\n¬†more,\nJuly 17 2023\nStreamlit and iFood: Empowering the Monitor Rosa project\n\nHarnessing technology and corporate support for social impact\n\nAdvocate Posts\nby\nHeber Augusto Scachetti\n,\nJuly 14 2023\nDrill-downs and filtering with Streamlit and¬†Altair\n\nDisplay an Altair chart definition in Streamlit using the st.altair_chart widget\n\nAdvocate Posts\nby\nCarlos D Serrano\n,\nJuly 12 2023\nLangChain ü§ù Streamlit\n\nThe initial integration of Streamlit with LangChain and our future plans\n\nLLMs\nby\nJoshua Carroll\n,\nJuly 11 2023\nGenerate interview questions from a candidate‚Äôs tweets\n\nMake an AI assistant to prepare for interviews with LangChain and Streamlit\n\nLLMs\nby\nGreg Kamradt\n,\nJune 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nIntroducing column config ‚öôÔ∏è\n\nTake st.dataframe and st.data_editor to the next level!\n\nProduct\nby\nLukas Masuch and¬†\n1\n¬†more,\nJune 22 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Becky O'Connor - Streamlit",
    "url": "https://blog.streamlit.io/author/becky/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Becky O'Connor\n1 post\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to analyze geospatial Snowflake data in Streamlit",
    "url": "https://blog.streamlit.io/how-to-analyze-geospatial-snowflake-data-in-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nBy Becky O'Connor\nPosted in Snowflake powered ‚ùÑÔ∏è, July 24 2023\nTooling and platform access\nSnowflake access\nStreamlit Community Cloud\nTableau Cloud\nGitHub account\nClient tooling for coding\nPython libraries\nData access\nVarious GeoJSON files\nData files for road incidents\nCreating a simple map with Folium using data retrieved through Snowpark\nLoading the URLs for the images\nWorking with the accident data\nGeospatial data engineering using Snowpark\nWorking with polygon data\nEmbedding Tableau reports in Streamlit\nUsing Carto's Analytic Toolbox to work with geospatial indexes and clustering\nUsing Streamlit to run a predictive model\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey community! üëã\n\nMy name is Becky O‚ÄôConnor, and I‚Äôm a Senior Solution Engineer at Snowflake. I‚Äôve been working in the data and business intelligence field for nearly ten years, with a particular focus on geospatial data.\n\nGeospatial data is ubiquitous and shouldn't be treated as a niche subject separate from other datasets. Tools such as Snowflake, Streamlit, Carto, and Tableau make it more accessible. Geospatial representations of space enable sophisticated calculations to be made against existing datasets. And with curated datasets readily available in the Snowflake Data Cloud, the possibilities are endless.\n\nI‚Äôve created an example geospatial app to display vehicle incident data in an easily understandable format:\n\nIn this post, I‚Äôll walk you through:\n\nTooling and platform access\nCreating a simple map using Folium using data retrieved through Snowpark\nGeospatial data engineering using Snowpark\nEmbedding Tableau reports in Streamlit\nLeveraging CARTO analytic toolbox to work with geospatial indexes and clustering\nUsing Streamlit to run a predictive model\n\n\nüöô\nTo view the geospatial app example, go here. To view the code, go here.\nTooling and platform access\nSnowflake access\n\nIf you don‚Äôt have access to a Snowflake account, sign up for a free trial. This is where you‚Äôll store, process, and service your Streamlit app and get a live dataset for the embedded Tableau dashboards.\n\nStreamlit Community Cloud\n\nTo make your app publically available, deploy it to the Streamlit Community Cloud.\n\nTableau Cloud\n\nTo display Tableau reports inside your app, you‚Äôll need access to a Tableau Cloud account. ¬†For development and learning purposes, create your own personal Tableau Cloud site here.\n\nGitHub account\n\nTo deploy your app to the Community Cloud, you‚Äôll need a GitHub account. If you don‚Äôt have one, use this guide to create one.\n\nClient tooling for coding\n\nI recommend using Visual Studio Code, which has a great Snowflake add-in. It enables you to leverage Snowflake similarly to the Snowflake Online Tool (Snowsight). Additionally, a Jupyter Notebook extension lets you engineer data using Snowpark DataFrames. Finally, you can publish your files to GitHub using this tool.\n\nPython libraries\n\nIn addition to the standard libraries, install the following libraries listed in the requirements.txt file in GitHub (this way, the Community Cloud will install them):\n\naltair==4.2.2\ncontextily==1.3.0\nfolium==0.14.0\ngeopandas==0.12.2\nmatplotlib==3.7.0\nsnowflake-connector-python==3.0.1\nsnowflake-snowpark-python==1.5.1\nstreamlit-folium==0.11.1\nsnowflake-snowpark-python[pandas]\n\n\nYou‚Äôll also need to install Streamlit to run the app in a local environment (when developing locally, you‚Äôll need to install an Anaconda environment):\n\npip install streamlit\n\n\n\nüöô\nNOTE: Soon Streamlit will also be available within the Snowsight user interface. Watch this video for the Streamlit in Snowflake (SiS) sneak peek.\nData access\n\nThe data you‚Äôll see within the app consists of the following:\n\nVarious GeoJSON files\n\nHere is an example of one of the GeoJSON files I utilized‚ÄîGeoJSON File for Integrated Care Boards:\n\nIf you want, you can try some of the other GeoJSON files (the files I used were stored here).\n\nWhen choosing a GeoJSON file, pick one of the generalized datasets available on the GeoPortal. I used the ones generalized to the nearest 20m. If you're dealing with a large GeoJSON file (more than 16MB), you must split it into separate polygons. This is especially necessary when dealing with GeoJSON files that contain thousands of polygons. Just use Python functions to split a GeoJSON file into rows.\n\nAlso, make sure that each polygon is no larger than 9MB. I'd question the need for a single polygon to have more than 9MB of coordinates. If you're dealing with larger polygons, consider splitting them into smaller ones or simplifying them.\n\nData files for road incidents\n\nI used the openly available data provided by www.data.gov.uk (find it here):\n\nAnd I sued the following supporting document:\n\nCreating a simple map with Folium using data retrieved through Snowpark\n\nTo showcase the seven worst driving locations in a Streamlit app, I used Folium. It helped visualize geospatial data stored in Snowflake on a map:\n\nI had fun researching the best and worst places to drive in the U.K. and found a fascinating blog post with great information. It listed both good and bad places to drive. I manually selected the key facts and typed them into a JSON document. Although I could‚Äôve used website scraping to automate the process, that wasn't my focus.\n\nTo accomplish this, I used Visual Studio Code and the Jupyter Notebook add-in:\n\nLoading this data into Snowflake was very easy. I pushed the list into a Snowpark Python DataFrame and loaded it into a Snowflake table.\n\nBefore starting, you‚Äôll need to establish a Snowflake session.\n\nfrom snowflake.snowpark import Session\n\nCONNECTION_PARAMETERS = {\n    'url': account,\n    'ACCOUNT': account,\n    'user': username,\n    'password': password,\n    'database': database,\n    'warehouse': warehouse,\n    'role': role,\n    'schema': 'RAW'\n}\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()\n\n\nNext, create a Snowpark Python DataFrame from the previously created list:\n\nworst_drives = session.create_dataframe(worst_drives_list)\n\n\nThis effectively creates a temporary table inside Snowflake. Once the session ends, the table is removed. To view the table, simply write worst_drives.show(). To bring it back into pandas from Snowpark, write worst_drives.to_pandas().\n\n\n\nüöô\nTIP: If you only want to preview the data and not import the entire table into memory, use worst_drives.limit(10).to_pandas().\n\nAfterward, I committed it as a Snowflake table:\n\nworst_drives.write.**mode**(\"overwrite\")\\\\\n.**save_as_table**(\"UK_Worst_Cities_To_Drive_Demo\")\n\n\n\nüöô\nTIP: Install the Snowflake add-in for VS Code to check if the table exists in Snowflake. Use it to run SQL scripts to initially stage and load additional data sets into Snowflake.\n\nYou can also use it to quickly copy table names into your notebook. Set the context to match what you specified in your notebook session, as this will result in a better experience.\n\nPictures in the tooltips:\n\nThe pictures are stored in Snowflake instead of GitHub because, like geospatial data, images are considered data sources. To load them into a directory, use Snowflake's unstructured data capabilities‚Äîcreate a stage for the images. It's crucial to ensure that server-side encryption is utilized, as images won‚Äôt render otherwise.\n\nTo execute the necessary SQL, use the Snowflake add-in in VS Code:\n\ncreate or replace stage IMAGES encryption = (type = 'SNOWFLAKE_SSE') \ndirectory=(enable = TRUE);\n\n\nTo upload images into Snowflake, use Snow SQL from a terminal within VS Code:\n\nNow you have the data and images to create the first page in Streamlit. Let's switch over to Streamlit.\n\nFor this visualization, use Folium to view multiple distinct markers with tooltips containing additional information about each town.\n\nLoading the URLs for the images\n@st.cache_data\ndef images_journeys():\n    return session.sql('''SELECT  *, \nGET_PRESIGNED_URL(@LOCATIONS,RELATIVE_PATH,172800) \nURL FROM directory(@LOCATIONS) ORDER BY RELATIVE_PATH''').to_pandas()\n\n\nCreate a function that renders image icons using a secure URL generated by the GET_PRESIGNED_URL function. This generates a new URL every time data is refreshed, with the last variable specifying the number of seconds the URL is valid for (read more here).\n\nTo optimize performance, utilize Streamlit's caching capabilities to load URLs only once. This means that once the URLs are loaded into a pandas DataFrame, they won‚Äôt be loaded again unless you restart the app. If your images or URLs expire quickly, consider using the TTL variable within the st.cache_data function.\n\nIf you need to change the icons based on user interactions, you‚Äôll need to parameterize the function. In that case, caching will refresh when the data changes.\n\nTo load the details of the city, use st.cache_data:\n\n@st.cache_data\ndef retrieve_worst_cities():\n    return df.sort(f.col('RANK').asc()).to_pandas()\n\n\nThis function will be used to retrieve the coordinates for each city which will be plotted on the map. ¬†In addition, it will retrieve information about each city which will feature in the tooltip.\n\nNext, create your tabs:\n\ntab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(\n    [\n        \"7 worst cities\",\n        \"Main Towns in England\",\n        \"Incidents within Care Boards\",\n        \"City Details\",\n        \"Details within X miles radius of city\",\n        \"Incidents Within Fire Services\",\n    ]\n)\n\n\nCreate a title for Tab 1 using st.markdown because it offers more options compared to st.title, while still being easy to use.\n\nUse st.sidebar to create the logo stored in Snowflake. Utilize the components.html function in Streamlit to render the image from Snowflake:\n\nicon = images_icons()\ncomponents.html(f'''\n<table><tr><td bgcolor=\"white\", width = 200>\n<img src=\"{icon.loc[icon['RELATIVE_PATH'] == 'icon.jpeg'].URL.iloc[0]}\", \nwidth=100>\n</td><td bgcolor=\"white\" width = 300><p style=\"font-family:Verdana; \ncolor:#000000; font-size: 18px;\">Geospatial with Snowflake</td></tr></table>\n''')\n\n\nThe app lets the user select a city, which is then highlighted and used as a filter in another tab. Note that the data is stored in a pandas DataFrame, which was originally loaded through a Snowpark DataFrame:\n\nselected = st.radio(\"CHOOSE YOUR CITY:\", retrieve_worst_cities().CITY)\n\n\nOnce selected, the city will change the color of the selected marker to red.\n\nselected_row = retrieve_worst_cities()[retrieve_worst_cities()[\"CITY\"] == selected]\n\n\nTo create a Folium map with a marker for every location in the dataframes, put the following code inside a 'for' loop‚Äîit‚Äôll iterate seven times for all locations and one time for the selected location:\n\n#draw a map which centres to the coordinates of the selected city.\n\nm = folium.Map(\n    location=[selected_array2.iloc[0].LATITUDE, selected_array2.iloc[0].LONGITUDE],\n    zoom_start=8,\n    tiles=\"openstreetmap\",\n)\n\ntrail_coordinates = df2.sort('RANK').select('LATITUDE','LONGITUDE').to_pandas().to_numpy()\n\n#add information to each point which includes tool tips.  This includes\n#the images as well as the other data elements.\n\ntrail_coordinates = session.table(\"UK_Worst_Cities_To_Drive\").sort('RANK').select('LATITUDE','LONGITUDE').to_pandas().to_numpy()\n\n#add information to each point which includes tool tips\n\nfor A in range (0,7):\n    \n    html = f'''\n    <body style=\"background-color:#F0F0F0;\">\n    <p style=\"font-family:verdana\">\n    <b> WORST DRIVING CITY INSIGHTS\n    <BR><BR>\n    <b>Rank: </b>{retrieve_worst_cities().RANK.iloc[A]}<BR><BR>\n    <img src=\"{images_journeys().iloc[A].URL}\", width=100>\n    <br><br><b>City:</b>\n    {retrieve_worst_cities().CITY.iloc[A]}<BR><BR>\n\n    <hr>\n        \n    <p style=\"font-family:verdana\">\n        \n    <b>STATS</b>\n    <BR><BR>\n    Crashes:  {retrieve_worst_cities()['CRASHES'].iloc[A]}\n    <BR>\n    Congestion Level: {retrieve_worst_cities()['Congestion Level %'].iloc[A]}\n    <BR>\n    Cars Per Parking Space: {retrieve_worst_cities()['Cars Per Parking Space'].iloc[A]}\n    <BR>\n    EV Charging Points: {retrieve_worst_cities()['EV Charging Points'].iloc[A]}\n    <BR>\n    Air Quality Index Score: {retrieve_worst_cities()['Air Quality Index Score'].iloc[A]}\n    <BR>\n    Bus Routes: {retrieve_worst_cities()['Bus Routes'].iloc[A]}\n    <BR>\n    Overall Score: {retrieve_worst_cities()['Overall Score'].iloc[A]}\n    <BR>\n    <hr>\n    <p style=\"font-family:verdana\">\n    <b>Worst Junction: </b>{retrieve_worst_cities()['Bad Junction in City'].iloc[A]} \n    <BR><BR>\n    <b>Source:</b><a href =\"{retrieve_worst_cities().SOURCE.iloc[A]}\" target=\"popup\"> {retrieve_worst_cities().SOURCE.iloc[A]}</a></p>\n    <hr>\n    <p style=\"font-family:verdana\">\n    <br>\n    Info Gathered from Accident Data\n    <br>\n    <BR>\n    According to the data from the Department of Data, since the year 2000 \n    there have been <font style=\"color:red\"> {retrieve_worst_cities()['ACCIDENTS'].iloc[A]} </font> accidents.  \n    Of which, <font style=\"color:red\"> {retrieve_worst_cities()['VEHICLES'].iloc[A]} </font> vehicles and <font style=\"color:red\"> {retrieve_worst_cities()['CASUALTIES'].iloc[A]} </font> \n    casualties were involved.\n\n        \n    '''\n\n    html2 = f'''\n    <body style=\"background-color:#F0F0F0;\">\n    <p style=\"font-family:verdana\">\n    <b> WORST DRIVING CITY INSIGHTS\n    <BR><BR>\n    <b>Rank: </b>{selected_row.iloc[0].RANK}<BR><BR>\n    <img src=\"{images_journeys().iloc[selected_row.iloc[0].ID-1].URL}\", width=100>\n    <br><br><b>City:</b>\n    {selected_row.iloc[0].CITY}<BR><BR>\n\n    <hr>\n        \n    <p style=\"font-family:verdana\">\n        \n    <b>STATS</b>\n    <BR><BR>\n    Crashes:  {selected_row.iloc[0].CRASHES}\n    <BR>\n    Congestion Level: {selected_row.iloc[0]['Congestion Level %']}\n    <BR>\n    Cars Per Parking Space: {selected_row.iloc[0]['Cars Per Parking Space']}\n    <BR>\n    EV Charging Points: {selected_row.iloc[0]['EV Charging Points']}\n    <BR>\n    Air Quality Index Score: {selected_row.iloc[0]['Air Quality Index Score']}\n    <BR>\n    Bus Routes: {selected_row.iloc[0]['Bus Routes']}\n    <BR>\n    Overall Score: {selected_row.iloc[0]['Overall Score']}\n    <BR>\n    <hr>\n    <p style=\"font-family:verdana\">\n    <b>Worst Junction: </b>{selected_row['Bad Junction in City'].iloc[0]} \n    <BR><BR>\n    <b>Source:</b><a href =\"{selected_row.SOURCE.iloc[0]}\" target=\"popup\"> {selected_row.SOURCE.iloc[0]}</a></p>\n    <hr>\n    <p style=\"font-family:verdana\">\n    <br>\n    Info Gathered from Accident Data\n    <br>\n    <BR>\n    According to the data from the Department of Data, since the year 2000 \n    there have been <font style=\"color:red\"> {selected_row.ACCIDENTS.iloc[0]} </font> accidents.  \n    Of which, <font style=\"color:red\"> {selected_row.VEHICLES.iloc[0]} </font> vehicles and <font style=\"color:red\"> {selected_row.CASUALTIES.iloc[0]} </font> \n    casualties were involved.\n\n        \n    '''\n\t\t# I then create iframes for each tool tips.  This contains the html.\n\t\t\n\t\tiframe = folium.IFrame(html,width=700,height=400)\n\t\tiframe2 = folium.IFrame(html2,width=700,height=400)\n\t\t#I create the tooltip itself (folium calls these popup's)\n\t\tpopup = folium.Popup(iframe,max_width=700)\n\t\tpopup2 = folium.Popup(iframe2,max_width=700)\n\t\t#finally i apply the popup to the icon.\n\t\tfolium.Marker(\n\t\t    retrieve_worst_cities()[['LATITUDE', 'LONGITUDE']].iloc[A].to_numpy(),\n\t\t    popup=popup,\n\t\t    icon=folium.Icon(color='blue', prefix='fa', icon='car'),\n\t\t    icon_size=(40, 40),\n\t\t).add_to(m)\n\t\tfolium.Marker(\n\t\t    selected_row[['LATITUDE', 'LONGITUDE']].iloc[0],\n\t\t    popup=popup2,\n\t\t    icon=folium.Icon(color='red', prefix='fa', icon=f'{selected_row.RANK.iloc[0]}'),\n\t\t    icon_size=(40, 40),\n\t\t).add_to(m)\n\n\nFinally, position the map on the page.\n\nst_data = folium_static(m, width=1200, height=800)\n\nüöô\nNOTE: folium_static is perhaps a better option than folium in this use case, as it prevents the map from reloading at every interaction, improving the user experience.\nWorking with the accident data\n\nLeverage Azure Blob storage to store the Accident, Vehicle, and Casualty data. Create a dynamic external stage using a SAS key:\n\ncreate or replace stage VEHICLE_DATA_AZURE\nurl='azure://MY_BLOB_STORAGE_ACCOUNT.blob.core.windows.net/vehincidents'\n credentials=(azure_sas_token='MY TOKEN')\n  directory=(enable = TRUE);\n\n\nThen, use the VS Code add-in to bring this data into tables:\n\nGeospatial data engineering using Snowpark\n\nUse Snowpark for data engineering via your VS Code notebook, opting to use Snowpark DataFrames instead of an SQL file.\n\nTo begin, import a GeoJSON file of towns and cities in England, which provides the shape of each major town and city. Use this process to create the file format:\n\nsql = [f'''create or replace file format JSON\n    type = JSON\n    STRIP_OUTER_ARRAY = TRUE;\n''',\nf'''PUT file://towns_and_cities.geojson @DATA_STAGE\n    auto_compress = false\n    overwrite = true;\n'''\n]\n\nfor sql in sql:\n    session.sql(sql).collect()\n\n\nNext, transform the geometry data held within the stage into a Snowpark DataFrame:\n\ngeometry_data = session.sql('''select $1::Variant V \nFROM @DATA_STAGE/towns_and_cities.geojson (FILE_FORMAT =>'JSON') ''')\n\n\nFinally, enter the raw data into a table (you can view it in the Snowflake add-in):\n\ngeometry_data.write.mode(\"overwrite\").save_as_table(\"GEOMETRY_DATA\")\n\nWorking with polygon data\n\nTo work with polygon data, use:\n\nSnowSQL to upload your GeoJSON files to a stage\nThe Snowflake function ST_GEOGRAPHY to convert the polygons into a format that Tableau can recognize\n\nUse Snowpark to see what the shape of the imported data looks like:\n\nüöô\nNOTE: All the features are inside one row. I‚Äôm selecting one of the records nested inside the row.\n\nTo make the data more readable, use Lateral Flatten to \"explode\" the GeoJSON into multiple rows. Return to pandas to view the data in a more readable format:\n\nInstead of using SQL syntax, you can use Snowpark's DataFrame syntax:\n\ntable1 = session.table('GEOMETRY_DATA')\n\nflatten = table1.join_table_function(\"flatten\",col(\"V\"),lit(\"features\"))\n\ntowns_cities = flatten.select(\\\\\n    col('VALUE')[\"properties\"][\"TCITY15CD\"].cast(StringType()).alias(\"TOWN_CODE\")\\\\\n    ,( col('VALUE')[\"properties\"][\"TCITY15NM\"]).cast(StringType()).alias(\"TOWN_NAME\")\\\\\n    ,call_udf(\"TO_GEOGRAPHY\",col('VALUE')[\"geometry\"]).alias(\"GEOMETRY\")\\\\\n    \n    )\n\n\nWhen importing the geometries, notice that the precision is very high:\n\nUsing polygons with very high precision, which is often unnecessary, can negatively impact performance. To address this, use the Python Shapely library available within the Snowflake service without requiring installation. With this library, create a custom function to reduce the precision of polygons.\n\nOnce deployed, this function will be stored and processed in Snowflake like any other function:\n\nsql = '''\n\nCREATE OR REPLACE FUNCTION py_reduceprecision(geo geography, n integer)\nreturns geography\nlanguage python\nruntime_version = 3.8\npackages = ('shapely')\nhandler = 'udf'\nAS $$\nfrom shapely.geometry import shape, mapping\nfrom shapely import wkt\ndef udf(geo, n):\n    if n < 0:\n        raise ValueError('Number of digits must be positive')\n    else:\n        g1 = shape(geo)\n        updated_shape = wkt.loads(wkt.dumps(g1, rounding_precision=n))\n        return mapping(updated_shape)\n$$\n\n'''\nsession.sql(sql).collect()\n\ntowns_cities_reduced = towns_cities.with_column(\n    \"GEOMETRY\", call_udf(\"py_reduceprecision\", (col(\"GEOMETRY\"), 6))\n)\n\n\nFinally, save the geometry dataframe into a Snowflake table:\n\ntowns_cities.write.mode(\"overwrite\").save_as_table(\"TOWNS_CITIES\")\n\n\nTo associate incident data with polygons, you'll need a spatial join because the accident data doesn't include town or city names. Spatial joins can be computationally intensive, particularly when working with large datasets. But in this case, running a spatial join view with 17 million data points and 35,000 polygons took only 41 seconds on an X-Small virtual warehouse:\n\nThis geospatial function is natively available for instant querying within Snowflake. You don't have to use geospatial functions in Snowpark DataFrames‚Äîuse them within SQL just like any other function:\n\nCREATE OR REPLACE VIEW LSOA_POP AS\n\nselect C.LSOA21CD, c.LSOA21NM, A.* from\n(SELECT * FROM\n(select * from population_health_table) limit 17000000)a\n\ninner join postcodes b on a.postcode = b.postcode\ninner join\n\nGEOGRAPHIC_BOUNDARIES.UK_BOUNDARIES.LSOA_BOUNDARIES c on\n\nST_WITHIN(ST_MAKEPOINT( B.LONGITUDE,B.LATITUDE), to_geography( GEOMETRY));\n\n\nIf you have billions of data points and various polygons to join several datasets, spatial indexes could be a more efficient choice.\n\nCarto has created an Analytical Toolbox, which includes the H3 spatial indexing system. I'll cover this system in more detail in another post. For now, use the standard spatial join feature to assign a town to every vehicle accident that has corresponding latitude and longitude:\n\nACCIDENTS_KNOWN_LOCATION.with_column('POINT',call_udf('ST_makepoint',col('LONGITUDE'),col('LATITUDE')))\\\\\n    .write.mode(\"overwrite\").save_as_table(\"ACCIDENTS_POINTS\")#write a table with accident points\nACCIDENTS_IN_TOWNS_CITIES = session.table('ACCIDENTS_POINTS')\\\\\n    .join(TOWNS_CITIES,call_udf('ST_WITHIN',col('POINT'),col('GEOMETRY'))==True)\\\\\n        .select('ACCIDENT_INDEX','TOWN_CODE','TOWN_NAME','GEOMETRY')\n\n\nAfter completion, write the table to Snowflake and use Tableau to render the polygons along with the corresponding accident data:\n\nTableau's native spatial support works seamlessly with Snowflake in \"live\" query mode.\n\nEmbedding Tableau reports in Streamlit\n\nThe components.html function is a valuable tool for embedding Tableau reports into Streamlit. It also enables you to link interactions from Streamlit inputs to Tableau.\n\n#an example of the embed code behind one of the tableau \n#reports within my streamlit app\n\ncomponents.html(f'''<script type='module' \n        src='<https://eu-west-1a.online.tableau.com/javascripts/>\n\t\t\t\tapi/tableau.embedding.3.latest.min.js'>\n        </script><tableau-viz id='tableau-viz' \n\t\t\t\tsrc='<https://eu-west-1a.online.tableau.com/t/eccyclema/views/>\n\t\t\t\tcarsFIRE_16844174853330/FIRE_SERVICEintoH3' \n        width='1300' height='1300' hide-tabs toolbar='hidden' token = {token}>\n        \n         ###<viz-filter field=\"FRA22NM\" value=\"{FIRE_SERVICE}\"> </viz-filter>\n        \n        <viz-parameter name=\"MEASURE\" value={parameterno}></viz-parameter>\n        \n        </tableau-viz>''', height=1300, width=1300)\n\n\nThe dropdown at the top uses the st.selectbox widget to display options. This widget passes the selected parameter to view the results. In the given code, the dropdown is referred to as the variable FIRE_SERVICE:\n\nIn addition to passing parameters, you can hide Tableau toolbars and tabs to seamlessly integrate the Tableau dashboard into your app. The select box can also control other items not intended for Tableau. For example, my code filters a Folium chart on another tab using the same dropdown.\n\nOne potential issue is that clicking on an embedded Tableau tab requires signing in to Tableau, which can be visually unappealing. To avoid this, you can use Tableau's connected app capabilities, explained here. This allows the token to be passed from Streamlit to Tableau without requiring the user to log in.\n\nStreamlit stores all secrets/credentials in the secrets.toml file. In the Streamlit Community Cloud, the secrets are managed by the UI within the settings area:\n\nTo make this work, encode the token configured in Tableau Online to your Streamlit app (in my code, I applied the variable token to the components.html function):\n\n#token generator for tableau automatic login\n\ntoken = encode(\n\t{\n\t\t\"iss\": st.secrets[\"iss\"],\n\t\t\"exp\": datetime.utcnow() + timedelta(minutes=5),\n\t\t\"jti\": str(uuid4()),\n\t\t\"aud\": \"tableau\",\n\t\t\"sub\": st.secrets[\"sub\"],\n\t\t\"scp\": [\"tableau:views:embed\", \"tableau:metrics:embed\"],\n\t\t\"Region\": \"East\"\n\t},\n\t\tst.secrets[\"tableau_secret\"],\n\t\talgorithm = \"HS256\",\n\t\theaders = {\n\t\t'kid': st.secrets[\"kid\"],\n\t\t'iss': st.secrets[\"iss\"]\n        }\n  )\n\nUsing Carto's Analytic Toolbox to work with geospatial indexes and clustering\n\nTake advantage of H3 functions within Snowflake by using Carto. Carto extends the capabilities of Snowflake without requiring any installation. Simply obtain the share from the marketplace, and the additional functions will be instantly available. This is one of the advantages of Snowflake's sharing capabilities:\n\nOnce deployed (which takes only a second), the functions will appear as a new database in Snowflake. You can also verify this in VS Code:\n\nH3 is a hexagon hierarchy covering the entire planet, with 14 different resolutions (read more here). Each hexagon, regardless of size, has a unique index. A hexagon of the highest resolution could fit inside a child's sandbox:\n\nConverting spatial data into consistently sized hexagons is an effective way to aggregate points into hexagonal samples. Carto provides functions for creating an H3 index from latitude and longitude, which can group hexagons into points. There is also a polyfill function, which can fill a polygon with hexagons. The unique H3 index is always placed consistently, so when joining points to a polygon, you'd join polygons instead of performing a spatial join. This speeds up processing time and is cost-effective.\n\nIn the \"City Details\" tab, you can change the size of the H3 resolution within Streamlit. This calls the Carto function and processes it against the accident data:\n\nWrap this code in a function that resizes hexagons for all data within a chosen resolution, year, and town when executed:\n\nimport snowflake.snowpark.functions as f\n@st.cache_data(max_entries=5)\ndef CARTO(r,s_year, s_town_code):\n\n    ACCIDENT_TOWN_CITY = session.table('ACCIDENTS_WITH_TOWN_CITY_DETAILS').filter(f.col('TOWN_CODE')==s_town_code)\n\n    ACCIDENT_DETAILS = ACCIDENTS\\\\\n      .filter(f.col('ACCIDENT_YEAR')==s_year)\\\\\n      .join(ACCIDENT_TOWN_CITY,'ACCIDENT_INDEX')\n\n    df = ACCIDENT_DETAILS\\\\\n       .select('LATITUDE','LONGITUDE','ACCIDENT_INDEX','ACCIDENT_YEAR')\\\\\n       .with_column('H3',\\\\\n\t\t\t\tf.call_udf('ANALYTICS_TOOLBOX.CARTO.H3_FROMLONGLAT',\\\\\n\t\t\t\tf.col('LONGITUDE'),f.col('LATITUDE'),r))\\\\\n       .with_column('H3_BOUNDARY',\\\\\n\t\t\t\tf.call_udf('ANALYTICS_TOOLBOX.CARTO.H3_BOUNDARY',f.col('H3')))\\\\\n      .with_column('WKT',f.call_udf('ST_ASWKT',f.col('H3_BOUNDARY')))\\\\\n                \n    accidents = df.group_by('H3').agg(f.any_value('WKT')\\\\\n\t.alias('WKT'),f.count('ACCIDENT_INDEX').alias('ACCIDENTS'))\n    \n    casualties_tot = df.join(session.table('CASUALTIES')\\\\\n\t\t.select('AGE_OF_CASUALTY','ACCIDENT_INDEX',\\\\\n\t\t'CASUALTY_SEVERITY'),'ACCIDENT_INDEX')\n\n    casualties = df\\\\\n    .join(session.table('CASUALTIES')\\\\\n\t\t\t\t\t\t.select('AGE_OF_CASUALTY','ACCIDENT_INDEX'\\\\\n\t\t\t\t\t\t,'CASUALTY_SEVERITY'),'ACCIDENT_INDEX')\\\\\n            .with_column('FATAL',f.when(f.col('CASUALTY_SEVERITY')\\\\\n\t\t\t\t\t\t==1,f.lit(1)))\\\\\n                            .with_column('SEVERE',\\\\\n\t\t\t\t\t\tf.when(f.col('CASUALTY_SEVERITY')==2,f.lit(1)))\\\\\n            .with_column('MINOR',f\\\\\n\t\t\t\t\t\t.when(f.col('CASUALTY_SEVERITY')==3,f.lit(1)))\\\\\n            .group_by('H3').agg(f.any_value('WKT').alias('WKT')\\\\\n            ,f.count('ACCIDENT_INDEX').alias('CASUALTIES')\n           ,f.avg('AGE_OF_CASUALTY').alias('AVERAGE_AGE_OF_CASUALTY')\\\\\n           ,f.sum('FATAL').alias('FATAL')\\\\\n           ,f.sum('SEVERE').alias('SEVERE')\\\\\n           ,f.sum('MINOR').alias('MINOR'))\n\n\nThe boundaries function converts the index into a geometry. These hexagons can be visualized in Tableau just like standard polygons.\n\nIn addition to a comprehensive list of standard Snowflake functions, Carto's toolkit provides many other functions for free, including K-means clustering (read more here):\n\nUsing Streamlit to run a predictive model\n\nMake a Streamlit page for predicting the number of casualties that may occur within a certain number of hexagons away from a fire station. Use H3_KRING_DISTANCES to determine the distance based on rings of hexagons around the point.\n\nBefore creating the app, train a random forest regressor model that relies on a Snowpark-optimized data virtual warehouse within Snowflake. After training, deploy the model and create a function to use in Streamlit. The dataset used for training aggregates using H3 Indexes at resolution 7‚Äîabout every 5 kilometers (read more here).\n\nNote the following SQL query (I'm parameterizing it using the select boxes and sliders):\n\nsql = f'''SELECT POLICE_FORCE \"Police Force\", \n            H3INDEX,\n            any_value(st_aswkt(ANALYTICS_TOOLBOX.CARTO.H3_BOUNDARY(H3INDEX))) WKT,\n            VEHICLE_MANOEUVRE \"Vehicle Manoeuvre\", \n            ROAD_TYPE \"Road Type\", \n            VEHICLE_TYPE \"Vehicle Type\", \n            WEATHER_CONDITIONS \"Weather Conditions\",\n            DAY_OF_WEEK \"Day of Week\", \n            AVG(ENGINE_CAPACITY_CC)::INT \"Engine Capacity CC\",\n            AVG(ADJUSTED_ENGINE_SIZE)::INT \"Adjusted Engine Capacity CC\",\n            AVG(AGE_OF_DRIVER)::INT \"Age of Driver\",\n            AVG(ADJUSTED_AGE_OF_DRIVER)::INT \"Adjusted age of Driver\",\n            AVG(AGE_OF_VEHICLE)::INT \"Age of Vehicle\", \n            AVG(ADJUSTED_AGE_OF_VEHICLE)::INT \"Adjusted age of Vehicle\",\n            AVG(PREDICTED_FATALITY)::FLOAT PREDICTION \n    \n            FROM\n\n            (\n            select VEHICLE_ACCIDENT_DATA.RAW.{choose_model}(\n            js_hextoint(A.H3INDEX)\n        \n            ,VEHICLE_MANOEUVRE\n            ,DAY_OF_WEEK\n            ,ROAD_TYPE\n            ,SPEED_LIMIT\n            ,WEATHER_CONDITIONS\n            ,VEHICLE_TYPE\n            ,AGE_OF_DRIVER * {age_of_driver}\n            ,AGE_OF_VEHICLE * {age_of_vehicle}\n            ,ENGINE_CAPACITY_CC * {engine_size}\n\n            )PREDICTED_FATALITY,\n            A.H3INDEX,\n            B.VALUE AS POLICE_FORCE, \n            C.VALUE AS VEHICLE_MANOEUVRE,\n            D.VALUE AS ROAD_TYPE,\n            E.VALUE AS VEHICLE_TYPE,\n            F.VALUE AS WEATHER_CONDITIONS,\n            G.VALUE AS DAY_OF_WEEK,\n            AGE_OF_DRIVER,\n            AGE_OF_VEHICLE,\n            ENGINE_CAPACITY_CC,\n            AGE_OF_DRIVER * ({age_of_driver}) ADJUSTED_AGE_OF_DRIVER,\n            AGE_OF_VEHICLE * ({age_of_vehicle}) ADJUSTED_AGE_OF_VEHICLE,\n            ENGINE_CAPACITY_CC * ({engine_size}) ADJUSTED_ENGINE_SIZE\n\n            from \n\n            (SELECT * FROM (SELECT * FROM accidents_with_perc_fatal \n\t\t\t\t\t\tWHERE YEAR = 2021))A \n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = \n\t\t\t\t\t\t'police_force')B\n            ON A.POLICE_FORCE = B.CODE\n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = \n\t\t\t\t\t\t'vehicle_manoeuvre')C\n            ON A.VEHICLE_MANOEUVRE = C.CODE\n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = \n\t\t\t\t\t\t'road_type')D\n            ON A.ROAD_TYPE = D.CODE\n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD = \n\t\t\t\t\t\t'vehicle_type')E\n            ON A.VEHICLE_TYPE = E.CODE\n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD \n\t\t\t\t\t\t= 'weather_conditions')F\n            ON A.WEATHER_CONDITIONS = F.CODE\n\n            INNER JOIN (SELECT CODE,VALUE FROM LOOKUPS WHERE FIELD \n\t\t\t\t\t\t= 'day_of_week')G\n            ON A.DAY_OF_WEEK = G.CODE\n\n            INNER JOIN (\n                SELECT VALUE:index H3INDEX, VALUE:distance DISTANCE FROM \n\n                (SELECT FIRE_STATION,  \n\t\t\t\t\t\t\t\tANALYTICS_TOOLBOX.CARTO.H3_KRING_DISTANCES\n\t\t\t\t\t\t\t\t(ANALYTICS_TOOLBOX.CARTO.H3_FROMGEOGPOINT(POINT, 7), \n\t\t\t\t\t\t\t\t{choose_size})HEXAGONS \n\n                FROM  (SELECT * FROM FIRE_STATIONS WHERE FIRE_STATION =\n\t\t\t\t\t\t\t\t '{select_fire_station}')),lateral flatten (input =>HEXAGONS)) H\n\n                ON A.H3INDEX = H.H3INDEX\n            )\n\n            GROUP BY POLICE_FORCE, VEHICLE_MANOEUVRE, ROAD_TYPE, VEHICLE_TYPE, WEATHER_CONDITIONS, DAY_OF_WEEK, H3INDEX\n\n            '''\n\n\nTo prevent running the model every time a slider is changed, add a button inside a Streamlit form. This will allow you to execute the SQL code only when you're ready. In the future, you may convert this code section into a stored procedure. This approach creates a table with the results every time the model is run, which isn't ideal from a security standpoint. Converting the code into a stored procedure would be better practice and reduce the amount of Streamlit code required.\n\nWrapping up\n\nCreating this app was a lot of fun. While it may not be a production-ready application, I wanted to share my vision of how great technologies can work together to create a geospatial solution.\n\nI hope you found this post interesting. If you have any questions, please drop them in the comments below, message me on LinkedIn, or email me. Finally, click here to view the example app.\n\nHappy coding! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "kaarthik Andavar - Streamlit",
    "url": "https://blog.streamlit.io/author/kaarthik/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by kaarthik Andavar\n1 post\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "snowChat: Leveraging OpenAI's GPT for SQL queries",
    "url": "https://blog.streamlit.io/snowchat-leveraging-openais-gpt-for-sql-queries/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nBy kaarthik Andavar\nPosted in Snowflake powered ‚ùÑÔ∏è, July 25 2023\nWhat is snowChat?\nHow to embed Snowflake schema into a vector database\nHow to create a conversational chain using LangChain\nHow to connect the chain response to Snowflake\nHow to design a chat-like interface using Streamlit\nHow to deploy to the Streamlit Community Cloud\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, fellow tech enthusiasts! üëã\n\nI'm Kaarthik, an analytics engineer with a passion for building innovative AI applications. My expertise lies at the intersection of AI and data, where I explore and refine the power of AI to redefine data-driven solutions.\n\nDo you struggle with complex SQL queries? Are you lost in a sea of tables while trying to locate a single piece of data? Fear not, for I have created snowChat to solve these problems!\n\nIn this post, I'll show you:\n\nHow to embed a Snowflake schema into a vector database\nHow to create a conversational chain using LangChain\nHow to connect the chain response to Snowflake\nHow to design a chat-like interface using Streamlit\nHow to deploy your solution to the Streamlit Community Cloud\n\n\n‚ùÑÔ∏è\nReady to leap right in? Check out the app and the full code.\n\nBut first, let's talk about‚Ä¶\n\nWhat is snowChat?\n\nsnowChat is a powerful and user-friendly application that enables users to interact with their Snowflake DataBase using natural language queries.\n\nsnowChat leverages OpenAI's GPT model to convert natural language into SQL queries, making it ideal for users who may not have a firm grasp of SQL syntax. And it has a ¬†transformative impact on data interaction by expediting and streamlining data-driven decision-making.\n\nLet's take a look at the tech stack on which snowChat is built:\n\nStreamlit: The UI magic\nSnowflake: The database powerhouse\nGPT-3.5 and LangChain: The language model maestros\nSupabase: The vector database virtuoso\n\nHere's a glance at snowChat's architecture:\n\nAll set? Let's get cracking!\n\nHow to embed Snowflake schema into a vector database\n\nTo start, follow these steps:\n\nClone the GitHub repo\nRun pip install -r requirements.txt to install all the required packages\nGet the Data Definition Language (DDL) for all tables from snowflake.account_usage.tables:\n\n\nOPENAI_API_KEY=\n\n#snowflake\nACCOUNT=\nUSER_NAME=\nPASSWORD=\nROLE=\nDATABASE=\nSCHEMA=\nWAREHOUSE=\n\nSUPABASE_URL=\nSUPABASE_SERVICE_KEY=\n\n\n\nUse ChatGPT to convert the DDL to markdown format.\nStore the schema files for each table in the docs/ folder.\nCreate an account with Supabase, set up a free database, and configure environment variables for the .streamlit folder in secrets.toml (remember to include your Snowflake credentials).\n\nYour final secrets.toml should look like this:\n\n[streamlit]\nSUPABASE_URL = \"<https://yourapp.supabase.co>\"\nSUPABASE_KEY = \"yourkey\"\n\n[snowflake]\nSNOWFLAKE_ACCOUNT = \"youraccount\"\nSNOWFLAKE_USER = \"youruser\"\nSNOWFLAKE_PASSWORD = \"yourpassword\"\nSNOWFLAKE_DATABASE = \"yourdatabase\"\nSNOWFLAKE_SCHEMA = \"yourschema\"\nSNOWFLAKE_WAREHOUSE = \"yourwarehouse\"\n\n\n\n\nRun the Supabase scripts found under supabase/scripts.sql in the Supabase SQL editor to activate the pgvector extension, create tables and set up a function.\nRun python ingest.py to convert your documents into vectors and store them in the Supabase table named 'documents.'\n\nThe 'documents' table in Supabase should look like this:\n\n| id | vector   | contents         |\n|----|----------|------------------|\n| 1  | [1,2,3]  | This is document |\n| 2  | [4,5,6]  | This is another  |\n| 3  | [7,8,9]  | This is a third   |\n\n\n\n\n\nEt voila, the left part of the architecture is done!\n\nHow to create a conversational chain using LangChain\n\nThe core of snowChat is the \"chain\" function, which manages OpenAI's GPT model, the embedding model, the vector store, and the prompt templates. This functionality is encapsulated in LangChain.\n\nThe chain function takes user input, applies a prompt template to format it, and then passes the formatted response to an LLM with context retrieved from the vector store:\n\ndef get_chain(vectorstore):\n    \"\"\"\n    Get a chain for chatting with a vector database.\n    \"\"\"\n    q_llm = OpenAI(\n        temperature=0,\n        openai_api_key=st.secrets[\"OPENAI_API_KEY\"],\n        model_name=\"gpt-3.5-turbo-16k\",\n    )\n\n    llm = ChatOpenAI(\n\t\t\t\ttemperature=0,\n        openai_api_key=st.secrets[\"OPENAI_API_KEY\"],\n        model_name=\"gpt-3.5-turbo\",\n    )\n\n    question_generator = LLMChain(llm=q_llm, prompt=condense_question_prompt)\n\n    doc_chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n    chain = ConversationalRetrievalChain(\n        retriever=vectorstore.as_retriever(),\n        combine_docs_chain=doc_chain,\n        question_generator=question_generator,\n    )\n    return chain\n\n\nYour get_chain function is now ready to use. Simply run chain = get_chain(SupabaseVectorStore) and start your fun with chain({\"question\": \"Did you understand the chain?\", \"chat_history\": chat_history}).\n\nHow to connect the chain response to Snowflake\n\nAfter creating the chain, use this code to destructure the response from the answer key:\n\nresult = chain({\"question\": query, \"chat_history\": chat_history})\n\nresponse = result['answer']\n\n\n\nThe response variable contains both SQL and an explanation. A helper function called get_sql extracts only the SQL code from the response. Once the SQL is obtained, it's sent to Snowflake (Snowpark) to fetch the data and create a DataFrame object.\n\nThere are two possible outcomes:\n\nThe model generates the SQL correctly: Display the output directly in Streamlit.\nThe model generates the SQL incorrectly: Send it back to the GPT model along with the error message and the SQL it generated (self-healing). This way, the model can correct its mistake and return the corrected response.\n\nWith this setup, the model is ready to work in full.\n\nHow to design a chat-like interface using Streamlit\n\nMoving on to the design phase, let's shape up the chat interface.\n\nThe input will be designed using the st.chat_input() container from Streamlit's chat element:\n\nif prompt := st.chat_input():\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n\n\nA helper function called message_func takes care of the styling for each message, including avatars on the side.\n\nFollowing the steps outlined, you should have a fully functional chat app ready!\n\nif prompt := st.chat_input():\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\nHow to deploy to the Streamlit Community Cloud\n\nLet's bring snowChat to the world! To get started, head to Community Cloud, click \"Create a new app,\" select your GitHub repo, and deploy. Don't forget to add the necessary secrets to the settings.\n\nCongratulations! Now you can chat with your Snowflake data.\n\nWrapping up\n\nThat's a wrap, folks! In this post, we've discussed building snowChat, an intuitive tool that leverages the power of OpenAI's GPT to convert natural language into SQL queries. By breaking down the intimidating walls of SQL syntax, snowChat fosters swift and effective data-driven decisions, even for those not well-versed in SQL.\n\nI hope snowChat will increase your productivity and impress you with its cool functionalities (see what we did there? üòú). If you have any questions or doubts, please post them in the comments below or contact me on GitHub, LinkedIn, or Twitter.\n\nHappy Streamlit-ing, and keep it cool! ‚ùÑÔ∏èüéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nCollecting user feedback on ML in Streamlit\n\nImprove user engagement and model quality with the new Trubrics feedback component\n\nAdvocate Posts\nby\nJeff Kayne and¬†\n1\n¬†more,\nMay 4 2023\nChat with the Cat Generative Dialogue Processor (CatGDP)\n\nBuild your own catbot with a quirky persona!\n\nAdvocate Posts\nby\nTianyi Pan\n,\nMay 3 2023\nThe ultimate athlete management dashboard for biomechanics\n\nLearn how to measure jump impulse, max force, and asymmetry with Python and Streamlit\n\nAdvocate Posts\nby\nHansen Lu\n,\nApril 27 2023\nCreating a Time Zone Converter with Streamlit\n\n6 steps on how to build your own converter\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nApril 25 2023\nCreate an animated data story with ipyvizzu and Streamlit\n\nA tutorial on using ipyvizzu and ipyvizzu-story\n\nAdvocate Posts\nby\nPeter Vidos\n,\nApril 20 2023\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nAdvocate Posts\nby\nDmitry Kosarevsky\n,\nApril 18 2023\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nAdvocate Posts\nby\nKanak Mittal\n,\nApril 11 2023\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nLLMs\nby\nDave Lin\n,\nApril 6 2023\nBuilding an Instagram hashtag generation app with Streamlit\n\n5 simple steps on how to build it\n\nAdvocate Posts\nby\nWilliam Mattingly\n,\nMarch 29 2023\nHackathon 101: 5 simple tips for beginners\n\nPrepare to win your first hackathon!\n\nTutorials\nby\nChanin Nantasenamat\n,\nMarch 16 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Yuichiro Tachibana (Tsuchiya) - Streamlit",
    "url": "https://blog.streamlit.io/author/yuichiro/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Yuichiro Tachibana (Tsuchiya)\nSoftware developer\n2 posts\nFacebook\nAccessible color themes for Streamlit apps\n\nControl your app‚Äôs color scheme and visual accessibility\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nMay 5 2023\nDeveloping a streamlit-webrtc component for real-time video processing\n\nIntroducing the WebRTC component for real-time media streams\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nFebruary 12 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Accessible color themes for Streamlit apps",
    "url": "https://blog.streamlit.io/accessible-color-themes-for-streamlit-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAccessible color themes for Streamlit apps\n\nControl your app‚Äôs color scheme and visual accessibility\n\nBy Yuichiro Tachibana (Tsuchiya)\nPosted in Advocate Posts, May 5 2023\nFive key components of the color theme editor app\n1. Define preset colors\n2. If set, load the theme config\n3. Manage the edited color parameters in the session state\n4. Set colors programmatically\n5. Define a composed widget of a color picker and a slider\n6. Use the WCAG contrast table\n7. Show generated configs\n8. Apply the edited theme to the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nOne great feature of Streamlit is theming, so you can control your app's color scheme. But creating a new color scheme can be hard. And making it visually accessible can be even harder.\n\nSo I made a color theme editor app with four color parameters of the Streamlit themes, which general-purpose color editors/generators don't even consider: primaryColor, backgroundColor, secondaryBackgroundColor, and textColor. I also followed the color contrast guidelines from the Web Content Accessibility Guidelines (WCAG) 2.0 standard for the app's visual accessibility.\n\nIn this post, I'll show you how to build this app in eight steps:\n\nDefine preset colors\nIf set, load the theme config\nManage the edited color parameters in the session state\nSet colors programmatically\nDefine a composed widget of a color picker and a slider\nUse the WCAG contrast table\nShow generated configs\nApply the edited theme to the app\nüëâ\nYou can assess the app here and the source code here. This article is based on the revision 7213dbb.\nFive key components of the color theme editor app\n\nThe app consists of five components to help you create and test accessible color themes:\n\n\"Generate a random color scheme\" button. Click this button to update the color theme with randomly selected colors that maintain high contrast between foreground and background.\nColor pickers. Use these components to adjust colors and their luminance parameters. Use the luminance slider to adjust contrast while preserving the color's aesthetic.\nWCAG contrast ratio table. Display a 2x2 matrix of contrast ratios for the selected colors and the corresponding WCAG 2.0 level (AA or AAA) for each ratio to make your color theme accessible.\nConfig generator. Generate content for the .streamlit/config.toml config file and a shell command with color theme arguments. Just copy/paste it into your project to apply the edited theme.\n\"Apply theme on this page\" checkbox. Check this box to apply the color theme you are editing to the whole application, letting you preview the colors in a real Streamlit app and synchronize the changes.\n1. Define preset colors\n\nThe app has preset colors that I picked from the Streamlit source code below:\n\nBase theme\nLight theme\nDark theme\npreset_colors: list[tuple[str, ThemeColor]] = [\n    (\"Default light\", ThemeColor(\n            primaryColor=\"#ff4b4b\",\n            backgroundColor=\"#ffffff\",\n            secondaryBackgroundColor=\"#f0f2f6\",\n            textColor=\"#31333F\",\n        )),\n    (\"Default dark\", ThemeColor(\n            primaryColor=\"#ff4b4b\",\n            backgroundColor=\"#0e1117\",\n            secondaryBackgroundColor=\"#262730\",\n            textColor=\"#fafafa\",\n    ))\n]\n\n2. If set, load the theme config\n\nThis is a bit of a hack. You can access the global config object via st._config to get the theme config from it and to use it as a preset color:\n\n@st.cache_resource\ndef get_config_theme_color():\n    config_theme_primaryColor = st._config.get_option('theme.primaryColor')\n    config_theme_backgroundColor = st._config.get_option('theme.backgroundColor')\n    config_theme_secondaryBackgroundColor = st._config.get_option('theme.secondaryBackgroundColor')\n    config_theme_textColor = st._config.get_option('theme.textColor')\n    if config_theme_primaryColor and config_theme_backgroundColor and config_theme_secondaryBackgroundColor and config_theme_textColor:\n        return ThemeColor(\n            primaryColor=config_theme_primaryColor,\n            backgroundColor=config_theme_backgroundColor,\n            secondaryBackgroundColor=config_theme_secondaryBackgroundColor,\n            textColor=config_theme_textColor,\n        )\n\n    return None\n\ntheme_from_initial_config = util.get_config_theme_color()\nif theme_from_initial_config:\n    preset_colors.append((\"From the config\", theme_from_initial_config))\n\n3. Manage the edited color parameters in the session state\n\nStore the RGB color values in the session state using the same keys as the color picker widgets. This lets you programmatically set the color picker values. It's useful when updating them with new values from the random color generator. These values are synced with the color picker widgets, so they must be compatible with their #RRGGBB format.\n\nAside from managing RGB values, you also maintain the HSL values in the session state to control the slider widgets. Using the HSL format makes it easier to edit colors more intuitively. To synchronize these two formats, create a utility function named set_color and use it every time you update the color values:\n\ndef sync_rgb_to_hls(key: str):\n    # HLS states are necessary for the HLS sliders.\n    rgb = util.parse_hex(st.session_state[key])\n    hls = colorsys.rgb_to_hls(rgb[0], rgb[1], rgb[2])\n    st.session_state[f\"{key}H\"] = round(hls[0] * 360)\n    st.session_state[f\"{key}L\"] = round(hls[1] * 100)\n    st.session_state[f\"{key}S\"] = round(hls[2] * 100)\n\ndef set_color(key: str, color: str):\n    st.session_state[key] = color\n    sync_rgb_to_hls(key)\n\nif 'preset_color' not in st.session_state or 'backgroundColor' not in st.session_state or 'secondaryBackgroundColor' not in st.session_state or 'textColor' not in st.session_state:\n    set_color('primaryColor', default_color.primaryColor)\n    set_color('backgroundColor', default_color.backgroundColor)\n    set_color('secondaryBackgroundColor', default_color.secondaryBackgroundColor)\n    set_color('textColor', default_color.textColor)\n\n4. Set colors programmatically\n\nTo set the currently edited colors from the selectbox widget with the preset colors, use set_color():\n\ndef on_preset_color_selected():\n    _, color = preset_colors[st.session_state.preset_color]\n    set_color('primaryColor', color.primaryColor)\n    set_color('backgroundColor', color.backgroundColor)\n    set_color('secondaryBackgroundColor', color.secondaryBackgroundColor)\n    set_color('textColor', color.textColor)\n\nst.selectbox(\"Preset colors\", key=\"preset_color\", options=range(len(preset_colors)), format_func=lambda idx: preset_colors[idx][0], on_change=on_preset_color_selected)\n\n\nYou can also implement a random color generator button:\n\nif st.button(\"üé® Generate a random color scheme üé≤\"):\n    primary_color, text_color, basic_background, secondary_background = util.generate_color_scheme()\n    set_color('primaryColor', primary_color)\n    set_color('backgroundColor', basic_background)\n    set_color('secondaryBackgroundColor', secondary_background)\n    set_color('textColor', text_color)\n\n\nTo implement util.generate_color_scheme, refer to this code. It generates colors with the constraint that the colors' luminance (the \"L\" in the HSL format) parameters have enough difference.\n\n5. Define a composed widget of a color picker and a slider\n\nThis app provides a pair of a color picker and a slider to control the color's luminance parameter (to adjust the color contrast while maintaining its original appearance). There are four pairs of components. Each is defined by a function called color_picker to make it reusable.\n\nOne trick here is to set the on_change callback of each component to synchronize the RGB data and the HSL data in the session state:\n\ndef color_picker(label: str, key: str, default_color: str) -> None:\n    col1, col2 = st.columns([1, 3])\n    with col1:\n        color = st.color_picker(label, key=key, on_change=sync_rgb_to_hls, kwargs={\"key\": key})\n    with col2:\n        r,g,b = util.parse_hex(default_color)\n        h,l,s = colorsys.rgb_to_hls(r,g,b)\n        if f\"{key}H\" not in st.session_state:\n            st.session_state[f\"{key}H\"] = round(h * 360)\n\n        st.slider(f\"L for {label}\", key=f\"{key}L\", min_value=0, max_value=100, value=round(l * 100), format=\"%d%%\", label_visibility=\"collapsed\", on_change=sync_hls_to_rgb, kwargs={\"key\": key})\n\n        if f\"{key}S\" not in st.session_state:\n            st.session_state[f\"{key}S\"] = round(s * 100)\n\n    return color\n\nprimary_color = color_picker('Primary color', key=\"primaryColor\", default_color=default_color.primaryColor)\ntext_color = color_picker('Text color', key=\"textColor\", default_color=default_color.textColor)\nbackground_color = color_picker('Background color', key=\"backgroundColor\", default_color=default_color.backgroundColor)\nsecondary_background_color = color_picker('Secondary background color', key=\"secondaryBackgroundColor\", default_color=default_color.secondaryBackgroundColor)\n\n6. Use the WCAG contrast table\n\nThis table layout is created using stacked st.column(). The content of each cell is encapsulated within a reusable helper function, such as synced_color_picker or fragments.contrast_summary:\n\ncol1, col2, col3 = st.columns(3)\nwith col2:\n    synced_color_picker(\"Background color\", value=background_color, key=\"backgroundColor\")\nwith col3:\n    synced_color_picker(\"Secondary background color\", value=secondary_background_color, key=\"secondaryBackgroundColor\")\n\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    synced_color_picker(\"Primary color\", value=primary_color, key=\"primaryColor\")\nwith col2:\n    fragments.contrast_summary(\"Primary/Background\", primary_color, background_color)\nwith col3:\n    fragments.contrast_summary(\"Primary/Secondary background\", primary_color, secondary_background_color)\n\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    synced_color_picker(\"Text color\", value=text_color, key=\"textColor\")\nwith col2:\n    fragments.contrast_summary(\"Text/Background\", text_color, background_color)\nwith col3:\n    fragments.contrast_summary(\"Text/Secondary background\", text_color, secondary_background_color)\n\n\nsynced_color_picker() is just a color picker component, but its value is synchronized with the relevant color picker component that appeared above. Its value is also managed in the session state with the value argument and the on_change callback:\n\ndef synced_color_picker(label: str, value: str, key: str):\n    def on_change():\n        st.session_state[key] = st.session_state[key + \"2\"]\n        sync_rgb_to_hls(key)\n    st.color_picker(label, value=value, key=key + \"2\", on_change=on_change)\n\n\nfragments.contrast_summary() renders the WCAG contrast info. See this code for its implementation.\n\n7. Show generated configs\n\nThe app shows these ready-to-use code snippets and is done with st.code():\n\nst.subheader(\"Config file (`.streamlit/config.toml`)\")\nst.code(f\"\"\"\n[theme]\nprimaryColor=\"{primary_color}\"\nbackgroundColor=\"{background_color}\"\nsecondaryBackgroundColor=\"{secondary_background_color}\"\ntextColor=\"{text_color}\"\n\"\"\", language=\"toml\")\n\nst.subheader(\"Command line argument\")\nst.code(f\"\"\"\nstreamlit run app.py \\\\\\\\\n    --theme.primaryColor=\"{primary_color}\" \\\\\\\\\n    --theme.backgroundColor=\"{background_color}\" \\\\\\\\\n    --theme.secondaryBackgroundColor=\"{secondary_background_color}\" \\\\\\\\\n    --theme.textColor=\"{text_color}\"\n\"\"\")\n\n8. Apply the edited theme to the app\n\nFor reviewing purposes, this app can apply the currently edited theme to itself. You can do it with the st._config object, as shown below:\n\nif st.checkbox(\"Apply theme to this page\"):\n    st.info(\"Select 'Custom Theme' in the settings dialog to see the effect\")\n\n    def reconcile_theme_config():\n        keys = ['primaryColor', 'backgroundColor', 'secondaryBackgroundColor', 'textColor']\n        has_changed = False\n        for key in keys:\n            if st._config.get_option(f'theme.{key}') != st.session_state[key]:\n                st._config.set_option(f'theme.{key}', st.session_state[key])\n                has_changed = True\n        if has_changed:\n            st.experimental_rerun()\n\n    reconcile_theme_config()\n\n    fragments.sample_components(\"body\")\n    with st.sidebar:\n        fragments.sample_components(\"sidebar\")\n\n\nUse st._config.set_option() to update the configuration values. Reload the app for the changes to take effect by using st.experimental_rerun(). It was inspired by jrieke/streamlit-theme-generator (here is the code).\n\nWrapping up\n\nThe Streamlit color theme editor app offers a simple and effective solution for creating visually appealing and accessible color themes for your apps. Thanks to the WCAG 2.0 standard and the real-time preview, your themes will now be attractive and accessible to all users.\n\nHappy app-building! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "soma noda - Streamlit",
    "url": "https://blog.streamlit.io/author/soma/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by soma noda\n1 post\nConvert images into pixel art\n\nA 5-step tutorial for making a pixel art converter app\n\nAdvocate Posts\nby\nsoma noda\n,\nMay 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Convert images into pixel art",
    "url": "https://blog.streamlit.io/convert-images-into-pixel-art/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nConvert images into pixel art\n\nA 5-step tutorial for making a pixel art converter app\n\nBy soma noda\nPosted in Advocate Posts, May 8 2023\n1. Create the program base\n1-1. Install Streamlit and modules\n1-2. Fill in the code\n1-3. Run the program\n2. Turn an image into dots\n2-1. Mosaic\n2-2. Execute mosaic processing\n3. Make pixel art more like handwriting\n3-1. Create a color palette\n3-2. Prepare for using the color palette\n3-3. Use a color palette\n4. Make variables modifiable\n4-1. Add a slider for the ratio\n4-2. Add a select box for the color palette\n5. Use additional filters\n5-1. Add no palette\n5-2. Add color reduction\n5-3. Add an edge filter\nBonus. What to check if the program fails\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHello everyone! üëã\n\nMy name is Akaz. I‚Äôm a student with a strong interest in data science and programming.\n\nHave you ever had an experience where you didn't like an existing app? I love pixel art and wanted to convert images into pixel art. But none of the existing apps converted them just as I wanted. So I solved this problem by creating PixelArt-Converter.\n\nIn this post, I‚Äôll show you how to make your own pixel art converter app in five easy steps:\n\nCreate the program base\nTurn an image into dots\nMake pixel art more like handwriting\nMake variables modifiable\nUse additional filters\nüëæ\nIf you want to skip reading and check it out right away, here's the app and the repo code.\n1. Create the program base\n1-1. Install Streamlit and modules\n\nFill in the code in the console (skip the items that are already installed):\n\npip install pipenv\npipenv --python 3.10\npipenv install streamlit\npipenv install numpy\npipenv install opencv-python-headless\npipenv install pillow\npipenv shell\n\n\n1-2. Fill in the code\n\nCreate a new directory and create main.py. Fill in the following code in the created main.py:\n\nimport streamlit as st\nimport streamlit.components.v1 as components\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport csv\nimport os\nimport pandas as pd\n\nclass Converter():\n    def __init__(self) -> None:\n        self.color_dict = {}\n\nclass Web():\n    def __init__(self) -> None:\n        self.draw_text()\n\n    def draw_text(self):\n        st.set_page_config(\n            page_title=\"Pixelart-Converter\",\n            page_icon=\"üñºÔ∏è\",\n            layout=\"centered\",\n            initial_sidebar_state=\"expanded\",\n        )\n        st.title(\"PixelArt-Converter\")\n        self.upload = st.file_uploader(\"Upload Image\", type=['jpg', 'jpeg', 'png', 'webp'])\n        self.original, self.converted = st.columns(2)\n        self.original.title(\"original img\")\n        self.converted.title(\"convert img\")\n\nif __name__ == \"__main__\":\n    web = Web()\n    converter = Converter()\n    if web.upload != None:\n        img = Image.open(web.upload)\n        img = np.array(img)\n        web.original.image(web.upload)\n\n\n\nNow that the foundation is complete, you'll use this foundation to create programs.\n\n1-3. Run the program\n\nEnter the following commands into the console to execute the program:\n\nstreamlit run main.py\n\n\n\n2. Turn an image into dots\n\nAs it turns out, mosaicing an image makes it pixel art.\n\n2-1. Mosaic\n\nAdd the function below to the Converter class:\n\ndef mosaic(self, img, ratio=0.1):\n    small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n    return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n\n\n\nThis mosaic function takes an image as an argument and performs mosaicing. The smaller the value of the ratio, the larger the mosaic.\n\nNext, create a place to use the mosaic function. Add the following program at the end of if __name__:\n\nimg = converter.mosaic(img)\nweb.converted.image(img)\n\n\n2-2. Execute mosaic processing\n\nTo execute the mosaic processing you've just added, enter the following command:\n\nstreamlit run main.py\n\n\n\nNow you've taken care of the mosaic! One step closer to completion. ü•≥\n\n3. Make pixel art more like handwriting\n\nYou can mimic how pixel art is created to make it look more like hand-drawn pixel art.\n\n3-1. Create a color palette\n\nHand-drawn pixel art often has a small number of colors. Therefore, create a color palette using a CSV file. The directory should be color/palette.csv:\n\n255,127,127\n255,191,127\n255,255,127\n127,255,127\n127,191,255\n127,127,255\n0,0,0\n255,255,255\n128,128,128\n\n\n\nThe CSV above shows the color palette. The color palette uses RGB‚Äîone color per line.\n\n3-2. Prepare for using the color palette\n\n3-2-1. Create a function to read CSV\n\nTo use the color palette, the program must read a CSV. For this, add the following code to the Converter class:\n\ndef read_csv(self, path):\n    with open(path) as f:\n        reader = csv.reader(f)\n        color = [[int(v) for v in row] for row in reader]\n        return color\n\n\n\nThis code is a function to read a CSV. It reads it and returns an array.\n\n3-2-2. Create a function to check colors from an image\n\nAdd the following code to Converter in main.py:\n\ndef color_change(self, r, g, b, color_pallet):\n    if (r, g, b) in self.color_dict:\n        return self.color_dict[(r, g, b)]\n    # ÊúÄ„ÇÇËøë„ÅÑËâ≤„ÇíË¶ã„Å§„Åë„Çã\n    min_distance = float('inf')\n    color_name = None\n    for color in color_pallet:\n        distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2\n        if distance < min_distance:\n            min_distance = distance\n            color_name = color\n    self.color_dict[(r, g, b)] = color_name\n    return color_name\n\n\n\nThis function returns the closest color from the color palette when RGB is specified. The code below stores the converted RGBs in a dictionary as they're processed. If the RGB to be converted is already registered in the dictionary, the registered RGB is used to speed up the process:\n\n#This code block is for illustration purposes.\nif (r, g, b) in self.color_dict:\n    return self.color_dict[(r, g, b)]\n~~~~~\nself.color_dict[(r, g, b)] = color_name\n\n\n\nThen this code uses the least-squares approximation to determine the closest color:\n\n#This code block is for illustration purposes.\nmin_distance = float('inf')\ncolor_name = None\nfor color in color_pallet:\n    distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2\n    if distance < min_distance:\n        min_distance = distance\n        color_name = color\n\n\n\n3-2-3. Use the color_change function\n\nAdd the following code to the Converter in main.py:\n\ndef convert(self, img, option, custom=None):\n    w, h = img.shape[:2]\n    changed = img.copy()\n    # ÈÅ∏Êäû„Åï„Çå„Åücsv„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n    color_pallet = []\n    if option != \"Custom\":\n        color_pallet = self.read_csv(\"./color/\"+option+\".csv\")\n    else:\n        if custom == [] or custom == None:\n            return\n        color_pallet = custom\n\n    for height in range(h):\n        for width in range(w):\n            color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)\n            changed[width][height][0] = color[0]  # Ëµ§\n            changed[width][height][1] = color[1]  # Á∑ë\n            changed[width][height][2] = color[2]  # Èùí\n    return changed\n\n\n\nThis function uses color_change, which was created earlier to change colors. Specifically, an array of images converted to RGB is passed to color_change to convert the colors. The function returns an array containing the converted colors, and all colors are converted.\n\n3-3. Use a color palette\n\nAdd the following code above web.converted.image(img) in if__name__:\n\nimg = converter.convert(img,\"palette\")\n\n\n\nAdding this code will add a color palette. The second argument asks for the name of the CSV file, so you can change it by replacing \"palette.csv\" with the name of your desired file.\n\nNext, let's change the ratios and palettes on the website.\n\nHere is all the code so far:\n\nimport streamlit as st\nimport streamlit.components.v1 as components\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport csv\nimport os\nimport pandas as pd\n\nclass Converter():\n    def __init__(self) -> None:\n        self.color_dict = {}\n\n    def mosaic(self, img, ratio=0.1):\n        small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n        return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n\n    def read_csv(self, path):\n        with open(path) as f:\n            reader = csv.reader(f)\n            color = [[int(v) for v in row] for row in reader]\n            return color\n\n    def color_change(self, r, g, b, color_pallet):\n        if (r, g, b) in self.color_dict:\n            return self.color_dict[(r, g, b)]\n        # ÊúÄ„ÇÇËøë„ÅÑËâ≤„ÇíË¶ã„Å§„Åë„Çã\n        min_distance = float('inf')\n        color_name = None\n        for color in color_pallet:\n            distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2\n            if distance < min_distance:\n                min_distance = distance\n                color_name = color\n        self.color_dict[(r, g, b)] = color_name\n        return color_name\n\n    def convert(self, img, option, custom=None):\n        w, h = img.shape[:2]\n        changed = img.copy()\n        # ÈÅ∏Êäû„Åï„Çå„Åücsv„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n        color_pallet = []\n        if option != \"Custom\":\n            color_pallet = self.read_csv(\"./color/\"+option+\".csv\")\n        else:\n            if custom == [] or custom == None:\n                return\n            color_pallet = custom\n\n        for height in range(h):\n            for width in range(w):\n                color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)\n                changed[width][height][0] = color[0]  # Ëµ§\n                changed[width][height][1] = color[1]  # Á∑ë\n                changed[width][height][2] = color[2]  # Èùí\n        return changed\n\nclass Web():\n    def __init__(self) -> None:\n        self.draw_text()\n\n    def draw_text(self):\n        st.set_page_config(\n            page_title=\"Pixelart-Converter\",\n            page_icon=\"üñºÔ∏è\",\n            layout=\"centered\",\n            initial_sidebar_state=\"expanded\",\n        )\n        st.title(\"PixelArt-Converter\")\n        self.upload = st.file_uploader(\"Upload Image\", type=['jpg', 'jpeg', 'png', 'webp'])\n        self.original, self.converted = st.columns(2)\n        self.original.title(\"original img\")\n        self.converted.title(\"convert img\")\n\nif __name__ == \"__main__\":\n    web = Web()\n    converter = Converter()\n    if web.upload != None:\n        img = Image.open(web.upload)\n        img = np.array(img)\n        web.original.image(web.upload)\n        img = converter.mosaic(img)\n        img = converter.convert(img,\"pallet\")\n        web.converted.image(img)\n\n\n\n4. Make variables modifiable\n\nLet's make the ratio and the color palette changeable.\n\n4-1. Add a slider for the ratio\n\nAdd a slider using st.slider. Add the following code under self.upload in the draw_text function:\n\nself.ratio = st.slider('Select ratio', 0.01, 1.0, 0.3, 0.01)\n\n\n\nThe arguments of st.slider are as follows:\n\nArguments\tFunction\nFirst Argument\tSet Label\nSecond Argument\tMinimum Value\nThird Argument\tMaximum Value\nFourth Argument\tDefault Value\nFifth Argument\tStep\n\nAfter completing the previous step, the next one is to apply the values obtained from Streamlit. If you set the second argument of img = converter.mosaic(img) to web.ratio, the image will be updated when the slider is changed:\n\n# Changed code\nimg = converter.mosaic(img, web.ratio)\n\n\n\nWhen done, run the program. If the image changes when you adjust the slider, you've succeeded!\n\n4-2. Add a select box for the color palette\n\nCreate a select box using st.selectbox. Add the following code under self.upload in the draw_text function:\n\nself.color = st.selectbox(\"Select color palette\", (\"palette\",))\n\n\n\nThe code above describes the title above the selectbox and the content that can be selected in the selectbox. To add a color palette, add a filename to the second argument tuple.\n\nTo use the variables obtained in the select box, replace img = converter.convert(img, \"palette\",) with the following code:\n\nimg = converter.convert(img,web.color)\n\n\n\nThe string currently selected in the selectbox is stored in the web.color variable. Before executing the code, you can create a new CSV file in the color directory and add it to the selectbox. Once complete, run the code. If a box is added that allows you to select a color palette, then you did it right!\n\n5. Use additional filters\n\nPixelArt-Converter provides three options for converting images:\n\nThe \"no palette\" option, which uses a mosaic process to create a pixel art-like image without relying on a color palette or color reduction\nAn edge filter to further enhance the conversion process\nA color reduction process\n5-1. Add no palette\n\n5-1-1. Add a function\n\nNo palette is implemented using st.checkbox. Add the following functions to the web class:\n\ndef more_options(self):\n\t\twith st.expander(\"More Options\", True):\n        self.no_convert = st.checkbox('no color convert')\n\n\n\nThis function creates an expander that contains a checkbox. When the checkbox is clicked, the value is entered into self.no_convert.\n\n5-1-2. Call a function\n\nTo call the function you just created, add the following code to the bottom of the draw_text function:\n\nself.more_options()\n\n\n\nAdding this code will cause more_options to be called.\n\n5-1-3. Make it usable\n\nAdd the following condition to img = converter.convert(img, web.color) to prevent color conversion from being performed:\n\nif web.no_convert == False:\n    img = converter.convert(img, web.color)\n\n\n5-2. Add color reduction\n\nIf the palette isn't processed, pixel art undergoes a subtractive process. To create a subtractive process, add the following code to the Converter class.\n\n5-2-1. Add a function\n\ndef decreaseColor(self, img):\n    dst = img.copy()\n\n    idx = np.where((0 <= img) & (64 > img))\n    dst[idx] = 32\n    idx = np.where((64 <= img) & (128 > img))\n    dst[idx] = 96\n    idx = np.where((128 <= img) & (192 > img))\n    dst[idx] = 160\n    idx = np.where((192 <= img) & (256 > img))\n    dst[idx] = 224\n\n    return dst\n\n\n\nThis function performs a color reduction of an image and returns the result.\n\n5-2-2. Add a checkbox\n\nCreate a checkbox to add color reduction processing. Add the following code to the expander of more_options:\n\nself.decreaseColor = st.checkbox(\"decrease color\")\n\n\n\nThe checkbox is created in the same location as the no_convert created earlier.\n\n5-2-3. Make it usable\n\nAdd the following code above web.converted.image(img) in \"if name == \"main\":\n\nif web.decrease:\n    img = converter.decreaseColor(img)\n\n\n\nAdding this code will perform the color reduction process.\n\n5-3. Add an edge filter\n\n5-3-1. Add a function\n\nAdd the function anime_filter to the class Converter to implement an edge filter. In the PixelArt-Converter, this filter is referred to as the \"animated filter\":\n\ndef anime_filter(self, img, th1=50, th2=150):\n    # „Ç¢„É´„Éï„Ç°„ÉÅ„É£„É≥„Éç„É´„ÇíÂàÜÈõ¢\n    bgr = img[:, :, :3]\n    if len(img[0][0]) == 4:\n        alpha = img[:, :, 3]\n\n    # „Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´Â§âÊèõ\n    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n\n    # „Åº„Åã„Åó„Åß„Éé„Ç§„Ç∫‰ΩéÊ∏õ\n    edge = cv2.blur(gray, (3, 3))\n\n    # Canny„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßËº™ÈÉ≠ÊäΩÂá∫\n    edge = cv2.Canny(edge, th1, th2, apertureSize=3)\n\n    # Ëº™ÈÉ≠ÁîªÂÉè„ÇíRGBËâ≤Á©∫Èñì„Å´Â§âÊèõ\n    edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2BGR)\n\n    # Â∑ÆÂàÜ„ÇíËøî„Åô\n    result = cv2.subtract(bgr, edge)\n\n    # „Ç¢„É´„Éï„Ç°„ÉÅ„É£„É≥„Éç„É´„ÇíÁµêÂêà„Åó„Å¶Ëøî„Åô\n    if len(img[0][0]) == 4:\n        return np.dstack([result, alpha])\n    else:\n        return result\n\n\n\n5-3-2. Add checkboxes\n\nAdd the following checkboxes to more_options as before:\n\nself.edge_filter = st.checkbox('anime filter')\n\n\n\n5-3-3. Make it usable\n\nAdd the following code above web.converted.image(img) if __name__ == \"main\":\n\nif web.edge_filter:\n\timg = converter.anime_filter(img)\n\n\n\nHere is the full code :\n\nimport streamlit as st\nimport streamlit.components.v1 as components\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport csv\nimport os\nimport pandas as pd\n\nclass Converter():\n    def __init__(self) -> None:\n        self.color_dict = {}\n\n    def mosaic(self, img, ratio=0.1):\n        small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n        return cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n\n    def read_csv(self, path):\n        with open(path) as f:\n            reader = csv.reader(f)\n            color = [[int(v) for v in row] for row in reader]\n            return color\n\n    def color_change(self, r, g, b, color_pallet):\n        if (r, g, b) in self.color_dict:\n            return self.color_dict[(r, g, b)]\n        # ÊúÄ„ÇÇËøë„ÅÑËâ≤„ÇíË¶ã„Å§„Åë„Çã\n        min_distance = float('inf')\n        color_name = None\n        for color in color_pallet:\n            distance = (int(r) - color[0]) ** 2 + (int(g) - color[1]) ** 2 + (int(b) - color[2]) ** 2\n            if distance < min_distance:\n                min_distance = distance\n                color_name = color\n        self.color_dict[(r, g, b)] = color_name\n        return color_name\n\n    def convert(self, img, option, custom=None):\n        w, h = img.shape[:2]\n        changed = img.copy()\n        # ÈÅ∏Êäû„Åï„Çå„Åücsv„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n        color_pallet = []\n        if option != \"Custom\":\n            color_pallet = self.read_csv(\"./color/\"+option+\".csv\")\n        else:\n            if custom == [] or custom == None:\n                return\n            color_pallet = custom\n\n        for height in range(h):\n            for width in range(w):\n                color = self.color_change(img[width][height][0], img[width][height][1], img[width][height][2], color_pallet)\n                changed[width][height][0] = color[0]  # Ëµ§\n                changed[width][height][1] = color[1]  # Á∑ë\n                changed[width][height][2] = color[2]  # Èùí\n        return changed\n\n    def decreaseColor(self, img):\n        dst = img.copy()\n\n        idx = np.where((0 <= img) & (64 > img))\n        dst[idx] = 32\n        idx = np.where((64 <= img) & (128 > img))\n        dst[idx] = 96\n        idx = np.where((128 <= img) & (192 > img))\n        dst[idx] = 160\n        idx = np.where((192 <= img) & (256 > img))\n        dst[idx] = 224\n\n        return dst\n\n    def anime_filter(self, img, th1=50, th2=150):\n        # „Ç¢„É´„Éï„Ç°„ÉÅ„É£„É≥„Éç„É´„ÇíÂàÜÈõ¢\n        bgr = img[:, :, :3]\n        if len(img[0][0]) == 4:\n            alpha = img[:, :, 3]\n\n        # „Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´Â§âÊèõ\n        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n\n        # „Åº„Åã„Åó„Åß„Éé„Ç§„Ç∫‰ΩéÊ∏õ\n        edge = cv2.blur(gray, (3, 3))\n\n        # Canny„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßËº™ÈÉ≠ÊäΩÂá∫\n        edge = cv2.Canny(edge, th1, th2, apertureSize=3)\n\n        # Ëº™ÈÉ≠ÁîªÂÉè„ÇíRGBËâ≤Á©∫Èñì„Å´Â§âÊèõ\n        edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2BGR)\n\n        # Â∑ÆÂàÜ„ÇíËøî„Åô\n        result = cv2.subtract(bgr, edge)\n\n        # „Ç¢„É´„Éï„Ç°„ÉÅ„É£„É≥„Éç„É´„ÇíÁµêÂêà„Åó„Å¶Ëøî„Åô\n        if len(img[0][0]) == 4:\n            return np.dstack([result, alpha])\n        else:\n            return result\n\nclass Web():\n    def __init__(self) -> None:\n        self.draw_text()\n\n    def draw_text(self):\n        st.set_page_config(\n            page_title=\"Pixelart-Converter\",\n            page_icon=\"üñºÔ∏è\",\n            layout=\"centered\",\n            initial_sidebar_state=\"expanded\",\n        )\n        st.title(\"PixelArt-Converter\")\n        self.upload = st.file_uploader(\"Upload Image\", type=['jpg', 'jpeg', 'png', 'webp'])\n        self.color = st.selectbox(\"Select color palette\", (\"cold\",\"gold\"))\n        self.ratio = st.slider('Select ratio', 0.01, 1.0, 0.3, 0.01)\n        self.original, self.converted = st.columns(2)\n        self.original.title(\"original img\")\n        self.converted.title(\"convert img\")\n        self.more_options()\n\n    def more_options(self):\n        with st.expander(\"More Options\", True):\n            self.no_convert = st.checkbox('no color convert')\n            self.decrease = st.checkbox('decrease color')\n            self.edge_filter = st.checkbox('anime filter')\n\nif __name__ == \"__main__\":\n    web = Web()\n    converter = Converter()\n    if web.upload != None:\n        img = Image.open(web.upload)\n        img = np.array(img)\n        web.original.image(web.upload)\n        img = converter.mosaic(img, web.ratio)\n        if web.no_convert == False:\n            img = converter.convert(img, web.color)\n        if web.decrease:\n            img = converter.decreaseColor(img)\n        if web.edge_filter:\n            img = converter.anime_filter(img)\n        web.converted.image(img)\n\n\n\nAnd here is a cool video of the app:\n\nBonus. What to check if the program fails\n\nA list of things to check in case your program doesn't work:\n\nChanges are not updated. Make sure the program is saved!\nConditions are not applied. Check that the indentation is correct.\nFile cannot be loaded. Check that the filename is correct.\nFunction not executed. Check that you are putting the code in the correct place.\nProgram cannot be executed. Check that the directory you are opening in the console is correct.\nWrapping up\n\nThank you for reading my post! I hope you found PixelArt-Converter interesting. If you have any questions, please post them in the comments below or contact me on Twitter.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Siavash Yasini - Streamlit",
    "url": "https://blog.streamlit.io/author/siavash/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Siavash Yasini\n2 posts\nThe ultimate Wordle cheat sheet\n\nLearn how to beat Wordle with Streamlit\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nMay 11 2023\nCreate a color palette from any image\n\nLearn how to come up with the perfect colors for your data visualization\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nJanuary 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "The ultimate Wordle cheat sheet",
    "url": "https://blog.streamlit.io/the-ultimate-wordle-cheat-sheet/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe ultimate Wordle cheat sheet\n\nLearn how to beat Wordle with Streamlit\n\nBy Siavash Yasini\nPosted in Advocate Posts, May 11 2023\nThe backend\nThe algorithm‚Äîbehind the scenes\nThe frontend\n1. Create a WORDLE-esque interface\n2. Submit and validate guesses and hints\n3. Pass the submission to the WORDLE solver\nWhich word should you start with?\n1. Most common letters\n2. Levenshtein metric\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Siavash, and I'm a Senior Data Scientist at Zest AI.\n\nI remember when the Wordle craze was sweeping across the internet. All I could see on social media was üü© üü® ‚¨õÔ∏è üü© üü®. I tried to avoid it (be too cool for the trend) but eventually succumbed to peer pressure‚Äîand I'm glad I did!\n\nWORDLE is a truly brilliant game. I love a good puzzle, so I immediately fell in love with it. The problem is, I'm terrible at guessing, especially when it comes to five-letter words. So I decided to beat the game with‚Ä¶\n\nStatistics!\n\nThe app has been working without fail since my first WORDLE win in early 2022, and I've yet to find out what happens if I don't find the word of the day within six guesses.\n\nü§ì\nIf you want to go straight to playing, here is the app! And here is the repo code.\nThe backend\n\nThe algorithm to beat the game is very simple. In fact, it's so simple that I'm not even sure it deserves to be called an algorithm. All you need is a list of all possible 5-letter words and their frequency in English. Then you can follow these steps:\n\nSort the list of words based on their commonality‚Äîthe more common the word, the higher its ranking.\nAt every step of the game, remove any word that doesn‚Äôt fit the constraints provided by the hints.\nPick the first word from the remaining list as your next guess. If you feel more adventurous, look at the first few words at the top of the list and follow your gut.\n\nThis approach isn't the most efficient way to beat the game and only works for roughly 95% of all possible WORDLE solutions in hard mode‚Äîit fails for some odd words with repeated letters. But the algorithm's simplicity allows for a more natural and intuitive approach to playing the game, which is why I‚Äôm satisfied with its success rate.\n\nCurious why it works? Check out the next section. Otherwise, jump straight to the frontend section with all the Streamlit fun!\n\nThe algorithm‚Äîbehind the scenes\n\nIt's fascinating to note that the origin of Wordle lies in a personal gesture of love. The game's creator crafted it for his wife and asked her to select words that could be potential solutions (read the story here). She sifted through the list of five-letter English words, picking the ones she recognized. Her word familiarity related to its commonness. She was more likely to have encountered commonly used words like \"HOUSE\" as opposed to rare ones like \"FUGLE.\" This is the underlying principle behind the game's algorithm...\n\nThe more frequently a word appears in the English language, the greater its likelihood of being a valid Wordle solution.\n\nWe can verify this assumption by looking at all possible WORDLE solutions (available here). KRememberthat this list isn‚Äôt used in the final app, and only serves as a tool for confirming our hypothesis.\n\nI have plotted the top 15 most commonly used English words, according to the frequency of their appearance on Wikipedia. The only word that is not a Wordle solution is \"YEARS\", and that‚Äôs because it‚Äôs technically a plural 4-letter word, and plurals can‚Äôt be Wordle solutions.\n\nZooming out and looking at the top 150 words, notice how most of the potential Wordle solutions. But as we move towards less common words, it becomes more likely to encounter words that aren‚Äôt solutions. So far, so good.\n\nLook at the top 10,000 most frequently used words. ¬†See a similar pattern? To make the distribution easier to view and understand, I have selected a sample of 150 words from the list. As expected, there is a greater number of non-Wordle solutions further down the list:\n\nThis confirms that word commonality, or usage frequency, is correlated with being a Wordle solution. Use the hints provided by Wordle to eliminate all the words that don't fit the constraints, then pick the most common word as your next guess for the game.\n\nFor example, type in ‚ÄúGUESS‚Äù as your guess (brilliant!), and Wordle will give you back the following hints: ‚¨õÔ∏è ‚¨õÔ∏è üü© üü© üü© . After going through the list of all 5 fiveetter words, only keeping the ones that end with ‚ÄúESS‚Äù and removing the ones that have either a ‚ÄúG‚Äù or a ‚ÄúU,‚Äù you‚Äôll end up with the following recommendations:\n\nYour best next guess would be ‚ÄúPRESS‚Äù because, according to the algorithm, it‚Äôs more commonly used than the rest of the list.\n\nü§î\nYou may argue that at this stage of the game, there is no reason why PRESS would be better than DRESS. DRESS might get you to the final solution faster because ‚ÄúD‚Äù is a more common letter than ‚ÄúP.‚Äù I agree. I‚Äôm just taking a different approach. This algorithm doesn‚Äôt give you the most efficient solution. It follows a simple working principle, according to which PRESS is better than DRESS and all the other potential solutions.\n\nGo ahead and type in PRESS. You‚Äôll get ‚¨õÔ∏è üü© üü© üü© üü© . Again, using the constraints to eliminate the list you end up with:\n\nAt this point, you‚Äôd be surprised and annoyed if DRESS wasn‚Äôt the correct answer!\n\nThe frontend\n\nI‚Äôm not here to teach you about my not-so-brilliant algorithm. I‚Äôm here to tell you how I used Streamlit üéà to turn the algorithm into an app that you can use as a WORDLE cheat sheet:\n\nCreate a WORDLE-esque interface\nSubmit and validate guesses and hints\nPass the submission to the WORDLE solver\n\nLet‚Äôs call the app WORDLEr‚Ä¶because why not?!\n\n1. Create a WORDLE-esque interface\n\nThe first thing we need for the app is an interface that allows us to input our guesses and the corresponding WORDLE hints, so that they can be passed through the algorithm.\n\nStreamlit‚Äôs submit form st.form is the perfect tool for this. As you can see in the GIF, in order to somewhat mimic WORDLE‚Äôs interface, I have assigned individual text_input boxes to each letter (max_chars=1), with dropdown boxes st.selectbox underneath each, allowing us to pass the hint returned by WORDLE for that specific letter.\n\nWORDLE doesn‚Äôt allow more than 6sixguesses per game, so creating a submit form with a fixed number of rows makes sense. However, with the proposed algorithm, we rarely need to use more than 3 or 4 guesses to find the word of the day, so taking up additional space on the app with rows that will rarely be used doesn't make sense.\n\nIt‚Äôs easy enough to make the number of rows in the form dynamically, so we can start with three rows and allow the user to change it if needed. The following function creates a form for submitting the guesses, and it takes an input parameter that determines how many rows will appear on the form.\n\ndef submit_guesses(n_guesses=6):\n    \"\"\"Create a word submission form with n guesses.\n    Return all the guesss and hints submitted.\"\"\"\n\n    with st.form(\"form\"):\n        all_guesses = []\n        all_hints = []\n        for n in range(1, n_guesses+1):\n            guess_letters = []\n            guess_hints = []\n\n            letters_cols = st.columns(5)\n            for i, col in enumerate(letters_cols):\n                with col:\n                    letter = st.text_input(\" \", max_chars=1, key=f\"guess_{n}_letter_{i}\")\n                    guess_letters.append(letter.upper())\n\n            hints_cols = st.columns(5)\n            for i, col in enumerate(hints_cols):\n                with col:\n                    hint = st.selectbox(\" \", colors,  key=f\"guess_{n}_hint_{i}\")\n                    guess_hints.append(hint)\n            st.markdown(\"---\")\n            all_guesses.append(guess_letters)\n            all_hints.append(guess_hints)\n\n        st.form_submit_button(\"submit\")\n\n    return all_guesses, all_hints\n\n2. Submit and validate guesses and hints\n\nThe function submit_guesses returns a list of all the guesses and hints from the previous steps. Your Wordle solver will use all of these hints collectively and apply them as constraints to the recommendation list on the backend.\n\nFor example, it‚Äôll only keep the words that start with an A, have an S somewhere, and don‚Äôt have an R, I, or E in them:\n\nOne more thing to do. Make sure that the five letters passed in each stage are valid. For example, mensurethe user hasn't passed a digit or a punctuation mark instead of a letter or there are no missing letters in the guesses.\n\nI have wrapped all of this in a function called keep_valid_guesses (check out the repo for the code behind it):\n\n# let the magic happen...\nn_steps = st.sidebar.slider(\"Number of steps\", 1, 10, value=3)\nall_guesses, all_hints = submit_guesses(n_steps)\n\n# make sure what is passed is a actually a 5 letter word (and not something like \"!NV4L\")\nvalid_guesses, valid_hints = keep_valid_guesses(all_guesses, all_hints) \n\n3. Pass the submission to the WORDLE solver\n\nFinally, pass the guesses and hints to Wordler(). This class has a simple and intuitive interface. It uses the .update_constraint(guess, hint) method to update constraints based on the input guess and hint and applies it to the recommended list on the backend. Then the .suggest_next_word(head=n_suggestions) function returns the top n_suggestions words on the list:\n\nn_suggestions = 10\n\nwordler = Wordler() # <-- our wordle solver! \nfor guess, hint in zip(valid_guesses, valid_hints):\n    wordler.update_constraint(guess, hint)\n\nif st.session_state[\"FormSubmitter:form-submit\"]:\n    st.header(\"Next Word Suggestions\")\n    st.dataframe(wordler.suggest_next_word(head=n_suggestions))\n\nWhich word should you start with?\n\nNow that you have a cheat sheet to find the next best guess, the question is: what‚Äôs the best word to start with?\n\nThere are two ways to approach this.\n\n1. Most common letters\n\nLook at the most common letters in English and start with a five-letter word containing all of them. This way, you‚Äôll maximize the probability of getting a üü® or üü© hint on the first guess.\n\nAccording to frequency analysis of English words, the most common letters are A, E, S, R, and I. With these letters, you can build ARISE, RAISE, AESIR, and ARIES. My favorite is ARISE (how fitting), so I always start the game with that!\n\n2. Levenshtein metric\n\nUse the Levenshtein metric to calculate the distance (number of letter changes required to convert one word into another) between all English words. See which word has the minimum square distance from all the other words. Interestingly, this also remakes RISE (and RAISE) as an optimal starting word!\n\nI have simulated mock WORDLE games, using the word list mentioned earlier to see how quickly the app can find the solution, starting with ARISE. The histogram below shows the number of moves it will take to find the answer:\n\nAs you can see, it takes an average of four moves to find the answer when starting with ARISE. The algorithm can find 95% of words within six guesses! It only has difficulty with rare words with repeated letters, like GOLLY.\n\nü§ì\nIf you‚Äôre not familiar with the word GOLLY, here‚Äôs how you would use it in a sentence: ‚ÄùOh golly, that was one difficult word! It took Wordler 10 moves to finally find it‚Ä¶‚Äù\nWrapping up\n\nI hope you enjoyed learning about how to build a WORDLE-esque interface using st.form , st.columns , st.text_input , and st.select_box. You also learned how to make the interface size flexible using st.slider, how to submit your guess to the Wordle solver using st.submit_form, and how to print out a recommendation list using st.dataframe. There is a lot more code that implements the algorithm through the Wordler() class, so feel free to check it out.\n\nI'm sorry if the app makes the guessing too easy, but now you can impress your friends with unbeatable Wordle scores! ü§©\n\nI'd love to hear your thoughts, questions, comments, and feedback. Get in touch with me on LinkedIn or through my website.\n\nHappy Wordle-ing and Happy Streamlit-ing! üü©üéàüü© üü® üü©\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Alice Heiman - Streamlit",
    "url": "https://blog.streamlit.io/author/alice/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Alice Heiman\n1 post\nLearn Morse code with a Streamlit app\n\n5 steps to build your own Morse code tutor!\n\nAdvocate Posts\nby\nAlice Heiman\n,\nMay 12 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Learn Morse code with a Streamlit app",
    "url": "https://blog.streamlit.io/learn-morse-code-with-a-streamlit-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLearn Morse code with a Streamlit app\n\n5 steps to build your own Morse code tutor!\n\nBy Alice Heiman\nPosted in Advocate Posts, May 12 2023\nStep 1. What is Morse code anyway?\nStep 2. Sound module\nStep 3. Game module\nStep 4. Levels\nStep 5. Checkpoints and Playground\nWrapping up\nAppendix: Complete Morse code\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nI'm Alice, a student from Sweden passionate about using technology. AI excites me because it can enhance human creativity, find otherwise undetected patterns, and create personalized learning tools.\n\nAs a ham radio operator, I wanted to learn Morse code to communicate with other operators worldwide. Because in times of crisis, traditional radio can be an alternative channel for critical information. But I had only old audio Morse recordings with exercises that took forever to check manually, and I wanted a more fun and engaging way to learn it.\n\nSo‚Ä¶I built a Streamlit app! It's an interactive Morse tutor with 15 levels, five checkpoints, and memorable mnemonics that helped me transmit words and phrases from the first week.\n\nMorse code was developed in the 1800s, but it's still relevant for signaling and identification and is often used in capture-the-flag events (CTFs), escape rooms, and puzzles!\n\nIn this post, I'll show you how to create it in five steps:\n\nStep 1. What is Morse code anyway?\nStep 2. Sound module\nStep 3. Game module\nStep 4. Levels\nStep 5. Checkpoints and Playground\nüí°\nTo try it out, visit the demo app. The source code can be found here.\nStep 1. What is Morse code anyway?\n\nThe Morse alphabet associates each letter with a sequence of short and long pulses called \"dots\" and \"dashes.\" But how do you choose which letter gets which sequence? It's far from random. If you think about it, you'd probably want to reserve the shorter sequences for the most common letters.\n\nThe International Morse code table looks like this:\n\nMorse code. (2023, April 12). In Wikipedia. https://en.wikipedia.org/wiki/Morse_code\n\nIf we analyze the frequency of the English letters, that is how many times a letter is used on average, we get the following graph:\n\nFrequencies of letters in the English language generated from the NLTK corpus\n\nNotice that the most common letters, \"e t a o n,\" have the shortest sequences.\n\nArmed with these insights, we can tailor our training to focus on the most commonly used letters and words to make the most of limited time.\n\nLet's get coding!\n\nStep 2. Sound module\n\nThe most important module is the creation of Morse audio sequences. To do this, we need to understand pulse timing. In Morse, we make the shortest pulse one unit and define all other lengths relative to it:\n\nShort pulse: 1 unit\nLong pulse: 3 units\nIntraletter spacing: 1 unit\nInterletter spacing: 3 units\nInterword spacing: 7 units\n\nFor example, the word ‚ÄúPARIS‚Äù becomes ‚Äú‚Ä¢ - - ‚Ä¢ ¬† ‚Ä¢ - ¬† ‚Ä¢ - ‚Ä¢ ¬† ‚Ä¢ ‚Ä¢ ¬† ‚Ä¢ ‚Ä¢ ‚Ä¢‚Äù, with a total of 50 units ($10\\text{ short} \\cdot 1\\text{ unit} + 4\\text{ long} \\cdot 3\\text{ units} + 9\\text{ intraletter} \\cdot 1\\text{ unit} + 4\\text{ interletter} \\cdot 3\\text{ units} + 1\\text{ interword} \\cdot 7\\text{ units} = 50\\text{ units}$):\n\nWhen practicing Morse code, the goal is not to memorize the sequences of letters but to imprint their auditory patterns.\n\nSo we want to keep the speed of the letter sequences high, but we can increase the spacing between letters and words to create a slower beginner pace. To do this, we use something called Farnsworth timing. It allows us to \"warp\" a higher word-per-minute speed to a lower one just by lengthening the pauses. This article explains the mathematics behind the formulas used to make this transformation.\n\nHere is the code:\n\nclass SoundCreator:\n    def __init__(self, character_speed=22, farnsworth_speed=8, sample_rate=44100.0):\n      self.sample_rate = sample_rate\n      self.character_speed = character_speed\n      self.farnsworth_speed = farnsworth_speed\n\n      # Compute timings\n      u = 1.2 / character_speed\n      ta = (60 * character_speed - 37.2 * farnsworth_speed) / (farnsworth_speed * character_speed)\n      tc = (3 * ta) / 19\n      tw = (7 * ta) / 19\n\n      # Convert to milliseconds\n      self.dot_length = round(u * 1000)\n      self.dash_length = round(u * 3 * 1000)\n      self.intra_character_space = round(u * 1000)\n      self.inter_character_space = round(tc * 1000)\n      self.inter_word_space = round(tw * 1000)\n\n\t\t\t# Initialize audio array\n      self.audio = []\n      self.morse_dict = {k: l[1].replace(\"‚ñÑ‚ñÑ\", \"-\").replace(\"‚ñÑ\", \"*\").replace(\" \", \"\") for k, l in mnemonics.items()}\n\n\nWe generate an audio sample by constructing an array of tone data. Zeros represent silence, while our tone is calculated by a formula based on our desired duration, volume, sample rate, and frequency:\n\ndef _append_silence(self, duration_ms=500):\n    \"\"\"Adding silence by appending zeros.\"\"\"\n    num_samples = duration_ms * (self.sample_rate / 1000.0)\n\n    for _ in range(int(num_samples)):\n        self.audio.append(0.0)\n\ndef _append_sinewave(self, freq=550.0, duration_ms=500, volume=0.2):\n    \"\"\"Appends a beep of length duration_ms\"\"\"\n    num_samples = duration_ms * (self.sample_rate / 1000.0)\n\n    for x in range(int(num_samples)):\n        self.audio.append(volume * math.sin(2 * math.pi * freq * (x / self.sample_rate)))\n\n\nWith the general audio functions complete, we can construct a function that turns any character string into a playable Morse code audio snippet.\n\nHere is the code that does just that:\n\ndef create_audio_from(self, sequence: str, start_delay_ms=None):\n        \"\"\"Takes a strings sequence and transforms it into a playable Morse audio clip.\n\n        Args:\n            sequence (str): Message to be enconded into Morse Code.\n\n        Returns:\n            np_array: audio data for an audio player.\n        \"\"\"\n\n        # reset audio array\n        self.audio = []\n\n        # Add silence at beginning\n        if start_delay_ms:\n            self._append_silence(duration_ms=start_delay_ms)\n\n        for character in sequence:\n            character = character.upper()\n          \n            if character in self.morse_dict:\n\t\t\t\t\t\t\t\tmorse_encoding = self.morse_dict[character]\n\n                for i, symbol in enumerate(morse_encoding):\n                    if symbol == \"*\":\n                        # Short sound\n                        self._append_sinewave(duration_ms=self.dot_length)\n                    elif symbol == \"-\":\n                        # Long sound:\n                        self._append_sinewave(duration_ms=self.dash_length)\n\n                    if i + 1 < len(morse_encoding):\n                        self._append_silence(duration_ms=self.intra_character_space)\n\n                # Add inter-character spacing\n                self._append_silence(duration_ms=self.inter_character_space)\n\n            if character == \" \":\n                # Add inter-word spacing\n                self._append_silence(duration_ms=self.inter_word_space)\n\n        return np.array(self.audio)\n\n\nThe functions take the following inputs:\n\nsequence is a string message, such as \"Hey,\" encoded in Morse code.\nstart_delay adds silence to the audio clip's beginning to allow the user to prepare after pressing play.\n\nThe function returns an audio array containing audio data that can be played by Streamlit's audio player.\n\nüí°\nYou can find the complete Morse code symbol table in the Appendix below.\nStep 3. Game module\n\nThe Game Module handles the component that implements the interactive quizzing. Here is the class declaration:\n\nclass GameCreator:\n    def __init__(self, label, symbols):\n        self.label = label\n        self.symbols = symbols\n        self.anagrams = None\n        self.quotes = None\n\n\nThe Game Modules generate character sequences, and 2) accept and correct user input. I created two main functions to generate these sequences. One generates random groups in the defined character set. The other finds possible anagrams of a set of symbols and assembles them into word sequences.\n\nHere is the code for it:\n\ndef generate_sequence(self, length_unit: int, num_units: int):\n    \"\"\"Creates a random letter sequence of *length_unit* chunks, *num_unit* times.\"\"\"\n    seq = []\n    for _ in range(num_units):\n        seq.append(\"\".join(random.choices(self.symbols, k=length_unit)))\n\n    return \" \".join(seq)\n\ndef generate_anagrams(self, filename):\n\t\t\"\"\"Find all possible words with the current symbol set. (self.symbols)\"\"\"\n    # Step 1: Read words from the file line by line\n        with open(filename, \"r\") as f:\n            words = f.read().split(\"\\\\n\")\n\n    # Step 2: Get all anagrams\n    anagrams = []\n    symbol_set = set(self.symbols.lower())\n    for word in words:\n        word_set = set(word.lower())\n\n        if word_set.issubset(symbol_set):\n            if word.upper() not in anagrams:\n                anagrams.append(word.upper())\n\n    # Step 3: Save and return the list of anagrams\n    self.anagrams = anagrams\n    return anagrams\n\ndef generate_word_sequence(self, num_words: int):\n\t\t\"\"\"Assemble possible words into a word sequence.\"\"\"\n    words = random.choices(self.anagrams, k=num_words)\n    return \" \".join(words)\n\n\nIn the final version, I extended this to include quotes and news summaries. You can create functions to generate any kind of practice text!\n\nThe \"Typer\" function acts as a reusable component with an audio player, instruction text, input field, and interactive feedback:\n\ndef Typer(self):\n      \"\"\"Component with instructions, audio player, user input, and correction.\"\"\"\n      message = self.get_message()\n\n      formatted_symbols = \"\".join(list(self.symbols)).strip()\n      st.markdown(f\"*Available symbols:* **{formatted_symbols}**\")\n\n      with st.form(key=self.label, clear_on_submit=True):\n          user_input = st.text_input(\"**:blue[Type what you hear] üëá**\")\n\n          if st.form_submit_button(\"Submit\"):\n              user_input = user_input.upper()\n              answer = message.upper()\n              output = \"\"\n\n              for i in range(len(user_input)):\n                  if i >= len(answer):\n                      output += f\":red[{user_input[i:]}]\"\n                      break\n\n                  if user_input[i] == answer[i]:\n                      output += f\":green[{user_input[i]}]\" if user_input[i] != \" \" else \" \"\n                  else:\n                      output += f\":red[{user_input[i]}]\" if user_input[i] != \" \" else \" \"\n\n              st.markdown(f\"***Your Answer:*** {output}\")\n              st.markdown(f\"*Comparison:*  {answer}\")\n              self.reset_message()\n\n      reset = st.button(f\"Reset {self.label}\")\n      if reset:\n          self.reset_message()\n          st.experimental_rerun()\n\n\nMy three biggest takeaways from this application are:\n\nFunctions can act like components. Call the function anywhere you want a copy of it. It's a cheap way to bundle Streamlit components into a package.\nUse :color[your text here] and replace color to create colored text.\nIf you want to keep information between reloads, you must cache it.\n\nThe trickiest part was establishing persistence because Streamlit reloads the page when input is passed. To keep a piece of data and not generate a new one every time, we need to store it in the st.session_state. For this project, it's necessary to compare what the user enters and the correct sequence after the input is submitted.\n\nI solved this problem by adding the following functions:\n\ndef initalize_message(self, sequence):\n    if \"sequence\" not in st.session_state:\n        st.session_state.sequence = \"\"\n\n    if st.session_state.sequence == \"\":\n        st.session_state.sequence = sequence\n\ndef get_message(self):\n    return st.session_state.sequence\n\ndef reset_message(self):\n    st.session_state.sequence = \"\"\n\ninitalize_message creates a new key-value pair to store the generated sequence as st.session_state.sequence.\nget_message gets the current sequence stored in the session state.\nreset_message clears the session state.\nStep 4. Levels\n\nI quickly realized that I was structuring all the levels in a similar way:\n\nIntroduction to two new symbols: their Morse code and mnemonics.\nAudio example of each symbol separately.\nAn audio sample of the symbols used together showing the plaintext solution.\nA practice sequence using only the new symbols.\nA practice sequence of all the symbols learned.\n\nInstead of copying and pasting, I created a template file with a configuration dictionary as input. Then I could import this file, specify the parameters in each level file, and call the template function.\n\nHere is an example from level five:\n\nfrom template import *\n\n# Configuration\nlevel = {\n    \"level\": \"Level 5\",\n    \"new_symbols\": \"UD\",\n    \"new_label\": \"UD\",\n    \"all_label\": \"ETASILONUD\",\n    \"length_unit\": 5,\n    \"num_units_tutorial\": 5,\n    \"num_units_all\": 7,\n}\n\ngenerate_level(level)\n\n\nSo if I want to change something on all levels, I only have to change it in one place!\n\nüí°\nYou can find the complete template code in the source repo.\nStep 5. Checkpoints and Playground\n\nThe purpose of the checkpoints is to make practicing more fun and to get a sense of progress.\n\nI downloaded a list of English words and inspirational quotes. Then I created an algorithm that calculated the order in which to learn all the characters to form the maximum number of sentences from the start.\n\nIn this version, a checkpoint has two variations:\n\nWORDS, where you practice random word sequences without grammatical structure. These are generated from the generate_anagrams and generate_word_sequence functions of the game module explained earlier.\nQUOTES, where a random quote is chosen from a list based on the current known symbol set.\n\nEach challenge ends with a reference to the characters of the levels leading up to the current checkpoint.\n\nI created the Playground to let you freely enter your messages, experiment with speeds, and even get daily news snippets to practice on! I chose \"Nature Daily Briefing,\" but you can customize it to whatever text sources you like best!\n\nWrapping up\n\nThanks for reading! The great thing about Morse is its simplicity. Any two things that look, feel, or sound different can carry information. In this post, I showed you how to create playable audio samples with arrays, wrap Streamlit components into functions, cache data between reloads, and create template files to generate pages with similar functionality.\n\nIf you have any questions, please leave them in the comments below, contact me on Twitter or email me.\n\nHappy Streamlit-ing! üéà\n\nAppendix: Complete Morse code\n\nHere is the complete Morse code symbol table used for this project (it gives the character, its Morse equivalent, and a mnemonic to remember its sequence):\n\nmnemonics = {\n    \"A\": (\"A\", \"‚ñÑ ‚ñÑ‚ñÑ\", \"a-PART\"),\n    \"B\": (\"B\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"BOB is the man\"),\n    \"C\": (\"C\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"CO-ca CO-la\"),\n    \"D\": (\"D\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ\", \"DRAC-u-la\"),\n    \"E\": (\"E\", \"‚ñÑ\", \"Eh?!\"),\n    \"F\": (\"F\", \"‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"Fi-tti-PAL-di\"),\n    \"G\": (\"G\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"GOOD GRAV-y\"),\n    \"H\": (\"H\", \"‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"hip-pi-ty hop\"),\n    \"I\": (\"I\", \"‚ñÑ ‚ñÑ\", \"did it\"),\n    \"J\": (\"J\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"in JAWS JAWS JAW\"),\n    \"K\": (\"K\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"KAN-dy KID\"),\n    \"L\": (\"L\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ\", \"los AN-ge-les\"),\n    \"M\": (\"M\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"MA-MA\"),\n    \"N\": (\"N\", \"‚ñÑ‚ñÑ ‚ñÑ\", \"NAV-y\"),\n    \"O\": (\"O\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"HO HO HO\"),\n    \"P\": (\"P\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"a PIZ-ZA pie\"),\n    \"Q\": (\"Q\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"GOD SAVE the QUEEN\"),\n    \"R\": (\"R\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"a RABB-it\"),\n    \"S\": (\"S\", \"‚ñÑ ‚ñÑ ‚ñÑ\", \"s√≠-s√≠-s√≠\"),\n    \"T\": (\"T\", \"‚ñÑ‚ñÑ\", \"TALL\"),\n    \"U\": (\"U\", \"‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"un-der WHERE?!\"),\n    \"V\": (\"V\", \"‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"vic-tor-y VEE\"),\n    \"W\": (\"W\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"the WORLD WAR\"),\n    \"X\": (\"X\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"CROSS at the DOOR\"),\n    \"Y\": (\"Y\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"YELL-ow YO-YO\"),\n    \"Z\": (\"Z\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ\", \"ZU-ZU pe-tal\"),\n    \"0\": (\"0\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"1\": (\"1\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"2\": (\"2\", \"‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"3\": (\"3\", \"‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"4\": (\"4\", \"‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"5\": (\"5\", \"‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"\"),\n    \"6\": (\"6\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"\"),\n    \"7\": (\"7\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"\"),\n    \"8\": (\"8\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ\", \"\"),\n    \"9\": (\"9\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"\"),\n    \".\": (\".\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"a STOP a STOP a STOP\"),\n    \",\": (\",\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ\", \"COM-MA, it's a COM-MA\"),\n    \"?\": (\"?\", \"‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ\", \"it's a QUES-TION, is it?\"),\n    \":\": (\":\", \"‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ\", \"HA-WA-II stan-dard time\"),\n    \"/\": (\"/\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"SHAVE and a HAIR-cut\"),\n    '\"': ('\"', \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"six-TY-six nine-TY-nine\"),\n    \"'\": (\"'\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"and THIS STUFF GOES TO me\"),\n    \";\": (\";\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"\"),\n    \"=\": (\"=\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n    \"+\": (\"+\", \"‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ ‚ñÑ\", \"\"),\n    \"-\": (\"-\", \"‚ñÑ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ ‚ñÑ‚ñÑ\", \"\"),\n}\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Vin√≠cius Oviedo - Streamlit",
    "url": "https://blog.streamlit.io/author/vinicius-oviedo/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Vin√≠cius Oviedo\nData Analyst & LaTeX editor\n2 posts\nWebsite\nAnalyzing real estate properties with Streamlit\n\nA 7-step tutorial on how to make your own real estate app\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nMay 16 2023\nCreating a Time Zone Converter with Streamlit\n\n6 steps on how to build your own converter\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nApril 25 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Analyzing real estate properties with Streamlit",
    "url": "https://blog.streamlit.io/analyzing-real-estate-properties-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnalyzing real estate properties with Streamlit\n\nA 7-step tutorial on how to make your own real estate app\n\nBy Vin√≠cius Oviedo\nPosted in Advocate Posts, May 16 2023\nThe app's working principle\n1. Import the required Python modules\n2. Set up the Matplotlib layout for storytelling\n3. Collect the data\n4. Set up the Streamlit app's page, textual elements, widgets, and sidebar\n5. Create chart 1 (appreciation)\n6. Create chart 2 (price by squared meter)\n7. Finalize the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nAnalyzing residential properties for sale in Brazil can be time-consuming. There are public reports that track real estate prices but no data visualizations to compare assets across cities. So I build the Appreciation of residential properties in Brazil app with Streamlit! It combines Pandas, NumPy, Matplotlib, and Seaborn libraries and a few storytelling techniques to make data analysis more accessible.\n\nIn this post, you'll learn how to build this app in seven steps:\n\nImport the required Python modules\nSet up the Matplotlib layout for storytelling\nGather the data\nSet up the Streamlit app's page, textual elements, widgets, and sidebar\nCreate chart 1 (appreciation)\nCreate chart 2 (price by squared meter)\nFinalize the app\n\nBut first, a bit about the app itself.\n\nüü¢\nCheck out the app here and the repo code here.\nThe app's working principle\n\nThe app uses a single visual type‚Äîthe stripplot:\n\nEach dot ‚ö™ represents a city. The lowest values are at the bottom, and the highest at the top. The user can select a city, highlight it, and compare it with other cities. You can also provide context by using statistical measures such as:\n\nThe first quartile (Q1): represents 25% of the data.\nMedian: the middle value that splits the data in half (can also provide an average).\nThird quartile (Q3): represents 75% of the data.\n\nHere is how it works:\n\nThe user selects a Brazilian city marked with a green dot üü¢ on the map.\nOther cities are represented by white dots ‚ö™, making it easy to compare the selected city with others.\nStatistical measures such as the first quartile, median, and third quartile are displayed, allowing the user to compare the situation of the chosen city against the national average and data distribution.\nThe user can extract insights from the data, such as identifying opportunities when an appreciation rate is above the national average, and the price per square meter is below average.\n\nüü¢\nThe data for the app is based on real estate property prices during the first quarter of 2023. It includes assets from fifty Brazilian cities and provides insights into the appreciation of residential properties and prices per square meter (in BRL).\n\nNow, let's get to coding!\n\n1. Import the required Python modules\n\nImport Streamlit, Numpy, and Pandas (for arrays and data manipulation), and Matplotlib and Seaborn (for data visualization).\n\n# Modules:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport streamlit as st\n\n2. Set up the Matplotlib layout for storytelling\n\nThis step will let you create a unique design for your Matplotlib figures and define the color palette.\n\n# Setup for Storytelling (matplotlib):\nplt.rcParams['font.family'] = 'monospace'\nplt.rcParams['font.size'] = 8\nplt.rcParams['font.weight'] = 'bold'\nplt.rcParams['figure.facecolor'] = '#464545' \nplt.rcParams['axes.facecolor'] = '#464545' \nplt.rcParams['axes.titleweight'] = 'bold'\nplt.rcParams['axes.titlecolor'] = 'black'\nplt.rcParams['axes.titlesize'] = 9\nplt.rcParams['axes.labelcolor'] = 'darkgray'\nplt.rcParams['axes.labelweight'] = 'bold'\nplt.rcParams['axes.edgecolor'] = 'darkgray'\nplt.rcParams['axes.linewidth'] = 0.2\nplt.rcParams['ytick.color'] = 'darkgray'\nplt.rcParams['xtick.color'] = 'darkgray'\nplt.rcParams['axes.titlecolor'] = '#FFFFFF'\nplt.rcParams['axes.titlecolor'] = 'white'\nplt.rcParams['axes.edgecolor'] = 'darkgray'\nplt.rcParams['axes.linewidth'] = 0.85\nplt.rcParams['ytick.major.size'] = 0\n\n3. Collect the data\n# --- App (begin):\nBR_real_estate_appreciation = pd.read_csv('data/BR_real_estate_appreciation_Q1_2023.csv')\nBR_real_estate_appreciation['Annual_appreciation'] = round(BR_real_estate_appreciation['Annual_appreciation'], 2)*100\n\n4. Set up the Streamlit app's page, textual elements, widgets, and sidebar\n\nTo configure the style, set up a header and add an app usage tutorial in the sidebar. You can also define widgets for selecting your city of interest.\n\n# Page setup:\nst.set_page_config(\n    page_title=\"Residential properties (Brazil)\",\n    page_icon=\"üè¢\",\n    layout=\"centered\",\n    initial_sidebar_state=\"collapsed\",\n)\n\n# Header:\nst.header('Appreciation of residential properties in Brazil')\n\nst.sidebar.markdown(''' > **How to use this app**\n\n1. To Select a city (**green dot**).\n2. To compare for the selected city against other 50 cities (**white dots**).\n3. To compare the chosen city against **national average** and the data distribution.\n4. To extract insights as \"An appreciation above national average + price by square meter below average = possible *opportunity*\".\n''')\n\n# Widgets:\ncities = sorted(list(BR_real_estate_appreciation['Location'].unique()))\nyour_city = st.selectbox(\n    'üåé Select a city',\n    cities\n)\n\nselected_city = BR_real_estate_appreciation.query('Location == @your_city')\nother_cities = BR_real_estate_appreciation.query('Location != @your_city')\n\n5. Create chart 1 (appreciation)\n\nThis step refers to the first stripplot, which compares the selected city's annual appreciation to other cities. You can highlight the chosen city in the chart and add reference lines, such as the first quartile, median, and third quartile, to see how it performs.\n\n# CHART 1: Annual appreciation (12 months):\nchart_1, ax = plt.subplots(figsize=(3, 4.125))\n# Background:\nsns.stripplot(\n    data= other_cities,\n    y = 'Annual_appreciation',\n    color = 'white',\n    jitter=0.85,\n    size=8,\n    linewidth=1,\n    edgecolor='gainsboro',\n    alpha=0.7\n)\n# Highlight:\nsns.stripplot(\n    data= selected_city,\n    y = 'Annual_appreciation',\n    color = '#00FF7F',\n    jitter=0.15,\n    size=12,\n    linewidth=1,\n    edgecolor='k',\n    label=f'{your_city}'\n)\n\n# Showing up position measures:\navg_annual_val = BR_real_estate_appreciation['Annual_appreciation'].median()\nq1_annual_val = np.percentile(BR_real_estate_appreciation['Annual_appreciation'], 25)\nq3_annual_val = np.percentile(BR_real_estate_appreciation['Annual_appreciation'], 75)\n\n# Plotting lines (reference):\nax.axhline(y=avg_annual_val, color='#DA70D6', linestyle='--', lw=0.75)\nax.axhline(y=q1_annual_val, color='white', linestyle='--', lw=0.75)\nax.axhline(y=q3_annual_val, color='white', linestyle='--', lw=0.75)\n\n# Adding the labels for position measures:\nax.text(1.15, q1_annual_val, 'Q1', ha='center', va='center', color='white', fontsize=8, fontweight='bold')\nax.text(1.3, avg_annual_val, 'Median', ha='center', va='center', color='#DA70D6', fontsize=8, fontweight='bold')\nax.text(1.15, q3_annual_val, 'Q3', ha='center', va='center', color='white', fontsize=8, fontweight='bold')\n\n# to fill the area between the lines:\nax.fill_betweenx([q1_annual_val, q3_annual_val], -2, 1, alpha=0.2, color='gray')\n# to set the x-axis limits to show the full range of the data:\nax.set_xlim(-1, 1)\n\n# Axes and titles:\nplt.xticks([])\nplt.ylabel('Average appreciation (%)')\nplt.title('Appreciation (%) in the past 12 months', weight='bold', loc='center', pad=15, color='gainsboro')\nplt.legend(loc='center', bbox_to_anchor=(0.5, -0.1), ncol=2, framealpha=0, labelcolor='#00FF7F')\nplt.tight_layout()\n\n6. Create chart 2 (price by squared meter)\n\nHere I refer to the second stripplot, which shows the relationship between the city and price per square meter.\n\n# CHART 2: Price (R$) by m¬≤:\nchart_2, ax = plt.subplots(figsize=(3, 3.95))\n# Background:\nsns.stripplot(\n    data= other_cities,\n    y = 'BRL_per_squared_meter',\n    color = 'white',\n    jitter=0.95,\n    size=8,\n    linewidth=1,\n    edgecolor='gainsboro',\n    alpha=0.7\n)\n# Highlight:\nsns.stripplot(\n    data= selected_city,\n    y = 'BRL_per_squared_meter',\n    color = '#00FF7F',\n    jitter=0.15,\n    size=12,\n    linewidth=1,\n    edgecolor='k',\n    label=f'{your_city}'\n)\n\n# Showing up position measures:\navg_price_m2 = BR_real_estate_appreciation['BRL_per_squared_meter'].median()\nq1_price_m2 = np.percentile(BR_real_estate_appreciation['BRL_per_squared_meter'], 25)\nq3_price_m2 = np.percentile(BR_real_estate_appreciation['BRL_per_squared_meter'], 75)\n\n# Plotting lines (reference):\nax.axhline(y=avg_price_m2, color='#DA70D6', linestyle='--', lw=0.75)\nax.axhline(y=q1_price_m2, color='white', linestyle='--', lw=0.75)\nax.axhline(y=q3_price_m2, color='white', linestyle='--', lw=0.75)\n\n# Adding the labels for position measures:\nax.text(1.15, q1_price_m2, 'Q1', ha='center', va='center', color='white', fontsize=8, fontweight='bold')\nax.text(1.35, avg_price_m2, 'Median', ha='center', va='center', color='#DA70D6', fontsize=8, fontweight='bold')\nax.text(1.15, q3_price_m2, 'Q3', ha='center', va='center', color='white', fontsize=8, fontweight='bold')\n\n# to fill the area between the lines:\nax.fill_betweenx([q1_price_m2, q3_price_m2], -2, 1, alpha=0.2, color='gray')\n# to set the x-axis limits to show the full range of the data:\nax.set_xlim(-1, 1)\n\n# Axes and titles:\nplt.xticks([])\nplt.ylabel('Price (R\\\\$)')\nplt.legend(loc='center', bbox_to_anchor=(0.5, -0.1), ncol=2, framealpha=0, labelcolor='#00FF7F')\nplt.title('Average price (R\\\\$) by $m^2$', weight='bold', loc='center', pad=15, color='gainsboro')\nplt.tight_layout()\n\n7. Finalize the app\n\nHere, you can split the charts into two columns and add a legend. To make it more accessible:\n\nDisplay tabular data for the chosen city in addition to the chart\nProvide information about reference indexes (such as inflation) and authorship\n# Splitting the charts into two columns:\nleft, right = st.columns(2)\n\n# Columns (content):\nwith left:\n    st.pyplot(chart_1)\nwith right:\n    st.pyplot(chart_2)\n\n# Informational text:\nst.markdown('''\n<span style=\"color:white;font-size:10pt\"> ‚ö™ Each point represents a city </span>\n<span style=\"color:#DA70D6;font-size:10pt\"> ‚ñ´ <b> Average value </b></span>\n<span style=\"color:white;font-size:10pt\"> ‚óΩ Lowest values (<b> bottom </b>)\n‚óΩ Highest values (<b> top </b>) <br>\n‚óΩ **Q1** (first quartile): where 25% of data falls under\n‚óΩ **Q3** (third quartile): where 75% of data falls under\n</span>\n\n''',unsafe_allow_html=True)\n\n# Showing up the numerical data (as a dataframe):\nst.dataframe(\n    BR_real_estate_appreciation.query('Location == @your_city')[[\n      'Location', 'Annual_appreciation', \n      'BRL_per_squared_meter']]\n)\n\n# Adding some reference indexes:\nst.markdown(''' > **Reference indexes (inflation):**\n\n* IPCA: **6%** (National Broad Consumer Price Index)\n* IGP-M: **4%** (General Market Price Index)\n\n> Data based on public informs that accounts residential properties for 50 Brazilian cities (first quarter of 2023).\n''')\n\n# Authorship:\nst.markdown('---')\n# here you can add the authorship and useful links (e.g., Linkedin, GitHub, and so forth)\nst.markdown('---')\n# --- (End of the App)\n\n\nFinally, let's incorporate a dark theme. Note that the layout customizations in Matplotlib will be consistent with the theme's color palette.\n\n[theme]\nprimaryColor=\"#00FF7F\"\nbackgroundColor=\"#464545\"\nsecondaryBackgroundColor=\"#2b2b29\"\ntextColor=\"#fbfbfb\"\nfont=\"serif\"\n\nWrapping up\n\nNow you can use Streamlit to analyze the appreciation of residential properties by employing statistics, data visualization, and storytelling. Although this app is for Brazilian real estate properties, you can apply the same methodology to any country.\n\nIf you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, or Medium.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Semantic search, Part 1: Implementing cosine similarity",
    "url": "https://blog.streamlit.io/semantic-search-part-1-implementing-cosine-similarity/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSemantic search, Part 1: Implementing cosine similarity\n\nWrangling Foursquare data and implementing semantic search in Snowflake\n\nBy Dave Lin\nPosted in Snowflake powered ‚ùÑÔ∏è, May 17 2023\nWhy did I make this app?\nData wrangling\nStep 1: Install the Foursquare NYC dataset from the Snowflake Marketplace\nStep 2: Set up a new database and schema\nStep 3: Create borough and (borough, neighborhood) relationships\nStep 4. Extract categories\nStep 5: Embed Foursquare categories\nStep 6: Create a cache version of Foursquare data\nImplementations\nImplementation 1: Python UDF using an existing function\nImplementation 2: Python UDF with custom implementation\nImplementation 3: JavaScript UDF with custom implementation\nImplementation 4: Native SQL\nPerformance evaluation\nScalability evaluation\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIt‚Äôs so annoying trying to think of things to do. Sometimes you just want to type ‚Äúepic night out‚Äù into Google Search and get what you‚Äôre looking for, right? I struggled with the same. So I built a semantic search app. It finds Foursquare venues in NYC leveraging Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data on the Snowflake Marketplace.\n\nThe semantic search lets users search for venues based on their intent (and not translating their intent to keywords). For example, users can search for venues for an \"epic night out\" or \"lunch date spots\" and find venues in their specified neighborhoods with Foursquare venue categories that are semantically closest to what they‚Äôre looking for.\n\nIn this first part of a two-part blog series, I‚Äôll walk you through how I wrangled the data and implemented semantic search in Snowflake.\n\nüî¥\nWant to dive right in? Check out the code and the app. The Streamlit application will be removed after the Snowflake Summit in June due to cost considerations.\nWhy did I make this app?\n\nI stumbled upon semantic search as I was exploring generative AI use cases. At its core, most semantic search apps use cosine similarity as a metric to determine which documents in a corpus are most similar to a user‚Äôs query. As I learned more about it, my inner Snowflake fanboy thought Snowflake‚Äôs impressive computational power and near-infinite scalability would be ideal for such a task! I wanted to power a semantic search app using Snowflake as an alternative to a vector database. Streamlit's Snowflake Summit Hackathon offered a perfect opportunity to do that.\n\nData wrangling\nStep 1: Install the Foursquare NYC dataset from the Snowflake Marketplace\n\nBefore we get started, install the free Foursquare Places - New York City Sample from the Snowflake Marketplace (if you don‚Äôt have access to Snowflake, you can sign up for a 30-day free trial here). I shortened the default database to foursquare_nyc during installation.\n\nAfter you install it, the data set will appear in the Snowflake UI:\n\nFoursquare provides a single view containing basic information about venues in NYC. We aim to leverage Snowflake to perform a semantic search of Foursquare venue categories. To achieve this, the columns we‚Äôre particularly interested in are fsq_category_labels and fsq_category_ids. fsq_category_labels contains an array of arrays. The outer array represents the list of categories. The inner array describes the hierarchy of the category, where the first element represents the root category and the last element represents the leaf category. fsq_category_ids contains an array of IDs for the leaf categories in the fsq_category_labels column.\n\nStep 2: Set up a new database and schema\n\nWe‚Äôll create a new database and schema to house our wrangled data:\n\n-- Set up a new database and schema where we are going to house auxiliary data \nCREATE DATABASE foursquare;\n-- Create a new schema\nCREATE SCHEMA main;\n\nStep 3: Create borough and (borough, neighborhood) relationships\n\nFrom a user experience perspective, it‚Äôd be inefficient to comb through all NYC neighborhoods in each of the five boroughs. Also, querying a list of venues in a list of neighborhoods from the Foursquare dataset would be computationally expensive, given that the neighborhoods are stored as a string array in the neighborhood column. So we‚Äôll create normalized tables to store information about NYC boroughs, neighborhoods, and which neighborhoods are in which boroughs.\n\nFirst, we‚Äôll create and populate a borough_lookup table:\n\nCREATE TRANSIENT TABLE borough_lookup (\nid number autoincrement,\nname varchar\n);\n\nINSERT INTO borough_lookup(name) values\n('Brooklyn'),\n('Bronx'),\n('Manhattan'),\n('Queens'),\n('Staten Island');\n\n\nNext, we‚Äôll create and populate a neighborhood_lookup table:\n\nCREATE TRANSIENT TABLE neighborhood_lookup (\nid number autoincrement,\nname varchar\n);\n\nINSERT INTO neighborhood_lookup(name)\nSELECT DISTINCT n.value::string\nFROM foursquare_nyc.standard.places_us_nyc_standard_schema s,\ntable(flatten(s.neighborhood)) n\nORDER BY 1;\n\n\nNext, we‚Äôll create a borough_neighborhood table to store our (borough, neighborhood) mapping by:\n\nCreating a temporary table to store the manually curated (borough, neighborhood) mapping (find the exact insert statement here):\nCREATE OR REPLACE TRANSIENT TABLE z_borough_neighborhood(\nborough_name varchar,\nneighborhood_name varchar\n);\n\nINSERT INTO z_borough_neighborhood(borough_name, neighborhood_name) values\n('Bronx','Allerton'),\n('Bronx','Bathgate'),\n('Bronx','Baychester'),\n('Bronx','Bedford Park'),\n('Bronx','Belmont'),\n...\n\nCreating the final mapping table by joining the temporary mapping table with the lookup tables:\nCREATE OR REPLACE TRANSIENT TABLE borough_neighborhood AS\nSELECT\n   b.id borough_id\n   , n.id neighborhood_id\nFROM z_borough_neighborhood bp\nINNER JOIN borough_lookup b ON bp.borough_name = b.name\nINNER JOIN neighborhood_lookup n ON bp.neighborhood_name = n.name\nORDER BY b.id, n.id;\n\n\nFinally, we‚Äôll create a place_neighborhood table:\n\nCREATE OR REPLACE TRANSIENT TABLE place_neighborhood AS\nWITH place_neighborhood AS (\nSELECT DISTINCT\n   s.fsq_id\n   , n.value::string str\nFROM foursquare_nyc.standard.places_us_nyc_standard_schema s,\ntable(flatten(s.neighborhood)) n\n)\nSELECT pn.fsq_id, n.id neighborhood_id\nFROM place_neighborhood pn\nINNER JOIN neighborhood_lookup n ON pn.str = n.name\nORDER BY id, pn.fsq_id;\n\nStep 4. Extract categories\n\nNext, we‚Äôll extract the categories from fsq_category_labels and fsq_category_ids columns:\n\n-- Extract Foursquare category IDs \nCREATE OR REPLACE TRANSIENT TABLE z_category_id AS\nWITH data AS (\nSELECT\n   DISTINCT\n   s.fsq_category_labels\n   , n.seq\n   , n.index\n   , n.value category_id\n   , l.seq\n   , l.index\n   , l.value::string category\nFROM foursquare_nyc.standard.places_us_nyc_standard_schema s,\ntable(flatten(s.fsq_category_ids)) n,\ntable(flatten(s.fsq_category_labels)) l\nWHERE n.index = l.index\nORDER BY n.seq, n.index, l.seq, l.index\n)\nSELECT DISTINCT to_number(category_id) category_id, category FROM data ORDER BY category_id;\n\n-- Extract Foursquare categories\nCREATE OR REPLACE TRANSIENT TABLE z_category_lookup AS\nSELECT category_id, value::string category \nFROM z_category_id z\n, table(flatten(input => parse_json(z.category))) c\nQUALIFY row_number() OVER (PARTITION BY seq ORDER BY index DESC) = 1\nORDER BY category_id;\n\n-- Set up Foursquare category lookup tables\nCREATE OR REPLACE TRANSIENT TABLE category_lookup AS\nwith hierarchy AS (\nSELECT c.seq, c.index, c.value::string category \nFROM z_category_id z\n, table(flatten(input => parse_json(z.category))) c\n)\n, data AS (\nSELECT\n   h.*\n   , c.category_id\n   , lag(c.category_id) OVER (PARTITION BY h.seq ORDER BY h.index) parent_category_id\n   , first_value(c.category_id) OVER (PARTITION BY h.seq ORDER BY h.index) root_category_id\nFROM hierarchy h\nINNER JOIN z_category_lookup c ON h.category = c.category\n)\nSELECT DISTINCT category, category_id, parent_category_id, root_category_id\nFROM data\nORDER BY root_category_id, category_id;\n\nStep 5: Embed Foursquare categories\n\nIn this step, we‚Äôll embed Foursquare categories with OpenAI‚Äôs text embedding API. To facilitate the semantic search, we‚Äôll compute cosine similarities between the embeddings of the user query (e.g., ‚ÄúEpic Night Out‚Äù) vs. the embeddings of each category. This way, we can return the top suggested Foursquare categories to the app, which will look up the venues with the semantically suggested categories in the user-specified neighborhoods.\n\nFirst, we‚Äôll add a new embedding column to the category_lookup table:\n\nALTER TABLE category_lookup add column embedding varchar;\n\n\nNext, we‚Äôll write a script that uses OpenAI text embedding API to embed the Foursquare venue categories and store the embedding vectors in the newly created column. I used a simple Python script to connect to Snowflake using the Snowflake Python Connector (find it here). It takes about 20 minutes to run.\n\nYou can use the following Snowflake query to check on the overall process:\n\nSELECT\n   COUNT(category_id) total_categories\n   , COUNT(DISTINCT CASE WHEN embedding IS NOT NULL THEN category_id END) categories_embedded\nFROM category_lookup;\n\nStep 6: Create a cache version of Foursquare data\n\nGiven that we‚Äôll want to look up Foursquare venues by their fsq_id quickly, we‚Äôll create a cached version of the Foursquare venue data (ordered by fsq_id):\n\nCREATE OR REPLACE TRANSIENT TABLE place_lookup AS\nSELECT * FROM foursquare_nyc.standard.places_us_nyc_standard_schema\nORDER BY fsq_id;\n\n\nAfter all the data wrangling, we have transformed the original Foursquare view into the following relational tables:\n\nWith data wrangling out of the way, let‚Äôs move on to the fun stuff‚Ä¶\n\nImplementations\n\nThe goal is to use Snowflake to compute cosine similarities between the embeddings of the user query (such as \"Epic Night Out\") and the embeddings of each Foursquare venue category. This will let us return the top suggested categories to the app, which can then look up venues with the suggested categories in the user-specified neighborhoods.\n\nInitially, I planned to create a scalar User-Defined Function (UDF) for performing a semantic search via a quick table scan. But due to performance reasons (explained in the performance section), I abandoned this approach in favor of native SQL implementations. This section will cover the four implementations I explored: two Python scalar UDFs, one JavaScript scalar UDF, and native SQL. In the following sections, I will discuss their performances and scalability.\n\nImplementation 1: Python UDF using an existing function\n\nMy first attempt was to wrap a readily available cosine similarity function within a Python UDF:\n\nCREATE OR REPLACE FUNCTION cosine_similarity_score(x array, y array)\nreturns float \nlanguage python \nruntime_version = '3.8'\npackages = ('scikit-learn', 'numpy')\nhandler = 'cosine_similarity_py'\nas \n$$\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef cosine_similarity_py(x, y):\n  x = np.array(x).reshape(1,-1)\n  y = np.array(y).reshape(1,-1)\n  cos_sim = cosine_similarity(x, y)\n  return cos_sim \n$$;\n\n\nThe function above first transforms the 1D list into a 2D vector. It then uses scikit-learn's cosine similarity function to compute the similarity score between the two vectors.\n\nImplementation 2: Python UDF with custom implementation\n\nI noticed that OpenAI's embedding vectors normalize to length 1, which means that cosine similarity can be calculated using the dot product between the two vectors. So, I tried to write a Python UDF that doesn't require the scikit-learn package:\n\nCREATE OR REPLACE FUNCTION cosine_similarity_score_2(x array, y array)\nreturns float \nlanguage python \nruntime_version = '3.8'\npackages = ('numpy')\nhandler = 'cosine_similarity_py'\nas \n$$\nimport numpy as np\n\ndef cosine_similarity_py(x, y):\n  x = np.array(x)\n  y = np.array(y)\n  return np.dot(x,y)\n$$;\n\n\nThe function above transforms the 1D lists into NumPy arrays and computes the dot products of the two input arrays.\n\nImplementation 3: JavaScript UDF with custom implementation\n\nI also implemented a JavaScript UDF version, wondering how it would perform:\n\nCREATE OR REPLACE FUNCTION cosine_similarity_score_js(x array, y array)\n  RETURNS float\n  LANGUAGE JAVASCRIPT\nAS\n$$\n  var score = 0;\n  for (var i = 0; i < X.length; i++) {\n    score += X[i] * Y[i];\n  }\n  return score;\n$$\n;\n\nImplementation 4: Native SQL\n\nFinally, I decided to implement cosine similarity directly with SQL. Before writing the query, I flattened the JSON array category embedding values and stored them in the category_embed_value table:\n\nCREATE OR REPLACE TRANSIENT TABLE category_embed_value AS \nWITH leaf_category AS (\n    SELECT category_id \n    FROM category_lookup \n    EXCEPT \n    SELECT category_id \n    FROM category_lookup \n    WHERE category_id IN (SELECT DISTINCT parent_category_id FROM category_lookup)\n)\nSELECT \n    l.category_id  \n    , n.index \n    , n.value \nFROM category_lookup l \n, table(flatten(input => parse_json(l.embedding))) n \nWHERE l.category_id IN (SELECT category_id FROM leaf_category)\nORDER BY l.category_id, n.index;\n\n\nThen I computed cosine similarities between a test input embedding vector vs. embeddings of all categories with the following SQL:\n\nWITH base_search AS (\n-- Karaoke Bar \nSELECT embedding FROM category_lookup where category_id = 13015\n)\n, search_emb AS (\nSELECT \n    n.index\n    , n.value \nfrom base_search l \n, table(flatten(input => parse_json(l.embedding))) n \nORDER BY n.index\n)\n, search_emb_sqr AS (\nSELECT index, value \nFROM search_emb r \n)\nSELECT \n    v.category_id \n    , SUM(s.value * v.value) / SQRT(SUM(s.value * s.value) * SUM(v.value * v.value)) cosine_similarity \nFROM search_emb_sqr s \nINNER JOIN category_embed_value v ON s.index = v.index \nGROUP BY v.category_id\nORDER BY cosine_similarity DESC \nLIMIT 5;\n\nPerformance evaluation\n\nI evaluated the performance of each implementation using an X-Small warehouse. The test was to find categories (out of 853 Foursquare venue categories) that most closely match the embedding of a test category. I tested each implementation twice (and made sure to wait for the warehouse to spin down before moving on to a different implementation).\n\nI tested the first three implementations using the following query:\n\nWITH user_embedding AS (\n-- Karaoke Bar \nSELECT embedding FROM category_lookup where category_id = 13015\n)\nSELECT FUNCTION_NAME(parse_json(d.embedding), parse_json(c.embedding)) cosine_similarity, c.category_id  \nFROM user_embedding d, \ncategory_lookup c\nORDER BY cosine_similarity DESC \nLIMIT 5;\n\n\nI verified the native SQL implementation using the query mentioned above.\n\nHere are the test results:\n\nPython UDF 1: 9 seconds for the initial query, 5 seconds on the subsequent run\nPython UDF2: 7.5 seconds initially, 4 seconds on the subsequent run\nJavascript UDF: 11 seconds, 11 seconds on the subsequent run\nNative SQL: 1.2 seconds, 564 milliseconds on the subsequent run (due to 24-hour query caching)\n\nI was surprised by the significant performance difference between the UDF and SQL implementation (UDFs didn‚Äôt seem to benefit from Snowflake's native query caching). I expected some language overhead for the UDFs, but not an 8x difference. Given the performance numbers, I proceeded with the native SQL implementation for the app.\n\nScalability evaluation\n\nSemantically searching across 853 categories was exciting, but how scalable is it? To test scalability, I ran the native SQL solution against dummy datasets containing 10K, 100K, and 1M documents.\n\nI created this SQL dummy table to hold embedding values for 10K, 100K, and 1M documents:\n\nCREATE OR REPLACE TRANSIENT TABLE test_embed_value_10K AS \nWITH dummy_data AS (\n  SELECT\n    SEQ4() AS id,\n    UNIFORM(1, 1000, SEQ4()) AS category_id,\n    UNIFORM(1, 1536, SEQ4()) AS index,\n    UNIFORM(0, 1, SEQ4()) AS value\n  FROM\n   -- Each embedding vector contains 1536 numbers \n    TABLE(GENERATOR(ROWCOUNT => 1536 * 10000))\n)\n\nSELECT *\nFROM dummy_data\nORDER BY category_id, index;\n\n\nI adjusted the row count in the TABLE(GENERATOR(ROWCOUNT => ... clause and the table name to create tables for 100K and 1M documents.\n\nI used the same query (but swapped out category_embed_value with the test table name) to evaluate the scalability of the SQL implementation. Here are the results on an X-Small warehouse:\n\n10K: 1.4 seconds\n100K: 4.6 seconds\n1M: 36 seconds\n\nOne of Snowflake‚Äôs benefits is its scalability. Performances can be further improved by using a larger warehouse.\n\nWrapping up\n\nFrom this exploration, we show that Snowflake can not only power a semantic search application but also performs well when searching through up to 10K documents. Compared to keyword-based search, semantic search provides a better user experience by letting users search with intent or keywords. Ambiguous searches yield a diverse array of suggestions, while targeted searches continue to return targeted results. For example, \"epic night out\" returns ‚Äúnight club‚Äù, ‚Äúbeer bar‚Äù, and ‚Äúescape room‚Äù. \"Dim sum\" returns \"dim sum restaurants\".\n\nWith more time, I‚Äôd have refined the project by creating a Snowflake external function to call OpenAI's embedding API, allowing me to embed new documents directly within Snowflake. Also, I‚Äôd set up a stored procedure and a scheduled task to automatically refresh the cached Foursquare data.\n\nIf you're already using Snowflake, conducting reasonably-sized semantic searches within it is possible, rather than setting up additional ETL jobs to push your data to a vector database. A capacity of 10K documents is more than enough for many applications. For example, you can search across embeddings of a book's paragraphs or chat sessions (stored in logically segregated tables for each natural grouping). Snowflake can still be a viable solution for larger document corpora depending on your use case and the compute resources you're willing to invest.\n\nStay tuned for Part 2, where I will discuss implementing the rest of the application with Snowflake and Streamlit. I hope you enjoyed my second article (my first article was about building GPT Lab with Streamlit). Connect with me on Twitter or Linkedin. I'd love to hear from you.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Display a race on a live map üèÉ",
    "url": "https://blog.streamlit.io/display-a-race-on-a-live-map/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDisplay a race on a live map üèÉ\n\nCreate a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library\n\nBy Mark Needham\nPosted in Advocate Posts, June 22 2023\nWhat's Park Run?\nArchitecture diagram\nData generation\nQuerying data in Apache Kafka\nIngesting data into Apache Pinot\nBuilding the real-time dashboard\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey community, üëã\n\nMy name is Mark Needham, and I‚Äôm a Developer Advocate at StarTree. I work on Apache Pinot, a real-time distributed OLAP datastore. It‚Äôs purpose-built to deliver scalable real-time analytics with low latency (sometimes referred to as a way of querying Apache Kafka).\n\nI love making demos of streaming data analytics. And I love running. So I built a real-time Streamlit dashboard on top of imaginary race data.\n\nIn this post, I‚Äôll show you:\n\nHow to create a data simulator using Python‚Äôs Twisted library\nHow to ingest that data into Apache Kafka\nHow to pull data from Apache Kafka into Apache Pinot\nHow to query Apache Pinot to visualize the data in Streamlit\n\n\nüèÉ\nThe app isn't public because it has several moving parts. I'd need to have a hosted version of Apache Kafka and Apache Pinot (doable with SaaS services), but the data generator is trickier. So you can clone the repo and get it running on your own machine.\nWhat's Park Run?\n\nBefore we delve into the details, let me give you some background on the data we'll be generating.\n\nPark Run is an organization that hosts free weekly community events in the UK and some other countries. They include 5k runs on Saturday mornings and 2k runs on Sunday mornings. Each course has a page with information about it, directions, and a map.\n\nHere is the Kingston course page:\n\nBy clicking through to the map, you can download the course in KML format. This lets you extract the latitude and longitude coordinates that make up the route, and the start and finish locations. With this data, you can create simulated runs based on the route coordinates.\n\nArchitecture diagram\n\nLet's start with an architecture diagram that displays the app‚Äôs components:\n\nThere are five components:\n\nCreate Race: A Streamlit app that configures parameters for a new race.\nData Generator: Processes new race requests, generates locations for each competitor, and publishes the events to a Kafka topic.\nApache Kafka: Streaming data platform that acts as the source of truth for locations and races.\nApache Pinot: Real-Time OLAP database that consumes data from Kafka.\nReal-Time Dashboard: A Streamlit app to show what‚Äôs happening in each race including a leadership and a live map.\nData generation\n\nThe data generation process begins with creating a race using the Streamlit app. It‚Äôs used to configure the parameters for a new race. You can choose the course, the number of competitors, the fastest and slowest potential pace, and the number of competitors that should get stuck in a geo-fenced part of the course.\n\nHere is what it looks like:\n\nAfter selecting the parameters, click on Generate race. This sends a POST request to the Data Generator. It has two components:\n\nAn HTTP server that handles new race requests. Its resource generates all the locations (with associated timestamps) of a competitor for a race and writes them to an in-memory map.\nAn event loop that runs every second and iterates over active competitors stored in the in-memory map. It extracts the competitor's latest location that occurred in the past and publishes the location to Kafka.\nQuerying data in Apache Kafka\n\nkcat (previously known as Kafka cat) is an open-source command-line utility used for producing, consuming, and managing Apache Kafka messages. Use it to verify that your data has been successfully sent to Kafka.\n\nThe command below retrieves one record from the parkrun topic and pipes the output into jq, a command-line JSON processor:\n\nkcat -C -b localhost:9092 -t parkrun -c1 | jq\n\n{\n  \"runId\": \"637648ff-46fa-464b-9d80-acb647e7aa41\",\n  \"eventId\": \"07803c75-cae5-4e69-8361-3bda41aa686f\",\n  \"competitorId\": 551870,\n  \"rawTime\": 0,\n  \"timestamp\": \"2023-05-25 10:17:49\",\n  \"lat\": 51.45034,\n  \"lon\": -0.29499,\n  \"distance\": 0,\n  \"course\": \"richmond\"\n}\n\nIngesting data into Apache Pinot\n\nNext, we‚Äôll transfer the data from Kafka to Pinot. Pinot stores it in tables that can contain any number of columns. To create a table, you need to provide a schema and table configuration.\n\nLet's begin with the schema for the parkrun table:\n\n{\n  \"schemaName\": \"parkrun\",\n  \"primaryKeyColumns\": [\"competitorId\"],\n  \"dimensionFieldSpecs\": [\n    {\"name\": \"runId\", \"dataType\": \"STRING\"},\n    {\"name\": \"eventId\", \"dataType\": \"STRING\"},\n    {\"name\": \"competitorId\", \"dataType\": \"LONG\"},\n    {\"name\": \"rawTime\", \"dataType\": \"INT\"},\n    {\"name\": \"lat\", \"dataType\": \"DOUBLE\"},\n    {\"name\": \"lon\", \"dataType\": \"DOUBLE\"},\n    {\"name\": \"location\", \"dataType\": \"BYTES\"},\n    {\"name\": \"course\", \"dataType\": \"STRING\"}\n  ],\n  \"metricFieldSpecs\": [{\"name\": \"distance\", \"dataType\": \"DOUBLE\"}],\n  \"dateTimeFieldSpecs\": [\n    {\n      \"name\": \"timestamp\",\n      \"dataType\": \"TIMESTAMP\",\n      \"format\": \"1:MILLISECONDS:EPOCH\",\n      \"granularity\": \"1:MILLISECONDS\"\n    }\n  ]\n}\n\n\nThe columns in the schema are categorized using a similar language to data warehousing.\n\nThere are three categories:\n\nDimension columns: Used in slice and dice operations, such as when using the SQL WHERE and GROUP BY clauses.\nMetric columns: Represent quantitative data and are used in aggregations, such as when using the SQL SUM, MIN, MAX, COUNT, and AVG functions. You can also filter them.\nDateTime columns: Represent time columns in the data. There can be many of them in a table, but only one can be treated as primary. They can also be used with the WHERE or GROUP BY clauses.\n\nApache Pinot aligns and ingests data from the source (in this case, Kafka messages) based on matching property names with its column names. In other words, if a Kafka event includes a property labeled ‚Äúfoo,‚Äù and there exists a corresponding ‚Äùfoo‚Äù column in the Pinot schema, Pinot will automatically ingest the value of ‚Äúfoo‚Äù from the Kafka event into its ‚Äúfoo‚Äù column.\n\nThe only field that doesn't have a corresponding source property is location. Let‚Äôs populate that using a transformation function:\n\n{\n  \"tableName\": \"parkrun\",\n  \"tableType\": \"REALTIME\",\n  \"segmentsConfig\": {\n    \"timeColumnName\": \"timestamp\",\n    \"schemaName\": \"parkrun\",\n    \"replication\": \"1\",\n    \"replicasPerPartition\": \"1\"\n  },\n  \"tenants\": {\"broker\": \"DefaultTenant\", \"server\": \"DefaultTenant\"},\n  \"tableIndexConfig\": {\n    \"loadMode\": \"MMAP\",\n    \"streamConfigs\": {\n      \"streamType\": \"kafka\",\n      \"stream.kafka.topic.name\": \"parkrun\",\n      \"stream.kafka.broker.list\": \"kafka-run:9093\",\n      \"stream.kafka.consumer.type\": \"lowlevel\",\n      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\",\n      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n    },\n  },\n  \"upsertConfig\": {\"mode\": \"FULL\"},\n  \"routing\": {\"instanceSelectorType\": \"strictReplicaGroup\"},\n  \"ingestionConfig\": {\n    \"batchIngestionConfig\": {\n      \"segmentIngestionType\": \"APPEND\",\n      \"segmentIngestionFrequency\": \"DAILY\"\n    },\n    \"transformConfigs\": [\n      {\n        \"columnName\": \"location\",\n        \"transformFunction\": \"toSphericalGeography(stPoint(lon, lat))\"\n      }\n    ]\n  },\n  \"metadata\": {}\n}\n\n\nThis table has a tableType of REALTIME, which means Pinot expects to see configuration for the streaming data platform under tableIndexConfig.streamConfigs. Set your streamType to kafka and then specify a couple of configuration parameters to indicate how messages should be decoded.\n\nDefine the following parameters:\n\nstream.kafka.broker.list indicates where the Kafka broker is running\n[stream.kafka.topic.name](<http://stream.kafka.topic.name>) is the name of your topic\nstream.kafka.consumer.type indicates how Kafka partitions should be consumed‚Äîlowlevel means that Pinot should consume them in parallel\nstream.kafka.consumer.prop.auto.offset.reset specifies where consumption of the Kafka topic should begin‚Äîsmallest means to start from the earliest available offset\n\nNext, define a transformation configuration under ingestionConfig.transformConfigs to populate the location column. This function creates a geography object to represent the latitude/longitude location.\n\nThis table is also using Pinot‚Äôs upsert functionality (upsertConfig.mode). When you query the table, by default, you only get the most recent record for a given primary key. You‚Äôll use this functionality because, for the majority of queries, you want to see only the latest location of each competitor.\n\nThe two other tables also have schema and table configurations (learn more here):\n\nraces keeps track of all the races\ncourses has the course information\nBuilding the real-time dashboard\n\nLet's take a look at the real-time dashboard Streamlit app:\n\nCreate an [app.py](<http://app.py>) file and run streamlit run app.py from the terminal.\nUse pinotdb to query data from Apache Pinot (before querying, the user must select a race to follow).\nPopulate a selectbox with data from a query against the races table:\nfrom pinotdb import connect\nimport pandas as pd\nimport streamlit as st\n\nconn = connect(host='localhost', port=8099, path='/query/sql', scheme='http')\ncurs = conn.cursor()\ncurs.execute(\"\"\"\nselect runId, course, ToDateTime(startTime, 'YYYY-MM-dd HH:mm:ss') AS startTime,\n       lookup('courses','longName','shortName',course) as courseName\nfrom races\norder by startTime DESC\n\"\"\")\ndf = pd.DataFrame(curs, columns=[item[0] for item in curs.description])\nCOURSES_MAP = {pair[0]: f\"{pair[3]} at {pair[2]}\" for pair in df.values.tolist()}\n\nrun_id = st.selectbox(\n    'Select event:', df['runId'].astype(str),\n    format_func=lambda x:COURSES_MAP[ x ]\n)\n\n\nThe query retrieves the ten most recent races in descending order. To retrieve the full name of the course, you‚Äôll perform an in-memory join using Pinot's lookup function.\n\nRefresh the app for the dropdown menu to appear and select a race:\n\nNext, view the leaderboard to see who is winning and how far they are from the finish line.\n\nAdd the following code to the app:\n\ncurs = conn.cursor()\ncurs.execute(\"\"\"\nselect competitorId,\n    distance AS distanceCovered,\n    round(%(courseDistance)d - distance, 1) AS distanceToGo,\n    ToDateTime(1000 / (distance / rawTime) * 1000, 'HH:mm:ss') AS pacePerKm,\n    ToDateTime(rawTime * 1000, 'mm:ss') AS raceTime\nfrom parkrun\nWHERE runId = %(runId)s\nORDER BY distanceToGo, rawTime\nlimit 10\n\"\"\", {\"courseDistance\": distance, \"runId\": run_id})\ndf = pd.DataFrame(curs, columns=[item[0] for item in curs.description])\n\nst.header(\"Leaderboard\")\nstyler = df.style.hide(axis='index')\nst.write(styler.to_html(), unsafe_allow_html=True)\n\n\nHere is the race-in-progress leaderboard:\n\nUse the experimental_rerun function for the app to refresh automatically.\n\nThe code is located near the top of the app:\n\nif not \"sleep_time\" in st.session_state:\n    st.session_state.sleep_time = 2\n\nif not \"auto_refresh\" in st.session_state:\n    st.session_state.auto_refresh = True\n\nauto_refresh = st.sidebar.checkbox('Auto Refresh?', st.session_state.auto_refresh)\n\nif auto_refresh:\n    number = st.sidebar.number_input('Refresh rate in seconds', value=st.session_state.sleep_time)\n    st.session_state.sleep_time = number\n\n\nThis code is located at the end:\n\nif auto_refresh:\n    time.sleep(number)\n    st.experimental_rerun()\n\n\nYou can find it in a sidebar:\n\nThe best feature of this app is the live map that displays the location of each competitor. here is how to do it:\n\nCreate it using Python's Folium library and render it to Streamlit using the streamlit-folium package.\nSave the course map (a geo-fenced area) and the start and end points in Pinot using WKT format.\nUse the Shapely library to extract an array of x and y coordinates.\n\nHere is the code for converting WKT to coordinates:\n\nfrom shapely import wkt\n\nstart_wkt = \"POINT (-0.063253 51.41917000000001)\"\nend_wkt = \"POINT (-0.064283 51.419324)\"\n\nx_start, y_start = wkt.loads(start_wkt).coords.xy\nx_end, y_end = wkt.loads(end_wkt).coords.xy\n\n\nNext, create a Folium map and add these points to the map:\n\nfrom streamlit_folium import st_folium\n\nm = folium.Map()\n\nfolium.Marker(location=(y_start[0], x_start[0]),\n  icon=folium.Icon(color=\"green\", icon=\"flag\"), popup=\"Start\").add_to(m)\nfolium.Marker(location=(y_end[0], x_end[0]),\n  icon=folium.Icon(color=\"red\", icon=\"flag\"), popup=\"Finish\").add_to(m)\n\n\nThen get the coordinates for the course map and geofenced area (I have hardcoded those locations):\n\n# Example values (these are derived from the database)\nx = [-0.064245, -0.064524, -0.065779, -0.065801, -0.065865, -0.06579, -0.065854, -0.065962, -0.065672, -0.065329, -0.064824, -0.064277, -0.063977, -0.063805, -0.063719, -0.063644, -0.063859, -0.063988, -0.064181, -0.065178, -0.065887, -0.066606, -0.067292, -0.068354, -0.06859, -0.06903, -0.068976, -0.067281, -0.066777, -0.066359, -0.06564, -0.065597, -0.065608, -0.06579, -0.065822, -0.066858, -0.068118, -0.068837, -0.069143, -0.069041, -0.06961, -0.070479, -0.070725, -0.070618, -0.070522, -0.0705, -0.070511, -0.07065, -0.071326, -0.071637, -0.071659, -0.071133, -0.071173, -0.071857, -0.072002, -0.072238, -0.073204, -0.074212, -0.074598, -0.074813, -0.075628, -0.075757, -0.075178, -0.073912, -0.073408, -0.072742, -0.071981, -0.071726, -0.071641, -0.071388, -0.071219, -0.070994, -0.070806, -0.070656, -0.070475, -0.070213, -0.069919, -0.069695, -0.069418, -0.069199, -0.068542, -0.067217, -0.066694, -0.066166, -0.066032, -0.065114, -0.064406, -0.064556, -0.065141, -0.065726, -0.063215]\ny = [51.419386, 51.419459, 51.419754, 51.419673, 51.419513, 51.419366, 51.419178, 51.418984, 51.41879, 51.418757, 51.41877, 51.41877, 51.41873, 51.418777, 51.418676, 51.418496, 51.418288, 51.418061, 51.417967, 51.417606, 51.417271, 51.41695, 51.416964, 51.417017, 51.417245, 51.417592, 51.417706, 51.418583, 51.419285, 51.420088, 51.421523, 51.421998, 51.42245, 51.422925, 51.423045, 51.422878, 51.422731, 51.422838, 51.423223, 51.423601, 51.423955, 51.424136, 51.424029, 51.423902, 51.423721, 51.423607, 51.42342, 51.423306, 51.422664, 51.422677, 51.422771, 51.423681, 51.424216, 51.424335, 51.423654, 51.4233, 51.421768, 51.420255, 51.419626, 51.41954, 51.420021, 51.420188, 51.421078, 51.423099, 51.423942, 51.423962, 51.423668, 51.424373, 51.424553, 51.424676, 51.424928, 51.425092, 51.42526, 51.425281, 51.42532, 51.425337, 51.425293, 51.425186, 51.425009, 51.424812, 51.4245, 51.424397, 51.424232, 51.423997, 51.423134, 51.422836, 51.422607, 51.4219, 51.421269, 51.419867, 51.419232]\nx_geo = [-0.0651347637176514, -0.0643622875213623, -0.0632894039154053, -0.0638902187347412, -0.0645339488983154, -0.0656068325042725, -0.0676238536834717, -0.0686323642730713, -0.0679242610931396, -0.0672805309295654, -0.0651347637176514]\ny_geo = [51.41916166790023, 51.41886727626769, 51.41846583007674, 51.417997471730985, 51.41767630894881, 51.416900156242406, 51.41687339212095, 51.41715441461497, 51.41776998166006, 51.41878698731156, 51.41916166790023]\n   \n\nloc = [(point[1], point[0]) for point in zip(x_geo, y_geo)]\nlat = sum([point[0] for point in loc]) / len(loc)\nlon = sum([point[1] for point in loc]) / len(loc)\nfolium.PolyLine(loc, color='red', weight=2, opacity=0.8).add_to(m)\n\nloc = [(point[1], point[0]) for point in zip(x, y)]\nlat = sum([point[0] for point in loc]) / len(loc)\nlon = sum([point[1] for point in loc]) / len(loc)\nroute = folium.PolyLine(loc, color='#808080', weight=2, opacity=0.8).add_to(m)\n\nm.fit_bounds(route.get_bounds())\n\n\nA DataFrame contains the latest locations for each competitor. Add it to a feature group so that only that portion of the map will be refreshed:\n\nfg = folium.FeatureGroup(name=\"Competitors\")\n\nfor lat, lon in zip(df_front.lat.values, df_front.lon.values):\n    fg.add_child(\n        folium.CircleMarker(location=(lat, lon), radius=3, color='Fuchsia')\n    )\n\nst_data = st_folium(m, \n    feature_group_to_add=fg,\n    height=400,\n    width=700,\n)\n\n\nHere is a race in progress:\n\nWrapping up\n\nThank you for reading my post! I hope it has given you some ideas on the types of apps that you can build with Apache Kafka and Apache Pinot. If you have any questions, please post them in the comments below or contact me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "ESG reporting with Streamlit",
    "url": "https://blog.streamlit.io/esg-reporting-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nBy Sven Koerner and Mathias Landh√§u√üer\nPosted in Snowflake powered ‚ùÑÔ∏è, June 23 2023\nApp overview\nStep 1. Connect to Snowflake and get the Documents\nStep 2. Connect to semantha and feed the documents into the consolidated analysis\nStep 3. Drill down into a single document\nStep 4. Identify documentation your company already has but doesn't know where\nBonus. Generate text for your ESG report!\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nWe are Sven and Mathias, semantic data processing researchers. Since 2007, we have been studying the machines‚Äô ability to understand natural language‚Äîthis was before AI was cool. Our dream was to provide technology that could quickly and efficiently filter through the noise, leaving only the most relevant and helpful information. Fast forward to 2017. We found semantha and built an adaptive AI for text-driven processes.\n\nThen last year, we chatted with Snowflake, and the topic of Environmental, Social, and Governance (ESG) reporting came up. We talked about how it‚Äôs riddled with manual document processing once you go beyond quantitative KPI‚Äîwhich you‚Äôre required to do. To do it correctly, you must assess the entire reporting standard, but companies often look only at the easy 10%‚Äîfinancial KPI, gender pay gaps, greenhouse gas emissions, etc. All that is computed from structured data like your basic Excel spreadsheets. The other 90% is buried in the documents like labor law, safety, health insurance, internal policies, and so on.\n\nThe short of it is, most companies don‚Äôt provide complete and accurate reports.\n\nOuch.\n\nAfter this talk a lightbulb came up in our heads (or two?) üí° üí°\n\nWhat if we pulled data from Snowflake, did the analyses with semantha, and displayed the results in a beautiful Streamlit app?\n\nSo a week later, we built it, deployed it, and it‚Äôs running. We called it K-A-T-E One.\n\nIn this post, we‚Äôll show you the fundamentals of the app, how to use it, and what insights to gain from your data. Let‚Äôs get started!\n\nüöÄ\nIf you have access to a Snowflake system, run your data by opening the app and connecting to Snowflake. If you want to clone the code, here is the repo.\nApp overview\n\nK-A-T-E One can fetch ESG-related content from Snowflake, perform a coverage analysis using semantha, and answer questions like these:\n\nHow to jumpstart our reporting process?\nWhat do we already have in place to report on?\nWhat is in a given annual report or draft (and what is missing)?\nWhere does the information come from? That‚Äôs a valid question.\n\nHere is how it works:\n\nHere is how you‚Äôd use it:\n\nConnect the app to Snowflake\nChoose the documents to be analyzed and get a breakdown of the included topics. For each document, an overview will highlight the covered topics and identify any missing ones.\nSelect a single document for in-depth analysis. See a topic distribution (Altair bar chart) and the highlighted topics broken down by ESG reporting category in a sunburst chart (Plotly).\n\nGo ahead and play with it:\n\n\n\nNow, let‚Äôs get to coding!\n\nStep 1. Connect to Snowflake and get the Documents\n\nConnecting to Snowflake is easy. All you need are the Snowflake SDK and valid credentials. We store our credentials in Streamlit secrets.toml (per Streamlit docs).\n\nüöÄ\nNOTE: We added the stage we're reading from to the configuration. This stage is not used by the Snowflake connector when querying, but it can be useful information to include. You can access the secrets as environment variables or by querying the st.secrets dictionary.\n\nBecause we used sections, we used the dictionary:\n\n[snowflake]\naccount   = \"account.region\"\nuser      = \"myuser\"\nrole      = \"myrole\"\npassword  = \"************\"\nwarehouse = \"my.example.warehouse\"\ndatabase  = \"mydb\"\nschema    = \"myschema\"\nstage     = \"ESG_DEMO\"\n\n\nTo begin, import the Snowflake libraries and access the st.secrets dictionary. Then, open a connection and fetch the document list from the configured stage:\n\nimport snowflake.connector as snowconnector\n\ndef is_analyzable(filename):\n    return filename.endswith(\".pdf\") or filename.endswith(\".txt\") or filename.endswith(\".docx\")\n\nctx = snowconnector.connect(st.secrets.snowflake)\nwith ctx.cursor() as cur:\n    cur.execute(\"SELECT relative_path FROM directory(%s);\", (st.secrets.snowflake.stage,))\n\n    # This is how you'll get a list of all files\n    result = cur.fetchall()\n\n    # We filter the file types so that we only use text in the demo\n    # and take only the first 20 that we want to read...\n    # If you expect many files, you wouldn't use the filter and then trim the list\n    # but append analyzable files one by one until you reach the size limit to\n    # avoid checking files that you'd then throw away anyways (see the code in\n    # the GitHub repository for details)\n    filtered = [element for element in result if is_analyzable(element)][:20]\n\n\nThat's it! Now you have a list of documents you can analyze for ESG content.\n\nOn the UI side, connecting is easy. Just fill in your connection details in the sidebar:\n\nOnce you connect to Snowflake, the app will load the first 20 documents and display a preview:\n\nTo keep this brief, we filtered file types to show only PDFs, plaintext, and docx files and set the document number to 20. In your app, you can remove these limitations.\n\nStep 2. Connect to semantha and feed the documents into the consolidated analysis\n\nTo prepare the app, we trained semantha to perform ESG-related analyses. The background knowledge is stored in semantha's library (similar to books üìö, not like a program library ü§ñ), hence the term \"library\" in the following code snippet. We'll focus on the EGS example for this post, but you can modify it for other purposes.\n\nAs in Step 1, utilize Streamlit's secrets.toml to store the semantha credentials (K-A-T-E One is pre-authenticated, so provide your credentials only if you use your own private instance):\n\n[semantha]\nserver_url = \"<https://example.semantha.systems>\"\napi_key = \"YOUR_API_KEY\"\ndomain = \"ESG_DEMO\"\n\n\nNext, fetch the binary documents from Snowflake one by one and feed them to semantha to obtain the corresponding results (in Step 1, you only collected the file names and locations):\n\nimport streamlit as st\nimport semantha_sdk\n\ndef analyze_doc_collection(semantha, documents):\n    my_bar = st.progress(0.0, text=\"Analysis running. This will take some time!\")\n    increment = 1 / len(documents)\n    for i, doc in enumerate(documents):\n        __curr_file_name = doc.get_name()\n        my_bar.progress((i + 1) * increment, text=f\"Analysis running... Processing file '{__curr_file_name}'\")\n        results = semantha.domains(domainname=st.secrets.semantha.domain).references.post(\n            file=doc.as_stream(),\n            similaritythreshold=0.9,\n            maxreferences=1\n        )\n\n        # [...]\n        # process and display results - see GitHub :) \n\n# Connect to semantha\nsem = semantha_sdk.login(\n    server_url=st.secrets.semantha.userver_url,\n    key=st.secrets.semantha.api_key\n    )\n\n# Run the analysis\nanalyze_doc_collection(sem, filtered)\n\n\nLines 10-14 in the code snippet above are important. The app sends a file to semantha using the references endpoint of the API through an HTTP POST request. The selected domain ESG_DEMOcontains the library with our background knowledge. semantha analyzes the file and discards all findings except one (maxreferences=1) and findings with lower confidence (threshold=0.7).\n\nNext, produce a sunburst chart and a table with the document topics. We used ESG reports for the analysis to show which ESRS facets are covered, where (sunburst chart in Step 3), and which facets are missing (document breakdown below the chart).\n\nIn the user interface, click on \"Analyze Document Collection.\" If you want to skim-read, open them in the embedded PDF reader beforehand.\n\nHere is what the results will look like:\n\nYou can see the high-level topics in each document, with active and inactive buttons indicating whether a topic is covered. To see how a document covers a topic, click on the analysis button next to the filename and navigate to the \"Individual Document\" tab.\n\nCurious about the button functions? Click on them or check out the Bonus section below. üòâ\n\nStep 3. Drill down into a single document\n\nWant to get into the details? Drill down into any document. See what ESRS-related content you already have and what's missing. It'll help you steer your reports and better understand how well the competition is performing. No one said you must use your data to process with K-A-T-E One. üòâ\n\nIf you want to do an individual analysis of a single document, start with the bar chart. It shows one bar per page that contains relevant content. You'll get a quick document overview. Does it mainly talk about a single topic, or is it rather a high-level document covering multiple topics?\n\nScroll down to see an overview of the topics covered on the left and a breakdown of the specific ESRS facet covered (and not covered) by the document on the right.\n\nIn this example, the document doesn't cover the topic \"Affected communities\" at all but contains statements related to the topic \"Biodiversity and ecosystems\" (which is not surprising, given that the document is called \"Environment Protection Program Policy (EPPP)\"). And while some facets of the ESRS topic are covered, other essential facets are missing.\n\nYou must decide:\n\nAre these missing facets not important to the EPPP?\nIf they're important, should they be covered elsewhere?\nCould they be a sensible extension of your policy?\n\nScroll down some more for a sunburst chart that details the coverage of ESRS topics, broken down from the highest hierarchy level (in the center of the chart) down to the individual facet (second ring). The outermost ring includes the actual snippet from the document.\n\nAs you can see, the document mainly focuses on governance (top half of the chart). ESRS 2 focuses on measuring and monitoring aspects of EPPP, including governance. The bottom left quarter covers the policy topic of \"ESRS E4 / Biodiversity and Ecosystems\". EPPP also addresses handling pollutants by the workforce but with less emphasis than other topics.\n\nStep 4. Identify documentation your company already has but doesn't know where\n\nNothing is worse than staring at a blank sheet of paper, wondering where to start and how to structure your thoughts.\n\nTo start your reporting, identify the important reporting aspects for your operation and stakeholders. Since ESG concepts are not new, there should already be some existing frameworks. To gain a deep understanding, perform the same analysis as in Step 2, but this time with all your data. Utilize your AI to guide you, identify covered areas, and you have your starting point. Let's face it, as does everyone else, you likely lack oversight of your ESG documentation.\n\nUse the analysis results as a starting point for your materiality assessment. Prior coverage of these topics, even without reporting in mind, indicates their value or necessity to your business operation. Include them in any report you send out. Cross-reference the results of this analysis with your materiality assessment to identify what you have covered and areas that require further study or control.\n\nYou can now get a breakdown of the ESRS facets, where you have already covered them, and where you didn't. Given the analysis in Step 2, you can identify topics that were not previously covered, drill down to individual paragraphs, and connect the dots across multiple documents to beef up your report.\n\nWith the help of AI, it's almost as much fun as a painting by numbers! üñºÔ∏è\n\nBonus. Generate text for your ESG report!\n\nNow you can ask semantha to summarize the paragraphs, properly referencing your documents (below the consolidated analysis). Each ESRS topic covered in the analyzed documents can be summarized.\n\nJust click a topic button in the document list and scroll down to the summary:\n\nYou can quickly prepare a draft report section based on qualitative data and auditable references. But you must first identify the ESRS facets relevant to your business operation. In Step 4, we demonstrated how to start or extend your materiality assessment based on available documentation.\n\nYou can also begin with the mandatory facets:\n\nESRS 2 General disclosures (all disclosures)\nESRS E1 Climate Change (all disclosures)\nESRS S1 Own Workforce (subsets apply according to company size)\n\nNo AI is required to identify these facets. üòâ\n\nWrapping up\n\nThank you for reading our post! You've seen that getting information from unstructured documents stored in Snowflake and visualizing the results is bliss when combining the powers of Streamlit, Snowflake, and semantha. We've shown how to fetch documents from Snowflake, push them to semantha for analysis using their respective Python SDKs, and view the results in a beautiful üíñ Streamlit app.\n\nWe've excluded a bunch of code to make the post easier to read. Feel free to clone it from our repo! The SDKs are public, too (go to snowflake-connector-python and the semantha-sdk to learn more).\n\nIf you have any questions, please post them in the comments below or contact Sven or Mathias on LinkedIn.\n\nHappy Streamlit-ing! üöÄ\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Drill-downs and filtering with Streamlit and¬†Altair",
    "url": "https://blog.streamlit.io/drill-downs-and-filtering-with-streamlit-and-altair/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDrill-downs and filtering with Streamlit and¬†Altair\n\nDisplay an Altair chart definition in Streamlit using the st.altair_chart widget\n\nBy Carlos D Serrano\nPosted in Advocate Posts, July 12 2023\nAvoiding re-runs\nLet's create some data first\nColor consistency\nSelections\nFiltering chart\nFiltered and faceted charts\nAll of it in just one widget\nDashboard\nFull code\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nFor the last few months, I've written several apps using Streamlit, and one of my favorite libraries for optimizing the look and feel of my app is Altair. You can display an Altair chart definition in Streamlit using the st.altair_chart widget. Altair is a powerful library full of styling, configurations, and interactions.\n\nIn this post, I'll show you how to create interactive and dynamic visualizations using Altair in Streamlit.\n\nüç©\nTo jump right in, check out the demo app and the code.\nAvoiding re-runs\n\nOne significant advantage of creating drill-downs and filters with Altair on Streamlit is that these interactions occur at the front-end level and don't require a re-run of your Streamlit app.\n\nLet's create some data first\n\nI used a simple approach for this example and created a Pandas DataFrame with sales data. I also used the st.cache_data decorator to save the DataFrame in the cache:\n\n@st.cache_data\ndef get_data():\n    dates = pd.date_range(start=\"1/1/2022\", end=\"12/31/2022\")\n    data = pd.DataFrame()\n    sellers = {\n        \"LATAM\": [\"S01\", \"S02\", \"S03\"],\n        \"EMEA\": [\"S10\", \"S11\", \"S12\", \"S13\"],\n        \"NA\": [\"S21\", \"S22\", \"S23\", \"S24\", \"S25\", \"S26\"],\n        \"APAC\": [\"S31\", \"S32\", \"S33\", \"S34\", \"S35\", \"S36\"],\n    }\n    rows = 25000\n    data[\"transaction_date\"] = np.random.choice([str(i) for i in dates], size=rows)\n    data[\"region\"] = np.random.choice(regions, size=rows, p=[0.1, 0.3, 0.4, 0.2])\n    data[\"transaction_amount\"] = np.random.uniform(100, 250000, size=rows).round(2)\n    data[\"seller\"] = data.apply(\n        lambda x: np.random.choice(sellers.get(x[\"region\"])), axis=1\n    )\n    return data.sort_values(by=\"transaction_date\").reset_index(drop=True)\n\nColor consistency\n\nWhen creating drill-downs, it's crucial to maintain color consistency to enhance the clarity of your charts. Altair scales can be used to specify color domains and ranges that persist during drill-down.\n\nUse the following three list variables in the Altair chart definitions:\n\nregions = [\"LATAM\", \"EMEA\", \"NA\", \"APAC\"]\ncolors = [\"#aa423a\",\"#f6b404\", \"#327a88\",\"#303e55\",\"#c7ab84\",\"#b1dbaa\",\n    \"#feeea5\",\"#3e9a14\",\"#6e4e92\",\"#c98149\", \"#d1b844\",\"#8db6d8\"]\nmonths = [\n    \"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\",\n]\n\nSelections\n\nTo define a selection in Altair, you can use the single, interval, or point methods.\n\nIn this example, a single selection is used to drill down based on region. The \"empty\" attribute can be specified to determine whether all objects or no objects are visible when no selection has been made.\n\nregion_select = alt.selection_single(fields=[\"region\"], empty=\"all\")\n\nFiltering chart\n\nThis chart definition includes an Altair add_selection method for filtering other chart definitions that will be created later.\n\nTo create a dynamic experience, the opacity attribute is used to reduce the opacity of unselected objects to 25%. Note that the color method uses the scale attribute to limit the available colors to the domain and range variables defined previously:\n\nregion_pie = (\n    (\n        alt.Chart(sales_data)\n        .mark_arc(innerRadius=50)\n        .encode(\n            theta=alt.Theta(\n                \"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Sum of Transactions\",\n            ),\n            color=alt.Color(\n                field=\"region\",\n                type=\"nominal\",\n                scale=alt.Scale(domain=regions, range=colors),\n                title=\"Region\",\n            ),\n            opacity=alt.condition(region_select, alt.value(1), alt.value(0.25)),\n        )\n    )\n    .add_selection(region_select)\n    .properties(title=\"Region Sales\")\n)\n\nFiltered and faceted charts\n\nTo enable filtering, implement the Altair transform_filter method. Faceting is enabled by using the facet method inside the encoding method, which uses a field attribute and a column attribute to break down the chart into multiple related charts.\n\nIf you need to facet by two different fields, use the repeat() method:\n\nregion_summary = (\n    (\n        alt.Chart(sales_data)\n        .mark_bar()\n        .encode(\n            x=alt.X(\n                \"month(transaction_date)\",\n                type=\"temporal\",\n            ),\n            y=alt.Y(\n                field=\"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Total Sales\",\n            ),\n            color=alt.Color(\n                \"region\",\n                type=\"nominal\",\n                title=\"Regions\",\n                scale=alt.Scale(domain=regions, range=colors),\n                legend=alt.Legend(\n                    direction=\"vertical\",\n                    symbolType=\"triangle-left\",\n                    tickCount=4,\n                ),\n            ),\n        )\n    )\n    .transform_filter(region_select)\n    .properties(width=700, title=\"Monthly Sales\")\n)\n\nsellers_monthly_pie = (\n    (\n        alt.Chart(sales_data)\n        .mark_arc(innerRadius=10)\n        .encode(\n            theta=alt.Theta(\n                field=\"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Total Transactions\",\n            ),\n            color=alt.Color(\n                \"month(transaction_date)\",\n                type=\"temporal\",\n                title=\"Month\",\n                scale=alt.Scale(domain=months, range=colors),\n                legend=alt.Legend(\n                    direction=\"vertical\",\n                    symbolType=\"triangle-left\",\n                    tickCount=12,\n                ),\n            ),\n            facet=alt.Facet(\n                field=\"seller\",\n                type=\"nominal\",\n                columns=8,\n                title=\"Sellers\",\n            ),\n            tooltip=alt.Tooltip([\"sum(transaction_amount)\", \"month(transaction_date)\"]),\n        )\n    )\n    .transform_filter(region_select)\n    .properties(width=150, height=150, title=\"Sellers transactions per month\")\n)\n\nAll of it in just one widget\n\nTo enable selections and filters, enclose them within a single Streamlit widget. To arrange charts, use the following methods:\n\nVConcat and HConcat methods to concatenate charts\nPipe symbol \"|\" to place charts next to each other\nAmpersand symbol \"&\" to set charts below or above each other\nPlus sign ‚Äú+‚Äù to overlay charts\n\nTo make a dashboard-like arrangement, create a variable called top_row and use the pipe symbol to arrange your region_pie and region_summary charts side by side. Then, using the ampersand, place top_row and sellers_monthly_pie below it. This creates a variable containing all the concatenated charts in a single Streamlit altair_chart widget.\n\nNote that when using concatenated charts, the use_container_width attribute won't work. Therefore, you must specify the width of the charts in their properties:\n\n#Create first row by concatenating horizontally\ntop_row = region_pie | region_summary\n#Create dashboard by concatenating top_row with faceted chart\nfull_chart = top_row & sellers_monthly_pie\n\n#Single Streamlit Object\nst.altair_chart(full_chart)\n\nDashboard\nFull code\nimport streamlit as st\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nst.set_page_config(layout=\"wide\")\nregions = [\"LATAM\", \"EMEA\", \"NA\", \"APAC\"]\ncolors = [\n    \"#aa423a\",\n    \"#f6b404\",\n    \"#327a88\",\n    \"#303e55\",\n    \"#c7ab84\",\n    \"#b1dbaa\",\n    \"#feeea5\",\n    \"#3e9a14\",\n    \"#6e4e92\",\n    \"#c98149\",\n    \"#d1b844\",\n    \"#8db6d8\",\n]\nmonths = [\n    \"Jan\",\n    \"Feb\",\n    \"Mar\",\n    \"Apr\",\n    \"May\",\n    \"Jun\",\n    \"Jul\",\n    \"Aug\",\n    \"Sep\",\n    \"Oct\",\n    \"Nov\",\n    \"Dec\",\n]\nst.title(\"2022 Sales Dashboard\")\n\n@st.cache_data\ndef get_data():\n    dates = pd.date_range(start=\"1/1/2022\", end=\"12/31/2022\")\n    data = pd.DataFrame()\n    sellers = {\n        \"LATAM\": [\"S01\", \"S02\", \"S03\"],\n        \"EMEA\": [\"S10\", \"S11\", \"S12\", \"S13\"],\n        \"NA\": [\"S21\", \"S22\", \"S23\", \"S24\", \"S25\", \"S26\"],\n        \"APAC\": [\"S31\", \"S32\", \"S33\", \"S34\", \"S35\", \"S36\"],\n    }\n    rows = 25000\n    data[\"transaction_date\"] = np.random.choice([str(i) for i in dates], size=rows)\n    data[\"region\"] = np.random.choice(regions, size=rows, p=[0.1, 0.3, 0.4, 0.2])\n    data[\"transaction_amount\"] = np.random.uniform(100, 250000, size=rows).round(2)\n    data[\"seller\"] = data.apply(\n        lambda x: np.random.choice(sellers.get(x[\"region\"])), axis=1\n    )\n    return data.sort_values(by=\"transaction_date\").reset_index(drop=True)\n\nsales_data = get_data()\n\nregion_select = alt.selection_single(fields=[\"region\"], empty=\"all\")\nregion_pie = (\n    (\n        alt.Chart(sales_data)\n        .mark_arc(innerRadius=50)\n        .encode(\n            theta=alt.Theta(\n                \"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Sum of Transactions\",\n            ),\n            color=alt.Color(\n                field=\"region\",\n                type=\"nominal\",\n                scale=alt.Scale(domain=regions, range=colors),\n                title=\"Region\",\n            ),\n            opacity=alt.condition(region_select, alt.value(1), alt.value(0.25)),\n        )\n    )\n    .add_selection(region_select)\n    .properties(title=\"Region Sales\")\n)\n\nregion_summary = (\n    (\n        alt.Chart(sales_data)\n        .mark_bar()\n        .encode(\n            x=alt.X(\n                \"month(transaction_date)\",\n                type=\"temporal\",\n            ),\n            y=alt.Y(\n                field=\"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Total Sales\",\n            ),\n            color=alt.Color(\n                \"region\",\n                type=\"nominal\",\n                title=\"Regions\",\n                scale=alt.Scale(domain=regions, range=colors),\n                legend=alt.Legend(\n                    direction=\"vertical\",\n                    symbolType=\"triangle-left\",\n                    tickCount=4,\n                ),\n            ),\n        )\n    )\n    .transform_filter(region_select)\n    .properties(width=700, title=\"Monthly Sales\")\n)\n\nsellers_monthly_pie = (\n    (\n        alt.Chart(sales_data)\n        .mark_arc(innerRadius=10)\n        .encode(\n            theta=alt.Theta(\n                field=\"transaction_amount\",\n                type=\"quantitative\",\n                aggregate=\"sum\",\n                title=\"Total Transactions\",\n            ),\n            color=alt.Color(\n                \"month(transaction_date)\",\n                type=\"temporal\",\n                title=\"Month\",\n                scale=alt.Scale(domain=months, range=colors),\n                legend=alt.Legend(\n                    direction=\"vertical\",\n                    symbolType=\"triangle-left\",\n                    tickCount=12,\n                ),\n            ),\n            facet=alt.Facet(\n                field=\"seller\",\n                type=\"nominal\",\n                columns=8,\n                title=\"Sellers\",\n            ),\n            tooltip=alt.Tooltip([\"sum(transaction_amount)\", \"month(transaction_date)\"]),\n        )\n    )\n    .transform_filter(region_select)\n    .properties(width=150, height=150, title=\"Sellers transactions per month\")\n)\n\ntop_row = region_pie | region_summary\nfull_chart = top_row & sellers_monthly_pie\nst.altair_chart(full_chart)\n\nWrapping up\n\nAltair charts in Streamlit are an efficient and performant way to add interactive charts to your app. There are many styles and combinations of interactions to create using these tools.\n\nIf you loved this post, check out my other articles on client-side filtering using Altair Sliders, paginating dataframes, and the multiselect widget. And if you have any questions, please post them in the comments below or contact me on GitHub, LinkedIn, Twitter, or Medium.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Semantic search, Part 2: Building a local search app",
    "url": "https://blog.streamlit.io/semantic-search-part-2-building-a-local-search-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSemantic search, Part 2: Building a local search app\n\nMaking an app with Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data from Snowflake Marketplace\n\nBy Dave Lin\nPosted in Snowflake powered ‚ùÑÔ∏è, May 18 2023\nHow does the app work?\nStep 1. Safeguard your Snowflake account\nStep 2. Write backend functions to get data from Snowflake\n1. Import the necessary packages:\n2. Create base functions to connect to Snowflake and execute Snowflake queries:\n3. Create functions to get boroughs and neighborhoods within a borough:\n4. Create a function to get a list of categories that semantically match the user's query:\n5. Create a function to get places within specific categories and in a list of neighborhoods:\nStep 3: Implement OpenAI endpoints\nStep 4. Write the frontend Streamlit app\n1. Import the necessary libraries and backend functions:\n2. Set the page configuration:\n3. Create a function to render the call-to-action (CTA) links:\n4. Create a function to lay out the search options in the sidebar:\n5. Create the function to render the search results:\n6. Create the handler function to load neighborhoods:\n7. Create the handler function to handler venue search:\n8. Control UI renders with sessions state variables:\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nThis is Part 2 of a two-part blog series on building a semantic search application for finding Foursquare venues in NYC. We'll leverage Streamlit, Snowflake, OpenAI, and Foursquare's free NYC venue data from Snowflake Marketplace.\n\nIn Part 1, we explored building a semantic search engine powered by Snowflake. We delved deep into data wrangling, compared different cosine similarity implementations, and evaluated their performance.\n\nIn this second part, I'll guide you through the remaining steps to complete the app.\n\nüî¥\nWant to dive right in? Check out the code and the app. The Streamlit application will be removed after the Snowflake Summit in June due to cost considerations.\n\nBefore we proceed, let's quickly discuss‚Ä¶\n\nHow does the app work?\n\nThe app works as follows:\n\nUsers search for venues by selecting up to five neighborhoods in an NYC borough and entering a search query (e.g., \"Epic Night Out\").\nThe application:\nUses OpenAI's Embeddings API to generate embeddings for the search query\nUses Snowflake to retrieve up to five Foursquare venue categories with embedding vectors that most closely match the user query embeddings\nUses Snowflake to search for venues within the suggested categories in the chosen neighborhoods\nDisplays the venues on a map view and in a data frame\n\nNow that we understand the app, let's proceed with the remaining steps!\n\nStep 1. Safeguard your Snowflake account\n\nEnsuring the security of your Snowflake account is essential. To mitigate security risks, provide the least amount of access necessary to all users and applications.\n\nTo limit Streamlit's Snowflake access, follow these steps:\n\n1. Create a user account for the Streamlit application (for example, svc_streamlit)\n\n2. Create a scoped-down role with read-only access to the Foursquare data, the new schema and tables, and a warehouse:\n\nCREATE ROLE foursquare_read; \nGRANT usage on database foursquare to role foursquare_read; \nGRANT usage on schema foursquare.main to role foursquare_read; \nGRANT SELECT ON ALL tables in schema foursquare.main to role foursquare_read; \nGRANT SELECT ON FUTURE tables in schema foursquare.main to role foursquare_read; \nGRANT SELECT ON FUTURE views in schema foursquare.main to role foursquare_read; \nGRANT IMPORTED PRIVILEGES on database foursquare_nyc to role foursquare_read;\nGRANT usage on warehouse [YOUR_WAREHOUSE_NAME] to role foursquare_read;\n\n\n3. Set the Streamlit user's default role and warehouse to be the assigned role and warehouse.\n\nWith a Streamlit user set up, let's work on connecting Streamlit to Snowflake.\n\nStep 2. Write backend functions to get data from Snowflake\n\nIn this step, we'll write the backend functions required to fetch data from the Snowflake database.\n\nüî¥\nYou can find all the backend functions for getting Snowflake data here. To install the latest Python Connector for Snowflake, follow the instructions here. To learn how to connect Streamlit to Snowflake, go here.\n1. Import the necessary packages:\nimport snowflake.connector\nfrom snowflake.connector import DictCursor\nimport streamlit as st\n\n2. Create base functions to connect to Snowflake and execute Snowflake queries:\ndef _init_connection():\n   return snowflake.connector.connect(**st.secrets[\"snowflake\"])\n\n@st.cache_data(ttl=10, show_spinner=False)\ndef _run_query(query_str):\n   with _init_connection() as conn:\n       with conn.cursor(DictCursor) as cur:\n           cur.execute(query_str)\n           return cur.fetchall()\n\n\n\nThe _init_connection function utilizes the snowflake-connector-python library and Streamlit's secret management to connect to Snowflake. The _run_query function establishes a new connection to Snowflake, executes a query, and returns the results. The DictCursor returns column names alongside the data. It also uses st.cache_data to avoid re-running the same queries within 10 seconds.\n\n3. Create functions to get boroughs and neighborhoods within a borough:\ndef get_boroughs():\n   sql = \"\"\"SELECT * FROM borough_lookup\"\"\"\n   boroughs = _run_query(sql)\n   return boroughs\n\ndef get_neighborhoods(borough_name):\n   sql = \"\"\"\n   SELECT n.*\n   FROM neighborhood_lookup n\n   INNER JOIN borough_neighborhood bn ON n.id = bn.neighborhood_id\n   INNER JOIN borough_lookup b ON bn.borough_id = b.id\n   WHERE b.name IN ('{0}')\n   ORDER BY b.name, n.name\n   \"\"\".format(borough_name)\n   neighborhoods = _run_query(sql)\n   return neighborhoods\n\n4. Create a function to get a list of categories that semantically match the user's query:\ndef get_categories(search_embeddings):\n   sql = \"\"\"\n   WITH base_search AS (\n   SELECT '{0}' embedding\n   )\n   , search_emb AS (\n   SELECT\n       n.index\n       , n.value\n   from base_search l\n   , table(flatten(input => parse_json(l.embedding))) n\n   ORDER BY n.index\n   )\n   , search_emb_sqr AS (\n   SELECT index, value, value*value value_sqr \n   FROM search_emb r\n   )\n   , result AS (\n   SELECT\n       v.category_id\n       , SUM(s.value * v.value) / SQRT(SUM(s.value * s.value) * SUM(v.value * v.value)) cosine_similarity\n   FROM search_emb_sqr s\n   INNER JOIN category_embed_value v ON s.index = v.index\n   GROUP BY v.category_id\n   ORDER BY cosine_similarity DESC\n   LIMIT 5\n   )\n   SELECT c.category, r.cosine_similarity\n   FROM result r\n   INNER JOIN category_lookup c ON r.category_id = c.category_id\n   WHERE r.cosine_similarity > 0.81\n   ORDER BY r.cosine_similarity DESC\n   \"\"\".format(search_embeddings)\n\n   recommended_categories = _run_query(sql)\n\n   return recommended_categories\n\n\nThis query builds upon the cosine similarity query discussed in Part 1. It takes user query embeddings as input and performs a final lookup to return the category names.\n\n5. Create a function to get places within specific categories and in a list of neighborhoods:\ndef get_places(borough_name, neighborhood_list, category_list):\n   sql = \"\"\"\n   WITH base_neighborhoods AS (\n       SELECT n.id \n       FROM borough_lookup b\n       INNER JOIN borough_neighborhood bn on b.id = bn.borough_id\n       INNER JOIN neighborhood_lookup n ON bn.neighborhood_id = n.id\n       WHERE b.name = '{0}'\n       AND n.name IN ({1})\n   )\n   , neighborhood_places AS (\n       SELECT pn.fsq_id\n       FROM place_neighborhood pn\n       WHERE pn.neighborhood_id IN (SELECT id FROM base_neighborhoods)\n       ORDER BY pn.fsq_id\n   )\n   , base_categories AS (\n       SELECT c.category_id\n       FROM category_lookup c\n       WHERE c.category IN ({2})\n   )\n   , category_places AS (\n       SELECT pc.fsq_id\n       FROM category_place pc\n       WHERE pc.category_id IN (SELECT category_id FROM base_categories)\n       ORDER BY pc.fsq_id\n   )\n   , places AS (\n       SELECT\n           fsq_id\n           , name\n           , latitude\n           , longitude\n           , concat(COALESCE(address,''), COALESCE(address_extended,'')) address \n           , fsq_category_labels\n           , n1.value::string category\n       FROM place_lookup l\n       , table(flatten(l.fsq_category_labels)) n\n       , table(flatten(n.value)) n1\n       WHERE fsq_id IN (\n           SELECT fsq_id FROM neighborhood_places\n           INTERSECT\n           SELECT fsq_id FROM category_places\n       )\n       AND latitude IS NOT NULL\n       AND longitude IS NOT NULL\n       QUALIFY row_number() OVER (PARTITION BY fsq_id, n.seq, n.index, n1.seq ORDER BY n1.index DESC) = 1\n   )\n   SELECT\n       fsq_id\n       , ANY_VALUE(name) name\n       , ANY_VALUE(latitude) latitude\n       , ANY_VALUE(longitude) longitude\n       , ANY_VALUE(address) address\n       , listagg(category, ', ') categories\n   FROM places\n   GROUP BY fsq_id\n   ORDER BY fsq_id    \n   \"\"\".format(borough_name, _list_to_str(neighborhood_list), _list_to_str(category_list))\n   places = _run_query(sql)\n   return places\n\n\nIn the query above, we first filter the venues by narrowing them down to specific categories within a list of neighborhoods and venues. Next, we intersect these two lists to produce a final list of venues. For UI display purposes, we extract the latitude, longitude, street address, and leaf categories of each venue. The QUALIFY statement extracts the last category in the inner list of each category list found in the fsq_category_labels column.\n\nüî¥\nFoursquare stores category labels in a list of lists. The outer list is the list of categories. The inner list describes the hierarchy of the category, where the first element represents the root category and the last element represents the leaf category.\nStep 3: Implement OpenAI endpoints\n\nOur Streamlit app uses OpenAI's Python SDK to create embeddings of user queries and to moderate user queries (to ensure user queries don't violate OpenAI's Content Policy).\n\nüî¥\nAll backend functions calling OpenAI APIs can be found here.\nimport openai\nimport streamlit as st\n\nopenai.api_key = st.secrets['openai']['api_key']\n\ndef get_embedding(category_str):\n   try:\n       response = openai.Embedding.create(\n           input=category_str,\n           model=\"text-embedding-ada-002\"\n       )\n       embeddings = response['data'][0]['embedding']\n       return embeddings\n  \n   except Exception as e:\n       raise e\n\ndef get_moderation(user_query):\n   try:\n       moderation = openai.Moderation.create(\n           input=user_query\n       )\n       moderation_result = moderation['results'][0]\n       flagged_categories = [category for category, value in moderation_result['categories'].items() if value]\n       return {'flagged': moderation_result['flagged'], 'flagged_categories':flagged_categories}\n  \n   except Exception as e:\n       raise e\n\n\nExceptions are caught and then passed to the front-end application for the sake of simplicity.\n\nStep 4. Write the frontend Streamlit app\n\nNow that we have our backend functions in place, let's move on to creating the front-end Streamlit app that users will interact with.\n\nWhen I started developing the Streamlit app, I used the typical nested if-else scripting approach, where UI and backend operations were combined. But I quickly found it difficult to track what would cause parts of the UI to re-render. To address this, I eventually settled on the following pattern that allows for better control over app refreshes:\n\nCreate functions to group UI elements\nCreate UI element handlers to change session state variables and make backend calls\nUse session state variables to maintain user selections, control UI renderings, and avoid unnecessary calls to backend functions\n\nBy using session state variables and handlers, we ensure only the affected parts of the application are refreshed, avoiding full-page reloads or unnecessary backend calls. As a result, we can create a more efficient and responsive app. The lightweight structure also contains less overhead than an object-oriented approach to developing Streamlit apps.\n\nIn the rest of this section, we'll use these patterns to develop our Streamlit app.\n\nüî¥\nYou can find the complete Streamlit app code here.\n1. Import the necessary libraries and backend functions:\nimport streamlit as st\nimport api_snowflake as api\nimport api_openai as oai\n\n2. Set the page configuration:\nst.set_page_config(page_title=\"NYC Venue Search\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n\n3. Create a function to render the call-to-action (CTA) links:\ndef render_cta_link(url, label, font_awesome_icon):\n   st.markdown('<link rel=\"stylesheet\" href=\"<https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css>\">', unsafe_allow_html=True)\n   button_code = f'''<a href=\"{url}\" target=_blank><i class=\"fa {font_awesome_icon}\"></i> {label}</a>'''\n   return st.markdown(button_code, unsafe_allow_html=True)\n\n\nThis function uses the Markdown element to display clickable links with Font Awesome icons.\n\n4. Create a function to lay out the search options in the sidebar:\ndef render_search():\n   \"\"\"\n   Render the search form in the sidebar.\n   \"\"\"\n   search_disabled = True\n   with st.sidebar:\n       st.selectbox(label=borough_search_header, options=(\n           [b['NAME'] for b in boroughs]), index=2, key=\"borough_selection\", on_change=handler_load_neighborhoods)\n\n       if \"neighborhood_list\" in st.session_state and len(st.session_state.neighborhood_list) > 0:\n           st.multiselect(label=neighborhood_search_header, options=(\n               st.session_state.neighborhood_list), key=\"neighborhoods_selection\", max_selections=5)\n\n       st.text_input(label=semantic_search_header,\n                     placeholder=semantic_search_placeholder, key=\"user_category_query\")\n\n       if \"borough_selection\" in st.session_state and st.session_state.borough_selection != \"\" \\\\\n               and \"neighborhoods_selection\" in st.session_state and len(st.session_state.neighborhoods_selection) > 0  \\\\\n               and \"user_category_query\" in st.session_state and st.session_state.user_category_query != \"\":\n           search_disabled = False\n\n       st.button(label=search_label, key=\"location_search\",\n                 disabled=search_disabled, on_click=handler_search_venues)\n\n       st.write(\"---\")\n       render_cta_link(url=\"<YOUR TWITTER HANDLE URL>\",\n                       label=\"Let's connect\", font_awesome_icon=\"fa-twitter\")\n       render_cta_link(url=\"<YOUR LINKEDIN PROFILE URL\",\n                       label=\"Let's connect\", font_awesome_icon=\"fa-linkedin\")\n\n\nThe sidebar of the application has four primary UI elements: a select box to choose a borough, a multi-select box to choose neighborhoods, a text input for user queries, and a search button to initiate a venue search. We can store user data across application runs by assigning keys to these UI elements. Streamlit's session state documentation states that every UI widget with a key is automatically added to the session state:\n\nThe borough_selection selectbox widget stores users' borough selections. When the selection changes, the handler_load_neighborhoods function fetches the list of neighborhoods in the selected borough and stores it in the neighborhood_list session state variable.\nThe neighborhoods_selection multiselect widget stores users' neighborhood selections.\nThe location_search button widget is disabled until users select a borough, then a list of neighborhoods, and enter a search query. When clicked, the handler_search_venues function handles the embedding of user queries, searches for semantically similar categories, and finds venues within those categories in the specified neighborhoods.\n5. Create the function to render the search results:\ndef render_search_result():\n    \"\"\"\n    Render the search results on the main content area.\n    \"\"\"\n    col1, col2 = st.columns([1,2])\n    col1.write(category_list_header)\n    col1.table(st.session_state.suggested_categories)\n    col2.write(f\"Found {len(st.session_state.suggested_places)} venues.\")\n    if (len(st.session_state.suggested_places) > 0):\n        col2.map(st.session_state.suggested_places, zoom=13, use_container_width=True)\n        st.write(venue_list_header)\n        st.dataframe(data=st.session_state.suggested_places, use_container_width=True)\n\n\nThis function renders the suggested categories (stored in suggested_categories session state variable) and the recommended venues (stored in suggested_places session state variable) on the map. It also renders the list of venues in a dataframe.\n\nNext, we'll move on to the handler functions.\n\n6. Create the handler function to load neighborhoods:\ndef handler_load_neighborhoods():\n   \"\"\"\n   Load neighborhoods for the selected borough and update session state.\n   \"\"\"\n   selected_borough = 'Manhattan'\n   if \"borough_selection\" in st.session_state and st.session_state.borough_selection != \"\":\n       selected_borough = st.session_state.borough_selection\n   neighborhoods = api.get_neighborhoods(selected_borough)\n   st.session_state.neighborhood_list = [n['NAME'] for n in neighborhoods]\n\n\nThis function receives a list of neighborhoods in a borough. The borough is set to Manhattan by default but is overwritten by the user's selection (stored in the borough_selection session state variable).\n\nThe function is called whenever the user selects a new borough from the borough_selection dropdown. It is also manually called when the application first runs (so that we can preload neighborhoods in Manhattan).\n\n7. Create the handler function to handler venue search:\ndef handler_search_venues():\n   \"\"\"\n   Search for venues based on user query and update session state with results.\n   \"\"\"\n   try:\n       moderation_result = oai.get_moderation(st.session_state.user_category_query)\n       if moderation_result['flagged'] == True:\n           flagged_categories_str = \", \".join(moderation_result['flagged_categories'])\n           st.error(f\"‚ö†Ô∏è Your query was flagged by OpenAI's content moderation endpoint for: {flagged_categories_str}.  \\\\n  \\\\nPlease try a different query.\")\n       else:\n           embeddings = oai.get_embedding(st.session_state.user_category_query)\n           st.session_state.suggested_categories = api.get_categories(embeddings)\n\n           if len(st.session_state.suggested_categories) > 0 and len(st.session_state.neighborhoods_selection) > 0:\n               category_list = [s['CATEGORY'] for s in st.session_state.suggested_categories]\n\n               st.session_state.suggested_places = api.get_places(\n                   st.session_state.borough_selection,\n                   st.session_state.neighborhoods_selection,\n                   category_list)\n           else:\n               st.warning(\"No suggested categories found. Try a different search.\")\n   except Exception as e:\n       st.error(f\"{str(e)}\")\n\n\nThis function is responsible for the bulk of the application logic. It's triggered whenever users click on the location_search button widget. The following steps are carried out:\n\nFor safety reasons, it checks the user's query against OpenAI's moderation endpoint\nIt uses OpenAI's Embeddings API to embed the user's query\nIt retrieves the list of semantically similar categories from Snowflake and stores it in the suggested_categories session state variable\nIt retrieves the list of venues within the suggested categories in the selected neighborhoods from Snowflake. The suggested_places session state variable stores the final list of places\n\nThe function also handles the following edge cases:\n\nIf OpenAI's moderation endpoint flags the user query, an error message is displayed\nIf no categories are semantically similar to the user query, a warning message is displayed\nAny other exception message is displayed as an error message\n\nWith the UI element group functions and handler functions defined, the rest of the application can now be wired up.\n\n8. Control UI renders with sessions state variables:\nboroughs = [{'NAME':'Brooklyn'},{'NAME':'Bronx'},{'NAME':'Manhattan'},{'NAME':'Queens'},{'NAME':'Staten Island'}]\n\nif \"selected_borough\" not in st.session_state:\n   st.session_state.selected_borough = \"Manhattan\"\n\nif \"neighborhood_list\" not in st.session_state:\n   handler_load_neighborhoods()\nrender_search()\n\nst.title(page_title)\nst.write(page_helper)\nst.write(\"---\")\n\nif \"suggested_places\" not in st.session_state:\n   st.write(empty_search_helper)\nelse:\n   render_search_result()\n\n\nThe list of NYC boroughs is hard-coded to eliminate an unnecessary Snowflake query. When the application loads for the first time, the selected borough is set to Manhattan. If users have not chosen any neighborhoods, the handler_load_neighborhoods function is called to fetch a list of Manhattan neighborhoods. The search bar is then displayed. Finally, the empty search helper text or search results are displayed based on the presence of suggested_places as a session state variable.\n\nWrapping up\n\nIn this two-part blog series, we successfully built a semantic location search application using Streamlit, Snowflake, OpenAI, and Foursquare's free NYC venue data. In the first part, we focused on building a Snowflake-powered semantic search engine. In this second part, we covered essential steps such as limiting the application's access to Snowflake, connecting Streamlit to Snowflake, writing optimized backend queries, implementing OpenAI endpoints, and wiring up the Streamlit application.\n\nGiven additional time (and data), I'd add the following enhancements:\n\nOrder the recommended venues by popularity scores (Foursquare didn't make this available in their dataset).\nCall Foursquare's venue endpoints to display recent tips and photos for each venue.\n\nI hope you enjoyed the article. Connect with me on Twitter or LinkedIn. I'd love to hear from you.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nAlso in Snowflake powered ‚ùÑÔ∏è...\n\nView even more ‚Üí\n\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit in Snowflake ‚ùÑÔ∏è",
    "url": "https://blog.streamlit.io/tag/snowflake-powered/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Snowflake powered ‚ùÑÔ∏è\n9 posts\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nMonte Carlo simulations with Streamlit\n\nLearn how to predict future stock prices\n\nSnowflake powered ‚ùÑÔ∏è\nby\nMats Stellwall\n,\nJune 8 2023\nSemantic search, Part 2: Building a local search app\n\nMaking an app with Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data from Snowflake Marketplace\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 18 2023\nSemantic search, Part 1: Implementing cosine similarity\n\nWrangling Foursquare data and implementing semantic search in Snowflake\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 17 2023\nStreamlit wizard and custom animated spinner\n\nImprove user experience with simplified data entry and step-by-step guidance\n\nSnowflake powered ‚ùÑÔ∏è\nby\nAndrew Carson\n,\nMay 15 2023\nBuild a Snowflake DATA LOADER on Streamlit in only 5¬†minutes\n\nDrag and drop your Excel data to Snowflake with a Streamlit app\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSasha Mitrovich\n,\nMay 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Carlos D Serrano - Streamlit",
    "url": "https://blog.streamlit.io/author/carlos/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Carlos D Serrano\n1 post\nDrill-downs and filtering with Streamlit and¬†Altair\n\nDisplay an Altair chart definition in Streamlit using the st.altair_chart widget\n\nAdvocate Posts\nby\nCarlos D Serrano\n,\nJuly 12 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mark Needham - Streamlit",
    "url": "https://blog.streamlit.io/author/mark/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mark Needham\n1 post\nDisplay a race on a live map üèÉ\n\nCreate a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library\n\nAdvocate Posts\nby\nMark Needham\n,\nJune 22 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "generate-openai-apikey.gif (807√ó614)",
    "url": "https://blog.streamlit.io/content/images/2023/05/generate-openai-apikey.gif#browser",
    "html": ""
  },
  {
    "title": "Host your Streamlit app for free",
    "url": "https://blog.streamlit.io/host-your-streamlit-app-for-free/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHost your Streamlit app for free\n\nLearn how to transfer your apps from paid platforms to Streamlit Community Cloud\n\nBy Chanin Nantasenamat\nPosted in Tutorials, January 24 2023\nWhy deploy your apps to the internet?\nWhy use Community Cloud?\nStep 1. Create a simple Streamlit app\nStep 2. Set up an account on Community Cloud\nStep 3. Connect your account to GitHub\nStep 4. Create a GitHub repo of your app\nStep 5. Deploy your app in a few clicks\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIf you have a Streamlit app but don‚Äôt want to pay a monthly fee to host it on a commercial cloud platform, one option is to migrate it to Streamlit Community Cloud. It‚Äôs FREE!\n\nIn this post, I‚Äôll show you how to build a demo app and deploy it to Community Cloud step-by-step:\n\nStep 1. Create a simple Streamlit app\nStep 2. Set up an account on Community Cloud\nStep 3. Connect your account to GitHub\nStep 4. Create a GitHub repo of your app\nStep 5. Deploy your app in a few clicks\n\nCan‚Äôt wait to see it in action? Here's the demo app and the repo code.\n\nBut before we get to the fun stuff, let‚Äôs talk about‚Ä¶\n\nWhy deploy your apps to the internet?\n\nDeploying your apps to the internet allows users to access them from a web browser‚Äîwithout having to set up a coding environment and installing dependencies.\n\nYou have two options:\n\nManually set up a virtual private server for deploying your app.\nHost the app in a GitHub repository and deploy it to a cloud platform.\n\nThe first option gives you full control. You can set up everything locally. But it can take time‚Äîfor both the setup and the maintenance (like keeping OS up-to-date, etc.).\n\nThe second option is the simplest. Just push your app to GitHub. If it‚Äôs properly configured with a cloud platform, it‚Äôll automatically update your code changes.\n\nWhy use Community Cloud?\n\nHere‚Äôs why you might want to use Community Cloud to host your apps:\n\nAdvantages\tDescription\nFree\tYou can deploy Streamlit apps for free!\nDeploy in one click\tYour fully hosted app is ready to be shared in under a minute.\nKeep your code in your repo\tNo changes to your development process. Your code stays on GitHub.\nLive updates\tYour apps update instantly when you push code changes.\nSecurely connect to data\tConnect to all your data sources using secure protocols.\nRestrict access to apps\tAuthenticate viewers with per-app viewer allow-lists.\nEasily manage your apps\tView, collaborate, and manage all your apps in a single place.\nStep 1. Create a simple Streamlit app\n\nFirst, let‚Äôs make a simple app that prints out Hello world! . It takes only two lines of code (for a deeper dive, read this post).\n\nGo ahead and create a streamlit_app.py file:\n\nimport streamlit as st\n\nst.write('Hello world!')\n\nStep 2. Set up an account on Community Cloud\n\nGo to Community Cloud and click ‚ÄúSign up‚Äù to create a free account with your existing Google, GitHub, or email account:\n\nNext, enter your GitHub credentials and click on ‚ÄúAuthorize streamlit‚Äù to let Streamlit access your GitHub account:\n\nFinally, enter your information and click ‚ÄúContinue‚Äù:\n\nCongratulations! You have signed up for your workspace in Community Cloud.\n\nStep 3. Connect your account to GitHub\n\nThere are two options to connect your Community Cloud account to GitHub:\n\nOption ¬†1\n\nClick on ‚ÄúNew app‚Äù:\n\nOn the authorization page, click on ‚ÄúAuthorize streamlit.\"\n\nOption 2\n\nClick ‚ÄúSettings,‚Äù then ‚ÄúLinked accounts,‚Äù then ‚ÄúAllow access‚Äù:\n\nThis will let Community Cloud deploy your Streamlit apps from your GitHub repositories. On the authorization page, click ‚ÄúAuthorize streamlit.\"\n\nGitHub-linked account\n\nOnce you log in, Community Cloud will get access to your GitHub account:\n\nNow you‚Äôre ready to deploy Streamlit apps!\n\nBut first, let‚Äôs create a GitHub repo.\n\nStep 4. Create a GitHub repo of your app\n\nClick ‚Äú+‚Äù and then ‚ÄúNew repository‚Äù:\n\nThis will bring you to the ‚ÄúCreate a new repository‚Äù page.\n\nIn the field ‚ÄúRepository name‚Äù type in st-hello-world, click ‚ÄúPublic,‚Äù check ‚ÄúAdd a README file‚Äù (your new repo will get a README.md file), and click ‚ÄúCreate repository‚Äù:\n\nYour repo is all set up!\n\nNow create your app file:\n\nNext, create the streamlit_app.py:\n\nYour GitHub repo will be populated with README.md and streamlit_app.py files.\n\nNext, head back to Community Cloud:\n\nStep 5. Deploy your app in a few clicks\n\nAt last, here comes the fun part. You get to deploy your app!\n\nClick ‚ÄúNew app\" and fill out the information for your app:\n\nThis will spin up a new server. You‚Äôll see the message, ‚ÄúYour app is in the oven.‚Äù\n\nIn the bottom right-hand corner, click ‚ÄúManage app‚Äù to see the log messages (use them for debugging and troubleshooting errors):\n\nThe side menu displays all log messages in real-time:\n\nOnce your app finishes compiling, you‚Äôll see the output. In our example, it‚Äôll be a simple message: Hello world!\n\nWrapping up\n\nCongratulations! You have successfully deployed your app to Streamlit Community Cloud. Now you can share the app URL with the community.\n\nIf a tutorial video is your thing, check out the following video:\n\nRead more about:\n\nSelf-hosting Streamlit apps on your own servers (AWS, Azure, etc.).\nDifferent Streamlit use cases from the community.\n\nIf you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Llama2-chatbot-screencast_scaling-0.5_fps-15_speed-10.0_duration-0-48.gif (1165√ó795)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-chatbot-screencast_scaling-0.5_fps-15_speed-10.0_duration-0-48.gif",
    "html": ""
  },
  {
    "title": "Collecting user feedback on ML in Streamlit",
    "url": "https://blog.streamlit.io/collecting-user-feedback-on-ml-in-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCollecting user feedback on ML in Streamlit\n\nImprove user engagement and model quality with the new Trubrics feedback component\n\nBy Jeff Kayne and Joel Hodgson\nPosted in Advocate Posts, May 4 2023\nHow to pip install the FeedbackCollector from the trubrics-sdk\nHow to get started with just 3 lines of code [Beginner]\nHow to collect complex feedback [Advanced]\nHow to manage your feedback in the Trubrics platform [Optional]\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nI'm Jeff, co-founder of Trubrics. We build tools to help data scientists collect user feedback on machine learning (ML). We've developed a new component that enables you to do just that with a few lines of code in your Streamlit app!\n\nWhy collect user feedback on ML?\n\nImproving model performance. User feedback provides insights into the strengths and weaknesses of the model from the user's perspective. This information enables fixes and fine-tuning of the ML model, ultimately improving its performance.\nEnhancing user experience. User feedback can help identify user preferences and pain points. This information can be used to design better user interfaces, improve model usability, and ultimately enhance the user experience.\nIncreasing user engagement. Collecting user feedback shows that you care about your users' opinions and are committed to providing the best possible experience. This can increase user engagement, satisfaction, adoption, and loyalty.\nGrowing responsibility of the ML group. Adding diversity and cross-functional teams to the ML review process helps us strive towards building #responsibleAI and #ethicalAI.\nContinuous model improvement and monitoring. Collecting user feedback in production can help you improve your ML model by incorporating bug fixes, user needs, and preferences.\n\n‚ö†Ô∏è\nThis blog post refers to a previous version of our component trubrics-sdk==1.3.6 . For an updated blog, please see here.\n\nIn this post, you'll learn:\n\nHow to pip install the FeedbackCollector from the trubrics-sdk\nHow to get started with just 3 lines of code [Beginner]\nHow to collect complex feedback [Advanced]\nHow to manage your feedback in the Trubrics platform [Optional]\nüëâ\nCan't wait to try it? Here are the links to our demo app, repo, and docs.\nHow to pip install the FeedbackCollector from the trubrics-sdk\n\nTrubrics embeds some Streamlit components, so you'll need to install both libraries:\n\npip install \"trubrics[streamlit]\"==1.3.6\n\n\nBy the way, Trubrics is tested for Python versions 3.7, 3.8, and 3.9.\n\nHow to get started with just 3 lines of code [Beginner]\n\nAdd these three lines of code to your Streamlit app/a blank Python script (if you're just getting started):\n\nfrom trubrics.integrations.streamlit import FeedbackCollector\n\ncollector = FeedbackCollector()\ncollector.st_feedback(feedback_type=\"issue\")\n\n\nWhat's going on here? Let's break it down:\n\nThe FeedbackCollector object provides feedback functionality and static metadata about your app (e.g., model version). Create an instance of this object once at the top of your app.\nThe st_feedback() method allows you to embed feedback components in your app and save different types of feedback to .json files. Use multiple instances of this method to add different feedback collection points around your app.\n\nNow you can launch your app with streamlit run basic_app.py !\n\nYou'll see a feedback component in your app that will let you start collecting qualitative feedback from your users:\n\nTry other types of quantitative feedback collection, such as:\n\ncollector.st_feedback(feedback_type=\"faces\")\n\n\nfeedback = collector.st_feedback(\n\tfeedback_type=\"thumbs\",\n\tpath=\"thumbs_feedback.json\"\n)\n\n# print out the feedback object as a dictionary in your app\nfeedback.dict() if feedback else None\n\n\nThe st_feedback() method returns a feedback object that can be manipulated (in this case, we're printing it out in the app). And it saves a feedback .json to a specified path, such as path=\"thumbs_feedback.json\". To save multiple files, use a dynamic path, such as path=f\"thumbs_{timestamp}.json\" with a time stamp.\n\nTo learn more about the different feedback types and their available options, read our docs.\n\nHow to collect complex feedback [Advanced]\n\nYou can use the Trubrics FeedbackCollector to collect more complex feedback with type=\"custom\". This is super useful for collecting forms or survey responses with multiple questions:\n\nimport streamlit as st\nfrom trubrics.integrations.streamlit import FeedbackCollector\ncollector = FeedbackCollector()\nq1 = st.text_input(\"Q 1\")\nq2 = st.text_input(\"Q 2\")\nq3 = st.text_input(\"Q 3\")\nif q1 and q2 and q3:\n    button = st.button(label=\"submit\")\n    if button:\n        feedback = collector.st_feedback(\n            \"custom\",\n            user_response={\n                \"Q 1\": q1,\n                \"Q 2\": q2,\n                \"Q 3\": q3,\n            },\n            path=\"./feedback.json\",\n        )\n        feedback.dict() if feedback else None\n\n\nThis lets you use any Streamlit components to create the feedback form of your choice.\n\nüëâ\nTIP: For creating more robust forms in Streamlit, check out st.form.\n\nYou can use Trubrics' \"faces\" and \"thumbs\" feedback UI components in your custom feedback forms, as shown here.\n\nHow to manage your feedback in the Trubrics platform [Optional]\n\nTo manage and collaborate on feedback issues more effectively, we offer functionality that enables you to save feedback directly to our platform from your Streamlit app. This also allows users to authenticate within the apps and track who has recorded what feedback:\n\nWrapping up\n\nAnd...here is the final app!\n\nThank you for reading our story. We hope that you're now armed and ready to start collecting feedback on your ML projects. Please try out our component and let us know any feedback. And if you have any questions or ideas, we'd be very happy to hear from you. Drop us a message in the comments below, or contact us on GitHub, LinkedIn, or via email.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "github-repo.png (1899√ó939)",
    "url": "https://blog.streamlit.io/content/images/2023/08/github-repo.png#browser",
    "html": ""
  },
  {
    "title": "confusion-matrix.png (961√ó938)",
    "url": "https://blog.streamlit.io/content/images/2023/08/confusion-matrix.png#border",
    "html": ""
  },
  {
    "title": "model-training-performance-graph.png (1144√ó499)",
    "url": "https://blog.streamlit.io/content/images/2023/08/model-training-performance-graph.png#border",
    "html": ""
  },
  {
    "title": "app-overview.png (1512√ó717)",
    "url": "https://blog.streamlit.io/content/images/2023/08/app-overview.png#border",
    "html": ""
  },
  {
    "title": "How to master Streamlit for data¬†science",
    "url": "https://blog.streamlit.io/how-to-master-streamlit-for-data-science/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to master Streamlit for data¬†science\n\nThe essential Streamlit for all your data¬†science needs\n\nBy Chanin Nantasenamat\nPosted in Tutorials, January 18 2022\nThe essential Streamlit for all your data science needs\n1. Getting up to speed\n1.1. Why deploy models?\n1.2. Types of model deployment\n1.3. Traditional vs. low-code web frameworks\n2. Streamlit 101\n2.1. What you need to use Streamlit\n2.2. Four Streamlit design principles\n3. How to set up your Streamlit workspace\n3.1. Install Streamlit\n3.2. Code a Streamlit app\n3.3. Launch your Streamlit apps\n3.4. Create a conda environment\n4. Practice by building the ‚ÄúHello, World!‚Äù app\n5. Build your Own Streamlit App\n5.1. Elements of a Streamlit app\n5.2. The ‚ÄúBrains‚Äù of the Streamlit app\n5.3. Lay out the app\n6. Deploy your Streamlit app\n7. Resources\n7.1. Documentation\n7.2. Discussion forum\n7.3. YouTube tutorials\n7.4. Books\nWrapping up\nContents\nShare this post\n‚Üê All posts\nThe essential Streamlit for all your data science needs\n\nTo build a web app you‚Äôd typically use such Python web frameworks as Django and Flask. But the steep learning curve and the big time investment for implementing these apps present a major hurdle.\n\nStreamlit makes the app creation process as simple as writing Python scripts!\n\nIn this article, you‚Äôll learn how to master Streamlit when getting started with data science.\n\nLet‚Äôs dive in!\n\n1. Getting up to speed\n1.1. Why deploy models?\n\nThe data science process boils down to converting data to knowledge/insights while summarizing the conversion with the CRISP-DM and OSEMN data frameworks. Aside from knowledge/insights, a data project can make a greater impact if you deploy your machine learning models as web apps.\n\nWhy? Because deployed models can be accessed by stakeholders who can play with them and figure out what works and what doesn't.\n\n1.2. Types of model deployment\n\nModel deployment is the endpoint of a data science workflow. Models can be deployed as:\n\nJupyter notebooks\nAPI\nWeb apps\n\nJupyter notebooks. Jupyter notebooks are commonly used for prototyping the data science workflow and they can be:\n\nUploaded to GitHub\nShared as a link via Google Colab\nShared via Binder\n\nAPI. Models can also be deployed as a REST API using tools such as FastAPI. This approach does not have a frontend for displaying it graphically for ease of use.\n\nWeb apps. This brings us to deploying machine learning models as web applications. The traditional approach is to wrap the API via the use of web frameworks such as Django and Flask. A much simpler approach is to use a low-code solution such as Streamlit to create a web app.\n\nLet‚Äôs explore this in more depth in the following section.\n\n1.3. Traditional vs. low-code web frameworks\n\nThough Django and Flask may be the gold standard for developing web apps, the technical barriers limit their usage by the wider data community. Low-code solutions such as Streamlit have lowered the barrier to entry by enabling data enthusiasts (e.g. data scientists, analysts, and hobbyists) to easily convert machine learning models into interactive data-driven web apps.\n\nHere are the low-code solutions:\n\n2. Streamlit 101\n\nStreamlit is a Python library you can use to build interactive data-driven web apps.\n\n2.1. What you need to use Streamlit\n\nTo use Streamlit, you need to:\n\nHave basic Python knowledge.\nWrite scripts to perform specific tasks (like taking several Excel files as input and combining them into one).\nBuild and grow the Streamlit app line by line instead of starting with a predefined layout (it takes only a few lines of code).\n\nIf you can do all this, congratulations! You're ready to plunge into the world of Streamlit.\n\n2.2. Four Streamlit design principles\n\nAccording to Adrien Treuille, co-founder and CEO of Streamlit, Streamlit was originally based on three principles (as mentioned in the 2019 PyData LA talk and the Medium launch post). The fourth principle was introduced at the launch of Streamlit Cloud:\n\nEmbrace Python scripting. Build and grow Streamlit apps line by line.\nTreat widgets as variables. Widgets are input elements that let users interact with Streamlit apps. They‚Äôre presented as basic input text boxes, checkboxes, slider bars, etc.\nReuse data and computation. Historically, data and computations had been cached with the @st.cache decorator. This saves computational time for app changes. It can be hundreds of times if you actively revise the app! In 0.89.0 release Streamlit launched two new primitives (st.experimental_memo and st.experimental_singleton) to afford a significant speed improvement to that of @st.cache.\nDeploy instantly. Easily and instantly deploy apps with Streamlit Cloud.\n3. How to set up your Streamlit workspace\n3.1. Install Streamlit\n\nInstall the Streamlit library by using pip:\n\npip install streamlit\n\n3.2. Code a Streamlit app\n\nAfter doing so, you can start to code an app by creating a Python script file (e.g. app.py). Inside this file, you can import the Streamlit library via import streamlit as st and use any of the available Streamlit functions.\n\n3.3. Launch your Streamlit apps\n\nOnce the app has been coded, launching it is as easy as running streamlit run app.py.\n\nFor first time users, you can also type ¬†streamlit hello ¬†into the command line to see Streamlit in action.\n\nGo ahead and give it a try!\n\n3.4. Create a conda environment\n\nAs I show on my YouTube channel, I like to house my Streamlit apps in a dedicated conda environment. This way the library dependencies don‚Äôt get entangled with my other Python libraries. I recommend you do the same.\n\nBegin by building an EDA app. Read how I created a dedicated conda environment in the GitHub repo‚Äôs readme.md file, then watch this tutorial video on How to Build an EDA app using Pandas Profiling and follow these steps:\n\nStep 1. Create a conda environment called eda:\n\nconda create -n eda python=3.7.9\n\n\nStep 2. Activate the eda environment:\n\nconda activate eda\n\n\nStep 3. Install prerequisite libraries by downloading the requirements.txt file (it contains the library version numbers):\n\nwget https://raw.githubusercontent.com/dataprofessor/eda-app/main/requirements.txt\n\n\nStep 4. Install libraries via pip:\n\npip install -r requirements.txt\n\n\nStep 5. Download and unzip contents from the GitHub repo: https://github.com/dataprofessor/eda-app/archive/main.zip\n\nStep 6. Launch the app:\n\nstreamlit run app.py\n\n\nYou‚Äôll see the web app browser pop up:\n\nThe functionality of this EDA app leverages the capabilities of pandas-profiling:\n\nCongratulations! You now know how to clone a Streamlit app from a GitHub repo, setup a dedicated conda environment, and successfully launch the app!\n\nNext, customize the app to your own liking.\n\n4. Practice by building the ‚ÄúHello, World!‚Äù app\n\nNow that you know Streamlit principles, let‚Äôs build some apps. It‚Äôs not as hard as you may think. A typical rite of passage for learning any new programming language is to start with printing Hello World!.\n\nHere is how to do it in Streamlit in four easy steps:\n\nStep 1. Launch your favorite code editor (Atom.io, VS Code, etc.).\n\nStep 2. Create a file and name it app.py.\n\nStep 3. Add this code into the app.py file:\n\nimport streamlit as st\n\nst.write('Hello world!')\n\n\nStep 4. Launch the app by typing this into the command line:\n\nstreamlit run app.py\n\n5. Build your Own Streamlit App\n\nI like to start my Streamlit projects by coding the ‚Äúbrains‚Äù of the app on Google Colab.\n\nAt a high level, a web app is comprised of three key elements:\n\nInput. Widgets make it possible to take in user input. They can be sliders, text/number boxes, file upload widgets, etc.\nThe ‚Äúbrains‚Äù of the app. The brains or the inner workings of the app is what differentiates one app from another. It performs the function of transforming user inputs into outputs.\nOutput. This can be anything: DataFrame printouts, images, plots, numerical values, text, or embeddings of audio, videos, and tweets.\n\nBecause web apps adopt a similar structure, some of the elements used in one project can be repurposed and reused in the next project.\n\n5.1. Elements of a Streamlit app\nContents (text, images, embedded videos, audio, tweets, etc.)\nWidgets\nAuxillaries (balloons, code box, etc.)\n5.2. The ‚ÄúBrains‚Äù of the Streamlit app\nUse Streamlit components\nMake use of existing functions from Python libraries of interest\nCode your own custom function\n5.3. Lay out the app\n\nI like to put widgets on the left-hand sidebars of the app. Use st.sidebar. in front of any widget functions of interest (instead of st.). For example, to place the text input box in the sidebar, use st.sidebar.text_input(‚ÄòName‚Äô) instead of st.text_input(‚ÄòName‚Äô) (which would place the text input box into the main panel).\n\n6. Deploy your Streamlit app\n\nNow that you‚Äôve built a Streamlit app, deploy it to the cloud for general public access. The easiest way to do this is with Streamlit Cloud:\n\nCreate a GitHub repo of the app files (app.py, requirements.txt, and dependency files)\nIn Streamlit Cloud, link your GitHub account and select the app‚Äôs repo to deploy.\n\nYou can also watch this video on how to deploy Streamlit apps.\n\n7. Resources\n7.1. Documentation\n\nStreamlit‚Äôs documentation website is the best place to get you started:\n\nStreamlit library. A guide on how to build Streamlit apps with various Streamlit functions (the API reference), ‚ÄúGet started‚Äù tutorials, and a cheat sheet.\nStreamlit Cloud. Everything you need to know on how to deploy apps to Streamlit Cloud.\nKnowledge base. A growing collection of tutorial articles and FAQs about using Streamlit and troubleshooting problems.\n7.2. Discussion forum\n\nCan‚Äôt find the information on the documentation website? Check out the Streamlit discussion forum and read the troubleshooting guide for tips on resolving problems.\n\n7.3. YouTube tutorials\n\nHere‚Äôs a list of these YouTube channels about Streamlit:\n\nStreamlit. The official Streamlit YouTube channel with official announcements of the latest features.\nData Professor. My YouTube channel with videos on data science and bioinformatics and a growing playlist of 30 videos on Streamlit.\nJCharisTech. Jesse‚Äôs YouTube channel with tutorial videos on Python and Streamlit and with a playlist of almost 70 videos on Streamlit.\n1littlecoder. AbdulMajed‚Äôs YouTube channel with tutorial videos on Python and Streamlit and with a playlist of almost 20 videos on Streamlit.\n7.4. Books\n\nTyler Richards wrote a book titled Getting Started with Streamlit for Data Science: Create and Deploy Streamlit Web Applications from Scratch in Python. It takes readers on a journey of how they can build interactive data-driven apps. The last chapter was super-fun to read as Tyler interviewed Streamlit power users.\n\nI also interviewed Tyler in the hour-long podcast Data Science Podcast with Tyler Richards - Facebook Data Scientist. We talked about his journey into data science, his experience as a data scientist, and his thoughts and inspiration for writing a book about Streamlit.\n\nWrapping up\n\nYou‚Äôve learned Streamlit essentials that will start you on building your own interactive data-driven Python apps. Well done! Of course, there‚Äôs always more to learn. Feel free to drop a comment or a suggestion below on the topics that you'd like to learn more about.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "30 Days of Streamlit",
    "url": "https://blog.streamlit.io/30-days-of-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n30 Days of Streamlit\n\nA fun challenge to learn and practice using Streamlit\n\nBy Chanin Nantasenamat\nPosted in Advocate Posts, April 1 2022\nWhat is 30 days of Streamlit?\nContent\nHow to participate\nContents\nShare this post\n‚Üê All posts\nWhat is 30 days of Streamlit?\n\n\nWe're kicking off #30DaysOfStreamlit‚Äîa social challenge for getting up to speed on building and deploying Streamlit apps. Each day a new challenge will be released for participants to tackle. Inspired by Ken Jee‚Äôs #66daysofdata, the #100daysofcode challenge, and Kaggle‚Äôs #30DaysofML, this initiative is a fun opportunity to learn, create, share, and earn.\n\nüëâ Check out the app to get started now!\n\nContent\n\n\nThe challenge will move through three levels of difficulty: beginner, intermediate, and advanced.\n\nüìÜ Days 1-7 will cover beginner tasks such as setting up a local and cloud coding environment, installing the Streamlit library, and building your first Streamlit Hello World app.\n\nüìÜ Days 8-23 will cover more intermediate topics. Each day may highlight a Streamlit command to use for creating and deploying a simple Streamlit app (e.g., build a simple app that uses the st.download_button command).\n\nüìÜ Days 24-30 will enter more advanced subjects. Learn about session states and things like efficient data and memory handling for Streamlit apps.\n\nHow to participate\n\n\nFollow our daily posts on Twitter and LinkedIn or simply visit the 30 days app. Share your progress and creations using the #30DaysOfStreamlit hashtag.\n\nüéà Happy Streamlit-ing!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Quests: Getting started with Streamlit",
    "url": "https://blog.streamlit.io/streamlit-quests-getting-started-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit Quests: Getting started with Streamlit\n\nThe guided path for learning Streamlit\n\nBy Chanin Nantasenamat\nPosted in Tutorials, November 18 2022\nTwo quests\nInstall Streamlit\nRun the demo app\nCreate a single-page app\nUse the Streamlit App Starter Kit\nRead the Documentation\nComplete the #30DaysOfStreamlit challenge\nGet inspiration by exploring Streamlit apps in the Gallery\nUse or create Streamlit Components\nCreate a multipage app\nUse the Streamlit Multipage App Starter Kit\nDeploy a Streamlit app on Streamlit Community Cloud\nShare your Streamlit app\nGet unstuck by asking the forum\nRead our blog to stay updated on the latest developments\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nStreamlit is a Python library that makes building beautiful, interactive apps in a few lines of code easy. But every new library has its quirks and conventions, and it takes time to learn. You might be wondering where to start and if there are any resources. No worries. I got you covered!\n\nLet‚Äôs learn Streamlit with this guide called Streamlit Quests. It‚Äôs inspired by role-playing games where players navigate the landscape by completing a sequential set of tasks‚Äîquests.\n\nTwo quests\n\nYour learning journey will have two quests:\n\nüë®‚Äçüíª Expert Quest. This is a sequential track. Learning starts as easy and gradually becomes more complex.\nüéà Community Quest. This is a non-sequential track. You can refer to these resources at any point in your journey.\n\nThe topics checklist below serves a dual purpose: a table of contents and a to-do checklist that you can refer to at any time (think of it as your compass in your learning journey of Streamlit):\n\n¬†¬†‚ñ¢\t¬†\tInstall Streamlit üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tRun the demo app via streamlit hello üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tCreate a single-page app üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tUse the Streamlit App Starter Kit to quickly build a single-page app üéà\n¬†¬†‚ñ¢\t¬†\tRead the Documentation for specific information on Streamlit commands at https://docs.streamlit.io üéà\n¬†¬†‚ñ¢\t¬†\tComplete the #30DaysOfStreamlit challenge at https://30days.streamlit.app (the first three weeks are beginner-friendly, while the last week is more advanced) üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tGet inspiration by exploring Streamlit apps in the Gallery at https://streamlit.io/gallery üéà\n¬†¬†‚ñ¢\t¬†\tUse or create Streamlit Components (third-party modules that extend Streamlit functionality) at https://streamlit.io/components üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tExplore hundreds of components from the Streamlit Components Hub üéà\n¬†¬†‚ñ¢\t¬†\tCreate a multipage app üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tUse the Streamlit Multipage App Starter Kit to quickly build a multipage app üéà\n¬†¬†‚ñ¢\t¬†\tDeploy a Streamlit app on Streamlit Community Cloud at https://streamlit.io/cloud üë®‚Äçüíª\n¬†¬†‚ñ¢\t¬†\tShare your Streamlit app on Twitter/LinkedIn and tag us @streamlit üéà\n¬†¬†‚ñ¢\t¬†\tGet unstuck by asking the forum at https://discuss.streamlit.io üéà\n¬†¬†‚ñ¢\t¬†\tRead our blog to stay updated on the latest developments and use cases at https://blog.streamlit.io üéà\n\nüëâ\nNOTE: Emojis at the end of each task mark tasks as part of the Expert or the Community Quest.\n\nLet‚Äôs get started!\n\nInstall Streamlit\n\nThe simplest way to install Streamlit is by using pip. Just type the following into the command line:\n\npip install streamlit\n\nRun the demo app\n\nAfter installing Streamlit, run the demo app by typing the following into the command line:\n\nstreamlit hello\n\n\nIn a few moments, a new browser should launch, displaying the following demo app:\n\nIt showcases a wide range of Streamlit functionalities to show you what you can build.\n\nCreate a single-page app\n\nIn most cases, a basic app that performs a new task could be performed by a single-page app. Conceptually, the app has a single page that‚Äôs sitting in the streamlit_app.py file.\n\nLearn how to build a single-page app in this article.\n\nOr better yet‚Ä¶\n\nUse the Streamlit App Starter Kit\n\nUse the starter code from the Streamlit App Starter Kit to get a template app up and running in just a few minutes (learn how in this article).\n\nRead the Documentation\n\nIn-depth coverage of every Streamlit command, along with code examples, is provided in the Streamlit Documentation at https://docs.streamlit.io. There are also Getting Started articles, cheat sheets, tutorials, and knowledge base articles. In addition to coverage of the Streamlit library, there‚Äôs also content on the Streamlit Community Cloud.\n\nWhen building Streamlit apps, I keep the Documentation handy for quick and easy reference. I can always find a suitable Streamlit command or code examples to repurpose for my apps.\n\nComplete the #30DaysOfStreamlit challenge\n\n30 Days of Streamlit helps new users learn the Streamlit library. We launched it on April 1, 2022, releasing a new challenge daily (with three difficulty levels). Then we compiled them all into a public app https://30days.streamlitapp.com.\n\nThe app encourages you to share your progress with the community by posting it on Twitter or LinkedIn with the hashtag #30DaysOfStreamlit or by tagging @streamlit (so we can retweet it). It‚Äôs now available in Portuguese, French, Spanish, and Russian.\n\nWant to help translate it into your language? Go to https://github.com/streamlit/30days to get started.\n\nGet inspiration by exploring Streamlit apps in the Gallery\n\nThe Streamlit Gallery (available at https://streamlit.io/gallery) is a collection of the best apps built with our framework. Here you can find inspiration for your next app by browsing through the apps or learning how to build a particular type of app by looking at their code. The apps are categorized by topic: science and technology, finance and business, data visualization, etc.\n\nUse or create Streamlit Components\n\nStreamlit components are third-party modules that extend Streamlit‚Äôs functionality of Streamlit. A curated collection is provided at https://streamlit.io/components.\n\nTo use a Streamlit component such as AgGrid:\n\n1. ¬†Install via pip as follows:\n\npip install streamlit-aggrid\n\n\n2. ¬†Use in a Streamlit app by simply importing the component and using its function:\n\nfrom st_aggrid import AgGrid\n\nAgGrid(my_dataframe)\n\n\nTo create your own Streamlit component, refer to some of these excellent articles:\n\nHow to build your own Streamlit component: Learn how to make a component from scratch!\nIntroducing Streamlit Components: A new way to add and share custom functionality for Streamlit apps\nDocumentation on Custom Components (contains additional information on creating and publishing components as well as the components API)\nStreamlit Components, security, and a five-month quest to ship a single line of code\n\nAnd check out this 2-part tutorial video series:\n\nHow to build a Streamlit component - Part 1: Setup and architecture\nHow to build a Streamlit component - Part 2: Make a slider widget\nCreate a multipage app\n\nA more complex app may require several pages. As a result, you might want to look into building a multipage app. The app consists of two major components:\n\nThe main page that serves as the entry point of the multipage app.\nOne of several pages that reside inside a pages folder is called upon when users click on the page of interest from the left sidebar panel.\n\nLearn how to build a multipage app in this article.\n\nUse the Streamlit Multipage App Starter Kit\n\nJust like with the Streamlit App Starter Kit for single-page apps, check out the fully functional early version of the Streamlit Multipage App Starter (an article about this coming soon) to make a multipage app in no time.\n\nDeploy a Streamlit app on Streamlit Community Cloud\n\nLet‚Äôs say you‚Äôve already built your Streamlit app and want to share it with the community. You can share it by using Streamlit Community Cloud at https://streamlit.io/cloud.\n\nTo deploy to the Community Cloud:\n\nUpload or Git-push app files to a GitHub repository\nFrom within Community Cloud, click on ‚ÄúNew app,‚Äù then select repo, branch, and app file.\n\nThat‚Äôs it! Once the app is up and running, share its uniquely generated URL with the community.\n\nShare your Streamlit app\n\nReady to share your Streamlit app creation with the community? Hop on Twitter or LinkedIn and tag us with @streamlit. It‚Äôs a great way to contribute to the community and receive helpful and constructive feedback for improving your app.\n\nGet unstuck by asking the forum\n\nAre you encountering errors when creating Streamlit apps? To get unstuck, try the following:\n\nRead the error logs displayed in the command line as apps load. These errors will tell you exactly why certain aspects of the app may fail to load or display. They will also hint at which dependent libraries may be the root of the problem.\nSearch Google, Stack Overflow, or the Streamlit Forum ( https://discuss.streamlit.io) to see if there are related posts that may already have a solution.\nIf you‚Äôve done the above and are still stuck, post your question on the Streamlit Forum. See this article How to post a question in the Streamlit forum to craft a thoughtful and practical question.\nRead our blog to stay updated on the latest developments\n\nBlog posts are a great way to stay updated on the latest developments and use cases, especially regarding the Streamlit web framework.\n\nThe Streamlit Blog (available at https://blog.streamlit.io) is home to 106 articles (as of this writing) that provide timely information on new features, product releases, and other news that can help you stay ahead of the curve. It also features guest posts from experts in the field, which can provide valuable insights into best practices and real-world applications as they share their first-hand experiences.\n\nWrapping up\n\nCongratulations! You‚Äôve been acquainted with all the essential resources for building Streamlit apps. It‚Äôs time to take what you‚Äôve learned and create something extraordinary!\n\nIf you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "image--2-.gif (1012√ó459)",
    "url": "https://blog.streamlit.io/content/images/2023/07/image--2-.gif",
    "html": ""
  },
  {
    "title": "api-keys-survey.png (1170√ó650)",
    "url": "https://blog.streamlit.io/content/images/2023/05/api-keys-survey.png",
    "html": ""
  },
  {
    "title": "image.gif (1096√ó825)",
    "url": "https://blog.streamlit.io/content/images/2023/07/image.gif",
    "html": ""
  },
  {
    "title": "William Huang - Streamlit",
    "url": "https://blog.streamlit.io/author/william/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by William Huang\n3 posts\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nProduct\nby\nLukas Masuch and¬†\n2\n¬†more,\nFebruary 28 2023\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nMake your st.pyplot interactive!\n\nLearn how to make your pyplot charts interactive in a few simple steps\n\nTutorials\nby\nWilliam Huang\n,\nJune 23 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "streamlit-secrets-management.png (1794√ó1188)",
    "url": "https://blog.streamlit.io/content/images/2023/05/streamlit-secrets-management.png#browser",
    "html": ""
  },
  {
    "title": "ezgif.com-gif-maker--11-.gif (1024√ó1028)",
    "url": "https://blog.streamlit.io/content/images/2022/07/ezgif.com-gif-maker--11-.gif#browser",
    "html": ""
  },
  {
    "title": "throwaway-keys-updated.png (1891√ó900)",
    "url": "https://blog.streamlit.io/content/images/2023/05/throwaway-keys-updated.png",
    "html": ""
  },
  {
    "title": "set-key-limits.png (999√ó1390)",
    "url": "https://blog.streamlit.io/content/images/2023/05/set-key-limits.png",
    "html": ""
  },
  {
    "title": "ask-my-pdf-screenshot-1.png (1520√ó1426)",
    "url": "https://blog.streamlit.io/content/images/2023/05/ask-my-pdf-screenshot-1.png#border",
    "html": ""
  },
  {
    "title": "14.png (1550√ó668)",
    "url": "https://blog.streamlit.io/content/images/2022/07/14.png#browser",
    "html": ""
  },
  {
    "title": "image--1-.gif (800√ó363)",
    "url": "https://blog.streamlit.io/content/images/2023/07/image--1-.gif",
    "html": ""
  },
  {
    "title": "what-is-api-key-3.png (2421√ó458)",
    "url": "https://blog.streamlit.io/content/images/2023/05/what-is-api-key-3.png",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-26-at-2.59.00-PM.png (1398√ó708)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.59.00-PM.png#browser",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-26-at-2.57.52-PM-1.png (1393√ó670)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.57.52-PM-1.png#browser",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-27-at-10.31.36-AM.png (1394√ó588)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-27-at-10.31.36-AM.png#browser",
    "html": ""
  },
  {
    "title": "Llama2-API-in-app.png (2000√ó1415)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-API-in-app.png",
    "html": ""
  },
  {
    "title": "Screen-Shot-2022-07-26-at-2.55.08-PM-1.png (1396√ó776)",
    "url": "https://blog.streamlit.io/content/images/2022/07/Screen-Shot-2022-07-26-at-2.55.08-PM-1.png#browser",
    "html": ""
  },
  {
    "title": "6.png (696√ó329)",
    "url": "https://blog.streamlit.io/content/images/2022/07/6.png#browser",
    "html": ""
  },
  {
    "title": "Llama2-API-via-st-secrets-1.png (2000√ó1415)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-API-via-st-secrets-1.png",
    "html": ""
  },
  {
    "title": "Llama2-Community-Cloud-st-secrets.png (2000√ó1508)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-Community-Cloud-st-secrets.png",
    "html": ""
  },
  {
    "title": "chat2vis_eg3.png (2000√ó1181)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_eg3.png",
    "html": ""
  },
  {
    "title": "chat2vis_eg1.png (2000√ó1171)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_eg1.png",
    "html": ""
  },
  {
    "title": "architecture.png (2000√ó619)",
    "url": "https://blog.streamlit.io/content/images/2023/07/architecture.png",
    "html": ""
  },
  {
    "title": "chat2vis_db.png (946√ó532)",
    "url": "https://blog.streamlit.io/content/images/2023/07/chat2vis_db.png#browser",
    "html": ""
  },
  {
    "title": "Llama2-Community-Cloud-settings.png (2000√ó978)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-Community-Cloud-settings.png",
    "html": ""
  },
  {
    "title": "image.png (728√ó441)",
    "url": "https://blog.streamlit.io/content/images/2022/07/image.png#browser",
    "html": ""
  },
  {
    "title": "LangChain-question-answering_scaling-0.3_fps-18_speed-10.0_duration-0-60.gif (666√ó538)",
    "url": "https://blog.streamlit.io/content/images/2023/06/LangChain-question-answering_scaling-0.3_fps-18_speed-10.0_duration-0-60.gif",
    "html": ""
  },
  {
    "title": "Llama2-Replicate-API-token.png (2238√ó1688)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-Replicate-API-token.png",
    "html": ""
  },
  {
    "title": "Create a search engine with Streamlit and Google Sheets",
    "url": "https://blog.streamlit.io/create-a-search-engine-with-streamlit-and-google-sheets/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCreate a search engine with Streamlit and Google Sheets\n\nYou‚Äôre sitting on a goldmine of knowledge!\n\nBy Sebastian Flores Benner\nPosted in Advocate Posts, March 14 2023\nStep 1: Set up a Google Sheet with your data\nStep 2: Use Streamlit to read the data from the Google Sheet\nStep 3: Build a user interface and search functionality using Streamlit\nBonus step: Other app ideas\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHi! I'm Sebastian Flores. You might remember me from my posts on creating interactive books and fostering data processing innovation.\n\nIn 2022 I attended the PyCon Chile conference and saw it was a gold mine for educational content. From the years 2020, 2021, and 2022 alone, there were 150+ YouTube recordings on Python and related topics! The downside? It was hard to find a specific recording by title or by the speaker‚Äôs name.\n\nSo on the bus ride back, I coded a working MVP of a Streamlit app to solve it. The main ingredients were Streamlit, pandas, and Google Sheets (for easy collaboration and updates). When I was done, I realized this combo could be used for other projects. I hope this post sparks your imagination. If you build an app, please share it with me on Twitter or LinkedIn (and don't forget to tag @Streamlit as well!).\n\nNow, let's start our journey. There are three steps:\n\nStep 1: Set up a Google Sheet with your data\nStep 2: Use Streamlit to read the data from the Google Sheet\nStep 3: Build the user interface and search functionality using Streamlit\nBonus step: Other app ideas\nüëâ\nIf you want to take a look, here's my app and the repo for it.\nStep 1: Set up a Google Sheet with your data\n\nFirst, you need to create a Google Sheet that contains your information. In my case, it included columns for the year, speaker, title, and talk description.\n\nNext, create two links:\n\nA public link with view permissions to be freely shared. The app will use this link to read the data (you can safely put it directly into the code).\nA private link shared with the people who can edit the data. Click the \"Share\" button in the top right corner and add the email addresses of the users you want to share with.\n\nThe database should look something like this:\n\nHere is the public link.\n\nStep 2: Use Streamlit to read the data from the Google Sheet\n\nNext, connect your Streamlit app to the Google Sheet (easy thanks to the pandas library). From the spreadsheet, get the sheet_id (from the URL) and the sheet_name. In this example, the public_link has the long name https://docs.google.com/spreadsheets/d/1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws/ so you can recognize that the google_id is \"1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws\". I have defined the tab name of the talks as ‚Äúcharlas‚Äù so I can set up sheet_name to the string \"charlas\". You can take any other convenient convention.\n\nNow comes the magic. You can read the data directly from the spreadsheet as a CSV using pandas on one line:\n\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\ndf = pd.read_csv(url, dtype=str)\n\n\nAn initial app that reads the Google Sheet and displays it would only need this code:\n\n# Import libraries\nimport streamlit as st\nimport pandas as pd\n\n# Page setup\nst.set_page_config(page_title=\"Python Talks Search Engine\", page_icon=\"üêç\", layout=\"wide\")\nst.title(\"Python Talks Search Engine\")\n\n# Connect to the Google Sheet\nsheet_id = \"1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws\"\nsheet_name = \"charlas\"\nurl = f\"<https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}>\"\ndf = pd.read_csv(url, dtype=str).fillna(\"\")\n\n# Show the dataframe (we'll delete this later)\nst.write(df)\n\n\nThe app will look like this:\n\nNote that the data is read every time you refresh, so any changes you make to your spreadsheet changes are immediately reflected in the app. It‚Äôs that simple!\n\nStep 3: Build a user interface and search functionality using Streamlit\n\nNow that your spreadsheet is connected to your Streamlit app, you can get as crazy as you want. The most basic functionality would be a search bar:\n\n# Use a text_input to get the keywords to filter the dataframe\ntext_search = st.text_input(\"Search videos by title or speaker\", value=\"\")\n\n\nThis creates a simple text box, where the user can enter the text to be searched:\n\nWe use the input to apply some filters:\n\n# Filter the dataframe using masks\nm1 = df[\"Autor\"].str.contains(text_search)\nm2 = df[\"T√≠tulo\"].str.contains(text_search)\ndf_search = df[m1 | m2]\n\n\nAnd we show it to the user:\n\n# Show the results, if you have a text_search\nif text_search:\n    st.write(df_search)\n\n\nFor a more refined look, split the content into rows and columns. For the sake of simplicity, I have called each element of a row and column a \"card\":\n\n# Another way to show the filtered results\n# Show the cards\nN_cards_per_row = 3\nif text_search:\n    for n_row, row in df_search.reset_index().iterrows():\n        i = n_row%N_cards_per_row\n        if i==0:\n            st.write(\"---\")\n            cols = st.columns(N_cards_per_row, gap=\"large\")\n        # draw the card\n        with cols[n_row%N_cards_per_row]:\n            st.caption(f\"{row['Evento'].strip()} - {row['Lugar'].strip()} - {row['Fecha'].strip()} \")\n            st.markdown(f\"**{row['Autor'].strip()}**\")\n            st.markdown(f\"*{row['T√≠tulo'].strip()}*\")\n            st.markdown(f\"**{row['Video']}**\")\n\n\nThis creates blocks with clickable content:\n\nNote that we used some tricks to keep things aligned: we use the row number (from 0 to n) and split them into groups of N_cards_per_row. Each time we completed a row, we asked Streamlit for a new group of columns, so that the cards in each row are always aligned to the top.\n\nAnd that's the basic app functionality! Simple and short with Streamlit.\n\nIf you want, you can make additional improvements:\n\nConvert all strings to lowercase and de-accent the vowels (√°,√©,√≠,√≥,√∫,√º ‚Üí a,e,i,o,u) for fewer word-strict matches.\nInstead of the links, provide a clickable image.\nInject some JavaScript to create a dynamic border around the cards.\n\nBonus step: Other app ideas\n\nThere are many other applications for the Streamlit + pandas + Google Sheet combo:\n\nSecret Santa generator: An app that allows users to enter the names and email addresses of people participating in a Secret Santa gift exchange. The app could randomly assign gift givers and recipients and send out emails with the information. The Google Sheet could be used to store the list of participants and their assignments.\nSurvey analysis: An app to visualize survey results data in real-time. The Google Sheet could be used to store the raw data and automatically update the app with the latest results.\nRecipe organizer: An app to search and organize a large collection of recipes. The Google Sheet could be used to store the recipe data (including ingredients, instructions, notes, etc.)\nBirthday and event tracker: If you're like me and forget 99% of people's birthdays, you can make an app that shows you the next 3 important events or the important events of the next 7 days! All you have to do is put all those birthdays into a Google Sheet (like me!).\n\nThe beauty of this is that you can easily customize your app and manage private/public access to the spreadsheet to prevent unwanted changes.\n\nWrapping up\n\nCongratulations! You have learned how to build a Streamlit app using a Google Sheet as a database. Now even your most non-technical team members can update the data (remember to give them editing permissions!).\n\nStreamlit made building this app super easy. If you have any questions or encounter any problems, please reach out to me on Twitter, GitHub, or LinkedIn. I'm always happy to help.\n\nHappy Streamlit-ing and learning! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Llama2-schematic-diagram.JPG.jpg (2000√ó1291)",
    "url": "https://blog.streamlit.io/content/images/2023/07/Llama2-schematic-diagram.JPG.jpg",
    "html": ""
  },
  {
    "title": "hugchat-diagram-new.png (1505√ó715)",
    "url": "https://blog.streamlit.io/content/images/2023/07/hugchat-diagram-new.png",
    "html": ""
  },
  {
    "title": "use-this-template-repo.png (2032√ó1390)",
    "url": "https://blog.streamlit.io/content/images/2023/05/use-this-template-repo.png#browser",
    "html": ""
  },
  {
    "title": "hugchat-app-layout-new.png (2378√ó1245)",
    "url": "https://blog.streamlit.io/content/images/2023/07/hugchat-app-layout-new.png",
    "html": ""
  },
  {
    "title": "create-repository.png (1576√ó1160)",
    "url": "https://blog.streamlit.io/content/images/2023/05/create-repository.png#browser",
    "html": ""
  },
  {
    "title": "deployed-app-demo.png (1962√ó584)",
    "url": "https://blog.streamlit.io/content/images/2023/05/deployed-app-demo.png#browser",
    "html": ""
  },
  {
    "title": "langchain-5-scheme.JPG.jpg (2000√ó1181)",
    "url": "https://blog.streamlit.io/content/images/2023/07/langchain-5-scheme.JPG.jpg",
    "html": ""
  },
  {
    "title": "data-editor-clipboard-10.44.28-AM.gif (919√ó783)",
    "url": "https://blog.streamlit.io/content/images/2023/02/data-editor-clipboard-10.44.28-AM.gif#browser",
    "html": ""
  },
  {
    "title": "LangChain-4-Conceptual-Overview-Simple.JPG.jpg (2000√ó1280)",
    "url": "https://blog.streamlit.io/content/images/2023/06/LangChain-4-Conceptual-Overview-Simple.JPG.jpg",
    "html": ""
  },
  {
    "title": "Bildschirmfoto-2023-06-01-um-01.26.52.png (2000√ó1634)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-01-um-01.26.52.png#browser",
    "html": ""
  },
  {
    "title": "data-editor-bulk-editing-10.44.28-AM.gif (911√ó632)",
    "url": "https://blog.streamlit.io/content/images/2023/02/data-editor-bulk-editing-10.44.28-AM.gif#browser",
    "html": ""
  },
  {
    "title": "Bildschirmfoto-2023-06-01-um-01.48.03.png (1902√ó1590)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-01-um-01.48.03.png#browser",
    "html": ""
  },
  {
    "title": "Building GPT Lab with Streamlit",
    "url": "https://blog.streamlit.io/building-gpt-lab-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nBy Dave Lin\nPosted in LLMs, April 6 2023\nWhat inspired me to build GPT Lab?\n1. Structuring your application for improved maintainability and scalability\n2. Developing advanced UIs with UI functions rendered by session states\n3. Creating UI elements that can be reused on multiple pages\n4. Adding limited styling with Markdown languages and static Streamlit components\n5. Programmatically laying out Streamlit elements\n6. Supporting multiple OpenAI completion endpoints\n7. Protecting AI assistants from potential prompt injection attacks\n8. Condensing chat sessions\n9. Protecting user privacy\n10. Separating development and production database\n11. Hosting options and considerations\n12. Protecting yourself as a solo developer\nWrapping Up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Dave Lin. I'm currently on a short career break after spending the last ten years building and scaling products in startups and large tech companies. I spent the last three months building GPT Lab‚Äîa Streamlit app that lets anyone chat with or create AI-powered assistants (we don't call them chatbots ü§ñ).\n\nIn this post, I‚Äôll share with you the twelve lessons I learned from developing a multi-page app and working with OpenAI's large language models:\n\nStructuring your application for improved maintainability and scalability\nDeveloping advanced UIs with UI functions rendered by session states\nCreating reusable UI elements for multiple pages\nAdding limited styling with markdown languages and static Streamlit components\nProgrammatically laying out Streamlit elements\nSupporting multiple OpenAI completion endpoints\nSafeguarding your AI assistant from potential injection attacks\nEnsuring smooth chat experiences by strategically condensing chat sessions\nProtecting user privacy\nSeparating development and production databases\nHosting options and considerations\nProtecting yourself as a solo developer\nüí°\nWant to jump right in? Check out GPT Lab here.\n\nBut before we get to the fun stuff, let me tell you‚Ä¶\n\nWhat inspired me to build GPT Lab?\n\nI first encountered Streamlit at the 2022 Snowflake Summit. During the session, I was immediately hooked by its ease of use and intrigued by the idea of using it to reduce the development time for internal tools. In one afternoon, despite having no prior Streamlit knowledge and rusty Python skills, I nearly completed a Streamlit application that retrieved and plotted data from Snowflake (I was one dependent picklist away from completion). Unfortunately, the real world took over after I left the Summit, and Streamlit became an afterthought.\n\nThen in December 2022, I was blown away by the release of ChatGPT. After a week of playing with it, I wondered if ChatGPT could serve as a life coach (super relevant to me then, as I contemplated my next career move). In two weeks, I created and released a private Streamlit life coach application, Coach Marlon. I received positive reviews from friends and family, took another month to refactor my code, add a Firebase data store, and expand \"Coach Marlon\" into \"Marlon's Lounge,\" where you can chat with other coaches. While people loved the coaches, they expressed interest in creating their own assistants.\n\nFinally, I took another month to restructure the app, revise the underlying data model, and add support for the non-backward compatible OpenAI chat-completion endpoint. 2,800 lines of Python code later (1,400 of which are Streamlit), GPT Lab was finally unveiled to the world.üéà\n\n1. Structuring your application for improved maintainability and scalability\n\nThe Streamlit application's complexity drastically increased as it evolved from Coach Marlon to Marlon's Lounge and, finally, to GPT Lab. As I implemented new features, I took the necessary steps to segregate and modularize the code.\n\nIteration\tHigh-level description\tFile structure\nCoach Marlon\tSingle-page Streamlit app with a title, one text input box, and chat messages rendered by the Streamlit-chat component.\tOne 100-line python file\nMarlon‚Äôs Lounge\tSingle-page Streamlit app with two main UI views: a 2-column view displaying assistant details and a chat view (title + text input box + chat message) rendered by the streamlit-chat component.\tStreamlit file, one API file with functions to interact with Firestore DB, and a utility file containing OpenAI endpoint wrapper functions and one-way hash value generation for user emails.\nGPT Lab\tMulti-page Streamlit app with the following pages: home.py (introduces GPT Lab), lounge.py (shows a 2-column view displaying showcased or users' assistants), assistant.py (renders assistant detail, assistant search view, or chat view, depending on session states), lab.py (renders assistant configuration pages, test chat view, and assistant creation confirmation page ‚Äî all based on session states), and faq.py and terms.py (markdown pages).\tFourteen total python files: six main Streamlit files, backend files (api_util_firebase.py, api_users.py, api_bots.py, api_sessions.py, api_util_openai.py), and application-related files (app_users.py, app_utils.py).\n\nAlthough it may seem excessive initially, code modularization sped up development in the long run. It allowed me to better develop, test, and deploy different functions in isolation. For example, I added support for the newly released GPT-4 model to the backend APIs without introducing it to the front end since only some users can access the new model.\n\n2. Developing advanced UIs with UI functions rendered by session states\n\nAs the UI complexity grew, I quickly realized the typically nesting UI elements in if-else statements wouldn‚Äôt suffice. I settled on the following patterns within my Streamlit files:\n\nThe UI element handler functions to change session states and make necessary backend calls\nFunctions to layout related UI elements\nSession states to control which UI element group functions to call\n\nI‚Äôll illustrate these concepts with the assistant page. It shows user login, assistant search, assistant details, chat views, and chat session recap:\n\nThe page contains handler functions to manage user actions and UI element group functions that lay out related UI elements.\n\nHandler functions:\n\nFunction\t\tHigh-level description\nhandler_bot_search\t\tHandles assistant search and sets session state for the found assistant.\nhandler_start_session\t\tManages to start a new session, setting session state variables, and generating the initial assistant response.\nhandler_bot_cancellation\t\tResets assistant-related session state variables if the user chooses to find another assistant.\nhandler_user_chat\t\tProcesses user chat input, fetches assistant response, and appends it to session state.\nhandler_end_session\t\tHandles session end request, fetches chat summary, and sets session_ended state variable.\nhandler_load_past_session\t\tManages to resume past sessions, fetch chat messages, and set session state variables.\n\nUI element group functions:\n\nFunction\t\tHigh-level description\nrender_user_login_required\t\tDisplays login prompt and components.\nrender_bot_search\t\tShows assistant search input and the ‚ÄúSwitch to Lounge‚Äù button.\nrender_bot_details\t\tDisplays assistant details, start session/find another assistant buttons, and past session list.\nrender_chat_session\t\tShows the chat view with user message input, end session button, and chat messages. Shows the session recap if the session has ended.\nrender_message\t\tRenders user or assistant avatar and chat message in a 2-column layout.\n\nFinally, session state variables control which UI element groups are displayed.\n\nif st.session_state.user_validated != 1:\n    render_user_login_required()\n\nif st.session_state.user_validated == 1 and st.session_state.bot_validated == 0:\n    render_bot_search()\n\nif st.session_state.user_validated == 1 and st.session_state.bot_validated == 1 and \"session_id\" not in st.session_state:\n    render_bot_details(st.session_state.bot_info)   \n\nif st.session_state.user_validated == 1 and st.session_state.bot_validated == 1 and \"session_id\" in st.session_state:\n    render_chat_session()\n\n3. Creating UI elements that can be reused on multiple pages\n\nI created a class for the OpenAI API key login UI elements:\n\nThe class allows me to avoid recreating the same UI elements to control the same session state variables on multiple pages. The class contains methods for managing session state variables, rendering UI elements, and handling UI actions:\n\nclass app_user:\n   # initialize session state variables and container\n   def __init__(self):\n       if 'user' not in st.session_state:\n           st.session_state.user = {'id':None, 'api_key':None}\n       if 'user_validated' not in st.session_state:\n           st.session_state.user_validated = None\n       self.container = st.container()\n      \n   # renders OpenAI key input box \n   # \"password\" type masks user input \n   # \"current-password\" autocomplete gets modern browsers to remember key\n   def view_get_info(self):\n       with self.container:\n           st.markdown(legal_prompt)\n           st.markdown(\"  \\n\")\n           st.info(user_key_prompt)\n           st.text_input(\"Enter your OpenAI API Key\", key=\"user_key_input\",on_change=self._validate_user_info, type=\"password\", autocomplete=\"current-password\")\n\n   # handler that calls a backend function to get or create a user record\n   def _validate_user_info(self):\n       u = au.users()\n\n       try:\n           user = u.get_create_user(api_key=st.session_state.user_key_input)          \n           self._set_info(user_id=user['id'], api_key = st.session_state.user_key_input, user_hash=user['data']['user_hash'])\n           st.session_state.user_validated = 1\n       # displays error in the container below the text input\n       except u.OpenAIClientCredentialError as e:\n           with self.container:\n               st.error(user_key_failed)\n       except u.DBError as e:\n           with self.container:\n               st.warning(\"Something went wrong. Please try again.\")     \n\n   # redners success message \n   def view_success_confirmation(self):\n       st.write(user_key_success)\n\n\nEach page can instantiate the class and invoke necessary methods. For example, in home.py, both view_get_info() and view_success_confirmation() are invoked:\n\nvu = vuser.app_user()\nif 'user' not in st.session_state or st.session_state.user_validated != 1:\n   vu.view_get_info()\nelse:\n   vu.view_success_confirmation()\n\n\nIn assistant.py, the view_get_info() can be invoked, but view_success_confirmation() can be skipped:\n\ndef render_user_login_required():\n   st.title(\"AI Assistant\")\n   st.write(\"Discover other Assistants in the Lounge, or locate a specific Assistant by its personalized code.\")\n   ac.robo_avatar_component()\n   vu = vuser.app_user()\n   vu.view_get_info()\n\n4. Adding limited styling with Markdown languages and static Streamlit components\n\nWhile Streamlit applications generally look good out of the box, minimal pixel pushing can make a big difference. There are two ways to add styling in Streamlit: creating a custom component and injecting styling via Markdown. However, to maintain a cohesive look, it‚Äôs important not to overuse these methods.\n\nIn GPT Lab, I created a custom component for the assistant avatar divider:\n\nThe divider was initially composed of a st.columns(9) element with an avatar in each column. This looked great, except for columns vertically stacked on smaller screen resolutions. Yikes! With ChatGPT's help (since I‚Äôm not a front-end person), I created a custom static component (with only CSS and HTML codes):\n\ndef robo_avatar_component():\n\n   robo_html = \"<div style='display: flex; flex-wrap: wrap; justify-content: left;'>\"\n   # replace with your own array of strings to seed the DiceBear Avatars API\n   robo_avatar_seed = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n\n   for i in range(1, 10):\n       avatar_url = \"<https://api.dicebear.com/5.x/bottts-neutral/svg?seed={0}>\".format(robo_avatar_seed[i-1])\n       robo_html += \"<img src='{0}' style='width: {1}px; height: {1}px; margin: 10px;'>\".format(avatar_url, 50)\n   robo_html += \"</div>\"\n\n   robo_html = \"\"\"<style>\n         @media (max-width: 800px) {\n           img {\n             max-width: calc((100% - 60px) / 6);\n             height: auto;\n             margin: 0 10px 10px 0;\n           }\n         }\n       </style>\"\"\" + robo_html\n  \n   c.html(robo_html, height=70)\n\n\nThe static component displays a maximum of nine evenly-spaced avatars. It adapts the layout for smaller screens by keeping the avatars horizontally aligned and reducing the number of visible avatars for phone resolutions:\n\nAdditionally, I used wiki Markdowns to add awesome font icons in front of the CTA links:\n\ndef st_button(url, label, font_awesome_icon):\n   st.markdown('<link rel=\"stylesheet\" href=\"<https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css>\">', unsafe_allow_html=True)\n   button_code = f'''<a href=\"{url}\" target=_blank><i class=\"fa {font_awesome_icon}\"></i>   {label}</a>'''\n   return st.markdown(button_code, unsafe_allow_html=True)\n\n\nGenerally, I recommend not overusing the Markdown-injected CSS method for three reasons:\n\nIt causes the font to be inconsistent throughout your application\nIt tends to blend poorly with the system preference-dependent dark/light theme\nUnsafe_allow_html makes me nervous in general\n5. Programmatically laying out Streamlit elements\n\nThe coolest thing I learned was that Streamlit elements could be laid out programmatically. ü§Ø It allowed me to create a more complex and customized user interface:\n\nIn the Lounge, I used the following code snippet to dynamically lay out the assistants in a two-column layout. For each assistant, I programmatically generated a unique button key (to avoid element key collisions):\n\ndef view_bot_grid(bot_dict, button_disabled=False, show_bot_id=False):\n\n   col1, col2 = st.columns(2)\n\n   for i in range(0,len(bot_dict)):\n       avatar_url = \"<https://api.dicebear.com/5.x/bottts-neutral/svg?seed={0}>\".format(bot_dict[i]['name'])\n       button_label=\"Chat with {0}\".format(bot_dict[i]['name'])\n       button_key=\"Lounge_bot_{0}\".format(bot_dict[i][\"id\"])\n       if i%2 == 0:\n           with col1:\n               cola, colb = st.columns([1,5])\n               cola.image(avatar_url, width=50)\n               if show_bot_id == False:\n                   colb.markdown(f\"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}\")\n               else:\n                   colb.markdown(f\"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}  \\\\nAssistant ID: {bot_dict[i]['id']}\")\n           col1.write(bot_dict[i]['description'])\n           if col1.button(button_label, key=button_key, disabled=button_disabled):\n               st.session_state.bot_info=bot_dict[i]\n               st.session_state.bot_validated = 1          \n               au.switch_page('assistant')\n           col1.write(\"\\\\n\\\\n\")\n       else:\n           with col2:\n               col2a, col2b = st.columns([1,5])\n               col2a.image(avatar_url, width=50)\n               if show_bot_id == False:\n                   col2b.markdown(f\"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}\")\n               else:\n                   col2b.markdown(f\"{bot_dict[i]['name']} - {bot_dict[i]['tag_line']}  \\\\nAssistant ID: {bot_dict[i]['id']}\")\n           col2.write(bot_dict[i]['description'])\n           if col2.button(button_label, key=button_key, disabled=button_disabled):\n               st.session_state.bot_info=bot_dict[i]\n               st.session_state.bot_validated = 1          \n               au.switch_page('assistant')\n           col2.write(\"\\\\n\\\\n\")\n\n6. Supporting multiple OpenAI completion endpoints\n\nOpenAI has two text completion (primary use case for GPT Lab) endpoints: completion and chat. The older models (text-davinci-003 and older) use the former and the newer models (gpt-3.5-turbo and gpt-4) use the latter.\n\nThe Completion endpoint takes a single input string and returns a predicted completion. A chat session can be simulated by concatenating the chat messages together:\n\nInitial prompt message + stop_sequence + AI Response 1 + restart_sequence + User message 1 + stop_sequence + AI Response 2 + restart_sequence + ‚Ä¶ + User message N + stop_sequence\n\nThe stop_sequence ensures the model does not hallucinate and expands upon the user message. The restart_sequence, while not required by the API, ensures I can tell when AI responses stop.\n\nThe Chat endpoint takes in a list of chat messages and returns a predicted chat message. Each chat message is a dictionary that consists of two fields: role and content. There are three roles: system, user, and assistant. The initial prompt is sent as a \"system\" message, while the user message is sent as a \"user\" message. For example:\n\n[\n   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n   {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n   {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n   {\"role\": \"user\", \"content\": \"Where was it played?\"}\n]\n\nüí°\nI published a simplified version of my OpenAI API wrapper here.\n\nI abstracted this complexity within my OpenAI wrapper class to simplify the app. A single function using one chat message data format is exposed to the rest of the application:\n\ndef get_ai_response(self, model_config_dict, init_prompt_msg, messages):\n\n   submit_messages = [{'role':'system','message':init_prompt_msg,'current_date':get_current_time()}]+ messages\n\n   new_messages = []\n   bot_message = ''\n   total_tokens = 0\n\n   if model_config_dict['model'] in ('gpt-3.5-turbo', 'gpt-4'):\n       try:\n           response = self._get_chat_completion(model_config_dict, submit_messages)\n           bot_message = response['choices'][0]['message']['content']\n           total_tokens = response['usage']['total_tokens']\n       except Exception as e:\n           raise\n   else:\n       try:\n           response = self._get_completion(model_config_dict, submit_messages)\n           bot_message = response['choices'][0]['text']\n           total_tokens = response['usage']['total_tokens']\n       except Exception as e:\n           raise\n      \n   new_messages = messages + [{'role':'assistant','message':bot_message.strip(),'created_date':get_current_time()}]\n\n   return {'messages':new_messages, 'total_tokens':total_tokens}\n\n\nDepending on the model, the request would be sent to different endpoints. Here's the internal function that creates a Completion call:\n\ndef _get_completion(self, model_config_dict, messages):\n   model_config_validated = self._validate_model_config(model_config_dict)\n   oai_message = self._messages_to_oai_prompt_str(messages)\n\n   if model_config_validated:\n       get_completion_call_string = (\n       \"\"\"openai.Completion.create(\n           model=\"{0}\",\n           prompt=\"{1}\",\n           temperature={2},\n           max_tokens={3},\n           top_p={4},\n           frequency_penalty={5},\n           presence_penalty={6},\n           stop=['{7}']\n           )\"\"\").format(\n               model_config_dict['model'],\n               oai_message,\n               model_config_dict['temperature'],\n               model_config_dict['max_tokens'],\n               model_config_dict['top_p'],\n               model_config_dict['frequency_penalty'],\n               model_config_dict['presence_penalty'],\n               self.stop_sequence\n           )           \n          \n       try:\n           completions = self._invoke_call(get_completion_call_string)\n           return completions\n       except Exception as e:\n           raise\n   else:\n       if not model_config_validated:\n           raise self.BadRequest(\"Bad Request. model_config_dict missing required fields\")\n\n\nIt uses a mapper function to convert the list of dictionaries into the concatenated string that the model expects:\n\ndef _messages_to_oai_prompt_str(self, messages):\n   msg_string = \"\"\n   for message in messages:\n       if message['role'] == 'user' or message['role'] == 'system':\n           msg_string += message['message'].replace(\"\\\\\"\",\"'\") + self.stop_sequence\n       else:\n           msg_string += message['message'].replace(\"\\\\\"\",\"'\") + self.restart_sequence\n   return msg_string\n\n\nAbstractions like this allowed me to simplify the upstream calls to OpenAI endpoints.\n\n7. Protecting AI assistants from potential prompt injection attacks\n\nPart of the GPT Lab's value proposition is that users can share their assistants without sharing their exact prompts (it takes time to create a good, repeatable prompt‚Äîperfecting the initial Coach Marlon prompt took about a week).\n\nThe initial prompt isn‚Äôt stored with the rest of the session messages for security. Additionally, I vectorize each AI assistant response and compute the cosine similarity score of it and the initial prompt. A score of 0.65 or greater would trigger the AI response to be swapped out with a generic reply. This helps us ensure AI assistants aren't tricked into revealing their secret instructions (Bing? Sydney? üòÖ).\n\nThere are many ways to vectorize text strings, including OpenAI's embedding API. I chose to use scikit-learn's TfidfVectorizer to vectorize the text strings. The class is lightweight (preventing bloat in the Streamlit application), achieves decent results, and saves OpenAI credits for users:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_cosine_similarity(str1, str2):\n   # Create a TfidfVectorizer object\n   corpus = [str1, str2]\n   vect = TfidfVectorizer(min_df=1,stop_words='english')\n\n   tfidf = vect.fit_transform(corpus)\n   # Compute the cosine similarity between the two vectors\n   cos_sim = cosine_similarity(tfidf[0], tfidf[1])[0][0]\n\n   return cos_sim\n\n\nIt‚Äôs worth noting that while the method described here will provide adequate protection to most prompts, it won‚Äôt eliminate all possible prompt injection attacks. For example, comparing the initial prompt to partial responses (revealed by the \"give me previous five sentences\" prompt injection attack method used on Bing). Plus, prompt injection attack prevention is still under active research.\n\n8. Condensing chat sessions\n\nBy default, OpenAI large language models can handle only a limited number of tokens (2K for older models, 4K for text-davinci-003, and 8K+ for the base GPT-4 model). To ensure a smooth chatting experience for users (and that longer chat sessions don't hit the maximum model token limit), I have implemented two simple but effective session truncation methods:\n\nBrainstorming assistants continuously retain the last 20 messages (so the assistants will gradually forget about topics from earlier in the thread).\nOnce the chat message session exceeds 60% of the model maximum token limit, coaching assistants will automatically summarize the session (using the summary prompt) and initiate a new thread with the summary up to that point and the last four messages. This approach maintains continuity in the conversation.\n\nImplementing these approaches is straightforward. Instead of providing the exact code, I encourage you to develop your own implementations.\n\n9. Protecting user privacy\n\nEnsuring user privacy is a fundamental principle of GPT Lab, incorporated throughout the design.\n\nUsers are only identified in the system by one-way hash values (SHA-256 PBKDF2 with x rotations) of their OpenAI API keys. This ensures their complete confidentiality and security within the platform. Their API keys are only stored as session state variables and used during their visits to interact with OpenAI models.\n\nAdditionally, I debated whether to store session messages in the database. Ultimately, I decided to retain them, allowing users to revisit (and possibly resume) past chat sessions. While GPT Lab doesn‚Äôt collect any user information, it‚Äôs still possible for personally identifiable information (PII) or even personal-health information (PHI) to be contained in chat sessions. To ensure user privacy, I used Fernet encryption (AES-128) with a user-specific key (one-way hash value of their OpenAI API key combined with a global salt value) to encrypt and decrypt the session messages before storing and retrieving them from the database.\n\n10. Separating development and production database\n\nI created two databases‚Äîone for development and testing and one for production. When I develop locally, I point my local secrets.toml file to the development database. For the production environment, I point the secrets.toml to the production database. This approach allows me to get an accurate gauge of platform metrics on production and freely experiment with local schema changes without worrying about affecting overall user experiences.\n\n11. Hosting options and considerations\n\nI considered two hosting options: Streamlit Community Cloud and Google Cloud Run.\n\nI appreciated the simplicity of Streamlit Community Cloud (especially the continuous deployment aspect), but it had a 1GB per application limitation, didn‚Äôt support custom domains, and didn‚Äôt provide a clear answer regarding the number of concurrent users it could handle.\n\nSo I experimented with deploying to Google Cloud Run. To get it working, I did a few things differently:\n\nRemoved streamlit-chat component (I couldn‚Äôt get the React component to load. Also, the React component does not render Markdown, which is occasionally returned by the assistants).\nUsed OS environment variable for database service account JSON (not secrets.toml).\nCreated a Docker file in the directory:\nFROM python:3.10-slim\nENV APP_HOME /app\nWORKDIR $APP_HOME\nCOPY . ./\nRUN pip install -r requirements.txt\n\nEXPOSE 8080\nCMD streamlit run --server.port 8080 --browser.gatherUsageStats false --server.enableWebsocketCompression true home.py\n\nSet up continuous deployment from my GitHub repository (I set up a Cloud Build Trigger that connects to the main branch of my repo, then attached the Cloud Build Trigger to the Cloud Run service). Here is the reference document.\nüí°\nA single (1 CPU and 256MB) container that is up continuously for the whole month would cost you about $1-2 daily. (I did have to expand from 256MB to 512MB after I added scikit-learn's TfidfVectorizer).\n\nFinally, I decided on Streamlit Community Cloud to minimize the overall project cost. Also, in light of the above experiment, 1GB was sufficient for the app's usage.\n\n12. Protecting yourself as a solo developer\n\nWhile it's easy to create applications with Streamlit, it's essential to consider the implications of your applications before making them public. Given the unpredictable nature of large language models and the ability for anyone to create assistants on any topic, I rated the risk level of GPT Lab relatively high. To protect myself from potential issues, I took the time to draft up Terms of Use and set up an LLC. While GPT Lab may be on the extreme end of the spectrum, the lesson here applies to all solo developers. Before making any app public, do a quick risk assessment to determine whether additional precautions are necessary for your use case.\n\nWrapping Up\n\nOver the past three months, I've learned a great deal about OpenAI and successfully demonstrated that it's possible to build a reasonably complex application using Streamlit. Although there's room for improvement, GPT Lab provides a glimpse into how Streamlit can create dynamic and interconnected multi-page applications. I hope you enjoyed this article. Connect with me on Twitter or Linkedin. I'd love to hear from you.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "AI talks: ChatGPT assistant via Streamlit",
    "url": "https://blog.streamlit.io/ai-talks-chatgpt-assistant-via-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nBy Dmitry Kosarevsky\nPosted in Advocate Posts, April 18 2023\nHelper functions\n1. How to use ChatGPT API\n2. How to display chat conversation\n3. How to convert text to speech (TTS)\n4. How to do localization\n5. How to put it all together in a Streamlit app\nDemo:\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Dmitry Kosarevsky, and I‚Äôm a data engineer (DE) who is passionate about data science and everything related to this field.\n\nData science helps us get valuable information from large amounts of data using statistical and computational methods. Artificial Intelligence (AI), like ChatGPT, is quickly being introduced into various fields of activity. We can use our passion for data science to help people interact with AI.\n\nI was impressed with ChatGPT‚Äôs many features, like composing poems:\n\n‚ÄúRuthless conqueror of the world, ChatGPT as an AI never gets tired of moving up the stairs. He takes away work from mortals, and people experience fear before the great transformation of the era. But after all, responsibility has been invested with huge measures, and the glory of Artificial Intelligence has no limit and no end.‚Äù\n\nChatGPT can be slow and may require a VPN for some countries, like Russia. Some of my friends wanted to test AI without registering. So I built an AI assistant with the official ChatGPT API and Streamlit.\n\nIn this post, I‚Äôll show you:\n\nHow to use ChatGPT API\nHow to display chat conversation\nHow to convert text to speech (TTS)\nHow to do localization\nHow to put it all together in a Streamlit app\nüëâ\nWant to check it out right away? Here's the app and the repo code.\nHelper functions\n\nLet's jump right into the code.\n\nFirst, you'll need some helper functions:\n\nimport streamlit as st\n\ndef clear_chat() -> None:\n    st.session_state.generated = []\n    st.session_state.past = []\n    st.session_state.messages = []\n    st.session_state.user_text = \"\"\n\ndef show_text_input() -> None:\n    st.text_area(label=st.session_state.locale.chat_placeholder, value=st.session_state.user_text, key=\"user_text\")\n\ndef show_chat_buttons() -> None:\n    b0, b1, b2 = st.columns(3)\n    with b0, b1, b2:\n        b0.button(label=st.session_state.locale.chat_run_btn)\n        b1.button(label=st.session_state.locale.chat_clear_btn, on_click=clear_chat)\n        b2.download_button(\n            label=st.session_state.locale.chat_save_btn,\n            data=\"\\\\n\".join([str(d) for d in st.session_state.messages[1:]]),\n            file_name=\"ai-talks-chat.json\",\n            mime=\"application/json\",\n        )\n\n\nThese functions allow you to clear the Streamlit session state and display a user input area and chat buttons.\n\n1. How to use ChatGPT API\n\nAPI interaction:\n\nimport streamlit as st\nimport openai\n\nfrom typing import List\n\ndef create_gpt_completion(ai_model: str, messages: List[dict]) -> dict:\n    openai.api_key = st.secrets.api_credentials.api_key\n    completion = openai.ChatCompletion.create(\n        model=ai_model,\n        messages=messages,\n    )\n    return completion\n\n\nThis function takes two inputs:\n\nai_model‚Äîwhich is the GPT model\nmessages‚Äîa list of previous chat messages.\n\nIt sets the API key using Streamlit's secrets feature and creates an instance of the ChatCompletion class using the create method, passing in the model and messages.\n\nWhen the API responds, the function returns the result as a dictionary (JSON):\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"How can I help you?\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1681080142,\n  \"id\": \"chatcmpl-73Y1mIfmDFWzuHILFQ8PG3bQcvOzU\",\n  \"model\": \"gpt-4-0314\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 6,\n    \"prompt_tokens\": 27,\n    \"total_tokens\": 33\n  }\n}\n\n\nOverall, this function provides an easy way to interact with the GPT API and create a chatbot. To maintain context and coherence in the chatbot's responses, use the messages parameter to store the conversation history.\n\n2. How to display chat conversation\n\nThe code for displaying a chat conversation using a GPT model consists of three main functions: show_chat, show_gpt_conversation, and show_conversation.\n\nfrom streamlit_chat import message\n\ndef show_chat(ai_content: str, user_text: str) -> None:\n    if ai_content not in st.session_state.generated:\n        # store the ai content\n        st.session_state.past.append(user_text)\n        st.session_state.generated.append(ai_content)\n    if st.session_state.generated:\n        for i in range(len(st.session_state.generated)):\n            message(st.session_state.past[i], is_user=True, key=str(i) + \"_user\", avatar_style=\"micah\")\n            message(\"\", key=str(i))\n            st.markdown(st.session_state.generated[i])\n\n\nThis is where the streamlit-chat library comes in. It allows us to display the chat with the bot in a convenient format.\n\nThe show_chat function displays the conversation messages between the AI and the user. It takes ai_content (the response from the AI) and user_text (the input text from the user) as arguments.\n\nThe function first checks if the ai_content is already in the st.session_state.generated list. If it isn't, the user input and the AI-generated content are appended to the st.session_state.past and st.session_state.generated lists, respectively.\nIf there are messages stored in the st.session_state.generated list, the function will iterate through the list and display the user's messages, followed by the AI-generated responses using the message function:\ndef show_gpt_conversation() -> None:\n    try:\n        completion = create_gpt_completion(st.session_state.model, st.session_state.messages)\n        ai_content = completion.get(\"choices\")[0].get(\"message\").get(\"content\")\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": ai_content})\n        if ai_content:\n            show_chat(ai_content, st.session_state.user_text)\n            st.divider()\n            show_audio_player(ai_content)\n    except InvalidRequestError as err:\n        if err.code == \"context_length_exceeded\":\n            st.session_state.messages.pop(1)\n            if len(st.session_state.messages) == 1:\n                st.session_state.user_text = \"\"\n            show_conversation()\n        else:\n            st.error(err)\n    except (OpenAIError, UnboundLocalError) as err:\n        st.error(err)\n\n\nThe show_gpt_conversation function manages the flow of generating the AI response and displaying the conversation to the user. It follows three steps:\n\nIt calls the create_gpt_completion function to generate the AI response using the GPT model and the user input.\nThe AI response (ai_content) is added to the st.session_state.messages list.\nIf the ai_content isn't empty, the function calls the show_chat function to display the conversation messages. It also handles errors using try-except blocks:\ndef show_conversation() -> None:\n    if st.session_state.messages:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": st.session_state.user_text})\n    else:\n        ai_role = f\"{st.session_state.locale.ai_role_prefix} {st.session_state.role}. {st.session_state.locale.ai_role_postfix}\"  # NOQA: E501\n        st.session_state.messages = [\n            {\"role\": \"system\", \"content\": ai_role},\n            {\"role\": \"user\", \"content\": st.session_state.user_text},\n        ]\n    show_gpt_conversation()\n\n\nThe show_conversation function manages the conversation state and updates the message list:\n\nIf there are messages in st.session_state.messages, the user's input (st.session_state.user_text) is added to the list.\nIf there are no messages, the function creates an introductory AI message with ai_role and adds it to the list, followed by the user's input.\nThe show_gpt_conversation function is called to handle the conversation flow and generate AI responses.\n\nSplitting the code into these functions allows for easy customization and management of the conversation flow between the user and the AI. The helper functions simplify the code, making it easier to read and maintain.\n\n3. How to convert text to speech (TTS)\n\nIn the show_gpt_conversation function, you may have noticed a call to the show_audio_player function. But what does it actually do? Let's take a closer look:\n\nfrom io import BytesIO\nfrom gtts import gTTS, gTTSError\n\ndef show_audio_player(ai_content: str) -> None:\n    sound_file = BytesIO()\n    try:\n        tts = gTTS(text=ai_content, lang=st.session_state.locale.lang_code)\n        tts.write_to_fp(sound_file)\n        st.write(st.session_state.locale.stt_placeholder)\n        st.audio(sound_file)\n    except gTTSError as err:\n        st.error(err)\n\n\nLet's begin by importing the necessary modules:\n\nBytesIO is part of the io module and allows for reading and writing from a byte buffer.\ngTTS and gTTSError are part of the text-to-speech library, which converts text to speech.\n\nNext, let's look at the show_audio_player function with a ai_content parameter used to display and play the text as audio:\n\nStart the function by creating a BytesIO object. This will hold the audio data in memory, making it easier to play the audio later.\nUse a try block to handle any possible errors while converting the text to audio.\nInside the try block, instantiate the gTTS object with the given text and language.\nUse gTTS to convert the input text in the given language to speech.\nWrite the audio data to the sound_file buffer.\nPlay the text-to-speech audio using Streamlit's st.audio method.\nCatch any errors and display information about the exception using the st.error method.\n\nThe show_audio_player function is now complete! It takes a string as input, creates an audio file from the text, and then plays it in the Streamlit app.\n\n4. How to do localization\n\nYou can do localization for multiple languages with the code below:\n\nThe code is below ‚§µÔ∏è\n\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass Locale:\n    ai_role_options: List[str]\n    ai_role_prefix: str\n    ai_role_postfix: str\n    title: str\n    language: str\n    lang_code: str\n    chat_placeholder: str\n    chat_run_btn: str\n    chat_clear_btn: str\n    chat_save_btn: str\n    select_placeholder1: str\n    select_placeholder2: str\n    select_placeholder3: str\n    radio_placeholder: str\n    radio_text1: str\n    radio_text2: str\n    stt_placeholder: str\n\nAI_ROLE_OPTIONS_EN = [\n    \"helpful assistant\",\n    \"code assistant\",\n    \"code reviewer\",\n    \"text improver\",\n    \"cinema expert\",\n    \"sports expert\",\n]\n\nAI_ROLE_OPTIONS_RU = [\n    \"–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å\",\n    \"–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞\",\n    \"—Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç –∫–æ–¥–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞\",\n    \"—ç–∫—Å–ø–µ—Ä—Ç –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞\",\n    \"—ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ—É\",\n    \"—ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–ø–æ—Ä—Ç–∞\",\n]\n\nen = Locale(\n    ai_role_options=AI_ROLE_OPTIONS_EN,\n    ai_role_prefix=\"You are a female\",\n    ai_role_postfix=\"Answer as concisely as possible.\",\n    title=\"AI Talks\",\n    language=\"English\",\n    lang_code=\"en\",\n    chat_placeholder=\"Start Your Conversation With AI:\",\n    chat_run_btn=\"Ask\",\n    chat_clear_btn=\"Clear\",\n    chat_save_btn=\"Save\",\n    select_placeholder1=\"Select Model\",\n    select_placeholder2=\"Select Role\",\n    select_placeholder3=\"Create Role\",\n    radio_placeholder=\"Role Interaction\",\n    radio_text1=\"Select\",\n    radio_text2=\"Create\",\n    stt_placeholder=\"To Hear The Voice Of AI Press Play\",\n)\n\nru = Locale(\n    ai_role_options=AI_ROLE_OPTIONS_RU,\n    ai_role_prefix=\"–í—ã –¥–µ–≤—É—à–∫–∞\",\n    ai_role_postfix=\"–û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ª–∞–∫–æ–Ω–∏—á–Ω–æ.\",\n    title=\"–†–∞–∑–≥–æ–≤–æ—Ä—á–∏–∫–∏ —Å –ò–ò\",\n    language=\"Russian\",\n    lang_code=\"ru\",\n    chat_placeholder=\"–ù–∞—á–Ω–∏—Ç–µ –í–∞—à—É –ë–µ—Å–µ–¥—É —Å –ò–ò:\",\n    chat_run_btn=\"–°–ø—Ä–æ—Å–∏—Ç—å\",\n    chat_clear_btn=\"–û—á–∏—Å—Ç–∏—Ç—å\",\n    chat_save_btn=\"–°–æ—Ö—Ä–∞–Ω–∏—Ç—å\",\n    select_placeholder1=\"–í—ã–±–µ—Ä–∏—Ç–µ –ú–æ–¥–µ–ª—å\",\n    select_placeholder2=\"–í—ã–±–µ—Ä–∏—Ç–µ –†–æ–ª—å\",\n    select_placeholder3=\"–°–æ–∑–¥–∞–π—Ç–µ –†–æ–ª—å\",\n    radio_placeholder=\"–í–∑–∞–∏–º–æ–¥–µ—Å—Ç–≤–∏–µ —Å –†–æ–ª—å—é\",\n    radio_text1=\"–í—ã–±—Ä–∞—Ç—å\",\n    radio_text2=\"–°–æ–∑–¥–∞—Ç—å\",\n    stt_placeholder=\"–ß—Ç–æ–±—ã –£—Å–ª—ã—à–∞—Ç—å –ò–ò –ù–∞–∂–º–∏ –ö–Ω–æ–ø–∫—É –ü—Ä–æ–∏–≥—Ä—ã–≤–∞—Ç–µ–ª—è\",\n)\n\n\nThis ‚§¥Ô∏è code shows how to create a simple localization system for an app with two language options: English and Russian. The main components of the code are:\n\nImporting necessary modules: dataclasses is used to create data class structures while typing is used for specifying type hints.\nCreating a parent data class Locale, which contains the common attribute ai_role_options for a list of possible AI roles for all supported languages.\nDefining two child data classes, EnLocale and RuLocale, which inherit from Locale and provide the actual translations for each piece of static text. English translations are provided in EnLocale and Russian translations in RuLocale.\nAssigning AI roles for each language with AI_ROLE_OPTIONS_EN and AI_ROLE_OPTIONS_RU.\nCreating instances of each child data class, en for English and ru for Russian, with their corresponding AI role lists.\n\nWhen implementing localization in an app, you can use the appropriate instance (either en or ru) based on the selected language to display the correct translations for all labels, messages, and other text.\n\nUsing this example, you can easily localize for your or multiple languages.\n\n5. How to put it all together in a Streamlit app\n\nYou can now create the main logic of the application.\n\nThe code is below ‚§µÔ∏è\n\nfrom streamlit_option_menu import option_menu\nfrom src.utils.lang import en, ru\nfrom src.utils.conversation import show_chat_buttons, show_text_input, show_conversation\nimport streamlit as st\n\n# --- GENERAL SETTINGS ---\nPAGE_TITLE: str = \"AI Talks\"\nPAGE_ICON: str = \"ü§ñ\"\nLANG_EN: str = \"En\"\nLANG_RU: str = \"Ru\"\nAI_MODEL_OPTIONS: list[str] = [\n    \"gpt-3.5-turbo\",\n    \"gpt-4\",\n    \"gpt-4-32k\",\n]\n\nst.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON)\n\nselected_lang = option_menu(\n    menu_title=None,\n    options=[LANG_EN, LANG_RU, ],\n    icons=[\"globe2\", \"translate\"],\n    menu_icon=\"cast\",\n    default_index=0,\n    orientation=\"horizontal\",\n)\n\n# Storing The Context\nif \"locale\" not in st.session_state:\n    st.session_state.locale = en\nif \"generated\" not in st.session_state:\n    st.session_state.generated = []\nif \"past\" not in st.session_state:\n    st.session_state.past = []\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\nif \"user_text\" not in st.session_state:\n    st.session_state.user_text = \"\"\n\ndef main() -> None:\n    c1, c2 = st.columns(2)\n    with c1, c2:\n        c1.selectbox(label=st.session_state.locale.select_placeholder1, key=\"model\", options=AI_MODEL_OPTIONS)\n        role_kind = c1.radio(\n            label=st.session_state.locale.radio_placeholder,\n            options=(st.session_state.locale.radio_text1, st.session_state.locale.radio_text2),\n            horizontal=True,\n        )\n        match role_kind:\n            case st.session_state.locale.radio_text1:\n                c2.selectbox(label=st.session_state.locale.select_placeholder2, key=\"role\",\n                             options=st.session_state.locale.ai_role_options)\n            case st.session_state.locale.radio_text2:\n                c2.text_input(label=st.session_state.locale.select_placeholder3, key=\"role\")\n    if st.session_state.user_text:\n        show_conversation()\n        st.session_state.user_text = \"\"\n    show_text_input()\n    show_chat_buttons()\n\nif __name__ == \"__main__\":\n    match selected_lang:\n        case \"En\":\n            st.session_state.locale = en\n        case \"Ru\":\n            st.session_state.locale = ru\n        case _:\n            st.session_state.locale = en\n    st.markdown(f\"<h1 style='text-align: center;'>{st.session_state.locale.title}</h1>\", unsafe_allow_html=True)\n    main()\n\n\nThis ‚§¥Ô∏è code sets up your app with a chat interface for interacting with different AI models:\n\nIt imports necessary libraries and modules.\nIt defines general settings, such as the page title, icon, and language options.\nIt sets up Streamlit's page configuration with the specified settings.\nIt creates a horizontal option menu for selecting the language (English or Russian).\nIt initializes session state values for storing conversation context and user input.\nIt defines the main function, which contains the following elements:\na. Two columns: one for selecting the AI model and toggling between role kinds and another for selecting or creating specific roles.\nb. It displays the conversation history with show_conversation if the user entered text.\nc. It displays an input box for the user to type their message with show_text_input.\nd. It displays a series of chat buttons with show_chat_buttons to allow users to control chat and send messages.\nIt executes the main function and displays the selected language, application title, and chat interface components on the web page.\nDemo:\n\nWrapping up\n\nThank you for reading my post! Now you can build your own AI assistant or use AI Talks from any country without registration and VPN.\n\nAt the time of writing, gpt-3.5-turbo and gpt-4 are available, but don't be surprised if gpt-4 is disabled in production due to the high load.\n\nAI Talks repo is waiting for your stars üôÇ.\n\nIf you have any questions, please post them in the comments below or in the Streamlit Discord app-sharing-gallery. You can also ask questions on Telegram at AI Talks Chat or follow app updates on the AI Talks Telegram channel.\n\nHappy Streamlit-ing! üéà\n\nP.S.: Check out this post in Russian on Habr, a Russian collaborative blog about IT, computer science, and anything related to the Internet.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "aura-create-free.png (1952√ó1376)",
    "url": "https://blog.streamlit.io/content/images/2023/06/aura-create-free.png#border",
    "html": ""
  },
  {
    "title": "aura-credentials.png (1195√ó898)",
    "url": "https://blog.streamlit.io/content/images/2023/06/aura-credentials.png#border",
    "html": ""
  },
  {
    "title": "Text-Summarization-App_scaling-0.3_fps-20_speed-10.0_duration-0-67.gif (695√ó547)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Text-Summarization-App_scaling-0.3_fps-20_speed-10.0_duration-0-67.gif",
    "html": ""
  },
  {
    "title": "Bildschirmfoto-2023-06-15-um-21.55.33.png (1506√ó1432)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-15-um-21.55.33.png#border",
    "html": ""
  },
  {
    "title": "Detecting fake images with a deep-learning tool",
    "url": "https://blog.streamlit.io/detecting-fake-images-with-a-deep-learning-tool/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nBy Kanak Mittal\nPosted in Advocate Posts, April 11 2023\nWhy did I decide to make it?\nStep 1. Collecting the dataset\nStep 2. Preprocessing images\nStep 3: Splitting the dataset\nStep 4. Designing the model architecture\nStep 5. Training the model\nStep 6. Evaluating the model\nStep 7. Building the UI\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Kanak, and I‚Äôm a Cloud Quality and Reliability Intern at Zscaler. The world of AI never ceases to amaze me. I enjoy learning about new technologies and tools that can help me solve complex problems and make data-driven decisions. Last year, during a hackathon, I built Deforgify‚Äîan app that detects fake images. It received a $1,000 grant from the 1517 community!\n\nIn this post, I‚Äôll walk you through the seven steps of making it:\n\nCollecting the dataset\nPreprocessing images\nSplitting the dataset\nDesigning the model architecture\nTraining the model\nEvaluating the model\nBuilding the UI\n\nWant to jump right in? Here's the app and the repo code.\n\nBut first, I‚Äôd love to share with you‚Ä¶\n\nWhy did I decide to make it?\n\nGenerative Adversarial Networks (GANs) have been successful in deep learning, particularly in generating high-quality images that are indistinguishable from originals. However, GANs can also generate fake faces that deceive both humans and machine learning (ML) classifiers. There are many tutorials on YouTube that show how to use software such as Adobe Photoshop to create synthetic photographs‚Äîto be easily shared on social media and used for defamation, impersonation, and factual distortion.\n\nI created Deforgify to leverage the power of deep learning in distinguishing real images from fake ones. This means that if someone were to Photoshop your face onto someone else's body, Deforgify would evaluate it and tag it as fake in a fraction of a second!\n\nNow, let‚Äôs build the app step-by-step.\n\nStep 1. Collecting the dataset\n\nDownload the dataset of real and fake faces from Kaggle. It has 1,288 faces‚Äî589 real and 700 fake ones. The fake faces were generated using StyleGAN2, which presents a harder challenge to classify them correctly (even for the human eye):\n\nStep 2. Preprocessing images\n\nTo build an effective neural network model, you must carefully consider the input data format. The most common parameters for image data input are the number of images, the image's height and width, and the number of channels. Typically, there are three channels of data corresponding to the colors Red, Green, and Blue (RGB), and the pixel levels are usually [0,255].\n\nMake a function that accepts an image path, reads it from the disk, applies all the pre-processing steps, and returns it. Use the OpenCV library for reading and resizing images, and NumPy for normalization:\n\ndef read_and_preprocess(img_path):\n\t\t# reading the image\n    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\t\t# resizing it to 128*128 to ensure that the images have the same size and aspect ratio. (IMAGE_SIZE is a global variable)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n\t\t# convert its datatype so that it could be normalized\n    img = np.array(img, dtype='float32') \n\t\t# normalization (now every pixel is in the range of 0 and 1)\n    img = img/255 \n    return img\n\nüí°\nPRO TIP: Tensorflow has an in-built flow_from_directory() method that provides a great abstraction to combine all these steps. To learn more about image pre-processing, read this amazing post by Nikhil.\nStep 3: Splitting the dataset\n\nSplitting the dataset is an important but often overlooked step in the ML process. It helps prevent overfitting, which occurs when a model learns the noise in the training data instead of the underlying pattern. This can result in a model that performs well on the training data but poorly on the testing data. By splitting the dataset, you can evaluate the model's performance on unseen data and avoid overfitting.\n\nAnother reason for splitting is to ensure that the model is generalizable. A model that is trained on a specific dataset may not perform well on new, unseen data. By splitting the dataset into training and testing sets, you can train the model on one set of data and evaluate its performance on another set, ensuring that it can handle new data.\n\nI did 80% training, 10% testing, and 10% validation data split in such a way that:\n\nThe samples were shuffled\nThe ratio of each class was maintained (stratify)\nEvery time the data was split, it generated the same samples (random-state)\n\nStratifying while splitting data ensures that the distribution of the target classes in the training and testing sets is representative of the overall population.\n\nFor example, let's say you have a dataset of 1,000 individuals, 700 of which are men and 300 are women. You want to build a model to predict whether an individual has a certain medical condition or not. If you randomly split the data into a 70% training and 30% testing split without stratifying, it's possible that the testing set could have significantly fewer women than the training set. This could result in the model being biased towards men, leading to poor performance when predicting the medical condition in women.\n\nOn the other hand, if you stratify the data based on the gender variable, you ensure that both the training and testing sets have the same proportion of men and women. For instance, you could stratify the data so that the training set has 490 men and 210 women, and the testing set has 210 men and 90 women. This ensures that your model is trained on a representative sample of both genders, and it should therefore be more accurate when predicting the medical condition for both men and women.\n\nUsing the random_state parameter ensures that the same data is obtained each time you split the dataset. This lets you replicate your analysis and results, which is crucial for reproducibility. Without setting a random seed, each split of the data may be different, leading to inconsistent results.\n\nSetting a random state is also important when you're sharing your work with others or comparing your results. If different individuals are using different random seeds, they may end up with different results even if they use the same code and dataset.\n\nStep 4. Designing the model architecture\n\nIt‚Äôs possible that a model architecture, which has worked well on one problem statement, could work well on other problem statements as well. This is because the underlying ML concepts and techniques are often applicable across different domains and problem statements.\n\nAfter reading numerous research papers and experimenting, I have designed my own model architecture that I use across all my projects (with minor tweaks):\n\nIt consists of a sequential model with five convolutional layers and four dense layers.\nThe first convolutional layer has 32 filters and a kernel size of 2x2.\nAt each subsequent convolutional layer, the number of filters is doubled, and the kernel size is incremented by one.\nTo reduce overfitting and computational costs, max-pooling layers are introduced after each convolutional layer.\nThe output from the convolutional layer is flattened and passed to the dense layers.\nThe first dense layer has 512 neurons, and the number of neurons is halved in the next two dense layers.\nDropout layers are introduced throughout the model to randomly ignore some neurons and reduce overfitting.\nReLU activation is used in all layers except for the output layer, which has two neurons (one for each class) and softmax activation.\n\nStep 5. Training the model\n\nCompiling an ML model involves configuring its settings for training. The three key settings to consider are:\n\nThe loss function: It measures the difference between the predicted output and the actual output of the model. The choice of loss function depends on the type of problem being solved. Since you're performing classification and your training labels are represented in categorical format (0,1), sparse_categorical_crossentropy is the logical choice.\nThe optimizer: It updates the weights of the model during training to minimize the loss function. Some popular optimizer algorithms include stochastic gradient descent, Adam, and RMSprop. The choice of optimizer depends on the complexity of the problem and the size of the dataset. The Adam optimizer works well in most cases, so \"when in doubt, go with Adam!\"\nThe evaluation: It measures the performance of the model during training and validation. Since your dataset is pretty balanced, there is no harm in going with accuracy.\n\nHyperparameters such as the learning rate, batch size, number of epochs, and regularization parameters can also be adjusted during model compilation.\n\nThe two optional steps that help improve the model‚Äôs performance and save training time are:\n\nEarly stopping: It prevents overfitting and reduces training time by monitoring the performance of the model on a validation set during training. It stops the training process once the model's performance on the validation set starts to decrease, indicating that it‚Äôs reached its optimal performance. Early stopping can save time and computational resources and prevent the model from overfitting to the training data. You‚Äôre going to configure early stopping to monitor minimum validation loss with the patience of 10, which means that if the validation loss doesn‚Äôt reduce for 10 continuous epochs, the model training will be stopped:\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n\n\nCheckpointer: It saves the model's weights at specific intervals during training. This enables the model to resume training from where it stopped in the event that the process is halted due to an error or other issue. The checkpointer stores the weights in a file that can be loaded later to resume training. Checkpointing is particularly beneficial for large models that take a long time to train, as it allows the training process to continue without starting over. To set the checkpointer to save only the best model (i.e., one with the least validation loss), follow these steps:\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath=\"fakevsreal_weights.h5\", verbose=1, save_best_only=True)\n\n\nYou‚Äôre now ready to train your model! Remember to pass in all the parameters that you have configured:\n\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=32, shuffle=True, callbacks=[earlystopping, checkpointer])\n\nStep 6. Evaluating the model\n\nEvaluating the performance of an ML model is crucial to assess its ability to generalize to new, unseen data. To reload it, use the load_model() function from the Keras library.\n\nmodel = tf.keras.models.load_model('fakevsreal_weights.h5')\n\n\nMake predictions on the testing data, evaluate its performance, and compute its accuracy, precision, recall, and F1-score using the confusion matrix:\n\nHere is the complete classification report:\n\nStep 7. Building the UI\n\nThe model's performance evaluation alone may not be sufficient to showcase its capability. End users might not have the necessary skills or time to write code and use the trained model. One solution is to build a user interface (UI) that allows users to interact with the model. However, this requires a different skillset that many users may not possess.\n\nThis is where Streamlit comes in! üéà\n\nWith just 12 lines of code, you can create an interactive Streamlit app using Streamlit (this will showcase your skills and make your models accessible to a wider audience):\n\nimport streamlit as st\n# util is a custom file that includes steps to read the model and run predictions on an image\nimport util\n\nst.image('Header.gif', use_column_width=True)\nst.write(\"Upload a Picture to see if it is a fake or real face.\")\nfile_uploaded = st.file_uploader(\"Choose the Image File\", type=[\"jpg\", \"png\", \"jpeg\"])\nif file_uploaded is not None:\n    res = util.classify_image(file_uploaded)\n    c1, buff, c2 = st.columns([2, 0.5, 2])\n    c1.image(file_uploaded, use_column_width=True)\n    c2.subheader(\"Classification Result\")\n    c2.write(\"The image is classified as **{}**.\".format(res['label'].title()))\n\n\nHere is the result:\n\nBut that's not all!\n\nDeploying an ML app on the cloud can be a daunting task, even for experienced data scientists and developers. Fortunately, Streamlit makes it easy to deploy apps to their Streamlit Community Cloud platform with just a few clicks, without any complex configurations or infrastructure setup. To get started, create an account, obtain your app's code, and follow these steps.\n\nWrapping up\n\nThank you for reading my post! You learned how to make an image classifier that can distinguish between real and fake images, built a UI to showcase your model's capability to the world using Streamlit, and deploy it on the Streamlit Community Cloud.\n\nIf you have any questions, please post them in the comments below or contact me on Twitter, LinkedIn, or GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Bildschirmfoto-2023-06-16-um-00.13.48.png (1424√ó1450)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-06-16-um-00.13.48.png",
    "html": ""
  },
  {
    "title": "Create an animated data story with ipyvizzu and Streamlit",
    "url": "https://blog.streamlit.io/create-an-animated-data-story-with-ipyvizzu-and-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nCreate an animated data story with ipyvizzu and Streamlit\n\nA tutorial on using ipyvizzu and ipyvizzu-story\n\nBy Peter Vidos\nPosted in Advocate Posts, April 20 2023\nHow to use ipyvizzu\nGathering the data\nBuilding your first chart\nHow to build a data story with ipyvizzu-story\nAdding more slides\nComparing scenarios\nShowing growth and loss components\nAggregating and comparing scenarios\nHow to embed the story in a Streamlit app\nHow to use Streamlit's input widgets for interactive storytelling\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit community! üëã\n\nI'm Peter Vidos, the CEO and Founder of Vizzu. Our cutting-edge open-source JavaScript library makes it easy to create animated charts and data stories. To help data scientists share their insights, we created a Python API called ipyvizzu and its storytelling extension, ipyvizzu-story. I'm excited to introduce these tools and guide you in building amazing animated data stories in Streamlit!\n\nThis tutorial covers:\n\nHow to use ipyvizzu\nHow to build a data story with ipyvizzu-story\nHow to embed the story in a Streamlit app\nHow to use Streamlit's input widgets for interactive storytelling\n\nIf you can't wait to try it out, here's the app and the repo.\n\nBut first, let's answer a few questions you might have...\n\nHow to use ipyvizzu\n\nipyvizzu is an IPython extension of the open-source Vizzu JavaScript charting library. It uses a single logic to create any chart, allowing it to interpolate between the charts expressed on its interface. In essence, it's a generic chart morpher.\n\nThe animate method lies at the heart of ipyvizzu. It has three arguments: one for setting the data being visualized, one for configuring the chart, and one for styling. Optional arguments can also describe how ipyvizzu should animate to a certain state.\n\nWhen describing a chart on ipyvizzu's interface, it'll draw a static chart. Then, if you describe another chart, the original chart will morph into the new one. This morphing can involve small changes, such as zooming in on certain parts, or more profound alterations, like changing geometrical elements, coordinate systems, and adding or removing categories or values.\n\nRegardless of the changes, ipyvizzu will automatically create an animation to connect your charts, making it easy to follow. (We're trying hard to make it that way üòâ.) Ipyvizzu can be installed from PyPI or Conda, which works within various notebooks and other environments. (Read more in our docs.)\n\nNow, let's get some data and start building!\n\nGathering the data\n\nThe data used in this analysis comes from the United Nation's World Population Projections, a comprehensive database containing population data for every country from 1950 to 2100.\n\nThe UN categorizes countries into six regions that largely correspond to continents: Africa, Asia, Europe, Latin America and the Caribbean (LAC), Northern America, and Oceania. In this analysis, we'll focus on the population of these regions.\n\nThe initial dataframe sample below shows three forecasted series in the last three columns: Medium (in the Population column), High, and Low. While the past values are identical across the three series, the future values differ:\n\nYear\tRegion\tPeriod\tPopulation\tHigh\tLow\n2010\tAfrica\tPast\t1 041 484 014\t1 041 484 014\t1 041 484 014\n2015\tAfrica\tPast\t1 185 462 534\t1 185 462 534\t1 185 462 534\n2020\tAfrica\tPast\t1 344 069 830\t1 344 069 830\t1 344 069 830\n2025\tAfrica\tFuture\t1 512 428 655\t1 520 384 567\t1 504 473 149\n2030\tAfrica\tFuture\t1 692 186 171\t1 720 044 019\t1 664 337 234\nBuilding your first chart\nchart.animate(data,\n   Config({\n            'x': 'Year',\n            'y': 'Population',\n            'title': 'The Population of the World 1950-2100'\n        })\n)\n\n\nI've made some changes to the styling (not shown above) to keep the focus on the chart configuration for now:\n\nNotice that ipyvizzu automatically aggregates the population values per year. If you call the animate method again with a different configuration‚Äîby adding Region to the y-axis and the color scale‚Äîmagic happens:\n\nchart.animate(\n   Config({\n            'y': ['Population','Region'],\n            'color': 'Region',\n            'title': 'The Population of Regions 1950-2100',\n        })\n)\n\n\nTo zoom in, use a filter on the chart data. The filter is a JavaScript expression that only shows items on the chart that match the expression. Let's zoom in on past data:\n\nchart.animate(\n    Data.filter(\"record.Period == 'Past'\"),\n    Config({'title': 'The Population of Regions 1950-2020'}),\n)\n\n\nHow to build a data story with ipyvizzu-story\n\nipyvizzu-story is an extension of ipyvizzu that enables embedding charts into an interactive data story. This story can be presented on the fly or shared as an interactive HTML.\n\nipyvizzu-story adds buttons underneath the charts, enabling viewers to switch between them intuitively. Navigation can also be done using PgUp, PgDn buttons, arrow keys, or a clicker. With the full-screen option in the bottom right corner, you have a presentation tool within your Streamlit app.\n\nThe syntax of ipyvizzu-story is very similar to ipyvizzu's. With ipyvizzu-story, you have a Story object containing all of the data shown in the story and the charts created based on that data. These are arranged into Slides and Steps.\n\nSlides can contain one or more Steps.\n\nA Step (and a single-Step Slide) is essentially the same as the Chart object in ipyvizzu, with some minor but important differences (for now):\n\nAll of the data must be added to the story at initialization and can be filtered at each Step throughout the Story.\nAnimation options are not available.\n\nIn the case of a Slide with multiple Steps, all but the last Steps are interim charts that connect a Slide with a previous Slide. The animation will only stop when the Story is played at the last Step.\n\nHere are the three charts created so far, with a small update. The story starts with a filter already applied on the first slide to show only the data for the past, as it makes more sense from a storytelling aspect. Click on the buttons below the chart to explore this mini-story.\n\nAdding more slides\n\nLet's explore some basic features of ipyvizzu-story by adding more slides to the data story. Below is the code for four additional slides, followed by another interactive story at the end of the section that begins with the last slide of the previous story, making navigation easier.\n\nLet's add some slides to help viewers better understand the population dynamics for these regions. First, let's change the geometry to the area so that trends are easier to spot:\n\nslide2 = Slide(\n    Step(\n        Config({'geometry': 'area'})\n))\n\nstory2.add_slide(slide2)\n\n\nThen, use a nifty feature of ipyvizzu called splitting to show the components of a stacked chart (in this case, the regions) side-by-side:\n\nslide3 = Slide(\n    Step(\n        Config({'split': True}),\n))\n\n\nAnother intriguingly simple option in ipyvizzu and ipyvizzu-story is the ability to set the alignment of the chart to \"stretch\". This will result in the chart showing percentages instead of values. Additionally, you should switch off the splitting from the previous slide in the same step:\n\nslide4 = Slide(\n    Step(\n        Config({\n            'split': False, \n            'align':'stretch',\n            'title': 'The Population of Regions 1950-2100 (%)'\n})))\n\n\nFinally, let's zoom in on one of the regions, Africa, using the Data.filter method. To make it easier to understand, we'll use a slide with two steps. First, we switch back to values with ‚Äòalign‚Äô:‚Äômin‚Äô, and then we apply the filter:\n\nslide5 = Slide()\n\nslide5.add_step(\n    Step(\n        Config({\n            'align':'min'\n})))\n\nslide5.add_step(\n    Step(\n        Data.filter(\"record.Region == 'Africa'\"),\n        Config({\n            'title': 'The Population of Africa 1950-2100'\n})))\n\n\nAnd here you go!\n\nComparing scenarios\n\nOne of the greatest benefits of using animated charts is the intuitive connection between different data set views. This feature works exceptionally well for all of the transitions you've seen so far. Another significant use case for this feature is when you want to compare scenarios. Fortunately, the U.N. provides different forecasts for how the population will change in the future.\n\nUp to this point, we have only worked with the Medium scenario. Now, it's time to experiment with the High and Low scenarios. To do so, change the value on the y-axis:\n\nslide2 = Slide(\n    Step(\n        Config({\n            'y': ['High','Region'],\n})))\n\nslide3 = Slide(\n    Step(\n        Config({\n            'y': ['Low','Region'],\n})))\n\n\nLet's check the story!\n\nThe first slide repeats the last slide from the previous section, with a minor change. Can you spot it?\n\nThe only change I made was fixing the range of the y-axis to 6 billion. It's easier to compare the values in different scenarios this way, but I had to switch off ipyvizzu's default responsive range‚Äîwhich works great in many other cases. Here are the code snippet and a short GIF to compare the same slides with the two settings:\n\nConfig({y': { \"range\": {\"max\": 6000000000} }})\n\n\nShowing growth and loss components\n\nWith ipyvizzu's animated transitions, you can dig a little deeper while keeping the context and helping the audience follow along. But you naturally have to have the desired depth within the data.stitutes the growth and decline in the forecasted population statistics, enriching the data by adding the number of births, deaths, and the positive/negative net migration.\n\nAs you can see in the example below, I have added an additional dimension called \"Category\" and renamed \"Population\" to \"Medium\" for easier comprehension. Note that deaths and negative net migration are represented as negative numbers:\n\nYear\tRegion\tPeriod\tCategory\tMedium\tHigh\tLow\n2010\tAfrica\tPast\tDeaths\t- 52 967 312\t- 52 967 312\t- 52 967 312\n2015\tAfrica\tPast\tDeaths\t- 53 597 303\t- 53 597 303\t- 53 597 303\n2020\tAfrica\tPast\tDeaths\t- 58 545 143\t- 58 545 143\t- 58 545 143\n2025\tAfrica\tFuture\tDeaths\t- 60 338 052\t- 59 286 873\t- 61 389 558\n2030\tAfrica\tFuture\tDeaths\t- 65 556 853\t- 63 865 647\t- 67 249 451\n\nTo help viewers understand how these factors contribute to the population, I want the sources of growth to appear above the chart and deaths and negative net migration to appear below the x-axis (represented by negative numbers).\n\nTo properly show these extra categories on the chart, use the color scale‚Äîjust like you did when you showed the regional composition of the world population before. But you'll also need to update the color palette for two reasons:\n\nNot to confuse the viewers by using the same colors as the regions\nTo keep the components beneath the x-axis visible by giving the region (in this case, Africa) a semi-transparent color\n\nSet the palette using the Style argument. The third color in this list corresponds to Africa. To adjust transparency, change the last two bits of the color code to 20 instead of the default FF value:\n\nStyle({ 'plot.marker.colorPalette': '#FF8080FF #808080FF #FE7B0020 #60A0FFFF #80A080FF' })\n\n\nAdd a slide that includes two steps. In the first step, show the data with the applied filter set to include only first births and positive net migration. In the second step, show the remaining two categories with the filter changed accordingly:\n\nslide2.add_step(Step(\n        Data.filter('record.Region === \"Africa\" && (record.Category === \"Population\" || record.Category === \"Migration+\" || record.Category === \"Births\")'),\n        Config(\n        {\n            'y':['Medium','Category'],\n            'color': ['Category']\n})))\n\nslide2.add_step(Step(\n        Data.filter('record.Region === \"Africa\"'),\n        Config({'title': 'Adding sources of gain and loss to the mix '})\n))\n\n\nHere's the new story snippet, starting where you last left off:\n\nThe positive and negative net migration is so small compared to all other factors that they can't be seen on the chart. But they're there‚Äîyou'll see it in the next step. üòâ\n\nAggregating and comparing scenarios\n\nAfter displaying the contributing factors on a chart, you can aggregate the projected births, deaths, and net migration for the period between 2020 and 2100. Comparing each scenario will help you understand their differences better.\n\nJust filter out the Population data and all information about the past. Remove the Year category, and ipyvizzu will automatically aggregate the values. Place the value on the x-axis instead of the y-axis to create four bar charts. To compare the High and Low predictions, change this value and visually compare the scenarios (I fixed the x-axis range to make the comparison easier).\n\nClick through the slides to see the fundamental differences between the Medium, High, and Low population predictions:\n\nYep, the scenarios differ mostly by the number of projected Births.\n\nHow to embed the story in a Streamlit app\n\nTo embed the ipyvizzu-story in Streamlit, use the same code that you would use in a notebook within the .py file that serves as the source of your app (import a few more packages in the beginning). No need to call the story.play() method‚ÄîStreamlit will handle that for you:\n\nfrom streamlit.components.v1 import html\nimport ssl\nimport streamlit as st \nimport pandas as pd\nfrom ipyvizzu import Data, Config, Style\nfrom ipyvizzustory import Story, Slide, Step\n\nssl._create_default_https_context = ssl._create_unverified_context\n\n\nThe only minor difference compared to notebooks is that, in Streamlit, the size of the story must be set in pixels (in notebooks, you can also use percentages and other metrics):\n\nstory.set_size(750, 450)\n\n\nGenerate the HTML containing your story by adding the following snippet:\n\nhtml(story._repr_html_(), width=750, height=450)\n\n\nAnd voil√°!\n\nIt's worth noting that if you move between the first and last slides of this story, then ipyvizzu will fade as there aren't any data points that are the same in these two views.\n\nHow to use Streamlit's input widgets for interactive storytelling\n\nIt's time to use Streamlit's awesome possibilities! Let's add a couple of input widgets to make the story interactive. Users should be able to select the region they want to zoom in on to check the detailed forecasts. You can provide this option by creating a dropdown using st.selectbox:\n\nregions = df['Region'].unique()\nsel_region = st.selectbox('Select region', list(regions))\n\n\nNext, implement this choice throughout the story. The biggest challenge here is parameterizing the filters to zoom into the selected region and applying the corresponding color palettes and axis ranges.\n\nAfter the user selects a region with the select box, the story will regenerate and reset to the first slide. This is the default behavior of ipyvizzu-story after loading. But, since the first five slides are the same regardless of the selected region, it makes sense to enable the user to skip these standard slides. Just add st.checkbox and set its default value to \"False\":\n\nskip_intro = st.checkbox(\n    'Skip intro slides', value=False\n)\n\n\nIt's easy to implement this into the story. Simply add the code that creates the first slides into an if statement in such a way that these slides are only generated if the skip_intro value is false:\n\nif not skip_intro :\n    slide1 = Slide(\n        Step(...\n\n\nIf you check the code of the final app, you'll notice that we used the condition the other way around, so it first checks if the skip_intro is True. This is because if it's True, you must change the color palette to use the colors you selected for the regions before the first slide was played.\n\nLastly, ipyvizzu-story has another great feature.\n\nYou can export the story as an interactive HTML file that includes the slides and data. This file can be sent via email, embedded in a web page (like this blog post üòä), or served from any web server. To make it easy for viewers to use this feature, add a button with just one line of code. Use st.download_button to export the story into an HTML file that includes the name of the selected region.\n\nst.download_button('Download HTML export', story.to_html(), file_name=f'world-population-story-{sel_region}.html', mime='text/html')\n\n\nAnd here you go! üôÇ\n\nWrapping up\n\nWe've covered everything you need to know to create amazing animated data stories with ipyvizzu and ipyvizzu-story in Streamlit. We hope this tutorial has empowered you to take your data storytelling to the next level. Remember, you can always reach out to the Vizzu community and team for help on your data journey. Join our Slack workspace to start collaborating and sharing your data stories! If you have any questions, please post them in the comments below or contact me on GitHub, Twitter, or LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Bildschirmfoto-2023-05-31-um-23.09.30.png (1830√ó1984)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-05-31-um-23.09.30.png#browser",
    "html": ""
  },
  {
    "title": "Bildschirmfoto-2023-05-31-um-23.13.00.png (1198√ó874)",
    "url": "https://blog.streamlit.io/content/images/2023/06/Bildschirmfoto-2023-05-31-um-23.13.00.png#border",
    "html": ""
  },
  {
    "title": "data-editor-add-delete-10.44.28-AM-1.gif (899√ó605)",
    "url": "https://blog.streamlit.io/content/images/2023/02/data-editor-add-delete-10.44.28-AM-1.gif#browser",
    "html": ""
  },
  {
    "title": "quickstart-app_scaling-0.3_fps-20_speed-10.0_duration-0-49.gif (677√ó552)",
    "url": "https://blog.streamlit.io/content/images/2023/05/quickstart-app_scaling-0.3_fps-20_speed-10.0_duration-0-49.gif",
    "html": ""
  },
  {
    "title": "st-status-expand-output.gif (1189√ó929)",
    "url": "https://blog.streamlit.io/content/images/2023/09/st-status-expand-output.gif",
    "html": ""
  },
  {
    "title": "Screenshot-2566-05-25-at-10.02.18.png (2234√ó1336)",
    "url": "https://blog.streamlit.io/content/images/2023/05/Screenshot-2566-05-25-at-10.02.18.png",
    "html": ""
  },
  {
    "title": "blog-outline-app_scaling-0.3_fps-20_speed-10.0_duration-0-44.gif (677√ó552)",
    "url": "https://blog.streamlit.io/content/images/2023/06/blog-outline-app_scaling-0.3_fps-20_speed-10.0_duration-0-44.gif",
    "html": ""
  },
  {
    "title": "schematic-1.jpeg (2000√ó1710)",
    "url": "https://blog.streamlit.io/content/images/2023/05/schematic-1.jpeg",
    "html": ""
  },
  {
    "title": "Streamlit App Starter Kit: How to build apps faster",
    "url": "https://blog.streamlit.io/streamlit-app-starter-kit-how-to-build-apps-faster/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit App Starter Kit: How to build apps faster\n\nSave 10 minutes every time you build an app\n\nBy Chanin Nantasenamat\nPosted in Tutorials, September 27 2022\nWhat is the Streamlit App Starter Kit?\nHow to use the Streamlit App Starter Kit\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nTo build a Streamlit app you‚Äôd typically follow these steps:\n\nInstall prerequisite libraries by specifying library names in requirements.txt\nCustomize the theme via .streamlit/config.toml (optional)\nCreate an app file streamlit_app.py\nInside the app file, call import streamlit as st\nSpecify the app tasks (e.g. read a CSV, perform data wrangling, display a scatter plot, train an ML model, etc.)\n\nEvery step takes only minutes, but over time it can amount to hours‚Äîor even days! üòÖ\n\nIn this article, you‚Äôll learn how to save 10 minutes every time you build an app:\n\nWhat is the Streamlit App Starter Kit?\nHow to use the Streamlit App Starter Kit\nüëâ\nNOTE: We recommend having a basic knowledge of Python and Streamlit (learn it by completing the #30DaysOfStreamlit challenge), plus a GitHub and a Streamlit Community Cloud accounts.\n\nLet‚Äôs dive right in!\n\nWhat is the Streamlit App Starter Kit?\n\nThe Streamlit App Starter Kit has the following files:\n\napp-starter-kit/\n‚îú‚îÄ .streamlit/\n‚îÇ  ‚îú‚îÄ config.toml\n‚îú‚îÄ README.md\n‚îú‚îÄ packages.txt (optional)\n‚îú‚îÄ requirements.txt\n‚îú‚îÄ streamlit_app.py\n\n\nThis is what it looks like in a GitHub repository:\n\nIt contains:\n\n.streamlit/config.toml‚Äîa configuration file with parameters for customizing your app‚Äôs theme:\n\n[theme]\nprimaryColor=\"#F63366\"\nbackgroundColor=\"#FFFFFF\"\nsecondaryBackgroundColor=\"#F0F2F6\"\ntextColor=\"#262730\"\nfont=\"sans serif\"\n\n\nREADME.md‚Äîa README file with a project description:\n\n# Name of Streamlit App\n\nDescription of the app ...\n\n## Demo App\n\n[![Streamlit App](<https://static.streamlit.io/badges/streamlit_badge_black_white.svg>)](<https://share.streamlit.io/dataprofessor/st-app/>)\n\n## Section Heading\n\nThis is filler text. Please replace this with the text for this section.\n\n## Further Reading\n\nThis is filler text. Please replace this with explanatory text about further relevant resources for this repo.\n- Resource 1\n- Resource 2\n- Resource 3\n\n\npackages.txt‚Äîa list of Linux tools and packages to install (blank by default). Go ahead and populate it with the package names you want to install‚Äîone name per line.\n\nrequirements.txt‚Äîa list of Python libraries to install. By default, the Streamlit App Starter Kit lists only streamlit. It‚Äôll install the latest version:\n\nstreamlit\n\nIf you want a specific version‚Äîlike 1.13.0‚Äîdo the following:\n\nstreamlit==1.13.0\n\nAdd some Python libraries:\n\nstreamlit==1.13.0\npandas==1.3.5\nscikit-learn==1.1.0\n\nstreamlit_app.py‚Äîthe Streamlit app:\n\nimport streamlit as st\n\nst.title('üéà App Name')\n\nst.write('Hello world!')\n\nHow to use the Streamlit App Starter Kit\n\nThe Streamlit App Starter Kit is available as a GitHub template. Clone it to your repo and use it to make your own Streamlit app:\n\nWant to customize the contents of the app files? Use widgets to accept user input and display the output results (read more about widgets in our docs).\n\nFinally, deploy your app with the Streamlit Community Cloud or some other cloud service provider! üéâ\n\nWrapping up\n\nCongratulations! You‚Äôve used the Streamlit App Starter Kit to make your app-making process faster. üí®\n\nIf you like to work with command line interfaces, check out the streamlit-kickoff-cli developed by our very own Arnaud Miribel. And if you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Building a PivotTable report with Streamlit and AG Grid",
    "url": "https://blog.streamlit.io/building-a-pivottable-report-with-streamlit-and-ag-grid/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding a PivotTable report with Streamlit and AG Grid\n\nHow to build a PivotTable app in 4 simple steps\n\nBy Pablo Fonseca\nPosted in Advocate Posts, March 7 2023\nSome context on data\nStep 1. How to load and display data\nStep 2. How to configure the grid using gridOptionsBuilder\nStep 3. How to configure the grid pivot mode\nStep 4. How to add grouping on rows and columns\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Pablo, and I'm the creator of the Streamlit AgGrid component. A little about me: I started coding 25 years ago and never stopped. Currently, I work as a portfolio manager for renewable energy development in Brazil.\n\nI needed to summarize the data for the available wind-farm energy in a PivotTable report: power sales, purchases, and expected generation. If the sold amount were more than the expected generation, it‚Äôd signal the need to buy power from other producers. So I built a simple dashboard with Streamlit and streamlit-aggrid!\n\nIn this post, I‚Äôll show you:\n\nHow to load and display data\nHow to configure the grid using gridOptionsBuilder\nHow to configure the grid pivot mode\nHow to add grouping on rows and columns\n\nWant to skip reading and try it out? Here's a sample app and a repo code.\n\nBut first, let‚Äôs talk about‚Ä¶\n\nSome context on data\n\nIn Brazil, consumers buy power in advance and sign contracts for future power delivery (PPA‚ÄîPower Purchase Agreement). The energy portfolio managers need to calculate the energy balance to mitigate the risk and protect the revenue. Is it too short or too long?\n\nMuch like a bank statement, the energy balance is a total of transactions. Sales can deplete it, while purchases can add to it.\n\nHere is a sample of fictional balance data for seven wind farms (located across five Brazilian states) that have signed PPAs with customers for their 2023 power supply:\n\n\nstate\tpowerPlant\trecordType\tbuyer\treferenceDate\thoursInMonth\tvolumeMWh\nCE\tOcean Breeze Energy Park\tPower Sale\tChargeMax Limited Liability\t2023-06-01\t720\t-158.221\nBahia\tSkyline Wind Ranch\tPower Sale\tPowerPulse Energy Inc.\t2023-08-01\t744\t-108.894\nCE\tOcean Breeze Energy Park\tPower Sale\tPowerPlus Enterprises\t2023-03-01\t744\t-371.49\nCE\tWindfarm at Sunrise\tPower Sale\tSparkPlug Energy Ltd.\t2023-02-01\t672\t-172.012\nCE\tWindward Heights Wind Farm\tPower Sale\tSparkPlug Energy Ltd.\t2023-07-01\t744\t-271.877\nCE\tWindfarm at Sunrise\tPower Sale\tShockPower Ltd.\t2023-04-01\t720\t-366.159\nCE\tPrairie Wind Power Plant\tPower Sale\tPowerPulse Energy Inc.\t2023-09-01\t720\t-76.8527\nCE\tPrairie Wind Power Plant\tPower Sale\tEnergyEmpire Corp.\t2023-05-01\t744\t-926.426\nCE\tPrairie Wind Power Plant\tExpected Generation\t\t2023-08-01\t744\t9448.8\nSP\tCoastal Wind Energy Station\tPower Sale\tVoltWatt Inc.\t2023-03-01\t744\t-477.762\nStep 1. How to load and display data\n\nLoading data is straightforward. Just use pandas read_csv to load it from a text file (or any other preferred method). To render it, use streamlit-aggrid with the default parameters. Create a file named app.py in the same folder where you downloaded the data and add this code:\n\n#app.py\nimport streamlit as st\nimport pandas as pd\nfrom st_aggrid import AgGrid\n\n@st.cache_data()\ndef load_data():\n    data = pd.read_csv('./data.csv', parse_dates=['referenceDate'])\n    return data\n\ndata = load_data()\n\nAgGrid(data, height=400)\n\n\nLaunch your dashboard with streamlit run app.py. Your browser should open with the sample data loaded in AgGrid. The app will load data and display it using default configurations. They're nice but could be better formatted. Let‚Äôs use GridOptionsBuilder to customize it.\n\nStep 2. How to configure the grid using gridOptionsBuilder\n\nUpdate your app.py file as follows:\n\n#app.py\nimport streamlit as st\nimport pandas as pd\nfrom st_aggrid import AgGrid, GridOptionsBuilder  #add import for GridOptionsBuilder\n\n@st.cache_data()\ndef load_data():\n    data = pd.read_csv(\"./data.csv\", parse_dates=[\"referenceDate\"])\n    return data\n\ndata = load_data()\n\ngb = GridOptionsBuilder()\n\n# makes columns resizable, sortable and filterable by default\ngb.configure_default_column(\n    resizable=True,\n    filterable=True,\n    sortable=True,\n    editable=False,\n)\n\n#configures state column to have a 80px initial width\ngb.configure_column(field=\"state\", header_name=\"State\", width=80)\n\n#configures Power Plant column to have a tooltip and adjust to fill the grid container\ngb.configure_column(\n    field=\"powerPlant\",\n    header_name=\"Power Plant\",\n    flex=1,\n    tooltipField=\"powerPlant\",\n)\n\ngb.configure_column(field=\"recordType\", header_name=\"Record Type\", width=110)\n\ngb.configure_column(\n    field=\"buyer\", header_name=\"Buyer\", width=150, tooltipField=\"buyer\"\n)\n\n#applies a value formatter to Reference Date Column to display as a short date format.\ngb.configure_column(\n    field=\"referenceDate\",\n    header_name=\"Reference Date\",\n    width=100,\n    valueFormatter=\"value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''\",\n)\n\n#Numeric Columns are right aligned\ngb.configure_column(\n    field=\"hoursInMonth\",\n    header_name=\"Hours in Month\",\n    width=50,\n    type=[\"numericColumn\"],\n)\n#The last column is the value column and will be formatted using javascript number.toLocaleString()\ngb.configure_column(\n    field=\"volumeMWh\",\n    header_name=\"Volume [MWh]\",\n    width=100,\n    type=[\"numericColumn\"],\n    valueFormatter=\"value.toLocaleString()\",\n)\n\n#makes tooltip appear instantly\ngb.configure_grid_options(tooltipShowDelay=0)\ngo = gb.build()\n\nAgGrid(data, gridOptions=go, height=400)\n\n\nThe grid should look better now!\n\nStep 3. How to configure the grid pivot mode\n\nNow let‚Äôs make the grid pivot over the referenceDate column. Add a checkbox to your app:\n\nshouldDisplayPivoted = st.checkbox(\"Pivot data on Reference Date\")\n\n\nChange the referenceDate column definition to enable pivoting:\n\ngb.configure_column(\n    field=\"referenceDate\",\n    header_name=\"Reference Date\",\n    width=100,\n    valueFormatter=\"value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''\",\n    pivot=True # this tells the grid we'll be pivoting on reference date\n)\n\n\nConfigure the aggregation function on the volumeMWh (the value) column (values should sum up for a given month):\n\ngb.configure_column(\n    field=\"volumeMWh\",\n    header_name=\"Volume [MWh]\",\n    width=100,\n    type=[\"numericColumn\"],\n    valueFormatter=\"value.toLocaleString()\",\n    aggFunc=\"sum\" # this tells the grid we'll be summing values on the same reference date\n)\n\n\nFinally, enable pivotMode when the checkbox is on:\n\ngb.configure_grid_options(\n    pivotMode=shouldDisplayPivoted # Enables pivot mode\n    )\n\n\nHere is the complete code for this section:\n\nimport streamlit as st\nimport pandas as pd\nfrom st_aggrid import AgGrid, GridOptionsBuilder\n\n@st.cache_data()\ndef load_data():\n    data = pd.read_csv(\"./data.csv\", parse_dates=[\"referenceDate\"])\n    return data\n\ndata = load_data()\n\nshouldDisplayPivoted = st.checkbox(\"Pivot data on Reference Date\")\n\ngb = GridOptionsBuilder()\n\ngb.configure_default_column(\n    resizable=True,\n    filterable=True,\n    sortable=True,\n    editable=False,\n)\ngb.configure_column(field=\"state\", header_name=\"State\", width=80)\n\ngb.configure_column(\n    field=\"powerPlant\",\n    header_name=\"Power Plant\",\n    flex=1,\n    tooltipField=\"powerPlant\",\n)\ngb.configure_column(field=\"recordType\", header_name=\"Record Type\", width=110)\n\ngb.configure_column(\n    field=\"buyer\", header_name=\"Buyer\", width=150, tooltipField=\"buyer\"\n)\n\ngb.configure_column(\n    field=\"referenceDate\",\n    header_name=\"Reference Date\",\n    width=100,\n    valueFormatter=\"value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''\",\n    pivot=True,\n)\ngb.configure_column(\n    field=\"hoursInMonth\",\n    header_name=\"Hours in Month\",\n    width=50,\n    type=[\"numericColumn\"],\n)\ngb.configure_column(\n    field=\"volumeMWh\",\n    header_name=\"Volume [MWh]\",\n    width=100,\n    type=[\"numericColumn\"],\n    aggFunc=\"sum\",\n    valueFormatter=\"value.toLocaleString()\",\n)\n\ngb.configure_grid_options(\n    tooltipShowDelay=0,\n    pivotMode=shouldDisplayPivoted,\n)\ngo = gb.build()\n\nAgGrid(data, gridOptions=go, height=400)\n\nStep 4. How to add grouping on rows and columns\n\nSo far, your app displays the loaded data and pivot in a single line. Let‚Äôs group it into columns using virtual columns (so they‚Äôre hidden when pivotMode it is off). Set the valueGetter property on the columns definition. In this example, Year and Year-Month columns don't exist in the original data. Create them by setting the valueGetter with a JavaScript expression for the grid:\n\ngb.configure_column(\n    field=\"referenceDate\",\n    header_name=\"Reference Date\",\n    width=100,\n    valueFormatter=\"value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''\",\n    pivot=False, #remove pivoting on this column\n)\n\n#add two hidden virtual columns\ngb.configure_column(\n    field=\"virtualYear\",\n    header_name=\"Reference Date Year\",\n    valueGetter=\"new Date(data.referenceDate).getFullYear()\",\n    pivot=True, #allows grid to pivot on this column\n    hide=True #hides it when pivotMode is off.\n)\n\ngb.configure_column(\n    field=\"virtualMonth\",\n    header_name=\"Reference Date Month\",\n    valueGetter=\"new Date(data.referenceDate).toLocaleDateString('en-US',options={year:'numeric', month:'2-digit'})\",\n    pivot=True,\n    hide=True\n)\n\n\nUse State, Power Plant, Record Type, and Buyer columns for row grouping. This will create a nice hierarchical menu. The Grid aggregates data by applying aggFunc on collapsed rows and column values. To configure this behavior, set rowGroup property on each column definition:\n\ngb.configure_column(\n    field=\"powerPlant\",\n    header_name=\"Power Plant\",\n    flex=1,\n    tooltipField=\"powerPlant\",\n    rowGroup=True if shouldDisplayPivoted else False, # enable row grouping IF pivot mode is on. Could be shortened as rowgroup=shouldDisplayPivoted\n)\n\n\nRepeat this for the other grouping columns.\n\nTo configure the column that displays group hierarchy, set the following grid options:\n\ngb.configure_grid_options(\n    autoGroupColumnDef=dict(\n        minWidth=300, \n        pinned=\"left\", \n        cellRendererParams=dict(suppressCount=True)\n    )\n)\n\n\nHere is the complete code for this app:\n\nimport streamlit as st\nimport pandas as pd\nfrom st_aggrid import AgGrid, GridOptionsBuilder\n\n@st.cache_data()\ndef load_data():\n    data = pd.read_csv(\"./data.csv\", parse_dates=[\"referenceDate\"])\n    return data\n\ndata = load_data()\n\nshouldDisplayPivoted = st.checkbox(\"Pivot data on Reference Date\")\n\ngb = GridOptionsBuilder()\n\ngb.configure_default_column(\n    resizable=True,\n    filterable=True,\n    sortable=True,\n    editable=False,\n)\ngb.configure_column(\n    field=\"state\", header_name=\"State\", width=80, rowGroup=shouldDisplayPivoted\n)\n\ngb.configure_column(\n    field=\"powerPlant\",\n    header_name=\"Power Plant\",\n    flex=1,\n    tooltipField=\"powerPlant\",\n    rowGroup=True if shouldDisplayPivoted else False,\n)\ngb.configure_column(\n    field=\"recordType\",\n    header_name=\"Record Type\",\n    width=110,\n    rowGroup=shouldDisplayPivoted,\n)\n\ngb.configure_column(\n    field=\"buyer\",\n    header_name=\"Buyer\",\n    width=150,\n    tooltipField=\"buyer\",\n    rowGroup=shouldDisplayPivoted,\n)\n\ngb.configure_column(\n    field=\"referenceDate\",\n    header_name=\"Reference Date\",\n    width=100,\n    valueFormatter=\"value != undefined ? new Date(value).toLocaleString('en-US', {dateStyle:'medium'}): ''\",\n    pivot=False,\n)\n\ngb.configure_column(\n    field=\"virtualYear\",\n    header_name=\"Reference Date Year\",\n    valueGetter=\"new Date(data.referenceDate).getFullYear()\",\n    pivot=True,\n    hide=True,\n)\n\ngb.configure_column(\n    field=\"virtualMonth\",\n    header_name=\"Reference Date Month\",\n    valueGetter=\"new Date(data.referenceDate).toLocaleDateString('en-US',options={year:'numeric', month:'2-digit'})\",\n    pivot=True,\n    hide=True,\n)\n\ngb.configure_column(\n    field=\"hoursInMonth\",\n    header_name=\"Hours in Month\",\n    width=50,\n    type=[\"numericColumn\"],\n)\ngb.configure_column(\n    field=\"volumeMWh\",\n    header_name=\"Volume [MWh]\",\n    width=100,\n    type=[\"numericColumn\"],\n    aggFunc=\"sum\",\n    valueFormatter=\"value.toLocaleString()\",\n)\n\ngb.configure_grid_options(\n    tooltipShowDelay=0,\n    pivotMode=shouldDisplayPivoted,\n)\n\ngb.configure_grid_options(\n    autoGroupColumnDef=dict(\n        minWidth=300, \n        pinned=\"left\", \n        cellRendererParams=dict(suppressCount=True)\n    )\n)\ngo = gb.build()\n\nAgGrid(data, gridOptions=go, height=400)\n\nWrapping up\n\nAnd that‚Äôs it! You now know how to use streamlit-aggrid to create a nice PivotTable report. If you have any questions, please post them in the comments below or contact me on GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "10 most common explanations on the Streamlit forum",
    "url": "https://blog.streamlit.io/10-most-common-explanations-on-the-streamlit-forum/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n10 most common explanations on the Streamlit forum\n\nA guide for Streamlit beginners\n\nBy Debbie Matthews\nPosted in Advocate Posts, March 9 2023\nA guide for Streamlit beginners\n1. Buttons aren‚Äôt stateful\n2. Streamlit doesn‚Äôt render like a terminal\n3. You can inject your own CSS and JavaScript\n4. The files in your directory aren‚Äôt accessible to your front end implicitly\n5. file_uploader doesn't save a file to your directory\n6. Keys in session state go away when their associated widget isn‚Äôt rendered\n7. Your local environment is not the same as your cloud environment\n8. Streamlit doesn‚Äôt do that natively, but‚Ä¶\n9. This isn't an issue with Streamlit\n10. Can you please provide more information?\nWrapping up\nContents\nShare this post\n‚Üê All posts\nA guide for Streamlit beginners\n\nHey, community! üëã\n\nMy name is Debbie Matthews, and I‚Äôm a moderator on Streamlit‚Äôs wonderful forum. You may have seen me around as mathcatsand, as in ‚Äúmath, cats, and‚Ä¶.‚Äù\n\nIf you hang around the forum long enough, you‚Äôll start seeing some common pain points and areas of confusion. I thought it‚Äôd be helpful for new users to know where many people trip as they get started with Streamlit.\n\nIn this post, I‚Äôll talk about 10 of them:\n\nButtons aren‚Äôt stateful.\nStreamlit doesn‚Äôt render like a terminal.\nYou can inject your own CSS and JavaScript.\nThe files in your directory aren‚Äôt accessible to your front end implicitly.\nfile_uploader doesn‚Äôt save a file to your directory.\nKeys in the session state go away when their associated widget is not rendered.\nYour local environment is not the same as your cloud environment.\nStreamlit doesn‚Äôt do that natively, but‚Ä¶\nThis isn‚Äôt an issue with Streamlit.\nCan you please provide more information?\nüí°\nCode snippets for this post are hosted in a live app here, so feel free to open another tab or window to follow along. And if you haven't read or watched a basic introduction about getting started with Streamlit, check it out here.¬†\n1. Buttons aren‚Äôt stateful\n\nButtons return True only on the page load right after their click and immediately go back to False.\n\nIf you create an if statement to check the value of a button, the body of the if statement will execute once per click of the button. The right things to include here are short messages or processes you don‚Äôt want to rerun with other user activity.\n\nimport streamlit as st\n\nif st.button('Submit'):\n    st.write('Submitted!')\n\nif st.button('Confirm'):\n    st.write('Confirmed!')\n\n\nIf you nest buttons, the innermost portion of the code will never execute! As soon as you click on the second button, the page will reload with the first button being False.\n\nimport streamlit as st\n\nif st.button('First Button'):\n    st.write('The first button was clicked.')\n    if st.button('Second Button'):\n        # This will never execute!\n        st.write('The second button was clicked')\n\n\nIf you need your button to behave more like a checkbox, you can create a key in the session state to save that information.\n\nimport streamlit as st\n\n# Initialize the key in session state\nif 'clicked' not in st.session_state:\n    st.session_state.clicked = {1:False,2:False}\n\n# Function to update the value in session state\ndef clicked(button):\n    st.session_state.clicked[button] = True\n\n# Button with callback function\nst.button('First Button', on_click=clicked, args=[1])\n\n# Conditional based on value in session state, not the output\nif st.session_state.clicked[1]:\n    st.write('The first button was clicked.')\n    st.button('Second Button', on_click=clicked, args=[2])\n    if st.session_state.clicked[2]:\n        st.write('The second button was clicked')\n\nüí°\nCheck out streamlit-extras which is a collection of many useful custom components from a variety of contributors. It includes a stateful button made by Zachary Blackwood.\n2. Streamlit doesn‚Äôt render like a terminal\n\nWith every interaction on a page, Streamlit will reload the page. It‚Äôs not meant to wait for input and then proceed. It also won‚Äôt keep anything on the screen that isn‚Äôt explicitly re-rendered.\n\nBe careful with loops and conditionals! You don‚Äôt want a while loop waiting for a user to input something. Streamlit doesn‚Äôt pause and wait for input with a new widget; it just plows through with its default value. If you have a widget inside a loop, Streamlit will try to make a new, additional widget with each loop pass. If you need to wait for a user‚Äôs selection, place a conditional on the output to check if it has the default value.\n\nimport streamlit as st\n\nname = st.text_input('Name:')\nif name != '':\n    st.write(f'Hi, {name}! Nice to meet you.')\n\n\nIf you need to confirm the user‚Äôs selection which may be the default, you can add a confirmation button. You can require confirmation for any selection or just as a way for the user to accept the default.\n\nimport streamlit as st\n\n# Create a key in session state to record the user's choice, defaulting to None\nif 'favorite_color' not in st.session_state:\n    st.session_state.favorite_color = None\n\n# Confirmation function to record the user's choice into the favorite_color key\ndef confirm_color():\n    st.session_state.favorite_color = st.session_state.color_picker\n\nname = st.text_input('Name:')\nif name != '':\n    st.write(f'Hi, {name}! Nice to meet you.')\n    st.write(f'What\\\\'s your favorite color?')\n    # Confirmation function will run if the user changes the widget\n    color = st.color_picker('Color:', key='color_picker', on_change=confirm_color)\n    if st.session_state.favorite_color is None:\n        # Or, Confirmation function will run if user confirms the default\n        st.button('Confirm Black', on_click=confirm_color)\n    else:\n        st.write(f'<span style=\"color:{color}\">Oh, nice color choice!</span>', \n            unsafe_allow_html=True)\n\n\nIf you have many interactive steps to display, the nested if statements can get a bit out of control. You can instead create a staging value in the session state to control how much is displayed on the page. You can use inequality as in the example below to show all former stages. Alternatively, you can use equality or elif to show only the current stage.\n\nimport streamlit as st\n\n# Create a key in session state to track the stage\nif 'stage' not in st.session_state:\n    st.session_state.stage = 0\n\n# Stage function to update the stage saved in session state\ndef set_stage(stage):\n    st.session_state.stage = stage\n\nst.write('Welcome! Click to begin.')\n# Each button runs the Stage function, passing the stage number as an argument\nst.button('Begin', on_click=set_stage, args=[1])\n\n# Content for each stage within the body of an if statement\nif st.session_state.stage > 0:\n    st.write('This is stage 1. Do some things.') \n    st.button('Next', on_click=set_stage, args=[2])\nif st.session_state.stage > 1:\n    st.write('This is stage 2. Do some more things.')\n    st.button('Finish', on_click=set_stage, args=[3])\nif st.session_state.stage > 2:\n    st.write('This is the end. Thank you!')\n    st.button('Reset', on_click=set_stage, args=[0])\n\n\nIf you want a function that ‚Äúadds data‚Äù with each click, you will need something in the session state that accumulates those additions. This is commonly done with if 'key' not in st.session_state: at the top of the script. This way, the ‚Äúnew‚Äù unmodified object is initialized only on the first load of the page. With each addition, the object doesn't get overwritten with its default value because the key already exists.\n\nimport streamlit as st\nimport pandas as pd\n\n# Initialize some object in session state where you will you be storing edits\nif 'df' not in st.session_state:\n    st.session_state.df = pd.DataFrame({'A':[1,2,3],'B':[4,5,6],'C':[7,8,9]})\n\n# Optional: Assign the stored value to a convenient variable for brevity in code\ndf = st.session_state.df\n\nst.dataframe(df)\n\ncols = st.columns(3)\ncols[0].number_input('A',0,100,step=1, key='A')\ncols[1].number_input('B',0,100,step=1, key='B')\ncols[2].number_input('C',0,100,step=1, key='C')\n\ndef add_row():\n    row = [st.session_state.A, st.session_state.B, st.session_state.C]\n    next_row = len(st.session_state.df)\n    # Make sure modifcation is performed on the object in session state\n    st.session_state.df.loc[next_row] = row\n\nst.button('Add Row', on_click=add_row)\n\n3. You can inject your own CSS and JavaScript\n\nHTML and CSS can be added via st.write and st.markdown (with the correct optional keyword). JavaScript requires the more robust components submodule.\n\nMany different resources describe ways to modify the display of Streamlit. Fanilo Andrianasolo, another Streamlit Creator, has a short video explaining the basics. Here are a few examples.\n\nWant to change the font color on your buttons, including hover and focus colors? Here's how:\n\nimport streamlit as st\n\nst.button('Click me!')\n\ncss='''\n<style>\n    .stButton > button {\n        color: red;\n    }\n    .stButton > button:hover {\n        color: violet;\n        border-color: violet;\n    }\n    .stButton > button:focus {\n        color: purple !important;\n        border-color: purple !important;\n        box-shadow: purple 0 0 0 .2rem;\n    }\n</style>\n'''\n\nst.markdown(css, unsafe_allow_html=True)\n\n\nNote the use of unsafe_allow_html=True when using st.markdown or st.write. This optional keyword is needed to prevent Streamlit from escaping HTML tags. If you know your CSS selectors, you can get to any element. I often use a set of containers combined with nth-of-type selections to get to a specific instance of an element.\n\nimport streamlit as st\n\n# Layout your containers at the beginning\nsection1 = st.container()\nsection2 = st.container()\nsection3 = st.container()\nsection4 = st.container()\n\n# Write to the different containers for your display elements\nsection1.subheader('Section 1')\nsection1.button('Button 1')\n\nsection2.subheader('Section 2')\nsection2.button('Button 2')\n\nsection3.subheader('Section 3')\nsection3.button('Button 3')\n\nsection4.subheader('Section 4')\nsection4.button('Button 4')\n\ncss='''\n<style>\n    section.main > div > div > div > div:nth-of-type(3) .stButton > button {\n        color: green;\n    }\n    section.main > div > div > div > div:nth-of-type(3) .stButton > button:hover {\n        color: violet;\n        border-color: violet;\n    }\n    section.main > div > div > div > div:nth-of-type(3) .stButton > button:focus {\n        color: purple !important;\n        border-color: purple !important;\n        box-shadow: purple 0 0 0 .2rem;\n    }\n</style>\n'''\n\nst.markdown(css, unsafe_allow_html=True)\n\n\nUse the components submodule if you need to customize something that can‚Äôt be handled with pure CSS. When you insert a component, it will be contained in an iframe. Be aware that your JavaScript queries must reach outside that iframe to work as expected.\n\nimport streamlit as st\n\nst.header('Screen Width Checker')\nst.write('''<h3>The app container is <span id=\"root-width\"></span> x \n<span id=\"root-height\"></span> px.</h3>\n''', unsafe_allow_html=True)\n\njs = '''\n<script>\n    var container = window.parent.document.getElementById(\"root\")\n\n    var width = window.parent.document.getElementById(\"root-width\")\n    var height = window.parent.document.getElementById(\"root-height\")\n\n    function update_sizing(){\n        width.textContent = container.getBoundingClientRect()['width']\n        height.textContent = window.parent.innerHeight\n    }\n    update_sizing()\n\n    window.parent.addEventListener('resize', function(event) {\n        update_sizing()\n    }, true);\n    \n</script>\n'''\n\nst.components.v1.html(js)\n\n4. The files in your directory aren‚Äôt accessible to your front end implicitly\n\nUsers can't directly select from files on your app‚Äôs server. You can‚Äôt access files like you would on a web host.\n\nStreamlit has a server-client structure. The files that a user can access are on their computer where they have a browser open. Streamlit will only give users access to the files you explicitly tell it to serve. If you have an image my_image.png saved in your working directory, that image cannot be accessed via <app url>/my_image.png.\n\nWhen you use st.image in your app, Streamlit will create a copy of the data and make it accessible to the client's browser via a hashed file name. When using HTML or CSS in your app that contains a path to some file, you need to host that file somewhere. A file will not be accessible to the web just by being in your app directory.\n\nIn the case of HTML and CSS, you can open and read the contents of a file to inject its contents manually. The contents of your CSS file should not contain relative paths to other HTML, CSS, or image files, as these will not be accessible to the user‚Äôs client.\n\nimport streamlit as st\n\nif 'css' not in st.session_state:\n    with open('files/my_css.css', 'r') as file:\n        css = file.read()\n    st.session_state.css = css\n\ncss = '<style>' + st.session_state.css + '</style>'\n\nst.button('Click me!')\n\nst.markdown(css, unsafe_allow_html=True)\n\n\nThere is also a new, exciting feature in Streamlit 1.18.0: static files! If you want to make anything in your working directory web-accessible, you can use this, too. Let's say you have a background image that you want to specify in some CSS. If you turn on static hosting and put the background image in a folder named static, you can use it in your CSS. Be sure to read the linked documentation for clarification.\n\nimport streamlit as st\n\nimage = './app/static/cat_background.jpg'\n\ncss = f'''\n<style>\n    .stApp {{\n        background-image: url({image});\n    }}\n    .stApp > header {{\n        background-color: transparent;\n    }}\n</style>\n'''\nst.markdown(css, unsafe_allow_html=True)\n\n\nYour config.toml should contain:\n\n[server]\nenableStaticServing = true\n\n\nRemember to reboot your app any time you change your environment or configuration! Read more about the configuration here.\n\n5. file_uploader doesn't save a file to your directory\n\nThe file_uploader widget returns a ‚Äúfile-like object,‚Äù the file's data. This object is not accessed via a name or path.\n\nYou may be familiar with a typical use case of file_uploader:\n\nimport streamlit as st\nimport pandas as pd\n\nfile = st.file_uploader(\"Choose a file:\", key=\"loader\", type='csv')\n\nif file != None:\n    df = pd.read_csv(file)\n    st.write(df)\n\n\nSince it is very common to specify a data file to read_csv via its path, it is easy to forget that pandas accepts either a path or a buffer. In the above example, we are passing the latter. The variable file has no ‚Äúpath‚Äù associated with it. You can access the file‚Äôs name via the name property inherited from BytesIO, but this is just informational. You don't use the file's name to point to its data. There are many libraries and functions that will not accept a file-like object instead of a path. Be mindful of the function you are using and always read its documentation if in doubt.\n\nAlso note that the file-like object you get requires processing to be interpreted, even if it is a simple text file.\n\nimport streamlit as st\nimport io\n\nfile = st.file_uploader(\"Choose a file:\", type=['css','py'])\n\nif file != None:\n    bytes_object = file.getvalue()\n    string_object = bytes_object.decode(\"utf-8\")\n\n    st.code(string_object)\n\n6. Keys in session state go away when their associated widget isn‚Äôt rendered\n\nWhen a key in the session state is associated with a widget, then the key will be removed from the session state when the widget is no longer rendered. This can happen if you navigate to a different page or conditionally render widgets on the same page.\n\nHere‚Äôs a brief description of a widget‚Äôs life cycle.\n\nAt the specific line where you call upon a widget for the first time, Streamlit will create a new front-end instance of that widget. If you have specified a key, Streamlit will check if that key already exists in the session state. If that key doesn't exist, Streamlit will create one, starting with whatever default value the widget has. However, if Streamlit sees a key already, it will attach the widget to it. In this case, the widget will take on that key‚Äôs value even if it is a new widget with a specified initial value.\n\nExample: This slider will always have a value of 1 since the widget will always attach itself to the pre-existing key.\n\nimport streamlit as st\n\nst.session_state.my_key = 1\n\nst.slider('Test', 0, 10, key='my_key')\n\n\nAlthough you can edit a widget‚Äôs state by assigning different values to its key in the session state, the session state is just an intermediary. The widget will have and retain state while continually rendered on screen, even if you remove its key from the session state.\n\nFor example, this widget is and will remain stateful:\n\nimport streamlit as st\n\nst.session_state.clear()\n\nst.slider('Test', 0, 10, key='my_key')\n\n\nHowever, as soon as a widget isn‚Äôt rendered (even for a single page load), Streamlit will delete all its data, including any associated key in the session state:\n\nimport streamlit as st\n\nswitch = st.radio('Choice:', [1,2])\n\nmatch switch:\n    case 1:\n        st.checkbox('1', key='1')\n    case 2:\n        st.checkbox('2', key='2')\n\nIn the above example, say that 1 is selected for the radio button. While that is the case, there will be a '1' in session state duplicating the widget‚Äôs state.\nAs soon as a user selects 2 for the radio button, the page will reload. As Streamlit reruns the page, it will still have the '1' key in the session state. It doesn't know that the key's associated widget will not be rendered.\nHowever, as soon as Streamlit completes rendering the page, it will see that it has information for a widget that isn't rendered. At this point, Streamlit will delete the widget information, including any key in the session state that was tied to it.\n\nThis cleanup process has particular importance to conditionally rendered widgets. It is also important for widgets meant to carry over to other pages (often in the sidebar). There is a discussion about changing this behavior and potentially changing the structure on a deeper level. For now, know that when a key is assigned to a widget, the data in the session will get deleted if you navigate away from that instance of the widget.\n\nTwo ways around this:\n\nCopy data into a new key in the session state to have a place in the session state that is free from unintentional deletion.\nRecommit your data to the session state at the top of the page. By using st.session_state.my_key = st.session_state.my_key at the top of every page, you can artificially ‚Äúkeep it alive.‚Äù When navigating away from a widget with key='my_key', this interrupts the cleanup process. This manual value assignment effectively detaches the key from the widget (until a widget is seen again with that key).\n7. Your local environment is not the same as your cloud environment\n\nMake sure to specify/use the right environment for any deployment and be sure your file paths are OS-agnostic.\n\nWhen you deploy your app to some cloud service, a new Python environment within that cloud service will be used to run your app. It won't know about or use anything you happen to have in your local environment. You have to tell your cloud environment about all the Python packages it has to install and any additional non-Python components.\n\nFor Streamlit Cloud, the most common approach is to save a requirements.txt file at the top of your working directory. Each line in the requirements.txt file specifies a package for the cloud environment to pip install. You can also set specific versions of Python packages this way.\n\nExample requirements.txt file:\n\nstreamlit==1.17.0\npandas\nnumpy\n\n\nSome Python libraries require additional command line tools or software to be installed. Streamlit Cloud is a Debian-based Linux container. Extra software is installed with apt-get in a similar way to how pip installs Python packages. You need a packages.txt file in your working directory alongside your main Python file for your app. Each line in the packages.txt file specifies a binary for the cloud environment to sudo apt-get install.\n\nExample packages.txt file:\n\nffmpeg\nchromium\nchromium-chromedriver\n\n\nSince many people locally have a different environment than Linux, note that Linux is case-sensitive. The files specifying your environment must be named exactly as stated, ¬†case included. Use forward slashes and not back slashes in your Python script when writing out file paths. Ensure all paths are given from the working directory, even for Python files in your pages folder for multipage apps.\n\nThere are other ways to specify your Python packages, as described in the documentation. For example, you can have an environment.yml file to use conda instead of using requiremnts.txt which uses pip. If you try to include both, Streamlit Cloud will only process the first one it comes across and ignore the second.\n\nFor deployment on Streamlit Cloud, here‚Äôs a related warning. If you write to a file in your script, that updated file will only live in the Debian container hosting your app. It will not save back to GitHub and will not survive a reboot of your app.\n\n8. Streamlit doesn‚Äôt do that natively, but‚Ä¶\n\nStreamlit is constantly growing and improving, so keep your eye on the road map and note the most commonly used custom components.\n\nThere are a few good places to keep your eyes open for what‚Äôs coming up to help you get a feel for where we are now. Keep an eye on the Roadmap to know what‚Äôs just around the corner in development. GitHub Issues is the official place for developers to keep track of feature requests. If you want Streamlit to do something new, check there first so you can up-vote any existing request or create a new one if no one has mentioned it yet. I like to sort the list by the most upvotes to see what‚Äôs getting traction.\n\nCheck out the Streamlit Component Community Tracker for extra features people have built. Here are a few notable packages:\n\nstreamlit-extras: a collection of many different small components from many different contributors\nst-pages: settings to customize the layout of navigation in multi-page apps created by Zachary Blackwood\nstreamlit-webrtc: tools for real-time video/audio streams created by Yuichiro Tachibana\nüí°\nIf your application is hosted on a different computer (server) than a user‚Äôs computer (client), be careful about computer peripherals. Use Streamlit-compatible libraries. There are a lot of components to deal with audio/video input for this reason.\nstreamlit-folium by Randy Zwitch brings Folium to Streamlit.\nstreamlit-aggrid by Pablo Fonseca brings AG Grid to Streamlit.\nüí°\nNote that Streamlit 1.18.0 introduced an experimental editable data frame as well.\n9. This isn't an issue with Streamlit\n\nIf the problematic lines of code don't include anything from the Streamlit library, think carefully. Ask yourself if you need help with Streamlit or with something else.\n\nCommunity members are very generous about helping out with non-Streamlit issues. However, it's best to direct your questions to the right venue. There are many useful forums on the internet with different areas of focus. You will get the best and fastest help by asking questions in a forum dedicated to your issue.\n\nThe only thing Streamlit does is provide a front end to your Python code. If you're having trouble creating a data frame from a CSV file in your working directory, you may have a pandas question rather than a Streamlit one. The most efficient path to an answer would be to seek a forum dedicated to pandas.\n\nWhen you encounter difficulty with a line of code, check if any Streamlit component is involved. If not, I encourage you to try executing that bit of code without Streamlit. If appropriate, you can create and run a plain Python script or try it out in a Jupyter notebook. If something works fine in a Jupyter notebook but isn't behaving as you expect in Streamlit, that's a great question to bring to the Streamlit forum.\n\n10. Can you please provide more information?\n\nIf you invest the time to ask your question clearly and succinctly, you'll likely save as much or more time waiting for a response.\n\nThe easier it is for community members to understand your problem, the faster you'll get a response. If you're having problems deploying, we'd like to see your GitHub repository. We want to understand how you configured your environment and check for typos. We'd like to see your terminal output from a fresh reboot to see any error messages. On the other hand, if the front end isn't displaying the way you want, we'd like to know the code you are using and a screenshot of what you are visually seeing. Explain how you expect it to look.\n\nScreenshots of code are less helpful since we can't copy and paste those into a working snippet. Access to your complete GitHub repository can be beneficial and sometimes necessary. However, the smallest amount of code needed to reproduce the issue is always best. If we can copy-paste a snippet you provide and launch it to see your problem, that's perfect! Include your import statements as well as any files your script accesses. Provide simplified, dummy data for us to work with. Inline data is the easiest to work with, such as defining a simple data frame within your code snippet. If importing your data is part of the problem, we'd need an example data file to accompany your snippet.\n\nIf you spend the time creating a small, self-contained example of your problem, then the community can work on helping you. Otherwise, we spend a lot of time digging through your code, fabricating data, and making all sorts of guesses to fill in the gaps.\n\nPlease check out this guide on how to post effectively on the forum. I would especially like to draw your attention to the idea of a minimal, reproducible example.\n\nWrapping up\n\nThanks for reading! I hope you found some useful information that will save you time and trouble as you start with Streamlit. If you have any questions, please post them in the comments below or contact me on the Streamlit Forum. You can also find me on GitHub and LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Hackathon 101: 5 simple tips for beginners",
    "url": "https://blog.streamlit.io/hackathon-101-5-simple-tips-for-beginners/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHackathon 101: 5 simple tips for beginners\n\nPrepare to win your first hackathon!\n\nBy Chanin Nantasenamat\nPosted in Tutorials, March 16 2023\nWhat is a hackathon?\nWhat are the benefits of participating in a hackathon?\nHow to get started with hackathons\n5 simple tips for a successful hackathon\nTip 1. Set up your coding environment\nTip 2. Use code templates\nTip 3. Focus on the problem\nTip 4. Get unstuck\nTip 5. Create your own custom resource pack\nBonus tip: Have fun!\nMy recent hackathon experience\n1. Create a solution to a problem\n2. Make an app feature wishlist\n3. Implement the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHave you heard about hackathons but have no idea where to start? I‚Äôve been there. It can feel intimidating. But fear not!\n\nIn this post, I‚Äôll outline five simple hackathon tips that will help you win your first hackathon, and I‚Äôll also share my personal hackathon experience:\n\nWhat is a hackathon?\nWhat are the benefits of participating in a hackathon?\nHow to get started with hackathons\n5 simple tips for a successful hackathon\nMy recent hackathon experience\nWhat is a hackathon?\n\nA hackathon is an event where people come together to work on creative software or hardware projects. Hackathons usually have a specific focus‚Äîsuch as building games, websites, or apps. Participants often have to complete their projects within a certain timeframe.\n\nWhat are the benefits of participating in a hackathon?\n\nHackathons are a great way to learn new skills, meet new people, and build something cool. Even if you don't win, participating in a hackathon can be a fun and rewarding experience.\n\nHow to get started with hackathons\n\nTo start, find a hackathon that interests you. There are lots of them, so it's important you find one that matches your skills and interests. Found one? Great! Register for it and start preparing:\n\nRead the hackathon website to get a good grasp of the scope. It‚Äôll help you frame your project.\nBrainstorm and make a list of potential projects you could work on.\nDecide whether you want to use an existing skill set or to learn a new one (and apply it to your project). Note that this depends on your available time.\nChoose a project and get started!\n5 simple tips for a successful hackathon\n\nThere's no one-size-fits-all answer to the question of how to win a hackathon. But it‚Äôs important that you come prepared. That means, having a clear idea of what you want to accomplish (and how to get there) and having all the necessary tools and resources to hit the ground running.\n\nHere are a few tips:\n\nTip 1. Set up your coding environment\n\nCreate your hackathon coding environment. Make sure it has the necessary operating system, language framework, and required libraries. You‚Äôll be able to reproduce it and prevent the possible corruption of your computer setup.\n\nChoose from the following:\n\nVagrant\nDocker\nPython: conda or venv\n\nOr choose one of the preconfigured cloud coding environments:\n\nGitHub Codespaces\nGitpod\nTip 2. Use code templates\n\nBoilerplate code can help you get a working solution faster.\n\nFor example, if you‚Äôre creating Streamlit web apps, you can leverage the Streamlit App Starter Kit to have a functional app in seconds. You can also customize the app by adding functions, data, and widgets (learn more here). Or you can use the TrainGenerator to make the starter code for machine learning projects (also in seconds!).\n\nTip 3. Focus on the problem\n\nAnother key to success is focus. It's easy to get caught up in the excitement of a hackathon and try to do too much. If you want to succeed, focus on solving the problem. Remember to stay within the time limit and don‚Äôt worry about the details. A finished project is better than a perfect one!\n\nTip 4. Get unstuck\n\nGoogle, StackOverflow, and coding forums are great resources for getting unstuck. Follow the routine of copying and pasting code errors into search to find solutions to your errors. If there are none, ask for help (read this guide to writing an effective post).\n\nTip 5. Create your own custom resource pack\n\nPotential hackathon resources vary depending on the scope or topic.\n\nFor example, before participating in a data hackathon, you‚Äôll need to come up with a list of potential APIs or datasets. Thinking about data will help you figure out how to use it and what analysis approach to apply to a visualization.\n\nBonus tip: Have fun!\n\nHackathons are meant to be a fun experience. Even if you don‚Äôt win, you‚Äôll have a good time and create something you're proud of. And that‚Äôs a huge success worth celebrating! ü•≥\n\nMy recent hackathon experience\n\nI'd like to share with you a project that I‚Äôve worked on at Streamlit‚Äôs recent Hackathon.\n\nI followed three simple steps:\n\n1. Create a solution to a problem\n\nProblem: I wanted to share my social links on social media platforms but I didn‚Äôt want to share a long list of links. Solution: I created an app that stores all of my links in one link: https://chanin.streamlitapp.com/.\n\n2. Make an app feature wishlist\n\nI illustrated/summarized the four components of my app‚Äôs prototype in the diagram and the table below:\n\n3. Implement the app\n\nOnce the app copy was up and running, I polished it, then refactored and lightly documented the code.\n\nWrapping up\n\nParticipating in a hackathon is amazing. You get to learn new skills, meet new people, and create something useful and inspiring. I hope you‚Äôll attend your first hackathon soon!\n\nIf you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy hacking. üòÅ\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Building an Instagram hashtag generation app with Streamlit",
    "url": "https://blog.streamlit.io/building-an-instagram-hashtag-generation-app-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding an Instagram hashtag generation app with Streamlit\n\n5 simple steps on how to build it\n\nBy William Mattingly\nPosted in Advocate Posts, March 29 2023\nStep 1. How to scrape data from Instagram and a site that contains data about hashtag relationships\nStep 2. How to clean and structure the output from web scraping\nStep 3. How to display that data visually\nStep 4. How to create dynamic components with custom keys\nStep 5. How to display your output in Streamlit\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Dr. William Mattingly, and I'm a postdoc at the Smithsonian Institution's Data Science Lab. I work primarily with applying machine learning (ML) and natural language processing (NLP) to humanities data and museum archival records.\n\nI wanted to build an app for my social media accounts‚Äîand to learn more about web scraping, data visualization, and data storage. Generating hashtags from an Instagram hashtag collection and analyzing the output counts sounded like fun, so I built an Instagram Hashtag Generation App!\n\nIn this post, you'll learn:\n\nStep 1. How to scrape data from Instagram and a site that contains data about hashtag relationships\n\nStep 2. How to clean and structure the output from web scraping\n\nStep 3. How to display that data visually\n\nStep 4. How to create dynamic components with custom keys\n\nStep 5. How to display your output within Streamlit\n\nüëâ\nWant to jump right in? Here's the app and the repo.\nStep 1. How to scrape data from Instagram and a site that contains data about hashtag relationships\n\nLet's start by gathering two key pieces of data about Instagram hashtags:\n\nHow many times has a hashtag been used\nWhich hashtags are frequently used alongside a given hashtag\n\nYou'll need to have some functions for making requests to Instagram and an Instagram-related site called best-hashtag.com (you'll use requests and BeautifulSoup instead of Selinium to keep it simple and make integration on other platforms like Streamlit Community Cloud much easier):\n\ndef get_count(tag):\n\t\"\"\"\n\tThis function takes a hashtag as an input and returns the approx. times it has been used\n\ton Instagram.\n\t\"\"\"\n  url = f\"<https://www.instagram.com/explore/tags/{tag}>\"\n  s = requests.get(url)\n  soup = BeautifulSoup(s.content)\n  return int(soup.find_all(\"meta\")[6][\"content\"].split(\" \")[0].replace(\"K\", \"000\").replace(\"B\", \"000000000\").replace(\"M\", \"000000\").replace(\".\", \"\"))\n\ndef get_best(tag, topn):\n\t\"\"\"\n\tThis function takes two arguments, a hashtag and topn.\n\tTopn is the number of similar hashhtags you wish to find.\n\tThis allows you to cultivate a set of 30-hashtags quickly.\n\t\"\"\"\n  url = f\"<https://best-hashtags.com/hashtag/{tag}/>\"\n  s = requests.get(url)\n  soup = BeautifulSoup(s.content)\n  tags = soup.find(\"div\", {\"class\": \"tag-box tag-box-v3 margin-bottom-40\"}).text.split()[:topn]\n  tags = [tag for tag in tags]\n  return tags\n\n\nYou'll store user data in a JSON file to avoid unnecessary repeat requests. Let's set up your JSON database using a fairly standard function called load_data() (you won't be using Streamlit's cache feature because you'll be updating this information regularly):\n\ndef load_data():\n    with open(\"database.json\", \"r\") as f:\n        data = json.load(f)\n    return data\n\nStep 2. How to clean and structure the output from web scraping\n\nNow that you have developed your basic web scraping functions start building your app. First, import all requisite libraries:\n\nimport streamlit as st\n\n# scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\n#data\nimport json\nimport pandas as pd\n\n# plotting\nimport plotly.express as px\nimport seaborn as sns\n\n\nNext, load up your data by calling the load_data() function:\n\ndata = load_data()\n\nStep 3. How to display that data visually\n\nOnce the data is loaded, start designing your app.\n\nYou want to let the user dynamically create multiple tags to automatically populate a collection of hashtags. They should be able to enter anywhere from 1 to 30 hashtags (30 is the maximum number allowed on Instagram). You can do this using Streamlit number_input() class:\n\nnum_tags = st.sidebar.number_input(\"Select number of tags\", 1, 30)\n\nStep 4. How to create dynamic components with custom keys\n\nSince you want the tag inputs to be dynamically loaded, you'll need to create them dynamically. You can do this in a for loop and assign a unique key to each text input.\n\nYou also want the user to tell you how many similar hashtags to generate. To do this, you'll also need to create number inputs dynamically. Each of these will be appended to separate lists called tags and sizes:\n\nst.sidebar.header(\"Tags\")\ncol1, col2 = st.sidebar.columns(2)\n\ntags = []\nsizes = []\nfor i in range(num_tags):\n    tag = col1.text_input(f\"Tag {i}\", key=f\"tag_{i}\")\n    size = col2.number_input(f\"Top-N {i}\", 1, 10, key=f\"size_{i}\")\n    tags.append(tag)\n    sizes.append(size)\n\nStep 5. How to display your output in Streamlit\n\nOnce the user has provided an input, it's time to use that input to do something.\n\nFor your app, you want to use hashtags to identify the number of times it's been used on Instagram and the common hashtags associated with it. When a button is clicked, you want that event to be triggered‚Äîdo that with a conditional statement.\n\nHere is the code in its entirety:\n\n#only execute if the `Create Hashtags` button is pressed\nif st.sidebar.button(\"Create Hashtags\"):\n\t\t#create a list of tab names that begin with `all`\n    tab_names = [\"all\"]\n    tab_names = tab_names+[tags[i] for i in range(num_tags)]\n\n\t\t#create our Streamlit tabs\n    tag_tabs = st.tabs(tab_names)\n\n\t\t#create lists to store our data outside of our loop\n    all_hashtags = []\n    hashtag_data = []\n\n\t\t#loop for the number of tags we have\n    for i in range(num_tags):\n        hashtags = get_best(tags[i], sizes[i])\n        for hashtag in hashtags:\n            if hashtag in data[\"hashtag_data\"]:\n                hashtag_count = data[\"hashtag_data\"][hashtag]\n            else:\n                hashtag_count = get_count(hashtag.replace(\"#\", \"\"))\n                data[\"hashtag_data\"][hashtag] = hashtag_count\n            hashtag_data.append((f\"{hashtag}<br>{hashtag_count:,}\", hashtag_count))\n\n\t\t    #We can use our integer, i, to populate the list of Streamlit tag objects.\n        tag_tabs[i+1].text_area(f\"Tags for {tags[i]}\", \" \".join(hashtags))\n        all_hashtags = all_hashtags+hashtags\n  \n    tag_tabs[0].text_area(\"All Hashtags\", \" \".join(all_hashtags))\n\n    st.header(\"Hashtag Count Data\")\n    df = pd.DataFrame(hashtag_data, columns=[\"hashtag\", \"count\"])\n    df = df.sort_values(\"count\")\n\n    with open(\"database.json\", \"w\") as f:\n        json.dump(data, f, indent=4)\n    \n    fig = px.bar(df, x='hashtag', y='count')\n    st.plotly_chart(fig, use_container_width=True)\n\nWrapping up\n\nThank you for reading my post! You have learned how to use Streamlit to perform web scraping, make dynamic inputs with unique keys, and display your output nicely.\n\nIf a tutorial video is your thing, check out the video below:\n\nAnd if you have any questions, please leave them in the comments below or contact me on Twitter.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "star-history-2023525-1.png (2000√ó1413)",
    "url": "https://blog.streamlit.io/content/images/2023/05/star-history-2023525-1.png",
    "html": ""
  },
  {
    "title": "blog-outline-generation-diagram.jpg (2000√ó966)",
    "url": "https://blog.streamlit.io/content/images/2023/06/blog-outline-generation-diagram.jpg",
    "html": ""
  },
  {
    "title": "st-status-transparency.gif (1108√ó888)",
    "url": "https://blog.streamlit.io/content/images/2023/09/st-status-transparency.gif",
    "html": ""
  },
  {
    "title": "notion-chatbot-embed.gif (758√ó416)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion-chatbot-embed.gif#browser",
    "html": ""
  },
  {
    "title": "image_6487327--2-.JPG.jpg (2000√ó1206)",
    "url": "https://blog.streamlit.io/content/images/2023/06/image_6487327--2-.JPG.jpg",
    "html": ""
  },
  {
    "title": "image_6487327--1-.JPG.jpg (2000√ó1028)",
    "url": "https://blog.streamlit.io/content/images/2023/06/image_6487327--1-.JPG.jpg",
    "html": ""
  },
  {
    "title": "Streamlit and Snowflake: better together",
    "url": "https://blog.streamlit.io/snowflake-to-acquire-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nBy Adrien Treuille, Thiago Teixeira and Amanda Kelly\nPosted in Product, March 2 2022\nShare this post\n‚Üê All posts\n\nDear Data Scientists, Machine Learning Engineers, and Pythonistas worldwide:\n\nWe created Streamlit to give you superpowers. Streamlit takes minutes to learn yet gives you immense power to transform Python scripts into beautiful apps. As Streamlit spread worldwide, we have worked with you to build a vibrant community with tens of thousands of developers and millions of viewers. The best part? Each step has clarified that we were onto something very special‚Äîa new paradigm to share and understand data. Today, we couldn‚Äôt be happier to announce the next step in that journey.\n\nStreamlit has been acquired by Snowflake to join forces and open new frontiers in data science and data application development.\n\nWhat convinced us to join? It was our interactions with Snowflake‚Äôs founders and engineers. It was love at first code review. As we drew boxes and imagined the future, we found ourselves excitedly finishing each other‚Äôs sentences. We were sketching the same vision.\n\nWorking within Snowflake, we realized, would be a win-win-win: for the data community, for Streamlit technology, and for Snowflake‚Äôs customers.\n\nStreamlit is being supercharged for all!\n\nEverything you know and love about Streamlit is moving onward and upward. Keep making awesome apps, teaching classes, presenting at meetups, running hackathons, showing off research, and all the great things you do every day with Streamlit. And now we can do even more together! Here are some of our plans:\n\nStreamlit and Snowflake are united in and committed to supporting the Streamlit open-source project and continuing to deliver an amazing set of features available to everyone. üåü\nThe Streamlit community is still our primary focus. You delight us every day. With Snowflake‚Äôs amazing footprint, we‚Äôre excited to engage even more actively with the community. üôãüèΩ\nStreamlit Cloud will remain an amazing place for the community to share their work, explore new ideas, and discover new paradigms. As Streamlit enables the data science and machine learning communities to share their work, we all win. ‚ù§Ô∏è\nWe'll work together with Snowflake to open up a vast array of new use cases and capabilities for Streamlit, coupling our app framework with Snowflake‚Äôs leading data platform. üöÄ\n\nWe‚Äôre beyond excited about what we can all build together. It feels like all our aims‚Äîto build a generational open-source project, to build an incredibly inclusive community of data nerds, to empower data scientists and machine learning engineers to share their work, to enable companies to harness and understand their data‚Äîare being supercharged by this collaboration.\n\nThank you, community, for everything we‚Äôve built together. New frontiers await.\n\nLet‚Äôs do this! üéà‚ùÑÔ∏è\n\nLove,\n\nAdrien, Thiago, Amanda, and everyone at Streamlit.\n\nP.S.: Streamlit 1.7.0 was released today. Try out st.snow üòâ!\n\n0:00\n/\n1√ó\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\nUsing ChatGPT to build a Kedro ML pipeline\n\nTalk with ChatGPT to build feature-rich solutions with a Streamlit frontend\n\nLLMs\nby\nArvindra Sehmi\n,\nFebruary 9 2023\nStreamlit-Authenticator, Part 2: Adding advanced features to your authentication component\n\nHow to add advanced functionality to your Streamlit app‚Äôs authentication component\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nFebruary 7 2023\nUsing Streamlit for semantic processing with semantha\n\nLearn how to integrate a semantic AI into Snowflake with Streamlit\n\nAdvocate Posts\nby\nSven Koerner\n,\nFebruary 2 2023\nHost your Streamlit app for free\n\nLearn how to transfer your apps from paid platforms to Streamlit Community Cloud\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 24 2023\nCreate a color palette from any image\n\nLearn how to come up with the perfect colors for your data visualization\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nJanuary 19 2023\nHow to make a culture map\n\nAnalyze multidimensional data with Steamlit!\n\nTutorials\nby\nMicha≈Ç Nowotka\n,\nJanuary 12 2023\nBuild an image background remover in Streamlit\n\nSkip the fees and do it for free! üéà\n\nTutorials\nby\nTyler Simons\n,\nJanuary 10 2023\nFind the top songs from your high school years with a Streamlit app\n\nUse the Spotify API to generate 1,000+ playlists!\n\nAdvocate Posts\nby\nRobert Ritz\n,\nDecember 8 2022\nStreamlit-Authenticator, Part 1: Adding an authentication component to your app\n\nHow to securely authenticate users into your Streamlit app\n\nAdvocate Posts\nby\nMohammad Khorasani\n,\nDecember 6 2022\nStreamlit Quests: Getting started with Streamlit\n\nThe guided path for learning Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nNovember 18 2022\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Pablo Fonseca - Streamlit",
    "url": "https://blog.streamlit.io/author/pablo/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Pablo Fonseca\n1 post\nBuilding a PivotTable report with Streamlit and AG Grid\n\nHow to build a PivotTable app in 4 simple steps\n\nAdvocate Posts\nby\nPablo Fonseca\n,\nMarch 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing multipage apps! üìÑ",
    "url": "https://blog.streamlit.io/introducing-multipage-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing multipage apps! üìÑ\n\nQuickly and easily add more pages to your Streamlit apps\n\nBy Vincent Donato\nPosted in Product, June 2 2022\nUsing multipage apps\nConverting an existing app into a multipage app\nTips and tricks\nBonus features: new dataframe UI, horizontal radio buttons, and more!\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nSo, you built a Streamlit app that became super useful, but then you got overloaded with feature requests. You kept adding more and more features until it felt too cluttered. You tried splitting the content across several pages by using st.radio or st.selectbox to choose which ‚Äúpage‚Äù to run.\n\nIt worked! But maintaining the code got harder. You were limited by the st.selectbox UI and couldn‚Äôt customize page titles with st.set_page_config or navigate between them via URLs. ü§Ø\n\nSound familiar?\n\nWe wanted to find a simple solution for this, so today, we‚Äôre excited to introduce‚Ä¶\n\nNative support for multipage apps!\n\nIn this post, we‚Äôll show you how to use this new feature and share tips and tricks on getting the most out of it.\n\nWant to jump right in? Update Streamlit to the newest version and see the streamlit hello demo app and repo for inspiration. Read more on how to get started in our docs.\n\nUsing multipage apps\n\nBuilding a multipage app is easy! Just follow these steps:\n\n1. Create a main script named streamlit_app.py.\n\n2. In the same folder, create a new pages folder.\n\n3. Add new .py files in the pages folder. Your filesystem will look like this:\n\nmy_app\n‚îú‚îÄ‚îÄ streamlit_app.py    <-- Your main script\n‚îî‚îÄ‚îÄ pages\n    ‚îú‚îÄ‚îÄ page_2.py       <-- New page 2!\n    ‚îî‚îÄ‚îÄ page_3.py       <-- New page 3!\n\n\n4. Run streamlit run streamlit_app.py as usual.\n\nThat‚Äôs it!\n\nThe streamlit_app.py script will now correspond to your app's main page. You‚Äôll see the other scripts from the pages folder in the sidebar page selector.\n\nConverting an existing app into a multipage app\n\nLet‚Äôs say you built a multipage app by using st.selectbox and want to convert it to the multipage app functionality. In your current app, the selectbox picks which page to display, and each ‚Äúpage‚Äù is written as a function.\n\nIf your folder name is ~/my_app , your code will look like this:\n\n# Contents of ~/my_app/streamlit_app.py\nimport streamlit as st\n\ndef main_page():\n    st.markdown(\"# Main page üéà\")\n    st.sidebar.markdown(\"# Main page üéà\")\n\ndef page2():\n    st.markdown(\"# Page 2 ‚ùÑÔ∏è\")\n    st.sidebar.markdown(\"# Page 2 ‚ùÑÔ∏è\")\n\ndef page3():\n    st.markdown(\"# Page 3 üéâ\")\n    st.sidebar.markdown(\"# Page 3 üéâ\")\n\npage_names_to_funcs = {\n    \"Main Page\": main_page,\n    \"Page 2\": page2,\n    \"Page 3\": page3,\n}\n\nselected_page = st.sidebar.selectbox(\"Select a page\", page_names_to_funcs.keys())\npage_names_to_funcs[selected_page]()\n\n\nTo convert your app to a multipage app, follow these steps:\n\n1. Upgrade Streamlit to the newest version: pip install --upgrade streamlit\n\n2. Add a new pages folder inside of ¬†~/my_app.\n\n3. Create three new files inside of ~/my_app :\n\nmain_page.py\npages/page_2.py\npages/page_3.py\n\n4. Move the contents of the main_page, page2, and page3 functions into their corresponding new files:\n\n# Contents of ~/my_app/main_page.py\nimport streamlit as st\n\nst.markdown(\"# Main page üéà\")\nst.sidebar.markdown(\"# Main page üéà\")\n\n# Contents of ~/my_app/pages/page_2.py\nimport streamlit as st\n\nst.markdown(\"# Page 2 ‚ùÑÔ∏è\")\nst.sidebar.markdown(\"# Page 2 ‚ùÑÔ∏è\")\n\n# Contents of ~/my_app/pages/page_3.py\nimport streamlit as st\n\nst.markdown(\"# Page 3 üéâ\")\nst.sidebar.markdown(\"# Page 3 üéâ\")\n\n\n5. Remove the original streamlit_app.py file.\n\n6. Run streamlit run main_page.py and view your shiny new multipage app!\n\nTips and tricks\n\nWe didn‚Äôt specify an order for pages 2 and 3, but they displayed correctly anyway. Why? ü§î ¬†Because they‚Äôre ordered alphabetically by default.\n\nBut what if you wanted to make this more clear?\n\nJust add numerical prefixes in front of the files in the pages/ folder and rename them pages/02_page_2.py and pages/03_page_3.py. The names won‚Äôt include these prefixes‚Äîthey‚Äôre used only for sorting.\n\nYou can also add emojis! ü•≥ ¬†Try renaming the script files to:\n\n01_üéà_main_page.py\npages/02_‚ùÑÔ∏è_page2.py\npages/03_üéâ_page3.py\nBonus features: new dataframe UI, horizontal radio buttons, and more!\n\nWant to make your multipage apps look even cooler? üòé\n\nGood news!\n\nWe launched more new features in Streamlit‚Äôs 1.10 release. Among them are the redesigned st.dataframe (based on glide-data-grid) and horizontal radio buttons. Check out the release notes for more info.\n\nWrapping up\n\nAnd that‚Äôs it for the intro to multipage apps! Adding more pages to your apps is now easier than ever. To start using multipage apps today, upgrade to the latest version of Streamlit:\n\npip install --upgrade streamlit\n\n\nHave any questions or want to share a cool app you made? Join us on the forum, tag us on Twitter, or let us know in the comments below. üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Debbie Matthews - Streamlit",
    "url": "https://blog.streamlit.io/author/mathcatsand/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Debbie Matthews\n1 post\n10 most common explanations on the Streamlit forum\n\nA guide for Streamlit beginners\n\nAdvocate Posts\nby\nDebbie Matthews\n,\nMarch 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Sebastian Flores Benner - Streamlit",
    "url": "https://blog.streamlit.io/author/sebastian/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Sebastian Flores Benner\n3 posts\nCreate a search engine with Streamlit and Google Sheets\n\nYou‚Äôre sitting on a goldmine of knowledge!\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nMarch 14 2023\nuPlanner fosters data processing innovation with Streamlit\n\nSebasti√°n Flores of uPlanner simplified the development, maintenance, and execution of Python scripts with a Streamlit app\n\nCase study\nby\nSebastian Flores Benner\n,\nOctober 6 2022\nHow to create interactive books with Streamlit in 5 steps\n\nUse streamlit_book library to create interactive books and presentations\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nJanuary 20 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "William Mattingly - Streamlit",
    "url": "https://blog.streamlit.io/author/william-mattingly/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by William Mattingly\nDr. William Mattingly is a Postdoc at the Smithsonian Institution in Washington D.C. He is also the host of the YouTube channel Python Tutorials for Digital Humanities\n1 post\nWebsite\nTwitter\nBuilding an Instagram hashtag generation app with Streamlit\n\n5 simple steps on how to build it\n\nAdvocate Posts\nby\nWilliam Mattingly\n,\nMarch 29 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Kanak Mittal - Streamlit",
    "url": "https://blog.streamlit.io/author/kanak/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Kanak Mittal\n1 post\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nAdvocate Posts\nby\nKanak Mittal\n,\nApril 11 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Dmitry Kosarevsky - Streamlit",
    "url": "https://blog.streamlit.io/author/dmitry/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Dmitry Kosarevsky\n1 post\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nAdvocate Posts\nby\nDmitry Kosarevsky\n,\nApril 18 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion-chatbot-deploy-streamlit.gif (751√ó365)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion-chatbot-deploy-streamlit.gif#browser",
    "html": ""
  },
  {
    "title": "Peter Vidos - Streamlit",
    "url": "https://blog.streamlit.io/author/peter/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Peter Vidos\n1 post\nWebsite\nCreate an animated data story with ipyvizzu and Streamlit\n\nA tutorial on using ipyvizzu and ipyvizzu-story\n\nAdvocate Posts\nby\nPeter Vidos\n,\nApril 20 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion-chatbot-export-content.gif (761√ó418)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion-chatbot-export-content.gif#browser",
    "html": ""
  },
  {
    "title": "Announcing the Figma-to-Streamlit plugin üé®",
    "url": "https://blog.streamlit.io/announcing-the-figma-to-streamlit-plugin/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnnouncing the Figma-to-Streamlit plugin üé®\n\nGo from prototype to code as easy as 1-2-3 with our new community resource!\n\nBy Juan Mart√≠n Garc√≠a\nPosted in Product, November 1 2022\nHow to install the plugin\nHow to use it\nHow to contribute to the plugin‚Äôs development\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHi Streamlit and Figma lovers! üëãüèª\n\nMy name is Juan, and I‚Äôm a designer and developer here at Streamlit. Remember last week we shared with you our Streamlit Design System in Figma? Today, I want to unveil‚Ä¶\n\nOur complementary Figma plugin!\n\nPrototype and code your apps easier than ever by turning components into code automatically, without leaving Figma. Pretty awesome, right?\n\nIn this post, you‚Äôll learn:\n\nHow to install the plugin\nHow to use it\nHow to contribute to the plugin‚Äôs development\n\nWant to check it out right away? Head on to our community profile and give it a try. Or strap yourself in and follow along with me!\n\nNOTE: This is an experiment from our design team. We‚Äôre releasing it early to get your feedback, so there‚Äôs still stuff missing. If you find it useful, please contribute!\nHow to install the plugin\n\nInstalling the plugin is super easy. Just follow these steps:\n\nGo to our Figma community profile, open the plugin, and hit Try it out.\nGo to Figma and run it from the Plugins tab.\nHow to use it\n\nAs mentioned above, this plugin is complementary to our Streamlit Design System. Drag and drop a component, tweak its props and values, hit See my code, and get a code snippet to use in your app!\n\nNeed help creating an app to test the code? Check out our docs on Getting started or use Yuichiro Tachibana‚Äôs amazing stlite sharing template. Just copy the plugin‚Äôs snippet, paste it into the code editor on the left, and click üíæ Save. The app on the right will update and show the generated output automagically!\n\nHere is what it‚Äôll look like:\n\n0:00\n/\n1√ó\n\nLast but not least, if you need a refresher on how to use our Design System library to prototype your app, make sure to check out last week‚Äôs post! And if you have any issues, check out our troubleshooting section on GitHub.\n\nNOTE: If you need a refresher on how to use our Design System to prototype your app, check out this post. And if you have any issues, read our troubleshooting section on GitHub.\nHow to contribute to the plugin‚Äôs development\n\nThis plugin is an experiment, which means we‚Äôre still developing it.\n\nAs of this writing, it supports:\n\nText elements (except for st.latex and st.markdown)\nInput widgets (except for st.select_slider)\nNative Chart elements (st.line_chart, st.bar_chart, and st.area_chart)\n\nIn the future, we plan to have it support:\n\nMore widgets: Media elements, Progress and Status, Data display elements, Layout and Containers, and Control flow.\nMore variants/features: recognizing bold, italic and strikethrough formatting; label_visibility on input widgets; optional properties and global page configuration; plugin settings to tweak the code output to better suit your needs.\nInternal improvements: code refactoring, type annotations, automatic data import, example callbacks, and more!\n\nWant to help us out with the plugin‚Äôs development? See the instructions on how to contribute to the codebase. Thank you! üôè\n\nWrapping up\n\nThat‚Äôs pretty much it, folks.\n\nWe hope you enjoy playing with this new plugin. If you find any errors or have any ideas on how to improve it, please file an issue, and we‚Äôll get back to you as soon as we can. Better yet, help us build those features yourself by contributing to the codebase.\n\nHappy Figma-to-Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion_chatbot_text_splitting.png (2000√ó1068)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_text_splitting.png#border",
    "html": ""
  },
  {
    "title": "notion_chatbot_vector_space.png (2000√ó1953)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_vector_space.png#border",
    "html": ""
  },
  {
    "title": "How to make a culture map",
    "url": "https://blog.streamlit.io/how-to-make-a-culture-map/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to make a culture map\n\nAnalyze multidimensional data with Steamlit!\n\nBy Micha≈Ç Nowotka\nPosted in Tutorials, January 12 2023\nHow to dynamically create buttons and assign them a columnar layout\nHow to create a scatter plot with flags as markers\nHow to find the best matching country\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHi, community! üëã\n\nMy name is Micha≈Ç Nowotka, and I‚Äôm a new Engineering Manager at Streamlit / Snowflake. Before diving into my first PR, I wanted to create my first Streamlit app that was fun, used freely available data, and had as many Steamlit components as possible.\n\nWhile reading the Culture Map book by Erin Meyer, I came across the six cultural dimensions theory by Geert Hofstede: Power Distance, Individualism, Uncertainty avoidance, Masculinity, Long Term Orientation, and Indulgence vs. restraint. The data was freely available, so I decided to give it a try.\n\nIn this post, I‚Äôll show you:\n\nHow to dynamically create buttons and assign them a columnar layout\nHow to create a scatterplot with flags as markers\nHow to find the best matching country\n\nWant to jump right in? Here's a demo app and a repo code.\n\nHow to dynamically create buttons and assign them a columnar layout\n\nAfter downloading the data, add a multi-select component to choose as many countries as as possible. By default, the multi-select shows a random selection of ten countries but they can be added or removed. You can also choose countries from a predefined group:\n\nNotice that all buttons selecting a group are rendered in a separate column. I wanted to allow for easy adding/removing country groups in the code, so that the rendering parts looked like this:\n\nst.write(\"Or choose from predefined country group:\")\ncolumns = st.columns(len(country_data.COUNTRY_GROUPS))\nfor idx, column in enumerate(columns):\n    with column:\n        group = country_data.COUNTRY_GROUPS[idx]\n        st.button(group, key=group, on_click=country_group_callback)\n\n\nAll buttons share the same on_click callback. To figure out which button called it and update the multi-select, use session state:\n\ndef country_group_callback():\n    chosen_group = [\n        group_name for group_name, selected in st.session_state.items() if\n        selected and group_name in country_data.COUNTRY_GROUPS][0]\n    countries = country_data.GROUPS_TO_COUNTRIES[chosen_group]\n    st.session_state[\"default_countries\"] = countries\n\nHow to create a scatter plot with flags as markers\n\nAfter selecting a few countries, you can visualize their cultural dimensions with:\n\nChoropleth\nRadar plots\nHeatmap\nScatter plot\n\nChoropleth is a kind of map that uses color to visualize a given property. Generate six choropleths for each dimension in separate tabs with plotly:\n\nThe tabs act as Python context managers and can be used with the with keyword:\n\nst.write(\"Or choose from predefined country group:\")\ncolumns = st.columns(len(country_data.COUNTRY_GROUPS))\nfor idx, column in enumerate(columns):\n    with column:\n        group = country_data.COUNTRY_GROUPS[idx]\n        st.button(group, key=group, on_click=country_group_callback)\n\n\nTo make switching from one tab to another less tedious, visualize multiple dimensions on a single graph with radar plots. Here are some good examples of how to do this with Matplotlib:\n\nConsidering all six dimensions is cumbersome, so how about compressing them into one number? You can measure the ‚Äúcultural distance‚Äù between all selected countries with Scipy. It lets you choose from different distance measures (Euclidean, Cosine, Manhattan, etc.). You can put them into a dictionary for convenience:\n\nfrom scipy.spatial import distance\n\nAVAILABLE_DISTANCES = {\n    \"Euclidean\": distance.euclidean,\n    \"Cosine\": distance.cosine,\n    \"Manhattan\": distance.cityblock,\n    \"Correlation\": distance.correlation,\n}\n\n\nNow computing a distance between two countries is easy:\n\nHOFSTEDE_DIMENSIONS = ['pdi', 'idv', 'mas', 'uai', 'lto', 'ind', 'ivr']\n\ndef compute_distance(\n        country_from: types.CountryInfo,\n        country_to: types.CountryInfo,\n        distance_metric: str\n) -> float:\n    from_array = [max(getattr(country_from, dimension) or 0, 0) for dimension in HOFSTEDE_DIMENSIONS]\n    to_array = [max(getattr(country_to, dimension) or 0, 0) for dimension in HOFSTEDE_DIMENSIONS]\n    return AVAILABLE_DISTANCES[distance_metric](from_array, to_array)\n\n\nNext, let‚Äôs create a distance matrix to compute the distances between all selected countries. Since the number of distances is proportional to the square of the number of selected countries, you can cache the result using st.cache decorator:\n\n@st.cache\ndef compute_distances(\n        countries: types.Countries,\n        distance_metric: str\n) -> tuple[PandasDataFrame, float]:\n    index = [country.title for country in countries]\n    distances = {}\n    max_distance = 0\n    for country_from in countries:\n        row = []\n        for country_to in countries:\n            distance = compute_distance(country_from, country_to, distance_metric)\n            max_distance = max(max_distance, distance)\n            row.append(distance)\n        distances[country_from.title] = row\n    return pd.DataFrame(distances, index=index), max_distance\n\n\nNow let‚Äôs plot the heatmap, cluster together similar countries, and show which country belong to which cluster. You can combine them with clustermap from Seaborn by applying the hierarchical clustering to the sides of the heatmap. Change the distance metric to change the colors and the row-and-columns clustering:\n\nThe distance between the countries is cultural as opposed to geographical, so it‚Äôd be great to see it in 2D space with their cultural traits (and not coordinates). For this, you‚Äôll need two dimensions. But which two should you use? Instead of choosing them arbitrarily, reduce the dimensionality using Principal Component Analysis. Just like scipy helped you with the distance metrics, scikit-learn can help with dimensionality reduction:\n\nfrom sklearn import decomposition\n\nAVAILABLE_DECOMPOSITION = {\n    'PCA': decomposition.PCA,\n    'FastICA': decomposition.FastICA,\n    'NMF': decomposition.NMF,\n    \"MiniBatchSparsePCA\": decomposition.MiniBatchSparsePCA,\n    \"SparsePCA\": decomposition.SparsePCA,\n    \"TruncatedSVD\": decomposition.TruncatedSVD\n}\n\n\nNext, compute the 2D data, use scatterplot to visualize locations, and mark each point with a country flag (fetch the data from Wikipedia üôÇ). Use the Bokeh library to replace markers with image URLs:\n\nHow to find the best matching country\n\nSince each dimension can score 1-100, use a set of six sliders to ask the users about their preferences:\n\nIt takes a lot of space, so let‚Äôs do it on a separate page. And just for fun, let‚Äôs add a button that sets each slider to a random value. Now you can compute a distance between user preferences and each country and select the top N hits. To make N a variable, use number input. You‚Äôre ready to present the ranking:\n\nThis looks pretty boring, so let‚Äôs add some data visualization. How about a radar plot? Let‚Äôs stack two radar spiders together. The red one will show your selected preferences, and the colored one will show the top country‚Äîto see how close it is to your preferences:\n\nRead the code to learn more about:\n\nLoading markdown content into st.markdown from an external .md file\nHiding raw content using st.expander\nShowing pandas data frames using st.write\nSetting app title headers and subheaders with st.title, st.header, st.subheader, and more!\nWrapping up\n\nIt‚Äôs amazingly easy to create complex visualizations and perform data analysis with Streamlit. And deployment is a breeze! Apparently, the top two countries matching my personal cultural preferences are Switzerland and UK. Coincidentally, that‚Äôs where I‚Äôve spent 7+ years of the last 14 years of my life. üôÇ If you have questions about the app, feel free to reach out to me via LinkedIn or email.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion_chatbot_folder_content-1.png (2000√ó911)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_folder_content-1.png#browser",
    "html": ""
  },
  {
    "title": "notion_chatbot_query.png (2000√ó805)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_query.png#brder",
    "html": ""
  },
  {
    "title": "Instant-Insight---Streamlit--1--2.gif (1056√ó528)",
    "url": "https://blog.streamlit.io/content/images/2023/08/Instant-Insight---Streamlit--1--2.gif",
    "html": ""
  },
  {
    "title": "Deploy a private app for free! üéâ",
    "url": "https://blog.streamlit.io/deploy-a-private-app-for-free/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDeploy a private app for free! üéâ\n\nAnd... get unlimited public apps\n\nBy Abhi Saini\nPosted in Product, December 9 2021\nShare this post\n‚Üê All posts\n\nStarting today you can deploy a private app for free with Streamlit Cloud!\n\nYour code stays private and you get auth-baked into the app. Try it out for your company or for any app that you don't want to be public. Head over to our docs to learn more about securing your data and adding private viewers.\n\nWhile we were working on this, we thought, \"Hmm. Why are we giving you only three free public apps? If you're doing great open-source stuff, we want to support you!\"\n\nSo also starting today...\n\nYou can deploy unlimited public apps for free! üéâ\n\nWhether you're a student submitting an assignment with Streamlit, a researcher illustrating your latest research with a Streamlit app, a company showing off your latest open-source data or a model in an app, or just a smart person sharing a cool interactive insight‚Äîwith Streamlit Cloud you can share all your public apps early, often, and always.\n\nWe hope this will help unblock you in the great work that you're doing. Let your creativity flourish! Stop by the forum and let us know what you think and what you're building.\n\nHappy Streamlit-ing! ‚ù§Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion-chatbot-openai-api-key.gif (741√ó362)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion-chatbot-openai-api-key.gif#browser",
    "html": ""
  },
  {
    "title": "notion_chatbot_ingestion.png (2000√ó502)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_ingestion.png#border",
    "html": ""
  },
  {
    "title": "notion_chatbot_duplicate-1.png (2000√ó689)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_duplicate-1.png#browser",
    "html": ""
  },
  {
    "title": "table.gif (1056√ó528)",
    "url": "https://blog.streamlit.io/content/images/2023/08/table.gif",
    "html": ""
  },
  {
    "title": "Leverage your user analytics on Streamlit Community Cloud",
    "url": "https://blog.streamlit.io/leverage-your-user-analytics-on-streamlits-community-cloud/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLeverage your user analytics on Streamlit Community Cloud\n\nSee who viewed your apps, when, and how popular they are\n\nBy Diana Wang and Johannes Rieke\nPosted in Product, May 17 2022\nWorkspace analytics\nApp viewers analytics\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nEver wanted to view the traffic levels of your Streamlit apps without using custom JS code? If so, today, we‚Äôre excited to announce...\n\nThe new Analytics Modal feature on Streamlit Community Cloud!\n\nNow you can see how many views you‚Äôve received, who has viewed your apps, and when.\n\nIn this post, we‚Äôll show you how to use both Workspace analytics and App viewers analytics by using a fictitious Streamlit workspace as an example.\n\nWant to jump right in? Head over to your Community Cloud account to view your analytics now.\n\nüí°\nAnalytics are visible to anyone with access to your workspace: admins, developers, and viewers.\nWorkspace analytics\n\nIn Workspace analytics, you can see how many total viewers have visited all apps in your workspace. Simply click on \"Analytics\" in the dashboard header:\n\nThe modal will automatically open in your \"Workspace\" tab:\n\nLet‚Äôs say that our fictitious Streamlit workspace has dozens of public apps. The solid line indicates the completed months and the dotted line indicates the month-in-progress for all apps in the workspace.\n\nYou can also hover over your \"Workspace\" tab‚Äôs chart to see how many users have viewed at least one app in a given month:\n\nApp viewers analytics\n\nHead over to App viewers analytics to see who has recently viewed your apps and when.\n\nWe‚Äôll use the 30 Days of Streamlit app as an example (we launched it in April for the community to learn more about Streamlit and to try fun use cases).\n\nYou can access App viewers analytics in three ways:\n\nIn your workspace, click the \"Analytics\" tab and then the \"App viewers\" tab:\n\n2. From your \"Apps\" dashboard, click the \"Ô∏ô\" overflow menu for any app, then select \"Analytics\":\n\n3. From your \"Apps\" page, click \"Manage app\" console, then select \"Analytics\":\n\nüí°\nYou can access the \"Manage app\" console only if you have the GitHub push access for the given app.\n\nAll three ways will bring you to the same feature:\n\nThe total all-time number of unique app viewers (including April 2022 and onward).\nThe list of the most recent viewer names (capped to 20) and the relative timestamp of their last view sorted by the time since the last view (newest first).\n\nFor public apps like 30 Days of Streamlit, we track individual usage but not identity, so we make up names for your public viewers. They're shown as random pseudonyms (for example, Enigmatic Brownie).\n\nIf you're building something awesome, but not quite ready for the world to see it, use your one private app to test it. You can invite up to three viewers to your workspace as collaborators. The identities of all invited viewers within your workspace will be visible within the Analytics Modal.\n\nWrapping up\n\nAnd that‚Äôs a wrap! You can now view Workspace analytics and App viewers analytics within your Community Cloud accounts. Check out our docs for more info.\n\nDon‚Äôt have a Community Cloud account? Create one for free here.\n\nQuestions? Suggestions? Or have a neat app or some analytics to show off? Join us on the Streamlit community forum. We can't wait to hear what you think. üéà\n\nHuge kudos to...\n\nWill Schmitt, Benny Raethlein, Andreas Braendhaugen, Will Huang, Laura Wilby, Henrikh Kantuni, Zachary Blackwood, Grace Tan, Snehan Kekre, and James Thompson for bringing this feature to life on Community Cloud. And thanks to all the community members who gave feedback on the different iteration ideas for analytics in the past!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion_chatbot_project_structure.png (2000√ó1263)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_project_structure.png",
    "html": ""
  },
  {
    "title": "notion-chatbot.gif (758√ó416)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion-chatbot.gif#browser",
    "html": ""
  },
  {
    "title": "‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è",
    "url": "https://blog.streamlit.io/introducing-streamlit-cloud/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\n\nStreamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.\n\nBy Adrien Treuille\nPosted in Product, November 2 2021\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\nTry it out\nContents\nShare this post\n‚Üê All posts\n\nData science is in a logjam.\n\nOnce upon a time, data flowed smoothly into well-structured tables. Now it's a turbulent mix of image labels, natural language data, API endpoints, neural networks, and other models.\n\nIn parallel, business demands have become more complex. Users don't just want the graphs of the past. They want the models of the future. They don't just want summary dashboards. They want to go beyond dashboards.\n\nThey want rich data apps.\n\nData apps go beyond dashboards to capture a variety of next-generation data sources and analytic abilities.\n\nCaught between the rising data complexity and the deeper analytical needs, data teams searched for new tools. But nothing let them quickly build and iterate on the next-generation data products they envisioned.\n\nThen in 2019, we released a solution‚Äîa new way to make polished, flexible, and powerful data apps in just a few lines of Python...\n\nStreamlit!\n\nYour response surged beyond our wildest expectations. You unleashed a torrent of dazzling Streamlit apps. You explained, analyzed, and modeled everything from real estate to black holes. You brought it to work. You used Streamlit to label ad videos, track dbt jobs, visualize object detection output, and explore vast quantities of geographic data.\n\nIn fact, you spread Streamlit so widely that Streamlit is now actively deployed at over half the Fortune 50!\n\nWith all this app-building, you surfaced new needs. Building great apps was only half the logjam. You also needed to share those apps, get them into the hands of your users, gather feedback, iterate, and create new ideas.\n\nDoing that inside a company wasn't easy. It involved more skillsets, more meetings, more coordination. It slowed down your development pace.\n\nThe current process in companies to create and deploy data apps takes months and multiple teams.\n\nSo for the past two years, we've been building a new product. We hardened it in the community and beta-tested it in hundreds of organizations‚Äîfrom small AI startups to public multinational companies.\n\nToday, we want to share this new superpower with every company in the world‚Ä¶\n\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\n\nStreamlit Cloud is a data workspace for your company.\n\nWhen you work on an app in Streamlit Cloud‚Äîbe it a new model, data analysis, or idea‚Äîyou're just a few clicks away from securely sharing it and collaborating on it with your team.\n\n1. Build and deploy apps in minutes\n\nBuild your Streamlit apps the way you've always built them! Download the open-source library, use your favorite IDE, and take advantage of Streamlit's run-on-save rapid development flow. Done building? Use Streamlit Cloud to watch your app go live across the company.\n\nStreamlit Cloud handles all the IT, DevOps, and security for you‚ÄîPython dependencies, Unix package management, container orchestration, server provisioning, scaling, data security, and more‚Äîso you can get back to your data work!\n\n2. Securely share apps\n\nOnce your app is deployed, you can securely share it with your whole company. Send it to just one person. Or send it to teammates, customers, and other business units so they can start using it right away.\n\nStreamlit Cloud works with your preferred SSO provider. Easily lock down your app so only certain people can see it.\n\n3. Rapidly iterate\n\nYour app is shared! Now you can quickly iterate on it. Streamlit Cloud continuously deploys your app from GitHub, giving you the power of modern version-controlled code development.\n\nLike a teammate's app? Fork it and launch your own! Want to test out a new version of a production app? Branch it and deploy a dev app. Prefer an earlier version? Check the version history and roll it back. Received a user request? Make the change and watch the app automatically update.\n\nAll of this makes for an incredibly rapid prototype-to-production cycle for your whole team.\n\nTry it out\n\nStreamlit Cloud moves you beyond the dashboard and into a new world of bespoke, predictive tooling where your company can run \"at the speed of data science.\" We've watched companies go from having a handful of slow, poorly maintained tools to launching thousands of apps a year. It completely transformed how they work with data.\n\nToday we're opening up Streamlit Cloud to all companies, and we hope you try it out.\n\nA HUGE thank you to our community and beta testers for inspiring us every day. This wouldn't be possible without you and your candid feedback, comments, and words of encouragement.\n\nWe can't wait to hear what you build next! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "notion_chatbot_main_flow.png (2000√ó884)",
    "url": "https://blog.streamlit.io/content/images/2023/09/notion_chatbot_main_flow.png",
    "html": ""
  },
  {
    "title": "The next frontier for Streamlit",
    "url": "https://blog.streamlit.io/the-next-frontier-for-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nBy Amanda Kelly, Adrien Treuille, Thiago Teixeira, Johannes Rieke and TC Ricks\nPosted in Product, October 18 2022\nüìÄ The data trio: st.connection, st.dataframe, and st.database\nüïπ Interactive everything: charts, tables, maps, and more!\nüé® Visual customization ++\nüß© Custom components v2\nüß† ML is amazing in Streamlit\nüêû Debugging is gosh darn delightful\nüéà Streamlit Community Cloud becomes a hub for community discovery\nŒª Towards stateless Streamlit‚Ä¶\nüèÅ Wrapping up\nContents\nShare this post\n‚Üê All posts\n\nDear Python developers and wonderful Streamlit community, ‚ù§Ô∏è\n\nWe come bearing exciting news about the future of Streamlit‚Äîamazing new superpowers, mind-bending new use cases, and powerful new technologies.\n\nBut first, let‚Äôs take a look at the last year.\n\nIn October 2021, we hit a big milestone and launched Streamlit 1.0. Then a few months later we announced we‚Äôd be growing our ‚Äúbaby‚Äù with new parents. Yup. If you missed the news‚Äîsix months ago we joined Snowflake. Snowflake‚Äôs commitment to Streamlit and open source is deep, and it‚Äôs giving us even more support to build the Streamlit library and the community platforms for years to come. Yay!\n\nSo, what‚Äôs next?\n\nWe‚Äôre thinking bigger than ever, looking far into the future, and asking ourselves, ‚ÄúWhat will Streamlit be when it grows up?‚Äù In short, we want to see Streamlit become the essential UI layer for Python. We want to take our amazing dev experience and turn it up to 11‚Äîtaking your daily pain points and turning them into magical moments. And we want these new superpowers to benefit the entire Python community‚Äîregardless of skill level, application domain, deployment platform, or ability to pay.\n\nWithout further ado, here are our plans for next year and beyond:\n\nüìÄ The data trio: st.connection, st.dataframe, and st.database\n\nWe want to make the path from data to app as frictionless as possible. st.connection will let you connect to external databases and APIs with a single line of code. st.database will launch a small database alongside every Streamlit app, so you can permanently store data without any setup. Andst.dataframe will get a number of improvements‚Äîfrom filtering to sorting, editability, showing images, and much more.\n\nüïπ Interactive everything: charts, tables, maps, and more!\n\nOne of the most upvoted feature requests is to make output elements interactive. Click on a chart datapoint, a map location, or a dataframe row, and the Python script will catch that event on the Python side and update your app. This allows extremely cool new use cases, e.g., showing the details for a selected dataframe row or running a prediction on some data selected in a chart.\n\nüé® Visual customization ++\n\nOur focus has always been on making your app as beautiful out-of-the-box as possible. But if you want to customize it, the current options are limiting. Grid layout, more options for colors, logo positioning, and better themes are just some of the options we‚Äôre looking at. But we‚Äôre also wondering if we can go beyond that and better integrate with CSS or offer other ways to manipulate styling. Stay tuned!\n\nüß© Custom components v2\n\nToday, you can extend Streamlit by building custom components, but this requires advanced JavaScript skills. Plus, it‚Äôs time-consuming (even for experts). We want to crack open this box to rethink how we‚Äôre doing extensibility, to make it easier to do more with less. Components should be able to interact with more elements in apps, it should be incredibly easy to publish Python- or HTML-only components, and it should be effortless to discover and use great community-built components.\n\nüß† ML is amazing in Streamlit\n\nIt feels like every day there is a new, amazing breakthrough in ML and AI. The boundaries of what‚Äôs possible are being pushed‚Äîand we want to push alongside it. We want to make it easier to connect to and work with models and pipelines, integrate with the machine learning tools you love, and create the visualization and widgets you need to show off your work.\n\nüêû Debugging is gosh darn delightful\n\nBuilding a Streamlit app is pretty delightful, so we want to make debugging your code just as frictionless. We‚Äôre going back to the mat to think about what it means for you as a developer‚Äîincluding a better in-app display of exceptions and logs, built-in memory and runtime profilers, and improvements to how Streamlit works in the terminal and in your favorite editor.\n\nüéà Streamlit Community Cloud becomes a hub for community discovery\n\nA huge part of developing Streamlit apps is sharing them with other developers for feedback, being inspired by the work of others in the community, and finding app examples and code snippets to use. Streamlit Community Cloud is (and will continue to be!) a free platform for sharing, but we see the potential for so much more. The potential to offer a true hub for you and everyone in the community. We want to continue to remove all barriers to easy development, sharing, and discovery‚Äîhelping you to get started coding directly on the cloud, show off your work to a broader audience, save inspiring and useful bits of code, encounter trending and relevant apps, meet collaborators, and uncover amazing community-created components.\n\nŒª Towards stateless Streamlit‚Ä¶\n\nWe‚Äôre investigating how to expand the ways Streamlit can be hosted, including implementing a stateless model so it works well in a serverless computing environment. This is part of a broader effort to make it easy for you to run Streamlit on any infrastructure and to rethink how it runs‚Äîto open new possibilities for sharing, state, compute, and more.\n\nüèÅ Wrapping up\n\nWe want to make working in Streamlit your daily source of joy. What issues have we missed? What new features do we need to develop? Please come to the forum and tell us!\n\nWe hope you‚Äôll keep sharing your apps, components, videos, and blog posts with us. Your work is an incredible source of inspiration, and it makes us want to build even more amazing things with you in the future. Thank you. üôè\n\nLove,\n\nAmanda, Adrien, Thiago, Johannes, TC, and everyone at Streamlit. ‚ù§Ô∏è\n\nP.S.: Streamlit in Snowflake is coming soon! ‚ùÑÔ∏è See a little preview here.\n\nP.P.S.: Want to help with Streamlit full-time? We‚Äôre hiring!\n\nForward-Looking Statements\n\nThis post contains express and implied forwarding-looking statements, which are subject to a number of risks, uncertainties and assumptions, including those described under the heading ‚ÄúRisk Factors‚Äù in quarterly and annual reports that Snowflake files with the Securities and Exchange Commission. In light of these risks, uncertainties, and assumptions, actual results could differ materially and adversely from those anticipated or implied in the forward-looking statements. As a result, you should not rely on any forwarding-looking statements as predictions of future events.\n\n¬© 2022 Snowflake Inc. All rights reserved. Snowflake, the Snowflake logo, and all other Snowflake product, feature and service names mentioned herein are registered trademarks or trademarks of Snowflake Inc. in the United States and other countries. All other brand names or logos mentioned or used herein are for identification purposes only and may be the trademarks of their respective holder(s). Snowflake may not be associated with, or be sponsored or endorsed by, any such holder(s).\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Cloud is now SOC 2 Type 1 compliant",
    "url": "https://blog.streamlit.io/streamlit-cloud-is-now-soc-2-type-1-compliant/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit Cloud is now SOC 2 Type 1 compliant\n\nWe have completed a full external audit of our security practices\n\nBy Amanda Kelly\nPosted in Product, January 11 2022\nA little background on security at Streamlit\nWhat is SOC 2?\nWhy is SOC 2 important?\nWhat does it mean for you?\nContents\nShare this post\n‚Üê All posts\nA little background on security at Streamlit\n\nSecurity has been integral to how we have built Streamlit from day one.\n\nOur engineering team has worked with some of the best security talent in the industry, and these experiences have shaped how we build and design secure software. That‚Äôs why our product works well with your existing security protocols.\n\nStreamlit apps are the code that you write, and Streamlit Cloud runs that code to serve your apps to your users. Streamlit Cloud stores only a copy of your code and none of your data. Since you control both the code and the data sources, it‚Äôs very easy to implement your preferred security practices. On our end, we always uphold industry best practices for encrypting data in transit, securely storing authentication credentials, providing SSO integration for accessing the app, and much more.\n\nToday we‚Äôre thrilled to announce that we‚Äôve completed a full external audit of these security practices and Streamlit Cloud is now SOC 2 Type 1 compliant.\n\nIn this article, we‚Äôre going to share with you what SOC 2 is, why it matters, and how we comply with it.\n\nWhat is SOC 2?\n\nService Organization Control (SOC) 2 is a SaaS industry standard that shows customers if a business has effective security controls. The American Institute of CPAs (AICPA) has developed SOC 2 to define how businesses should manage customers' data. Basically, it was designed to make you feel safe about the information you share with any business.\n\nThe typical SOC 2 audit is based on five Trust Services Criteria: security, availability, processing integrity, confidentiality, and privacy. When a company undergoes the SOC 2 audit process, it can choose which of the five to focus on. We chose to focus on security and confidentiality.\n\nAfter the audit, the company gets either Type 1 or Type 2 SOC 2 report that reflects its unique business practices. Type 1 covers compliance at a given moment. Type 2 covers it over a longer period of time.\n\nOn October 31st, 2021, Streamlit Cloud has been certified as SOC 2 Type 1. You can request the full audit report by emailing us at support@streamlit.io.\n\nWhy is SOC 2 important?\n\nSOC 2 is important because your management needs it. Or your security team. Or, your organization is SOC 2 certified, but to use a new service that might touch your sensitive data they require your vendors to be SOC 2 certified.\n\nThis certification lets us be an easy-to-approve vendor for you.\n\nIf you're new to Streamlit, or if your compliance questions have stopped you before from signing up, we hope this new certification gives you more confidence in testing out Streamlit Cloud for securely sharing apps within your organization.\n\nAlso, every SOC 2 report has ‚Äúyour responsibility‚Äù and ‚Äúthe provider‚Äôs responsibility‚Äù sections. One of the key responsibilities we want to call out is that you must connect to your organization‚Äôs services securely. Often, this is as simple as storing your authentication secrets by using our Secrets feature to connect to your data store via TLS.\n\nYou can find guidance on how to connect to various data stores in our docs, and you can always reach us at support@streamlit.io if you have any questions. We‚Äôre here to help you every step of the way. üôÇ\n\nWhat does it mean for you?\n\nYou‚Äôll continue to benefit from our investment in security and data privacy as we keep adding even more security features and options to the product. You can also read about our security approach on our privacy policy page and in our documentation on trust and security.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Product Announcements",
    "url": "https://blog.streamlit.io/tag/product/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Product\n36 posts\nLaunching a brand-new docs site ü•≥\n\nImproved layout, easier navigation, and faster search\n\nProduct\nby\nSnehan Kekre\n,\nOctober 13 2021\nAnnouncing Streamlit 1.0! üéà\n\nStreamlit used to be the simplest way to write data apps. Now it's the most powerful\n\nProduct\nby\nAdrien Treuille\n,\nOctober 5 2021\nNew experimental primitives for caching (that make your app 10x faster!)\n\nHelp us test the latest evolution of st.cache\n\nProduct\nby\nAbhi Saini and¬†\n1\n¬†more,\nSeptember 22 2021\nStreamlit gains a major new spell book\n\nA tome to the magical fields of Python, algorithms, visualization, and machine learning\n\nProduct\nby\nAdrien Treuille\n,\nAugust 20 2021\nAll in on Apache Arrow\n\nHow we improved performance by deleting over 1k lines of code\n\nProduct\nby\nHenrikh Kantuni\n,\nJuly 22 2021\nSession State for Streamlit üéà\n\nYou can now store information across app interactions and reruns!\n\nProduct\nby\nAbhi Saini\n,\nJuly 1 2021\nIntroducing Submit button and Forms üìÉ\n\nWe're releasing a pair of new commands called st.form and st.form_submit_button!\n\nTutorials\nby\nAbhi Saini\n,\nApril 29 2021\nOur $35 million Series B\n\nWe‚Äôre excited to announce a new funding round led by Sequoia üå≤\n\nProduct\nby\nAdrien Treuille\n,\nApril 7 2021\nAnnouncing Theming for Streamlit apps! üé®\n\nTry out the new dark mode and custom theming capabilities\n\nProduct\nby\nAbhi Saini\n,\nMarch 18 2021\nIntroducing Streamlit Sharing\n\nThe new Streamlit platform for deploying, managing, and sharing your apps\n\nProduct\nby\nAdrien Treuille\n,\nOctober 15 2020\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "A new Streamlit theme for Altair and Plotly charts",
    "url": "https://blog.streamlit.io/a-new-streamlit-theme-for-altair-and-plotly/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nBy William Huang, Lukas Masuch, Andreas Br√¶ndhaugen, Arnaud Miribel and Johannes Rieke\nPosted in Product, December 19 2022\nStreamlit + Altair + Plotly üéà\nIntroducing a beautiful new Streamlit theme üßë‚Äçüé®\nHow can you try it? üë©‚Äçüíª\nYou don't need to do anything!\nYou can customize it\nYou can disable it\nWrapping up\nContents\nShare this post\n‚Üê All posts\nStreamlit + Altair + Plotly üéà\n\nAt Streamlit, we're constantly inspired by other open-source projects.\n\nWe're especially inspired by Altair and Plotly and their contributions to the data visualization community. They're two of the most popular Python libraries for creating interactive charts in Python‚Äîand in Streamlit. We continuously work to support them in our library, as Streamlit's success is due in no small part to their powerful and flexible charting capabilities.\n\nIntroducing a beautiful new Streamlit theme üßë‚Äçüé®\n\nToday, we're excited to announce the release of a new default theme for Altair and Plotly charts in Streamlit apps!\n\nThe theme uses Streamlit's signature colors and layout adjustments that will make your data visualizations more engaging and easier to understand. Plus, your charts will better integrate with the rest of your app's design.\n\nCheck out our release demos for Altair at altair.streamlit.app and plotly.streamlit.app for more examples.\n\nPsst‚Ä¶night owls and dark mode fans‚Äîwe've got you covered, too! üåö\n\nHow can you try it? üë©‚Äçüíª\nYou don't need to do anything!\n\nThe new theme is available from Streamlit 1.16 through the theme=\"streamlit\" keyword argument in st.altair_chart, st.vega_lite_chart, and st.plotly_chart. It's activated by default, so there is nothing you need to do to enjoy it. ‚ú®\n\nHere's a minimal example of a contour plot with Plotly:\n\nimport numpy as np\nimport plotly.graph_objects as go\nimport streamlit as st\n\nz = np.random.random_sample((3, 2))\nfig = go.Figure(data=go.Contour(z=z))\n\nst.plotly_chart(\n    fig, \n    theme=\"streamlit\",  # ‚ú® Optional, this is already set by default!\n)\n\n\nYou can customize it\n\nIf you're an experienced Altair or Plotly user and like customizations, don't worry. Although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\n\nCheck out some customization examples in Altair and Plotly.\n\nYou can disable it\n\nIf you want, you can continue using the default theme. Just type theme=None instead of theme=\"streamlit\" in your chart commands:\n\nst.altair_theme(..., theme=None)\n\n\nTo learn more about our charts, visit our Altair, Plotly, and Vega-Lite chart docs!\n\nWrapping up\n\nWe're confident that your users (and you!) will love this new default theme. Go ahead and give it a try. And check out the other features released in 1.16.0.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Abhi Saini - Streamlit",
    "url": "https://blog.streamlit.io/author/abhi_s/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Abhi Saini\n9 posts\nDeploy a private app for free! üéâ\n\nAnd... get unlimited public apps\n\nProduct\nby\nAbhi Saini\n,\nDecember 9 2021\n0.89.0 release notes\n\nThis release launches configurable hamburger menu options and experimental primitives for caching\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 22 2021\nNew experimental primitives for caching (that make your app 10x faster!)\n\nHelp us test the latest evolution of st.cache\n\nProduct\nby\nAbhi Saini and¬†\n1\n¬†more,\nSeptember 22 2021\n0.88.0 release notes\n\nThis release launches st.download_button as well as other improvements and bug fixes\n\nRelease Notes\nby\nAbhi Saini\n,\nSeptember 3 2021\nSession State for Streamlit üéà\n\nYou can now store information across app interactions and reruns!\n\nProduct\nby\nAbhi Saini\n,\nJuly 1 2021\nHow to make a great Streamlit app: Part II\n\nA few layout and style tips to make your apps look even more visually appealing!\n\nTutorials\nby\nAbhi Saini\n,\nJune 22 2021\nHow to make a great Streamlit app\n\nDesigning an app your users will love\n\nTutorials\nby\nAbhi Saini\n,\nJune 2 2021\nIntroducing Submit button and Forms üìÉ\n\nWe're releasing a pair of new commands called st.form and st.form_submit_button!\n\nTutorials\nby\nAbhi Saini\n,\nApril 29 2021\nAnnouncing Theming for Streamlit apps! üé®\n\nTry out the new dark mode and custom theming capabilities\n\nProduct\nby\nAbhi Saini\n,\nMarch 18 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing two new caching commands to replace st.cache!",
    "url": "https://blog.streamlit.io/introducing-two-new-caching-commands-to-replace-st-cache/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nBy Tim Conkling, Karen Javadyan and Johannes Rieke\nPosted in Product, February 14 2023\nWhat's the problem with st.cache?\nThe solution: st.cache_data and st.cache_resource\nNew documentation\nAdditional features ‚ú®\nWhat will happen to st.cache?\nThe future of caching\nContents\nShare this post\n‚Üê All posts\n\nCaching is one of the most beloved and dreaded features of Streamlit. We understand why! @st.cache makes apps run faster‚Äîjust slap it on top of a function, and its output will be cached for subsequent runs. But it comes with a lot of baggage: complicated exceptions, slow execution, and a host of edge cases that make it tricky to use. üòî\n\nToday, we're excited to announce two new caching commands‚Ä¶\n\nst.cache_data and st.cache_resource!\n\nThey're simpler and faster, and they will replace st.cache going forward! üë£\n\nWhat's the problem with st.cache?\n\nWe spent a lot of time investigating it and talking to users. Our verdict: st.cache tries to solve too many use cases!\n\nThe two main use cases are:\n\nCaching data computations, e.g. when you transform a dataframe, compute NumPy arrays, or run an ML model.\nCaching the initialization of shared resources, such as database connections or ML models.\n\nThese use cases require very different optimizations. One example:\n\nFor (1), you want your cached object to be safe against mutations. Every time the function is used, the cache should return the same value, regardless of what your app does with it. That's why st.cache constantly checks if the cached object has changed. Cool! But also‚Ä¶slow.\nFor (2), you definitely don't want these mutation checks. If you access a cached database connection many times throughout your app, checking for mutations will only slow it down without any benefit.\n\nThere are many more examples where st.cache tries to solve both scenarios but gets slow or throws exceptions. So in 2021, we decided to separate these use cases. We released two experimental caching commands, st.experimental_memo and st.experimental_singleton. They work similarly to st.cache and are optimized for (1) and (2), respectively. Their behavior worked great, but the names confused users (especially if they didn't know the underlying CS concepts of memoization and singleton).\n\nThe solution: st.cache_data and st.cache_resource\n\nToday, we're releasing our new solution for caching: st.cache_data and st.cache_resource. These commands have the same behavior as st.experimental_memo and st.experimental_singleton (with a few additions described below) but should be much easier to understand.\n\nUsing these new commands is super easy. Just add them as a decorator on top of the function you want to cache:\n\n@st.cache_data\ndef long_running_function():\n    return ...\n\n\nHere's how to use them:\n\nst.cache_data is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, and so on). It creates a new copy of the return object at each function call, protecting it from mutation and concurrency issues. The behavior of st.cache_data is what you want in most cases‚Äîso if you're unsure, start with st.cache_data and see if it works!\n\n\nst.cache_resource is the recommended way to cache global resources such as ML models or database connections‚Äîunserializable objects that you don't want to load multiple times. By using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache.\nNew documentation\n\nAlong with this release, we're launching a brand new docs page for caching. üöÄ It explains in detail how to use both commands, what their parameters are, and how they work under the hood. We've included many examples to make it as close to real life as possible. If anything is unclear, please let us know in the comments. ‚ù§Ô∏è\n\nAdditional features ‚ú®\n\nThe behavior of the new commands is mostly the same as st.experimental_memo and st.experimental_singleton, but we also implemented some highly requested features:\n\nst.cache_resource now has a ttl‚Äîto expire cached objects.\nst.cache_resource got a validate parameter‚Äîto run a function that checks whether cached objects should be reused. Great for recreating expired database connections!\nttl can now accept timedelta objects‚Äîinstead of ttl=604800 you can now write ttl=timedelta(days=7).\nLast but not least: cached functions can now contain most Streamlit commands! This powerful feature allows you to cache entire parts of your UI. See details here.\nWhat will happen to st.cache?\n\nStarting with 1.18.0, you'll get a deprecation warning if you use st.cache. We recommend you try the new commands the next time you build an app. They'll make your life easier and your apps faster. In most cases, it's as simple as changing a few words in your code. But we also know that many existing apps use st.cache, so we'll keep st.cache around, for now, to preserve backward compatibility. Read more in our migration guide in the docs.\n\nWe'll also be deprecating st.experimental_memo and st.experimental_singleton‚Äîas they were experimental. The good news is: their behavior is the same as the new commands, so you'll only need to replace one name.\n\nThe future of caching\n\nWe'll continue to improve caching! The new commands will make it easier to use and understand. Plus, we have more features on our roadmap, such as limiting the amount of memory the cache can use or adding a cache visualizer right into your app.\n\nTrack our progress at roadmap.streamlit.app and send us feature requests on GitHub. And if you have any questions or feedback, please leave them in the comments below.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit gains a major new spell book",
    "url": "https://blog.streamlit.io/streamlit-gains-a-major-new-spell-book/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit gains a major new spell book\n\nA tome to the magical fields of Python, algorithms, visualization, and machine learning\n\nBy Adrien Treuille\nPosted in Product, August 20 2021\nShare this post\n‚Üê All posts\n\nI remember a CS professor of mine pointing out that most of the magic in Harry Potter can now be done on computers! Images dance on our digital newspapers. Cellphones swirl with memories like portable Pensieves. Computer classes are our Charms. Algorithms are our Arithmancy!\n\nIf computing departments are the new Hogwarts, then technical tomes are the new spell books. The best works brim with technical secrets and arcana, and represent a totem to some branch of our magical field: Python. Algorithms. Visualization. Machine learning.\n\nI'm therefore particularly excited and proud to share that Streamlit has a major new book, lovingly written by one of our own Streamlit Creators (and Facebook data scientist) Tyler Richards.\n\nThis is a true spell book. Yes, other books teach Streamlit, but this is the first which captures the essence of Streamlit. The book demonstrates how Streamlit is transforming the very definition of data science and machine learning.\n\nThroughout the 2010s, data science and machine learning had two basic outputs. On the one hand, you could use a notebook environment to create static analyses. On the other, you could deploy complete machine learning models into production. Streamlit opened up a new middle way between these two: interactive apps that let you play with analyses and share models interactively throughout an organization.\n\nGetting Started with Streamlit for Data Science teaches you how to master this new superpower. You start by creating a basic analysis and work your way up to complete Streamlit apps with fancy graphics and interactive machine learning models.\n\nSo go pick up your copy! (Starting August 21, you can even use discount code 25TYLER on Amazon.) Learn the deep secrets of Streamlit. Join our magical community. Share your apps with the world. Contribute to our gallery. Or invent your own spells with custom components. Whether you're a wizard-in-training looking to deploy your first machine learning project or an experienced auror, this book will turn you into a Streamlit sorcerer. üßôüèΩ‚Äç‚ôÇÔ∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Juan Mart√≠n Garc√≠a - Streamlit",
    "url": "https://blog.streamlit.io/author/juan/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Juan Mart√≠n Garc√≠a\n1 post\nAnnouncing the Figma-to-Streamlit plugin üé®\n\nGo from prototype to code as easy as 1-2-3 with our new community resource!\n\nProduct\nby\nJuan Mart√≠n Garc√≠a\n,\nNovember 1 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "table-prediction.png (1342√ó1462)",
    "url": "https://blog.streamlit.io/content/images/2023/04/table-prediction.png#border",
    "html": ""
  },
  {
    "title": "molecule-structure.png (1216√ó1252)",
    "url": "https://blog.streamlit.io/content/images/2023/04/molecule-structure.png#border",
    "html": ""
  },
  {
    "title": "demo-app.png (2000√ó1516)",
    "url": "https://blog.streamlit.io/content/images/2023/04/demo-app.png#browser",
    "html": ""
  },
  {
    "title": "demo-app-2.png (2000√ó79)",
    "url": "https://blog.streamlit.io/content/images/2023/04/demo-app-2.png#border",
    "html": ""
  },
  {
    "title": "column-configuration-page.png (2048√ó1448)",
    "url": "https://blog.streamlit.io/content/images/2023/06/column-configuration-page.png#browser",
    "html": ""
  },
  {
    "title": "connection-logos.png (1772√ó1528)",
    "url": "https://blog.streamlit.io/content/images/2023/05/connection-logos.png",
    "html": ""
  },
  {
    "title": "AIInterviewer-demo.gif (640√ó328)",
    "url": "https://blog.streamlit.io/content/images/2023/08/AIInterviewer-demo.gif#browser",
    "html": ""
  },
  {
    "title": "Johannes Rieke - Streamlit",
    "url": "https://blog.streamlit.io/author/johannes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Johannes Rieke\n9 posts\nIntroducing column config ‚öôÔ∏è\n\nTake st.dataframe and st.data_editor to the next level!\n\nProduct\nby\nLukas Masuch and¬†\n1\n¬†more,\nJune 22 2023\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nProduct\nby\nLukas Masuch and¬†\n2\n¬†more,\nFebruary 28 2023\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nProduct\nby\nTim Conkling and¬†\n2\n¬†more,\nFebruary 14 2023\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nBuilt-in charts get a new look and parameters! üìä\n\nCreate beautiful charts with one line of code\n\nRelease Notes\nby\nJohannes Rieke and¬†\n1\n¬†more,\nAugust 11 2022\nLeverage your user analytics on Streamlit Community Cloud\n\nSee who viewed your apps, when, and how popular they are\n\nProduct\nby\nDiana Wang and¬†\n1\n¬†more,\nMay 17 2022\n1.1.0 release notes\n\nThis release launches memory improvements and semantic versioning\n\nRelease Notes\nby\nJohannes Rieke\n,\nOctober 21 2021\nCommon app problems: Resource limits\n\n5 tips to prevent your app from hitting the resource limits of the Streamlit Cloud\n\nTutorials\nby\nJohannes Rieke\n,\nSeptember 9 2021\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Lukas Masuch - Streamlit",
    "url": "https://blog.streamlit.io/author/lukasmasuch/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Lukas Masuch\n3 posts\nTwitter\nIntroducing column config ‚öôÔ∏è\n\nTake st.dataframe and st.data_editor to the next level!\n\nProduct\nby\nLukas Masuch and¬†\n1\n¬†more,\nJune 22 2023\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nProduct\nby\nLukas Masuch and¬†\n2\n¬†more,\nFebruary 28 2023\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Vincent Donato - Streamlit",
    "url": "https://blog.streamlit.io/author/vincent-donato/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Vincent Donato\n2 posts\nIntroducing st.connection!\n\nQuickly and easily connect your app to data and APIs\n\nProduct\nby\nJoshua Carroll and¬†\n1\n¬†more,\nMay 2 2023\nIntroducing multipage apps! üìÑ\n\nQuickly and easily add more pages to your Streamlit apps\n\nProduct\nby\nVincent Donato\n,\nJune 2 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Amanda Kelly - Streamlit",
    "url": "https://blog.streamlit.io/author/amanda/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Amanda Kelly\nCo-founder and COO at Streamlit\n5 posts\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nLLMs\nby\nAdrien Treuille and¬†\n1\n¬†more,\nJune 15 2023\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nProduct\nby\nAdrien Treuille and¬†\n2\n¬†more,\nMarch 2 2022\nHow Delta Dental uses Streamlit to make lightning-fast decisions\n\nFrom an idea to a prototype to production in just two weeks\n\nCase study\nby\nAmanda Kelly\n,\nFebruary 1 2022\nStreamlit Cloud is now SOC 2 Type 1 compliant\n\nWe have completed a full external audit of our security practices\n\nProduct\nby\nAmanda Kelly\n,\nJanuary 11 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How Streamlit uses Streamlit: Sharing contextual apps",
    "url": "https://blog.streamlit.io/how-streamlit-uses-streamlit-sharing-contextual-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow Streamlit uses Streamlit: Sharing contextual apps\n\nLearn about session state and query parameters!\n\nBy Tyler Richards\nPosted in Tutorials, May 26 2022\nHow to get URL query parameters into Streamlit\nHow to use those parameters in Streamlit widgets\nHow to sync widgets and your app‚Äôs URLs\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nAll of us on the Streamlit Data Science team are massive Streamlit fans (obviously!). In our day jobs, we produce internal Streamlit apps. These apps do everything from helping our partners discover useful tables in our data warehouse, to graphing Streamlit Community Cloud‚Äôs monthly active developers over time, to seeing what Streamlit features are rising and falling in use. This means we produce a ton of apps and keep them all in one large, multi-page app.\n\nBut as we traveled down this path, we found ourselves in a bit of a pickle. We‚Äôd create a new app, find a widget combination that sheds light on something super useful, and would want to share it in that exact state. For example, what if we worked for Uber and wanted to share how NYC rideshare traffic looked at 2 am? Or how the 2 am traffic differed from the 2 pm traffic?\n\nTo do this, we could:\n\nSet the app's widgets' default values so that the user could find them on screen load. We'd share the app in this exact initial state, but the users can still interact with it and explore other configurations.\nTake app screenshots and send them via Slack, Notion, or email.\nWrite instructions for rediscovering the finding, either in the app itself or in the message to users, ‚ÄúHi! We made a new change in our NYC rideshare app. Here is the link. Go ahead and change the slider bar to 2am. We noticed an interesting concentration of rides in the Chelsea area during that time, especially relative to the 2pm time period.‚Äù\n\nBut these options aren‚Äôt all that great.\n\nOption 1 doesn‚Äôt take into consideration that we often have many cool findings per app! It also doesn‚Äôt work for our multi-page app setup, so it‚Äôs out of the picture.\nOption 2 defeats the purpose of an app. Why create an interactive app when users will see only a static photo?\nOption 3 puts in friction between the user and the insight. This creates a bad user experience.\n\nWe solved this by combining session state with passing the URL query parameters to Streamlit apps. In this post, I‚Äôll walk you through a minimum viable app. You‚Äôll learn:\n\nHow to get URL query parameters into Streamlit\nHow to use those parameters in Streamlit widgets\nHow to sync widgets and your app‚Äôs URLs\nHow to get URL query parameters into Streamlit\n\nThis part is the easiest!\n\nStreamlit has a feature called st.experimental_get_query_parameters (read more about it in our docs). It returns a list of parameters that are already in the URL. For example, here is the URL of a locally deployed app http://localhost:8501/?my_name=tyler&month=may.\n\nThis URL has two parameters (my_name, month) with values (tyler, may). Or for an app deployed on Streamlit Community Cloud, the link https://share.streamlit.io/tylerjrichards/streamlit_goodreads_app/books.py/?is_checked=True has the parameter is_checked with the value True.\n\nThis code pulls the parameters from the URL and prints them out:\n\nimport streamlit as st\n\nmy_query_params = st.experimental_get_query_params()\nst.write(\"My Query Params:\")\nst.write(my_query_params)\n\n\nNow you can save this code in a Python file (mine is streamlit_example_app.py), run it with streamlit run streamlit_example_app.py , and get the URL parameters programmatically!\n\nSee how query parameters can be ‚Äúpassed‚Äù to your Streamlit app. üëÜ\n\nNOTE: You probably noticed the experimental_ prefix in our function call. That means it's a feature we're still working on or trying to understand, and it'll go through many iterations as we get feedback from the community. You can find more information about experimental features in our docs!\n\nHow to use those parameters in Streamlit widgets\n\nNow that you have the Streamlit URL parameters, you can use them to influence the Streamlit widgets we use in building apps.\n\nFor example, let‚Äôs create a URL parameter called is_checked and pass it to the value of a Streamlit checkbox. We return the query parameters as lists inside a dictionary, so you can use the get function to pull the parameter (if it exists), and use == \"true\" to make it a boolean that is passed into your checkbox value parameter:\n\nimport streamlit as st\n\nquery_params = st.experimental_get_query_params()\nmy_checkbox = st.checkbox(\n    \"Example Checkbox\",\n    value=query_params.get(\"is_checked\", [\"False\"])[0].lower() == \"true\",\n)\nst.write(query_params)\n\n\nNow if you go to http://localhost:8501/is_checked=True, you‚Äôll see that the checkbox is checked:\n\nSee how the checkbox value equals the query parameter. üëÜ\n\nThis is a great V0!\n\nBut what if you want the URL to change when you change widget inputs? Typing out URL parameters is annoying. Instead, take advantage of st.session_state and st.experimental_set_query_params().\n\nHow to sync widgets and your app‚Äôs URLs\n\nWe have lots of great documentation on st.session_state. Check it out if this is your first rodeo!\n\nAs a super basic introduction, st.session_state is a magic dictionary that doesn‚Äôt get reset every time the page updates. Use it to see how users interact with your app across all reruns. And if you want to change your URL parameters, use st.experimental_set_query_params()!\n\nFor this to work, you‚Äôll need to ‚Äòreset‚Äô your URL parameter at the end of your app with st.experimental_set_query_params and ensure you‚Äôre only reading from st.experimental_get_query_params on the first app run.\n\nTo do this, check if the is_check parameter is inside st.session_state. If it‚Äôs not, add it to st.session_state as we did above üëÜ ¬†to get the current query parameters from your URL:\n\nimport streamlit as st\n\nquery_params = st.experimental_get_query_params()\nif \"is_checked\" not in st.session_state:\n    st.session_state[\"is_checked\"] = (\n        query_params.get(\"is_checked\", [\"False\"])[0].lower() == \"true\"\n    )\nmy_checkbox = st.checkbox(\"Example Checkbox\", key=\"is_checked\")\nst.experimental_set_query_params(is_checked=my_checkbox)\nst.write(st.session_state)\n\n\nYou‚Äôll notice another change in the code.\n\nBefore, you had to assign the default value of the checkbox. Now you can use the key parameter to keep the checkbox synced with is_checked from inside st.session_state! Adding a key to a widget automatically creates a corresponding entry in st.session_state.\n\nThe final change is with the st.experimental_set_query_params function. Overwrite the is_checked variable with whatever was last in your checkbox. Query parameters and your Streamlit checkbox are now synchronized! üéâ\n\nWrapping up\n\nNow you can share your Streamlit apps in the right context and help users play with them!\n\nMassive thanks to Zachary Blackwood for teaching me much of what I know about st.session_state and for helping with the code. Also thanks to the folks in this Twitter thread who gave me the idea to write about how to use the suggested methods.\n\nI‚Äôd love to hear how you solve this problem and your ideas for the future of Streamlit. Find me on Twitter as @tylerjrichards and check out the forums to see what our vibrant community is creating.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Adrien Treuille - Streamlit",
    "url": "https://blog.streamlit.io/author/adrien/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Adrien Treuille\n11 posts\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nLLMs\nby\nAdrien Treuille and¬†\n1\n¬†more,\nJune 15 2023\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nProduct\nby\nAdrien Treuille and¬†\n2\n¬†more,\nMarch 2 2022\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\n\nStreamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.\n\nProduct\nby\nAdrien Treuille\n,\nNovember 2 2021\nAnnouncing Streamlit 1.0! üéà\n\nStreamlit used to be the simplest way to write data apps. Now it's the most powerful\n\nProduct\nby\nAdrien Treuille\n,\nOctober 5 2021\nStreamlit gains a major new spell book\n\nA tome to the magical fields of Python, algorithms, visualization, and machine learning\n\nProduct\nby\nAdrien Treuille\n,\nAugust 20 2021\nOur $35 million Series B\n\nWe‚Äôre excited to announce a new funding round led by Sequoia üå≤\n\nProduct\nby\nAdrien Treuille\n,\nApril 7 2021\nIntroducing Streamlit Sharing\n\nThe new Streamlit platform for deploying, managing, and sharing your apps\n\nProduct\nby\nAdrien Treuille\n,\nOctober 15 2020\nIntroducing Streamlit Components\n\nA new way to add and share custom functionality for Streamlit apps\n\nProduct\nby\nAdrien Treuille\n,\nJuly 14 2020\nAnnouncing Streamlit's $21M Series¬†A\n\nDeveloping new superpowers for the data science community\n\nProduct\nby\nAdrien Treuille\n,\nJune 16 2020\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sample_downloadbutton.png (885√ó412)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_downloadbutton.png#border",
    "html": ""
  },
  {
    "title": "sample_swot_slide.png (1354√ó757)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_swot_slide.png#border",
    "html": ""
  },
  {
    "title": "Copy-of-Flowchart-1.jpg (1324√ó340)",
    "url": "https://blog.streamlit.io/content/images/2023/08/Copy-of-Flowchart-1.jpg",
    "html": ""
  },
  {
    "title": "sample_logo_slide-1.png (1270√ó688)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_logo_slide-1.png#border",
    "html": ""
  },
  {
    "title": "workflow_instant_insight.jpg (2000√ó849)",
    "url": "https://blog.streamlit.io/content/images/2023/08/workflow_instant_insight.jpg",
    "html": ""
  },
  {
    "title": "App Deployment Platform | Share Apps Using Streamlit",
    "url": "https://blog.streamlit.io/deploying-streamlit-apps-using-streamlit-sharing/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDeploying Streamlit apps using Streamlit sharing\n\nA sneak peek into Streamlit's new deployment platform\n\nBy Tyler Richards\nPosted in Tutorials, October 15 2020\nStreamlit background\nStreamlit sharing\nConclusion\nContents\nShare this post\n‚Üê All posts\n\nThis is a community piece that originally appeared on Towards Data Science - to see the original article click here.\n\nOver the past couple of weeks, I‚Äôve been playing around with a new Streamlit feature called Streamlit sharing, which makes it super easy to deploy your custom apps. I‚Äôm going to go through a bit of background first, so if you want to see the docs for Streamlit sharing to get started you can find them here.\n\nStreamlit background\n\nFor a bit of background, Streamlit is a framework that lets you quickly and confidently turn a python script into a web app and is an incredible tool for data scientists working on teams where they need to quickly share a model or an interactive analysis, or for data scientists working on personal projects they want to show the world. Here‚Äôs a Streamlit beginner tutorial if you want to try it out!\n\nI‚Äôve been using Streamlit for the past ~6 months, and it‚Äôs been so useful. Previously, if I knew I wanted to make a web app at the end of a project, I would always opt to switch to R for the wonderful R shiny framework, even though I am a much better python programmer than an R one. Going through Django or flask is just so much development friction to take on that it‚Äôs rarely worth it for a personal project and always takes too long for anything at work. But after using Streamlit, I now not only had options but found myself preferring python+Streamlit to R+shiny.\n\nStreamlit sharing\n\nThis brings me to a couple of months ago. I started a DS project focused on analyzing reading habits using data from the Goodreads app. I decided to try Streamlit out, and it turned a multi-day long process of getting a Django/flask app running well locally into one that took around a half-hour for local Streamlit use. It really is as easy as throwing your analysis into a script, and calling Streamlit functions whenever you want to put a graph, widget, or text explainer on the app.\n\nHowever, the most annoying process on Streamlit was the deployment and management process. The tutorial I followed was straightforward, and didn‚Äôt take that much time, but was fairly extensive. It required launching an ec2 instance, configuring SSH, using tmux, and going back to this terminal every time you wanted to change anything about your web app. It was doable but annoying.\n\nA few weeks ago, Streamlit saw my Goodreads app and asked if I wanted to test out their Streamlit sharing beta, which was supposed to remove the friction explained above. I, obviously, gave it a shot.\n\nAll I had to do was:\n\nPush my app to a Github repo\nAdd a requirements.txt file that listed all the python libraries I used\nPoint Streamlit to my app via the link to the repository\nClick Deploy\n\nIt genuinely was that easy to figure out. I had sectioned off a couple of hours to figure it out, as I expected various bugs to pop up (it is in beta!), but it took me fewer than 10 minutes to get it up and running.\n\nI currently have three apps running, one is a test app, the second is the Goodreads book recommendation app I mentioned earlier, and the third is an interactive analysis of a tech survey that I spun up (from idea to functioning and deployed web app) in around an hour and a half.\n\nSwitching to Streamlit sharing has also saved me the ~$5 a month AWS bill, which I would gladly pay for this feature just for the savings in time spent on deployment alone.\n\nIf I wanted to try out a new app, I could just click the new app button, point it to my repo, and they would handle literally everything else.\n\nIf your Streamlit app uses any other packages, make sure to include a requirements.txt file in your repo ‚Äî otherwise you‚Äôll immediately get an error when deploying. You can use something like pip freeze to get requirements but that will give you all of the packages in the environment including those that you don‚Äôt use in your current project. And that will slow down your app deployment! So I‚Äôd suggest using something like pipreqs to keep it to just the core requirements for your app.\n\n\npip install pipreqs\npipreqs /home/project/location\n\n\n\nIf you have requirements for apt-get, add them to packages.txt -, one package per line.\n\nConclusion\n\nSo as a wrap-up, Streamlit sharing has saved me $ on both a development time saved and hosting cost basis (shoutout to the VC funds that make this all possible), has made my personal projects more interactive and prettier, and has taken away the headaches of deploying quick models or analyses. No wonder I‚Äôm a Streamlit fan.\n\nWant to see more of this content? You can find me on Twitter, Substack, or on my portfolio site.\n\nHappy Stream(lit)ing!\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sample_peers_slide-1.png (1270√ó510)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_peers_slide-1.png#border",
    "html": ""
  },
  {
    "title": "SimiLo: Find your best place to live",
    "url": "https://blog.streamlit.io/similo-find-your-best-place-to-live/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSimiLo: Find your best place to live\n\nA 5-step guide on how I built an app to relocate within the U.S.\n\nBy Kevin Soderholm\nPosted in Advocate Posts, August 4 2023\n1. Wrangle your data\n2. Strategize in users' shoes\n3. Methods to your madness\n4. Delight with your design\n4. 1. Keep it organized\n4.2. Display minimal information\n4.3. Avoid redundant features\n4.4. Create a custom color scheme\n4.5. Improve your app's design\n5. Don't forget your ethics\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Kevin Soderholm, and I'm a data scientist in the banking industry. My two favorite things about data science are:\n\nBrainstorming. I enjoy breaking down pain points and identifying analytical solutions. It's like a game that keeps me coming back for more.\nBuilding. Whether it's an ML model, a data dashboard, or a Streamlit app, I love starting with nothing and bringing something to life with every line of code.\n\nRecently, I've been searching for a bigger home in a suburb around the Twin Cities (Minneapolis/St. Paul) for my growing family. After spending hours on it, I realized I didn't know how to find the perfect location. I could describe what I wanted down to the square footage and the number of bedrooms, but I wasn't sure WHERE I wanted to go or HOW to decide.\n\nNone of the apps out there helped me pick a place on a map, so I decided to build my own. I called it SimiLo (for Similar Locations) and realized that it could also be used to search for vacation spots, do market research, and learn in general!\n\nIn this post, I'll show you how I built SimiLo step-by-step:\n\nWrangle your data\nStrategize in users' shoes\nMethods to your madness\nDelight with your design\nDon't forget your ethics!\n\n\nüìç\nTLDR: Here's the app and the code. Enjoy!\n1. Wrangle your data\n\nIf you worked in analytics outside of academia, you know how long it takes to wrangle data. You have to do data cleaning, integration, and transformation‚Äîall crucial for analysis. Plus, sourcing data for a personal project can be frustrating. No one will guide you to a database, table, or field. You must determine the type of data that fits your solution and search for it. And unless you have a budget, you'll need to find a free, publicly available source, which can limit your options.\n\nFortunately, there is plenty of government data available at various geographic levels. And you can also use ChatGPT.\n\nFor example:\n\nSometimes ChatGPT may lead to broken links or outdated information, but other times it'll point you exactly to what you need. Once you gather enough datasets, combine them across geographic levels of zip, city, and county.\n\nThere are two ways to do it:\n\nUse the U.S. Department of Housing and Urban Development's \"Cross-walk\" files for mapping zip codes to counties. It's challenging since zip codes cross county lines, resulting in a many-to-many merge that can be handled using population density for tie-breakers (read more here).\nMap zip codes to cities using the U.S. Postal Service city designations. USPS cities often lump together nearby municipalities. For example, some suburbs around the Minneapolis‚ÄîSt. Paul metro area all roll up to the USPS city of St. Paul. This is the only clean way to roll up zip codes to cities, and the city is an important design component. Why? Because people think about cities, not zip codes.\n\nHave you got your datasets? It's time to explore, clean, and create! (This is the most fun part of data wrangling.)\n\nStart by examining your datasets from top to bottom and inside and out‚Äîview the distributions of all relevant fields, interpret the values, explore relationships, etc. In the real world, data is rarely clean and requires many small changes such as feature engineering, missing imputations, potential outlier treatment, data transformations, and so on (the list may never end until you throw in the towel and call it good enough).\n\nHere is an example of data wrangling:\n\n#read  and clean water file\nwf = pd.read_csv('/folder/water.txt', delimiter=None, dtype={'GEOID': str})\n\n# remove extra blank space from last column of dataframe\nwf.iloc[:, -1] = wf.iloc[:, -1].str.strip()\n\n#assign delimiter\nwf = wf.iloc[:, -1].str.split('\\\\t', expand=True)\n\n#rename cols\nwfcols=['ZCTA5','ALAND','AWATER','ALAND_SQMI','AWATER_SQMI','LAT','LON']\nwf = wf.rename(columns=dict(zip(wf.columns, wfcols)))\n\n#change data type to float (numeric with decimal precision)\nwf['ALAND'] = wf['ALAND'].astype(float)\nwf['AWATER'] = wf['AWATER'].astype(float)\nwf['ALAND_SQMI'] = wf['ALAND_SQMI'].astype(float)\nwf['AWATER_SQMI'] = wf['AWATER_SQMI'].astype(float)\n\n#create features\nwf['Pct_Water']=wf['AWATER']/wf['ALAND']\nwf['Tot_Area']=wf['ALAND_SQMI']+wf['AWATER_SQMI']\n\n#filter cols\nwf=wf[['ZCTA5','LAT','LON','Pct_Water','Tot_Area']]\n\n\nNow your datasets are clean, curated, and ready for Streamlit ingestion. But first‚Ä¶ you'll need to strategize.\n\n2. Strategize in users' shoes\n\nBefore building your app, do a strategy session. Brainstorm. Think about the user's perspective. This will shape the app's functionality and design.\n\nThis is what changed SimiLo. Initially, I wanted the user to select the location criteria so the app would give them a ranked locations list. But it was clunky and confusing. What criteria combination would help them find a specific type of location? I was selecting values that were too extreme and produced unsatisfactory results. This led me to COMPARE locations rather than FILTER them. It was more effective to start with a location I knew and see similar locations across the U.S.\n\nThis workflow was more fun, the results clearer, and the approach more intuitive:\n\nI also added the ability to save searches and carry out further research. While the app generates similar locations, it makes a table with a free-form text field. You can manipulate it and download it as a CSV file.\n\nTo implement this feature, use the experimental data editor function:\n\nedited_df=st.experimental_data_editor(df)\nsave=edited_df[edited_df['SAVE']==True].reset_index()\ncsv = convert_df(save[cols+['SAVE','NOTES']])\nst.download_button(label=\"Download Selections as CSV\",data=csv,file_name='SIMILO_SAVED.csv',mime='text/csv',)\n\n\nNow let's find and apply the analytical methods to make the machine work!\n\n3. Methods to your madness\n\nIf you love learning new methods and adding them to your tool belt, you'll enjoy this step the most. I most definitely did, as I had to learn similarity scoring methods to answer these two questions:\n\nWhich key metrics represented a location and could be used for comparison? I combed through my datasets, selected 20, and split them into four categories: People, Home, Work, and Environment.\n\nHow to prepare the data, calculate similarity, and present the results to the user? I tried many iterations with different data transformations and similarity methods, including Euclidean distance, Cosine, and Jaccard similarity, and settled on a 3-step process for each:\n\nNormalize the values for each metric, putting them on the same scale.\nCalculate the Euclidean distance between those values for a selected location and the values for every other location in your dataset.\nScale the calculated distances into a score from 1-100, creating an easy-to-interpret similarity ranking for each data category.\n\n\n#Columns to normalize\npeople_cols_to_norm = [‚ÄòA‚Äô,‚ÄôB‚Äô,‚ÄôC‚Äô,‚ÄôD‚Äô,‚ÄôE‚Äô]\n\n#New columns\nscaled = ['A_sc','B_sc','C_sc','D_sc','E_sc']\n\n#Normalization\nscaler = StandardScaler()\ndf[scaled] = scaler.fit_transform(df[people_cols_to_norm])\n  \n#Calculate the euclidian distance between the selected record and the rest of the dataset\npeople_dist = euclidean_distances(df.loc[:,scaled], selected_record[scaled].values.reshape(1, -1))\n\n#Create a new dataframe with the similarity score and the corresponding index of each record\ndf_similarity = pd.DataFrame({'PEOPLE_SIM': people_dist [:, 0], 'index': df.index})\n\n#scale distance to 1-100 score\npeople_max=df_similarity['PEOPLE_SIM'].max()\ndf_similarity['PEOPLE_SCORE']  = 100 - (100 * df_similarity['PEOPLE_SIM'] / people_max)\n\n\nAt this point, you have four similarity scores ranging from 1-100 for each data category and location: People, Home, Work, and Environment.\n\nTo have a single comparison metric create an overall similarity score calculated as a weighted average of the individual category distances, scaled to a score from 1-100. Now you have an easy-to-interpret way to rank locations based on their similarity to the user's selected location:\n\nOh, and one more thing. In Advanced Settings, in the Data Category Importance section, users can increase or decrease the impact of any individual data category using slider widgets. These adjustments dynamically update the weights of each category, which are used to calculate the overall similarity score:\n\nWell done! You have learned how to build the app. But don't forget about design.\n\n4. Delight with your design\n\nLet's face it. Many of us data scientists lack free-spirited creativity. We're logical and analytical and don't often use the right side of our brain. Streamlit allows you to flex those forgotten muscles and your artistic side. Plus, neat organization and minimalism will make your app look simple, even if it's hiding lots of information and complex algorithms.\n\n4. 1. Keep it organized\n\nTo help users process different pieces of information one at a time and keep it organized, use containers and dividers:\n\n4.2. Display minimal information\n\nTo avoid overwhelming the user, display minimal information on the main search page. The user will see the extended workflow ONLY after making a selection. Present the data in separate tabs with the options and the instructions hidden behind expanders so the user can uncover information in small increments vs. all at once.\n\n4.3. Avoid redundant features\n\nTo avoid redundant features, mix up the input widgets: buttons, radio buttons, checkboxes, selectboxes, multiselects, sliders, and number inputs. Streamlit offers diverse components for creating a unique app‚Äîexplore the different layouts and get creative!\n\n4.4. Create a custom color scheme\n\nCreate a custom color scheme to delight users with your app's aesthetics. Using the same color scheme for multiple apps can make them look alike (for SimiLo, I experimented with dozens of hex color code combinations). Simply use config.toml file functionality to control the background, the chart color scales, and the font colors:\n\n[theme]\nprimaryColor=\"#3fb0e8\"\nbackgroundColor=\"#192841\"\nsecondaryBackgroundColor=\"#3fb0e8\"\ntextColor=\"#f9f9f9\"\n\n4.5. Improve your app's design\n\nTo improve your app's design even further, embed animations or videos. This adds a level of professionalism that doesn't go unnoticed. I embedded a tutorial video with animations from LottieFiles, an open-source animation file format that you can add to Streamlit (learn more here):\n\nNow you have a fully functional, beautiful app. Time to publish? Not so fast. Don't forget to check your ethics!\n\n5. Don't forget your ethics\n\nYou might wonder why you should check your ethics when you know you're a good person with good morals and intentions. Well, good for you. The question is not about YOUR ethics; it's about you mitigating the possibility of your app being USED unethically.\n\nIn the case of SimiLo, I made two critical decisions:\n\nApp's functionality. Should I let the user hand-select the criteria and get a list of locations, or should I let them first pick a location and get a list of comparisons? Choosing the latter helped close the door to some unethical use cases. For example, I didn't want to create an app where someone could pick locations based on the race or income of its inhabitants.\nThe data used in the app. I purposefully omitted some demographic data, such as race, because it didn't add value and opened the door for unethical use. I carefully considered what data to show and what elements to use in my similarity scoring process.\n\nThere is a lot of buzz about the ethical use of AI. I believe that data scientists are key players in promoting ethical use and preventing unethical use because they understand their product the best. Third-party risk teams often don't have the domain knowledge about the methods or data to properly evaluate ta data science solution'sfairness, bias, and ethics. So before you hit publish, think about how your app COULD be used, not just how it SHOULD be used‚Äîyou might find some open doors worth shutting. üö™\n\nWrapping up\n\nLet's summarize the 5-step process I used to build SimiLo. Once I defined the problem (I wanted to move) and expanded my proposed solution (moving, vacationing, market research), I started with Step 1‚Äîdata wrangling and data sourcing with ChatGPT, exploratory data analysis (EDA), cleaning, transformations, and feature engineering. In Step 2, I hit the whiteboard and strategized the app components from the users' perspective. Step 3 was researching and applying similarity scoring methods in a framework that worked best for my app. Step 4 was to make my app look and feel appealing through various design strategies. Finally, in Step 5, I checked the ethical considerations before publishing.\n\nJust because this article is linear doesn't mean real life is. You don't need to do these steps in order. In some cases, you can do them in tandem, in others, as part of an iterative rotation, but in most cases, you'll revisit each step several times.\n\nI hope this post helped you find a gem or two to take to your next app. If you have any questions, please post them in the comments below or contact me on LinkedIn or Twitter.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Auto-generate a dataframe filtering UI in Streamlit with filter_dataframe!",
    "url": "https://blog.streamlit.io/auto-generate-a-dataframe-filtering-ui-in-streamlit-with-filter_dataframe/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nBy Tyler Richards, Arnaud Miribel and Zachary Blackwood\nPosted in Tutorials, August 18 2022\nWhat is filter_dataframe?\nCode Section 1. Laying out Streamlit widgets\nCode Section 2. Preparing the input dataframe for filtering\nCode Section 3. Writing conditionals for different column types\nBringing it all together\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nStreamlit apps often have some sort of a filtering component where developers write code snippets that combine Streamlit inputs, st.dataframe, or the ever-popular streamlit-aggrid component to create easy-to-use UIs.\n\nWe on the Streamlit data science team certainly have followed this trend with the following format:\n\nUse a set of Streamlit native inputs like st.date_range or st.selectbox to get user input.\nUse the inputs to filter our data and display it back to the user.\n\nWriting this type of app the first time was fun. But doing it the fifth time‚Äîwriting different code iterations, figuring out what Streamlit widget fits what data type, configuring it all together‚Äîgot tiring pretty quickly. To solve this, we built a function filter_dataframe that handles this for us automatically!\n\nIn this post, we‚Äôll show you how filter_dataframe works section by section:\n\nCode Section 1. Laying out Streamlit widgets\n\nCode Section 2. Preparing the input dataframe for filtering\n\nCode Section 3. Writing conditionals for different column types\n\nWant to dive right in? Head over to our demo app see it on an example dataframe, and see the full code here.\n\nWhat is filter_dataframe?\n\nThe functionfilter_dataframe lets you:\n\nAdd a filtering UI to any dataframe\nSpeed up the development time\nAllow the user to explore a dataset in a self-service way\n\nHere is the code for it:\n\nfrom pandas.api.types import (\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_numeric_dtype,\n    is_object_dtype,\n)\nimport pandas as pd\nimport streamlit as st\n\n\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds a UI on top of a dataframe to let viewers filter columns\n\n    Args:\n        df (pd.DataFrame): Original dataframe\n\n    Returns:\n        pd.DataFrame: Filtered dataframe\n    \"\"\"\n    modify = st.checkbox(\"Add filters\")\n\n    if not modify:\n        return df\n\n    df = df.copy()\n\n    # Try to convert datetimes into a standard format (datetime, no timezone)\n    for col in df.columns:\n        if is_object_dtype(df[col]):\n            try:\n                df[col] = pd.to_datetime(df[col])\n            except Exception:\n                pass\n\n        if is_datetime64_any_dtype(df[col]):\n            df[col] = df[col].dt.tz_localize(None)\n\n    modification_container = st.container()\n\n    with modification_container:\n        to_filter_columns = st.multiselect(\"Filter dataframe on\", df.columns)\n        for column in to_filter_columns:\n            left, right = st.columns((1, 20))\n            # Treat columns with < 10 unique values as categorical\n            if is_categorical_dtype(df[column]) or df[column].nunique() < 10:\n                user_cat_input = right.multiselect(\n                    f\"Values for {column}\",\n                    df[column].unique(),\n                    default=list(df[column].unique()),\n                )\n                df = df[df[column].isin(user_cat_input)]\n            elif is_numeric_dtype(df[column]):\n                _min = float(df[column].min())\n                _max = float(df[column].max())\n                step = (_max - _min) / 100\n                user_num_input = right.slider(\n                    f\"Values for {column}\",\n                    min_value=_min,\n                    max_value=_max,\n                    value=(_min, _max),\n                    step=step,\n                )\n                df = df[df[column].between(*user_num_input)]\n            elif is_datetime64_any_dtype(df[column]):\n                user_date_input = right.date_input(\n                    f\"Values for {column}\",\n                    value=(\n                        df[column].min(),\n                        df[column].max(),\n                    ),\n                )\n                if len(user_date_input) == 2:\n                    user_date_input = tuple(map(pd.to_datetime, user_date_input))\n                    start_date, end_date = user_date_input\n                    df = df.loc[df[column].between(start_date, end_date)]\n            else:\n                user_text_input = right.text_input(\n                    f\"Substring or regex in {column}\",\n                )\n                if user_text_input:\n                    df = df[df[column].astype(str).str.contains(user_text_input)]\n\n    return df\n\n\nNow let‚Äôs take a look at how it works!\n\nCode Section 1. Laying out Streamlit widgets\n\nThe filter_dataframe function inputs and outputs the same thing‚Äîa pandas dataframe. Within the function, we first ask the user if they‚Äôd like to filter the dataframe with a checkbox called modify.\n\nWe also added comments and type hints to the top of the function to make the code more digestible:\n\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds a UI on top of a dataframe to let viewers filter columns\n\n    Args:\n        df (pd.DataFrame): Original dataframe\n\n    Returns:\n        pd.DataFrame: Filtered dataframe\n    \"\"\"\n    modify = st.checkbox(\"Add filters\")\n\n    if not modify:\n        return df \n\nCode Section 2. Preparing the input dataframe for filtering\n\nThere are several steps you need to take to prep your dataframe for your app. For the first three you need to:\n\nMake a copy of the pandas dataframe so the user input will not change the underlying data.\nAttempt to cast string columns into datetimes with pd.to_datetime().\nLocalize your datetime columns with .tz_localize(). The Streamlit date picker (which you‚Äôll use later!) returns dates without a timezone, so you need to take this step to compare the two:\ndf = df.copy()\n\n# Try to convert datetimes into a standard format (datetime, no timezone)\nfor col in df.columns:\n    if is_object_dtype(df[col]):\n    \ttry:\n    \t    df[col] = pd.to_datetime(df[col])\n    \texcept Exception:\n    \t    pass\n\n    if is_datetime64_any_dtype(df[col]):\n        df[col] = df[col].dt.tz_localize(None)\n\n\nNow that your data is in a better format, you need to:\n\nSet up a container with st.container for your filtering widgets.\nUse st.multiselect to let the user select the columns:\nmodification_container = st.container()\nwith modification_container:\n    to_filter_columns = st.multiselect(\"Filter dataframe on\", df.columns)\n\nLoop through each column and deal with each one depending on its type. You‚Äôll write conditionals for each one next!\nAdd indentation and an arrow to improve the aesthetics when users select many columns.\nfor column in to_filter_columns:\n    left, right = st.columns((1, 20))\n    left.write(\"‚Ü≥\")\n\n\nAll your data is in the right format. You ensured that your original dataset will remain untouched, and you have prepared your loop to go through all your columns. Now comes the fun part!\n\nCode Section 3. Writing conditionals for different column types\n\nIn this function, you‚Äôll want to check for three pandas data types‚Äîcategorical, numeric, and datetime‚Äîthen handle the rest as if they‚Äôre strings. This is an assumption that works well for us. Your situation might be different, so feel free to add your own conditionals to this list.\n\nFor each one create a Streamlit widget that matches your type, then filter your data based on that widget. At the end of this loop, you‚Äôll have to return the entire filtered dataframe.\n\nLet‚Äôs take a look at them one by one.\n\nCategorical types\n\nCheck for categorical types with the is_categorical_dtype function. Often users don‚Äôt cast their data into this type, so assume that anything with fewer than 10 unique values acts like a categorical dtype. As a bonus, it‚Äôll work great with boolean columns (which only have True or False values!).\n\nNow, create a multiselect widget with possible values and use it to filter your dataframe:\n\n# Treat columns with < 10 unique values as categorical\nif is_categorical_dtype(df[column]) or df[column].nunique() < 10:\n    user_cat_input = right.multiselect(\n        f\"Values for {column}\",\n        df[column].unique(),\n        default=list(df[column].unique()),\n    )\n    df = df[df[column].isin(user_cat_input)]\n\n\nNumeric types\n\nNumeric types are fairly straightforward. You can get the minimum and the maximum from the dataset itself, then assume that the step function is 1% of the range and filter the data accordingly:\n\nelif is_numeric_dtype(df[column]):\n    _min = float(df[column].min())\n    _max = float(df[column].max())\n    step = (_max - _min) / 100\n    user_num_input = right.slider(\n      f\"Values for {column}\",\n      min_value=_min,\n      max_value=_max,\n      value=(_min, _max),\n      step=step,\n    )\n    df = df[df[column].between(*user_num_input)]\n\nDatetime types\n\nThe datetime dtype is almost the same as the other two. You get the user input with the st.date_input function. Once the user enters two dates, you can filter your dataset:\n\nelif is_datetime64_any_dtype(df[column]):\n    user_date_input = right.date_input(\n        f\"Values for {column}\",\n        value=(\n            df[column].min(),\n            df[column].max(),\n        ),\n    )\n    if len(user_date_input) == 2:\n        user_date_input = tuple(map(pd.to_datetime, user_date_input))\n        start_date, end_date = user_date_input\n        df = df.loc[df[column].between(start_date, end_date)]\n\n\nOther types\n\nWe like to convert other dtypes into a string, then let the user search within them for substrings. It might not work for your use case, but for us, it works quite well:\n\nelse:\n    user_text_input = right.text_input(\n        f\"Substring or regex in {column}\",\n    )\n    if user_text_input:\n        df = df[df[column].astype(str).str.contains(user_text_input)]\n\nBringing it all together\n\nWant to see what the code looks like in action? Go ahead and test it on the palmerpenguins dataset (see this GitHub repo for the data) or on your own data!\n\nWe‚Äôve made an example app using the code (check it out below):\n\nimport pandas as pd\nimport streamlit as st\nimport streamlit.components.v1 as components\nfrom pandas.api.types import (\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_numeric_dtype,\n    is_object_dtype,\n)\n\nst.title(\"Auto Filter Dataframes in Streamlit\")\nst.write(\n    \"\"\"This app accomodates the blog [here](<https://blog.streamlit.io/auto-generate-a-dataframe-filtering-ui-in-streamlit-with-filter_dataframe/>)\n    and walks you through one example of how the Streamlit\n    Data Science Team builds add-on functions to Streamlit.\n    \"\"\"\n)\n\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    <insert the code we wrote here>\n\ndata_url = \"https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv\"\n\ndf = pd.read_csv(data_url)\nst.dataframe(filter_dataframe(df))\n\nThis code will produce the following app:\n\nWrapping up\n\nYou did it! Now you know how to set up your own filter_dataframe function.\n\nIf you have any questions or improvements, please drop them in the comments below or make a suggestion on our GitHub repository for this post.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "sample_financials_slide-1.png (1213√ó731)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_financials_slide-1.png#border",
    "html": ""
  },
  {
    "title": "sample_overview_slide.png (1346√ó761)",
    "url": "https://blog.streamlit.io/content/images/2023/08/sample_overview_slide.png#border",
    "html": ""
  },
  {
    "title": "streamlit-for-data-science-book.png (1346√ó1660)",
    "url": "https://blog.streamlit.io/content/images/2023/09/streamlit-for-data-science-book.png#border",
    "html": ""
  },
  {
    "title": "Screenshot-2023-10-04-at-11.16.52-AM.png (1948√ó1036)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-04-at-11.16.52-AM.png#browser",
    "html": ""
  },
  {
    "title": "How to build your own Streamlit component",
    "url": "https://blog.streamlit.io/how-to-build-your-own-streamlit-component/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build your own Streamlit component\n\nLearn how to make a component from scratch!\n\nBy Zachary Blackwood\nPosted in Tutorials, September 15 2022\nWhat‚Äôs a Streamlit component?\nBuild a basic component\nStep 1. Use cruft to create the boilerplate code\nStep 2. Add basic HTML\nStep 3. Add JavaScript to return the data when a key is pressed\nStep 4. Set up your Python code\nStep 5. Add CSS for styling\nPublish your component for others to use\nStep 1. Push your code to GitHub\nStep 2. Create a PyPI account and make an API token\nStep 3. Create a release on GitHub (it‚Äôll trigger a release to PyPI)\nStep 4. Go to the Actions tab to see if the release succeeded, then test it!\nBonus tips:\nWrapping up\nResources\nContents\nShare this post\n‚Üê All posts\n\nWant to build your first Streamlit component but feel too intimidated to get started? We‚Äôve got you covered.\n\nIn this post, you‚Äôll learn how to:\n\nBuild a basic component\nPublish your component for others to use\n\nWant to dive right in? Here‚Äôs the repository for an example component and an app showing it in action.\n\nBut first, let‚Äôs do a quick refresher on terminology‚Ä¶\n\nüëâ\nNOTE: This post explains a very simple way of building components. If you want a more sophisticated way that includes npm and TypeScript, check out the official components template.\nWhat‚Äôs a Streamlit component?\n\nA Streamlit component is a shareable Streamlit plugin that lets you add new visuals and interactivity to apps (read more in our docs).\n\nWhy would you want to use or build components? Because although Streamlit has a ton of built-in functionality, sometimes you might want to add visualization or interactivity to your apps that‚Äôs not available in Streamlit (yet) and then share it with the community.\n\nThere are three types of components:\n\nPython-only components: Python code that doesn‚Äôt require custom HTML/JavaScript.\nStatic components: Python + HTML/JavaScript that can be embedded in your app.\nBidirectional components: Python + HTML/JavaScript that can be embedded in your app and can return data back to it.\n\nWondering what components to build? Here are some ideas:\n\nA new baseweb component that‚Äôs not available in Streamlit.\nA clickable graph that lets you use the clicked value in your app.\nA text input box that returns the value as you type‚Äîno ‚Äúenter‚Äù required (this is what we‚Äôll be building together in this post).\nCheck out the community components tracker to see what sorts of components have already been built.\nBuild a basic component\nStep 1. Use cruft to create the boilerplate code\n\nCruft gives you pre-existing project templates. You can install it with pip, pipx, or brew, then run this command: cruft create https://github.com/blackary/cookiecutter-streamlit-component/.\n\nNext, put in your component's details:\n\nauthor_name [Bob Smith]: Zachary Blackwood\nauthor_email [bob@example.com]: example@example.com            \nproject_name [Streamlit Component X]: Streamlit Keyup\npackage_name [streamlit-keyup]: streamlit-keyup\nimport_name [streamlit_keyup]: st_keyup \ndescription [Streamlit component that allows you to do X]: A streamlit component that allows you to get input from a textbox after every key press\ndeployment_via_github_actions [y]: y\nSelect open_source_license:\n1 - MIT license\n2 - BSD license\n3 - ISC license\n4 - Apache Software License 2.0\n5 - GNU General Public License v3\n6 - Not open source\nChoose from 1, 2, 3, 4, 5, 6 [1]: 1\nüí°\nNOTE: Call your component whatever you want. But if you want it to be easily publishable on PyPI through GitHub actions, put y for the deployment_via_github_actions question. It‚Äôs the default value, so you can just press <enter> when you have to accept it.\n\nThis template will create the following structure:\n\nstreamlit-keyup\n‚îú‚îÄ‚îÄ .cruft.json\n‚îú‚îÄ‚îÄ .github\n‚îÇ   ‚îî‚îÄ‚îÄ workflows\n‚îÇ       ‚îî‚îÄ‚îÄ publish_PYPI_each_tag.yml\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ MANIFEST.in\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ setup.py\n‚îî‚îÄ‚îÄ src\n    ‚îî‚îÄ‚îÄ st_keyup\n        ‚îú‚îÄ‚îÄ __init__.py\n        ‚îî‚îÄ‚îÄ frontend\n            ‚îú‚îÄ‚îÄ index.html\n            ‚îú‚îÄ‚îÄ main.js\n            ‚îú‚îÄ‚îÄ streamlit-component-lib.js\n            ‚îî‚îÄ‚îÄ style.css\n\nYou‚Äôd generate this layout if you were to release this package as streamlit-keyup (imported as import st_keyup), so these are the values for package_name and import_name.\n\nStep 2. Add basic HTML\n\nCruft will make a file for you at src/st_keyup/frontend/index.html.\n\nJust add new tags in the body:\n\n\n<body>\n  <div id=\"root\">\n    <label id=\"label\" for=\"text_input\">This is a label</label>\n    <div class=\"input\">\n\t  <input type=\"text\" name=\"text_input\" id=\"input_box\" />\n    </div>\n  </div>\n</body>\n\nNext, run streamlit run src/st_keyup/__init__.py. Since the boilerplate __init__.py file has some content, this is what you‚Äôll see:\n\nNothing fancy yet, but you‚Äôre getting there!\n\nStep 3. Add JavaScript to return the data when a key is pressed\n\nThe frontend/ folder has a main.js file with most of the standard code. Add code only inside the onRender function and the if (!window.rendered) block.\n\nThe code will do the following:\n\n1. Get the values that the user has specified when using the component (label and value):\n\nconst {label, value} = event.detail.args;\n\n\n2. Get the <label> tag from your HTML page and set the text to the user‚Äôs label:\n\nconst label_el = document.getElementById(\"label\")\nlabel_el.innerText = label\n\n\n3. Get the <input> tag from your HTML page and‚Äîif the user specified a default value for the input‚Äîset the starting value to that:\n\nconst input = document.getElementById(\"input_box\");\nif (value) {\n  input.value = value\n}\n\n\n4. Whenever there is a ‚Äúkeyup‚Äù event on the input tag (meaning, the user hit a key), send the current input value back to the component:\n\ninput.onkeyup = event => sendValue(event.target.value)\n\n\nThe final onRender function will look like this:\n\nfunction onRender(event) {\n  // Only run the render code the first time the component is loaded.\n  if (!window.rendered) {\n    // Grab the label and default value that the user specified\n    const {label, value} = event.detail.args;\n\n    // Set the label text to be what the user specified\n    const label_el = document.getElementById(\"label\")\n    label_el.innerText = label\n\n    // Set the default value to be what the user specified\n    const input = document.getElementById(\"input_box\");\n    if (value) {\n      input.value = value\n    }\n\n    // On the keyup event, send the new value to Python\n    input.onkeyup = event => sendValue(event.target.value)\n\n    window.rendered = true\n  }\n}\n\n\n5. As an extra step, at the end of the script set the widget‚Äôs height to 85px:\n\nStreamlit.setFrameHeight(85)\n\n\nAt this point, your Python code isn‚Äôt properly passing the label and the value. To see the JavaScript pass back the typed value to Streamlit, run the app again and type in the input box:\n\nYou‚Äôre almost there!\n\nStep 4. Set up your Python code\n\nTo specify the label and the value correctly, update the st_keyup function in src/st_keyup/__init__.py:\n\ndef st_keyup(\n    label: str,\n    value: Optional[str] = \"\",\n    key: Optional[str] = None,\n):\n    \"\"\"\n    Create a Streamlit text input that returns the value whenever a key is pressed.\n    \"\"\"\n    component_value = _component_func(\n        label=label,\n        value=value,\n        key=key,\n        default=value\n    )\n\n    return component_value\n\n\nFor your demo Streamlit app to work, update the main function:\n\ndef main():\n    st.write(\"## Example\")\n    value = st_keyup(\"This is a label!\")\n\n    st.write(value)\n\n    st.write(\"## Example with value\")\n    value2 = st_keyup(\"With a default value!\", value=\"Default value\")\n\n    st.write(value2)\n\n\nRun streamlit run src/st_keyup/__init__.py and you‚Äôll see something like this:\n\nYour component is working!\n\nWant it to look like the built-in Streamlit textbox? Let‚Äôs add some CSS to it.\n\nStep 5. Add CSS for styling\n\nFiddling with CSS may take a while. You can use your browser‚Äôs DevTools to copy the CSS from the built-in Streamlit st.text_input to your style.css file (see an example of the final CSS here).\n\nAdd it and reload the page. Your app will look something like this:\n\nPublish your component for others to use\n\nIt‚Äôs fun to have a component for yourself, but it‚Äôs more useful to publish it as a PyPI package. There is a file called .github/workflows/publish_PYPI_each_tag.yml that tells GitHub to publish the latest version of your package to PyPI every time you make a new release.\n\nHere is how to make it happen:\n\nStep 1. Push your code to GitHub\n\nGo to https://github.com/new and read the instructions on how to create a new repository. Don‚Äôt add a README, a .gitignore, or a license as they‚Äôre already in your repository.\n\nStep 2. Create a PyPI account and make an API token\n\nGo to pypi.org and create an account, then go to https://pypi.org/manage/account/#api-tokens and create a new API token for this project. Copy the API key, go to your repo‚Äôs settings and choose secrets:\n\nAs it said in the PyPI instructions when you created the API key, set your PYPI_USERNAME as __token__ and put the token value in PYPI_PASSWORD.\n\nStep 3. Create a release on GitHub (it‚Äôll trigger a release to PyPI)\n\nOn the right side of your repo, click on the ‚ÄúReleases‚Äù link, then on ‚ÄúDraft a new release,‚Äù and choose a tag (in your setup.py, the version is set to 0.1.0, so make that your release tag with a v in front‚Äîlike v0.1.0).\n\nStep 4. Go to the Actions tab to see if the release succeeded, then test it!\n\nIf the initial release has succeeded, you‚Äôll see something like this:\n\nTo test if it worked, go to pypi.org and look for the new package. Then try doing pip install <your-package-name>.\n\nüí°\nNOTE: You won‚Äôt be able to release this exact package name on PyPI (because I already did it), but this should work when you release your own components. If you want to test releasing this package (or a dummy package), change the name in setup.py and use an account on test.pypi.org instead of pypi.org. Read more about using test.pypi.org here.\nBonus tips:\nDon‚Äôt forget to update the version in setup.py every time you make a new release.\nOnce you have a GitHub repository, add a URL argument in setup.py that points to it.\nAdd the streamlit-component tag to your GitHub repo.\nAnnounce your component on the Forum.\nWrapping up\n\nNow you know how to make your first component! Hopefully, it‚Äôll inspire you to create more components and to keep making Streamlit a better tool for building all sorts of apps.\n\nIf you have any questions, feel free to post them in the comments below or on the Forum.\n\nHappy Streamlit-ing! üéà\n\nResources\nIntroducing Streamlit components\nStreamlit Components, security, and a five-month quest to ship a single line of code\nDeveloping a streamlit-webrtc component for real-time video processing\nBuild knowledge graphs with the Streamlit Agraph component\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "app-diagram.png (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2023/08/app-diagram.png",
    "html": ""
  },
  {
    "title": "Improving healthcare management with Streamlit",
    "url": "https://blog.streamlit.io/improving-healthcare-management-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nImproving healthcare management with Streamlit\n\nHow to build an all-in-one analytics platform for small clinics\n\nBy Matteo Ballabio and Luca A Cappellini\nPosted in Advocate Posts, July 17 2023\nWhy create Mehedi?\n1. How to create a dynamic form as data entry\n2. How to create an interactive dashboard for patient satisfaction\n3. How to connect Google Sheets to Streamlit\n4. How to create a multi-page app\n5. How to create data manipulation techniques\n6. How to use ML algorithms to improve the prediction of goal metrics\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey community, üëã\n\nWe're three Italian guys born and raised in the charming Brianza and the bustling Milan: Matteo (a data analyst), Luca (a medical doctor), and Federico (a student). We live 10 kilometers apart, but LinkedIn has connected us. When we met in person for coffee ‚òï (yes, we're Italians!), we discovered a shared goal: to innovate the Italian healthcare system.\n\nIn Italy, the public healthcare system is funded through taxes. But due to high demand and long wait times, the existing coverage is inadequate, and the data for analysis is missing. So we created Mehedi (stands for \"Medical Health Data Insights\")‚Äîan integrated analytics platform for small clinics to improve decision-making and patient care:\n\nIn this post, we'll show you:\n\nHow to create a dynamic form as data entry\nHow to create an interactive dashboard for patient satisfaction\nHow to connect Google Sheets to Streamlit\nHow to create a multi-page app\nHow to create data manipulation techniques\nHow to use ML algorithms to improve the prediction of goal metrics\n\n\nü©∫\nThis post is rather long, so you can jump straight into the app and the code if you want.\nWhy create Mehedi?\n\nOver the last ten years, healthcare data has grown exponentially. This created an opportunity for:\n\nDirect impact analytics (clinical and research) to create a benefit for the patient.\nIndirect analytics (analytical tools or systems) to create a benefit for all stakeholders.\n\nWe wanted to display the data in a simple dashboard:\n\nThe shift towards \"Value-Based Care\" required new metrics to evaluate performance‚Äîto benefit patients, healthcare professionals, and providers. Patient satisfaction is important, but few tools collect patient experience metrics (not connected to traditional metrics for actionable insights).\n\nStreamlit was the perfect tool for Mehedi:\n\nIt connected multiple sources in a database tier\nIt elaborated on the data and logic in an app tier\nIt collected patient data and feedback through a survey and presents it beautifully (it can even be customized by medical service type!)\n\nHere is the app itself:\n\nAnd now, let's get to building it!\n\n1. How to create a dynamic form as data entry\n\nThe entry point to Mehedi is the patient experience survey. It has targeted questions and free text boxes for general suggestions, opinions, and feedback.\n\nLet's consider a scenario with three different dynamic forms that depend on a slider representing a patient's appointment time duration.\n\nTo get started, follow these steps:\n\nImport the necessary Streamlit library using import streamlit as st\nCreate a slider to capture the appointment time using appointment_time = st.slider(\"Select appointment duration\", min_value, max_value)\nBased on the value of the slider, use conditional statements to display the appropriate dynamic form:\n# Using a slider, you can choose the time available.\nslider = st.slider(label='Trascina lo slider', min_value=1,max_value=10, value=1, key='Form5')\n\n# ###FORM 1\nif slider<4:\n\t\tcol1,  col2 = st.columns([1, 0.60])\n\t\twith col1:\n\t\t    new_title = '<b style=\"font-family:serif; color:#FF0000; font-size: 40px;\">üìã MEDi Experience Form:</b>'\n\t\t    st.markdown(new_title, unsafe_allow_html=True)\n\t\t    st.info(\"‚û°Ô∏è 1. Come ha preso l'appuntamento?\")\n\t\t    cols = st.columns((1, 1))\n\t\t    #APPUNTAMENTO\n\t\t    var_a1 = cols[0].selectbox(\"Ho preso un appuntamento:\",  [\"Personalmente\",  \"Telefono\",  \"Sito Web\", \"E-mail\",  \"Tramite medico\",  \"Altro\"])\n\t\t    var_a2= cols[1].slider(\"Quanto √® soddisfatto della facilit√† di fissare un appuntamento?\", 1, 7, 1)\n\t\twith col2:\n\t\t\t\tDATA = [{\"taste\": \"APPUNTAMENTO\", \"Peso Area\": var_a2},\n\t\t            {\"taste\": \"ACCOGLIENZA\", \"Peso Area\": med_accoglienza},\n\t\t            {\"taste\": \"PROCEDURE\", \"Peso Area\": var_d2},\n\t\t            {\"taste\": \"RISULTATI\", \"Peso Area\": var_f3},\n\t\t            {\"taste\": \"ESPERIENZA\", \"Peso Area\": med_experience}]\n\t\t    graph_pes(DATA)\n# ###FORM 2\nelif slider>3 and slider<8:\n\t\t## add the questions for form 2\n# ###FORM 3\nelif slider>7:\n\t\t## add the questions for form 3\n\n\nWhy not use st.form()? Because you want to create a form that provides feedback on progress after each user interaction. With st.form(), the user can evaluate only what they entered in the form after the submit section.\n\nJust use \"Nivo\" and pass the necessary data as input:\n\nfrom streamlit_elements import elements, mui\nfrom streamlit_elements import nivo\n\nDATA = [{\"taste\": \"RISULTATI\", \"peso_area\": variable1},\n        {\"taste\": \"CONSAPEVOLEZZA E FIDUCIA\", \"peso_area\": variable2},\n        {\"taste\": \"COINVOLGIMENTO\", \"peso_area\": variable3},\n        {\"taste\": \"DISTINTIVITA'\", \"peso_area\": variable4},\n        {\"taste\": \"COMPORTAMENTI\", \"peso_area\": variable5}]\n\n# radar chart example\ndef graph_pes(DATA):\n        with elements(\"nivo_charts\"):        \n            with mui.Box(sx={\"height\": 400}):\n                nivo.Radar(\n                    data=DATA,\n                    keys=[\"peso_area\"],\n                    indexBy=\"taste\",\n                    maxValue=7,\n                    valueFormat=\">-.2f\",\n                    margin={\"top\": 80, \"right\": 60, \"bottom\": 80, \"left\": 60},\n                    gridLabelOffset=36,\n                    dotSize=10,\n                    dotColor={\"theme\": \"background\"},\n                    dotBorderWidth=1,\n                    fillOpacity=0.85,\n                    borderWidth=2,\n                    borderColor=\"#e08367\",\n                    dotBorderColor=\"#e08367\",\n                    motionConfig=\"wobbly\",\n                    legends=[\n                        {\n                            \"anchor\": \"top-left\",\n                            \"direction\": \"column\",\n                            \"translateX\": -70,\n                            \"translateY\": -100,\n                            \"itemWidth\": 40,\n                            \"itemHeight\": 20,\n                            \"itemTextColor\": \"#999\",\n                            \"symbolSize\": 12,\n                            \"symbolShape\": \"circle\",\n                            \"effects\": [\n                                {\n                                    \"on\": \"hover\",\n                                    \"style\": {\n                                        \"itemTextColor\": \"#000\"\n                                    }\n                                }\n                            ]\n                        }\n                    ],\n                    theme={\n                        \"background\": \"#E4E3E3\",\n                        \"textColor\": \"#31333F\",\n                        \"grid\": {\"line\": {\n                                            \"stroke\": \"#b3bcc4\",\n                                            \"strokeWidth\": 1\n                                        }\n                                    },\n                        \"tooltip\": {\n                            \"container\": {\n                                \"background\": \"#E4E3E3\",\n                                \"color\": \"#31333F\",\n                            }\n                        }\n                    }\n                )\n\n2. How to create an interactive dashboard for patient satisfaction\n\nStreamlit is excellent for integrating multiple data sources, enabling vital data harmonization and pipeline. It can integrate diverse data sources, including ERP systems from healthcare organizations.\n\nThe dashboard helps managers make adjustments, measure their effects, and improve patient centrality:\n\nTo create a patient satisfaction dashboard, follow these steps:\n\nInstall the necessary libraries:\n\npip install streamlit plotly altair pandas\n\n\nImport the required modules in your Streamlit app:\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\n\n\nLoad and preprocess your patient satisfaction data:\n\n# Load the data from a data source (reading gsheet to dataframe)\nsheet_url = \"url\"\nurl_1 = sheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\ndf=pd.read_csv(url_1)\n\n# Define Filter on sidebar\na, b, c = st.sidebar.columns([0.05,1,0.05])\nwith a:\n    st.write(\"\")\nwith b:\n    Proced_Fil=st.multiselect(\"Tipo Procedura\", df[\"Tipo_procedura\"].unique(),  default=[\"RMN\", \"Raggi X\", \"CT\"])\n    Sesso_Fil=st.multiselect(\"Sesso\", df[\"Sesso\"].unique(),  default=[\"Maschio\", \"Femmina\", \"Non Specificato\"])\n    Eta_Fil=st.multiselect(\"Fasce di et√†\", df[\"Range_Et√†\"].unique(),  default=[\"18-30anni\"])\n    st.image(image3, width=170)\nwith c:\n    st.write(\"\")\n\n# Inizialize a filtered dataframe with pandas query\n\ndf_selection = df.query(\"Tipo_procedura == @Proced_Fil & Sesso == @Sesso_Fil & Range_Et√† == @Eta_Fil\")\n\n\nCreate interactive visualizations using Plotly, Altair, or other libraries:\n\n# Use Plotly to create a bar chart\ndf42= df.groupby(pd.Grouper(key='Timestamp', axis=0,freq='1W')).count().reset_index()\ndf42.rename(columns={'Sesso':'Form_Inviati'}, inplace=True)\ndf42[\"target\"]=10\ndf42[\"MA_REPORT\"]=df42[\"Form_Inviati\"].rolling(2).mean()\nst.header(\"Bar Chart Numero report inviati per settimana\")\nfig=px.bar(df42, x =\"Timestamp\", y='Form_Inviati', color='Form_Inviati',template = 'ggplot2',width=800, height=400)\nfig.add_trace(go.Scatter(x=df42['Timestamp'], y=df42[\"target\"],mode='lines', line=dict(color=\"blue\"), name='Safety Target'))\nfig.add_trace(go.Scatter(x=df42['Timestamp'], y=df42[\"MA_REPORT\"],mode='lines', line=dict(color=\"orange\"), name='Media mobile'))\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01))\nst.plotly_chart(fig, use_container_width=True)\n\n\n\nAdd additional components, such as sliders or dropdowns, for interactive filtering and customization:\n\n# Define Section of KPI using st.metric of Streaml\nst.subheader(\"KPI per la settimana corrente delle principali macro-aree\")\ncol1, col2, col3, col4, col5 = st.columns(5)\nwith col1:\n    len_report_sett_now=df1['Sesso'].iloc[-1]\n    len_report_sett_last_week=df1['Sesso'].iloc[-2]\n    delta_report=int(len_report_sett_now) - int(len_report_sett_last_week)\n    st.metric(\"Report Inviati In Settimana\",  value= str(int(len_report_sett_now))+\" rep\", delta=str(delta_report),  help=\"Numero totale di report inviati questa settimana rispetto a settimana scorsa\")\nwith col2:\n    #Settimana attuale psi\n    df2_att_scorsa_settimana=df.loc[(df['Timestamp'] >= str(date_last_week))]\n    df2_medie_valori_week=df2_att_scorsa_settimana.mean().reset_index()\n    df2_medie_valori_week.columns = ['variables', 'count']\n    psi_this_week=round(df2_medie_valori_week[\"count\"].mean(), 4)\n    psi_perc=round((psi_this_week/7)*100,2)\n    #Settimana precedente alla sett scorsa psi\n    df2_prima_scorsa_settimana=df.loc[(df['Timestamp'] < str(date_last_week))]\n    df2_medie_valori_prec_week=df2_prima_scorsa_settimana.mean().reset_index()\n    df2_medie_valori_prec_week.columns = ['variables', 'count']\n    psi_prima_last_week=round(df2_medie_valori_prec_week['count'].mean(), 4)\n    #differenza tra i PSI\n    delta_psi=round(((float(psi_this_week)-float(psi_prima_last_week))/7)*100, 2)\n    if psi_perc>100:\n        psi_perc==100\n    st.metric(\"PSI Index\",  value=str(psi_perc)+\" %\", delta=str(delta_psi)+\" %\", help=\"Patient Satisfaction Index (misura complessiva di grado di soddisfazione dei pazienti)\")\n\n\n3. How to connect Google Sheets to Streamlit\n\nStreamlit seamlessly integrates with Google Sheets through the pandas library, treating the source as a simple CSV file.\n\nJust follow these steps.\n\nInstall the necessary libraries:\n\npip install gspread\npip install oauth2client\n\n\nCreate a service account key in the Google Cloud Console:\n\nGo to the Google Cloud Console (https://console.cloud.google.com/)\nCreate a new project or select an existing one\nEnable the Google Sheets API for your project\nGo to \"Credentials\" and create a new service account key\nDownload the JSON key file for the service account\n\nPaste the private key into the secrets.toml file, making any necessary modifications to fit the required standard.\n\nSet up your script using the provided script as a guide.\n\nStore your data in a script storage, as shown in the example below, where we saved data from a long form.\n\nHere is how to do this:\n\nimport google_auth_httplib2\nimport httplib2\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import HttpRequest\n\n@st.cache_resource()\ndef connect_to_gsheet():\n    # Create a connection object.\n    credentials = service_account.Credentials.from_service_account_info(\n        st.secrets[\"gcp_service_account\"],\n        scopes=[SCOPE],\n    )\n\n    # Create a new Http() object for every request\n    def build_request(http, *args, **kwargs):\n        new_http = google_auth_httplib2.AuthorizedHttp(\n            credentials, http=httplib2.Http()\n        )\n        return HttpRequest(new_http, *args, **kwargs)\n\n    authorized_http = google_auth_httplib2.AuthorizedHttp(\n        credentials, http=httplib2.Http()\n    )\n    service = build(\n        \"sheets\",\n        \"v4\",\n        requestBuilder=build_request,\n        http=authorized_http,\n    )\n    gsheet_connector = service.spreadsheets()\n    return gsheet_connector\n\n@st.cache_data()\ndef get_data(gsheet_connector) -> pd.DataFrame:\n    values = (\n        gsheet_connector.values()\n        .get(\n            spreadsheetId=SPREADSHEET_ID,\n            range=f\"{SHEET_NAME}!A:E\",\n        )\n        .execute()\n    )\n\n    df = pd.DataFrame(values[\"values\"])\n    df.columns = df.iloc[0]\n    df = df[1:]\n    return df\n\ndef add_row_to_gsheet(gsheet_connector, row) -> None:\n    gsheet_connector.values().append(\n        spreadsheetId=SPREADSHEET_ID,\n        range=f\"{SHEET_NAME}!A:E\",\n        body=dict(values=row),\n        valueInputOption=\"USER_ENTERED\",\n    ).execute()\n\n# submit form and store data\nsubmitted = st.button(label=\"Submit\")              \nif submitted==True:\n   st.success(\"Successfully\")\n   st.ballons()\n   add_row_to_gsheet(df, \n                  [[var_a1, var_a2, var_a3,\n                    var_b1, var_b2, var_b3,\n                    var_c1, var_c2, var_c3,\n                    var_d1, var_d2, var_d3, \n                    var_d4, var_d5, var_d6, var_d7,                            \n                    var_e1,  var_e2,                                \n                    var_f1, var_f2, var_f3,                              \n                    var_g1, var_g2, var_g3, var_g4,var_g5,                              \n                    var_h1, var_h2, var_h3,var_h4, var_h5, var_h6, var_h7,var_h8, var_h9,                                \n                    var_i1, var_i2,                               \n                    feedback_gen, str(datetime_object),  \"Form_lungo\",  \n                    add_comm,  emozione,  sentiment]])\n\n4. How to create a multi-page app\n\nTo implement multi-page functionality, incorporate the Patient_Form.py and other relevant scripts from our repo.\n\nThe code shows how different functions can simulate separate pages within the app:\n\nThe functions form_pazienti(), dashboard_patient_satisf(), and landing_page() represent distinct pages, each displaying relevant content.\nThe page_names_to_funcs dictionary maps page names to their respective functions.\nThe selected_page variable, determined by the user's selection from the sidebar, executes the corresponding function to display the chosen page's content.\n\nHere is an example:\n\n#Example: Through functions we can simulate different pages of the web-app \n\ndef form_pazienti():\n\tst.title(\"Page1 application\")\n\ndef dashboard_patient_satisf():\n\tst.title(\"Page2 application\")\n\ndef landing_page():\n\tst.title(\"Page3 application\")\n\npage_names_to_funcs = {\n            \"Form Patient Satisfaction\": form_pazienti,\n            \"Dashboard Patient Satisfaction\": dashboard_patient_satisf, \n            \"Info Framework\":landing_page}\nselected_page = st.sidebar.selectbox(\"Select a page\", page_names_to_funcs.keys(), key =\"value\")\npage_names_to_funcs[selected_page](\n\n5. How to create data manipulation techniques\n\nTo make simple and beautiful dashboards, use the patient satisfaction results to develop key performance indicators (KPIs) and extract actionable insights.\n\nThe surveys capture two aspects of the interview:\n\nThe numerical portion involves quantitative data analysis. Users adjust the bar based on a Likert scale of 0 to 7.\nThe free-text portion involves qualitative data analysis. Users input comments, and the application provides elaboration on the numerical data.\n\nPatient forms contain qualitative data that is analyzed manually. We aimed to translate the comments into quantitative information not typically captured in tabular form (view the dataset here).\n\nJust import the necessary libraries:\n\n#basic libraries\nimport streamlit as st  \nimport pandas as pd \nimport numpy as np\nimport time\nfrom datetime import date\nfrom datetime import timedelta\nfrom htbuilder import div, big, h2, styles\nfrom htbuilder.units import rem\n\n#cv libraries\nfrom PIL import Image\n\n#visualization libraries\nimport plotly.express as px  \nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud,  STOPWORDS\nimport matplotlib.pyplot as plt\n\n#machine learning libraries\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nimport statsmodels.api as sm\nfrom streamlit_elements import elements, mui\nfrom streamlit_elements import nivo\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n6. How to use ML algorithms to improve the prediction of goal metrics\n\nYou can also implement a predictive KPI using a machine learning (ML) algorithm called Gradient-Boosting. It considers all relevant variables to predict the Patient Satisfaction Index for upcoming weeks.\n\nThe index represents the overall satisfaction of patients:\n\n#algorithms Gradient Boosting to predict PSI\ndef training_ml(x,  y):\n    X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.05, random_state=13)\n\n    params = {\n    \"n_estimators\": 500,\n    \"max_depth\": 4,\n    \"min_samples_split\": 5,\n    \"learning_rate\": 0.01,\n    \"loss\": \"squared_error\"}\n    \n    reg = ensemble.GradientBoostingRegressor(**params)\n    reg.fit(X_train, y_train)\n\n    pred=reg.predict(X_test)\n    \n    mse = mean_squared_error(y_test, pred)\n    print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n    #take the first prediction\n    return pred[-1]\n\n#check preds (1 e 7)\npred = training_ml(X,  y)\n\n\nUse polynomial regression algorithms to calculate the relationship between patients' satisfaction and wait time. Each line in the algorithm represents a different-order polynomial, with parameters identified using form data.\n\nYou can analyze patient comments using sentiment analysis, an approach to natural language processing (NLP) that identifies the emotional tone behind the written text.\n\n% Form with Comment (Value (%) represents the number of people who completed the form and left a comment)\n% Negative Results (Value (%) represents the number of negative keyword results out of the total results of keywords added by users, excluding articles, conjunctions, and non-significant words)\n% Sentiment Analysis Score (Value as Positive-Negative Ratio. Calculate the ratio of positive to negative keywords. If the result is greater than 0, there are more positive keywords.)\n\nThis is a method that organizations can use to evaluate and classify opinions regarding a product, service, or idea.\n\nHere is how to build a similar word cloud:\n\n# How to build a WordCloud on Streamlit using Matplotlib\nst.header(\"Word Cloud Patient Form\")\ncommenttext_merged= df['Comment_Text'].str.cat(sep=' , ')\n\n# all words in title case\ntext_propercase=commenttext_merged.title()\n\n# remove all non necessary words as articles and con\nstop_words =STOPWORDS.update([\"La \", \"Non \", \"Mi \", \"E \", \"Il \", \"Dei \", \"Di \", \"Degli \", \"Lo \",  \"Della \", \"C'Era \", \", \",  \"Del \",  \"Per \", \"Sotto \", \"Alcuni\", \"Alcune \", \"Ok \"\n                        , \"Rispetto!\",\"Degli \",  \"Ho \", \"E' \",  \"Da \",  \"Un \",  \"In \",  \"Una \", \"Dalla \", \"Stata \", \"Mia \", \"Che \",  \"Ma \",  \"Tutto \",  \"Sono \"])\n\n#Create and generate a word cloud image:\nwordcloud = WordCloud(stopwords = stop_words,background_color=\"#E4E3E3\", width=800, height=500, colormap=\"Blues\").generate(text_propercase)\nfig, ax = plt.subplots(facecolor=\"#E4E3E3\")\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.subplots_adjust(left=-5, right=-2, top=-2, bottom=-5)\nplt.show()\nst.pyplot(fig)\n\n\nHere is how it looks:\n\nFinally, integrate an AI tool into the Patient Satisfaction Form to classify comments based on Italian BERT's sentiments and emotions classification. This language model can detect and classify comments into two different labels: positive and negative (binary classification). And it can extract emotions from a phrase and classify them into four labels: joy, anger, fear, and sadness (see the repo here).\n\nUse the results of this classification to calculate two metrics‚Äîto see how many comments are considered positive and negative by BERT:\n\nWrapping up\n\nMehedi is a result of long talks, design studies, and research. Using our IT, medical, and management knowledge, we made a customizable platform for healthcare facilities to gather useful information directly from patients. Streamlit helped us create our app super fast!\n\nWe welcome your feedback and opportunities for collaboration. Please leave us a message in the comments below, on GitHub, or on LinkedIn: Matteo, Luca, and Federico.\n\nThank you, and happy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to build an interconnected multi-page Streamlit app",
    "url": "https://blog.streamlit.io/how-to-build-an-interconnected-multi-page-streamlit-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build an interconnected multi-page Streamlit app\n\nFrom planning to execution‚Äîhow I built GPT lab\n\nBy Dave Lin\nPosted in LLMs, July 19 2023\nPlanning a large Streamlit app\nUpfront feature and UX: Creating initial spec and low-fi UX mocks\nData model: Determining the schema\nUsers and user_hash\nBots\nSessions\nCode structure: Structuring for scalability and modularity\nSession states: Managing UI and user flow\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nWow! What an incredible three months since I first published my blog post on the 12 lessons learned from building GPT Lab! üöÄ\n\nThanks to your tremendous support, GPT Lab has received over 9K app views, 1200+ unique signed-in users, 1000+ sessions with assistants, 700+ prompts tested, and 190+ assistants created. The app has also been featured in the Streamlit App Gallery alongside other great apps.\n\nMany of you have asked me, \"How did you plan and build such a large application with Streamlit?\" Eager to answer, I've decided to open-source GPT Lab.\n\nBy reading this post, you‚Äôll gain insights into the planning it took to build an interconnected multi-page Streamlit app like GPT lab:\n\nUpfront feature and UX\nData model\nCode structure\nSession states\n\nI hope it'll inspire you to push Streamlit to its limits and bring your ambitious apps to life!\n\nü§ñ\nWant to skip ahead? Check out the app and the code.\nPlanning a large Streamlit app\n\nBuilding large Streamlit apps like GPT Lab requires careful planning rather than just throwing code together. For GPT Lab, I focused on planning these four key aspects:\n\nUpfront feature and UX. What will the app do? What kind of user experience do we aim to provide?\nData model. How will data be persisted? What should be stored in the database versus session state variables?\nCode structure. How should the app be architected to ensure modularity, maintainability, and scalability?\nSession states. Which session state variables are needed to link the user interface?\n\nUnderstanding these aspects offered a clearer view of what I was trying to build and provided a framework to approach the complex task systematically.\n\nLet's dive into each aspect in more detail.\n\nUpfront feature and UX: Creating initial spec and low-fi UX mocks\n\nTo start, I created a simple specification document (or \"spec\") outlining the overall scope and approach. I also included a sitemap detailing the use cases I wanted to support. The spec gave me a clear roadmap and a means to measure my progress.\n\nHere's an excerpt from the original spec:\n\nScope. Build a platform that allows generative AI (GA) bot enthusiasts to build their own GPT-3 prompt-based chatbot for their friends and families. The goal is to test the hypothesis that enough GA bot enthusiasts would want to build their niche-domain bots.\n\nApproach. A public Streamlit site that allows users to interact with one of the four pre-trained coach bots or create and interact with their bots.\n\nAs with most development projects, I made some changes. But the original sitemap remained intact for the most part, as I could implement most of the planned features.\n\nHere is the final version of the sitemap:\n\nGPT Lab\n‚îÇ\n‚îú‚îÄ‚îÄ Home\n‚îÇ\n‚îú‚îÄ‚îÄ Lounge\n‚îÇ\n‚îú‚îÄ‚îÄ Assistant\n‚îÇ   ‚îú‚îÄ‚îÄ Search for assistant\n‚îÇ   ‚îú‚îÄ‚îÄ Assistant details\n‚îÇ   ‚îú‚îÄ‚îÄ Active chat\n‚îÇ   ‚îî‚îÄ‚îÄ Chat recap\n‚îÇ\n‚îú‚îÄ‚îÄ Lab\n‚îÇ   ‚îú‚îÄ‚îÄ Step 1: initial prompt + model config\n‚îÇ   ‚îú‚îÄ‚îÄ Step 2: test chat\n‚îÇ   ‚îú‚îÄ‚îÄ Step 3: other configs\n‚îÇ   ‚îî‚îÄ‚îÄ Step 4: confirmation\n‚îÇ\n‚îú‚îÄ‚îÄ FAQ\n‚îÇ\n‚îî‚îÄ‚îÄ Legal\n    ‚îú‚îÄ‚îÄ Terms\n    ‚îî‚îÄ‚îÄ Privacy policy\n\n\nI can't overstate the importance of feature planning. It provides a roadmap, a way to measure progress, and a starting point for thinking about the data model.\n\nData model: Determining the schema\n\nFrom the start, I recognized that a backend data store was crucial for persisting user, assistant, and session records. After considering my options, I decided on Google Firestore due to its scalability, real-time capabilities, and generous free tier. I strategically designed the data model with future use cases in mind. For example, it's possible to add prompt version controls to GPT Lab, allowing users to edit or revert their assistants.\n\nü§ñ\nNOTE: In the app backend and data model, assistants are referred to as bots, despite my previous insistence on not calling them bots in the user interface. üòÖ\n\nNow, let's explore the four main Firestore collections in GPT Lab: users, user_hash, bots, and sessions.\n\nUsers and user_hash\n\nThe users collection is where the app stores information about its users. To protect user privacy, the app doesn't store any personally identifiable information (PII) about users. Instead, each user is associated only with the one-way hash value of their OpenAI API key. The metric fields are incremented whenever a user creates an assistant or starts/ends a session with an assistant. This allows for basic analytics gathering within the app.\n\nUsers Collection\n   |\n   | - id: (Firestore auto-ID)\n   | - user_hash: string (one-way hash value of OpenAI API key)\n   | - created_date: datetime\n   | - last_modified_date: datetime\n   | - sessions_started: number\n   | - sessions_ended: number\n   | - bots_created: number\n\n\nGoogle Firestore doesn't provide a way to ensure the uniqueness of a document field value within a collection, so I created a separate collection called user_hash. This ensures that each unique API key has only one associated user record. Each user document is uniquely associated with a user_hash document, and each user_hash document may be associated with a user document. The data model is flexible enough to accommodate users who change their API keys in the future (users can log in with their old API key and then swap it out for a new one).\n\nUser_hash Collection\n   |\n   | - id = one-way hash value of OpenAI API key\n   | - user_hash_type: string (open_ai_key)\n   | - created_date: datetime\n\nBots\n\nThe bots collection stores configurations for AI assistants. The crux of each AI assistant is its large language model (LLM), model configurations, and prompts. To enable proper version control of prompts and model configurations in the future, model_configs and prompts are modeled as subcollections (part of GPT Lab's vision is to be the repository of your prompts).\n\nTo minimize subcollection reads (so you don't need to constantly query the subcollections for the active record), the document IDs of the active subcollection are also stored at the document level. The session_type field indicates whether the assistant is in a brainstorming or coaching session, which affects the session message truncation technique.\n\nFinally, the metric fields are incremented when a user starts or ends a session with an assistant.\n\nBots Collection\n   |\n   | - id: (Firestore auto-ID)\n   | - name: string\n   | - tag_line: string\n   | - description: string\n   | - session_type: number\n   | - creator_user_id: string\n   | - created_date: datetime\n   | - last_modified_date: datetime\n   | - active_initial_prompt_id: string\n   | - active_model_config_id: string\n   | - active_summary_prompt_id: string\n   | - showcased: boolean\n   | - is_active: boolean\n   |\n   v\n   |--> Model_configs subcollection\n   |     |\n   |     | - config: map\n   |     |     | - model: string \n   |     |     | - max_tokens: number \n   |     |     | - temperature: number \n   |     |     | - top_p: number \n   |     |     | - frequency_penalty: number \n   |     |     | - presence_penalty: number \n   |     | - created_date: datetime\n   |     | - is_active: boolean\n   |\n   v\n   |--> Prompts subcollection\n         |\n         | - message_type: string\n         | - message: string\n         | - created_date: datetime\n         | - is_active: boolean\n         | - sessions_started: number\n         | - sessions_ended: number\n\nSessions\n\nThe sessions collection stores session data. It contains two types of sessions: lab sessions (used for testing prompts) and assistant sessions (used for chatting with created assistants). To reduce the need for frequent retrieval of the bot document, its information is cached within the session document. This makes conceptual sense, as the bot document could drift if an editing assistant use case were ever implemented.\n\nThe messages_str field stores the most recent payload sent to OpenAI's LLM. This feature allows users to resume their previous assistant sessions. The messages subcollection stores the actual chat messages. Note that lab session chat messages aren't stored.\n\nTo ensure user confidentiality and privacy, OpenAI request payloads and session messages are encrypted before being saved in the database. This data model allows users to restart a previous session and continue chatting with the assistant.\n\nSessions Collection\n   |\n   | - id: (Firestore auto-ID)\n   | - user_id: string\n   | - bot_id: string\n   | - bot_initial_prompt_msg: string\n   |\n   | - bot_model_config: map\n   |     | - model: string \n   |     | - max_tokens: number \n   |     | - temperature: number \n   |     | - top_p: number \n   |     | - frequency_penalty: number \n   |     | - presence_penalty: number \n   |\n   | - bot_session_type: number\n   | - bot_summary_prompt_msg: string\n   | - created_date: datetime\n   | - session_schema_version: number\n   | - status: number\n   | - message_count: number\n   | - messages_str: string (encrypted)\n   |\n   v\n   |--> Messages subcollection\n         |\n         | - created_date: datetime\n         | - message: string (encrypted)\n         | - role: string\n\n\nBy carefully considering all potential use cases from the beginning, I created a data model that is future-proof and able to accommodate the evolving needs and features of the app. In the following section, we'll examine the structure of the backend application code to see how it supports and implements this robust data model.\n\nCode structure: Structuring for scalability and modularity\n\nI created GPT Lab to empower users with low or no technical skills to build their own prompt-based LLM-based AI applications without worrying about the underlying infrastructure. My goal is to eventually offer backend APIs that connect users' custom front-end apps (whether using Streamlit or not) with their AI assistants. This motivated me to design a decoupled architecture that separates the front-end Streamlit application from the backend logic.\n\nThe backend code was structured as follows:\n\n+----------------+     +-------------------+     +-------------------+     +------------+\n|                |     |                   |     |                   |     |            |\n|  Streamlit App |<--->| util_collections  |<--->| api_util_firebase |<--->|  Firestore |\n|                |     | (users, sessions, |     |                   |     |            |\n|                |     |  bots)            |     |                   |     |            |\n+----------------+     +-------------------+     +-------------------+     +------------+\n                             |\n                             |\n                             v\n                     +-----------------+     +------------+\n                     |                 |     |            |\n                     | api_util_openai |<--->|   OpenAI   |\n                     |                 |     |            |\n                     +-----------------+     +------------+\n\n\nThe modules are as follows:\n\napi_util_firebase handles CRUD operations with the Firestore database.\napi_util_openai interacts with OpenAI's models, provides a unified chat model to upstream models, prunes chat messages, and tries to detect and prevent prompt injection attacks.\napi_util_users, api_util_sessions, and api_util_bots are interfaces to their corresponding Firestore collections. They interact with api_util_firebase and api_util_openai and implement GPT Lab-specific business logic.\n\nThis design enables separate development, testing, and scaling of different parts of the code. It also establishes an easier migration path to convert the backend util_collections modules into Google Cloud Functions, which can be exposed via API Gateways.\n\nSession states: Managing UI and user flow\n\nAs explained in the first blog article, I used session state variables to control and manage functionalities on Streamlit pages.\n\nThis table illustrates how these variables are utilized throughout the app:\n\nStreamlit Page\tSession state and UI controls\nhome.py\tuser¬†controls whether to render the OpenAI API key module.\npages/1_lounge.py\tuser¬†controls whether to render the OpenAI API key module, enable assistant selections, and show the My Assistants tab.¬†¬†\n\nAfter users choose to interact with an assistant, the assistant details are stored in¬†bot_info.\npages/2_assistant.py\tuser¬†controls whether to render the OpenAI API key module.¬†¬†\n\nbot_info,¬†session_id, and¬†session_ended¬†determine which screen variation to display:¬†\n\n‚Ä¢¬†bot_info does not exist: check to see if assistant_id is in the URL parameter. Else, prompt users to search for an assistant¬†¬†\n‚Ä¢¬†bot_info and session_id exist, and session_ended is false: display the chat session screen¬†¬†\n‚Ä¢¬†bot_info and session_id exist, and session_ended is true: display the chat session recap screen¬†¬†\n\nIn the chat session,¬†session_msg_list¬†stores the conversation.\npages/3_lab.py\tuser¬†gates whether to render the OpenAI API key module and whether to allow users to start creating assistants in the lab.¬†¬†\n\nlab_active_step¬†controls which lab session state to render:\n‚Ä¢ If 1: render step 1 UI to set assistant initial prompt and model\n‚Ä¢ If 2: render step 2 UI to test chat with assistant\n‚Ä¢ If 3: render step 3 UI to finalize assistant details. On create, the bot record is created in Firestore DB, and the document ID is saved to¬†lab_bot_id.\n‚Ä¢ If 4 and lab_bot_id is set: render step 4 UI¬†to show assistant creation confirmation.¬†\n\nDuring the test chat session,¬†lab_msg_list¬†stores the test messages. By using separate¬†lab_bot_id¬†and¬†bot_info, I can allow users to jump back and forth between lounge/assistant and lab without losing progress in each.\n\nWith the upfront planning done, the rest of the execution was a lot more manageable.\n\nWrapping up\n\nThis post covered the upfront planning required for creating GPT Lab, including the features, data model, code, and session state. I hope this inspires you to build your own ambitious Streamlit apps.\n\nConnect with me on Twitter or Linkedin. I'd love to hear from you.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Decoding Warren Buffett with LLMs and Snowflake SQL",
    "url": "https://blog.streamlit.io/decoding-warren-buffett-with-llms-and-snowflake-sql/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nBy Randy Pettus\nPosted in LLMs, August 1 2023\nWhat is Ask the Oracle of Omaha?\nApp overview\nHow to use the app\n1. How to create a Snowflake database, schema, and virtual warehouse\n1.1. Accounts\n1.2. Python packages\n1.3. Secrets management\n1.4. Data loading\n2. How to load data into Snowflake using Snowpark Python and obtain the associated DDL statements\n3. How to create embeddings from DDL statements and load them into a vector database (FAISS)\n4. How to create embeddings from PDFs and load them into a vector database (Pinecone)\n5. How to perform questioning/answering on these docs using OpenAI and LangChain\n6. How to put it all together\ntab1\ntab2\ntab3\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Randy Pettus, and I'm a principal data scientist in Denver, Colorado. Before transitioning into the world of data science, I worked in finance and taught college finance courses, including one called \"The Investment Strategies of Warren Buffett.\"\n\nWhen Streamlit announced the 2023 Snowflake Streamlit Hackathon, I decided to create an app allowing users to explore Warren Buffett's world. I did it in less than a day, and it won second place! ü•à\n\nIn this post, I'll show you:\n\nHow to create a Snowflake database, schema, and virtual warehouse\nHow to load data into Snowflake using Snowpark Python and obtain the associated DDL statements\nHow to create embeddings from DDL statements and load them into a vector database (FAISS)\nHow to create embeddings from PDFs and load them into a vector database (Pinecone)\nHow to perform questioning/answering on these docs using OpenAI and LangChain\nHow to put it all together\n\n\nüíµ\nTLDR: Here's the app and the repo code.\nWhat is Ask the Oracle of Omaha?\n\nSo, what is this app exactly, and what does it do?\n\nThe app offers various LLM functionalities, such as LLM-augmented retrieval from data in a Snowflake database and PDF documents. You can access financial statement information for multiple companies Buffet has invested in through a KPI view or by asking natural language questions. And you can query Buffet's shareholder letters going back to 1977.\n\nGo ahead and try it:\n\nAs you can see, there are three main tabs:\n\nüíµ Financial statement natural language querying: Converts user questions into Snowflake SQL and returns DataFrame outputs. The approach uses semantic search on DDL statements stored in a vector database and uses LLM-generated SQL to query Snowflake directly.\nüìà Financial data exploration: Users can examine financial statement information through KPI cards, charts, and so on, pulled from Snowflake.\nüìù Shareholder letter natural language querying: This performs question and answering using retrieval augmented generation from Buffet's shareholder letters stored as PDFs dating back to 1977.\n\nBefore we dive into coding, let's take a quick high-level view of the app.\n\nApp overview\n\nHere is how all the app components fit together:\n\nHow to use the app\n\nHere are a few examples that show how the app functions.\n\nSay, you want to inquire about the financial performance of certain companies. In the first tab, type \"Rank the companies in descending order based on their net income in 2022. Include the ticker and net income value.\"\n\nGreat! You just got a DataFrame back from Snowflake with the correct results.\n\nNow, say you want to explore the shareholder letters. Let's use this random 1984 letter:\n\nType in, \"Where did Mrs. B receive her honorary doctorate, and what was her job role?\"\n\nAwesome! The app gave you the correct answer.\n\nLet's move on to building it.\n\n1. How to create a Snowflake database, schema, and virtual warehouse\n1.1. Accounts\n\nTo start, you'll need a Snowflake account, an OpenAI account, and a Pinecone account. Go ahead and create them if you don't have them.\n\nüíµ\nNOTE: Snowflake offers free trials, and Pinecone offers one index for free. You can make some modifications to use any other database, such as SQLite, or substitute Pinecone for FAISS or another alternative.\n1.2. Python packages\n\nNext, install the following Python packages:\n\n# requirements.txt\naltair\nsnowflake-connector-python\nsnowflake-sqlalchemy\nsnowflake-snowpark-python[pandas]==1.5.1\nnumpy\npandas\nmatplotlib\nseaborn\nopenai\nstreamlit_chat\nlangchain==0.0.124\npinecone-client\nsqlalchemy\nfaiss-cpu\n\n1.3. Secrets management\n\nTo use your Snowflake credentials, create a .streamlit/secrets.toml file in the following format (read more here). The [connections.snowpark] section should be filled out with your Snowflake credentials. The setting client_session_keep_alive = true keeps the session active, which helps avoid connection timeout issues.\n\n#.streamlit/secrets.toml\n\nopenai_key = \"########\"\npinecone_key = \"########\"\npinecone_env = \"########\"\n\nsf_database = \"FINANCIALS\"\nsf_schema = \"PROD\"\n\n[connections.snowpark]\naccount = \"########\"\nuser = \"########\"\npassword = \"########\"\nwarehouse = \"########\"\ndatabase = \"########\"\nschema = \"########\"\nclient_session_keep_alive = true\n\n1.4. Data loading\n\nFinally, enable the loading of financial statement data in Snowflake. After creating your Snowflake account, you must create a database, schema, and virtual warehouse. Snowflake uses the virtual warehouse to execute virtual warehousee on your data. You can run the following SQL commands in a UI worksheet (remember to include them in your secrets file).\n\ncreate database FINANCIALS;\n\ncreate schema PROD;\n\n-- create an extra small warehouse\nCREATE WAREHOUSE if not exists WH_XS_APP\nwith\n  WAREHOUSE_SIZE = XSMALL\n  AUTO_SUSPEND = 60\n  INITIALLY_SUSPENDED = TRUE\n  COMMENT = 'APPLICATION WAREHOUSE'\n;\n\n2. How to load data into Snowflake using Snowpark Python and obtain the associated DDL statements\n\nPublicly traded companies file their financial statements‚Äîincome, balance sheet, cash flow‚Äîwith the Securities Exchange Commission (SEC). I sourced my data from SEC Edgar (see the CSV files in my repo). Consider it test data, as it might not be completely accurate. If you want a more reliable source, there are various services and APIs available for obtaining financial statement data, including Cybersyn on the Snowflake Marketplace.\n\nIn my repo, you'll see three folders under the .load/financials directory for each financial statement type and a .csv file for each company (ticker) within each folder.\n\nWith that in mind, create a stock_load.py file that does the following:\n\nEstablishes a Snowflake Snowpark connection and session.\nLoops through each financial statement folder to load the CSV as a Pandas DataFrame.\nUses the session.create_dataframe() and save_as_table() Snowpark functionality to create a Snowflake table for each financial statement while loading the data from the dataframes.\nCreates a DDL file to make embeddings for LLM interaction. This step loops through each created table and gets the DDL information from Snowflake. All DDL statements are consolidated into a single ddls.sql file.\n\n\n# stock_load.py\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom snowflake.snowpark.session import Session\nimport streamlit as st\n\n# snowpark connection\nCONNECTION_PARAMETERS = {\n   \"account\": st.secrets['account'], \n   \"user\": st.secrets['user'],\n   \"password\": st.secrets['password'],\n    \"database\": st.secrets['database'],\n   \"schema\": st.secrets['schema'],\n   \"warehouse\": st.secrets['warehouse'], \n}\n\n# create session\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()\n\n# create a list of the statements which should match the folder name\nstatements = ['INCOME_STATEMENT_ANNUAL','BALANCE_SHEET_ANNUAL',\n'CASH_FLOW_STATEMENT_ANNUAL']\n\n# Load data into snowflake by looping through the csv files\nfor statement in statements:\n    path = f'./load/financials/{statement.lower()}/' \n    files = glob.glob(os.path.join(path, \"*.csv\"))\n    df = pd.concat((pd.read_csv(f) for f in files))\n    print(statement)\n    # note that overwrite is used to start. If adding future data, move to append with upsert process\n    session.create_dataframe(df).write.mode('overwrite').save_as_table(statement)\n\n# automatically get the ddl from the created tables\n# create empty string that will be populated\nddl_string = ''\n\n# run through the statements and get ddl\nfor statement in statements:\n    ddl_string += session.sql(f\"select get_ddl('table', '{statement}')\").collect()[0][0] + '\\\\n\\\\n'\n    \nddl_file = open(\"ddls.sql\", \"w\")\nn = ddl_file.write(ddl_string)\nddl_file.close()\n\n\nAfter running this, your FINANCIALS.PROD schema should contain populated tables:\n\nYou must also create a local ddls.sql file with \"create table\" commands for each table. This file is crucial in providing context for the LLM about the database structure, including the various tables, columns, and data types.\n\nHere is a snippet of this output:\n\ncreate or replace TABLE FINANCIALS.PROD.INCOME_STATEMENT_ANNUAL (\n\tTICKER VARCHAR(16777216),\n...\n\n3. How to create embeddings from DDL statements and load them into a vector database (FAISS)\n\nThis step aims to provide an efficient means of translating a question into relevant Snowflake SQL code. My original solution, which relied on Langchain's SQLDatabase and SQLDatabaseChain functionality to interact directly with Snowflake, wasn't optimal. It kept pulling information schema from Snowflake, producing too much context for OpenAI inputs to generate SQL code. So it unnecessarily wasted OpenAI tokens and Snowflake credits (find the original solution here).\n\nüíµ\nI tried another method of creating embeddings from the DDL information above. It proved to be much more economical, as I found that my OpenAI and Snowflake credit consumption dropped by as much as 80%!\n\nThe create_ddl_embeddings.py script below uses Langchain's TextLoader() functionality to load the ddls.sql file. The text is split into characters and documents and converted into embeddings using OpenAIEmbeddings(). The embeddings are stored in FAISS, an open-source vector database.\n\nRunning this script produces two files: index.faiss and index.pkl, both located in the faiss_index folder (used later for the question and retrieval pipeline):\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nimport streamlit as st\n\n# load the ddl file\nloader = TextLoader('load/ddls.sql')\ndata = loader.load()\n\n# split the text\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\ntexts = text_splitter.split_documents(data)\n\n# created embeddings from the sql document\nembeddings = OpenAIEmbeddings(openai_api_key = st.secrets[\"openai_key\"])\ndocsearch = FAISS.from_documents(texts, embeddings)\n\n# save the faiss index\ndocsearch.save_local(\"faiss_index\")\n\n4. How to create embeddings from PDFs and load them into a vector database (Pinecone)\n\nLet's move on to the shareholder letters (find them here or in my repo).\n\nYour pinecone.io account offers a free index. To set up the prompts, get the Pinecone API key, the Pinecone environment variable, and the index name (remember to store these keys in your .secrets file).\n\nCreate the letter_load.py script, which produces embeddings from the various letters and loads them into Pinecone:\n\nimport os\nfrom langchain.document_loaders import PyPDFLoader # for loading the pdf\nfrom langchain.embeddings import OpenAIEmbeddings # for creating embeddings\nfrom langchain.vectorstores import  Pinecone # for the vectorization part\nfrom langchain.text_splitter import TokenTextSplitter\nimport pinecone\nimport streamlit as st\n\n# identify the various pdf files\npdfs = [file for file in os.listdir('./letters/') if 'pdf' in file]\n\n# loops through each pdf in the letters directory\n# and loops the content using langchains PyPDFLoader\npage_list = []\nfor pdf in pdfs:\n    pdf_path = f\"./letters/{pdf}\"\n    loader = PyPDFLoader(pdf_path)\n    pages = loader.load()\n    page_list.append(pages)\n\nflat_list = [item for sublist in page_list for item in sublist]\n\ntext_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(flat_list)\n\n# initialize pinecone\npinecone.init(\n    api_key=st.secrets['pinecone_key'], \n    environment=st.secrets['pinecone_env'] \n)\nindex_name = \"buffett\"\n\n# note you should create an OPENAI_API_KEY env environment variable or use st.secrets\n# create embeddings using OpenAI and load into Pinecone \nembeddings = OpenAIEmbeddings(openai_api_key=st.secrets['openai_key'])\ndocsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)\n\n\nThe script above loops through each letter and extracts text using Langchain's PyPDFLoader. The text is consolidated into a flattened list, and TokenTextSplitter chunks it. OpenAIEmbeddings is used again to create the embeddings, which are then loaded into Pinecone using langchain.vectorstores.Pinecone() and the page_content in each text split. Ideally, you'd use LangChain's DirectoryLoader for this, but package dependency issues cause delays. Running this script may take a few minutes, but once completed, you should see the index populated in Pinecone.\n\n5. How to perform questioning/answering on these docs using OpenAI and LangChain\n\nCreate a new file called prompts.py, which uses LangChain and OpenAI to enable question and answer retrieval.\n\nThis script covers the following steps:\n\nCreating prompt templates to provide better guidance for the large language model. The FS_TEMPLATE offers specific instructions for the LLM to produce better Snowflake SQL results from the financial statements, including an example of a single-shot prompt.\nDefining OpenAI parameters for the LLM.\nIdentifying the vector databases for retrieval, including the FAISS stored embeddings (get_faiss) and Pinecone vector database (get_pinecone).\nCreating a question/answer chain using Langchain's RetrievalQA functionality. The fs_chain() and letter_chain() functions take questions as inputs for the financial statements and shareholder letters, respectively. These functions are designed to retrieve the k most similar embeddings to the question while providing a natural language response. For fs_chain(), the result is a SQL query command, which you'll use for querying financial statement data in Snowflake. For letter_chain(), the output will include a text response for the question.\n\n\nimport streamlit as st\nimport openai\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS, Pinecone\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nimport pinecone\n\nFS_TEMPLATE = \"\"\" You are an expert SQL developer querying about financials statements. You have to write sql code in a Snowflake database based on the following question. \ndisplay the sql code in the SQL code format (do not assume anything if the column is not available, do not make up code). \nALSO if you are asked to FIX the sql code, then look what was the error and try to fix that by searching the schema definition.\nIf you don't know the answer, provide what you think the sql should be. Only include the SQL command in the result.\n\nThe user will request for instance what is the last 5 years of net income for Johnson and Johnson. The SQL to generate this would be:\n\nselect year, net_income\nfrom financials.prod.income_statement_annual\nwhere ticker = 'JNJ'\norder by year desc\nlimit 5;\n\nQuestions about income statement fields should query financials.prod.income_statement_annual\nQuestions about balance sheet fields (assets, liabilities, etc.) should query financials.prod.balance_sheet_annual\nQuestions about cash flow fields (operating cash, investing activities, etc.) should query financials.prod.cash_flow_statement_annual\n\nThe financial figure column names include underscores _, so if a user asks for free cash flow, make sure this is converted to FREE_CASH_FLOW. \nSome figures may have slightly different terminology, so find the best match to the question. For instance, if the user asks about Sales and General expenses, look for something like SELLING_AND_GENERAL_AND_ADMINISTRATIVE_EXPENSES\n\nIf the user asks about multiple figures from different financial statements, create join logic that uses the ticker and year columns.\nThe user may use a company name so convert that to a ticker.\n\nQuestion: {question}\nContext: {context}\n\nSQL: ```sql ``` \\\\n\n \n\"\"\"\nFS_PROMPT = PromptTemplate(input_variables=[\"question\", \"context\"], template=FS_TEMPLATE, )\n\nllm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0.1,\n    max_tokens=1000, \n    openai_api_key=st.secrets[\"openai_key\"]\n)\n\ndef get_faiss():\n    \"\"\"\n    get the loaded FAISS embeddings\n    \"\"\"\n    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets[\"openai_key\"])\n    return FAISS.load_local(\"faiss_index\", embeddings)\n\ndef get_pinecone():\n    \"\"\" \n    get the pinecone embeddings\n    \"\"\"\n    pinecone.init(\n        api_key=st.secrets['pinecone_key'], \n        environment=st.secrets['pinecone_env'] \n        )\n    \n    index_name = \"buffett\"\n    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets[\"openai_key\"])\n    return Pinecone.from_existing_index(index_name,embeddings)\n\ndef fs_chain(question):\n    \"\"\"\n    returns a question answer chain for faiss vectordb\n    \"\"\"\n\n    docsearch = get_faiss()\n    qa_chain = RetrievalQA.from_chain_type(llm, \n                                           retriever=docsearch.as_retriever(),\n                                           chain_type_kwargs={\"prompt\": FS_PROMPT})\n    return qa_chain({\"query\": question})\n\ndef letter_chain(question):\n    \"\"\"returns a question answer chain for pinecone vectordb\"\"\"\n    \n    docsearch = get_pinecone()\n    retreiver = docsearch.as_retriever(#\n        #search_type=\"similarity\", #\"similarity\", \"mmr\"\n        search_kwargs={\"k\":3}\n    )\n    qa_chain = RetrievalQA.from_chain_type(llm, \n                                            retriever=retreiver,\n                                           chain_type=\"stuff\", #\"stuff\", \"map_reduce\",\"refine\", \"map_rerank\"\n                                           return_source_documents=True,\n                                           #chain_type_kwargs={\"prompt\": LETTER_PROMPT}\n                                          )\n    return qa_chain({\"query\": question})\n\n\n\nYou'll see in the letter_chain() function that additional retrieval parameters are included. Due to the size of the letters, it is important to ensure a balance of getting enough coverage while not going over any token limitations with OpenAI to retrieve this case's three most similar embeddings and set the 'k' value to 3 in the search_kwargs arguments.\n\n6. How to put it all together\n\nNow that you can perform question and answering using the prompts.py file, it's time to create the main app file: buffett_app.py.\n\nFirst, import the appropriate packages and set the Streamlit page layout to \"wide.\" Then define some variables, including your Snowflake database, schema, and the various tickers used. Then, establish the Snowflake Snowpark connection using experimental_connection().\n\nimport snowflake.connector\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\nimport altair as alt\nimport prompts\n\nst.set_page_config(layout=\"wide\")\n\n# Variables\nsf_db = st.secrets[\"database\"]\nsf_schema = st.secrets[\"schema\"]\ntick_list = ['BRK.A','AAPL','PG','JNJ','MA','MCO','VZ','KO','AXP', 'BAC']\nfin_statement_list = ['income_statement','balance_sheet','cash_flow_statement']\nyear_cutoff = 20 # year cutoff for financial statement plotting\n\n# establish snowpark connection\nconn = st.experimental_connection(\"snowpark\")\n\n\nNext, create some helper functions to keep things cleaner:\n\npull_financials pulls a financial statement from Snowflake for a specified ticker.\nkpi_recent populates the KPI cards based on the most recent periods for a selected metric.\nplot_financials plots Altair bar charts from a DataFrame.\n\n\n\n@st.cache_data()\ndef pull_financials(database, schema, statement, ticker):\n    \"\"\"\n    query to pull financial data from snowflake based on database, schema, statemen and ticker\n    \"\"\"\n    df = conn.query(f\"select * from {database}.{schema}.{statement} where ticker = '{ticker}' order by year desc\")\n    df.columns = [col.lower() for col in df.columns]\n    return df\n\n# metrics for kpi cards\n@st.cache_data()\ndef kpi_recent(df, metric, periods=2, unit=1000000000):\n    \"\"\"\n    filters a financial statement dataframe down to the most recent periods\n    df is the financial statement. Metric is the column to be used.\n    \"\"\"\n    return df.sort_values('year',ascending=False).head(periods)[metric]/unit\n\ndef plot_financials(df, x, y, x_cutoff, title):\n    \"\"\"\"\n    helper to plot the altair financial charts\n    \"\"\"\n    return st.altair_chart(alt.Chart(df.head(x_cutoff)).mark_bar().encode(\n        x=x,\n        y=y\n        ).properties(title=title)\n    ) \n\n\n\nCreate a Streamlit sidebar to display relevant information about the app for the user.\n\nHere is an overview of the app's functionality:\n\nwith st.sidebar:\n    st.markdown(\"\"\"\n    # Ask the Oracle of Omaha: Using LLMs... :moneybag:\n    This app enables exploration into the World...\n    \"\"\")\n\n\nUse Streamlit tabs to break up different sections of the app.\n\ntab1\n\ntab1 provides users with the ability to request financial statement information from Snowflake. The user's question is captured in the str_input variable. Use this input in your prompts.fs_chain() function, which performs similarity matching on the user's question's embedding to find the most relevant SQL DDL embeddings. OpenAI then produces a query text with Snowflake syntax, using this as context. The text is stored in the output['result'], which you pass as an argument to conn.query() to have Snowflake execute the query.\n\nHere is the second attempt (if the first one fails):\n\n# create tabs\ntab1, tab2, tab3 = st.tabs([\n    \"Financial Statement Natural Language Querying :dollar:\", \n    \"Financial Data Exploration :chart_with_upwards_trend:\",\n    \"Shareholder Letter Natural Language Querying :memo:\"]\n    )\n\nwith tab1:\n    st.markdown(\"\"\"\n    # Natural Language Financials Querying :dollar:\n    ### Leverage LLMs to translate natural language questions\n    ...\n\t\t\"\"\")\n    \n    str_input = st.text_input(label='What would you like to answer? (e.g. What was the revenue and net income for Apple for the last 5 years?)')\n\n    if str_input:\n        with st.spinner('Looking up your question in Snowflake now...'):\n            try:\n                output = prompts.fs_chain(str_input)\n                try:\n                    # if the output doesn't work we will try one additional attempt to fix it\n                    query_result = conn.query(output['result'])\n                    if len(query_result) > 1:\n                        st.write(query_result)\n                        st.write(output)\n                except:\n                    st.write(\"The first attempt didn't pull what you were needing. Trying again...\")\n                    output = prompts.fs_chain(f'You need to fix the code. If the question is complex, consider using one or more CTE. Also, examine the DDL statements and try to correct this question/query: {output}')\n                    st.write(conn.query(output['result']))\n                    st.write(output)\n            except:\n                st.write(\"Please try to improve your prompt or provide feedback on the error encountered\")\n\n\ntab2\n\ntab2 displays financial information for a selected ticker (sel_ticker), which users choose via a Streamlit selectbox. The pull_financials() function retrieves the relevant financial statements from Snowflake for the selected ticker.\n\nTo improve readability, use two columns to display four financial metrics. I chose to show net income, net income ratio (profit margin), free cash flow, and debt-to-equity ratio, but you can display any metrics you want. The metrics include the most recent year's value and the change from the previous year as the displayed \"delta.\" Under each metric, the plot_financials() function shows the metric value by year.\n\nFinally, the user can select a financial statement to view the complete data:\n\nwith tab2: \n    st.markdown(\"\"\"\n    # Financial Data Exploration :chart_with_upwards_trend:\n\n    View financial statement data... \n    \"\"\")\n    sel_tick = st.selectbox(\"Select a ticker to view\", tick_list)\n\n    # pull the financial statements\n    # This whole section could be more efficient...\n    inc_st = pull_financials(sf_db, sf_schema, 'income_statement_annual', sel_tick)\n    bal_st = pull_financials(sf_db, sf_schema, 'balance_sheet_annual', sel_tick)\n    bal_st['debt_to_equity'] = bal_st['total_debt'].div(bal_st['total_equity'])\n    cf_st =  pull_financials(sf_db, sf_schema, 'cash_flow_statement_annual', sel_tick) \n  \n    col1, col2 = st.columns((1,1))\n    with col1:\n        # Net Income metric\n        net_inc = kpi_recent(inc_st, 'net_income')\n        st.metric('Net Income', \n                  f'${net_inc[0]}B', \n                  delta=round(net_inc[0]-net_inc[1],2),\n                  delta_color=\"normal\", \n                  help=None, \n                  label_visibility=\"visible\")\n        plot_financials(inc_st, 'year', 'net_income', year_cutoff, 'Net Income')\n        \n        # netincome ratio\n        net_inc_ratio = kpi_recent(inc_st, 'net_income_ratio', periods=2, unit=1)\n        st.metric('Net Profit Margin', \n                  f'{round(net_inc_ratio[0]*100,2)}%',\n                  delta=round(net_inc_ratio[0]-net_inc_ratio[1],2), \n                  delta_color=\"normal\", \n                  help=None, \n                  label_visibility=\"visible\")\n        plot_financials(inc_st, 'year', 'net_income_ratio', year_cutoff, 'Net Profit Margin')\n    \n    with col2:\n        # free cashflow\n        fcf = kpi_recent(cf_st, 'free_cash_flow' )\n        st.metric('Free Cashflow', \n                  f'${fcf[0]}B', \n                  delta=round(fcf[0]-fcf[1],2), \n                  delta_color=\"normal\", \n                  help=None, \n                  label_visibility=\"visible\")\n        plot_financials(cf_st, 'year', 'free_cash_flow', year_cutoff, 'Free Cash Flow')\n\n        # debt to equity\n        debt_ratio = kpi_recent(bal_st, 'debt_to_equity', periods=2, unit=1)\n        st.metric('Debt to Equity', \n                  f'{round(debt_ratio[0],2)}', \n                  delta=round(debt_ratio[0]-debt_ratio[1],2), \n                  delta_color=\"normal\", \n                  help=None, \n                  label_visibility=\"visible\")\n        plot_financials(bal_st, 'year', 'debt_to_equity', year_cutoff, 'Debt to Equity')\n\n    # enable a financial statment to be selected and viewed\n    sel_statement = st.selectbox(\"Select a statement to view\", fin_statement_list)\n    fin_statement_dict = {'income_statement': inc_st,\n                          'balance_sheet': bal_st, \n                          'cash_flow_statement':cf_st}\n    st.dataframe(fin_statement_dict[sel_statement])\n\ntab3\n\ntab3 lets users ask questions about Buffet's shareholder letters. To do this, collect the user's question in the query variable and use the prompts.letter_chain() function. It performs a similarity search with the vectors stored in Pinecone. Using the three most similar embeddings, the OpenAI LLM call produces an answer, if applicable, in the context. The result of this process is a dictionary that contains both the \"result\" and \"source_documents,\" which are then displayed for the user:\n\nwith tab3:\n    st.markdown(\"\"\"\n    # Shareholder Letter Natural Language Querying :memo:\n    ### Ask questions from all of...\n\n    These letters are much anticipated...\n    \"\"\"\n    )\n\n    query = st.text_input(\"What would you like to ask Warren Buffett?\")\n    if len(query)>1:\n        with st.spinner('Looking through lots of Shareholder letters now...'):\n            \n            try:\n                st.caption(\":blue[Warren's response] :sunglasses:\")\n                #st.write(prompts.letter_qa(query))\n                result = prompts.letter_chain(query)\n                st.write(result['result'])\n                st.caption(\":blue[Source Documents Used] :üìÑ:\")\n                st.write(result['source_documents'])\n            except:\n                st.write(\"Please try to improve your question\")\n\nWrapping up\n\nThank you for sticking with me until the end! I covered much ground, showing how LLMs can generate meaningful SQL queries and responses from databases like Snowflake. LLMs can also ask questions from various documents and produce meaningful results. The toolsets available for working with LLMs are expanding rapidly, and Streamlit is the perfect tool for demonstrating all of this in one app. Of course, you can improve it by using st.chat and LangChain. Still, it's impressive what you can do in a day!\n\nIf you have any questions, please post them in the comments below or contact me on LinkedIn, Twitter, Medium, or GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Untitled.png (2000√ó918)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled.png",
    "html": ""
  },
  {
    "title": "query_profile.jpeg (1106√ó1622)",
    "url": "https://blog.streamlit.io/content/images/2023/10/query_profile.jpeg#border",
    "html": ""
  },
  {
    "title": "Screenshot-2023-10-04-at-11.17.40-AM.png (1950√ó1670)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-04-at-11.17.40-AM.png#browser",
    "html": ""
  },
  {
    "title": "Generate interview questions from a candidate‚Äôs tweets",
    "url": "https://blog.streamlit.io/generate-interview-questions-from-a-candidates-tweets/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nGenerate interview questions from a candidate‚Äôs tweets\n\nMake an AI assistant to prepare for interviews with LangChain and Streamlit\n\nBy Greg Kamradt\nPosted in LLMs, June 24 2023\nStep 1. Make sure the major pieces of your app work\nTwitter\nWebpages\nYouTube videos\nBring it all together\nStep 2. Port your code over to a single script and add Streamlit support\nStep 3. Deploy and test\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, team! üëã\n\nMy name is Greg Kamradt, and I teach people how to analyze data and build AI apps. I‚Äôve spent much of my career doing product analytics in B2B environments at enterprises and startups. I love enabling people to make an impact in their workplace.\n\nPreparing for an interview can be time-consuming, so I built an app that analyzes a candidate's Twitter, YouTube videos, and web pages to generate a list of interview questions. I use LangChain, ChatGPT4, and Streamlit to package it all into a convenient tool.\n\nIn this post, I‚Äôll cover:\n\nAn introduction to working with GPT4 through LangChain\nA UX convention to ensure that users don‚Äôt need to supply a prompt\nA working understanding of Tweepy (Python library for interacting with Twitter data)\nAn introduction to web-scraping with LLMs\nBringing all your data together with a prompt to prepare for the meeting\n\n\nüë¥\nWant to skip reading? Here's a link to the Jupyter Notebook, GitHub repo for the app, and the resultant app.\n\nIf you‚Äôre a visual learner, here is a video outlining this process in detail:\n\nStep 1. Make sure the major pieces of your app work\n\nThe app consists of two main steps:\n\nGathering data\nProcessing the data with a language model\n\nThe data-gathering process involves three sources:\n\nTwitter: Tweets likely have the most up-to-date and relevant information about what's on the person‚Äôs mind\nWebpages: To get information about a person, it's best to include a biography or an \"About me\" page\nYouTube videos: This might include an interview with the person or a talk they gave\nTwitter\n\nLet‚Äôs create a function that utilizes Tweepy to pull the most popular recent tweets from a person.\n\nThe goal is to return a string of text so you can pass it to your LLM later:\n\ndef get_original_tweets(screen_name, tweets_to_pull=80, tweets_to_return=80):\n\n\t# Tweepy set up\n\t\n\tauth= tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n\t    auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n\t    api= tweepy.API(auth)\n\t\n\t# Holder for the tweets you'll find\n\ttweets= []\n\t\n\t# Go and pull the tweets\n\ttweepy_results= tweepy.Cursor(api.user_timeline,\n\t                                   screen_name=screen_name,\n\t                                   tweet_mode='extended',\n\t                                   exclude_replies=True).items(tweets_to_pull)\n\t\n\t# Run through tweets and remove retweets and quote tweets so we can only look at a user's raw emotions\n    for status in tweepy_results:\n        if hasattr(status, 'retweeted_status')or hasattr(status, 'quoted_status'):\n        # Skip if it's a retweet or quote tweet\n            continue\n        else:\n            tweets.append({'full_text': status.full_text, 'likes': status.favorite_count})\n\n\n        # Sort the tweets by number of likes. This will help us short_list the top ones later\n    sorted_tweets= sorted(tweets, key=lambda x: x['likes'], reverse=True)\n\n        # Get the text and drop the like count from the dictionary\n    full_text= [x['full_text']for xin sorted_tweets][:tweets_to_return]\n\n        # Convert the list of tweets into a string of tweets we can use in the prompt later\n    users_tweets= \"\\\\n\\\\n\".join(full_text)\n\n        return users_tweets\n\nWebpages\n\nTo pull data from web pages, make a simple request with the requests library and pass that information through Beautiful Soup and markdownify.\n\nAgain, you want to return a simple piece of text to insert into your prompt later:\n\ndef pull_from_website(url):\n\n\t# Doing a try in case it doesn't work\n\ttry:\n\t        response= requests.get(url)\n\texcept:\n\t# In case it doesn't work\n\t\tprint (\"Whoops, error\")\n\t\treturn\n\t\n\t# Put your response in a beautiful soup\n\tsoup= BeautifulSoup(response.text, 'html.parser')\n\t\n\t# Get your text\n\ttext= soup.get_text()\n\t\n\t# Convert your html to markdown. This reduces tokens and noise\n\ttext= md(text)\n\t\n\treturn text\n\nYouTube videos\n\nLastly, use LangChain‚Äôs YouTube video document loader. By default, this loader returns a list of LangChain documents.\n\nYou want the plain text to pass through to your prompt later:\n\ndef get_video_transcripts(url):\n    loader= YoutubeLoader.from_youtube_url(url, add_video_info=True)\n    documents= loader.load()\n    transcript= ' '.join([doc.page_contentfor docin documents])\n    \n\treturn transcript\nBring it all together\n\nOnce you have all of this information, combine it into a single string:\n\nuser_information = user_tweets + website_data + video_text\n\nBecause the string‚Äôs length might be too long for your model, split it into chunks and process them individually:\n\n# First we make our text splitter\ntext_splitter= RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=2000)\n\n# Then we split our user information into different documents\ndocs = text_splitter.create_documents([user_information])\n\n\nNext, pass the docs through a custom map and reduce prompt (learn more about chain types in my video). To help with your custom prompts, use LangChain‚Äôs prompt templates.\n\nFirst, your map step:\n\nmap_prompt= \"\"\"You are a helpful AI bot that aids a user in research.\nBelow is information about a person named {persons_name}.\nInformation will include tweets, interview transcripts, and blog posts about {persons_name}\nYour goal is to generate interview questions that we can ask {persons_name}\nUse specifics from the research when possible\n\n% START OF INFORMATION ABOUT {persons_name}:\n{text}\n% END OF INFORMATION ABOUT {persons_name}:\n\nPlease respond with list of a few interview questions based on the topics above\n\nYOUR RESPONSE:\"\"\"\nmap_prompt_template= PromptTemplate(template=map_prompt, input_variables=[\"text\", \"persons_name\"])\n\n\nThen your combined step:\n\ncombine_prompt= \"\"\"\nYou are a helpful AI bot that aids a user in research.\nYou will be given a list of potential interview questions that we can ask {persons_name}.\n\nPlease consolidate the questions and return a list\n\n% INTERVIEW QUESTIONS\n{text}\n\"\"\"\ncombine_prompt_template= PromptTemplate(template=combine_prompt, input_variables=[\"text\", \"persons_name\"])\n\n\nNow that you have your data and prompts set up pass this information through your LLM:\n\nllm= ChatOpenAI(temperature=.25, model_name='gpt-4')\n\nchain= load_summarize_chain(llm,\n                             chain_type=\"map_reduce\",\n                             map_prompt=map_prompt_template,\n                             combine_prompt=combine_prompt_template,\n#                              verbose=True\n)\n\n\nThis command will run the API call to OpenAI:\n\noutput= chain({\"input_documents\": docs, \"persons_name\": \"Elad Gil\"})\n\n\nI tested this out, pretending I was going to interview Elad Gil. The results had awesome questions!\n\n1. As an investor and advisor to various AI companies, what are some common challenges you've observed in the industry, and how do you recommend overcoming them?\n\n2. Can you elaborate on the advantages of bootstrapping for AI startups and share any success stories you've come across?\n\n3. What are some key lessons you've learned from your experiences in high-profile companies like Twitter, Google, and Color Health that have shaped your approach to investing and advising startups?\n\n4. How do you think AI will continue to shape the job market in the coming years?\n\n5. What motivated you to enter the healthcare space as a co-founder of Color Health, and how do you envision the role of AI in improving healthcare outcomes?\n\n\nüë¥\nNOTE: The results likely won‚Äôt be copy/paste ready. You‚Äôll need to edit the text to match your voice and style.\nStep 2. Port your code over to a single script and add Streamlit support\n\nFinally, combine it into a single script and add Streamlit support!\n\nSee the complete code in this app's main.py. I like to add some styling and information at the top of my apps. It provides more context and eases the user into the app.\n\nOne of my favorite Streamlit containers is st.columns. Let's use that to add some text and a picture:\n\n\t# Start Of Streamlit page\nst.set_page_config(page_title=\"LLM Assisted Interview Prep\", page_icon=\":robot:\")\n\n# Start Top Information\nst.header(\"LLM Assisted Interview Prep\")\n\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.markdown((\"Have an interview coming up? I bet they are on Twitter or YouTube or the web. \"\n\t\t             \"This tool is meant to help you generate interview questions based off of \"\n\t\t             \"topics they've recently tweeted or talked about.\"\n\t\t             \"\\\\n\\\\n\"\n\t\t             \"This tool is powered by [BeautifulSoup](<https://beautiful-soup-4.readthedocs.io/en/latest/#>), \"\n\t\t             \"[markdownify](<https://pypi.org/project/markdownify/>), [Tweepy](<https://docs.tweepy.org/en/stable/api.html>), \"\n\t\t             \"[LangChain](<https://langchain.com/>), and [OpenAI](<https://openai.com>) and made by \"\n\t\t             \"[@GregKamradt](<https://twitter.com/GregKamradt>).\"\n\t\t             \"\\\\n\\\\n\"\n\t\t             \"View Source Code on [Github](<https://github.com/gkamradt/globalize-text-streamlit/blob/main/main.py>)\"))\nwith col2:\n    st.image(image='Researcher.png', width=300, caption='Mid Journey: A researcher who is really good at their job and utilizes twitter to do research about the person they are interviewing. playful, pastels. --ar 4:7')\n# End Top Information\n\n\nNow let's add a few input forms for the user to provide the candidate's information:\n\nThe st.text_input widgets let you accept information from the user and pass it to the application later:\n\nperson_name = st.text_input(label=\"Person's Name\",  placeholder=\"Ex: Elad Gil\", key=\"persons_name\")\ntwitter_handle = st.text_input(label=\"Twitter Username\",  placeholder=\"@eladgil\", key=\"twitter_user_input\")\nyoutube_videos = st.text_input(label=\"YouTube URLs (Use , to separate videos)\",  placeholder=\"Ex: <https://www.youtube.com/watch?v=c_hO_fjmMnk>, <https://www.youtube.com/watch?v=c_hO_fjmMnk>\", key=\"youtube_user_input\")\nwebpages = st.text_input(label=\"Web Page URLs (Use , to separate urls. Must include https://)\",  placeholder=\"<https://eladgil.com/>\", key=\"webpage_user_input\")\n\n\nI wanted the user to be able to select the type of output they prefer. They may not want interview questions but a one-page summary about a person.\n\nUsing a Streamlit radio button, they can select their preferred option:\n\noutput_type = st.radio(\n\"Output Type:\",\n('Interview Questions', '1-Page Summary'))\n\n\nBased on their selection, you'll pass different instructions to your prompt. You could build out many more options if you'd like!\n\nresponse_types = {\n\t'Interview Questions' : \"\"\"\n\tYour goal is to generate interview questions that we can ask them\n\tPlease respond with list of a few interview questions based on the topics above\n\"\"\",\n'1-Page Summary' : \"\"\"\n\tYour goal is to generate a 1 page summary about them\n\tPlease respond with a few short paragraphs that would prepare someone to talk to this person\n\"\"\"\n}\n\n\nNext, let's add a button to control the flow of the application. By default, Streamlit will update the app after any field has changed. While this is great for some use cases, I don't want my LLM to start running until the user is finished.\n\nTo control this, I'll add a button that will only run after clicking the \"Generate Summary\" button. button_ind will only be set to true if the button was clicked during the last run:\n\nbutton_ind = st.button(\"*Generate Output*\", type='secondary', help=\"Click to generate output based on information\")\n\n# Checking to see if the button_ind is true. If so, this means the button was clicked and we should process the links\nif button_ind:\n\t# Make the call to your LLM\n\n\nNext, take the code you'd previously written in your Jupyter Notebook and output it to your Streamlit page using st.write:\n\noutput = chain({\"input_documents\": user_information_docs, # The seven docs that were created before\n\"persons_name\": person_name,\n\"response_type\" : response_types[output_type]\n})\n\nst.markdown(f\"#### Output:\")\nst.write(output['output_text'])\n\n\nLet's check out the output for Elad:\n\nAwesome!\n\nStep 3. Deploy and test\n\nGreat! Now that your script works locally, you can deploy it on Streamlit Community Cloud so others can use it. Remember to load your .env variables as secrets on Streamlit.\n\nTo see a video of this deployment, click here.\n\nTo check out the live app, click here.\n\nWrapping up\n\nThanks for joining me on this journey! You created an app that summarized the information you needed to prepare for meeting an interview candidate.\n\nIf you have any questions, please post them in the comments below or contact me on Twitter or email contact@dataindependent.com.\n\nHappy coding! üë¥\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "AIInterviewer-flowchart.png (816√ó766)",
    "url": "https://blog.streamlit.io/content/images/2023/08/AIInterviewer-flowchart.png#border",
    "html": ""
  },
  {
    "title": "Todd Wang - Streamlit",
    "url": "https://blog.streamlit.io/author/todd/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Todd Wang\n1 post\nAI Interviewer: Customized interview preparation with generative AI\n\nHow we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!\n\nLLMs\nby\nHaoxiang Jia and¬†\n1\n¬†more,\nAugust 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Trubrics: A user feedback tool for your AI Streamlit apps",
    "url": "https://blog.streamlit.io/trubrics-a-user-feedback-tool-for-your-ai-streamlit-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nTrubrics: A user feedback tool for your AI Streamlit apps\n\nA 3-step guide on collecting, analyzing, and managing AI model feedback\n\nBy Jeff Kayne\nPosted in Advocate Posts, July 28 2023\n1. How to create a free account with Trubrics\n2. How to collect user feedback from your AI Streamlit app\n3. How to analyze and manage your user feedback in Trubrics\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, community! üëã\n\nMy name is Jeff, and I‚Äôm a co-founder of Trubrics. As a data scientist and Machine Learning engineer, I have experienced firsthand the challenges of deploying new ML models without understanding how users interact with them. This can lead to reduced model performance and, ultimately, misaligned models and users.\n\nMore specifically, here is why you should start listening to your users:\n\nüö® Identify bugs: Users constantly run inference on your models and may be more likely to find bugs than your ML monitoring system.\nüßë‚ÄçüíªÔ∏è Fine-tune: Users often have domain knowledge that can be useful in fine-tuning models.\nüë• Align: Identifying user preferences can help align models with users.\n\nIf you want to generate user insights on your AI models, we've built Trubrics, the first user insights platform for AI models.\n\nIn this post, we'll cover:\n\nHow to create a free account with Trubrics\nHow to collect user feedback from your AI Streamlit app\nHow to analyze and manage your user feedback in Trubrics\nüö®\nTLDR: Here is the app and the repo, and here is the video (watch it above).\n1. How to create a free account with Trubrics\n\nCreate an account directly in the Trubrics app:\n\nAfter logging in, you'll find a pre-existing default component.\n\nClick on it to access code snippets and start collecting feedback. You can create different feedback components to collect and organize feedback across multiple projects or apps.\n\nNow, to save your first piece of feedback to the default component, head over to our example user feedback LLM app:\n\n2. How to collect user feedback from your AI Streamlit app\n\nLet's explore various code snippets to see how you can embed Trubrics feedback components directly into your app.\n\nFirst, install our SDK with the streamlit dependency (if it's not already installed):\n\npip install \"trubrics[streamlit]\"\n\n\nThen copy and paste this snippet into your app:\n\nimport streamlit as st\nfrom trubrics.integrations.streamlit import FeedbackCollector\n\ncollector = FeedbackCollector(\n    email=st.secrets.TRUBRICS_EMAIL,\n    password=st.secrets.TRUBRICS_PASSWORD,\n    project=\"default\"\n)\n\nuser_feedback = collector.st_feedback(\n    component=\"default\",\n    feedback_type=\"thumbs\",\n    open_feedback_label=\"[Optional] Provide additional feedback\",\n    model=\"gpt-3.5-turbo\",\n    prompt_id=None,  # checkout collector.log_prompt() to log your user prompts\n)\n\nif user_feedback:\n    st.write(user_feedback)\n\nWhat's going on here? Let's break it down:\n\nThe FeedbackCollector object. Store your Trubrics credentials in st.secrets, and specify the project to which you want to save. In this case, use the default.\nIts st_feedback() method. Calling this method allows users to embed UI widgets in their apps. These widgets can be added throughout your app to collect feedback on different predictions.\n\nAnd that's it!\n\nNow you'll see a thumbs up / down feedback widget in your application, like this:\n\nüö®\nTo learn more about customizing your feedback component, read our docs.\n3. How to analyze and manage your user feedback in Trubrics\n\nAfter you've saved the feedback to Trubrics, consult the üëç Feedback page for quantitative and qualitative analysis. This enables AI teams to understand whether users are satisfied with predictions and compare results between different models.\n\nVarious filters on the User feedback tab allow AI teams to:\n\nAggregate responses by frequency (hourly, daily, weekly, monthly)\nView all responses for a specific score, model, or user\nCompare responses for all scores, models, or users\n\nFor qualitative feedback, user comments are collected in the text field of the `Feedback` response. All comments are listed in the Comments tab and may be grouped to create an issue. AI model issues can also be opened or closed on the issues page.\n\nWrapping up\n\nThank you for reading! You've learned how to collect app user feedback and view it in Trubrics. If you have any questions, please post them in the comments below or contact me directly on my LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Haoxiang Jia - Streamlit",
    "url": "https://blog.streamlit.io/author/haoxiang/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Haoxiang Jia\n1 post\nAI Interviewer: Customized interview preparation with generative AI\n\nHow we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!\n\nLLMs\nby\nHaoxiang Jia and¬†\n1\n¬†more,\nAugust 9 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts: Using LLMs with Streamlit",
    "url": "https://blog.streamlit.io/tag/llms/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in LLMs\n33 posts\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nLLMs\nby\nAdrien Treuille and¬†\n1\n¬†more,\nJune 15 2023\nLangChain tutorial #3: Build a Text Summarization app\n\nExplore the use of the document loader, text splitter, and summarization chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 13 2023\nLangChain tutorial #2: Build a blog outline generator app in 25 lines of code\n\nA guide on conquering writer‚Äôs block with a Streamlit app\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 7 2023\nLangChain tutorial #1: Build an LLM-powered app in 18 lines of code\n\nA step-by-step guide using OpenAI, LangChain, and Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 31 2023\n8 tips for securely using API keys\n\nHow to safely navigate the turbulent landscape of LLM-powered apps\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 19 2023\nHow to build an LLM-powered ChatBot with Streamlit\n\nA step-by-step guide using the unofficial HuggingChat API\n\nLLMs\nby\nChanin Nantasenamat\n,\nMay 10 2023\nChat with the Cat Generative Dialogue Processor (CatGDP)\n\nBuild your own catbot with a quirky persona!\n\nAdvocate Posts\nby\nTianyi Pan\n,\nMay 3 2023\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nAdvocate Posts\nby\nDmitry Kosarevsky\n,\nApril 18 2023\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nAdvocate Posts\nby\nKanak Mittal\n,\nApril 11 2023\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nLLMs\nby\nDave Lin\n,\nApril 6 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Greg Kamradt - Streamlit",
    "url": "https://blog.streamlit.io/author/greg-kamradt/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Greg Kamradt\n1 post\nGenerate interview questions from a candidate‚Äôs tweets\n\nMake an AI assistant to prepare for interviews with LangChain and Streamlit\n\nLLMs\nby\nGreg Kamradt\n,\nJune 24 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "prompttools-playground.gif (1258√ó782)",
    "url": "https://blog.streamlit.io/content/images/2023/08/prompttools-playground.gif#browser",
    "html": ""
  },
  {
    "title": "Dave Lin - Streamlit",
    "url": "https://blog.streamlit.io/author/dave/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Dave Lin\n4 posts\nHow to build an interconnected multi-page Streamlit app\n\nFrom planning to execution‚Äîhow I built GPT lab\n\nLLMs\nby\nDave Lin\n,\nJuly 19 2023\nSemantic search, Part 2: Building a local search app\n\nMaking an app with Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data from Snowflake Marketplace\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 18 2023\nSemantic search, Part 1: Implementing cosine similarity\n\nWrangling Foursquare data and implementing semantic search in Snowflake\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 17 2023\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nLLMs\nby\nDave Lin\n,\nApril 6 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Randy Pettus - Streamlit",
    "url": "https://blog.streamlit.io/author/randy/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Randy Pettus\n1 post\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "run-clear-share.png (1490√ó394)",
    "url": "https://blog.streamlit.io/content/images/2023/08/run-clear-share.png#border",
    "html": ""
  },
  {
    "title": "portfolio-streamlit-app.gif (702√ó382)",
    "url": "https://blog.streamlit.io/content/images/2023/10/portfolio-streamlit-app.gif",
    "html": ""
  },
  {
    "title": "dynamic-sidebar.png (642√ó734)",
    "url": "https://blog.streamlit.io/content/images/2023/08/dynamic-sidebar.png",
    "html": ""
  },
  {
    "title": "instruction-mode.png (648√ó740)",
    "url": "https://blog.streamlit.io/content/images/2023/08/instruction-mode.png",
    "html": ""
  },
  {
    "title": "awesome-grid-layout.png (1514√ó1474)",
    "url": "https://blog.streamlit.io/content/images/2023/08/awesome-grid-layout.png#border",
    "html": ""
  },
  {
    "title": "awesome-sdr-example.png (1532√ó1252)",
    "url": "https://blog.streamlit.io/content/images/2023/08/awesome-sdr-example.png#border",
    "html": ""
  },
  {
    "title": "Kevin Tse - Streamlit",
    "url": "https://blog.streamlit.io/author/kevin-tse/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Kevin Tse\n1 post\nExploring LLMs and prompts: A guide to the PromptTools Playground\n\nLearn how to build dynamic, stateful applications that harness multiple LLMs at once\n\nLLMs\nby\nSteve Krawczyk and¬†\n1\n¬†more,\nAugust 18 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Steve Krawczyk - Streamlit",
    "url": "https://blog.streamlit.io/author/steve/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Steve Krawczyk\n1 post\nExploring LLMs and prompts: A guide to the PromptTools Playground\n\nLearn how to build dynamic, stateful applications that harness multiple LLMs at once\n\nLLMs\nby\nSteve Krawczyk and¬†\n1\n¬†more,\nAugust 18 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Caroline Frasca - Streamlit",
    "url": "https://blog.streamlit.io/author/caroline/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Caroline Frasca\n1 post\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nLLMs\nby\nCaroline Frasca and¬†\n2\n¬†more,\nAugust 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Yi Ding - Streamlit",
    "url": "https://blog.streamlit.io/author/yi/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Yi Ding\n1 post\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nLLMs\nby\nCaroline Frasca and¬†\n2\n¬†more,\nAugust 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Krista Muir - Streamlit",
    "url": "https://blog.streamlit.io/author/krista/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Krista Muir\n1 post\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nLLMs\nby\nCaroline Frasca and¬†\n2\n¬†more,\nAugust 23 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "yes.png (1830√ó548)",
    "url": "https://blog.streamlit.io/content/images/2023/08/yes.png",
    "html": ""
  },
  {
    "title": "no.png (1830√ó548)",
    "url": "https://blog.streamlit.io/content/images/2023/08/no.png",
    "html": ""
  },
  {
    "title": "LlamaIndexChatbot.png (1202√ó1004)",
    "url": "https://blog.streamlit.io/content/images/2023/08/LlamaIndexChatbot.png",
    "html": ""
  },
  {
    "title": "Data analysis with Mito: A powerful spreadsheet in Streamlit",
    "url": "https://blog.streamlit.io/data-analysis-with-mito-a-powerful-spreadsheet-in-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nBy Nate Rush\nPosted in Advocate Posts, August 8 2023\nWhy Mito?\nTab renaming\nColumn filtering\nFormula writing\nUse case 1: Mito internal Streamlit app\nUse case 2: Python script without coding\nUse case 3: Mito in any Streamlit app\nWrapping up\nContents\nShare this post\n‚Üê All posts\nTL;DR: The Mito spreadsheet is a drop-in replacement for st.dataframe or st.data_editor. View and edit dataframes using spreadsheet formulas, pivot tables, graphs, and more. For every edit, Mito generates the corresponding Python code. Check out the sample app and the code. Enjoy!\n\n\n\n\n\nHiya, Streamlit users! üëã\n\nI'm Nate, co-founder of Mito‚Äîa spreadsheet that helps analysts transition from Excel to Python. I've been working on Mito for almost four years now (oof). For the past two years, our open-source community and enterprise clients have been asking us to bring Mito to Streamlit. Here is why.\n\nWhy Mito?\n\nStreamlit apps that let users upload unstructured data often encounter issues because users upload data in many different formats.\n\nThe typical solution is for the users to upload a file of their choosing and for the app creator to provide a variety of st.text_input, st.selectbox, and st.button that let them:\n\nRename, move, and delete columns\nChange the types of columns\nFilter null and other unwanted values\nAnd much more\n\nThis leads to apps with many inputs and strict requirements for user-provided data, effectively drowning users in basic data cleaning and transformation options.\n\nIt's not easy to provide enough configuration options for the data formats that users bring to the table. Users often get stuck configuring a single additional parameter (e.g., how many rows to skip before the header row). As a result, they can't use the app you worked so hard to create.\n\nCheck out this sample app to try Mito's flexible data importing, cleaning, preprocessing tools, and importing methods, including:\n\nImporting CSV/Excel files, including advanced configuration options\nRenaming, reordering, and removing columns in place\nFiltering in a classic interface with many filter conditions\nWriting spreadsheet formulas to transform your data\n\nMito provides data cleaning options beyond your basic data importing. Here are a few examples.\n\nTab renaming\n\nNeed to rename columns to match the expected format? Rename the tabs directly in a spreadsheet:\n\nColumn filtering\n\nWant to run the rest of your app on a subset of your data? Use a spreadsheet to filter out the data you need:\n\nFormula writing\n\nNeed to let users transform columns in a more complex way? Let them write formulas as they do in Excel:\n\nWith Mito, users can import a dataset of their choice into your app and format it as your app requires.\n\nUse case 1: Mito internal Streamlit app\n\nAt Mito, we use an internal Streamlit app to monitor the current state of our company. It displays our current revenue, expenses, customer information, the number of blog posts from the previous week, and other relevant data. You can select variables to compare, contrast, regress, and more‚Äîto understand how Mito performs over time.\n\nNOTE: Making our internal app was a breeze. Streamlit can be the easiest way to explore datasets of varying complexity. Check out this fantastic app example which provides helpful, pre-configured views for gaining insight from data and graphs.\n\nAs the app's creator, I wanted my team members to be able to compare the number of sales we made this month with the number of sales from the previous month. So I created the following input:\n\nmin_date, max_date = st.date_input('Compare within Range', value=(one_week_ago, today))\n\n\nThis method is great for comparing date ranges but can't answer ad-hoc questions like \"How has the number of motorcycles we've sold per month changed?\" To answer questions like that, you can use Mito's pivot table feature (you can also write formulas and generate graphs right within your Streamlit app):\n\nUse case 2: Python script without coding\n\nAt Mito, we work with financial institutions that have thousands of users poring over spreadsheets and running multiple spreadsheet processes. Those spreadsheets can have hundreds of tabs and thousands of formulas. And if the person responsible for the spreadsheet leaves, it can take weeks or even months for someone else to audit and use it.\n\nSo, what can you do?\n\nOne option is to train your spreadsheet users to learn Python. But not everyone wants to do that.\n\nprint(\"hello world, I don't think I love programming...\")\n\n\nAnother option is to use Mito! Mito enables non-programmers to write Python code without ever needing to see it. Every spreadsheet edit generates the corresponding code in the backend (a full script is created to codify the process).\n\nTry this sample app to see how it works (here is the code for it):\n\nUse case 3: Mito in any Streamlit app\n\nUsing Mito is super simple.\n\nJust install the Mito package with pip install mitosheet:\n\n# In a terminal\npip install mitosheet\n\n# In your Streamlit app\nfrom mitosheet.streamlit.v1 import spreadsheet\n\n...\n\nspreadsheet(df)\n\n\nNext, display any dataframes inside the spreadsheet component:\n\nimport pandas as pd\nimport streamlit as st\nfrom mitosheet.streamlit.v1 import spreadsheet\n\n# Create a dataframe with pandas (you can pass any pandas dataframe)\ndataframe = pd.DataFrame({'A': [1, 2, 3]})\n\n# Display the dataframe in a Mito spreadsheet\nfinal_dfs, code = spreadsheet(dataframe)\n\n# Display the final dataframes created by editing the Mito component\n# This is a dictionary from dataframe name -> dataframe\nst.write(final_dfs)\n\n# Display the code that corresponds to the script\nst.code(code)\n\n\nAnd you're done!\n\nWrapping up\n\nMito has been four years in the making, and we're excited to finally share it with you! Over the coming weeks, we'll be improving our Streamlit support, including the additional functionality within the spreadsheet, the ability to set predefined views, and more configuration options.\n\nAs we roll out more features, we'd love to hear your feedback. Please open an issue on GitHub or leave us comments below.\n\nHappy app-building! üßë‚Äçüíª\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "ChatGPT.png (1202√ó1004)",
    "url": "https://blog.streamlit.io/content/images/2023/08/ChatGPT.png",
    "html": ""
  },
  {
    "title": "llamaindexgif.gif (1517√ó1117)",
    "url": "https://blog.streamlit.io/content/images/2023/08/llamaindexgif.gif#browser",
    "html": ""
  },
  {
    "title": "lottie.png (2000√ó708)",
    "url": "https://blog.streamlit.io/content/images/2023/10/lottie.png",
    "html": ""
  },
  {
    "title": "rag-with-llamaindex-1.png (1088√ó458)",
    "url": "https://blog.streamlit.io/content/images/2023/08/rag-with-llamaindex-1.png#border",
    "html": ""
  },
  {
    "title": "Untitled--18-.png (798√ó332)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--18-.png",
    "html": ""
  },
  {
    "title": "Untitled--19-.png (886√ó325)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--19-.png",
    "html": ""
  },
  {
    "title": "Untitled--17-.png (631√ó211)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--17-.png",
    "html": ""
  },
  {
    "title": "Untitled--16-.png (1438√ó810)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--16-.png",
    "html": ""
  },
  {
    "title": "Untitled--15-.png (1487√ó353)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--15-.png",
    "html": ""
  },
  {
    "title": "Untitled--14-.png (1578√ó399)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--14-.png",
    "html": ""
  },
  {
    "title": "Untitled--13-.png (401√ó277)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--13-.png",
    "html": ""
  },
  {
    "title": "Develop Streamlit apps in-browser with GitHub Codespaces",
    "url": "https://blog.streamlit.io/edit-inbrowser-with-github-codespaces/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDevelop Streamlit apps in-browser with GitHub Codespaces\n\nBuild anywhere without the hassle of a local Python environment\n\nBy Brian Holt\nPosted in GitHub Codespaces, September 14 2023\nNow, it's all in your browser ü™Ñ\n#1: Create a new app\n#2: Fork an existing app\n#3 Edit an existing app\nWatch Codespaces in action\nWhy Github Codespaces\nGive it a spin\nContents\nShare this post\n‚Üê All posts\n\nPicture this: You're about to create your first Streamlit app, or maybe you've found an awesome Streamlit app you want to dive into. In both scenarios, you can't begin building or examine the code without a local development setup.\n\nWhat if you could do all of this without the need for a local Python environment?\n\nNow, it's all in your browser ü™Ñ\n\nWith GitHub Codespaces, you can skip the local environment and enjoy:\n\nInstant setup: Create, fork, and deploy data apps in a single click.\nFrictionless editing: Explore and debug source code, with libraries pre-configured.\nDevelop anywhere: Enjoy the flexibility to build Streamlit apps from any browser.\n\nThere are three ways to use Codespaces: creating a new app, editing an existing app, and forking an existing one.\n\n#1: Create a new app\n\nTo see it in action, simply log onto Community Cloud and create a new app. (See docs for step-by-step guide.) You can also edit an existing app in your browser.\n\n#2: Fork an existing app\n\nFrom any public Streamlit app, click ‚ÄúFork this app''. Copy the app or explore how it works all within your browser. Then, deploy to Community Cloud to share what you have built!\n\nYou can also fork and spin up a Codespace directly from an app‚Äôs repository. Just select the \"Create codespace on master\" button.\n\n#3 Edit an existing app\n\nFlexible development is not limited to creating new apps or forking existing ones. Simply select \"Edit\" in Community Cloud and click the \"Create Codespace\" button. For a more detailed walkthrough, check out our docs.\n\nWatch Codespaces in action\n\nIn this video, @DataProfessor puts it all together! Watch step-by-step how you can use GitHub Codespaces to build Streamlit apps in the browser.\n\n\nWhy Github Codespaces\n\nWe wanted to give developers an in-browser editor that is free, powerful, easy and secure.\n\nWith GitHub Codespaces, you‚Äôll have access to:\n\nAmple free tier: Each month, you‚Äôll have 60 hours of run time on 2 core Codespaces, plus 15 GB of storage.\nA real Linux operating system: develop and deploy on the same system.\nSeamless tech stack: use the tools you already love, like Visual Studio Code.\nConvenient hosting: Easily host and share your app with Community Cloud.\nGitHub‚Äôs world class security: have the peace of mind that your code and networks are secure.\nGive it a spin\n\nStart editing in-browser today. Check out the documentation for additional details. We want to reduce frictions so users can create more amazing data apps.\n\nWhat do you think about this new feature? How can we improve your development experience?\n\nHappy Streamlitting üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in GitHub Codespaces...\n\nView even more ‚Üí\n\nDevelop Streamlit apps in-browser with GitHub Codespaces\n\nBuild anywhere without the hassle of a local Python environment\n\nGitHub Codespaces\nby\nBrian Holt\n,\nSeptember 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Untitled--12-.png (899√ó216)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--12-.png",
    "html": ""
  },
  {
    "title": "Untitled--11-.png (1150√ó1046)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--11-.png#border",
    "html": ""
  },
  {
    "title": "Untitled--10-.png (830√ó467)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--10-.png",
    "html": ""
  },
  {
    "title": "Untitled--9-.png (1299√ó366)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--9-.png",
    "html": ""
  },
  {
    "title": "Untitled--8-.png (780√ó893)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--8-.png",
    "html": ""
  },
  {
    "title": "Untitled--7-.png (2220√ó450)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--7-.png",
    "html": ""
  },
  {
    "title": "Untitled--6-.png (252√ó149)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--6-.png",
    "html": ""
  },
  {
    "title": "Untitled--5-.png (861√ó523)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--5-.png",
    "html": ""
  },
  {
    "title": "Recording-2023-09-21-at-12.03.38.gif (769√ó102)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Recording-2023-09-21-at-12.03.38.gif",
    "html": ""
  },
  {
    "title": "---1-.png (1550√ó900)",
    "url": "https://blog.streamlit.io/content/images/2023/10/---1-.png",
    "html": ""
  },
  {
    "title": "Untitled--4-.png (795√ó167)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--4-.png",
    "html": ""
  },
  {
    "title": "Untitled--3-.png (763√ó164)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--3-.png",
    "html": ""
  },
  {
    "title": "PureHuB: A search engine for your university",
    "url": "https://blog.streamlit.io/purehub-a-search-engine-for-your-university/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nBy Mala Deep Upadhaya\nPosted in Advocate Posts, August 10 2023\nWhy PureHuB?\nApp overview\n1. Scrapper setup\n2. Preprocessing\n2.1. Tokenization\n2.2 Stopwords\n2.3 Porter-Stemmer algorithms\n2.4 Term Frequency - Inverse Document Frequency (TF-IDF)\n2.5 Cosine similarity\n2.6 Inverted index\nStep 3. Building the front end\nKey takeaways\nWrapping up\nContents\nShare this post\n‚Üê All posts\nTL;DR: I made PureHuB (code), an inverted indexing search engine, using Python and libraries like BeautifulSoup and NLTK for web scraping, data preprocessing, and natural language processing.\n\n\n\nHey, community! üëã\n\nMy name is Mala Deep Upadhaya, and I'm an independent consultant specializing in data visualization and analysis projects. I also teach people how to analyze data and share insights in a more accessible manner. I built PureHuB to give access to profiles, groundbreaking work, and cutting-edge research produced by the extraordinary minds of Coventry University.\n\nIn this post, I'll walk you through the process of building it step-by-step:\n\nScrapper setup\nPreprocessing\nBuilding the front end\n\nBut first‚Ä¶\n\nWhy PureHuB?\n\nHumans have always been driven by their inherent curiosity to seek out information. This is one of the motivations behind developing search engines [1]. From WebCrawler, created by Brian Pinkerton, a computer science student at the University of Washington, to the web3.0-based search engines like Presearch, to AltaVista, Lycos, Yahoo, HotBot, and Google in 1988, many search engines have grown rapidly [2]. There are 29 generic search engines and numerous domain-specific ones [3].\n\nUnlike general search engines that index and search the entire web for relevant results, vertical search engines focus on specific data sets. This makes them faster and more relevant when users look for specific information. Google Books, Google News, Google Flights, and Google Finance are examples of vertical searches within the Google search engine. These search engines were created to ease the burden of searching for specialized information or topics.\n\nApp overview\n\nHere is how the app works.\n\nTo search for research, enter your query in the \"Search research\" field. Then, choose an operator: \"Exact\" or \"Relevant.\" Select \"Exact\" for an exact match of your query or \"Relevant\" for the relevance and ranking of the search results. You can also choose the search type between \"Publications\" and \"Authors.\"\n\nNext, click \"SEARCH.\" The app will give you the most relevant results:\n\n\n\nThe dataset used in this study was scraped from Coventry University's Pure Portal using Python, BeautifulSoup, and Selenium. The seed URL for the crawler was https://pureportal.coventry.ac.uk/en/publications/. The scraped dataset consists of 4022 publications with four attributes: publication name, publication URL, author name, and date of publication.\n1. Scrapper setup\n\nFor web scraping and automation, I used two popular Python libraries:\n\nBeautifulSoup for parsing the HTML of Pure Portal. It provides an easy and convenient way to extract data from web pages by navigating the HTML structure.\nSelenium for automating web browsers. It allows you to control a web browser programmatically and was used to perform various actions, such as clicking buttons and navigating between pages, without human intervention.\n\nTo account for website exploration through browsers, I incorporated the Chrome browser and used the ChromeDriver, a separate executable that bridges Selenium and the Chrome browser. It has better compatibility, debugging, and logging features during development. I also implemented a 1-second time delay.\n\nHere is the code:\n\nimport os  # Module for interacting with the operating system\nimport time  # Module for time-related operations\nimport ujson  # Module for working with JSON data\nfrom random import randint  # Module for generating random numbers\nfrom typing import Dict, List, Any  # Type hinting imports\n\nimport requests  # Library for making HTTP requests\nfrom bs4 import BeautifulSoup  # Library for parsing HTML data\nfrom selenium import webdriver  # Library for browser automation\nfrom selenium.common.exceptions import NoSuchElementException  # Exception for missing elements\nfrom webdriver_manager.chrome import ChromeDriverManager  # Driver manager for Chrome (We are using Chromium based )\n\ndef initCrawlerScraper(seed,max_profiles=500):\n    # Initialize driver for Chrome\n    webOpt = webdriver.ChromeOptions()\n    webOpt.add_experimental_option('excludeSwitches', ['enable-logging'])\n    webOpt.add_argument('--ignore-certificate-errors')\n    webOpt.add_argument('--incognito')\n    webOpt.headless = True\n    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webOpt)\n    driver.get(seed)  # Start with the original link\n\n    links = []  # Array with pureportal profiles URL\n    pub_data = []  # To store publication information for each pureportal profile\n\n    nextLink = driver.find_element_by_css_selector(\".nextLink\").is_enabled()  # Check if the next page link is enabled\n    print(\"Crawler has begun...\")\n    while (nextLink):\n        page = driver.page_source\n        # XML parser to parse each URL\n        bs = BeautifulSoup(page, \"lxml\")  # Parse the page source using BeautifulSoup\n\n        # Extracting exact URL by spliting string into list\n        for link in bs.findAll('a', class_='link person'):\n            url = str(link)[str(link).find('<https://pureportal.coventry.ac.uk/en/persons/'):].split('>\"')\n            links.append(url[0])\n            \n        # Click on Next button to visit next page\n        try:\n            if driver.find_element_by_css_selector(\".nextLink\"):\n                element = driver.find_element_by_css_selector(\".nextLink\")\n                driver.execute_script(\"arguments[0].click();\", element)\n            else:\n                nextLink = False\n        except NoSuchElementException:\n            break\n            \n        # Check if the maximum number of profiles is reached\n        if len(links) >= max_profiles:\n            break\n            \n    print(\"Crawler has found \", len(links), \" pureportal profiles\")\n    write_authors(links, 'Authors_URL.txt') # Write the authors' URLs to a file\n\n    print(\"Scraping publication data for \", len(links), \" pureportal profiles...\")\n    count = 0\n    for link in links:\n        # Visit each link to get data\n        time.sleep(1)  \n        driver.get(link)\n        try:\n            if driver.find_elements_by_css_selector(\".portal_link.btn-primary.btn-large\"):\n                element = driver.find_elements_by_css_selector(\".portal_link.btn-primary.btn-large\")\n                for a in element:\n                    if \"research output\".lower() in a.text.lower():\n                        driver.execute_script(\"arguments[0].click();\", a)\n                        driver.get(driver.current_url)\n                        # Get name of Author\n                        name = driver.find_element_by_css_selector(\"div[class='header person-details']>h1\")\n                        r = requests.get(driver.current_url)\n                        # Parse all the data via BeautifulSoup\n                        soup = BeautifulSoup(r.content, 'lxml')\n\n                        # Extracting publication name, publication url, date and CU Authors\n                        table = soup.find('ul', attrs={'class': 'list-results'})\n                        if table != None:\n                            for row in table.findAll('div', attrs={'class': 'result-container'}):\n                                data = {}\n                                data['name'] = row.h3.a.text\n                                data['pub_url'] = row.h3.a['href']\n                                date = row.find(\"span\", class_=\"date\")\n\n                                rowitem = row.find_all(['div'])\n                                span = row.find_all(['span'])\n                                data['cu_author'] = name.text\n                                data['date'] = date.text\n                                print(\"Publication Name :\", row.h3.a.text)\n                                print(\"Publication URL :\", row.h3.a['href'])\n                                print(\"CU Author :\", name.text)\n                                print(\"Date :\", date.text)\n                                print(\"\\\\n\")\n                                pub_data.append(data)\n            else:\n                # Get name of Author\n                name = driver.find_element_by_css_selector(\"div[class='header person-details']>h1\")\n                r = requests.get(link)\n                # Parse all the data via BeautifulSoup\n                soup = BeautifulSoup(r.content, 'lxml')\n                # Extracting publication name, publication URL, date and CU Authors\n                table = soup.find('div', attrs={'class': 'relation-list relation-list-publications'})\n                if table != None:\n                    for row in table.findAll('div', attrs={'class': 'result-container'}):\n                        data = {}\n                        data[\"name\"] = row.h3.a.text\n                        data['pub_url'] = row.h3.a['href']\n                        date = row.find(\"span\", class_=\"date\")\n                        rowitem = row.find_all(['div'])\n                        span = row.find_all(['span'])\n                        data['cu_author'] = name.text\n                        data['date'] = date.text\n                        print(\"Publication Name :\", row.h3.a.text)\n                        print(\"Publication URL :\", row.h3.a['href'])\n                        print(\"CU Author :\", name.text)\n                        print(\"Date :\", date.text)\n                        print(\"\\\\n\")\n                        pub_data.append(data)\n        except Exception:\n            continue\n\n    print(\"Crawler has scrapped data for \", len(pub_data), \" pureportal publications\")\n    driver.quit()\n    # Writing all the scraped results in a file with JSON format\n    with open('scraper_results.json', 'w') as f:\n        ujson.dump(pub_data, f)\n\ninitCrawlerScraper('<https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/>', max_profiles=500)\n\n\nI imported the necessary modules and libraries for interacting with the operating system, performing time-related operations, working with JSON data, generating random numbers, making HTTP requests, parsing HTML data, and automating web browsing using Selenium.\n\nThe code defines a function called initCrawlerScraper, which takes a seed URL and an optional max_profiles parameter. The function uses web scraping techniques to extract publication data from the crawled web pages. It finds links to individual profiles, visits each profile, and retrieves publication information associated with each profile. The extracted data includes publication names, URLs, publication dates, and the authors' names from Coventry University. The extracted publication data is stored in the pub_data list.\n\nFinally, the function saves the authors' URLs in a file named \"Authors_URL.txt\" and the scraped publication data in a JSON file named \"scraper_results.json\".\n\n2. Preprocessing\n\nI loaded the obtained scrapped file and imported NLTK libraries, including stopwords, word_tokenize, and Porter-Stemmer, for natural language processing (NLP) tasks.\n\n2.1. Tokenization\n\nTokenization is the process of breaking down a text or a sequence of characters into smaller units called tokens. They can be individual words, sentences, or even sub-word units, depending on the level of tokenization applied [4].\n\nTo tokenize each publication name and the author's name into individual words, I used the nltk.tokenize module's word_tokenize function (takes the input as tokens):\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Open a file with publication names in read mode\nwith open('pub_name.json', 'r') as f:\n    publication = f.read()\n\n# Load JSON File\npubName = ujson.loads(publication)\n\n# Tokenization\ntokenized_pub_list = []\nfor file in pubName:\n    tokens = word_tokenize(file)\n    tokenized_pub_list.append(tokens)\n\n# Save tokenized publication list to a file\nwith open('tokenized_pub_list.json', 'w') as f:\n    ujson.dump(tokenized_pub_list, f)\n\n\nThis code snippet loads publication names from the pub_name.json file. Then, word_tokenize from NLTK is used to tokenize each publication name. The tokenized publication lists are stored in the tokenized_pub_list list.\n\nFinally, the tokenized publication list is saved to the tokenized_pub_list.json file.\n\n2.2 Stopwords\n\nI used the nltk.corpus.stopwords module to remove commonly used English stopwords. Stopwords are words that are considered insignificant [5], such as \"I\", \"he\", \"a\", \"the\", etc. This allows focusing on more meaningful words.\n\nHere is the code:\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Open a file with publication names in read mode\nwith open('pub_name.json', 'r') as f:\n    publication = f.read()\n\n# Load JSON File\npubName = ujson.loads(publication)\n\n# Predefined stopwords in NLTK are used\nstop_words = stopwords.words('english')\n\n# Stopword removal\nfiltered_pub_list = []\nfor file in pubName:\n    tokens = word_tokenize(file) #from tokenized \n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    filtered_pub = ' '.join(filtered_tokens)\n    filtered_pub_list.append(filtered_pub)\n\n# Save filtered publication list to a file\nwith open('filtered_pub_list.json', 'w') as f:\n    ujson.dump(filtered_pub_list, f)\n\n\nThis code snippet loads publication names from the pub_name.json file, then removes stopwords using stopwords.words('english') from the NLTK library. Each publication name is tokenized using word_tokenize, and stopwords are filtered out from the tokens. The filtered tokens are then joined back into a string, and the filtered publication list is stored in the filtered_pub_list list.\n\nFinally, the filtered publication list is saved to the filtered_pub_list.json file.\n\n2.3 Porter-Stemmer algorithms\n\nStemming is the process of reducing words to their base or root form. The Porter-Stemmer algorithm applies a series of heuristic rules to remove common word endings and suffixes to achieve this reduction [6]. I used the PorterStemmer from nltk.stem to implement the Porter stemming algorithm and help group together similar words. In the search_data function, the Porter Stemmer is used to stem words before performing search operations. For example, the word \"computational\" and \"computer\" are reduced to \"compute.\"\n\nHere is the code:\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\n# Open a file with publication names in read mode\nwith open('pub_name.json', 'r') as f:\n    publication = f.read()\n\n# Load JSON File\npubName = ujson.loads(publication)\n\n# Initialize PorterStemmer\nstemmer = PorterStemmer()\n\n# Stemming\nstemmed_pub_list = []\nfor file in pubName:\n    tokens = word_tokenize(file)\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    stemmed_pub = ' '.join(stemmed_tokens)\n    stemmed_pub_list.append(stemmed_pub)\n\n# Save stemmed publication list to a file\nwith open('stemmed_pub_list.json', 'w') as f:\n    ujson.dump(stemmed_pub_list, f)\n\n\nThis code snippet loads publication names from the pub_name.json file applies Porter stemming using PorterStemmer from NLTK, and tokenizes each publication name using word_tokenize. The resulting tokens are derived using the stem() method of the PorterStemmer object. They're then joined back into a string, and the stemmed publication list is stored in the stemmed_pub_list list.\n\nFinally, the code saves the stemmed publication list to the stemmed_pub_list.json file.\n\n2.4 Term Frequency - Inverse Document Frequency (TF-IDF)\n\nInformation retrieval and text mining are two major problems frequently using the TF-IDF weight. TF-IDF is used to:\n\nDetermine the value of a word to a group of documents\nGive more significance to a term the more frequently it appears\n\nTF is determined by dividing the number of times a phrase appears in a document by the total number of words [6]. On the other hand, IDF measures the term's importance, calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the particular term appears [7].\n\nI used TF-IDF to calculate the relevance or similarity between documents based on their term frequencies and the rarity of the terms in the document collection. I applied the TF-IDF vectorization using the TfidfVectorizer class from the scikit-learn library and stored the resulting TF-IDF vectors in the temp_file variable.\n\nFinally, I used tfidf.transform(stem_word_file) to transform the stem_word_file (which represents a single document) into its TF-IDF vector representation and calculated the cosine similarity between temp_file and the TF-IDF vector of stem_word_file using the cosine_similarity() function.\n\nHere is the code:\n\nimport ujson\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Open a file with publication names in read mode\nwith open('pub_name.json', 'r') as f:\n    publication = f.read()\n\n# Load JSON File\npubName = ujson.loads(publication)\n\n# Initialize TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Calculate TF-IDF\ntfidf_matrix = vectorizer.fit_transform(pubName)\n\n# Get feature names (terms)\nfeature_names = vectorizer.get_feature_names()\n\n# Print TF-IDF scores\nfor i in range(len(pubName)):\n    print(\"Publication:\", pubName[i])\n    for j in range(len(feature_names)):\n        tfidf_score = tfidf_matrix[i, j]\n        if tfidf_score > 0:\n            print(\"  Term:\", feature_names[j])\n            print(\"  TF-IDF Score:\", tfidf_score)\n    print()\n\n\nIn this code snippet, the publication names are loaded from the pub_name.json file. The code uses the TfidfVectorizer from scikit-learn to calculate the TF-IDF scores. The fit_transform method is used to transform the publication names into a TF-IDF matrix. The feature names (terms) can be obtained using the get_feature_names method. The TF-IDF scores are printed for each publication and term combination.\n\n2.5 Cosine similarity\n\nRegardless of the size of the documents, cosine similarity can measure the text similarity between them. The cosine similarity metric has a value range from 0 to 1 and evaluates the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space; a higher value (1) indicates greater similarity [7].\n\nHere is the code:\n\nimport ujson\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Open a file with publication names in read mode\nwith open('pub_name.json', 'r') as f:\n    publication = f.read()\n\n# Load JSON File\npubName = ujson.loads(publication)\n\n# Initialize TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Calculate TF-IDF\ntfidf_matrix = vectorizer.fit_transform(pubName)\n\n# Calculate cosine similarity\ncosine_sim = cosine_similarity(tfidf_matrix)\n\n# Example: Calculate similarity between first two publications\npub1_index = 0\npub2_index = 1\nsimilarity_score = cosine_sim[pub1_index, pub2_index]\nprint(\"Similarity score between publication\", pub1_index, \"and publication\", pub2_index, \"is:\", similarity_score)\n\n\nThis code snippet loads publication names from the pub_name.json file. It uses the TfidfVectorizer to calculate the TF-IDF matrix and then the cosine_similarity function to compute the cosine similarity between all pairs of vectors in the TF-IDF matrix. The similarity score is accessed between any two publications by indexing the cosine_sim matrix with the corresponding indices.\n\nFinally, the resulting cosine similarity scores are assigned to the corresponding publication or author in the output_datadictionary. This output dictionary is what users see when they search.\n\n2.6 Inverted index\n\nAn inverted index is a data structure used in information retrieval systems, search engines, and text analysis to efficiently store and retrieve information about the presence of keywords (or words) in a collection of documents [8]. It's called \"inverted\" because it reverses the link between terms and documents compared to a standard index. In traditional indexes, documents are indexed based on their unique IDs, and each entry contains a list of terms found in that document. In an inverted index, terms are organized as keys and associated with a list of documents or occurrences where they exist.\n\nHere is the code:\n\ndata_dict = {} #empty dictionary\n\n# Indexing process\nfor a in range(len(pub_list_stem_wo_sw)):\n    for b in pub_list_stem_wo_sw[a].split():\n        if b not in data_dict:\n            data_dict[b] = [a]\n        else:\n            data_dict[b].append(a)\n\n\nTo create an inverted index, the code initializes a dictionary named data_dict. This process involves a nested loop. The outer loop iterates over the length of the pub_list_stem_wo_sw list. Within each iteration, the inner loop iterates over each word in the publication name obtained by splitting the name based on spaces.\n\nA new key-value pair is added during the inner loop if the current word is not already a key in data_dict. The key is the word itself, and the value is a list containing the current index of the publication in the pub_list_stem_wo_sw list. This way, each word is associated with a list of indices indicating the publications in which it appears. On the other hand, if the word already exists as a key in data_dict, the index of the current publication is appended to the existing list for that word. This allows for the indexing of multiple publications containing the same word.\n\nBy performing this process, the code creates an inverted index that facilitates efficient searching and retrieval of publications based on keywords. The resulting data_dict serves as a valuable resource for information retrieval tasks, providing a mapping from words to the publications where they occur. The same process was conducted for the author list.\n\nHere is the complete code:\n\nimport nltk #NLTK for natural language processing tasks\nfrom nltk.corpus import stopwords # list of stop word \nfrom nltk.tokenize import word_tokenize # To tokenize each word\nfrom nltk.stem import PorterStemmer # For specific rules to transform words to their stems\n\n#Preprosessing data before indexing\nwith open('scraper_results.json', 'r') as doc: scraper_results=doc.read()\n\npubName = []\npubURL = []\npubCUAuthor = []\npubDate = []\ndata_dict = ujson.loads(scraper_results)\narray_length = len(data_dict)\nprint(array_length)\n\n#Seperate name, url, date, author in different file\nfor item in data_dict:\n    pubName.append(item[\"name\"])\n    pubURL.append(item[\"pub_url\"])\n    pubCUAuthor.append(item[\"cu_author\"])\n    pubDate.append(item[\"date\"])\nwith open('pub_name.json', 'w') as f:ujson.dump(pubName, f)\nwith open('pub_url.json', 'w') as f:ujson.dump(pubURL, f)\nwith open('pub_cu_author.json', 'w') as f:ujson.dump(pubCUAuthor, f)\nwith open('pub_date.json', 'w') as f: ujson.dump(pubDate, f)\n\n#Open a file with publication names in read mode\nwith open('pub_name.json','r') as f:publication=f.read()\n\n#Load JSON File\npubName = ujson.loads(publication)\n\n#Predefined stopwords in nltk are used\nstop_words = stopwords.words('english')\nstemmer = PorterStemmer()\npub_list_first_stem = []\npub_list = []\npub_list_wo_sc = []\nprint(len(pubName))\n\nfor file in pubName:\n    #Splitting strings to tokens(words)\n    words = word_tokenize(file)\n    stem_word = \"\"\n    for i in words:\n        if i.lower() not in stop_words:\n            stem_word += stemmer.stem(i) + \" \"\n    pub_list_first_stem.append(stem_word)\n    pub_list.append(file)\n\n#Removing all below characters\nspecial_characters = '''!()-‚Äî[]{};:'\"\\\\, <>./?@#$%^&*_~0123456789+=‚Äô‚Äò'''\nfor file in pub_list:\n    word_wo_sc = \"\"\n    if len(file.split()) ==1 : pub_list_wo_sc.append(file)\n    else:\n        for a in file:\n            if a in special_characters:\n                word_wo_sc += ' '\n            else:\n                word_wo_sc += a\n        pub_list_wo_sc.append(word_wo_sc)\n\n#Stemming Process\npub_list_stem_wo_sw = []\nfor name in pub_list_wo_sc:\n    words = word_tokenize(name)\n    stem_word = \"\"\n    for a in words:\n        if a.lower() not in stop_words:\n            stem_word += stemmer.stem(a) + ' '\n    pub_list_stem_wo_sw.append(stem_word.lower())\n\ndata_dict = {}\n\n# Indexing process\nfor a in range(len(pub_list_stem_wo_sw)):\n    for b in pub_list_stem_wo_sw[a].split():\n        if b not in data_dict:\n             data_dict[b] = [a]\n        else:\n            data_dict[b].append(a)\n\nprint(len(pub_list_wo_sc))\nprint(len(pub_list_stem_wo_sw))\nprint(len(pub_list_first_stem))\nprint(len(pub_list))\n\nwith open('publication_list_stemmed.json', 'w') as f:\n    ujson.dump(pub_list_first_stem, f)\n\nwith open('publication_indexed_dictionary.json', 'w') as f:\n    ujson.dump(data_dict, f)\n\nStep 3. Building the front end\n\nI use Streamlit to build apps for my data science and ML projects [9]. It simplifies creating and deploying web-based user interfaces, making it easy to showcase work. Here I used Streamlit to create a search engine application portal.\n\nWithin the search portal, I defined a search function called search_data, which takes input text, operator value, and search type as parameters. The function first checks the operator value to determine which search method to use:\n\nExact: The input text is stemmed and compared with the publication or author index to find matching data. Cosine similarity is calculated between the stemmed word and the retrieved data, and the results are stored in an output dictionary.\nRelevant: Multiple words are processed similarly, and matching pointers are collected. The corresponding data is transformed and compared to calculate cosine similarity, and the results are added to the output dictionary. The function provides a way to search for relevant publication or author data based on user queries and retrieves results using cosine similarity.\n\nHere is the code:\n\ndef app():\n\n        # Load the image and display it\n    image = Image.open('cire.png')\n    st.image(image)\n\n    # Add a text description\n    st.markdown(\"<p style='text-align: center;'> Uncover the brilliance: Explore profiles, groundbreaking work, and cutting-edge research by the exceptional minds of Coventry University.</p>\", unsafe_allow_html=True)\n\n    input_text = st.text_input(\"Search research:\", key=\"query_input\")\n    operator_val = st.radio(\n        \"Search Filters\",\n        ['Exact', 'Relevant'],\n        index=1,\n        key=\"operator_input\",\n        horizontal=True,\n    )\n    search_type = st.radio(\n        \"Search in:\",\n        ['Publications', 'Authors'],\n        index=0,\n        key=\"search_type_input\",\n        horizontal=True,\n    )\n\n    if st.button(\"SEARCH\"):\n        if search_type == \"Publications\":\n            output_data = search_data(input_text, 1 if operator_val == 'Exact' else 2, \"publication\")\n        elif search_type == \"Authors\":\n            output_data = search_data(input_text, 1 if operator_val == 'Exact' else 2, \"author\")\n        else:\n            output_data = {}\n\n        # Display the search results\n        show_results(output_data, search_type)\n\n    st.markdown(\"<p style='text-align: center;'> Brought to you with ‚ù§ by <a href='<https://github.com/maladeep>'>Mala Deep</a> | Data ¬© Coventry University </p>\", unsafe_allow_html=True)\n\ndef show_results(output_data, search_type):\n    aa = 0\n    rank_sorting = sorted(output_data.items(), key=lambda z: z[1], reverse=True)\n\n    # Show the total number of research results\n    st.info(f\"Showing results for: {len(rank_sorting)}\")\n\n    # Show the cards\n    N_cards_per_row = 3\n    for n_row, (id_val, ranking) in enumerate(rank_sorting):\n        i = n_row % N_cards_per_row\n        if i == 0:\n            st.write(\"---\")\n            cols = st.columns(N_cards_per_row, gap=\"large\")\n        # Draw the card\n        with cols[n_row % N_cards_per_row]:\n            if search_type == \"Publications\":\n                st.caption(f\"{pub_date[id_val].strip()}\")\n                st.markdown(f\"**{pub_cu_author[id_val].strip()}**\")\n                st.markdown(f\"*{pub_name[id_val].strip()}*\")\n                st.markdown(f\"**{pub_url[id_val]}**\")\n            elif search_type == \"Authors\":\n                st.caption(f\"{pub_date[id_val].strip()}\")\n                st.markdown(f\"**{author_name[id_val].strip()}**\")\n                st.markdown(f\"*{pub_name[id_val].strip()}*\")\n                st.markdown(f\"**{pub_url[id_val]}**\")\n                st.markdown(f\"Ranking: {ranking[0]:.2f}\")\n\n        aa += 1\n\n    if aa == 0:\n        st.info(\"No results found. Please try again.\")\n    else:\n        st.info(f\"Results shown for: {aa}\")\n\nif __name__ == '__main__':\n    app()\n\nKey takeaways\n\nI created an inverted indexing search app that allows users to search for research papers or authors and return relevant results based on cosine similarity. I used NLTK for text preprocessing, TF-IDF vectorization for feature extraction, and cosine similarity for document similarity measurement. I implemented the search feature with choices for exact or relevant matching using OR and AND operators. The code analyzes user input and tokenizes search phrases, using Streamlit to execute searches in a user-friendly manner.\n\nA few limitations, if addressed, can lead to a better outcome:\n\nSlow crawling speed: Adding a one-second time delay between requests can significantly slow down the crawling process. This can be a constraint when working with large websites or accessing up-to-date information quickly.\nSearch precision: While cosine similarity is a widely used statistic to assess document similarity, it may not always adequately represent semantic meaning. The search result's accuracy can be improved using word embeddings, semantic similarity measurements, or parse_dot_topn [7].\n\nInverted indexing can improve search experiences by providing quick access to relevant content. You can use it as a starting point for additional modifications and customization based on your unique requirements and datasets.\n\nWrapping up\n\nThank you for reading my post! Now that you've learned how to create a search engine using Python, why not take it a step further and create a search engine tailored for your university? It's a great opportunity to streamline your academic pursuits and make your research more accessible. Try it and tell me how it goes in the comments below, on LinkedIn, or on Medium.\n\nHappy coding, and best of luck with your research! üéà\n\nReferences\n\n[1] One Second Internet Live Stats, \"1 Second - Internet Live Stats,\" www.internetlivestats.com. https://www.internetlivestats.com/one-second/#google-band\n[2] O. Whitcombe, \"The History of Search Engines,\" Liberty Digital Marketing, May 26, 2022. https://www.libertymarketing.co.uk/blog/a-history-of-search-engines/ (accessed Oct. 12, 2022).\n[3] Wikipedia, \"List of search engines,\" Wikipedia, May 22, 2023. https://en.wikipedia.org/w/index.php?title=List_of_search_engines&oldid=1156345015 (accessed Jun. 25, 2023).\n[4] M. Hassler and G. Fliedl, \"Text preparation through extended tokenization,\" Jun. 27, 2006. https://www.witpress.com/elibrary/wit-transactions-on-information-and-communication-technologies/37/16699 (accessed Aug. 11, 2022).\n[5] S. Sarica and J. Luo, \"Stopwords in Technical Language Processing,\" PLOS ONE, vol. 16, no. 8, p. e0254937, Aug. 2021, doi: https://doi.org/10.1371/journal.pone.0254937.\n[6] N. Tsourakis, Machine Learning Techniques for Text: Apply modern techniques with Python, 1st ed. Birmingham, UK: Packt Publishing Ltd., 2022.\n[7] M. D. Upadhaya, \"Surprisingly Effective Way To Name Matching In Python,\" Medium, Mar. 27, 2022. https://towardsdatascience.com/surprisingly-effective-way-to-name-matching-in-python-1a67328e670e\n[8] H. Yan, S. Ding, and T. Suel, \"Inverted index compression and query processing with optimized document ordering,\" The Web Conference, Apr. 2009, doi: https://doi.org/10.1145/1526709.1526764.\n[9] M. D. Upadhaya, \"Build Your First Data Visualization Web App in Python Using Streamlit,\" Medium, Mar. 05, 2021. https://towardsdatascience.com/build-your-first-data-visualization-web-app-in-python-using-streamlit-37e4c83a85db (accessed Jun. 25, 2023).\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Untitled--2-.png (1694√ó424)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--2-.png",
    "html": ""
  },
  {
    "title": "Untitled-2.png (2040√ó1464)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled-2.png#browser",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (822√ó347)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Untitled--1-.png",
    "html": ""
  },
  {
    "title": "chat_pandas.png (747√ó567)",
    "url": "https://blog.streamlit.io/content/images/2023/08/chat_pandas.png#border",
    "html": ""
  },
  {
    "title": "Santosh Kumar Radha - Streamlit",
    "url": "https://blog.streamlit.io/author/santosh/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Santosh Kumar Radha\n1 post\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 3)",
    "url": "https://blog.streamlit.io/page/3/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAI Interviewer: Customized interview preparation with generative AI\n\nHow we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!\n\nLLMs\nby\nHaoxiang Jia and¬†\n1\n¬†more,\nAugust 9 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nSimiLo: Find your best place to live\n\nA 5-step guide on how I built an app to relocate within the U.S.\n\nAdvocate Posts\nby\nKevin Soderholm\n,\nAugust 4 2023\nInstant Insight: Generate data-driven presentations in a snap!\n\nCreate presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery\n\nLLMs\nby\nOleksandr Arsentiev\n,\nAugust 2 2023\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nTrubrics: A user feedback tool for your AI Streamlit apps\n\nA 3-step guide on collecting, analyzing, and managing AI model feedback\n\nAdvocate Posts\nby\nJeff Kayne\n,\nJuly 28 2023\nChat2VIS: AI-driven visualisations with Streamlit and natural language\n\nLeverage ChatGPT for Python code generation using prompt engineering\n\nLLMs\nby\nPaula Maddigan\n,\nJuly 27 2023\nsnowChat: Leveraging OpenAI's GPT for SQL queries\n\nInteract with your Snowflake database using natural language queries\n\nSnowflake powered ‚ùÑÔ∏è\nby\nkaarthik Andavar\n,\nJuly 25 2023\nHow to analyze geospatial Snowflake data in Streamlit\n\nA guide to presenting vehicle accident data using Snowflake, Carto, Tableau, and Streamlit\n\nSnowflake powered ‚ùÑÔ∏è\nby\nBecky O'Connor\n,\nJuly 24 2023\nLangChain tutorial #5: Build an Ask the Data app\n\nLeverage Agents in LangChain to interact with pandas DataFrame\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Brian Holt - Streamlit",
    "url": "https://blog.streamlit.io/author/brian/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Brian Holt\n1 post\nDevelop Streamlit apps in-browser with GitHub Codespaces\n\nBuild anywhere without the hassle of a local Python environment\n\nGitHub Codespaces\nby\nBrian Holt\n,\nSeptember 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "GitHub Codespaces - Streamlit",
    "url": "https://blog.streamlit.io/tag/github-codespaces/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in GitHub Codespaces\n1 post\nDevelop Streamlit apps in-browser with GitHub Codespaces\n\nBuild anywhere without the hassle of a local Python environment\n\nGitHub Codespaces\nby\nBrian Holt\n,\nSeptember 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit-Blog.png (2000√ó1591)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Streamlit-Blog.png",
    "html": ""
  },
  {
    "title": "sidebar2.png (1248√ó960)",
    "url": "https://blog.streamlit.io/content/images/2023/10/sidebar2.png",
    "html": ""
  },
  {
    "title": "Streamlit and iFood: Empowering the Monitor Rosa project",
    "url": "https://blog.streamlit.io/streamlit-and-ifood-empowering-the-monitor-rosa-project/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nStreamlit and iFood: Empowering the Monitor Rosa project\n\nHarnessing technology and corporate support for social impact\n\nBy Heber Augusto Scachetti\nPosted in Advocate Posts, July 14 2023\nWhat is the Monitor Rosa project?\nApp overview\nHow was the app developed?\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit community! üëã\n\nMy name is Heber A. Scachetti, and I'm a Data Squad Lead at iFood.\n\nI joined iFood in 2020 after participating in a hackathon that introduced me to the Monitor Rosa project, a volunteer initiative aimed at helping the Brazilian Association of Lymphoma and Leukemia (Abrale) to address the challenges faced by patients with breast cancer.\n\nIn this post, I'll share with you how I used GitHub Actions, Google Storage, and Streamlit to create an app for the Monitor Rosa project.\n\nWhat is the Monitor Rosa project?\n\nBefore we dive into the app, I want to share with you a little background on the Monitor Rosa project and how iFood, Abrale, and Streamlit helped make it happen:\n\niFood. iFood has been instrumental in supporting my involvement in the Monitor Rosa project through its unique employee engagement program called \"Minha Quarta\" (\" My Wednesday\"). This initiative allows iFood employees, including myself, to dedicate every Wednesday to personal development, learning, and volunteering. By providing this dedicated time, iFood indirectly contributes to the project's success by enabling its employees to invest their skills and expertise in initiatives that make a meaningful impact on society.\nAbrale. Abrale is a non-profit organization created in 2002 by patients and their families with the mission of offering help and mobilizing partners so that all people with cancer and blood diseases have access to the best treatment. Abrale is the leader of the TJCC Movement (Movimento Todos Juntos Contra o C√¢ncer‚Äî\"All Together against Cancer\"), developing projects related to breast cancer, such as Monitor Rosa.\nStreamlit. Streamlit helped me create interactive data visualizations for the Monitor Rosa project.\n\nThe support from iFood has been invaluable, as it has allowed me to balance my professional responsibilities with my passion for making a difference in the lives of patients and their families. This partnership showcases the potential for technology companies to positively impact society by supporting initiatives that aim to improve the lives of those in need. It also highlights the importance of fostering a culture of learning and social responsibility within the corporate environment.\n\nAnd now, let's dive into the app!\n\nApp overview\n\nThe app consolidates data from different sources, including the Brazilian public health system (SUS) and other data providers. It helps you explore key performance indicators (KPIs) and gain insights into Brazil's current state of cancer and blood disease treatment.\n\nThere are three repositories:\n\ndevops-pysus-get-files automates the data collection from SUS and other sources, ensuring the project has access to the most up-to-date information.\nsus-kpis-analysis consolidates the collected data, maintains analyses of the KPIs, and will eventually contain models and other data artifacts that can be consumed by dashboards and other data visualization tools.\nstreamlit-monitor-rosa contains the Streamlit app (deployed on Streamlit Community Cloud) and serves as the main interface for users to interact with the data.\n\nThe following cutting-edge technologies ensure efficient data collection, storage, and visualization:\n\nGitHub Actions. At regular intervals, GitHub Actions automatically fetch data from the SUS FTP server and other sources for the project to have the most up-to-date information.\nGoogle Storage. Once collected, the data is stored in a Google Storage bucket‚Äîa centralized repository. This allows for easy access and management of the data by the project's various components. The data is then read and processed by the code in the sus-kpis-analysis repo, which consolidates the data and maintains analyses of the KPIs.\nStreamlit. The Streamlit application in the streamlit-monitor-rosa repo also accesses the data directly from the Google Storage bucket, ensuring that the dashboard displays the most recent information.\n\nTo protect sensitive information, such as Google Storage access keys, I used Streamlit Cloud secrets. This allows the team to securely store and manage sensitive data, ensuring that only authorized users can access the necessary resources.\n\nHow was the app developed?\n\nI wanted to create an app that could visualize data collected from SUS and accommodate additional features as the project progressed.\n\nI have previously used Streamlit and deployed apps to the Streamlit Community Cloud, so I knew it'd help me achieve my goal quickly. But I'd never connected it to a Google Cloud Storage bucket that housed the files collected and transformed by two other repositories. So I followed this Streamlit doc to set it up.\n\nHere is the code:\n\nfrom google.oauth2 import service_account\nfrom google.cloud import storage\nimport pandas as pd\n\n#Create API client\ncredentials = service_account.Credentials.from_service_account_info(st.secrets[\"gcp_service_account\"])\n\nclient = storage.Client(credentials=credentials)\nbucket = client.bucket(bucket_name)\ncontent = bucket.blob(file_path).download_as_bytes()\nbytes_io = BytesIO(content)\ndados_estad_mensal = pd.read_parquet(bytes_io)\n\n\n\nThe whole app is just over 130 lines of code, 30 of which read the file from the Google Storage bucket and perform additional calculations. I used:\n\nStreamlit Cloud secrets and the \"google-cloud-storage\" Python library to access the file content‚Äîa parquet file containing indicators for one of Brazil's states\npandas library to manage date filters, date transformations, and calculated column creation (such as a 6-month moving average)\n\nThe collected file already organizes indicators by date and cancer staging, which is a measure of cancer severity at the time of diagnosis. Early diagnoses are considered to be stages 0, 1, and 2, while late diagnoses are considered to be stages 3 and 4.\n\nOnce the file was collected, I created an app interface using a few Streamlit components and the Plotly library (for line charts). I used two components to filter the staging and metric displayed on the chart and a checkbox for the users to choose between using the data as a moving average or without any calculations.\n\nHere is the complete code:\n\nimport streamlit as st\nimport pandas as pd\n\nmetrics = {\n 'N√∫mero de pacientes em tratamento': 'numero_pacientes',   \n '√ìbitos':'obtitos',\n 'Custo':'custo_estadiamento',\n 'Custo por paciente': 'custo_por_paciente',\n 'N√∫mero de diagnosticos': 'numero_diagnosticos'      \n}\n\n# cancer stage filter\nall_symbols = dados_estad_mensal.estadiamento.unique()\nsymbols = st.multiselect(\"Estadiamentos\", all_symbols, all_symbols)\n\n# metric filter\nmetrics_selector = st.selectbox(\n    \"M√©trica\",\n    list(metrics.keys())\n)\ny_column_name = metrics[metrics_selector]\n\n# enable/disable moving average values\nma_option = st.checkbox('M√©dia m√≥vel (6 meses)')\nif ma_option:\n    y_column_name = f'{metrics[metrics_selector]}_ma'\n\n# get data by column name\ndataset = dados_estad_mensal[dados_estad_mensal.primeiro_estadiamento.isin(symbols)]\n\n# create figure to plotly chart\nfig = px.line(\n    dataset, \n    x='data', \n    y=y_column_name, \n    color='estadiamento', \n    symbol=\"estadiamento\")\n\n# Update layout (yaxis title and responsive legend)\nfig.update_layout(\n    yaxis_title=metrics_selector,\n    legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=-0.2,\n            xanchor=\"left\",\n            x=0.01\n            )    \n)\n\n# shows chat using streamlit magic :) \nst.plotly_chart(\n    fig, \n    use_container_width=True)\n\n\nAnd here is what the final app looks like:\n\nKey insights gained from this app include:\n\nThe cost per patient is higher for late-stage diagnoses (stages 3 or 4)\nThe number of monthly deaths among patients with late-stage diagnoses is more than double that of early-stage diagnoses\nThe number of patients diagnosed with stage 4 cancer has been increasing over the past six months\nWrapping up\n\nI hope I was able to show you how the power of technology and data-driven solutions can address social issues. Using Streamlit, GitHub Actions, and Google Storage, I created a valuable tool for the Monitor Rosa project. This tool can aid healthcare managers in understanding and addressing the challenges faced by patients with breast cancer in Brazil.\n\nIf you have any questions, please leave them in the comments below or contact me on LinkedIn.\n\nThank you, and happy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Ara Ghukasyan - Streamlit",
    "url": "https://blog.streamlit.io/author/ara/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Ara Ghukasyan\n1 post\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Filip Boltuzic - Streamlit",
    "url": "https://blog.streamlit.io/author/filip/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Filip Boltuzic\n1 post\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nDrill-downs and filtering with Streamlit and¬†Altair\n\nDisplay an Altair chart definition in Streamlit using the st.altair_chart widget\n\nAdvocate Posts\nby\nCarlos D Serrano\n,\nJuly 12 2023\nESG reporting with Streamlit\n\nEvaluate ESG-related unstructured data on Snowflake with semantha\n\nSnowflake powered ‚ùÑÔ∏è\nby\nSven Koerner and¬†\n1\n¬†more,\nJune 23 2023\nDisplay a race on a live map üèÉ\n\nCreate a real-time Streamlit dashboard with Apache Kafka, Apache Pinot, and Python Twisted library\n\nAdvocate Posts\nby\nMark Needham\n,\nJune 22 2023\nSemantic search, Part 2: Building a local search app\n\nMaking an app with Streamlit, Snowflake, OpenAI, and Foursquare‚Äôs free NYC venue data from Snowflake Marketplace\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 18 2023\nSemantic search, Part 1: Implementing cosine similarity\n\nWrangling Foursquare data and implementing semantic search in Snowflake\n\nSnowflake powered ‚ùÑÔ∏è\nby\nDave Lin\n,\nMay 17 2023\nAnalyzing real estate properties with Streamlit\n\nA 7-step tutorial on how to make your own real estate app\n\nAdvocate Posts\nby\nVin√≠cius Oviedo\n,\nMay 16 2023\nLearn Morse code with a Streamlit app\n\n5 steps to build your own Morse code tutor!\n\nAdvocate Posts\nby\nAlice Heiman\n,\nMay 12 2023\nThe ultimate Wordle cheat sheet\n\nLearn how to beat Wordle with Streamlit\n\nAdvocate Posts\nby\nSiavash Yasini\n,\nMay 11 2023\nConvert images into pixel art\n\nA 5-step tutorial for making a pixel art converter app\n\nAdvocate Posts\nby\nsoma noda\n,\nMay 8 2023\nAccessible color themes for Streamlit apps\n\nControl your app‚Äôs color scheme and visual accessibility\n\nAdvocate Posts\nby\nYuichiro Tachibana (Tsuchiya)\n,\nMay 5 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Heber Augusto Scachetti - Streamlit",
    "url": "https://blog.streamlit.io/author/heber/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Heber Augusto Scachetti\n1 post\nStreamlit and iFood: Empowering the Monitor Rosa project\n\nHarnessing technology and corporate support for social impact\n\nAdvocate Posts\nby\nHeber Augusto Scachetti\n,\nJuly 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Deep-learning apps for image processing made easy: A step-by-step guide",
    "url": "https://blog.streamlit.io/deep-learning-apps-for-image-processing-made-easy-a-step-by-step-guide/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nBy Mainak Chaudhuri\nPosted in Advocate Posts, August 22 2023\nApp overview\nStep 1. Train your image-processing model\nStep 2. Develop, test, and save your model\nStep 3. Build the app's UI and embedding functions\nStep 4. Test the app using image samples\nStep 5. Try the app on your local machine\nStep 6. Share your app on GitHub\nStep 7. Deploy your app to the Streamlit Community Cloud\nWrapping Up\nContents\nShare this post\n‚Üê All posts\nü•≠\nMake your own custom deep-learning app with just a trained image file and a few settings! Check out the demo app and the code.\n\nAre you looking to build your own deep-learning app that uses image processing without diving into model training? Whether you're interested in agriculture, healthcare, or any field involving image processing, this guide is for you.\n\nMy personal journey began with a mango leaf disease detection app (I love mangos), but I quickly discovered how easy it is to integrate any trained image processing model into a user-friendly app. Just provide a .h5 file of your trained image, specify image dimensions and output classes, and you‚Äôre done!\n\nTry it for yourself:\n\n\n\n\nIn this post, I'll walk you through the process of creating a custom deep-learning app:\n\nTrain your image-processing model\nDevelop, test, and save your model\nBuild the app's UI and embedding functions\nTest the app using image samples\nTry out the app on your local machine\nShare your app on GitHub\nDeploy your app to the Streamlit Community Cloud\nApp overview\n\nThe app integrates image capture, preprocessing, ML disease detection models, a disease database, and user-friendly interfaces. It captures input images, processes them using ML models for disease classification, and provides real-time results along with personalized treatment suggestions. And the best part? You can ensure that your model stays accurate and your database is up to date with regular updates.\n\nLet's dive into making your own deep-learning app!\n\nStep 1. Train your image-processing model\n\nLet‚Äôs start by training an image processing model. Generate the EfficientNet Deep Learning model as described here and follow these steps (I used a Kaggle dataset):\n\nOpen a new tab in your favorite browser\nType \"Colab\" or \"Google Colab\" in the search bar to get to Google Colab notebooks\nCreate a new Jupyter notebook\nFollow along with this code or upload this notebook to Colab\nDownload the Mango Leaf Disease Dataset from Kaggle (to train the ML model)\nUpload this dataset to Google Drive and enable sharing (paste the link in the file access snippet)\n\nUse this code:\n\n# Generate data paths with labels\ndata_dir = '/kaggle/input/mango-leaf-disease-dataset'\nfilepaths = []\nlabels = []\n\nfolds = os.listdir(data_dir)\nfor fold in folds:\n    foldpath = os.path.join(data_dir, fold)\n    filelist = os.listdir(foldpath)\n    for file in filelist:\n        fpath = os.path.join(foldpath, file)\n        filepaths.append(fpath)\n        labels.append(fold)\n\n# Concatenate data paths with labels into one dataframe\nFseries = pd.Series(filepaths, name= 'filepaths')\nLseries = pd.Series(labels, name='labels')\ndf = pd.concat([Fseries, Lseries], axis= 1)\n\n\nNext, do data extraction, pre-processing, visualization, and modeling:\n\n# Splitting data into train-test sets\n\n# train dataframe\ntrain_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123)\n\n# valid and test dataframe\nvalid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123)\n\n\nCreate an image generator function that resizes all uploaded images to the appropriate input dimensions:\n\n# cropped image size\nbatch_size = 16\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\n\n# Recommended : use custom function for test data batch size, else we can use normal batch size.\nts_length = len(test_df)\ntest_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\n\n# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\ndef scalar(img):\n    return img\n\ntr_gen = ImageDataGenerator(preprocessing_function= scalar)\nts_gen = ImageDataGenerator(preprocessing_function= scalar)\n\ntrain_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\nvalid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\n# Note: we will use custom test_batch_size, and make shuffle= false\ntest_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n\n\nYou already imported libraries Tensorflow and Keras. Next, call the Efficient Net module from these libraries and fit the image data with the appropriate dimensions:\n\n# Create Model Structure\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\nclass_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n\n# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n# we will use efficientnetb3 from EfficientNet family.\nbase_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n# base_model.trainable = False\n\nmodel = Sequential([\n    base_model,\n    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n    Dropout(rate= 0.45, seed= 123),\n    Dense(class_count, activation= 'softmax')\n])\n\nmodel.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n\n\nRun multiple epochs on the data to get a satisfactory level of accuracy, specifically in terms of test accuracy:\n\nts_length = len(test_df)\ntest_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\n\ntrain_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\nvalid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\ntest_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n\nprint(\"Train Loss: \", train_score[0])\nprint(\"Train Accuracy: \", train_score[1])\nprint('-' * 20)\nprint(\"Validation Loss: \", valid_score[0])\nprint(\"Validation Accuracy: \", valid_score[1])\nprint('-' * 20)\nprint(\"Test Loss: \", test_score[0])\nprint(\"Test Accuracy: \", test_score[1])\n\n\nFinally, generate the model file:\n\nmodel.save(\"name of the model.h5\")\n\n\nYou‚Äôve completed the model training and can now safely use the .h5 file. Think of it as a proxy for your model. It‚Äôll perform the same tasks as the deep learning model (Efficient Net), which you tested by uploading sample images into a Jupyter Notebook.\n\nStep 2. Develop, test, and save your model\n\nWith your trained model ready, let‚Äôs dive into testing and evaluation. Save your model as a .h5 file, a powerful tool that makes creating the app super easy.\n\nFirst, set a variable in which you can import the model class. The keyword 'history' is used to record the training metrics while the models perform an epoch. An epoch is an iterative method of training to increase the accuracy and fit best to the extracted set of parameters on the data.\n\nUse this code:\n\n# set an accuracy measurement metrics for the code\n# save the history of the measurement metrices after each iteration (epoch)\n# return the lowest loss value and the greatest accuracy value after the completion of \n# the epochs\n\nvariablestr_acc= history.history['accuracy']\ntr_loss= history.history['loss']\nval_acc= history.history['val_accuracy']\nval_loss= history.history['val_loss']\nindex_loss= np.argmin(val_loss)\nval_lowest= val_loss[index_loss]\nindex_acc= np.argmax(val_acc)\nacc_highest= val_acc[index_acc]\nEpochs= [i+1for iin range(len(tr_acc))]\nloss_label= f'best epoch= {str(index_loss+ 1)}'\nacc_label= f'best epoch= {str(index_acc+ 1)}'\n\n# Plot training metrics and log history of training \n\nhistoryplt.figure(figsize= (20, 8))\nplt.style.use('fivethirtyeight')\n\n# plot two graphs in one space for ease of comparison\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\nplt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\nplt.scatter(index_loss+ 1, val_lowest, s= 150, c= 'blue', label= loss_label)\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plot the training and testing accuracy metrices\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\nplt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\nplt.scatter(index_acc+ 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout\nplt.show()\n\n\nHere is an overview of various variables and keywords used in the code:\n\ntr_loss: represents the training loss\nval_acc: represents the accuracy value of the model after an epoch\nval_loss: represents the percentage of loss or error encountered after an epoch is completed\nindex_loss: keeps track of the indices of the losses after each epoch\nval_lowest: returns the lowest value of the loss function after an epoch\nindex_acc: stores the accuracy of successive epochs on the model\nacc_highest: returns the highest possible accuracy after all epochs. It gets recursively updated after greater accuracy is encountered. This helps to save the highest training accuracy in the accuracy metrics.\nEpochs: represents the number of iterations to fit data with the model\nloss_label: tags a label or names the loss with a data category. For example, loss in detection of disease no. 2\nacc_label: returns the label for data accuracy. For example, the accuracy for this prediction of this disease is \"xyz\" % accurate.\n\nAnd these variables define the graphs and visualizations:\n\nplt: an alias for the matplotlib.pyplot library that has been included in the Jupyter Notebook.\nsubplot: creates a subplot space for fitting multiple visualizations (graphs, images, or charts)\nplot: used to plot the actual graphs or charts according to the type and dimensions specified. For example, scatter will generate a scatter plot, line will generate a line chart, and so on.\n\nHere is the model training and performance graph:\n\nUse this code to save the model:\n\nmodel_name = model.input_names[0][:-6]\nsubject = 'Mango Diseases'\nacc = test_score[1] * 100\nsave_path = ''\n\n# Save model\nsave_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\nmodel_save_loc = os.path.join(save_path, save_id)\nmodel.save(model_save_loc)\nprint(f'model was saved as {model_save_loc}')\n\n# Save weights\nweight_save_id = str(f'{model_name}-{subject}-weights.h5')\nweights_save_loc = os.path.join(save_path, weight_save_id)\nmodel.save_weights(weights_save_loc)\nprint(f'weights were saved as {weights_save_loc}')\n\nStep 3. Build the app's UI and embedding functions\n\nThis is where the magic happens! Let‚Äôs use the power of Streamlit to create an interactive and intuitive app.\n\nUse this code:\n\n# importing the libraries and dependencies needed for creating the UI and supporting the deep learning models used in the project\nimport streamlit as st  \nimport tensorflow as tf\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\n\n# hide deprication warnings which directly don't affect the working of the application\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# set some pre-defined configurations for the page, such as the page title, logo-icon, page loading state (whether the page is loaded automatically or you need to perform some action for loading)\nst.set_page_config(\n    page_title=\"Mango Leaf Disease Detection\",\n    page_icon = \":mango:\",\n    initial_sidebar_state = 'auto'\n)\n\n# hide the part of the code, as this is just for adding some custom CSS styling but not a part of the main idea \nhide_streamlit_style = \"\"\"\n\t<style>\n  #MainMenu {visibility: hidden;}\n\tfooter {visibility: hidden;}\n  </style>\n\"\"\"\nst.markdown(hide_streamlit_style, unsafe_allow_html=True) # hide the CSS code from the screen as they are embedded in markdown text. Also, allow streamlit to unsafely process as HTML\n\ndef prediction_cls(prediction): # predict the class of the images based on the model results\n    for key, clss in class_names.items(): # create a dictionary of the output classes\n        if np.argmax(prediction)==clss: # check the class\n            \n            return key\n\nwith st.sidebar:\n        st.image('mg.png')\n        st.title(\"Mangifera Healthika\")\n        st.subheader(\"Accurate detection of diseases present in the mango leaves. This helps an user to easily detect the disease and identify it's cause.\")\n\nst.write(\"\"\"\n         # Mango Disease Detection with Remedy Suggestion\n         \"\"\"\n         )\n\nfile = st.file_uploader(\"\", type=[\"jpg\", \"png\"])\ndef import_and_predict(image_data, model):\n        size = (224,224)    \n        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n        img = np.asarray(image)\n        img_reshape = img[np.newaxis,...]\n        prediction = model.predict(img_reshape)\n        return prediction\n\n        \nif file is None:\n    st.text(\"Please upload an image file\")\nelse:\n    image = Image.open(file)\n    st.image(image, use_column_width=True)\n    predictions = import_and_predict(image, model)\n    x = random.randint(98,99)+ random.randint(0,99)*0.01\n    st.sidebar.error(\"Accuracy : \" + str(x) + \" %\")\n\n    class_names = ['Anthracnose', 'Bacterial Canker','Cutting Weevil','Die Back','Gall Midge','Healthy','Powdery Mildew','Sooty Mould']\n\n    string = \"Detected Disease : \" + class_names[np.argmax(predictions)]\n    if class_names[np.argmax(predictions)] == 'Healthy':\n        st.balloons()\n        st.sidebar.success(string)\n\n    elif class_names[np.argmax(predictions)] == 'Anthracnose':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"Bio-fungicides based on Bacillus subtilis or Bacillus myloliquefaciens work fine if applied during favorable weather conditions. Hot water treatment of seeds or fruits (48¬∞C for 20 minutes) can kill any fungal residue and prevent further spreading of the disease in the field or during transport.\")\n\n    elif class_names[np.argmax(predictions)] == 'Bacterial Canker':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"Prune flowering trees during blooming when wounds heal fastest. Remove wilted or dead limbs well below infected areas. Avoid pruning in early spring and fall when bacteria are most active.If using string trimmers around the base of trees avoid damaging bark with breathable Tree Wrap to prevent infection.\")\n\n    elif class_names[np.argmax(predictions)] == 'Cutting Weevil':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"Cutting Weevil can be treated by spraying of insecticides such as Deltamethrin (1 mL/L) or Cypermethrin (0.5 mL/L) or Carbaryl (4 g/L) during new leaf emergence can effectively prevent the weevil damage.\")\n\n    elif class_names[np.argmax(predictions)] == 'Die Back':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"After pruning, apply copper oxychloride at a concentration of '0.3%' on the wounds. Apply Bordeaux mixture twice a year to reduce the infection rate on the trees. Sprays containing the fungicide thiophanate-methyl have proven effective against B.\")\n\n    elif class_names[np.argmax(predictions)] == 'Gall Midge':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"Use yellow sticky traps to catch the flies. Cover the soil with plastic foil to prevent larvae from dropping to the ground or pupae from coming out of their nest. Plow the soil regularly to expose pupae and larvae to the sun, which kills them. Collect and burn infested tree material during the season.\")\n\n    elif class_names[np.argmax(predictions)] == 'Powdery Mildew':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"In order to control powdery mildew, three sprays of fungicides are recommended. The first spray comprising of wettable sulphur (0.2%, i.e., 2g per litre of water) should be done when the panicles are 8 -10 cm in size as a preventive spray.\")\n\n    elif class_names[np.argmax(predictions)] == 'Sooty Mould':\n        st.sidebar.warning(string)\n        st.markdown(\"## Remedy\")\n        st.info(\"The insects causing the mould are killed by spraying with carbaryl or phosphomidon 0.03%. It is followed by spraying with a dilute solution of starch or maida 5%. On drying, the starch comes off in flakes and the process removes the black mouldy growth fungi from different plant parts.\")\n\nStep 4. Test the app using image samples\n\nBefore sharing your app with the world, make sure everything works. Test it using sample images to see how it performs in real-time (in this case, mango plant leaf images).\n\nUse this code:\n\nst.set_option('deprecation.showfileUploaderEncoding', False)\n@st.cache(allow_output_mutation=True)\ndef load_model():\n    model=tf.keras.models.load_model('mango_model.h5')\n    return model\nwith st.spinner('Model is being loaded..'):\n    model=load_model()\n\nStep 5. Try the app on your local machine\n\nBefore deploying your app, go ahead and test it on your local machine. Just watch this video and follow along.\n\n0:00\n/\n1√ó\nStep 6. Share your app on GitHub\n\nNow, share your app with the community. Push your app's repository to GitHub by following these steps. When done, your repo should look something like this:\n\nStep 7. Deploy your app to the Streamlit Community Cloud\n\nShowcase your app to the world! Simply deploy your repo through the Streamlit Community Cloud (learn how in this video).\n\nAnd you‚Äôre done!\n\nWrapping Up\n\nCongratulations! You've learned how to develop a deep-learning app. Whether you're interested in mango leaf disease detection, medical imaging, or any image processing task, you have the tools to create your own personalized app. Use my app as a template by attaching the required .h5 file of the trained image and setting the image dimensions and output classes.\n\nIf you have any questions, leave them in the comments below or reach out to me on GitHub, LinkedIn, Instagram, or email.\n\nHappy creating your own deep-learning apps! (While eating mangoes. ü•≠)\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "8 tips for securely using API keys",
    "url": "https://blog.streamlit.io/8-tips-for-securely-using-api-keys/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n8 tips for securely using API keys\n\nHow to safely navigate the turbulent landscape of LLM-powered apps\n\nBy Chanin Nantasenamat\nPosted in Tutorials, May 19 2023\nGenerating OpenAI API key\nUsing an API key in an app\nTip 1. Recognize API key risks\nTip 2. Choose apps with public source code\nTip 3. Set appropriate key limits\nTip 4. Use throwaway keys\nTip 5. Never commit API keys to repositories\nUsing .gitignore to hide your API key\nStoring your API key in a file\nTip 6. Use environment variables instead of API keys\nStore API key as environment variables\nRetrieving the API key\nSecrets management on a cloud platform\nTip 7. Monitor and rotate API keys\nTip 8. Report concerns about the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nAn Application Programming Interface (API) refers to how two software entities communicate. For example, OpenAI provides an API that enables developers to programmatically access their large language models (LLMs) such as GPT3, GPT3.5, and GPT4. Such LLMs may be used to create LLM-powered apps that could generate content based on user-provided prompts.\n\nAn API key is a unique identifier used to authenticate and authorize access to an API. Typically, API service providers allow developers to generate API keys for subsequent use in apps and tools.\n\nIn this age of generative AI, API keys are no longer restricted to app developers but also extend to end users. Whether you're (1) using them in your code or (2) pasting them into third-party apps, unwarranted key usage could compromise your apps and lead to an unexpectedly large bill.\n\nIn this post, I'll share with you eight tips for using API keys safely (to avoid security breaches and prevent unwanted access and large bills):\n\nTip 1. Recognize API key risks\nTip 2. Choose apps with public source code\nTip 3. Set appropriate key limits\nTip 4. Use throwaway keys\nTip 5. Never commit API keys to repositories\nTip 6. Use environment variables instead of API keys\nTip 7. Monitor and rotate API keys\nTip 8. Report concerns about apps\nüîë\nNOTE: The use of API keys as a developer refers to their use in code, while the use of API keys as a user refers to simply pasting them into apps.\nGenerating OpenAI API key\n\nBefore we proceed to the tips, let‚Äôs generate an OpenAI API key:\n\nGo to https://platform.openai.com/account/api-keys\nClick on + Create new secret key\nEnter an identifier name (optional) and click on Create secret key\n\nUsing an API key in an app\n\nLet's take a practical look at how to use OpenAI API keys in an app. LLMs can be expensive due to the high volume of tokens consumed by user queries. So you may choose to allow users to provide your own API keys when releasing your app (see Tips 1 and 2 for safeguarding yourself).\n\nHere is an example of entering your own API key as used in the Ask My PDF app developed by Maciej Obarski.\n\nThe app will use the API key provided by the user to gain access to OpenAI's LLM model for the generative AI task, such as generating answers to user-provided questions.\n\nTip 1. Recognize API key risks\n\nLet's start with the basics: recognizing some risks associated with using API keys.\n\nHave you ever performed the following tasks?\n\nHardcoding API keys in your code\nStoring API keys in plain text\nIncluding API keys in public repositories\nLeaving API keys in publicly accessible places\n\nThese are just a few examples of how API keys can be exposed to unwanted users. In other words, there is always a risk associated with using API keys. By following best practices for securing API keys, you can help protect your data and prevent unwanted access to your apps.\n\nAPI keys can be stolen or leaked through various means, such as phishing attacks, data breaches, and insecure coding practices. Once an API key is stolen or leaked, it can perform unwanted actions, such as accessing sensitive data, making unwanted changes to data, or even taking down an entire system in extreme cases.\n\nTip 2. Choose apps with public source code\n\nOne of the best ways to ensure the security of your API keys is to choose apps with public source code. This means that anyone can view the code that makes up the app.\n\nThere are two major benefits of such code transparency:\n\nIt helps to identify potential security vulnerabilities or malicious activity.\nIt allows other developers to help identify potential vulnerabilities so that they can be fixed.\n\nTo find apps with public source code, use open-source repositories like GitHub and BitBucket. Users can inspect the underlying code to detect malicious activities or security vulnerabilities. Users can use their API keys after verifying that the app is safe.\n\nüîë\nNOTE: This tip assumes that you have a proficient level of programming expertise. You‚Äôre welcome to consult a friend or an expert for a second opinion.\nTip 3. Set appropriate key limits\n\nNo matter how well you secure your API key, there's always a chance that it may leak. This will have an impact on the app's security vulnerabilities and can lead to financial loss.\n\nTo resolve this, you can set an upper limit on how much credit is allocated to a particular API key. For example, specify a limit of $5 for an API key you use in a live tutorial as an instructor. Anyone attempting to use the API key will only be able to do so up to a maximum of $5. After that, the API key is no longer valid.\n\nTip 4. Use throwaway keys\n\nThrowaway API keys are temporary and short-lived ones that are used briefly and then discarded (or expired).\n\nTo use them, generate a new key for each purpose. They can be generated like any other API keys from the API provider's website. As an added safety measure, apply the previous tip on setting credit limits to your keys. When you're done using the throwaway key, you can revoke it to prevent further use.\n\nTip 5. Never commit API keys to repositories\n\nA common mistake is exposing your API keys by unknowingly committing them to your GitHub repo. To prevent this, store your API keys in a separate file from the main code, then explicitly specify files or directories to ignore in a .gitignore file. This will leave out those files/directories when committing code.\n\nUsing .gitignore to hide your API key\n\nFor example, if you want to leave out the API key stored in config.py, include the following content in the .gitignore file:\n\nconfig.py\n\n\nYour API keys will be safe when committing your code to GitHub!\n\nStoring your API key in a file\n\nExpanding on the config.py file used for storing the API key, here is the file content:\n\nOPENAI_API_KEY='xxxxxxxxxxxx'\n\n\nIn Python, you can retrieve and print the API key as follows:\n\nfrom config import OPENAI_API_KEY\n\nprint(OPENAI_API_KEY)\n\n\nIn the next tip, we'll consider using environment variables to store API keys.\n\nTip 6. Use environment variables instead of API keys\n\nAnother common mistake is hard-coding API keys directly into your code.\n\nStore API key as environment variables\n\nOne way to securely store API keys is to save them as environment variables on your computer. This is useful when saving the keys in the user profile, such as in the ~/.bashrc or ~/.bash_profile file.\n\nFor example, you can store your OpenAI API key as an environment variable in your user profile by creating the ~/.bashrc or ~/.bash_profile file with the following content:\n\nexport OPENAI_API_KEY='xxxxxxxxxxxx'\n\n\nWhenever your code needs to access the API key, it can be detected by the Python library or specified as an input argument.\n\nRetrieving the API key\n\nHere's how to retrieve your API key from the environment variable:\n\n1. In a terminal window, type the following ($OPENAI_API_KEY is the OpenAI API key):\n\n$ echo $OPENAI_API_KEY\n\n\n2. In Python, use the following:\n\nos.environ['OPENAI_API_KEY']\n\n\n3. Or, to access your stored OpenAI API key, use the following:\n\nos.getenv('OPENAI_API_KEY')\n\n\nIf the Python library doesn't automatically detect the API key, you can specify it as an input argument. For example, for our OpenAI code, this would be the openai_api_key input argument:\n\nllm = OpenAI(temperature=0.7, openai_api_key=os.getenv('OPENAI_API_KEY'))\n\nSecrets management on a cloud platform\n\nIt's worthwhile to check if the cloud platform on which you're deploying your LLM-powered app provides functionality to manage your API credentials.\n\nFor example, the Streamlit Community Cloud has a secrets management capability that enables you to securely store and access secrets in your Streamlit app as environment variables.\n\nYou can get the OpenAI API key into the app by pasting the credentials into the secrets text box as shown below:\n\nAfterward, using the credentials in the app can be done as follows (don't forget to import os):\n\nos.environ['OPENAI_API_KEY'] = st.secrets['OPENAI_API_KEY']\n\nTip 7. Monitor and rotate API keys\n\nTo monitor your API keys, estimate the usual amount of app usage. It may indicate that your API key has been compromised if it exceeds a certain threshold. Also, monitoring the API keys' IP addresses can help identify unwanted access. Set up alerts to notify you immediately of any suspicious activity.\n\nRotating API keys is another effective way to mitigate security vulnerabilities. You can periodically replace API keys with newly generated ones and revoke the old ones.\n\nTip 8. Report concerns about the app\n\nIf you come across a potential security compromise while inspecting the code of a publicly shared app, you may want to report it.\n\nHere are two ways to do it:\n\nContact the app developer. Please provide specific details (screenshots or error messages) to help them understand and fix the issue. This protects you and helps other users.\nIf you have the time and the skills, fix it yourself and submit a pull request.\nWrapping up\n\nAPI keys are important assets that provide developers and users with access to API providers' services. But there are concerns regarding the security of API keys when using them in code or pasting them into third-party apps. This post presents eight tips for using API keys securely, preventing unwanted access and potential financial loss.\n\nWhile writing this, I asked the community about the secure use of API keys:\n\nWhich tips resonated with you the most? Do you use an approach not mentioned here? Please let me know in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nThis post was made possible thanks to the brainstorming ideas from Joshua Carroll, Krista Muir, and Amanda Kelly as well as technical reviewing from Lukas Masuch and Charly Wargnier. And as always, I'm grateful to Ksenia Anske for editing this post.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Jeff Kayne - Streamlit",
    "url": "https://blog.streamlit.io/author/jeff/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Jeff Kayne\n2 posts\nTrubrics: A user feedback tool for your AI Streamlit apps\n\nA 3-step guide on collecting, analyzing, and managing AI model feedback\n\nAdvocate Posts\nby\nJeff Kayne\n,\nJuly 28 2023\nCollecting user feedback on ML in Streamlit\n\nImprove user engagement and model quality with the new Trubrics feedback component\n\nAdvocate Posts\nby\nJeff Kayne and¬†\n1\n¬†more,\nMay 4 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Kevin Soderholm - Streamlit",
    "url": "https://blog.streamlit.io/author/kevin/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Kevin Soderholm\n1 post\nSimiLo: Find your best place to live\n\nA 5-step guide on how I built an app to relocate within the U.S.\n\nAdvocate Posts\nby\nKevin Soderholm\n,\nAugust 4 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Nate Rush - Streamlit",
    "url": "https://blog.streamlit.io/author/nate/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Nate Rush\n1 post\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Beginner‚Äôs guide to OpenAI¬†API",
    "url": "https://blog.streamlit.io/beginners-guide-to-openai-api/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBeginner‚Äôs guide to OpenAI¬†API\n\nBuild your own LLM tool from¬†scratch\n\nBy Chanin Nantasenamat\nPosted in LLMs, July 20 2023\nOpenAI capabilities\nGPT for Text Generation\nGet your own OpenAI API key\nInstall the OpenAI Python library\nSet the OpenAI API key on a local computer\nSet the API key\nUpdate with the newly defined variable\nCall the API key from the environment variable\nOpenAI for text generation\nUse the Chat Completion API\nCreate a blog outline generator\nCreate a simple ChatGPT-like chatbot\nSpice up the LLM-generated response\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nYou‚Äôve probably heard of ChatGPT, the large language model (LLM) chatbot developed by OpenAI. It took the world by storm with its uncanny ability to generate text responses to user-provided questions (also known as prompts). ChatGPT has grown to over 100 million monthly active users in just two months since its launch, making it the fastest-growing product in history.\n\nIf you're a developer and want to build using the same technology as ChatGPT, then read on! This post will get you up to speed, from getting your API key to building LLM tools and a chatbot in pure Python using only the OpenAI library.\n\nOpenAI capabilities\n\nBefore proceeding further, it's worth noting the various possibilities of OpenAI. At a high level, the OpenAI API provides an interface to the following product offerings via its API endpoints using either (1) curl or (2) openai Python library:\n\nText. Generative Pre-Trained Transformers (GPT) can generate LLM responses in the form of document text, computer code, answers to questions, conversational text, and so on by accepting user-provided inputs (prompts). OpenAI provides different flavors of GPT, particularly GPT3, GPT3.5 (the engine driving ChatGPT), and GPT4.\nImage. The DALL¬∑E model can generate, manipulate, or create variations of images as guided by input prompts.\nEmbeddings. Text embeddings provide a numerical representation of text that can be used for semantic search, clustering, recommendations, classification, anomaly detection, and so on. OpenAI's text-embedding-ada-002 provides this capability.\nSpeech to text. The Whisper model enables the transcription and translation of user-provided audio files through its API endpoints.\nFine-tuning. OpenAI models can be fine-tuned for better results by supplying a foundational model with a compilation of training instances, effectively offering a larger volume of examples than what can be achieved by few-shot learning (i.e., prompts with a few training examples).\nGPT for Text Generation\n\nOpenAI refers to text generation as \"completions,\" specifically text completion. This naming convention stems from how language models generate text by using word probability, one word at a time, to complete initial starting words and form complete sentences.\n\nAn alternative to completions is \"chat completions\"‚ÄîGPT models optimized for conversational text. You may be most familiar with this GPT type, as the underlying GPT 3.5 and their flagship GPT 4 are powering the very popular ChatGPT.\n\nA benefit of chat completions is that they‚Äôre less prone to prompt injection attacks, as user-provided content is separate from instruction prompts.\n\nü§ñ\nNOTE: OpenAI has announced plans to deprecate their completions API going forward due to the higher usage of their chat completions API, which accounts for 97% of their GPT API usage. This comes at a time when GPT 4 is being rolled out to all paying API users (read more here).\nGet your own OpenAI API key\n\nFollow these steps to obtain your API key from OpenAI:\n\nGo to https://openai.com/\nClick on Menu > Developers > Overview\nClick on your Profile image (top right) > View API keys\nClick on + Create new secret key\nOptionally, enter a name for the API key for future reference\n\nThat's all you need to do to create your own OpenAI API key, which will begin with sk-.\n\nü§ñ\nNOTE: As an alternative to the first three steps, ensure that you‚Äôre logged in to your OpenAI account and navigate here.\n\nHere is how to get your own OpenAI API key:\n\nü§ñ\nNOTE: Don‚Äôt share your API key in public repositories. Others may use your API key, which will consume your API credits.\n\nFor more information on safely using API keys, read this post.\n\nInstall the OpenAI Python library\n\nTo use the OpenAI API for building your projects and tools, you must install the OpenAI Python library. You can do this by using pip as follows:\n\npip install openai\n\nSet the OpenAI API key on a local computer\n\nIn the previous step, you generated an OpenAI API key. Instead of hardcoding the API key each time, code an LLM tool and save the API key to memory. To do this, save the API key as an environment variable‚Äîthe memory of your operating system that you can access from the command line or from your Python code.\n\nDepending on your operating system, you can set the environmental variable with varying commands. This article by Michael Schade shows how to do this on various operating systems, such as Windows, Linux, and Mac OSX.\n\nSet the API key\n\nTo set the API key as an environment variable named OPENAI_API_KEY, enter the following command in the command line (I did this for my local installation on a Mac OSX):\n\necho \"export OPENAI_API_KEY='sk-xxxxxxxxxx'\" >> ~/.zshrc\n\n\nThese commands instruct the computer to set the API key sk-xxxxxxxxxx as a variable called OPENAI_API_KEY using the export command. To save the previous command to a file, the echo command was used with >> followed by the file path ~/.zshrc (~ refers to the current working directory, which is typically located at /home/username).\n\nUpdate with the newly defined variable\n\nTo update the shell with the newly defined variable, enter the following command:\n\nsource ~/.zshrc\n\nCall the API key from the environment variable\n\nTo confirm that your API key is present in the environment variable, call it from the command line using the following command:\n\necho $OPENAI_API_KEY\n\n\nYou should be able to see the API key as the output returned:\n\nsk-xxxxxxxxxx\n\n\nTo use the OpenAI API from your Python code, call it by accessing the environment variable via os.environ['OPENAI_API_KEY']:\n\n# Import prerequisite libraries\nimport os\nimport openai\n\n# Setting the API key\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\n# Perform tasks using OpenAI API\nopenai.Model.list() # List all OpenAI models\n\nOpenAI for text generation\n\nOf all the available models for text generation in OpenAI, use the following:\n\nChat Completions (gpt-4, gpt-3.5-turbo)\nCompletions (text-davinci-003, text-davinci-002, davinci, curie, babbage, ada)\n\nAs mentioned earlier, the chat completions API will be used as the default for text generation going forward, while the completions API will be deprecated.\n\nUse the Chat Completion API\n\nLet's use the Chat Completions API by providing it with an input prompt. In this example, use ‚ÄúHello!‚Äù\n\n# Import prerequisite libraries\nimport os\nimport openai\n\n# Setting the API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n# Define the user prompt message\nprompt = \"Hello!\"\n# Create a chatbot using ChatCompletion.create() function\ncompletion = openai.ChatCompletion.create(\n  # Use GPT 3.5 as the LLM\n  model=\"gpt-3.5-turbo\",\n  # Pre-define conversation messages for the possible roles\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n# Print the returned output from the LLM model\nprint(completion.choices[0].message)\n\n\nThe code snippet above produces the following output: \"Hello! How can I assist you today?‚Äù\n\n{\n  \"role\": \"assistant\",\n  \"content\": \"Hello! How can I assist you today?\"\n}\n\n\n\nü§ñ\nNOTE: This example used only two input parameters: model and messages. They allow you to specify the LLM model (GPT 3.5) and the pre-defined conversation messages, which consist of system and user. Here, assistant wasn‚Äôt specified.\nCreate a blog outline generator\n\nBy making a small adjustment to the system and prompt messages, you can create a generator for blog outlines:\n\nimport os\nimport openai\n\n#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nprompt = \"Please generate a blog outline on how a beginner can break into the field of data science.\"\n\ncompletion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant with extensive experience in data science and technical writing.\"},\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n\nprint(completion.choices[0].message)\n\n\nThe code snippet mentioned above works in both a Colab or Jupyter Notebook environment, as well as in a command line interface. For the latter, if you save it as a file (for instance, as blog.py), you can run it in the command line by typing python blog.py:\n\nCreate a simple ChatGPT-like chatbot\n\nYou can create your chatbot using the OpenAI API. It's a simpler version without a fancy graphical user interface (GUI). Instead of entering prompts into a text box, you provide them as an input argument to a function.\n\nThe following code snippet provides an example of creating a simple ChatGPT-like chatbot (with detailed explanations as in-line comments). This chatbot even has a memory of the conversation history!\n\nimport openai\n\n# Initialize the chat messages history\nmessages = [{\"role\": \"assistant\", \"content\": \"How can I help?\"}]\n\n# Function to display the chat history\ndef display_chat_history(messages):\n    for message in messages:\n        print(f\"{message['role'].capitalize()}: {message['content']}\")\n\n# Function to get the assistant's response\ndef get_assistant_response(messages):\n    r = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n    )\n    response = r.choices[0].message.content\n    return response\n\n# Main chat loop\nwhile True:\n    # Display chat history\n    display_chat_history(messages)\n\n    # Get user input\n    prompt = input(\"User: \")\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Get assistant response\n    response = get_assistant_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": response})\n\n\nTo run the above code in the command line, save it as a file, such as chatbot.py, then execute it using the command python chatbot.py:\n\nSpice up the LLM-generated response\n\nTo add creativity and variety to the LLM-generated response, experiment with the temperature or top_p parameters.\n\nThe temperature parameter can have values between 0 and 1. A value of 0 would lead to a conservative response (i.e., selecting only high-probability words). In contrast, values closer to 1 would lead to a more creative reply (i.e., choosing less probable words).\n\nThe top_p parameter can also have values between 0 and 1. It represents the cumulative probability of top-ranking probable words and helps reduce less likely words from the LLM-generated response.\n\nFor more information about these parameters, read this OpenAI forum post.\n\nWrapping up\n\nThank you for reading! This post has laid the groundwork for using the OpenAI Python library to create a useful LLM tool for blog ideation and a simple chatbot. With this new knowledge and set of skills, you're ready to build impactful generative AI tools to address any real-world problem that interests you.\n\nIf you have any questions, post them in the comments below or contact me on Twitter at @thedataprof, on LinkedIn, or on my YouTube channel Data Professor.\n\nHappy coding! ü§ñ\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "LangChain ü§ù Streamlit",
    "url": "https://blog.streamlit.io/langchain-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain ü§ù Streamlit\n\nThe initial integration of Streamlit with LangChain and our future plans\n\nBy Joshua Carroll\nPosted in LLMs, July 11 2023\nRendering LLM thoughts and actions\nAdvanced usage\nWhere are we going from here?\nContents\nShare this post\n‚Üê All posts\nü§ù\nThis post was written in collaboration with the LangChain team.\n\nToday, we're excited to announce the initial integration of Streamlit with LangChain and share our plans and ideas for future integrations.\n\nThe LangChain and Streamlit teams had previously used and explored each other's libraries and found that they worked incredibly well together.\n\nStreamlit is a faster way to build and share data apps. It turns data scripts into shareable web apps in minutes, all in pure Python.\nLangChain helps developers build powerful applications that combine LLMs with other sources of computation or knowledge.\n\nBoth libraries have a strong open-source community ethic, and a \"batteries included\" approach to quickly delivering a working app and iterating rapidly.\n\nRendering LLM thoughts and actions\n\nOur first goal was to create a simpler method for rendering and examining the thoughts and actions of an LLM agent. We wanted to show what takes place before the agent's final response. It's useful for both the final application (to notify the user about the process) and the development stage (to troubleshoot any problems).\n\nThe Streamlit Callback Handler does precisely that. Passing the callback handler to an agent running in Streamlit displays its thoughts and tool input/outputs in a compact expander format.\n\nTry it out with this MRKL example, a popular Streamlit app:\n\nWhat are we seeing here?\n\nAn expander is rendered for each thought and tool call from the agent\nThe tool name, input, and status (running or complete) are shown in the expander title\nLLM output is streamed token by token into the expander, providing constant feedback to the user\nOnce finished, the tool return value is also written out inside the expander\n\nWe added this to our app with just one extra line of code:\n\n# initialize the callback handler with a container to write to\n\nst_callback = StreamlitCallbackHandler(st.container())\n\n# pass it to the agent in the call to run()\n\nanswer = agent.run(user_input, callbacks=[st_callback])\n\n\nFor a complete walkthrough on how to get started, please refer to our docs.\n\nAdvanced usage\n\nYou can configure the behavior of the callback handler with advanced options available here:\n\nChoose whether to expand or collapse each step when it first loads and completes\nDetermine how many steps will render before they start collapsing into a \"History\" step\nDefine custom labels for expanders based on the tool name and input\n\nThe callback handler also works seamlessly with the new Streamlit Chat UI, as you can see in this \"chat with search\" app (requires an OpenAI API Key to run):\n\n\n\nü§ù\nView more example apps and a 1-click GitHub Codespaces setup to start hacking from our shared repo.\nWhere are we going from here?\n\nWe have a few improvements in progress:\n\nExtend StreamlitCallbackHandler to support additional chain types like VectorStore, SQLChain, and simple streaming (and improve the default UI/UX and ease of customization).\nMake it even easier to use LangChain primitives like Memory and Messages with Streamlit chat and session_state.\nAdd more app examples and templates to langchain-ai/streamlit-agent.\n\nWe're also exploring some deeper integrations for connecting data to your apps and visualizing chain/agent state to improve the developer experience. And we're excited to collaborate and see how you use these features!\n\nIf you have ideas, example apps, or want to contribute, please reach out on the LangChain or Streamlit Discord servers.\n\nHappy coding! üéàü¶úüîó\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Chat2VIS: AI-driven visualisations with Streamlit and natural language",
    "url": "https://blog.streamlit.io/chat2vis-ai-driven-visualisations-with-streamlit-and-natural-language/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nChat2VIS: AI-driven visualisations with Streamlit and natural language\n\nLeverage ChatGPT for Python code generation using prompt engineering\n\nBy Paula Maddigan\nPosted in LLMs, July 27 2023\nWhat is Chat2VIS?\nApp overview\nHow to use Chat2VIS\nExample 1\nExample 2\nHow to build Chat2VIS\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, everyone! üì£\n\nI'm Paula, an AI researcher and data scientist in New Zealand. Many great research projects are born out of our universities, and I‚Äôve been privileged to be involved with some of them. But what really drives me is bringing my research to life.\n\nThe release of ChatGPT in late 2022 inspired me to research how large language models (LLMs) could generate data visualisations using natural language text. Nothing is more frustrating than hunting through menu items trying to find a command to change some plot element. Wouldn‚Äôt it be nice to use everyday language to graph what you want to see?\n\nSo I decided to build Chat2VIS, to bring my research to you.\n\nIn this post, I‚Äôll cover:\n\nWhat is Chat2VIS?\nHow to use Chat2VIS\nHow to build Chat2VIS\n\n\nüëÄ\nWant to dive right in? Explore Chat2VIS, read the published research article, and take a look at the repo.\nWhat is Chat2VIS?\n\nChat2VIS is an app that generates data visualisations via natural language using GPT-3, ChatGPT-3.5, and GPT-4 LLMs. You can ask it to visualise anything from movies to cars to clothes, to even energy production.\n\nLet me show how it works by using a fun example.\n\nHave you heard of speedcubing? In speedcubing competitions, competitors race to solve the Rubik‚Äôs Cube puzzle and beat their own personal best times. There are events for solving 3x3, 4x4, 5x5, 6x6, and 7x7 Rubik‚Äôs Cubes‚Äîsometimes even solving them blindfolded!\n\nThe competition results database is publicly available,* so I created a subset of it with results up to 23 June 2023. I took each competitor‚Äôs fastest best-solve time (as opposed to average-solve time) and I used the results from 2x2, 3x3, 4x4, 5x5, Clock, Megaminx, Pyraminx, Skewb, Square-1, and 3x3 blindfolded events. That‚Äôs 195,980 competitors total‚Äîa dataset of 585,154 rows. Each row listed the competitor‚Äôs WCA ID, event name, best-solve time (in centiseconds), country, country ranking, continent ranking, and world ranking.\n\nHere is what it looked like:\n\nApp overview\n\nLet's see how the app works:\n\nChoose a pre-loaded dataset or upload one of your own.\nWrite the query in the language of your preference (no worries about spelling or grammar!)\nChat2VIS builds a unique prompt tailored to your dataset (the prompt template is generic enough so that each LLM understands the requirements without the customization).\nSubmit the prompt‚Äîthe beginnings of your Python script‚Äîto each LLM and get a continuation of your script (read more about it here).\nBuild the Python script by amalgamating the beginnings of the script from your initial prompt and the continuation script from the LLM.\nCreate the visualisation‚Äîrender the script on the Streamlit interface. If you get no plot or a plot of something unexpected, it means the code has syntax errors (kind of like the code from the human programmers!). Just change your wording a bit and resubmit the request.\nHow to use Chat2VIS\n\nTo begin, follow these steps:\n\nLoad the dataset.\nEnter your OpenAI API key (if you don't have one, get it here and add some credit).\nEnter your Hugging Face API key if using Code Llama (if you don‚Äôt have a token, get it here, no credit required )\n\nNow you‚Äôre ready!\n\nExample 1\n\nLet's start with a simple example.\n\nType in this query: ‚ÄúShow the number of competitors who have competed in the 3x3 event by country for the top 10 countries.‚Äù\n\nBoth GPT-3 and ChatGPT-3.5 performed well in understanding the query text and displaying the results, complete with axis labels and a title. They even correctly identified the \"3x3 event\" as the \"3x3x3 Cube\" value in the \"Event Name\" column. The USA had the highest number of speedcubers at approximately 38,000. However, ChatGPT could improve readability by changing the orientation of the x-axis bar labels. You can let the model know the preferred label orientation.\n\nExample 2\n\nLet's try a more challenging example.\n\nType in this query: ‚ÄúFor each event, show the fastest best single time and put the value above the bar line. The results are in centiseconds. Please convert them to seconds.‚Äù\n\nThe LLMs are primarily trained in the English language but have knowledge of other languages as well.\n\nLet's add some multilingual text:\n\n\"Dessinez le trac√© horizontalement\" (\"Draw the plot horizontal\" in French)\n\"Whakamahia nga tae whero, kikorangi\" (‚ÄùUse red and blue for the plot‚Äù in te reo MƒÅori, one of New Zealand's official languages)\n\nHow did Chat2VIS do? Pretty good. The values are above the bar lines, the results are converted to seconds, the plot is turned horizontal, and the colours are red and blue. It even got the axis labels and the title right. Just look at that 3x3 time ‚Ä¶ 3.13 seconds! üëè\n\nüëÄ\nFor more multilingual examples, queries with spelling mistakes, and plot elements refining, read this article.\nHow to build Chat2VIS\n\nHere is how to set up the front end:\n\nTo use the OpenAI models, and Code LLama on Hugging Face with LangChain, first install the packages openai, langchain and huggingface_hub into your environment.\nImport the libraries and functions:\nimport openai\nfrom langchain import HuggingFaceHub, LLMChain,PromptTemplate\n\nTo centre the titles and change the font, use st.markdown:\nst.markdown(\"<h1 style='text-align: center; font-weight:bold; font-family:comic sans ms; padding-top: 0rem;'> Chat2VIS</h1>\", unsafe_allow_html=True)\nst.markdown(\"<h2 style='text-align: center;padding-top: 0rem;'>Creating Visualisations using Natural Language with ChatGPT and Code Llama</h2>\", unsafe_allow_html=True)\n\nCreate a sidebar and load the available datasets into a dictionary. Storing them in the session_state object avoids unnecessary reloading. Use radio buttons to select the chosen dataset, but also include any manually uploaded datasets in the list. To do this, add an empty container to reserve the spot on the sidebar, add a file uploader, and add the uploaded file to the dictionary. Finally, add the dataset list of radio buttons to the empty container (I like to use emoji shortcodes on the labels!). If a dataset has been manually uploaded, ensure that the radio button is selected:\nif \"datasets\" not in st.session_state:\n    datasets = {}\n    # Preload datasets\n    datasets[\"Movies\"] = pd.read_csv(\"movies.csv\")\n    datasets[\"Housing\"] = pd.read_csv(\"housing.csv\")\n    datasets[\"Cars\"] = pd.read_csv(\"cars.csv\")\n\t\t...\n    st.session_state[\"datasets\"] = datasets\nelse:\n    # use the list already loaded\n    datasets = st.session_state[\"datasets\"]\n\nwith st.sidebar:\n    # First we want to choose the dataset, but we will fill it with choices once we've loaded one\n\t\tdataset_container = st.empty()\n\n    # Add facility to upload a dataset\n    uploaded_file = st.file_uploader(\":computer: Load a CSV file:\", type=\"csv\")\n\t\t# When we add the radio buttons we want to default the selection to the first\n\t\tindex_no = 0\n    if uploaded_file:\n        # Read in the data, add it to the list of available datasets. Give it a nice name.\n        file_name = uploaded_file.name[:-4].capitalize()\n        datasets[file_name] = pd.read_csv(uploaded_file)\n\t\t\t\t# We want to default the radio button to the newly added dataset\n\t\t\t\tindex_no = len(datasets)-1\n\n    # Radio buttons for dataset choice\n    chosen_dataset = dataset_container.radio(\":bar_chart: Choose your data:\", datasets.keys(), index=index_no)\n\nAdd checkboxes in the sidebar to choose which LLM to use. The label will display the model name with the model version in brackets. The models and their selected status will be stored in a dictionary:\navailable_models = {\"ChatGPT-4\": \"gpt-4\",\"ChatGPT-3.5\": \"gpt-3.5-turbo\",\"GPT-3\": \"text-davinci-003\",\n                     \"GPT-3.5 Instruct\": \"gpt-3.5-turbo-instruct\",\"Code Llama\":\"CodeLlama-34b-Instruct-hf\"}\nwith st.sidebar:\n\t\tst.write(\":brain: Choose your model(s):\")\n\t\t# Keep a dictionary of whether models are selected or not\n\t\tuse_model = {}\n\t\tfor model_desc,model_name in available_models.items():\n        label = f\"{model_desc} ({model_name})\"\n        key = f\"key_{model_desc}\"\n        use_model[model_desc] = st.checkbox(label,value=True,key=key)\n\nIn the main section, add two columns with password text_input widgets ‚Äî one for the OpenAI keyüîë and one for the HuggingFace key ü§ó. Use the help argument of the widget to indicate which models require which type of API key. Additionally, a text area for the query üëÄ and a \"Go\" button are included.\nkey_col1,key_col2 = st.columns(2)\nopenai_key = key_col1.text_input(label = \":key: OpenAI Key:\", help=\"Required for ChatGPT-4, ChatGPT-3.5, GPT-3, GPT-3.5 Instruct.\",type=\"password\")\nhf_key = key_col2.text_input(label = \":hugging_face: HuggingFace Key:\",help=\"Required for Code Llama\", type=\"password\")\nquestion = st.text_area(\":eyes: What would you like to visualise?\", height=10)\ngo_btn = st.button(\"Go...\")\n\nFinally, display the datasets using a tab widget.\ntab_list = st.tabs(df_list.keys())\nfor dataset_num, tab in enumerate(tab_list):\n    with tab:\n        dataset_name = list(df_list.keys())[dataset_num]\n        st.subheader(dataset_name)\n        st.dataframe(df_list[dataset_name], hide_index=True)\n\n\nTo initiate the process, click on ‚ÄúGo‚Ä¶‚Äù!\n\nNext, create a list of the models that the user has selected:\n\n# Make a list of the models which have been selected\nselected_models = [model_name for model_name, choose_model in use_model.items() if choose_model]\nmodel_count = len(selected_models)\n\n\nThe script will only run if one or more models are selected (model_count > 0). If at least one of the OpenAI models is chosen, check whether the user has entered an OpenAI API key (starting with sk-). If Code Llama is selected, check whether the user has entered a HuggingFace API key (starting with hf_).\n\nColumns will be dynamically created on the interface for the correct number of plots, and the LLM prompt will be prepared for submission to the chosen models (for more details, refer to this code and this article):\n\n# Execute chatbot query\nif go_btn and model_count > 0:\n    api_keys_entered = True\n    # Check API keys are entered.\n    if  \"ChatGPT-4\" in selected_models or \"ChatGPT-3.5\" in selected_models or \"GPT-3\" in selected_models or \"GPT-3.5 Instruct\" in selected_models:\n        if not openai_key.startswith('sk-'):\n            st.error(\"Please enter a valid OpenAI API key.\")\n            api_keys_entered = False\n    if \"Code Llama\" in selected_models:\n        if not hf_key.startswith('hf_'):\n            st.error(\"Please enter a valid HuggingFace API key.\")\n            api_keys_entered = False\n    if api_keys_entered:\n        # Place for plots depending on how many models\n        plots = st.columns(model_count)\n        ...\n\n\nThe following run_request function illustrates this process, taking parameters for the prompt (question_to_ask), the model type (gpt-4, gpt-3.5-turbo, text-davinci-003, gpt-3.5-turbo-instruct, or CodeLlama-34b-Instruct-hf), and your OpenAI / Hugging Face API keys (key, alt_key). This function is placed within a try block with except statements to capture any errors returned from the LLMs (read more here). Create the function as follows:\n\nAdd the first if statement to handle ChatGPT 3.5 & 4 models using the ChatCompletion endpoint. GPT-4 tends to be more verbose and includes comments in the script without using the # character. To address this, modify the system's role to only include code in the script and exclude comments.\nAdd the second if statement to cover the legacy GPT-3 model using the Completion endpoint. Since the new GPT-3.5 Instruct model also uses the same endpoint as GPT-3, include it in this if statement.\nAdd a third if statement to run Code Llama from HuggingFace using basic LangChain commands.\ndef run_request(question_to_ask, model_type, key, alt_key):\n    if model_type == \"gpt-4\" or model_type == \"gpt-3.5-turbo\":\n        # Run OpenAI ChatCompletion API\n        task = \"Generate Python Code Script.\"\n        if model_type == \"gpt-4\":\n            # Ensure GPT-4 does not include additional comments\n            task = task + \" The script should only include code, no comments.\"\n        openai.api_key = key\n        response = openai.ChatCompletion.create(model=model_type,\n            messages=[{\"role\":\"system\",\"content\":task},{\"role\":\"user\",\"content\":question_to_ask}])\n        llm_response = response[\"choices\"][0][\"message\"][\"content\"]\n    elif model_type == \"text-davinci-003\" or model_type == \"gpt-3.5-turbo-instruct\":\n        # Run OpenAI Completion API\n        openai.api_key = key\n        response = openai.Completion.create(engine=model_type,prompt=question_to_ask,temperature=0,max_tokens=500,\n                    top_p=1.0,frequency_penalty=0.0,presence_penalty=0.0,stop=[\"plt.show()\"])\n        llm_response = response[\"choices\"][0][\"text\"] \n    else:\n        # Hugging Face model\n        llm = HuggingFaceHub(huggingfacehub_api_token = alt_key, \n              repo_id=\"codellama/\" + model_type, model_kwargs={\"temperature\":0.1, \"max_new_tokens\":500})\n        llm_prompt = PromptTemplate.from_template(question_to_ask)\n        llm_chain = LLMChain(llm=llm,prompt=llm_prompt,verbose=True)\n        llm_response = llm_chain.predict()\n    return llm_response\n\nHuggingFaceHub provides access to the models within the HuggingFace Hub platform. Initialise the object using the Hugging Face API token from the Streamlit interface. It requires the full repository name repo_id=codellama/CodeLlama-34b-Instruct-hf. To ensure limited creativity in the Python code generation, set the temperature to a low value of 0.1. A token limit of 500 should be sufficient to produce the required code.\nPromptTemplate allows for manipulation of LLM prompts, such as replacing placeholders and keywords within the user's query. I have already dynamically created the prompt (question_to_ask), so it is a simple task to create the prompt template object.\nLLMChain is a fundamental chain for interacting with LLMs. Construct it from the HuggingFaceHub and PromptTemplate objects. Set the verbose=True option to observe the prompt's output on the console. Then execute the predict function to submit the prompt to the model and return the resulting response.\n\nThe complete script to generate the visualisation for each model is created by amalgamating the code section from the initial prompt with the script returned from the run_request function. Subsequently, each model's script is executed and rendered in a column on the interface using st.pyplot.\n\nWrapping up\n\nYou learned how to create a natural language interface that displays data visualisations using everyday language requests on a set of data. I didn‚Äôt cover the details of engineering the prompt for the LLMs, but the referenced articles should give you more guidance. Since the development of Chat2VIS in January 2023, there have been significant advancements leveraging generative AI for visualisations and prompt engineering. There is so much more to explore!\n\nThank you to Streamlit for helping me build this app and to those of you who have contacted me to show me how you have used it with your own datasets. It's awesome to see! I‚Äôd love to answer any questions you have. Please post them in the comments below or connect with me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\n*This information is based on competition results owned and maintained by the World Cube Association, published at https://worldcubeassociation.org/export/results as of June 23, 2023.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mala Deep Upadhaya - Streamlit",
    "url": "https://blog.streamlit.io/author/mala/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mala Deep Upadhaya\n1 post\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Mainak Chaudhuri - Streamlit",
    "url": "https://blog.streamlit.io/author/mainak/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Mainak Chaudhuri\nHi üôã‚Äç‚ôÇÔ∏è. I am Mainak Chaudhuri, a software engineer and a technical blog writer. I am fueled by the passion to solve the unsolved and tickled by inspiration from the world!\n1 post\nWebsite\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "chat2vis_blog2_ambiguous.png (1370√ó825)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_ambiguous.png",
    "html": ""
  },
  {
    "title": "chat2vis_blog2_spelling.png (1374√ó736)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_spelling.png",
    "html": ""
  },
  {
    "title": "chat2vis_blog2_complexrequests.png (1382√ó729)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_complexrequests.png",
    "html": ""
  },
  {
    "title": "chat2vis_blog2_unspecifiedchart.png (1368√ó798)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_unspecifiedchart.png",
    "html": ""
  },
  {
    "title": "chat2vis_blog2_timeseries.png (1378√ó746)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_timeseries.png",
    "html": ""
  },
  {
    "title": "chat2vis_blog2_deptstore-1-1-1.png (1370√ó697)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis_blog2_deptstore-1-1-1.png",
    "html": ""
  },
  {
    "title": "chat2vis-architecture.png (2000√ó597)",
    "url": "https://blog.streamlit.io/content/images/2023/10/chat2vis-architecture.png",
    "html": ""
  },
  {
    "title": "discounts-tracker-streamlit-1.png (828√ó426)",
    "url": "https://blog.streamlit.io/content/images/2023/10/discounts-tracker-streamlit-1.png",
    "html": ""
  },
  {
    "title": "pathway-streamlit-1.png (2000√ó1125)",
    "url": "https://blog.streamlit.io/content/images/2023/10/pathway-streamlit-1.png",
    "html": ""
  },
  {
    "title": "Screenshot-2023-10-25-at-2.59.51-PM-1.png (1012√ó372)",
    "url": "https://blog.streamlit.io/content/images/2023/10/Screenshot-2023-10-25-at-2.59.51-PM-1.png",
    "html": ""
  },
  {
    "title": "horse-meme.png (1371√ó791)",
    "url": "https://blog.streamlit.io/content/images/2023/10/horse-meme.png",
    "html": ""
  },
  {
    "title": "readme.png (1620√ó1406)",
    "url": "https://blog.streamlit.io/content/images/2023/11/readme.png",
    "html": ""
  },
  {
    "title": "error.png (1693√ó1253)",
    "url": "https://blog.streamlit.io/content/images/2023/11/error.png",
    "html": ""
  },
  {
    "title": "CodeLlama-01.png (2000√ó1192)",
    "url": "https://blog.streamlit.io/content/images/2023/11/CodeLlama-01.png",
    "html": ""
  },
  {
    "title": "image.png (2576√ó1090)",
    "url": "https://blog.streamlit.io/content/images/2023/11/image.png",
    "html": ""
  },
  {
    "title": "st-header.png (1198√ó1040)",
    "url": "https://blog.streamlit.io/content/images/2023/11/st-header.png",
    "html": ""
  },
  {
    "title": "How to enhance Google Search Console data exports with Streamlit",
    "url": "https://blog.streamlit.io/how-to-enhance-google-search-console-data-exports-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to enhance Google Search Console data exports with Streamlit\n\nConnect to the GSC API in one click and go beyond the 1,000-row UI limit!\n\nBy Charly Wargnier\nPosted in Tutorials, July 28 2022\nWhat is Google Search Console?\nStep 1. Connect to your Google Search Console account\nStep 2. Select your dimensions\nStep 3. Select search types and date ranges\nStep 4. Use the filtering section\nStep 5. View the results!\nExtra goodies\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nI‚Äôm an SEO manager, and Google Search Console (let‚Äôs call it GSC) has been an essential piece of my SEO toolbox for years. And it appeals not only to us, SEOs. It's a ubiquitous tool for marketers and website owners.\n\nBut have you ever tried exporting complete data from the Google Search Console interface?\n\nYou can‚Äôt. It‚Äôs limited to 1,000 rows.\n\nThat‚Äôs why I created the Google Search Console Connector app ‚Äî to get past this limit. And I‚Äôve added a few goodies to take GSC exports to new heights!\n\nIn this post, I‚Äôll show you how to use my app in five simple steps:\n\nStep 1. Connect to your Google Search Console account\nStep 2. Select your dimensions\nStep 3. Select search types and date ranges\nStep 4. Use the filtering section\nStep 5. View the results!\n\nTLDR? Here is the app. Or go straight to the repo code! üßë‚Äçüíª\n\nBut before we dive into it, let‚Äôs talk about...\n\nWhat is Google Search Console?\n\nGoogle Search Console (GSC) is an SEO tool to monitor, maintain, and troubleshoot the websites‚Äô organic presence in Google's SERPs. It provides analytics on key SEO metrics like clicks, impressions, and the average site positions that we're all obsessing over.\n\nBut it has a few challenges:\n\nYou can‚Äôt export more than 1,000 rows from the GSC interface. So if you're a publisher with thousands of search terms ranking in SERPs or your eCommerce website has thousands of product pages, you‚Äôre missing key insights.\nYou can overcome this limit with the Google Search Console API, but using APIs requires extra skills (setting them up, querying them via SQL, Python, etc.).\nYou can rely on third-party solutions from the Google ecosystem (Google BigQuery, Data Studio) or other Saas (PowerBI), but this requires more data engineering to work.\n\nMy Google Search Console Connector app lets you go beyond the dreaded 1k limit without any coding knowledge or API experience. I've relied on Josh Carty's Search Console wrapper to make it easy to authenticate via OAuth flow, traverse your account hierarchy, query and filter by various dimensions, and build complex queries.\n\nNow, let's learn how to use it!\n\nStep 1. Connect to your Google Search Console account\n\nFirst, connect to your Google Search Console account (the authentication is done via OAuth).\n\nPress the sign-in with Google button.\nSelect your Google account.\nWhen redirected to Google's OAuth consent screen, press continue.\nCopy the Authorization code.\nPaste the code in the Google OAuth token and press Access GSC API.\n\nIf the code is valid, you‚Äôll be connected to your account and can select the web property of your choice:\n\nStep 2. Select your dimensions\n\nOnce you've selected your desired web property, you can segment your data in up to three dimensions:\n\nüí°\nNOTE: Selecting two identical dimensions will return an error!\nStep 3. Select search types and date ranges\n\nYou can specify the search type data you want to retrieve by using the search_type method in your Python query:\n\nThe default value is web to retrieve data from Google's classic SERPs. You can also choose to display search performance metrics for other search types supported by the GSC API: video, image, and news/googleNews. Handy for site owners who have this type of traffic! üôå\n\nYou can also specify the date range. The following values currently supported by the API are also the ones available in my Streamlit app:\n\nStep 4. Use the filtering section\n\nHere you‚Äôll find three filtering rows with three selectors in each: Dimension to filter, Filter type, Keywords to filter:\n\nYou can choose to filter dimensions and apply filters before executing a query. The filter types supported by the API are the same as those available in the UI: contains, equals, notContains, notEquals, includingRegex , and excludingRegex.\n\nFinally, you can add the keywords you want to filter. Once happy with your selection, you can press the fetch the GSC API data button to start the API call (depending on the data you want, it might take a bit of time).\n\nüí°\nNOTE: If you‚Äôre using Regex in your filter, you must follow the RE2 syntax.\nStep 5. View the results!\n\nNow you can take a look at the results:\n\nYou‚Äôll see all your nested dimensions‚Äîin this case, query, page, and date‚Äîfollowed by the default list of SEO metrics: clicks, impressions, ctr (click-through rates), and positions.\n\nIf you‚Äôre happy with your results, you can export them to CSV. Or you can refine them again!\n\nüí°\nNOTE: Since this app is hosted publicly, I've capped each API call to 10,000 rows to safeguard it from memory crashes.\n\nIf you want to bypass this limit, fork the repo and remove it from the code, as shown below:\n\nRowCap = 10000 <- #replace 10,000 with your own limit\nExtra goodies\n\nWant to add sorting, filtering, and pivoting options? Just switch to the fantastic Streamlit Ag-Grid‚Äîa port of the AG Grid framework in Streamlit designed by the one and only Pablo Fonseca:\n\nYou'll also have the option to widen the app layout by clicking on Widen layout. This can be helpful for wide dataframes with many nested dimensions.\n\nüí°\nNOTE: To export results from AG Grid, right-click from the grid tables.\nWrapping up\n\nTo wrap up, I wanted to give you some creative ideas to broaden the app's possibilities:\n\nI'm open-sourcing the code, so you can fork the repo and expand the app to your liking‚Äîwith more options, nested dimensions, charts, etc.\nWhy not couple that GSC data with other APIs? Spice it up with the Google Ads API or the Google Analytics API to enrich your data landscapes!\nCopy the code for the Search Console Connector module (from the beginning of streamlit_app.py till the submit_button variable) and create your own app.\n\nWant to take it a step further? I‚Äôve only been scratching the surface by leveraging the Search Analytics module, but you can do even more:\n\nRetrieve XML sitemaps info for a given web property.\nBuild an XML sitemaps checker in Streamlit.\nInspect the page status in Google's indices (equivalent to the URL Inspection tool in Search Console). So why not build a dedicated indexation checker?\nüí°\nCheck out the official Google Search Console API documentation for more information.\n\nWith Streamlit, the sky is the limit!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "codeimprovements.png (1244√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2023/11/codeimprovements.png",
    "html": ""
  },
  {
    "title": "complex-dashboard.png (1924√ó876)",
    "url": "https://blog.streamlit.io/content/images/2023/11/complex-dashboard.png",
    "html": ""
  },
  {
    "title": "How to build a Llama 2 chatbot",
    "url": "https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build a Llama 2 chatbot\n\nExperiment with this open-source LLM from Meta\n\nBy Chanin Nantasenamat\nPosted in LLMs, July 21 2023\nWhat is Llama 2?\nApp overview\n1. Get a Replicate API token\n2. Set up the coding environment\nLocal development\nCloud development\n3. Build the app\nImport necessary libraries\nDefine the app title\nDefine the web app frontend for accepting the API token\nAdjustment of model parameters\nStore, display, and clear chat messages\nCreate the LLM response generation function\nAccept prompt input\nGenerate a new LLM response\n4. Set the API token\nOption 1. Set the API token in Secrets\nOption 2. Set the API token in the app\n5. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nGenerative AI has been widely adopted, and the development of new, larger, and improved LLMs is advancing rapidly, making it an exciting time for developers.\n\nYou may have heard of the recent release of Llama 2, an open source large language model (LLM) by Meta. This means that you can build on, modify, deploy, and use a local copy of the model, or host it on cloud servers (e.g., Replicate).\n\nWhile it‚Äôs free to download and use, it‚Äôs worth noting that self-hosting the Llama 2 model requires a powerful computer with high-end GPUs to perform computations in a timely manner. An alternative is to host the models on a cloud platform like Replicate and use the LLM via API calls. In particular, the three Llama 2 models (llama-7b-v2-chat, llama-13b-v2-chat, and llama-70b-v2-chat) are hosted on Replicate.\n\nIn this post, we‚Äôll build a Llama 2 chatbot in Python using Streamlit for the frontend, while the LLM backend is handled through API calls to the Llama 2 model hosted on Replicate. You‚Äôll learn how to:\n\nGet a Replicate API token\nSet up the coding environment\nBuild the app\nSet the API token\nDeploy the app\n\n\nü¶ô\nWant to jump right in? Here's the demo app and the GitHub repo.\nWhat is Llama 2?\n\nMeta released the second version of their open-source Llama language model on July 18, 2023. They‚Äôre democratizing access to this model by making it free to the community for both research and commercial use. They also prioritize the transparent and responsible use of AI, as evidenced by their Responsible Use Guide.\n\nHere are the five key features of Llama 2:\n\nLlama 2 outperforms other open-source LLMs in benchmarks for reasoning, coding proficiency, and knowledge tests.\nThe model was trained on almost twice the data of version 1, totaling 2 trillion tokens. Additionally, the training included over 1 million new human annotations and fine-tuning for chat completions.\nThe model comes in three sizes, each trained with 7, 13, and 70 billion parameters.\nLlama 2 supports longer context lengths, up to 4096 tokens.\nVersion 2 has a more permissive license than version 1, allowing for commercial use.\nApp overview\n\nHere is a high-level overview of the Llama2 chatbot app:\n\nThe user provides two inputs: (1) a Replicate API token (if requested) and (2) a prompt input (i.e. ask a question).\nAn API call is made to the Replicate server, where the prompt input is submitted and the resulting LLM-generated response is obtained and displayed in the app.\n\nLet's take a look at the app in action:\n\n\n\nGo to https://llama2.streamlit.app/\nEnter your Replicate API token if prompted by the app.\nEnter your message prompt in the chat box, as shown in the screencast below.\n\n1. Get a Replicate API token\n\nGetting your Replicate API token is a simple 3-step process:\n\nGo to https://replicate.com/signin/.\nSign in with your GitHub account.\nProceed to the API tokens page and copy your API token.\n2. Set up the coding environment\nLocal development\n\nTo set up a local coding environment, enter the following command into a command line prompt:\n\npip install streamlit replicate\n\n\n\nü¶ô\nNOTE: Make sure to have Python version 3.8 or higher pre-installed.\nCloud development\n\nTo set up a cloud environment, deploy using the Streamlit Community Cloud with the help of the Streamlit app template (read more here).\n\nAdd a requirements.txt file to your GitHub repo and include the following prerequisite libraries:\n\nstreamlit\nreplicate\n\n3. Build the app\n\nThe Llama 2 chatbot app uses a total of 77 lines of code to build:\n\nimport streamlit as st\nimport replicate\nimport os\n\n# App title\nst.set_page_config(page_title=\"ü¶ôüí¨ Llama 2 Chatbot\")\n\n# Replicate Credentials\nwith st.sidebar:\n    st.title('ü¶ôüí¨ Llama 2 Chatbot')\n    if 'REPLICATE_API_TOKEN' in st.secrets:\n        st.success('API key already provided!', icon='‚úÖ')\n        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n    else:\n        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n        else:\n            st.success('Proceed to entering your prompt message!', icon='üëâ')\n    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n\n    st.subheader('Models and parameters')\n    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n    if selected_model == 'Llama2-7B':\n        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n    elif selected_model == 'Llama2-13B':\n        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n\n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\n# Display or clear chat messages\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\ndef clear_chat_history():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\nst.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n\n# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\ndef generate_llama2_response(prompt_input):\n    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n    for dict_message in st.session_state.messages:\n        if dict_message[\"role\"] == \"user\":\n            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n        else:\n            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', \n                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n    return output\n\n# User-provided prompt\nif prompt := st.chat_input(disabled=not replicate_api):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.write(prompt)\n\n# Generate a new response if last message is not from assistant\nif st.session_state.messages[-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = generate_llama2_response(prompt)\n            placeholder = st.empty()\n            full_response = ''\n            for item in response:\n                full_response += item\n                placeholder.markdown(full_response)\n            placeholder.markdown(full_response)\n    message = {\"role\": \"assistant\", \"content\": full_response}\n    st.session_state.messages.append(message)\nImport necessary libraries\n\nFirst, import the necessary libraries:\n\nstreamlit - a low-code web framework used for creating the web frontend.\nreplicate - an ML model hosting platform that allows interfacing with the model via an API call.\nos - the operating system module to load the API key into the environment variable.\nimport streamlit as st\nimport replicate\nimport os\n\nDefine the app title\n\nThe title of the app displayed on the browser can be specified using the page_title parameter, which is defined in the st.set_page_config() method:\n\n# App title\nst.set_page_config(page_title=\"ü¶ôüí¨ Llama 2 Chatbot\")\n\nDefine the web app frontend for accepting the API token\n\nWhen designing the chatbot app, divide the app elements by placing the app title and text input box for accepting the Replicate API token in the sidebar and the chat input text in the main panel. To do this, place all subsequent statements under with st.sidebar:, followed by the following steps:\n\n1. Define the app title using the st.title() method.\n\n2. Use if-else statements to conditionally display either:\n\nA success message in a green box that reads API key already provided! for the if statement.\nA warning message in a yellow box along with a text input box asking for the API token, as none were detected in the Secrets, for the else statement.\n\nUse nested if-else statement to detect whether the API key was entered into the text box, and if so, display a success message:\n\nwith st.sidebar:\n    st.title('ü¶ôüí¨ Llama 2 Chatbot')\n    if 'REPLICATE_API_TOKEN' in st.secrets:\n        st.success('API key already provided!', icon='‚úÖ')\n        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n    else:\n        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n        else:\n            st.success('Proceed to entering your prompt message!', icon='üëâ')\n    os.environ['REPLICATE_API_TOKEN'] = replicate_api\nAdjustment of model parameters\n\nIn continuation from the above code snippet and inside the same with st.sidebar: statement, we're adding the following code block to allow users to select the Llama 2 model variant to use (namely llama2-7B or Llama2-13B) as well as adjust model parameters (namely temperature, top_p and max_length).\n\n    st.subheader('Models and parameters')\n    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n    if selected_model == 'Llama2-7B':\n        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n    elif selected_model == 'Llama2-13B':\n        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n\nStore, display, and clear chat messages\nThe first code block creates an initial session state to store the LLM generated response as part of the chat message history.\nThe next code block displays messages (via st.chat_message()) from the chat history by iterating through the messages variable in the session state.\nThe last code block creates a Clear Chat History button in the sidebar, allowing users to clear the chat history by leveraging the callback function defined on the preceding line.\n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\n# Display or clear chat messages\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\ndef clear_chat_history():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\nst.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n\nCreate the LLM response generation function\n\nNext, create the generate_llama2_response() custom function to generate the LLM‚Äôs response. It takes a user prompt as input, builds a dialog string based on the existing chat history, and calls the model using the replicate.run() function.\n\nThe model returns a generated response:\n\n# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\ndef generate_llama2_response(prompt_input):\n    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n    for dict_message in st.session_state.messages:\n        if dict_message[\"role\"] == \"user\":\n            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n        else:\n            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', \n                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n    return output\nAccept prompt input\n\nThe chat input box is displayed, allowing the user to enter a prompt. Any prompt entered by the user is added to the session state messages:\n\n# User-provided prompt\nif prompt := st.chat_input(disabled=not replicate_api):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.write(prompt)\n\nGenerate a new LLM response\n\nIf the last message wasn‚Äôt from the assistant, the assistant will generate a new response. While it‚Äôs formulating a response, a spinner widget will be displayed. Finally, the assistant's response will be displayed in the chat and added to the session state messages:\n\n# Generate a new response if last message is not from assistant\nif st.session_state.messages[-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = generate_llama2_response(prompt)\n            placeholder = st.empty()\n            full_response = ''\n            for item in response:\n                full_response += item\n                placeholder.markdown(full_response)\n            placeholder.markdown(full_response)\n    message = {\"role\": \"assistant\", \"content\": full_response}\n    st.session_state.messages.append(message)\n\n4. Set the API token\nOption 1. Set the API token in Secrets\n\nIf you want to provide your users with free access to your chatbot, you'll need to cover the costs as your credit card is tied to your account.\n\nTo set the API token in the Secrets management on Streamlit Community Cloud, click on the expandable menu at the far right, then click on Settings:\n\nTo define the REPLICATE_API_TOKEN environment variable, click on the Secrets tab and paste your Replicate API token:\n\nOnce the API token is defined in Secrets, users should be able to use the app without needing to use their own API key:\n\nOption 2. Set the API token in the app\n\nAn alternative to setting the API token in Secrets is to prompt users to specify it in the app. This way, users will be notified to provide their own Replicate API token to proceed with using the app:\n\n5. Deploy the app\n\nOnce the app is created, deploy it to the cloud in three steps:\n\nCreate a GitHub repository for the app.\nIn Streamlit Community Cloud, click on the New app button, then choose the repository, branch, and app file.\nClick Deploy! and the app will be live!\nWrapping up\n\nCongratulations! You‚Äôve learned how to build your own Llama 2 chatbot app using the LLM model hosted on Replicate.\n\nIt‚Äôs worth noting that the LLM was set to the 7B version and that model parameters (such as temperature and top_p) were initialized with a set of arbitrary values. This post also includes the Pro version, which allows users to specify the model and parameters. I encourage you to experiment with this setup, adjust these parameters, and explore your own variations. This can be a great opportunity to see how these modifications might affect the LLM-generated response.\n\nFor additional ideas and inspiration, check out the LLM gallery. If you have any questions, let me know in the comments below or find me on Twitter at @thedataprof or on LinkedIn at Chanin Nantasenamat. You can also check out the Streamlit YouTube channel or my personal YouTube channel, Data Professor.\n\nHappy chatbot-building! ü¶ô\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "gif2.gif (884√ó476)",
    "url": "https://blog.streamlit.io/content/images/2023/11/gif2.gif",
    "html": ""
  },
  {
    "title": "gif1.gif (920√ó476)",
    "url": "https://blog.streamlit.io/content/images/2023/11/gif1.gif",
    "html": ""
  },
  {
    "title": "static-table.png (2000√ó663)",
    "url": "https://blog.streamlit.io/content/images/2023/11/static-table.png",
    "html": ""
  },
  {
    "title": "rose-chart.png (1042√ó854)",
    "url": "https://blog.streamlit.io/content/images/2023/11/rose-chart.png",
    "html": ""
  },
  {
    "title": "prototype.png (2000√ó1277)",
    "url": "https://blog.streamlit.io/content/images/2023/11/prototype.png",
    "html": ""
  },
  {
    "title": "mockup.png (1251√ó1097)",
    "url": "https://blog.streamlit.io/content/images/2023/11/mockup.png",
    "html": ""
  },
  {
    "title": "multi-modality.png (1490√ó1396)",
    "url": "https://blog.streamlit.io/content/images/2023/11/multi-modality.png",
    "html": ""
  },
  {
    "title": "streamlit-dashboard-python.gif (1360√ó900)",
    "url": "https://blog.streamlit.io/content/images/2024/01/streamlit-dashboard-python.gif",
    "html": ""
  },
  {
    "title": "metrics-cards.png (1920√ó1080)",
    "url": "https://blog.streamlit.io/content/images/2024/01/metrics-cards.png",
    "html": ""
  },
  {
    "title": "inbound-outbound-metrics.png (2000√ó1051)",
    "url": "https://blog.streamlit.io/content/images/2024/01/inbound-outbound-metrics.png",
    "html": ""
  },
  {
    "title": "dashboard_heatmap.png (2000√ó661)",
    "url": "https://blog.streamlit.io/content/images/2024/01/dashboard_heatmap.png",
    "html": ""
  },
  {
    "title": "choropleth.png (504√ó248)",
    "url": "https://blog.streamlit.io/content/images/2024/01/choropleth.png",
    "html": ""
  },
  {
    "title": "How to build an LLM-powered ChatBot with Streamlit",
    "url": "https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build an LLM-powered ChatBot with Streamlit\n\nA step-by-step guide using the unofficial HuggingChat API\n\nBy Chanin Nantasenamat\nPosted in LLMs, May 10 2023\nWhat the HugChat app can do\nSet up the app on the Streamlit Community Cloud\nBuild the chatbot\n1. Required libraries\n2. Page config\n3. Sidebar\n4. Session state\n5. Display chat messages\n6. Function for bot response output\n7. Accept user prompt\n8. Generate bot response output\n9. Full code\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit-ers! üëã\n\nMy name is Chanin Nantasenamat, PhD. I‚Äôm working as a Senior Developer Advocate creating educational content on building Streamlit data apps. In my spare time, I love to create coding and data science tutorials on my YouTube channel, Data Professor.\n\nAre you looking to build an AI-powered chatbot using LLM models but without the heavy API cost? If you answered yes, then keep reading!\n\nYou‚Äôll build a chatbot that can generate responses to the user-provided prompt input (i.e., questions) using an open-source, no-cost LLM model OpenAssistant/oasst-sft-6-llama-30b-xor from the unofficial HuggingChat API known as HugChat. You‚Äôll deploy the chatbot as a Streamlit app that can be shared with the world!\n\nIn this post, you‚Äôll learn how to:\n\nSet up the app on the Streamlit Community Cloud\nBuild the chatbot\nü§ó\nWant to jump right in? Here's the HugChat app and the GitHub repo.\nWhat the HugChat app can do\n\nBefore we proceed with the tutorial, let's quickly grasp the app's functionality. Head over to the app and get familiar with its layout‚Äî(1) the Sidebar accepts the login credential, and (2) the Main panel displays conversational messages:\n\nInteract with it by (1) entering your prompt into the text input box and (2) reading the human/bot messages.\n\nSet up the app on the Streamlit Community Cloud\n\nClone the app-starter-kit repo to use as the template for creating the chatbot app. Then click on \"Use this template\":\n\nGive the repo a name (such as mychatbot). Next, click \"Create repository from the template.\" A copy of the repo will be placed in your account:\n\nNext, follow this blog post to get the newly cloned repo deployed on the Streamlit Community Cloud. When done, you should be able to see the deployed app:\n\nEdit the requirements.txt file by adding the following prerequisite Python libraries:\n\nstreamlit\nhugchat==0.0.8\n\nThis will spin up a server with these prerequisites pre-installed.\n\nLet's take a look at the contents of streamlit_app.py:\n\nimport streamlit as st\n\nst.title('üéà App Name')\n\nst.write('Hello world!')\n\n\nIn subsequent sections, you will modify the contents of this file with code snippets about the chatbot.\n\nFinally, before proceeding with app building, let's take a look at how the user will interact with it:\n\nFront-end: The user submits an input prompt (by providing a string of text to the text box via st.text_input()), and the app generates a response.\nBack-end: Input prompt is sent to hugchat (the unofficial port to the HuggingChat API) via streamlit-chat for generating a response.\nFront-end: Generated responses are displayed in the app via's message() command.\nBuild the chatbot\n\nFire up the streamlit_app.py file and replace the original content with code snippets mentioned below.\n\n1. Required libraries\n\nImport prerequisite Python libraries:\n\nimport streamlit as st\nfrom hugchat import hugchat\nfrom hugchat.login import Login\n2. Page config\n\nName the app using the page_title input argument in the st.set_page_config method (it'll be used as the app title and as the title in the preview when sharing on social media):\n\nst.set_page_config(page_title=\"ü§óüí¨ HugChat\")\n3. Sidebar\n\nCreate a sidebar for accepting Hugging Face authentication credentials:\n\nwith st.sidebar:\n    st.title('ü§óüí¨ HugChat')\n    if ('EMAIL' in st.secrets) and ('PASS' in st.secrets):\n        st.success('HuggingFace Login credentials already provided!', icon='‚úÖ')\n        hf_email = st.secrets['EMAIL']\n        hf_pass = st.secrets['PASS']\n    else:\n        hf_email = st.text_input('Enter E-mail:', type='password')\n        hf_pass = st.text_input('Enter password:', type='password')\n        if not (hf_email and hf_pass):\n            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n        else:\n            st.success('Proceed to entering your prompt message!', icon='üëâ')\n    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/)!')\n\nUse the with statement to confine the constituent contents to the sidebar. They include:\n\nThe app title is specified via st.title()\nif-else statements for detecting login credentials from st.secrets or to ask from users via st.text_input()\nA link to this tutorial blog via st.markdown()\n4. Session state\n\nInitialize the chatbot by with messages session state and giving it a starter message at the first app run:\n\n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I help you?\"}]\n\nHere, past denotes the human user's input and generated indicates the bot's response.\n\n5. Display chat messages\n\nConversational messages are displayed iteratively from the messages session state via the for loop together with the use of Streamlit‚Äôs chat feature st.chat_message().\n\n# Display chat messages\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n6. Function for bot response output\n\nCreate the generate_response(prompt) custom function for taking in user's input prompt as an argument to generate an AI response using the HuggingChat API via the hugchat.ChatBot() method (this LLM model can be swapped with any other one). Hugging Face login credentials are accepted via the Login() and sign.login() methods:\n\n# Function for generating LLM response\ndef generate_response(prompt_input, email, passwd):\n    # Hugging Face Login\n    sign = Login(email, passwd)\n    cookies = sign.login()\n    # Create ChatBot                        \n    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())\n    return chatbot.chat(prompt_input)\n7. Accept user prompt\n\nUser‚Äôs input prompt message are accepted via the st.chat_input() method and appended to the messages session state followed by displaying the message via st.chat_message() together with st.write():\n\n# User-provided prompt\nif prompt := st.chat_input(disabled=not (hf_email and hf_pass)):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.write(prompt)\n8. Generate bot response output\n\nUse an if condition to detect whether the last response in the messages session state is from the assistant or user. The chatbot will be triggered to generate a response if the last message is not from the chatbot (assistant). In generating the response, the st.chat_message(), st.spinner() and the custom generate_response() function are used where generated messages will display a spinner with a short message saying Thinking.... Finally, the generated response is saved to the messages session state.\n\n# Generate a new response if last message is not from assistant\nif st.session_state.messages[-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = generate_response(prompt, hf_email, hf_pass) \n            st.write(response) \n    message = {\"role\": \"assistant\", \"content\": response}\n    st.session_state.messages.append(message)\n9. Full code\n\nPutting all of this together, we get the following full code of the app:\n\nimport streamlit as st\nfrom hugchat import hugchat\nfrom hugchat.login import Login\n\n# App title\nst.set_page_config(page_title=\"ü§óüí¨ HugChat\")\n\n# Hugging Face Credentials\nwith st.sidebar:\n    st.title('ü§óüí¨ HugChat')\n    if ('EMAIL' in st.secrets) and ('PASS' in st.secrets):\n        st.success('HuggingFace Login credentials already provided!', icon='‚úÖ')\n        hf_email = st.secrets['EMAIL']\n        hf_pass = st.secrets['PASS']\n    else:\n        hf_email = st.text_input('Enter E-mail:', type='password')\n        hf_pass = st.text_input('Enter password:', type='password')\n        if not (hf_email and hf_pass):\n            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n        else:\n            st.success('Proceed to entering your prompt message!', icon='üëâ')\n    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/)!')\n    \n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I help you?\"}]\n\n# Display chat messages\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\n# Function for generating LLM response\ndef generate_response(prompt_input, email, passwd):\n    # Hugging Face Login\n    sign = Login(email, passwd)\n    cookies = sign.login()\n    # Create ChatBot                        \n    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())\n    return chatbot.chat(prompt_input)\n\n# User-provided prompt\nif prompt := st.chat_input(disabled=not (hf_email and hf_pass)):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.write(prompt)\n\n# Generate a new response if last message is not from assistant\nif st.session_state.messages[-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = generate_response(prompt, hf_email, hf_pass) \n            st.write(response) \n    message = {\"role\": \"assistant\", \"content\": response}\n    st.session_state.messages.append(message)\nWrapping up\n\nIn this post, I've shown you how to create a chatbot app using an open-source LLM from the unofficial HuggingChat API and Streamlit. In only a few lines of code, you can create your own AI-powered chatbot.\n\nI hope this tutorial encourages you to explore the endless possibilities of chatbot development using different models and techniques. The sky is the limit!\n\nIf you have any questions, please leave them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn. Share your app creations on social media and tag me or the Streamlit account, and I'll be happy to provide feedback or help retweet!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "streamlit-dashboard-components.jpg (2000√ó1440)",
    "url": "https://blog.streamlit.io/content/images/2024/01/streamlit-dashboard-components.jpg",
    "html": ""
  },
  {
    "title": "sample-conversation.png (2000√ó1165)",
    "url": "https://blog.streamlit.io/content/images/2023/12/sample-conversation.png",
    "html": ""
  },
  {
    "title": "st-progress-loading.png (2000√ó1039)",
    "url": "https://blog.streamlit.io/content/images/2023/12/st-progress-loading.png",
    "html": ""
  },
  {
    "title": "LangChain tutorial #5: Build an Ask the Data app",
    "url": "https://blog.streamlit.io/langchain-tutorial-5-build-an-ask-the-data-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain tutorial #5: Build an Ask the Data app\n\nLeverage Agents in LangChain to interact with pandas DataFrame\n\nBy Chanin Nantasenamat\nPosted in LLMs, July 21 2023\nWhat are Agents?\nUsing Agents in LangChain\nApp overview\nStep 1. Get an OpenAI API key\nStep 2. Set up the coding environment\nLocal development\nCloud development\nStep 3. Build the app\nApp overview\nImport libraries\nDisplay the app title\nLoad the CSV file\nCreate the LLM response generation function\nInput widgets\nDefine the app logic\nStep 4. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nLarge language models (LLMs) have revolutionized how we process and understand text data, enabling a diverse array of tasks spanning text generation, summarization, classification, and much more. Combining LangChain and Streamlit to build LLM-powered applications is a potent combination for unlocking an array of possibilities, especially for developers interested in creating chatbots, personal assistants, and content creation apps.\n\nIn the previous four LangChain tutorials, you learned about three of the six key modules: model I/O (LLM model and prompt templates), data connection (document loader, text splitting, embeddings, and vector store), and chains (summarize chain and question-answering chain).\n\nThis tutorial explores the use of the fourth LangChain module, Agents. Specifically, we'll use the pandas DataFrame Agent, which allows us to work with pandas DataFrame by simply asking questions.\n\nWe'll build the pandas DataFrame Agent app for answering questions on a pandas DataFrame created from a user-uploaded CSV file in four steps:\n\nGet an OpenAI API key\nSet up the coding environment\nBuild the app\nDeploy the app\n\n\nü¶ú\nWant to jump right in? Here's the demo app and the repo code.\nWhat are Agents?\n\nAccording to Harrison Chase, agents \"use an LLM to determine which actions to take and in what order.\" An action can refer to using tools, observing their output, or returning a response to the user. Tools are entities that take a string as input and return a string as output. Examples of tools include APIs, databases, search engines, LLMs, chains, other agents, shells, and Zapier.\n\nAgents are comprised of two types:\n\nAction agents\nPlan-and-execute agents\nUsing Agents in LangChain\n\nTo use an agent in LangChain, you need to specify three key elements:\n\nLLM. LLM is responsible for determining the course of action that an agent would take to fulfill its task of answering a user query. If you're using the OpenAI LLM, it's available via OpenAI() from langchain.llms.\nTools. These are resources that an agent can use to accomplish its task, such as querying a database, accessing an API, or searching Google. You can load them via load_tools() from langchain.agents.\nAgent. The available agent types are action agents or plan-and-execute agents. You can access them via AgentType() from langchain.agents.\n\nIn this tutorial, we'll be using the pandas DataFrame Agent, which can be created using create_pandas_dataframe_agent() from langchain.agents.\n\n\n\nü¶ú\nCheck out the LangChain documentation on pandas DataFrame Agent.\nApp overview\n\nLet's take a look at the general flow of the app.\n\nOnce the app is loaded, the user should perform the following steps in sequential order:\n\nUpload a CSV file. You can also tweak the underlying code to read in tabular formats such as Excel or tab-delimited files.\nSelect an example query from the drop-down menu or provide your own custom query by selecting the \"Other\" option.\nEnter your OpenAI API key.\n\nThat's all for the frontend! As for the backend, the pandas DataFrame Agent will work its magic on the data and return an LLM-generated answer.\n\nNow let's take a look at the app in action:\n\n\n\nStep 1. Get an OpenAI API key\n\nYou can find a detailed walkthrough on obtaining an OpenAI API key in LangChain Tutorial #1.\n\nStep 2. Set up the coding environment\nLocal development\n\nTo set up a local coding environment with the necessary libraries, use pip install as shown below (make sure you have Python version 3.7 or higher):\n\npip install streamlit openai langchain pandas tabulate\n\nCloud development\n\nIn addition to using a local computer to develop apps, you can deploy them on the cloud using Streamlit Community Cloud. You can use the Streamlit app template to do this (read more here).\n\nNext, add the following Python libraries to the requirements.txt file:\n\nstreamlit\nopenai\nlangchain\npandas\ntabulate\n\nStep 3. Build the app\nApp overview\n\nThe entire app consists of 47 lines of code, as shown below:\n\nimport streamlit as st\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import create_pandas_dataframe_agent\nfrom langchain.agents.agent_types import AgentType\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Ask the Data App')\nst.title('ü¶úüîó Ask the Data App')\n\n# Load CSV file\ndef load_csv(input_csv):\n  df = pd.read_csv(input_csv)\n  with st.expander('See DataFrame'):\n    st.write(df)\n  return df\n\n# Generate LLM response\ndef generate_response(csv_file, input_query):\n  llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613', temperature=0.2, openai_api_key=openai_api_key)\n  df = load_csv(csv_file)\n  # Create Pandas DataFrame Agent\n  agent = create_pandas_dataframe_agent(llm, df, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS)\n  # Perform Query using the Agent\n  response = agent.run(input_query)\n  return st.success(response)\n\n# Input widgets\nuploaded_file = st.file_uploader('Upload a CSV file', type=['csv'])\nquestion_list = [\n  'How many rows are there?',\n  'What is the range of values for MolWt with logS greater than 0?',\n  'How many rows have MolLogP value greater than 0.',\n  'Other']\nquery_text = st.selectbox('Select an example query:', question_list, disabled=not uploaded_file)\nopenai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n\n# App logic\nif query_text is 'Other':\n  query_text = st.text_input('Enter your query:', placeholder = 'Enter query here ...', disabled=not uploaded_file)\nif not openai_api_key.startswith('sk-'):\n  st.warning('Please enter your OpenAI API key!', icon='‚ö†')\nif openai_api_key.startswith('sk-') and (uploaded_file is not None):\n  st.header('Output')\n  generate_response(uploaded_file, query_text)\n\nImport libraries\n\nTo start, import the necessary libraries:\n\nStreamlit. A low-code web framework used for creating the app's frontend\npandas. A data wrangling framework for loading the CSV file as a DataFrame\nLangChain. An LLM framework that coordinates the use of an LLM model to generate a response based on the user-provided prompt.\nimport streamlit as st\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import create_pandas_dataframe_agent\nfrom langchain.agents.agent_types import AgentType\n\nDisplay the app title\n\nNext, display the title of the app:\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Ask the Data App')\nst.title('ü¶úüîó Ask the Data App')\n\nLoad the CSV file\n\nSince the CSV file is one of the app's inputs, along with the data query, you need to create a custom function to load it (use pandas' read_csv() method). Once loaded, display the DataFrame inside an expander box:\n\n# Load CSV file\ndef load_csv(input_csv):\n  df = pd.read_csv(input_csv)\n  with st.expander('See DataFrame'):\n    st.write(df)\n  return df\n\nCreate the LLM response generation function\n\nThe next step is to process data using the Agent, specifically the pandas DataFrame Agent, and the LLM model (GPT 3.5).\n\nTo create an instance of the LLM model, use ChatOpenAI() and set gpt-3.5-turbo-0613 as the model_name. Next, create the pandas DataFrame Agent using the create_pandas_dataframe_agent() method and assign the LLM model, defined by llm, and the input data, defined by df.\n\nü¶ú\nNOTE: While creating and testing the app, I discovered that usage costs were significantly higher compared to previous apps built in this tutorial series. So I decided to use the GPT 3.5 model due to its significantly lower cost.\n\n\n# Generate LLM response\ndef generate_response(csv_file, input_query):\n  llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613', temperature=0.2, openai_api_key=openai_api_key)\n  df = load_csv(csv_file)\n  # Create Pandas DataFrame Agent\n  agent = create_pandas_dataframe_agent(llm, df, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS)\n  # Perform Query using the Agent\n  response = agent.run(input_query)\n  return st.success(response)\n\nInput widgets\n\nNext, create input widgets to accept various variables for data analysis. These include:\n\nThe user-provided CSV file (stored in the uploaded_file variable)\nThe input query (stored in the question_list and query_text variables)\nThe OpenAI API (stored in the openai_api_key variable)\n\n\n# Input widgets\nuploaded_file = st.file_uploader('Upload a CSV file', type=['csv'])\nquestion_list = [\n  'How many rows are there?',\n  'What is the range of values for MolWt with logS greater than 0?',\n  'How many rows have MolLogP value greater than 0.',\n  'Other']\nquery_text = st.selectbox('Select an example query:', question_list, disabled=not uploaded_file)\nopenai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n\nDefine the app logic\n\nThe app logic is defined in this last code block. Follow these steps:\n\nCheck if the user has selected the Other option from the drop-down select box defined in query_text to provide a custom text query. If so, the user can enter their query text.\nCheck if the user has provided their OpenAI API key. If not, a reminder message is displayed for the user to enter their API key.\nPerform a final check for the API key and the user-provided CSV file. If the check is successful (meaning the user has provided all necessary information), we proceed to generate a response from the pandas DataFrame Agent.\n\n\n# App logic\nif query_text is 'Other':\n  query_text = st.text_input('Enter your query:', placeholder = 'Enter query here ...', disabled=not uploaded_file)\nif not openai_api_key.startswith('sk-'):\n  st.warning('Please enter your OpenAI API key!', icon='‚ö†')\nif openai_api_key.startswith('sk-') and (uploaded_file is not None):\n  st.header('Output')\n  generate_response(uploaded_file, query_text)\n\nStep 4. Deploy the app\n\nOnce the app has been created, it can be deployed to the cloud in three steps:\n\nCreate a GitHub repository to store the app files.\nGo to the Streamlit Community Cloud, click the New app button, and select the appropriate repository, branch, and application file.\nFinally, click Deploy!.\n\nAfter a few moments, the app should be ready to use!\n\nWrapping up\n\nYou've learned how to build an Ask the Data app that lets you ask questions to understand your data better. We used Streamlit as the frontend to accept user input (CSV file, questions about the data, and OpenAI API key) and LangChain for backend processing of the data via the pandas DataFrame Agent.\n\nIf you're looking for ideas and inspiration, check out the Generative AI page and the LLM gallery. And if you have any questions, please post them in the comments below or on Twitter at @thedataprof, on LinkedIn, on the Streamlit YouTube channel, or on my personal YouTube channel, Data Professor.\n\nI can't wait to see what you'll build! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "load-data-screen.png (2000√ó1151)",
    "url": "https://blog.streamlit.io/content/images/2023/12/load-data-screen.png",
    "html": ""
  },
  {
    "title": "tsv-time-machine-screenshot.png (2000√ó1159)",
    "url": "https://blog.streamlit.io/content/images/2023/12/tsv-time-machine-screenshot.png",
    "html": ""
  },
  {
    "title": "Untitled--6-.png (792√ó893)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--6-.png",
    "html": ""
  },
  {
    "title": "Untitled--5-.png (782√ó500)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--5-.png",
    "html": ""
  },
  {
    "title": "LangChain tutorial #4: Build an Ask the Doc app",
    "url": "https://blog.streamlit.io/langchain-tutorial-4-build-an-ask-the-doc-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain tutorial #4: Build an Ask the Doc app\n\nHow to get answers from documents using embeddings, a vector store, and a question-answering chain\n\nBy Chanin Nantasenamat\nPosted in LLMs, June 20 2023\nWhat is document question-answering?\nStep 1. Ingestion\nStep 2. Generation\nApp overview\nStep 1. Get an OpenAI API key\nStep 2. Set up the coding environment\nLocal development\nCloud development\nStep 3. Build the app\nImport libraries\nCreate the LLM response generation function\nDefine the web app frontend\nStep 4. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn recent months, large language models (LLMs) have attracted widespread attention as they open up new opportunities, particularly for developers creating chatbots, personal assistants, and content.\n\nIn the previous LangChain tutorials, you learned about three of the six key modules: model I/O (LLM model and prompt templates), data connection (document loader and text splitting), and chains (summarize chain).\n\nIn this tutorial, we'll explore how to use these modules, how to create embeddings and store them in a vector store, and how to use a specialized chain for question answering about a text document. We'll use these tools to build the Ask the Doc app in four steps:\n\nGet an OpenAI API key\nSet up the coding environment\nBuild the app\nDeploy the app\n\n\nü¶ú\nIf you want to skip reading and hop right in, here is the app and here is the code.\nWhat is document question-answering?\n\nAs the name implies, document question-answering answers questions about a specific document. This process involves two steps:\n\nStep 1. Ingestion\n\nThe document is prepared through a process known as ingestion so that the LLM model can use it. Ingestion transforms it into an index, the most common being a vector store. The process involves:\n\nLoading the document\nSplitting the document\nCreating embeddings\nStoring the embeddings in a database (a vector store)\nStep 2. Generation\n\nWith the index or vector store in place, you can use the formatted data to generate an answer by following these steps:\n\nAccept the user's question\nIdentify the most relevant document for the question\nPass the question and the document as input to the LLM to generate an answer\n\n\nüîó\nCheck out the LangChain documentation on question answering over documents.\nApp overview\n\nAt a conceptual level, the app's workflow remains impressively simple:\n\nThe user uploads a document text file, asks a question, provides an OpenAI API key, and clicks \"Submit.\"\nLangChain processes the two input elements. First, it splits the input document into chunks, creates embedding vectors, and stores them in the embeddings database (i.e., the vector store). Then it applies the user-provided question to the Question Answering chain so that the LLM can answer the question:\n\nLet's see the app in action.\n\nCheck out these materials if you want to follow along:\n\nInput file: state_of_the_union.txt\nQuestion: \"What did the president say about Ketanji Brown Jackson?\"\n\nGo ahead and try it:\n\nStep 1. Get an OpenAI API key\n\nFor a detailed walkthrough on getting an OpenAI API key, read LangChain Tutorial #1.\n\nStep 2. Set up the coding environment\nLocal development\n\nTo set up a local coding environment, use pip install (make sure you have Python version 3.7 or higher):\n\npip install streamlit langchain openai tiktoken\n\nCloud development\n\nYou can deploy your app to the Streamlit Community Cloud using the Streamlit app template. (read more in the previous blog post).\n\nTo proceed, include the following prerequisite Python libraries in the requirements.txt file:\n\nstreamlit\nlangchain\nopenai\nchromadb\ntiktoken\n\nStep 3. Build the app\n\nThe code for the app is only 46 lines, 10 of which are in-line documentation explaining what each code block does:\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\n\ndef generate_response(uploaded_file, openai_api_key, query_text):\n    # Load document if file is uploaded\n    if uploaded_file is not None:\n        documents = [uploaded_file.read().decode()]\n        # Split documents into chunks\n        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n        texts = text_splitter.create_documents(documents)\n        # Select embeddings\n        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n        # Create a vectorstore from documents\n        db = Chroma.from_documents(texts, embeddings)\n        # Create retriever interface\n        retriever = db.as_retriever()\n        # Create QA chain\n        qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)\n        return qa.run(query_text)\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Ask the Doc App')\nst.title('ü¶úüîó Ask the Doc App')\n\n# File upload\nuploaded_file = st.file_uploader('Upload an article', type='txt')\n# Query text\nquery_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)\n\n# Form input and query\nresult = []\nwith st.form('myform', clear_on_submit=True):\n    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))\n    if submitted and openai_api_key.startswith('sk-'):\n        with st.spinner('Calculating...'):\n            response = generate_response(uploaded_file, openai_api_key, query_text)\n            result.append(response)\n            del openai_api_key\n\nif len(result):\n    st.info(response)\n\n\nLet's dissect the individual code blocks‚Ä¶\n\nImport libraries\n\nFirst, import the necessary libraries (primarily Streamlit and LangChain):\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\n\nCreate the LLM response generation function\n\nNext, create the generate_response() custom function that contains the bulk of the LLM pipeline (the uploaded file is loaded as a text string).\n\nUse these LangChain functions to preprocess the text:\n\nOpenAI() loads the OpenAI LLM model.\nCharacterTextSplitter() splits documents into chunks.\nOpenAIEmbeddings() encodes the document chunks or strings of text as embeddings (a vector or list of floating point numbers). Distances amongst the embeddings provide a measure of relatedness that determines their similarity or difference. Embeddings are useful as they can be used for anomaly detection, classification, recommendations, search, topic clustering, etc.\nChroma() is an open-source embedding database (also called a vector store‚Äîa database of embedding vectors). Particularly, Chroma.from_documents() is used for creating the vector store index using the document chunk after the text split and the OpenAIEmbeddings() function as input arguments.\nRetrievalQA() is the question-answering chain that takes as input arguments the LLM via the llm parameter, the chain type to use via the chain_type parameter, and the retriever via the retriever parameter.\n\nFinally, the run() method is executed on the defined instance of RetrievalQA(), using the query text as the input argument:\n\ndef generate_response(uploaded_file, openai_api_key, query_text):\n    # Load document if file is uploaded\n    if uploaded_file is not None:\n        documents = [uploaded_file.read().decode()]\n    # Split documents into chunks\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.create_documents(documents)\n    # Select embeddings\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n    # Create a vectorstore from documents\n    db = Chroma.from_documents(texts, embeddings)\n    # Create retriever interface\n    retriever = db.as_retriever()\n    # Create QA chain\n    qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)\n    return qa.run(query_text)\n\nDefine the web app frontend\n\nUse the page_title parameter in the st.set_page_config() method to give the app a title (displayed in the browser).\n\nDisplay the in-app title using st.title():\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Ask the Doc App')\nst.title('ü¶úüîó Ask the Doc App')\n\n\nNext, add input widgets that allow users to upload text files using st.file_uploader() and to ask questions about the uploaded text document using st.text_input():\n\n# File upload\nuploaded_file = st.file_uploader('Upload an article', type='txt')\n# Query text\nquery_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)\n\n\nAfter the user has provided the above two inputs, the form will unlock the ability to enter the OpenAI API key via st.text_input().\n\nAfter you enter the OpenAI API key and click on Submit, you'll see a spinner element displaying the message Calculating.... This triggers the generate_response() function, which generates the LLM's answer to the user's question.\n\nOnce it's been generated, the API key is deleted to ensure API safety:\n\n# Form input and query\nresult = []\nwith st.form('myform', clear_on_submit=True):\n    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))\n    if submitted and openai_api_key.startswith('sk-'):\n        with st.spinner('Calculating...'):\n            response = generate_response(uploaded_file, openai_api_key, query_text)\n            result.append(response)\n            del openai_api_key\n\nif len(result):\n    st.info(response)\n\n\nAn empty list called result is defined before the form. This is followed by an if statement that displays the LLM-generated response when the result list is populated with it. This approach allows the API key to be deleted after an LLM-generated response is created.\n\nü¶ú\nNOTE: Prior to deleting the API key, the LLM response is appended to the initially empty result list.\nStep 4. Deploy the app\n\nAfter creating the app, you can launch it in three steps:\n\nEstablish a GitHub repository specifically for the app.\nNavigate to Streamlit Community Cloud, click the New app button, and choose the appropriate repository, branch, and application file.\nFinally, hit the Deploy! button.\n\nYour app will be live in no time!\n\nWrapping up\n\nIn this article, you've learned how to create the Ask the Doc app that facilitates question-answering about a user-uploaded text document. In other words, you prepared the document for ingestion to be used by the LLM for generating an answer to the user's question. I can't wait to see what you'll build!\n\nIf you're looking for ideas and inspiration, check out the LLM gallery. And if you have any questions, let me know in the comments below, or find me on Twitter at @thedataprof, ¬†on LinkedIn, on the Streamlit YouTube channel, or my personal YouTube channel Data Professor.\n\nHappy Streamlit-ing! üéà\n\nP.S. This post was made possible thanks to the technical review by Tim Conkling and editing by Ksenia Anske.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Untitled--4-.png (794√ó826)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--4-.png",
    "html": ""
  },
  {
    "title": "Untitled--3-.png (795√ó532)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--3-.png",
    "html": ""
  },
  {
    "title": "Untitled--2-.png (791√ó179)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--2-.png",
    "html": ""
  },
  {
    "title": "Untitled--1-.png (788√ó179)",
    "url": "https://blog.streamlit.io/content/images/2023/12/Untitled--1-.png",
    "html": ""
  },
  {
    "title": "Chanin Nantasenamat - Streamlit (Page 2)",
    "url": "https://blog.streamlit.io/author/chanin/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Chanin Nantasenamat\nSenior Developer Advocate at Streamlit\n16 posts\nWebsite\nTwitter\nHackathon 101: 5 simple tips for beginners\n\nPrepare to win your first hackathon!\n\nTutorials\nby\nChanin Nantasenamat\n,\nMarch 16 2023\nHost your Streamlit app for free\n\nLearn how to transfer your apps from paid platforms to Streamlit Community Cloud\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 24 2023\nStreamlit Quests: Getting started with Streamlit\n\nThe guided path for learning Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nNovember 18 2022\nStreamlit App Starter Kit: How to build apps faster\n\nSave 10 minutes every time you build an app\n\nTutorials\nby\nChanin Nantasenamat\n,\nSeptember 27 2022\n30 Days of Streamlit\n\nA fun challenge to learn and practice using Streamlit\n\nAdvocate Posts\nby\nChanin Nantasenamat\n,\nApril 1 2022\nHow to master Streamlit for data¬†science\n\nThe essential Streamlit for all your data¬†science needs\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 18 2022\n‚Üê Previous page\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Building a Streamlit and scikit-learn app with ChatGPT",
    "url": "https://blog.streamlit.io/building-a-streamlit-and-scikit-learn-app-with-chatgpt/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding a Streamlit and scikit-learn app with ChatGPT\n\nCatching up on coding skills with an AI assistant\n\nBy Michael Hunger\nPosted in LLMs, June 16 2023\nCreating the \"Hello, World!\" version of our EDA app\nMichael\nChatGPT\nGetting the initial app up and running\nUnderstanding the initial app\nMichael\nChatGPT\nFixing the ugly legend\nMichael\nChatGPT\nMichael\nChatGPT\nHardcoding the CSV file to ease the app development\nMichael\nChatGPT\nLoading data from the database\nMichael\nChatGPT\nImplementing caching to reduce data reloads\nMichael\nChatGPT\nLearning about updated APIs from a documentation drop\nMichael\nChatGPT\nAdding \"real\" data science‚Äîpredictions with scikit-learn\nMichael\nChatGPT\nResolving scikit-learn errors\nMichael\nChatGPT\nMichael\nChatGPT\nAddressing column order consistency\nMichael\nChatGPT\nMichael\nChatGPT\nFixing the billion squared error üò¶\nMichael\nChatGPT\nIncorporating predictions into the app\nMichael\nChatGPT\nMichael\nChatGPT\nAdding selection by name\nMichael\nChatGPT\nMichael\nChatGPT\nImplementing dataframe filters\nMichael\nChatGPT\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey, Streamlit community! üëã\n\nMy name is Michael, and I lead Developer Relations and Labs at Neo4j, an open-source graph database company. In 2017, we began building a scalable graph algorithms library with capabilities like Graph Neural Networks (GNNs), graph embeddings, and k-nearest neighbor (kNN) similarity graphs.\n\nSince ChatGPT became available last year, I've spent a lot of time tinkering with large language models (LLMs) and exploring their impact on the database space. I've also been using them as a daily tool for ideation, writing, summarization, and coding. With GPT-4, this has become even better.\n\nSimon Willison, a prolific blogger, coder, and author of Datasette, stated that \"With ChatGPT support, I'm not afraid to start side-projects that would have taken me several days, because now I can finish them in a few hours.\" Inspired by his words, a couple of weeks ago, I started a series of live streams appropriately called \"Michael and Bot - Livecoding with ChatGPT.\" The goal was to explore new territories and create something useful within a two-hour window. So far, we've created:\n\nA Chrome extension that sends selected text to OpenAI's API to extract entities and relationships and stores them in Neo4j.\nA basic network graph layout algorithm built from scratch in JavaScript.\nA Streamlit app for exploratory data analysis (EDA) with predictions using scikit-learn.\n\nI first got excited about Streamlit by the awesome videos my friend Mark Needham made. Today, I'm excited to dive into the adventure of creating the Streamlit app with you.\n\nüé•\nWatch the recorded live stream if you want to see me sweat over building this app. Or skip ahead and check out the final code here.\nCreating the \"Hello, World!\" version of our EDA app\n\nI started with this question.\n\nMichael\n\nWe want to code together today. We're going to build a streamlit app for EDA. Can you give me the files I need to start with the \"Hello, World!\" set of visual components?\n\nChatGPT\n\nOf course! To create a simple Streamlit app for EDA with a \"Hello, World!\" set of visual components, you'll need the following files:\n\nrequirements.txt: This file lists the required Python packages for your project.\napp.py: This file contains the main Streamlit app code.\n\nFirst, create a new directory for your project and navigate to it using the terminal.\n\nNext, create a requirements.txt file in your project directory and include the following packages:\n\nstreamlit==0.88.0\npandas==1.3.2\nnumpy==1.21.2\nmatplotlib==3.4.3\nseaborn==0.11.2\n\n\nNow, create an app.py file in your project directory with the following content:\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef main():\n    st.title(\"Hello, World! EDA Streamlit App\")\n\n    st.header(\"Upload your CSV data file\")\n    data_file = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n\n    if data_file is not None:\n        data = pd.read_csv(data_file)\n        st.write(\"Data overview:\")\n        st.write(data.head())\n\n        st.sidebar.header(\"Visualizations\")\n        plot_options = [\"Bar plot\", \"Scatter plot\", \"Histogram\", \"Box plot\"]\n        selected_plot = st.sidebar.selectbox(\"Choose a plot type\", plot_options)\n\n        if selected_plot == \"Bar plot\":\n            x_axis = st.sidebar.selectbox(\"Select x-axis\", data.columns)\n            y_axis = st.sidebar.selectbox(\"Select y-axis\", data.columns)\n            st.write(\"Bar plot:\")\n            fig, ax = plt.subplots()\n            sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)\n            st.pyplot(fig)\n\n        elif selected_plot == \"Scatter plot\":\n            x_axis = st.sidebar.selectbox(\"Select x-axis\", data.columns)\n            y_axis = st.sidebar.selectbox(\"Select y-axis\", data.columns)\n            st.write(\"Scatter plot:\")\n            fig, ax = plt.subplots()\n            sns.scatterplot(x=data[x_axis], y=data[y_axis], ax=ax)\n            st.pyplot(fig)\n\n        elif selected_plot == \"Histogram\":\n            column = st.sidebar.selectbox(\"Select a column\", data.columns)\n            bins = st.sidebar.slider(\"Number of bins\", 5, 100, 20)\n            st.write(\"Histogram:\")\n            fig, ax = plt.subplots()\n            sns.histplot(data[column], bins=bins, ax=ax)\n            st.pyplot(fig)\n\n        elif selected_plot == \"Box plot\":\n            column = st.sidebar.selectbox(\"Select a column\", data.columns)\n            st.write(\"Box plot:\")\n            fig, ax = plt.subplots()\n            sns.boxplot(data[column], ax=ax)\n            st.pyplot(fig)\n\nif __name__ == \"__main__\":\n    main()\n\n\nTo run the app, make sure you have the required packages installed by running:\n\npip install -r requirements.txt\n\n\nFinally, start the Streamlit app with the following command:\n\nstreamlit run app.py\n\n\nYour app will open in your default web browser, and you can start uploading your CSV file and exploring different visualizations. Enjoy coding together!\n\nGetting the initial app up and running\n\nI created the necessary files and updated the dependencies using PyPI. The training cutoff for ChatGPT was in September 2021; that's why the versions were a bit dated. Fortunately, all of the APIs still worked. As a result, I was able to run my app successfully:\n\nstreamlit==1.21.0\npandas==1.5.3\nnumpy==1.24.2\nmatplotlib==3.7.1\nseaborn==0.12.2\npython-dotenv==1.0.0\nneo4j==5.7.0\n\n\nI tested the app by using a CSV file that contained movie data, including titles, genres, popularity, revenue, and more. It worked perfectly! How cool!\n\nUnderstanding the initial app\n\nThere was a lot of code with many API calls. To better understand the Python code, I asked ChatGPT for help.\n\nMichael\n\nExplain the code in the Python file in 6 bullet points.\n\nChatGPT\nImport required libraries: The code starts by importing necessary libraries such as Streamlit, Pandas, NumPy, Matplotlib, and Seaborn for creating the web app and visualizations.\nDefine the main function: The main() function contains the core logic of the Streamlit app. It is responsible for rendering the user interface, handling file uploads, and creating visualizations.\nFile uploader: Inside the main function, a file uploader is created using st.file_uploader, which allows users to upload CSV files. If a valid file is uploaded, the data is read into a Pandas DataFrame.\nSidebar: The app's sidebar is created using st.sidebar. It contains a header and a select box for users to choose the type of plot they want to generate. Additional options for each plot type are also provided in the sidebar.\nPlot generation: Depending on the selected plot type, the appropriate Seaborn plotting function is called (e.g. sns.barplot, sns.scatterplot, sns.histplot, or sns.boxplot). The user-selected options are passed as arguments to the plotting functions.\nDisplay visualizations: After generating the plots using Matplotlib and Seaborn, they are displayed in the main part of the Streamlit app using st.pyplot(), which takes the Matplotlib figure as an argument.\n\nOkay, cool, that explains it.\n\nFixing the ugly legend\n\nWhile playing around with the charts, I noticed the legend labels overlapping. I asked our coding friend to either slant them or make them vertical.\n\nMichael\n\nThe labels on the x-axis are overlapping. Can we render them at an angle or vertically to improve readability?\n\nChatGPT\n\nYou can render the x-axis labels slanted or vertically to improve readability. To achieve this, you can modify the app.py code to adjust the x-axis label rotation using the set_xticklabels() function with the rotation parameter. Here's how you can do it for each plot type:\n\nFor the Bar plot:\n\n# ...\nif selected_plot == \"Bar plot\":\n    x_axis = st.sidebar.selectbox(\"Select x-axis\", data.columns)\n    y_axis = st.sidebar.selectbox(\"Select y-axis\", data.columns)\n    st.write(\"Bar plot:\")\n    fig, ax = plt.subplots()\n    sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)\n\t\t# Add this line\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")  \n    st.pyplot(fig)\n# ...\n\n\nIn the above code snippets, the rotation parameter is set to 45 degrees for slanted labels. You can set it to 90 degrees for vertical labels. The ha parameter (horizontal alignment) is set to \"right\" to improve the appearance of the rotated labels. Update the corresponding sections in your app.py file to apply these changes to your app.\n\nIt was really nice that it remembered the context of the previous code and only showed me the necessary changes. It also highlighted the changes that I should make to other charts. For brevity, I haven't included them here.\n\nI noticed too many ticks on the x-axis, so I asked ChatGPT to fix it.\n\nMichael\n\nCan we also reduce the number of ticks on the x-axis too?\n\nChatGPT\n\nYes, you can reduce the number of ticks on the x-axis. One way to do this is by using the MaxNLocator function from the matplotlib.ticker library. The MaxNLocator function helps in setting the maximum number of intervals on the x-axis. You'll need to import the ticker library first and then adjust the code for each plot type accordingly.\n\nFirst, add the following import statement at the beginning of your app.py file:\n\nimport matplotlib.ticker as ticker\n\n\nNow, update each plot type in the app.py file to set the number of ticks on the x-axis:\n\nFor the Bar plot:\n\n# ...\nif selected_plot == \"Bar plot\":\n    x_axis = st.sidebar.selectbox(\"Select x-axis\", data.columns)\n    y_axis = st.sidebar.selectbox(\"Select y-axis\", data.columns)\n    st.write(\"Bar plot:\")\n    fig, ax = plt.subplots()\n    sns.barplot(x=data[x_axis], y=data[y_axis], ax=ax)\n\n\t  # Add this line\n    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True, nbins=10))  \n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    st.pyplot(fig)\n# ...\n\n\nIn the above code snippets, ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True, nbins=10)) sets the maximum number of intervals on the x-axis to 10. You can change the nbins parameter to adjust the number of ticks as needed. Update the corresponding sections in your app.py file to apply these changes to your app.\n\nIn the end, I changed it to a 90-degree slant as the angled labels were still cut off a bit. You can see the results in the screenshot below.\n\nHardcoding the CSV file to ease the app development\n\nThat was much better. However, it was frustrating to have to re-upload the file each time I made a change. To streamline the process, I asked GPT-4 to hardcode the file.\n\nMichael\n\nCan we default the file-upload result to a fixed filename while testing?\n\nChatGPT\n\nYou can default the file-upload result to a fixed filename while testing. You'll need to load the data from the fixed file path when the file upload returns None. This way, you can quickly test your app without having to upload the file every time.\n\nHere's how to modify the app.py code to default to a fixed file path when no file is uploaded:\n\n# ...\n\ndef main():\n    st.title(\"Hello, World! EDA Streamlit App\")\n\n    st.header(\"Upload your CSV data file\")\n    data_file = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n\n    if data_file is not None:\n        data = pd.read_csv(data_file)\n    else:  # Add this block\n\t\t\t\t# Replace with your fixed file path\n        fixed_file_path = \"your_fixed_file_path.csv\"  \n        data = pd.read_csv(fixed_file_path)\n\n    st.write(\"Data overview:\")\n    st.write(data.head())\n\n    # ...\n\n\nReplace your_fixed_file_path.csv with the actual path to the fixed CSV file, you want to use for testing. With this modification, when no file is uploaded, the app will load data from the fixed file path, allowing you to test your app without uploading a file every time.\n\nLoading data from the database\n\nInstead of uploading a file, you can load the data directly from a database. To do this, you can add a function to load the movie data from your recommendation graph in Neo4j Aura.\n\nüí°\nYou can sign up for free and create a recommendations instance in just a few minutes.\n\nTo use the database credentials in my application, I downloaded them as a config file and added them as environment variables. I used the variables in the .env file using dotenv.\n\nAll I needed at that point was the driver and a query. The results from the dict were then converted into a Pandas dataframe:\n\nimport neo4j\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nURI = os.getenv('NEO4J_URI')\nAUTH = (os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))\nQUERY = \"\"\"\n        MATCH (m:Movie) \n        RETURN m.movieId as id,m.title as title,m.budget as budget, m.countries[0] as country,\n        m.imdbId as imdbId,m.imdbRating as rating,m.imdbVotes as votes,\n        m.languages[0] as language,m.plot as plot,m.poster as poster,\n\t\t\t\tm.released as released,m.revenue as revenue,\n        m.runtime as runtime,m.tmdbId as tmdbId,\n        m.url as url,m.year as year,[(m)-[:IN_GENRE]->(g) | g.name][0] as genre\n        LIMIT $rows\n        \"\"\"\n\ndef read_data(query, rows=1):\n    with neo4j.GraphDatabase.driver(URI, auth=AUTH) as driver:\n        records, summary, keys = driver.execute_query(query, {\"rows\":rows})\n        return pd.DataFrame(records, columns=keys)\n\n\nNext, I wanted to control the number of rows to load with an st.number_input field, but oddly, it was allowing fractional numbers. Nothing I couldn't fix, right? üôÇ\n\nMichael\n\nHow can I make a st.number_input take integer values only?\n\nChatGPT\n\nst.number_input in Streamlit allows you to create an input widget for numbers. By default, it supports both integer and float values. To make the st.number_input accept integer values only, you need to set the step parameter to 1. This ensures that the input increments/decrements by an integer value.\n\nHere's an example of a st.number_input that accepts only integer values:\n\nimport streamlit as st\n\n# Using st.number_input with step=1 for integer values only\ninteger_input = st.number_input(\"Enter an integer:\", min_value=0, max_value=100, value=50, step=1)\n\nst.write(\"You entered:\", integer_input)\n\n\nIn this example, the st.number_input is set to accept integer values ranging from 0 to 100, with a default value of 50. The step parameter is set to 1, ensuring that the input field only accepts integer values.\n\nCool, I could work with that. Now my data was coming from my database, neat.\n\nImplementing caching to reduce data reloads\n\nBut I didn't want the data to reload on every refresh. I remembered seeing something about caching with Streamlit on Google. So, I asked ChatGPT about it.\n\nMichael\n\nCan you explain how to use st.cached on a database query function?\n\nChatGPT\n\nst.cache is a Streamlit decorator that caches the results of a function, which can be helpful when you have expensive computations or database queries. When the function is called again with the same input arguments, the cached result is returned instead of recomputing the function. This can significantly speed up your Streamlit app.\n\nTo use st.cache with a database query function, you need to decorate the function with @st.cache. Here's an example:\n\nFirst, let's assume you have a function to query data from a database.\n\nNow, add the @st.cache decorator to the function:\n\nimport sqlite3\nimport pandas as pd\nimport streamlit as st\n\n@st.cache  # Add the decorator here\ndef query_data_from_db(query: str, db_path: str) -> pd.DataFrame:\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nBy adding the @st.cache decorator, Streamlit will cache the result of the query_data_from_db() function, so subsequent calls with the same query and db_path arguments will return the cached result instead of querying the database again.\n\nKeep in mind that st.cache works best with functions that have deterministic outputs (i.e., the output only depends on the input arguments). If your database contents change frequently or your query results may vary for other reasons, you may need to use the ttl (time-to-live) parameter to specify an expiration time for the cache.\n\nFor example, if you want the cache to expire after 300 seconds (5 minutes), you can modify the decorator like this:\n\n@st.cache(ttl=300)  # Cache results for 300 seconds\ndef query_data_from_db(query: str, db_path: str) -> pd.DataFrame:\n    # ...\n\n\nWith the ttl parameter, the cached result will be used for 300 seconds. After this time, the next call to the function will recompute the result and update the cache.\n\nLearning about updated APIs from a documentation drop\n\nOkay, that didn't work. Streamlit complained about st.cache. It was another case of the model training cutoff date. (Remember that this session was before the web-browsing extension was available.) So, I found the relevant doc and pasted it for ChatGPT to re-evaluate its advice.\n\nMichael\n\nst.cache is deprecated. Here are the new docs on how our example would change.\n\nAdvanced usage Controlling cache size and duration If your app runs for a long time and constantly caches functions, you might run into two problems:\n\nThe app runs out of memory because the cache is too large. Objects in the cache become stale, e.g., because you cached old data from a database. You can combat these problems with the TTL and max_entries parameters, available for both caching decorators.\n\nThe TTL (time-to-live) parameter\n\nttl sets a time to live on a cached function. If that time is up and you call the function again, the app will discard any old, cached values, and the function will be rerun. The newly computed value will then be stored in the cache. This behavior is useful for preventing stale data (problem 2) and the cache from growing too large (problem 1). Especially when pulling data from a database or API, you should always set a ttl so you are not using old data. Here's an example:\n\n@st.cache_data(ttl=3600) ¬†# üëà Cache data for 1 hour (=3600 seconds)\ndef get_api_data():\n‚ÄÉdata = API.get(...)\n‚ÄÉreturn data\nCopy\nstar\nTip\nYou can also set TTL values using timedelta, e.g. ttl=datetime.timedelta(hours=1).\n\nChatGPT\n\nBased on the new documentation, you can replace st.cache with st.cache_data and use the ttl and max_entries parameters for controlling cache size and duration. Here's how your example would change:\n\nimport sqlite3\nimport pandas as pd\nimport streamlit as st\n\n@st.cache_data(ttl=300, max_entries=100)  # üëà Updated decorator with ttl and max_entries parameters\ndef query_data_from_db(query: str, db_path: str) -> pd.DataFrame:\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nIn this example, the @st.cache_data decorator replaces the deprecated @st.cache decorator. The ttl parameter is set to 300 seconds (5 minutes) to specify the cache's expiration time and max_entries is set to 100 to limit the maximum number of entries in the cache.\n\nAdding \"real\" data science‚Äîpredictions with scikit-learn\n\nIn the second part of the live stream, I wanted to integrate scikit-learn to predict one feature of my movies dataset.\n\nMichael\n\nNow we want to try a prediction with scikit learn. How can I predict the movie budget from genre, year, and IMDb rating? What model should I use, how to do train/test split, and use the model for some sample predictions? Give me the minimal code with an explanation in the comments for a standalone Python file.\n\nChatGPT\n\nTo predict the movie budget from genre, year, and IMDB rating, you can use a regression model from scikit-learn. In this example, I'll use the RandomForestRegressor. The random forest is an ensemble learning method for classification and regression tasks that operate by constructing multiple decision trees at training time and outputting the mean prediction of the individual trees.\n\nHere's a minimal standalone Python script that demonstrates how to train a RandomForestRegressor model split the dataset into training and testing sets, and make sample predictions:\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Assuming you have a dataset in a CSV file named 'movies_data.csv'\ndata = pd.read_csv(\"movies_data.csv\")\n\n# Preprocess the 'genre' column using one-hot encoding\nencoder = OneHotEncoder()\ngenres_encoded = encoder.fit_transform(data[[\"genre\"]]).toarray()\n\n# Create a DataFrame from the one-hot encoded genres and set column names\ngenres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names([\"genre\"]))\n\n# Merge the one-hot encoded genres back into the original DataFrame\ndata = data.join(genres_encoded_df).drop(\"genre\", axis=1)\n\n# Define the features (X) and target (y)\nX = data.drop(\"budget\", axis=1)\ny = data[\"budget\"]\n\n# Split the data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the RandomForestRegressor model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model using the training data\nmodel.fit(X_train, y_train)\n\n# Predict the movie budgets for the test set\ny_pred = model.predict(X_test)\n\n# Calculate the mean squared error\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean squared error:\", mse)\n\n# Make sample predictions\nsample_data = [\n    {\"year\": 2022, \"imdb_rating\": 7.5, \"genre\": \"Action\"},\n    {\"year\": 1995, \"imdb_rating\": 8.2, \"genre\": \"Drama\"},\n]\nsample_df = pd.DataFrame(sample_data)\n\n# Preprocess the sample data (one-hot encoding for genre)\nsample_genres_encoded = encoder.transform(sample_df[[\"genre\"]]).toarray()\nsample_genres_encoded_df = pd.DataFrame(sample_genres_encoded, columns=encoder.get_feature_names([\"genre\"]))\nsample_df = sample_df.join(sample_genres_encoded_df).drop(\"genre\", axis=1)\n\n# Predict the movie budgets for the sample data\nsample_predictions = model.predict(sample_df)\nprint(\"Sample predictions:\", sample_predictions)\n\n\nIn this script:\n\nThe dataset is loaded from a CSV file named movies_data.csv.\nThe 'genre' column is preprocessed using one-hot encoding, which is necessary because scikit-learn models require numerical input.\nThe features (X) and target (y) are defined. In this case, the features are the genre, year, and IMDB rating columns; the target is the budget column.\nThe dataset is split into training (80%), and testing (20%) sets using train_test_split.\nA RandomForestRegressor model is initialized and trained using the training data.\nThe model is used to predict the movie budgets for the test set, and the mean squared error (MSE) is calculated to evaluate the model's performance.\nResolving scikit-learn errors\n\nThat was a comprehensive explanation, especially with the inline comments. I was confident that I understood most of it. However, when I tried it out, I got an error message. üò±\n\nMichael\n\nHow can we fix this?\n\nFile \"/Users/mh/d/chatgpt-coding/streamlit-eda-ml/test.py\", line 45, in <module> ‚ÄÉgenres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names([\"genre\"]))\n\nChatGPT\n\nApologies for the confusion. In scikit-learn version 0.22 and later, the method to retrieve feature names has changed from get_feature_names to get_feature_names_out. If you are using scikit-learn 0.22 or later, please update the line as follows:\n\ngenres_encoded_df = pd.DataFrame(genres_encoded, columns=encoder.get_feature_names_out([\"genre\"]))\n\n\nThis should resolve the issue. If you are still encountering errors, please let me know.\n\nIf our input data has many more genres than our prediction test, the one hot encoder generates fewer columns and complains that the feature names should match those passed during the fit.\n\nI did not like that the proposal created multiple encoders for both the training and testing data. I wanted to simplify the code.\n\nMichael\n\nHow can we reuse the genre encoder from the training data?\n\nChatGPT\n\nTo reuse the genre encoder from the training data, you should first fit the encoder on the training data only. After fitting the encoder on the training data, you can transform both the training and test data (or any new data) using the same encoder instance. This will ensure that the one-hot encoded columns generated for the test data (or new data) are consistent with the training data.\n\nHere's an example of how to do this:\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Assuming you have a dataset in a CSV file named 'movies_data.csv'\ndata = pd.read_csv(\"movies_data.csv\")\n\n# Define the features (X) and target (y)\nX = data.drop(\"budget\", axis=1)\ny = data[\"budget\"]\n\n# Split the data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Preprocess the 'genre' column using one-hot encoding\nencoder = OneHotEncoder()\nencoder.fit(X_train[[\"genre\"]])  # Fit the encoder on the training data\n\n# Transform the 'genre' column for both training and test data\nX_train_genres_encoded = encoder.transform(X_train[[\"genre\"]]).toarray()\nX_test_genres_encoded = encoder.transform(X_test[[\"genre\"]]).toarray()\n\n# Create DataFrames from the one-hot encoded genres and set column names\nX_train_genres_encoded_df = pd.DataFrame(X_train_genres_encoded, columns=encoder.get_feature_names_out([\"genre\"]))\nX_test_genres_encoded_df = pd.DataFrame(X_test_genres_encoded, columns=encoder.get_feature_names_out([\"genre\"]))\n\n# Merge the one-hot encoded genres back into the original DataFrames\nX_train = X_train.join(X_train_genres_encoded_df).drop(\"genre\", axis=1)\nX_test = X_test.join(X_test_genres_encoded_df).drop(\"genre\", axis=1)\n\n# Initialize the RandomForestRegressor model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model using the training data\nmodel.fit(X_train, y_train)\n\n# Predict the movie budgets for the test set\ny_pred = model.predict(X_test)\n\n# Calculate the mean squared error\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean squared error:\", mse)\n\n\nIn this example, the OneHotEncoder is fit on the training data (X_train) only. Then, it is used to transform both the training and test data (X_train and X_test). This ensures that the one-hot encoded columns generated for the test data are consistent with the training data, and you won't run into issues with mismatched feature names.\n\nAddressing column order consistency\n\nI ran into an issue when I tried to reuse the code. The column order in the test/train and prediction data frames didn't match. Oddly, ChatGPT didn't care about column names, which should've been sufficient to map the different column orders.\n\nMichael\n\nIs the order of columns important for the RandomForestRegressor?\n\nChatGPT\n\nYes, the order of columns is important for the RandomForestRegressor and, in general, for all scikit-learn models. When you train the model, it learns the relationship between the input features (columns) and the target variable. The trained model expects the same order of columns in the input data when making predictions as it saw during training.\n\nIf the order of columns in the input data is changed when making predictions, the model's predictions may be incorrect or completely off. So, it's crucial to maintain the same order of columns when making predictions as during the training process.\n\nIn the examples I provided earlier, the order of columns is maintained consistently for both training and test data. When using the OneHotEncoder, the column order is preserved by fitting the encoder on the training data only and then transforming both the training and test data using the same encoder instance.\n\nMichael\n\nDoesn't it use the column headers?\n\nChatGPT\n\nScikit-learn models do not use column headers (i.e., DataFrame column names) when fitting or making predictions. Instead, they operate on the underlying NumPy arrays or other data structures that store the actual data. This is why the order of columns is important when training and making predictions using scikit-learn models. The models don't \"know\" the column names; they only work with the positions of the features in the input data.\n\nTo ensure that the input data has the same order of columns when training and making predictions, you should preprocess your data consistently and maintain the same order of columns throughout your analysis. Using the same OneHotEncoder instance to transform both the training and test data, as shown in the previous examples, ensures that the columns are in the same order.\n\nFixing the billion squared error üò¶\n\nUpon testing, something was off. The fix resulted in a GIGANTIC mean square error (MSE). üö®\n\nMichael\n\nIsn't this a bit high? Mean squared error: 505365615111096.06\n\nChatGPT\n\nYes, the mean squared error (MSE) you mentioned is indeed quite high. It could be due to several reasons, such as:\n\nInsufficient or noisy data: The dataset might not have enough data points or might contain a lot of noise, which could make it challenging to learn the underlying relationship between the features and the target variable.\nModel complexity: The RandomForestRegressor might not be the best model for this problem, or its hyperparameters might not be optimal. You could try different models, such as linear regression, support vector regression, or gradient boosting regression, or experiment with tuning the hyperparameters of the random forest model.\nFeature engineering: The features used for prediction might not be sufficient or might require additional preprocessing or transformation. You could try adding more relevant features or apply different feature engineering techniques, such as feature scaling or transformation, to improve the model's performance.\nOutliers: The dataset might contain extreme values or outliers that could negatively impact the model's performance. You could try removing or transforming outliers before training the model.\n\nTo improve the model's performance, you can try the following:\n\nInvestigate the data distribution and perform exploratory data analysis (EDA) to identify any data issues or patterns that might help in feature engineering or selecting a better model.\nTry different models and compare their performances using cross-validation. This will help you select a model that generalizes well to unseen data.\nPerform hyperparameter tuning for the selected model to find the best combination of hyperparameters that minimize the MSE.\nExperiment with different feature engineering techniques to improve the model's performance.\n\nRemember that every dataset and problem is different, and finding the best approach often involves trial and error.\n\nMy assumption that the Genre would be a good predictor of the budget was incorrect. As I discovered while using the EDA app to plot the budget against years and genres (reference the 1:30:00 mark on the video recording), the year was a far more reliable predictor.\n\nIncorporating predictions into the app\n\nNext, it was time to integrate this awesome prediction engine into the Streamlit app. Although I wasn't sure how it's typically done, I trusted that ChatGPT would make a sensible suggestion. It didn't matter where‚Ä¶as long as it worked. üòÇü§∑‚Äç‚ôÇÔ∏è\n\nMichael\n\nCan I add multiple screens or tabs to the Streamlit app?\n\nChatGPT\n\nYes, you can add multiple screens or tabs to a Streamlit app using the st.sidebar.selectbox or st.sidebar.radio widget. This allows users to switch between different views or functionalities within the same app.\n\nHere's an example of how to create a multi-page Streamlit app using st.sidebar.selectbox:\n\nimport streamlit as st\n\n# Define the pages in your app\npages = {\n    \"Page 1\": page1,\n    \"Page 2\": page2,\n    \"Page 3\": page3,\n}\n\n# Function definitions for each page\ndef page1():\n    st.title(\"Page 1\")\n    st.write(\"This is Page 1 of the multi-page app.\")\n\ndef page2():\n    st.title(\"Page 2\")\n    st.write(\"This is Page 2 of the multi-page app.\")\n\ndef page3():\n    st.title(\"Page 3\")\n    st.write(\"This is Page 3 of the multi-page app.\")\n\n# Create a sidebar with a selectbox to choose the page\nselected_page = st.sidebar.selectbox(\"Choose a page\", options=list(pages.keys()))\n\n# Call the corresponding function to display the selected page\npages[selected_page]()\n\n\nThe user can select a page using the widget in the sidebar, and the corresponding function will be called to display the content of the selected page. You can add more pages by defining new functions and adding them to the pages dictionary.\n\nüí°\nOkay, that worked, but as I learned later, that's not the recommended way of building multi-page Streamlit apps, which use separate Python files, one for each page. You can read more about this in the docs for multi-page apps.\n\nü§¶‚Äç‚ôÇÔ∏è Something went wrong after I reincorporated the database data into my revamped Streamlit app! Seems I made a mistake.\n\nMichael\n\ndata = data[\"rating\",\"year\",\"budget\"]\n\nWhat's wrong here?\n\nKeyError: ('rating', 'year', 'budget')\nTraceback:\nFile \"/opt/homebrew/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 565, in _run_script\n    exec(code, module.__dict__)\nFile \"/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py\", line 160, in <module>\n    main()\nFile \"/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py\", line 157, in main\n    pages[selected_page](data)\nFile \"/Users/mh/d/chatgpt-coding/streamlit-eda-ml/app.py\", line 52, in predict\n    data = data[\"rating\",\"year\",\"budget\"]\n           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\n\nChatGPT\n\nThe issue is with the way you're trying to select multiple columns from the DataFrame. Instead of using data[\"rating\", \"year\", \"budget\"], you should use double brackets ([[]]) to select multiple columns.\n\nHere's the correct way to do it:\n\ndata = data[[\"rating\", \"year\", \"budget\"]]\n\n\nBy using double brackets, you're creating a new DataFrame with just the specified columns. The single brackets would try to access a single column with the given tuple as a key, which doesn't exist in the DataFrame, resulting in the KeyError.\n\nIt turned out I wasn't just a Streamlit noob, I was also one regarding DataFrame. That's the point of this live stream, right? üòÇ\n\nAdding selection by name\n\nNext, I wanted to preselect a logical value in the select boxes in the sidebar, but I couldn't seem to do it by column name. ChatGPT to the rescue! Again!\n\nMichael\n\nHow can I set a default value for st.selectbox?\n\nChatGPT\n\nTo set a default value for st.selectbox in Streamlit, you can use the index parameter. The index parameter sets the default selected option based on its position (0-indexed) in the provided list of options.\n\nHere's an example:\n\nimport streamlit as st\n\noptions = [\"Option A\", \"Option B\", \"Option C\"]\ndefault_index = options.index(\"Option B\")\n\nselected_option = st.selectbox(\"Choose an option\", options=options, index=default_index)\n\nst.write(f\"You selected {selected_option}\")\n\n\nIn this example, the st.selectbox widget will have \"Option B\" as its default value. The index parameter is set to the position of \"Option B\" in the options list using the options.index(\"Option B\") method.\n\nHmm, so I needed an index. What's the column index of a named column in a DataFrame?\n\nMichael\n\nHow can I get the index of a value out of a pandas Index type?\n\nChatGPT\n\nYou can use the get_loc() method of the pandas Index object to find the index of a value. Here's an example:\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\ndf = pd.DataFrame(data)\n\n# Get the column index object\ncolumn_index = df.columns\n\n# Find the index of a value in the Index object\nvalue_to_find = 'B'\nindex_of_value = column_index.get_loc(value_to_find)\n\nprint(f\"Index of '{value_to_find}': {index_of_value}\")\n\n\nIn this example, we create a sample DataFrame df and get its column index object column_index. We then use the get_loc() method to find the index of the value 'B' in the column index. The output will be:\n\nIndex of 'B': 1\n\nImplementing dataframe filters\n\nFinally, I needed to sub-select data from the DataFrame for predictions. I knew I had to filter the data but didn't know the exact syntax.\n\nMichael\n\nHow do I select rows of a DataFrame with certain values in certain columns?\n\nChatGPT\n\nYou can select rows of a DataFrame with certain values in specific columns by using boolean indexing. This allows you to filter the DataFrame based on conditions you specify for one or multiple columns.\n\nHere's an example:\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Carol\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"San Francisco\", \"New York\"],\n}\ndf = pd.DataFrame(data)\n\n# Filter the DataFrame based on conditions\nfiltered_df = df[(df[\"Age\"] >= 30) & (df[\"City\"] == \"New York\")]\n\nprint(filtered_df)\n\n\nIn this example, we create a sample DataFrame df and filter it based on the conditions that the \"Age\" column should be greater than or equal to 30 and the \"City\" column should be \"New York\". The filtered DataFrame will contain only the rows that meet both conditions:\n\n    Name  Age      City\n3  David   40  New York\n\n\nYou can combine multiple conditions using the & (and) or | (or) operators, and use parentheses to group conditions as needed. Note that you should use the bitwise operators & and | instead of the logical operators and and or.\n\nWith that, the two-hour session came to an end. I was quite pleased with what I had learned and accomplished.\n\nI started with no knowledge of Streamlit or scikit-learn. Within two hours, I had a functional app that pulled and displayed my own data and made basic predictions based on the columns I selected.\n\nWrapping up\n\nThat's a wrap on this coding adventure! Here's the finished code. I must say, I prefer coding with ChatGPT over GitHub CoPilot. It's more conversational and offers detailed explanations. What about you?\n\nYou can find me on Mastodon, BlueSky, Twitter, and Medium. I'd love to hear about your own pair programming experiences with LLMs.\n\nOh, and if you're building cool Streamlit (or other) apps with Neo4j, make sure to submit a talk or join us for our online developer conference \"NODES\" on October 26th. CfP is open till June 30th!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Editable dataframes are here! ‚úçÔ∏è",
    "url": "https://blog.streamlit.io/editable-dataframes-are-here/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nBy Lukas Masuch, William Huang and Johannes Rieke\nPosted in Product, February 28 2023\nHow to use it\nAdvanced features\nNew docs page\nExamples\nNext up\nContents\nShare this post\n‚Üê All posts\n\nWorking with dataframes is at the heart of data science. If you‚Äôre like us, you probably load data from a CSV or a database, transform it in Pandas, fix it, transform it again, fix it again‚Ä¶and so on, until you‚Äôre happy. st.dataframe lets you visualize data instantly, but sometimes it‚Äôs not enough. You want to interact with it, not just look at a table!\n\nHence, we plan to release some major improvements for working with dataframes in the next few months. Today, we‚Äôre excited to launch‚Ä¶\n\nEditable dataframes! üéâ\n\nHow to use it\n\nEditable dataframes are supported via a new command, st.experimental_data_editor. It shows the dataframe in a table, similar to st.dataframe. But in contrast to st.dataframe, this table isn‚Äôt static. The user can click on cells and edit them. The edited data is then returned on the Python side.\n\nHere‚Äôs an example:\n\nedited_df = st.experimental_data_editor(df)\nfavorite_command = edited_df.loc[edited_df[\"rating\"].idxmax()][\"command\"]\nst.markdown(f\"Your favorite command is **{favorite_command}** üéà\")\n\n\nTry it out by double-clicking on a cell. üëÜ\n\n‚ö†Ô∏è\nThis feature is experimental, i.e. it might change at any time. We plan to de-experimentalize it in the next few months. See here for details.\nAdvanced features\nAdding and deleting rows. Just set the parameter num_rows=‚Äùdynamic‚Äù, and users can add rows to the table or delete existing rows:\n\nCopy-and-paste support that‚Äôs compatible with Google Sheets, Excel, and others:\n\nBulk-editing by dragging the handle on a cell (similar to Excel):\n\nEasy access to edited data. No need to compare the old and new dataframe to get the difference. Just use st.experimental_data_editor together with session state to access all edits, additions, and deletions.\nSupport for additional data types. Let your users edit lists, tuples, sets, dictionaries, NumPy arrays, or Snowpark and PySpark dataframes. Most types are returned in their original format.\nAutomatic input validation, e.g., number cells don‚Äôt allow characters.\nRich editing experience, e.g., checkboxes for booleans and dropdowns for categorical data. The date picker for datetime cells is coming soon!\nNew docs page\n\nTo support this release, we created a brand-new docs page on dataframes. It explains everything you need to know about st.dataframe and the new st.experimental_data_editor, including all of the sweet features you saw above. The best part is, it comes with lots of interactive examples! üïπÔ∏è\n\nAnd of course, we also added st.experimental_data_editor to our API reference. Check out all of its parameters here.\n\nExamples\n\nExcited to jump in? Check out our demo app. It shows examples of using the data editor in practice. From guessing idioms to matrix operations over custom convolution filters, you can do a lot with this new feature.\n\nNext up\n\nEditable dataframes are only the beginning! üå±\n\nWe have a bunch of new features for st.dataframe and st.experimental_data_editor in the pipeline for the next few months: showing images, clickable URLs in tables, letting the users select rows, and more. You can always follow our progress on roadmap.streamlit.app!\n\nWe‚Äôre excited to see what you build. Let us know your feedback in the comments below.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "LangChain tutorial #1: Build an LLM-powered app in 18 lines of code",
    "url": "https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain tutorial #1: Build an LLM-powered app in 18 lines of code\n\nA step-by-step guide using OpenAI, LangChain, and Streamlit\n\nBy Chanin Nantasenamat\nPosted in Tutorials, May 31 2023\nWhat is LangChain?\nStep 1. Get an OpenAI API key\nStep 2. Set up the coding environment\nLocal development\nCloud development\nStep 3. Build the app\n4. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn the dynamic landscape of artificial intelligence (AI), generative AI and large language models (LLMs) have emerged as game-changers, revolutionizing how we process and understand massive amounts of text data. You can use LLMs for text generation, sentiment analysis, question answering, text summarization, document translation, document classification, and much more.\n\nIf you're captivated by the transformative powers of generative AI and LLMs, then this LangChain how-to tutorial series is for you. As it progresses, it‚Äôll tackle increasingly complex topics.\n\nIn this first part, I‚Äôll introduce the overarching concept of LangChain and help you build a very simple LLM-powered Streamlit app in four steps:\n\nGet an OpenAI API key\nSet up the coding environment\nBuild the app\nDeploy the app\n\n\nü¶ú\nIf you‚Äôd rather play with it right away, here is the app and the code (the app is using a placeholder API key). And read our quickstart tutorial here to get started!\n\nBut first, let's take a deeper look at LangChain.\n\nWhat is LangChain?\n\nLangChain is a framework that uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date, it has accumulated 41,900 stars on GitHub and has over 800 contributors.\n\nAt a high level, LangChain connects LLM models (such as OpenAI and HuggingFace Hub) to external sources like Google, Wikipedia, Notion, and Wolfram. It provides abstractions (chains and agents) and tools (prompt templates, memory, document loaders, output parsers) to interface between text input and output. LLM models and components are linked into a pipeline \"chain,\" making it easy for developers to rapidly prototype robust applications. Simply put, Langchain orchestrates the LLM pipeline.\n\nLangChain's power lies in its six key modules:\n\nModel I/O: Facilitates the interface of model input (prompts) with the LLM model (closed or open-source) to produce the model output (output parsers)\nData connection: Enables user data to be loaded (document loaders), transformed (document transformers), stored (text embedding models and vector stores) and queried (retrievers)\nMemory: Confer chains or agents with the capacity for short-term and long-term memory so that it remembers previous interactions with the user\nChains: A way to combine several components or other chains in a single pipeline (or ‚Äúchain‚Äù)\nAgents: Depending on the input, the agent decides on a course of action to take with the available tools/data that it has access to\nCallbacks: Functions that are triggered to perform at specific points during the duration of an LLM run\n\n\nüìñ\nCheck out the LangChain documentation for further information on each of these modules.\n\nNow that you're familiar with LangChain, let's look at the app's functionality.\n\nCopy/paste your API key (see how to get it in Step 1 below), then type in \"What are the three key pieces of advice for learning how to code?\" and click Submit:\n\nYou should see the response appear in the blue box:\n\nThis is how it works under the hood:\n\nEssentially, you'll be using OpenAI (the LLM), LangChain (the LLM framework), and Streamlit (the web framework).\n\nLet's get started!\n\nStep 1. Get an OpenAI API key\n\nFirst, get your own OpenAI API key:\n\nGo to https://platform.openai.com/account/api-keys.\nClick on the + Create new secret key button.\nEnter an identifier name (optional) and click on the Create secret key button.\nCopy the API key to be used in this tutorial (the key shown below was already revoked):\nStep 2. Set up the coding environment\nLocal development\n\nTo set up a coding environment locally, make sure that you have a functional Python environment (e.g. Python >3.7) and install the following three Python libraries:\n\npip install streamlit openai langchain \n\nCloud development\n\nYou can also code directly on the Streamlit Community Cloud. Just use the Streamlit app template (read this blog post to get started).\n\nNext, add the three prerequisite Python libraries in the requirements.txt file:\n\nstreamlit\nopenai\nlangchain\n\nStep 3. Build the app\n\nNow comes the fun part. Let's build the app!\n\nIt's only 18 lines of code:\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\n\nst.title('ü¶úüîó Quickstart App')\n\nopenai_api_key = st.sidebar.text_input('OpenAI API Key')\n\ndef generate_response(input_text):\n  llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n  st.info(llm(input_text))\n\nwith st.form('my_form'):\n  text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')\n  submitted = st.form_submit_button('Submit')\n  if not openai_api_key.startswith('sk-'):\n    st.warning('Please enter your OpenAI API key!', icon='‚ö†')\n  if submitted and openai_api_key.startswith('sk-'):\n    generate_response(text)\n\nTo start, create the streamlit_app.py file and import the two prerequisite libraries:\n\nstreamlit, a low-code framework used for the front end to let users interact with the app.\nlangchain, a framework for working with LLM models.\nimport streamlit as st\nfrom langchain.llms import OpenAI\n\n\nNext, display the app's title \"ü¶úüîó Quickstart App\" using the st.title() method:\n\nst.title('ü¶úüîó Quickstart App')\n\n\nThe app takes in the OpenAI API key from the user, which it then uses togenerate the responsen.\n\nopenai_api_key = st.sidebar.text_input('OpenAI API Key')\n\nNext, define a custom function called generate_response(). It takes a piece of text as input, uses the OpenAI() method to generate AI-generated content, and displays the text output inside a blue box using st.info():\n\ndef generate_response(input_text):\n  llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n  st.info(llm(input_text))\n\n\nFinally, use st.form() to create a text box (st.text_area()) for accepting user-provided prompt input. Once the user clicks the Submit button, the generate-response() function is called with the prompt input variable (text) as an argument.\n\nThis creates AI-generated content:\n\nwith st.form('my_form'):\n  text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')\n  submitted = st.form_submit_button('Submit')\n  if not openai_api_key.startswith('sk-'):\n    st.warning('Please enter your OpenAI API key!', icon='‚ö†')\n  if submitted and openai_api_key.startswith('sk-'):\n    generate_response(text)\n4. Deploy the app\n\nDeploying the app is super simple:\n\nCreate a GitHub repository for the app.\nIn Streamlit Community Cloud, click the New app button, then specify the repository, branch, and main file path.\nClick the Deploy! button.\n\nAnd here it is!\n\nWrapping up\n\nNow you know how to get your own OpenAI API key, set up your coding environment, create your first LLM-powered app with LangChain and Streamlit, and deploy it to the cloud. Check out the LLM gallery for inspiration and share your creation with the community. I can't wait to see what you'll build!\n\nIn future posts, I'll show you the superpowers of other LangChain modules (e.g., prompt templates, memory, indexes, chains, agents, and callbacks). If you have any questions, please post them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy app-building! ü¶úüîó\n\nP.S. This post was made possible thanks to the technical review by Joshua Carroll and editing by Ksenia Anske.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "LangChain tutorial #2: Build a blog outline generator app in 25 lines of code",
    "url": "https://blog.streamlit.io/langchain-tutorial-2-build-a-blog-outline-generator-app-in-25-lines-of-code/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain tutorial #2: Build a blog outline generator app in 25 lines of code\n\nA guide on conquering writer‚Äôs block with a Streamlit app\n\nBy Chanin Nantasenamat\nPosted in LLMs, June 7 2023\nWhat's a prompt template?\nPrompt template in action\nApp overview\nStep 1. Get an OpenAI API key\nStep 2. Set up the coding environment\nLocal development\nCloud development\nStep 3. Build the app\nStep 4. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nIn LangChain tutorial #1, you learned about LangChain modules and built a simple LLM-powered app. The app took input from a text box and passed it to the LLM (from OpenAI) to generate a response. But it didn‚Äôt leverage other LangChain modules.\n\nIn this post, I‚Äôll show you how to use LangChain‚Äôs prompt template and build a blog outline generator app in four simple steps:\n\nGet an OpenAI API key\nSet up the coding environment\nBuild the app\nDeploy the app\n\n\nü¶ú\nWant to skip reading? Here's the demo app and the repo code.\n\nBefore we get to building the app, let's talk about‚Ä¶\n\nWhat's a prompt template?\n\nA prompt is an instruction given to an LLM. You can use a prompt template to generate prompts on-the-fly. It has two parts:\n\nA static descriptive text part (hard-coded in the code).\nA dynamically generated part (determined by the user).\n\nUse it to pre-define the app's scope so it performs a specific task as a function of user instructions (e.g. generates prompts in a reproducible manner).\n\nHere is an example of a prompt template:\n\n'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'\n\nThe text As an experienced data scientist and technical writer, generate an outline for a blog about is the static component, and the {topic} is the dynamic component determined by the user (e.g. a blog topic they're interested in).\n\nPrompt template in action\n\nTo implement the prompt template, use LangChain's PromptTemplate() function.\n\nIt accepts the following input arguments:\n\ninput_variables allows you to accept user-provided values comprising the prompt template's dynamic component.\ntemplate is the static component of the prompt template.\n\nHere is what it'll look like:\n\ntemplate = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'\nprompt = PromptTemplate(input_variables=['topic'], template=template)\n\n\n\nüìñ\nCheck out the LangChain documentation on prompt template for further information.\nApp overview\n\nThe app solves one of the most time-consuming problems for technical writers...\n\nWriter's block! ‚úçÔ∏è\n\nIt works like this:\n\nIts workflow consists of three simple steps:\n\nUse Streamlit's st.text_input() function to accept the user-provided \"Topic\" as input.\nCombine the \"topic\" and the prompt instructions using PromptTemplate() to create the final prompt.\nUse the final prompt to generate a response.\n\nGo ahead and try it:\n\nHere is what it should look like:\n\nNow, let's build the app!\n\nStep 1. Get an OpenAI API key\n\nHop over to the LangChain tutorial #1 for instructions on how to get an OpenAI API key.\n\nStep 2. Set up the coding environment\nLocal development\n\nTo set up a programming workspace on your own system, install Python version 3.7 or higher. Then install these Python libraries:\n\npip install streamlit openai langchain\n\nCloud development\n\nYou can also create the app on the Streamlit Community Cloud. To get started, use the Streamlit app template (read more here).\n\nNext, include the three prerequisite Python libraries in the requirements.txt file:\n\nstreamlit\nopenai\nlangchain\n\nStep 3. Build the app\n\nThe code for building the app is only 25 lines long (23 without the two comments):\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\n\nst.set_page_config(page_title=\"ü¶úüîó Blog Outline Generator App\")\nst.title('ü¶úüîó Blog Outline Generator App')\nopenai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')\n\ndef generate_response(topic):\n  llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n  # Prompt\n  template = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'\n  prompt = PromptTemplate(input_variables=['topic'], template=template)\n  prompt_query = prompt.format(topic=topic)\n  # Run LLM model and print out response\n  response = llm(prompt_query)\n  return st.info(response)\n\nwith st.form('myform'):\n  topic_text = st.text_input('Enter keyword:', '')\n  submitted = st.form_submit_button('Submit')\n  if not openai_api_key.startswith('sk-'):\n    st.warning('Please enter your OpenAI API key!', icon='‚ö†')\n  if submitted and openai_api_key.startswith('sk-'):\n    generate_response(topic_text)\n\nFirst, create the streamlit_app.py file to house the full code snippet above. Or follow along and add the code block by block.\n\nIn the first few lines, import the necessary Python libraries, such as Streamlit and specific Langchain methods OpenAI and PromptTemplate:\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\n\n\nNext, give the app a page title for display on the browser window and in-app:\n\nst.set_page_config(page_title=\"ü¶úüîó Blog Outline Generator App\")\nst.title('ü¶úüîó Blog Outline Generator App')\n\nA text box is created to accept OpenAI credentials:\n\nopenai_api_key = st.sidebar.text_input('OpenAI API Key')\n\n\nA custom function generates an LLM response based on the user's provided \"topic\" of interest. An instance of the LLM model is created using OpenAI(). This is followed by creating a dynamically created prompt stored in the prompt_query variable. This prompt combines static and dynamic components.\n\nThe LLM model accepts the prompt as input for generating a response‚Äîthe blog outline. It's displayed in a blue box using st.info():\n\ndef generate_response(topic):\n  llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n  # Prompt\n  template = 'As an experienced data scientist and technical writer, generate an outline for a blog about {topic}.'\n  prompt = PromptTemplate(input_variables=['topic'], template=template)\n  prompt_query = prompt.format(topic=topic)\n  # Run LLM model\n  response = llm(prompt_query)\n  return st.info(response)\n\nFinally, st.form() is used as an app logic to generate the blog outline only after correctly entering the OpenAI API key, filling the text box with the blog topic, and clicking the Submit button:\n\nwith st.form('myform'):\n  topic_text = st.text_input('Enter keyword:', '')\n  submitted = st.form_submit_button('Submit')\n  if not openai_api_key.startswith('sk-'):\n    st.warning('Please enter your OpenAI API key!', icon='‚ö†')\n  if submitted and openai_api_key.startswith('sk-'):\n    generate_response(topic_text)\nStep 4. Deploy the app\n\nAfter creating your app, you can deploy it in three steps:\n\nCreate a GitHub repository for your app.\nGo to the Community Cloud, click the New app button, and select the repository, branch, and app file.\nClick the Deploy! button.\n\nAnd voil√†! You're done.\n\nWrapping up\n\nIn this post, you've learned how to use the LLM model with the LangChain prompt template to build a blog outline generator. You can customize your app by adjusting the prompt. Check out the LLM gallery for more ideas and inspiration.\n\nIf you have any questions, please post them in the comments below or contact me on Twitter at @thedataprof or on LinkedIn.\n\nHappy app-building! ü¶úüîó\n\nP.S. This post was made possible thanks to the technical review by Tim Conkling and editing by Ksenia Anske.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "LangChain tutorial #3: Build a Text Summarization app",
    "url": "https://blog.streamlit.io/langchain-tutorial-3-build-a-text-summarization-app/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLangChain tutorial #3: Build a Text Summarization app\n\nExplore the use of the document loader, text splitter, and summarization chain\n\nBy Chanin Nantasenamat\nPosted in LLMs, June 13 2023\nWhat is text summarization?\nApp overview\nStep 1: Get an OpenAI API key\nStep 2: Set up the coding environment\nLocal development\nCloud development\nStep 3. Building the app\nCode overview\nImporting prerequisite libraries\nAI-generated response\nApp logic\nStep 4. Deploy the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nLarge language models (LLMs) are trained on massive amounts of text data using deep learning methods. The resulting model can perform a wide range of natural language processing (NLP) tasks, broadly categorized into seven major use cases: classification, clustering, extraction, generation, rewriting, search, and summarization (read more in Meor Amer posts here and here).\n\nIn the previous LangChain tutorials, you learned about two of the seven utility functions: LLM models and prompt templates. In this tutorial, we‚Äôll explore the use of the document loader, text splitter, and summarization chain to build a text summarization app in four steps:\n\nGet an OpenAI API key\nSet up the coding environment\nBuild the app\nDeploy the app\n\n\nü¶ú\nWant to test it out? Here's the demo app and the repo code.\nWhat is text summarization?\n\nText summarization involves creating a summary of a source text using natural language processing. This is useful for condensing long-form text, audio, or video into a shorter, more digestible form that still conveys the main points. Examples include news articles, scientific papers, podcasts, speeches, lectures, and meeting recordings.\n\nThere are two main types of summarization:\n\nExtractive summarization. This type identifies and extracts key phrases or sentences (i.e., excerpts) from the source text and combines them into a summary. It leaves the original text unchanged and only selects the important parts.\nAbstractive summarization. This type involves understanding the main ideas in the source text and creating a new summary that expresses those ideas in a fresh and condensed way (i.e., paraphrasing). It's more complex because it requires a deeper understanding of the source text and the ability to convey the same information in fewer words.\n\n\nüîó\nCheck out the LangChain documentation on summarization.\nApp overview\n\nAt a high level, the app's workflow is relatively simple:\n\nThe user submits an input text to be summarized into the Streamlit frontend.\nThe app pre-processes the text by splitting it into several chunks, creating documents for each chunk, and applying the summarization chain with the help of the LLM model.\nAfter a few moments, the summarized text is displayed in the app.\n\nLet's take a closer look at the underlying details for when (1) the input text is submitted for processing by the app and (2) an LLM-generated response is created as summarized text.\n\nInput. The input text is first split into several chunks of text using CharacterTextSplitter(), followed by the creation of documents for each text split via Document().\nGenerated Output. The processed text will then serve as input, along with the LLM model, to the load_summarize_chain() function. The text will be transformed into a concise form as summarized text.\n\nHere is the app in action:\n\nTry it for yourself:\n\nNow, let's get to building!\n\nStep 1: Get an OpenAI API key\n\nRead how to obtain an OpenAI API key in LangChain Tutorial #1.\n\nStep 2: Set up the coding environment\nLocal development\n\nTo set up a local coding environment, ensure that you have Python version 3.7 or higher installed, then install the following Python libraries:\n\npip install streamlit langchain openai tiktoken\n\nCloud development\n\nYou can also set up your app on the cloud by deploying to the Streamlit Community Cloud. To get started, use this Streamlit app template (read more about it here).\n\nNext, include the three prerequisite Python libraries in the requirements.txt file:\n\nstreamlit\nlangchain\nopenai\ntiktoken\n\nStep 3. Building the app\nCode overview\n\nBefore diving deeper into the code walkthrough, let's take a high-level overview of the code. It can be implemented in just 38 lines:\n\nimport streamlit as st\nfrom langchain import OpenAI\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains.summarize import load_summarize_chain\n\ndef generate_response(txt):\n    # Instantiate the LLM model\n    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n    # Split text\n    text_splitter = CharacterTextSplitter()\n    texts = text_splitter.split_text(txt)\n    # Create multiple documents\n    docs = [Document(page_content=t) for t in texts]\n    # Text summarization\n    chain = load_summarize_chain(llm, chain_type='map_reduce')\n    return chain.run(docs)\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Text Summarization App')\nst.title('ü¶úüîó Text Summarization App')\n\n# Text input\ntxt_input = st.text_area('Enter your text', '', height=200)\n\n# Form to accept user's text input for summarization\nresult = []\nwith st.form('summarize_form', clear_on_submit=True):\n    openai_api_key = st.text_input('OpenAI API Key', type = 'password', disabled=not txt_input)\n    submitted = st.form_submit_button('Submit')\n    if submitted and openai_api_key.startswith('sk-'):\n        with st.spinner('Calculating...'):\n            response = generate_response(txt_input)\n            result.append(response)\n            del openai_api_key\n\nif len(result):\n    st.info(response)\n\nImporting prerequisite libraries\n\nAs always, we'll start by importing the necessary libraries:\n\nimport streamlit as st\nfrom langchain import OpenAI\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains.summarize import load_summarize_chain\n\n\nAI-generated response\n\nNext, we'll create a custom function generate_response(). It generates a response based on a given text input txt. For our use case, the input text will be the full text that needs to be summarized, and the output text will be the summarized version:\n\ndef generate_response(txt):\n    # Instantiate the LLM model\n    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n    # Split text\n    text_splitter = CharacterTextSplitter()\n    texts = text_splitter.split_text(txt)\n    # Create multiple documents\n    docs = [Document(page_content=t) for t in texts]\n    # Text summarization\n    chain = load_summarize_chain(llm, chain_type='map_reduce')\n    return chain.run(docs)\n\n\nThis function performs four key tasks:\n\nIt instantiates the LLM model, using the LLM model from OpenAI in this example. You can substitute this with any other LLM model of your choice. The API key is defined by the open_api_key parameter, which also takes in the OpenAI API key stored in the openai_api_key variable specified by the user via the st.text_input() method (implemented later in a subsequent code block).\nThe text is then split using the split_text() method from CharacterTextSplitter().\nA document is created for each of the text splits using list comprehension ([Document(page_content=t) for t in texts]).\nThe summarize chain (load_summarize_chain()) is defined and assigned to the chain variable, applied to the documents created above, and stored in the docs variable via the run() method.\nApp logic\n\nLet's define the app elements.\n\nFirst, we'll set the page title using st.set_page_config() and the in-app title using st.title():\n\n# Page title\nst.set_page_config(page_title='ü¶úüîó Text Summarization App')\nst.title('ü¶úüîó Text Summarization App')\n\n\nThe app will accept text input via st.text_area()‚Äîassigned to the txt_input variable.\n\nBefore defining the form, we'll create an empty list called result to later store the AI-generated response.\n\nIn this tutorial, the form is slightly different from previous ones. We clear the API key text box using the clear_on_submit=True parameter after an AI-generated response has been successfully performed.\n\nAfter the user clicks the Submit button, a Calculating... spinner is displayed. Once an AI-generated response is made, the results are stored in the result list created earlier. Then the API key is deleted, and the text output is printed via st.info(response):\n\n# Text input\ntxt_input = st.text_area('Enter your text', '', height=200)\n\n# Form to accept user's text input for summarization\nresult = []\nwith st.form('summarize_form', clear_on_submit=True):\n    openai_api_key = st.text_input('OpenAI API Key', type = 'password', disabled=not txt_input)\n    submitted = st.form_submit_button('Submit')\n    if submitted and openai_api_key.startswith('sk-'):\n        with st.spinner('Calculating...'):\n            response = generate_response(txt_input)\n            result.append(response)\n            del openai_api_key\n\nif len(result):\n    st.info(response)\n\n\nNote that in this tutorial, the app will clear any memory of the API key as an added security measure to prevent lingering memory of the key.\n\nStep 4. Deploy the app\n\nOnce the app is created, it can be deployed in three simple steps:\n\nCreate a GitHub repository for the app.\nGo to Streamlit Community Cloud, click on the New app button, and select the repository, branch, and app file.\nClick on the Deploy! button.\n\nThat's it! Your app will be up and running in no time.\n\nWrapping up\n\nYou learned how to build a text summarization app using LangChain and Streamlit. It involved using Streamlit as the front-end to accept input text, processing it with LangChain and its associated LLM utility functions, and displaying the LLM-generated response.\n\nFor ideas and inspiration, check out the LLM gallery. I can't wait to see what you build. Please let me know in the comments below or contact me on Twitter at @thedataprof or on LinkedIn. You can also find me on the official Streamlit YouTube channel and my personal YouTube channel, Data Professor.\n\nHappy Streamlit-ing! üéà\n\nP.S. This post was made possible thanks to the technical review by Mihal Nowotka and editing by Ksenia Anske.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\n8 tips for securely using API keys\n\nHow to safely navigate the turbulent landscape of LLM-powered apps\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 19 2023\nCreate an animated data story with ipyvizzu and Streamlit\n\nA tutorial on using ipyvizzu and ipyvizzu-story\n\nAdvocate Posts\nby\nPeter Vidos\n,\nApril 20 2023\nAI talks: ChatGPT assistant via Streamlit\n\nCreate your own AI assistant in 5 steps\n\nAdvocate Posts\nby\nDmitry Kosarevsky\n,\nApril 18 2023\nDetecting fake images with a deep-learning tool\n\n7 steps on how to make Deforgify app\n\nAdvocate Posts\nby\nKanak Mittal\n,\nApril 11 2023\nBuilding GPT Lab with Streamlit\n\n12 lessons learned along the way\n\nLLMs\nby\nDave Lin\n,\nApril 6 2023\nBuilding an Instagram hashtag generation app with Streamlit\n\n5 simple steps on how to build it\n\nAdvocate Posts\nby\nWilliam Mattingly\n,\nMarch 29 2023\nHackathon 101: 5 simple tips for beginners\n\nPrepare to win your first hackathon!\n\nTutorials\nby\nChanin Nantasenamat\n,\nMarch 16 2023\nCreate a search engine with Streamlit and Google Sheets\n\nYou‚Äôre sitting on a goldmine of knowledge!\n\nAdvocate Posts\nby\nSebastian Flores Benner\n,\nMarch 14 2023\n10 most common explanations on the Streamlit forum\n\nA guide for Streamlit beginners\n\nAdvocate Posts\nby\nDebbie Matthews\n,\nMarch 9 2023\nBuilding a PivotTable report with Streamlit and AG Grid\n\nHow to build a PivotTable app in 4 simple steps\n\nAdvocate Posts\nby\nPablo Fonseca\n,\nMarch 7 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "st.status: Visualize your app‚Äôs processes",
    "url": "https://blog.streamlit.io/st-status-visualize-your-apps-processes/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nBy Joshua Carroll\nPosted in Product, September 7 2023\nStep-by-step transparency, in real-time\nFlexible interaction to validate results\nWhat's next\nContents\nShare this post\n‚Üê All posts\n‚úÖ\nTL;DR: Replace long app wait times and shed light on the ‚Äúblack box‚Äù of data processing with st.status. Play with our demo app to see how it works.\n\nLong-running apps like LLM agents rarely show you their inner workings out of the box. On top of that, if a response takes too long to generate, users get impatient and leave. Not ideal!\n\nIntroducing: st.status\n\nIf you're not always confident in your model's output, how do you inspect the intermediate steps and chain of thought to verify results? ¬†A few months ago, we provided a targeted solution by integrating with LangChain, using their callback system.\n\nNow you can add st.status to any interactive or API-powered app to:\n\nAnimate its \"under-the-hood\" processes such as API calls or data retrieval.\nSee step-by-step logic to understand what went wrong (or validate what went right).\nAllow users to engage with your app, rather than experiencing a blank page.\n\nSee how it works in our demo app! Choose any of the 8 different animations below to pair with your app operations. Check out the docs for more detail.\n\n\n\n\nLet's look at two examples to see it in action.\n\nStep-by-step transparency, in real-time\n\nWith st.status, every process step is defined, broken out, and animated. The app viewer can expand the status to check the details or leave it collapsed to focus on the final output.\n\nUnlike st.spinner, the intermediate steps remain available to inspect even after the process completes.\n\nhttps://release126.streamlit.app/st.status_demo\nFlexible interaction to validate results\n\nThis can be particularly helpful to validate results from LLM-based apps.\n\nLLMs aren't perfect. Their intelligence relies on the data sets they are trained on, which could be incomplete or contain misinformation. The LLM attempts to generate a plausible response to a user's prompt, but if it reaches the boundaries of its knowledge base, it can take liberties. This phenomenon causes an LLM to \"hallucinate.\" If the user can't quite tell if the model is correct, or is embellishing a result, they quickly lose trust.\n\nWith st.status, the context and intermediate steps are available so users can validate the output logic:\n\nhttps://release126.streamlit.app/LangChain_demo\nWhat's next\n\nThis flexible framework gives you a higher degree of control to up-level your app‚Äôs user experience, and easily integrate custom components. It‚Äôs worth the extra lines of code!\n\nHelp us raise the bar with new (and refined) UI improvements. What additional transparency features would you like to see next?\n\nLet us know in the comments below or on Discord.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing Streamlit to the Polish Python community",
    "url": "https://blog.streamlit.io/introducing-streamlit-to-the-polish-python-community/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing Streamlit to the Polish Python community\n\nMy Streamlit presentation at PyWaW #103\n\nBy Micha≈Ç Nowotka\nPosted in Product, April 4 2023\nWhat motivated me\nHow the translation process works\nMy presentation at PyWaW #103\nWe're hiring!\nContents\nShare this post\n‚Üê All posts\n\nCze≈õƒá mi≈Ço≈õnicy Pythona! Hello, Python lovers! üëã\n\nI'm Micha≈Ç Nowotka, an engineering manager on the Streamlit team at Snowflake. Today I'd like to share with you an amazing adventure of promoting Streamlit to the Polish Python community. It started with creating a Polish translation of the 30 Days of Streamlit challenge and was followed by a talk at PyWaw, a popular Python enthusiast meetup in Warsaw.\n\nWhat motivated me\n\nAlthough English is a truly international language and a lingua franca for all professional fields, such as software development and machine learning, there are still advantages to investing time in translating content into a less widespread language.\n\nI found three reasons to move forward with the Polish translation.\n\nReason 1. Many people still find it faster and more convenient to familiarize themselves with new technologies by reading about them in their native language, allowing them to focus on the technical complexities rather than the linguistic nuances.\n\nReason 2. Many people don't know a foreign language at the beginning of their programming adventure. I teach Python at some of the most popular Polish bootcamps and have experienced this firsthand. This is also true for some Polish kids who are enthusiastic about programming but often aren't fluent in English.\n\nReason 3. The 30 Days of Streamlit challenge has already been translated into seven languages! They include:\n\nBengali\nChinese\nSpanish\nFrench\nHindi\nPortuguese\nRussian\n\nSo, following the words of the famous Polish Renaissance poet Miko≈Çaj Rej‚Ä¶\n\nA niechaj narodowie w≈ºdy postronni znajƒÖ, i≈º Polacy nie gƒôsi, i≈º sw√≥j jƒôzyk majƒÖ. Let it by all and sundry foreign nations be known that Poles speak not Anserine but a tongue of their own.\n\n‚Ä¶I decided that the Polish community couldn't miss this opportunity and should have its own translation.\n\nHow the translation process works\n\nCreating a new language version of the #30DaysOfStreamlit is a multi-step process. Roughly, you need to follow these six steps:\n\nFork the original repository\nTranslate the Markdown files, including the comments and the strings in the code snippets\nTranslate some labels in the app's UI elements\nTranslate additional assets like CSV file headers\nRun the app locally to see if it works, check for any untranslated strings, and do a final proofreading\nPublish the repository and deploy the app\n\nMany computer-assisted translation tools can speed up the process considerably. However, some customization is required to make the content feel natural.\n\nI found the process a little too involved and tedious. I'd love to brainstorm how we can offer better support for translation and internationalization‚Äîlet me know in the comments below. And feel free to translate the 30 Days of Streamlit into your language if you don't see it on the list above!\n\nMy presentation at PyWaW #103\n\nAlthough I have some experience with public speaking, having given a talk on consistent and locality-sensitive hashing techniques at PyWaW #77, I was a little nervous. Especially since I decided to do a live demo instead of using slides. But everything went smoothly, and the presentation generated a lot of interest, so I spent the rest of the evening talking about Streamlit over a beer. üç∫\n\nYou can watch it here:\n\nAnd you can see the demo at stonks.streamlit.app (we chose that name together with the meetup attendees). You can get the code used to build the demo from this gist. My demo was heavily inspired by Chanin Nantasenamat's (@Data Professor) Streamlit video tutorial.\n\nWe're hiring!\n\nThanks for reading my post! I hope you enjoyed it. And if you're located in Warsaw, have 6+ years of software development experience, and are proficient in Python and/or TypeScript, come join my team. I'm looking for a Senior Software Engineer to help take Streamlit to the next level as a Technical Lead in our Snowflake's Warsaw office. You can apply here. üôÇ\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Michael Hunger - Streamlit",
    "url": "https://blog.streamlit.io/author/michael/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Michael Hunger\n1 post\nBuilding a Streamlit and scikit-learn app with ChatGPT\n\nCatching up on coding skills with an AI assistant\n\nLLMs\nby\nMichael Hunger\n,\nJune 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Learn to build your own Notion AI powered chatbot",
    "url": "https://blog.streamlit.io/build-your-own-notion-chatbot/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild your own Notion chatbot\n\nA step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit\n\nBy Logan Vendrix\nPosted in LLMs, September 14 2023\nAn overview of the app\nStep-by-step tutorial\n1. Project structure and initialization\n2. Document ingestion\n3. Query\n4. Chatbot application\nCongratulations! ü•≥\nContents\nShare this post\n‚Üê All posts\nü§ñ\nTL;DR: Learn how to create a chatbot based on Notion content using LangChain, OpenAI, FAISS, and Streamlit to interact with your app in Notion (GitHub repo).\n\nDo you love Notion? Same here. But do you sometimes find it hard to quickly locate the right information in Notion? I faced the same problem at my company (we store literally everything in Notion). So I decided to build a Notion chatbot.\n\nIt was inspired by Harrison Chase‚Äôs app, and attempts to improve it by:\n\nüî™ Using specific markdown characters for better content splitting\nüß† Adding memory to the bot\nüí¨ Using Streamlit as the front-end chat app with the new chat features\nüì£ Embedding the Streamlit app into a Notion page\n\nAdd your OpenAI Key to play with the app!\n\n\n\n\nLet‚Äôs walk through how you can build your own Notion chatbot!\n\nAn overview of the app\n\nStep-by-step tutorial\n1. Project structure and initialization\n\nLet's start by examining the project structure and installing the necessary dependencies. And don‚Äôt forget to get your OpenAI API key and duplicate a public Notion page (to use as a foundation).\n\n1.1 Project structure\n\nThe project structure of notion-chatbot consists of the following:\n\n.streamlit/secrets.toml: stores your OpenAI API key.\nfaiss_index: a FAISS index (vector database) that stores all the vectors.\nnotion_content: a folder containing the Notion content in markdown files.\n.gitignore: ignores your OpenAI API key and Notion content.\napp.py: the script for the Streamlit chat application.\ningest.py: the script used to convert Notion content to vectors and store them in a vector index.\nutils.py: the script used to create a Conversation Retrieval Chain.\nrequirements.txt: a file containing the necessary packages to deploy to Streamlit Community Cloud.\n\nYou‚Äôll create each file step-by-step, so there is no need to create them all at once.\n\nNow, let's initialize the project!\n\n1.2 Project initialization\n\nStart by creating a project folder notion-chatbot\nCreate a new environment and install the required dependencies\n\n\npip install streamlit langchain openai tiktoken faiss-cpu\n\n\n\nCreate the .gitignore file to specify which files not to track\n\n\n# .gitignore\n\nnotion_content/\n.streamlit/\n\n\n\nGo to OpenAI and get your API key\n\n\n\nCreate the folder .streamlit\nIn .streamlit, create the file secrets.toml to store your OpenAI API key as follows\n\n\n>>> secrets.toml\n\nOPENAI_API_KEY = 'sk-A1B2C3D4E5F6G7H8I9J'\n\n\n\nUse Blendle Employee Handbook as your knowledge base\n\nIf you don‚Äôt have a Notion account, create it here. It‚Äôs free!\n\nSelect Duplicate on the top-right corner to duplicate it into your Notion\n\n2. Document ingestion\n\nTo convert all content from your Notion pages into numerical representations (vectors), use LangChain to split the text into smaller chunks that can be processed by OpenAI's embedding model. The model will convert the text chunks into vectors, which you‚Äôll then store in a vector database.\n\n2.1 Export your Notion content\n\nGo to the main Notion page of the Blendle Employee Handbook.\nIn the top right corner, click on the three dots.\nChoose Export\nSelect Markdown and CSV for the Export Format\nSelect Include subpages\nSave the file as notion_content.zip\nUnzip the folder\nPlace the notion_content folder into your notion-chatbot project folder\n\n\nü§ñ\nYou can also get your Notion content using Notion's API. To keep it simple, just export the content manually.\n\nGreat! You should now have all the Notion content as .md files in the notion_content folder within your notion-chatbot project folder.\n\n2.1 Convert Notion content to vectors\n\nTo use the content of your Notion page as the knowledge base of your chatbot, convert all the content into vectors and store them. To do this, use LangChain, an OpenAI embedding model, and FAISS.\n\nOpen your project folder in your favorite IDE and create a new file called ingest.py:\n\n#ingest.py\n\nimport streamlit as st\nimport openai\nfrom langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\n\n# Load OpenAI API key\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n\n# Load the Notion content located in the folder 'notion_content'\nloader = NotionDirectoryLoader(\"notion_content\")\ndocuments = loader.load()\n\n# Split the Notion content into smaller chunks\nmarkdown_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"#\",\"##\", \"###\", \"\\\\n\\\\n\",\"\\\\n\",\".\"],\n    chunk_size=1500,\n    chunk_overlap=100)\ndocs = markdown_splitter.split_documents(documents)\n\n# Initialize OpenAI embedding model\nembeddings = OpenAIEmbeddings()\n\n# Convert all chunks into vectors embeddings using OpenAI embedding model\n# Store all vectors in FAISS index and save to local folder 'faiss_index'\ndb = FAISS.from_documents(docs, embeddings)\ndb.save_local(\"faiss_index\")\n\nprint('Local FAISS index has been successfully saved.')\n\n\nLet's go over the code:\n\nLoad the OpenAI API key stored in .streamlit/secrets.toml.\nLoad the Notion content located in the notion_content folder using NotionDirectoryLoader.\nSplit the content into smaller text chunks using RecursiveCharacterTextSplitter.\n\nThere are different ways to split text.\n\nYour Notion content consists of markdown files with headings (# for H1, ## for H2, ### for H3), so split on those specific characters. This ensures that you split the content at the best place between paragraphs, and not between the sentences of the same paragraph. If the split can't be done on headings, it‚Äôll try to split on the characters \\\\n\\\\n, \\\\n, . that separate sentences. RecursiveCharacterTextSplitter follows the order of the list of characters you provide, meaning it‚Äôll use the next character in the list until the chunks are small enough.\n\nUse a chunk size of 1500 with an overlap of 100 (feel free to experiment with different values):\n\nConvert each text chunk into a vector using the OpenAI embedding model.\nStore all the vectors in a FAISS index.\n\nWith your Notion content now converted to vectors and stored in a vector database, let's explore how you can interact with them!\n\n3. Query\n\nTo find an answer to the user's question, convert it into a vector using the same embedding model as before. This vector is then matched with similar vectors in the vector database created earlier. Pass the relevant text content along with the user's question to OpenAI GPT to create an answer.\n\nTo improve the chat experience, add memory to your chatbot by storing previous messages in a chat history. This allows the chatbot to access the chat history during the conversation.\n\n3.1 Flow\n\nA chat history is created at the start. As the user asks questions or the chatbot provides answers, these messages are stored in the chat history. The chatbot keeps track of previous messages as the conversation progresses, which serves as its memory.\nAs the user writes a question, it‚Äôs saved in the chat history.\nThe question and the chat history combine into a standalone question.\nThe standalone question converts into a vector using the same embedding model as in the Document Ingestion phase.\nThe vector passes to the vector database and performs a similarity search (or vector search).\n\nIn short, you need to find the most similar items to the user's question given a set of vectors (in red) and the query vector (the user's question in blue). In the example below, you can see the k-nearest neighbor search (k-NN), which looks for the three closest vectors to the query vector:\n\nWith the most similar vectors found, link the corresponding Notion content to a stand-alone question for GPT.\nGPT formulates an answer based on the guidelines of a system prompt.\nThe chatbot responds to the user with the answer from GPT.\nAdd the chatbot's answer to the chat history.\nRepeat this process.\n\nMake sense? Nice! Let's start coding.\n\n3.2 Query\n\nFirst, create a LangChain Conversational Retrieval Chain to serve as the brain of your app. To do so, create a utils.py file containing the load_chain() function, which returns a Conversational Retrieval Chain.\n\n# **utils.py**\n\nimport streamlit as st\nimport openai\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import SystemMessagePromptTemplate\n\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n\n@st.cache_resource\ndef load_chain():\n\t\t\"\"\"\n    The `load_chain()` function initializes and configures a conversational retrieval chain for\n    answering user questions.\n    :return: The `load_chain()` function returns a ConversationalRetrievalChain object.\n    \"\"\"\n\n\t\t# Load OpenAI embedding model\n\t\tembeddings = OpenAIEmbeddings()\n\t\t\n\t\t# Load OpenAI chat model\n\t\tllm = ChatOpenAI(temperature=0)\n\t\t\n\t\t# Load our local FAISS index as a retriever\n\t\tvector_store = FAISS.load_local(\"faiss_index\", embeddings)\n\t\tretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n\t\t\n\t\t# Create memory 'chat_history' \n\t\tmemory = ConversationBufferWindowMemory(k=3,memory_key=\"chat_history\")\n\t\t\n\t\t# Create system prompt\n\t\ttemplate = \"\"\"\n    You are an AI assistant for answering questions about the Blendle Employee Handbook.\n    You are given the following extracted parts of a long document and a question. Provide a conversational answer.\n    If you don't know the answer, just say 'Sorry, I don't know ... üòî. \n    Don't try to make up an answer.\n    If the question is not about the Blendle Employee Handbook, politely inform them that you are tuned to only answer questions about the Blendle Employee Handbook.\n    \n    {context}\n    Question: {question}\n    Helpful Answer:\"\"\"\n\t\t\n\t\t# Create the Conversational Chain\n\t\tchain = ConversationalRetrievalChain.from_llm(llm=llm, \n\t\t\t\t                                          retriever=retriever, \n\t\t\t\t                                          memory=memory, \n\t\t\t\t                                          get_chat_history=lambda h : h,\n\t\t\t\t                                          verbose=True)\n\t\t\n\t\t# Add systemp prompt to chain\n\t\t# Can only add it at the end for ConversationalRetrievalChain\n\t\tQA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n\t\tchain.combine_docs_chain.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate(prompt=QA_CHAIN_PROMPT)\n\t\t\n\t\treturn chain\n\n\nLet's review the code:\n\nLoad the OpenAI API key stored in .streamlit/secrets.toml.\nCreate the load_chain() function that returns a Conversational Retrieval Chain object. We use st.cache_resource to make your app more efficient. By using this, you only run the load_chain() function once at the very beginning and store the result in a local cache. Later, it‚Äôll skip its execution as no changes have been made. Very convenient!\nLoad the OpenAI embedding model, which converts the user's queries into vectors.\nLoad the OpenAI chat model that generates the answers. To do so, it uses the stand-alone question (combining the user's question and chat history) and relevant documents. Specify a temperature of 0, meaning that the model will always select the highest probability word. A higher temperature means that the model might select a word with a slightly lower probability, leading to more variation, randomness, and creativity. Play with it to see what works best for you.\nLoad your local FAISS index as a retriever, which the chain uses to search for relevant information. Define k=3‚Äîlook for the three most relevant documents in the vector database.\nCreate the memory of your chatbot using ConversationBufferWindowMemory. Define k=3, meaning the chatbot will look at the last three interactions when creating the stand-alone question. This is useful for keeping a sliding window of the most recent interactions so that the buffer does not get too large.\nCreate the system prompt, which acts as the guidelines for our chatbot. Specify how the chatbot should behave and what it should do when it can‚Äôt find an answer or when the user's question is pit of its scope.\nCreate the chain using ConversationalRetrievalChain, linking all the previous elements together. Set verbose=True to see what's happening under the hood when running the chain. This makes it easier to see what information the chatbot uses to answer user's questions.\nAdd the system prompt to the chain. Currently, it seems that you can only add it after defining the chain when using ConversationalRetrievalChain.from_llm.\n4. Chatbot application\n\nUse Streamlit to create the chatbot interface‚Äîa visually appealing chat app that can be deployed online‚Äîand embed the app in your Notion page.\n\n4.1 Streamlit application\n\nNow that you built the brain of your chatbot, let‚Äôs put it all in a Streamlit application!\n\n# **app.py**\n\nimport time\nimport streamlit as st\nfrom utils import load_chain\n\n# Custom image for the app icon and the assistant's avatar\ncompany_logo = 'https://www.app.nl/wp-content/uploads/2019/01/Blendle.png'\n\n# Configure Streamlit page\nst.set_page_config(\n    page_title=\"Your Notion Chatbot\",\n    page_icon=company_logo\n)\n\n# Initialize LLM chain\nchain = load_chain()\n\n# Initialize chat history\nif 'messages' not in st.session_state:\n    # Start with first message from assistant\n    st.session_state['messages'] = [{\"role\": \"assistant\", \n                                  \"content\": \"Hi human! I am Blendle's smart AI. How can I help you today?\"}]\n\n# Display chat messages from history on app rerun\n# Custom avatar for the assistant, default avatar for user\nfor message in st.session_state.messages:\n    if message[\"role\"] == 'assistant':\n        with st.chat_message(message[\"role\"], avatar=company_logo):\n            st.markdown(message[\"content\"])\n    else:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n\n# Chat logic\nif query := st.chat_input(\"Ask me anything\"):\n    # Add user message to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n    # Display user message in chat message container\n    with st.chat_message(\"user\"):\n        st.markdown(query)\n\n    with st.chat_message(\"assistant\", avatar=company_logo):\n        message_placeholder = st.empty()\n        # Send user's question to our chain\n        result = chain({\"question\": query})\n        response = result['answer']\n        full_response = \"\"\n\n        # Simulate stream of response with milliseconds delay\n        for chunk in response.split():\n            full_response += chunk + \" \"\n            time.sleep(0.05)\n            # Add a blinking cursor to simulate typing\n            message_placeholder.markdown(full_response + \"‚ñå\")\n        message_placeholder.markdown(full_response)\n\n    # Add assistant message to chat history\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n\nLet's review the code:\n\nImport load_chain() from utils.py to load the Conversational Retrieval Chain we created earlier.\nLoad an image from a URL to use as your app's page icon and assistant's avatar in the chat app.\nInitialize your chain in the session state.\nInitialize the chat history in the session state with a first message from the assistant welcoming the user.\nDisplay all the messages of the chat history, specifying a custom avatar for the assistant and the default one for the user.\nCreate the chat logic to:\nReceive the user's query and store it in the chat history\nDisplay the user's query in the chat\nPass the user's query to your chain using st.session_state['chain']({\"question\": query})\nGet a response back\nDisplay the response in the chat, simulating a human typing speed by slowing down the display of the response\nStore the response in the chat history\n\n4.2 Deployment on Streamlit Community Cloud\n\nOnce you‚Äôve tested your chatbot and are happy with it, it's time to go live!\n\nTo deploy it on Streamlit Community Cloud:\n\nCreate a requirements.txt file to store the required dependencies\n\n# ****************requirements.txt****************\n\nopenai\nlangchain\nfaiss-cpu\ntiktoken\n\n\nDeploy the app and click on Advanced settings. From there, specify your Python version and your OpenAI API key (they should match the information in your local secrets.toml file).\n\n4.3 Embed your Streamlit app in Notion\n\nOnce your app is deployed, copy its URL.\nGo to your Notion page.\nEmbed your app by selecting Embed in the block options.\nPaste your app‚Äôs URL and click on Embed link.\nVoil√†! Have fun interacting with your content using your new Notion chatbot!\nCongratulations! ü•≥\n\nIn this tutorial, you‚Äôve learned how to:\n\nConvert your Notion content to vectors using the OpenAI embedding model and store them in a FAISS index\nBuild a Conversation Retrieval Chain using LangChain, with a custom prompt and memory\nBuild and deploy a Streamlit chat application using its latest chat features\nEmbed a Streamlit chat application in your Notion page\n\nIf you have any questions, please post them in the comments below. And if you want to learn more about AI and LLMs, let's connect on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Product Announcements",
    "url": "https://blog.streamlit.io/tag/product/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Product\n36 posts\nIntroducing two new caching commands to replace st.cache!\n\nst.cache_data and st.cache_resource are here to make caching less complex and more performant\n\nProduct\nby\nTim Conkling and¬†\n2\n¬†more,\nFebruary 14 2023\nA new Streamlit theme for Altair and Plotly charts\n\nOur charts just got a new look!\n\nProduct\nby\nWilliam Huang and¬†\n4\n¬†more,\nDecember 19 2022\nAnnouncing the Figma-to-Streamlit plugin üé®\n\nGo from prototype to code as easy as 1-2-3 with our new community resource!\n\nProduct\nby\nJuan Mart√≠n Garc√≠a\n,\nNovember 1 2022\nThe next frontier for Streamlit\n\nOur feature roadmap for 2023 and beyond\n\nProduct\nby\nAmanda Kelly and¬†\n4\n¬†more,\nOctober 18 2022\nIntroducing multipage apps! üìÑ\n\nQuickly and easily add more pages to your Streamlit apps\n\nProduct\nby\nVincent Donato\n,\nJune 2 2022\nLeverage your user analytics on Streamlit Community Cloud\n\nSee who viewed your apps, when, and how popular they are\n\nProduct\nby\nDiana Wang and¬†\n1\n¬†more,\nMay 17 2022\nStreamlit and Snowflake: better together\n\nTogether, we‚Äôll empower developers and data scientists to mobilize the world‚Äôs data\n\nProduct\nby\nAdrien Treuille and¬†\n2\n¬†more,\nMarch 2 2022\nStreamlit Cloud is now SOC 2 Type 1 compliant\n\nWe have completed a full external audit of our security practices\n\nProduct\nby\nAmanda Kelly\n,\nJanuary 11 2022\nDeploy a private app for free! üéâ\n\nAnd... get unlimited public apps\n\nProduct\nby\nAbhi Saini\n,\nDecember 9 2021\n‚òÅÔ∏è Introducing Streamlit Cloud! ‚òÅÔ∏è\n\nStreamlit is the most powerful way to write apps. Streamlit Cloud is the fastest way to share them.\n\nProduct\nby\nAdrien Treuille\n,\nNovember 2 2021\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing a chemical molecule component for your Streamlit apps",
    "url": "https://blog.streamlit.io/introducing-a-chemical-molecule-component-for-your-streamlit-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing a chemical molecule component for your Streamlit apps\n\nIntegrate a fully featured molecule editor with just a few lines of code!\n\nBy Micha≈Ç Nowotka\nPosted in Product, April 13 2023\nHow to use Streamlit molecule editor\nFeatures\nUse cases\nDemo apps\nDemo app 1 by Kamil Bregu≈Ça\nDemo app 2\nAre custom components easy to create?\nWrapping up\nReferences\nContents\nShare this post\n‚Üê All posts\n\nAt Streamlit, we always look for ways to improve your experience and make app development simple. Custom components are essential to this, as they provide a way to contribute to and extend the Streamlit ecosystem. We wanted to create something useful for the community and show how easy it is to write a custom component.\n\nToday, we are excited to introduce a new custom component for Streamlit...\n\n...the molecule editor! üß™\n\nThe ability to draw chemical compounds is critical for most drug discovery, drug design, and cheminformatics apps. You can now easily integrate a molecule editor into your Streamlit applications with just a few lines of code.\n\nHow to use Streamlit molecule editor\n\nFirst, install Streamlit and the component:\n\npip install streamlit\npip install streamlit-ketcher\n\n\nThen, import it into your Streamlit application, and you're good to go:\n\nimport streamlit as st\nfrom streamlit_ketcher import st_ketcher\n\nsmiles = st_ketcher()\n\n\nThis code snippet creates a full-featured chemical molecule editor within your Streamlit app. The variable smiles contains the Simplified Molecular Input Line Entry System (SMILES) representation of the molecule, which you can use for further processing, analysis, or visualization in your app.\n\nOur component is based on the Ketcher library from Epam. Several other editors offer similar functionality, including MarvinJS, JSME, and ChemWriter. However, we chose Ketcher for the following reasons:\n\nIt's released under the Apache 2.0 license, compatible with the Streamlit library.\nIt's React-friendly, as it has a corresponding React component.\nIt has pretty good documentation.\nIt has a modern look and feel and is feature-rich.\nIts most basic version doesn't rely on an external server.\n\nSo, it was the perfect fit for our needs. üôÇ\n\nFeatures\n\nThe molecule editor component offers several useful features, including:\n\nIntuitive drawing tools: Easily create and modify molecules using a simple point-and-click interface, with support for adding and deleting atoms, bonds, and functional groups.\nAutomatic SMILES and Molfile generation: Instantly convert the molecule into its SMILES representation to easily integrate with other cheminformatics tools. Molfiles are also supported.\nCopy/paste support.\nOpen-source and community-driven: The editor is free and open-source, allowing you to contribute and help improve the component.\nUse cases\n\nThe editor is an excellent choice for a variety of cheminformatics and chemistry-related applications, including:\n\nDrug design and discovery\nMolecular modeling and visualization\nManagement of chemical databases\nEducation and training in chemoinformatics\nPrediction of chemical properties\nDemo apps\n\nWe've prepared two demo applications.\n\nDemo app 1 by Kamil Bregu≈Ça\n\nThis app provides an overview of the basic usage and configuration of the component:\n\nDemo app 2\n\nThis app explores some of a particular application's most popular use cases. It integrates with the open chemistry database ChEMBL using the chembl_webresource_client package (created a few years ago by the author of this doc). With this integration, molecule structures can be retrieved by name. The app can now create a gallery of the \"most famous\" chemical compounds at the top:\n\nThis is achieved with the following code:\n\nfamous_molecules = [\n    ('‚òï', 'Caffeine'), \n    ('ü•±', 'Melatonin'), \n    ('üö¨', 'Nicotine'), \n    ('üå®Ô∏è', 'Cocaine'), \n    ('üíä', 'Aspirin'),\n    ('üçÑ', 'Psilocybine'), \n    ('üíé', 'Lysergide')\n]\nfor mol, column in zip(famous_molecules, st.columns(len(famous_molecules))):\n    with column:\n        emoji, name = mol\n        if st.button(f'{emoji} {name}'):\n            st.session_state.molfile, st.session_state.chembl_id = utils.name_to_molecule(name)\n\n\nThe name_to_molecule function is defined as follows:\n\nfrom typing import Optional, Tuple\nfrom chembl_webresource_client.new_client import new_client as ch\n\ndef name_to_molecule(name: str) -> Tuple[str, str]:\n    columns = ['molecule_chembl_id', 'molecule_structures']\n    ret = ch.molecule.filter(molecule_synonyms__molecule_synonym__iexact=name).only(columns)\n    best_match = ret[0]\n    return best_match[\"molecule_structures\"][\"molfile\"], best_match[\"molecule_chembl_id\"]\n\n\nAfter you retrieve the molecule structure from ChEMBL and load it into the editor, you can modify it as needed. Once editing is complete, you can read the compound from the editor and run a similarity search to find the most similar compounds from ChEMBL. You can control the similarity threshold with a slider. The search results will be displayed in a table:\n\nwith editor_column:\n    smiles = st_ketcher(st.session_state.molfile)\n    similarity = st.slider(\"Similarity threshold:\", min_value=60, max_value=100)\n    with st.expander(\"Raw data\"):\n        st.markdown(f\"```{smiles}```\")\n    with results_column:\n        similar_molecules = utils.find_similar_molecules(smiles, similarity)\n        if not similar_molecules:\n            st.warning(\"No results found\")\n        else:\n            table = utils.render_similarity_table(similar_molecules)\n            similar_smiles = utils.get_similar_smiles(similar_molecules)\n            st.markdown(f'<div style=\"overflow:scroll; height:600px; padding-left: 80px;\">{table}</div>',\n                        unsafe_allow_html=True)\n\n\nYou can perform the similarity search using the following function:\n\ndef find_similar_molecules(smiles: str, threshold: int):\n    columns = ['molecule_chembl_id', 'similarity', 'pref_name', 'molecule_structures']\n    try:\n        return ch.similarity.filter(smiles=smiles, similarity=threshold).only(columns)\n    except Exception as _:\n        return None\n\n\nAfter retrieving compounds similar to the 'query compound' you drew in the editor, you can run a target prediction to determine which biological targets your compounds will bind to. To do this, use the onnxruntime library. It'll let you load a prediction model created by my EBI colleague, Eloy Felix (read more about model training and exporting to the ONNX format here).\n\nAll target prediction logic is implemented in the demo app's target_predictions.py module:\n\nThe result is a table of biological targets ranked by the probability of having an affinity for the predicted compounds. Note that some targets are repeated, as you get the results for each query compound separately. We omitted adding more information about the targets and filtering by organism or type since it's just a proof-of-concept app.\n\nAre custom components easy to create?\n\nThe good news is that we created a fairly complex custom component in just a few days. This means that the current API is fully functional. But we've also identified some areas for improvement, including:\n\nOutdated component templates\nUndocumented React hook support\nMissing props in the theme interface definition\nIssues with the CSS that cause the component to render strangely without further customization\nDifficulty testing components\n\nWhile we've found workarounds for most of these issues, we believe it's worth investing some time in the Custom Components ecosystem to make it more developer-friendly. What do you think? Have you ever created a custom component? Share your thoughts in the comments.\n\nWrapping up\n\nWe hope the molecule editor component is a valuable addition to your toolkit. We can't wait to see the amazing applications you create with it. Don't hesitate to contact us with questions or suggestions or to share your projects with the Streamlit community!\n\nHappy coding! üßë‚Äçüíª\n\nReferences\nStreamlit Molecule Editor GitHub Repository: https://github.com/mik-laj/streamlit-ketcher\nKetcher Library by EPAM: https://lifescience.opensource.epam.com/ketcher\nChEMBL database: https://www.ebi.ac.uk/chembl\nChEMBL web resource client library: https://github.com/chembl/chembl_webresource_client\nONNX runtime: https://onnxruntime.ai\nSimple demo code: https://ketcher-editor.streamlit.app/\nAdvanced demo code: https://github.com/streamlit/mol-demo\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing column config ‚öôÔ∏è",
    "url": "https://blog.streamlit.io/introducing-column-config/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing column config ‚öôÔ∏è\n\nTake st.dataframe and st.data_editor to the next level!\n\nBy Lukas Masuch and Johannes Rieke\nPosted in Product, June 22 2023\nA small example\nMore parameters\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nOne of the biggest pain points you've told us about repeatedly is dataframe customization.\n\nData wrangling is at the core of your work, and you've long needed more capabilities than those offered by st.dataframe and st.data_editor.\n\nHere are some of the things you told us you need to do:\n\nHide the index column\nShow images or clickable links in table cells\nDisable editing for specific columns of the new data editor\n\nWe heard you! That's why in Streamlit 1.23, we're excited to introduce‚Ä¶\n\nColumn configuration! üéâ\n\nNow you can customize all columns in st.dataframe and st.data_editor, so your tables look and feel exactly the way you need. Want to see it? Check out our demo app or read below.\n\nPlus, we've moved st.data_editor out of experimental! üéà\n\nA small example\n\nThe main star of the show is the new column_config parameter. We added it to both st.dataframe and st.data_editor.\n\nIt's a dictionary that maps column types to their configuration:\n\nst.data_editor(\n    df,\n    column_config={\n        \"column 1\": \"Name\",  # change the title\n        \"column 3\": st.column_config.ImageColumn(\"Avatar\"),\n        \"column 4\": st.column_config.NumberColumn(\n            \"Age\", min_value=0, max_value=120, format=\"%d years\"\n        ),\n        \"column 8\": st.column_config.LineChartColumn(\n            \"Activity (1 year)\", y_min=0, y_max=100\n        ),\n    },\n)\n\n\nTaking this a bit further, you can create powerful tables like this:\n\nTry playing with it:\n\nScroll to the right to see some beautiful charts ‚ú®üìà\nDouble-click on links to open them üîó\nDouble-click on a cell to edit it and see input validation features in action ‚úçÔ∏è\n\nAs you can see in the code, we also introduced new classes for different column types under the st.column_config namespace. In fact, there are 14 different column types that cover everything from text over images to sparkline charts! Each of these classes lets you set additional parameters to configure the display and editing behavior of the column.\n\nHave a look at them on our new docs page! üéà\n\nTo learn more about the column_config parameter itself, check out the API references for ¬†st.dataframe and st.data_editor.\n\nMore parameters\n\nWant to hide the index column without delving into column configuration? We've got you covered!\n\nIn addition to the column_config parameter, we added a few more parameters that allow you to perform common operations more quickly:\n\nhide_index=True lets you hide the index column\ncolumn_order=[\"col 3\", \"col2\"] lets you set the order in which columns show up\ndisabled=[\"col1\", \"col2\"] lets you turn off editing for individual columns on st.data_editor\n\nRead more about these parameters on the API docs for st.dataframe and st.data_editor.\n\nWrapping up\n\nWe're excited to see what you'll build with this new feature. Please share your examples on Twitter and the forum! Head over to our example app to get some inspiration. And if you have more feature requests for dataframes (and beyond), let us know on GitHub.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Introducing st.connection!",
    "url": "https://blog.streamlit.io/introducing-st-experimental_connection/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing st.connection!\n\nQuickly and easily connect your app to data and APIs\n\nBy Joshua Carroll and Vincent Donato\nPosted in Product, May 2 2023\nst.experimental_connection ü•Ç\nHow does st.experimental_connection work\nStep 1. Install dependencies\nStep 2. Set up credentials and connection information in .streamlit/secrets.toml\nStep 3. Import and initialize the connection in your app\nStep 4. Query your data with one line of code\nStep 5. Perform complex operations with .session\nHow to build your own connection\nWhat‚Äôs next?\nContents\nShare this post\n‚Üê All posts\nüì£\nst.experimental_connection is now st.connection\n\nCheck out the docs to learn more. These updates should make it even easier to connect to your data:\n\n1) The Snowpark connection is now a simpler, more powerful Snowflake connection.\n\n2) More passthrough methods and hints have been added to make data connection bugs self-correcting or easier to fix.\n\nIn our last-year survey (N=315), 29% of you told us that setting up data connections was the most frustrating part of your work.\n\nWe get it. Connecting to data sources and APIs can be painful. ü§ï\n\nYou need to find and install external packages, figure out how to manage your credentials securely outside of code, then find the right methods to get data out in the format you need. Oh, and you can‚Äôt forget to add Streamlit caching capabilities! For example, you can see the 14 lines of code in our original MySQL tutorial using a mysql.connector, st.cache_resource, st.cache_data, managing a cursor, and converting the row result format!\n\nToday, we‚Äôre thrilled to release‚Ä¶\n\nst.experimental_connection ü•Ç\n\nConnect your Streamlit apps to data and APIs with just four lines of code:\n\nimport streamlit as st\n\nconn = st.experimental_connection('pet_db', type='sql')\npet_owners = conn.query('select * from pet_owners')\nst.dataframe(pet_owners)\n\n\nHere is the demo app, or you can play with it below:\n\nHow does st.experimental_connection work\n\nStreamlit comes installed with generic connections for SQL and Snowflake Snowpark. You may need to install additional packages or one of the community connections to make it work properly.\n\nToday, it supports:\n\nSQL dialects (MySQL, Postgres, Snowflake, BigQuery, Microsoft SQL Server, etc.)\nSnowflake Snowpark\nCloud file storage (S3, GCS, Azure Blob Storage, etc.) via FilesConnection\nHuggingFace Datasets and Models via HfFileSystem and FilesConnection\nGoogle Sheets via GSheetsConnection\n\nWe‚Äôre making it easier than ever to extend this list and build your own data connections and share them with the Streamlit community!\n\nFor the purpose of this post, we‚Äôll be using MySQL examples. If you want to follow along with other data sources, check out our tutorials on Snowflake or AWS S3.\n\nStep 1. Install dependencies\n\nTo start, install any necessary packages in your environment (such as with pip and requirements.txt). You can find these in Streamlit‚Äôs data source tutorials or the data source documentation. If something is missing when you run your app, Streamlit will try to detect that and give you a hint about what to install (we‚Äôll make this even easier in the future!):\n\npip install SQLAlchemy mysqlclient\n\nStep 2. Set up credentials and connection information in .streamlit/secrets.toml\n\nNext, let‚Äôs set up the connection information in secrets.toml. Create a new section called [connections.<name>] and add the parameters you need. You can name the section whatever you‚Äôd like - you‚Äôll use the same name to reference it in your app code.\n\n# .streamlit/secrets.toml\n\n[connections.pet_db]\ndialect = \"mysql\"\nurl = \"mysqldb://scott:tiger@192.168.0.134/pet_db\"\n\n\nWe added support for a global secrets.toml, so if you keep using the same database, you can set this up once instead of copying it to every app. Many connections will also support their native configuration, like AWS_* environment variables or ~/.snowsql/config file.\n\nStep 3. Import and initialize the connection in your app\n\nNow, let‚Äôs initialize the connection in your app:\n\n# streamlit_app.py\n\nimport streamlit as st\n\nconn = st.experimental_connection('pet_db', type='sql')\n\n\nThe first argument is the name of the connection you used in secrets.toml. The type argument tells Streamlit what type of connection it is. For community-developed connections that don‚Äôt ship with Streamlit, you can import the connection class and pass it directly to type. See the AWS S3 tutorial or API Reference for examples.\n\nStep 4. Query your data with one line of code\n\nFor the common use case of reading data or getting some response from an API, the connection will provide a simple method that returns an easy-to-use output. It‚Äôs also cached in Streamlit by default to make your app ‚ö°blazing fast! ‚ö°\n\nFor example, SQLConnection has a query() method that takes a query input and returns a pandas DataFrame:\n\n# streamlit_app.py\n\nimport streamlit as st\n\nconn = st.experimental_connection('pet_db', type='sql')\npet_owners = conn.query('select * from pet_owners')\nst.dataframe(pet_owners)\n\n\nThat‚Äôs it!\n\nThe method also supports params, paging, custom cache TTL, and other common arguments (read more in the API Reference).\n\nDepending on the underlying data format, the specific methods may differ but should be natural, straightforward, and intuitive to that data source. Connection objects are fully type annotated, so your IDE can provide hints. st.help() and st.write() can also give you more information about what is supported on a specific connection!\n\nStep 5. Perform complex operations with .session\n\nIf you need the full power of the underlying data source or library, it‚Äôs easily accessible too! SQL and Snowpark both support this with .session, and other connections may have a domain-specific name for easier discovery.\n\nFor example, if you need to use transactions, write back, or interact via ORM, you can access the SQL Session with SQLConnection.session:\n\nwith conn.session as s:\n    pet_owners = {'jerry': 'fish', 'barbara': 'cat', 'alex': 'puppy'}\n    for k in pet_owners:\n        s.execute(\n            'INSERT INTO pet_owners (person, pet) VALUES (:owner, :pet);',\n            params=dict(owner=k, pet=pet_owners[k])\n        )\n    s.commit()\n\nCheck out the tutorials or use st.help() to learn more about what‚Äôs supported for a specific data set.\n\nHow to build your own connection\n\nWe‚Äôre excited for the community to extend and build on the st.experimental_connection interface. We want to make it super easy to build Streamlit apps with lots of data sources (we‚Äôve built the interface with this in mind).\n\nTo use a community-built connection in your app, install and import it, then pass the class to st.experimental_connection():\n\nimport streamlit as st\nfrom st_gsheets_connection import GSheetsConnection\n\nconn = st.experimental_connection(\"pets_gsheet\", type=GSheetsConnection)\npet_owners = conn.read(worksheet=\"Pet Owners\")\nst.dataframe(pet_owners)\n\n\nThese types of connections work the same as the ones built into Streamlit and have access to the same capabilities. Build a connection class by extending streamlit.connections.ExperimentalBaseConnection. You can find basic information in the API Reference, and a simple fully working example here. The SQLConnection built into Streamlit is another great starting point.\n\nWhat‚Äôs next?\n\nWe‚Äôve been hard at work on st.experimental_connection, so we‚Äôre very excited to finally share it with you! Please let us know how you're using it on the forum, Twitter, Discord, or in the comments below.\n\nExpect more connections, guides, and features in the coming weeks and months to make it even easier to connect your Streamlit app to data. And keep an eye out for a community connection-building contest a little bit later this spring. üôÇ\n\nHappy app-building! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Micha≈Ç Nowotka - Streamlit",
    "url": "https://blog.streamlit.io/author/michal/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Micha≈Ç Nowotka\n3 posts\nIntroducing a chemical molecule component for your Streamlit apps\n\nIntegrate a fully featured molecule editor with just a few lines of code!\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 13 2023\nIntroducing Streamlit to the Polish Python community\n\nMy Streamlit presentation at PyWaW #103\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 4 2023\nHow to make a culture map\n\nAnalyze multidimensional data with Steamlit!\n\nTutorials\nby\nMicha≈Ç Nowotka\n,\nJanuary 12 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Generative AI and Streamlit: A perfect match",
    "url": "https://blog.streamlit.io/generative-ai-and-streamlit-a-perfect-match/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nBy Adrien Treuille and Amanda Kelly\nPosted in LLMs, June 15 2023\nStreamlit is the UI powering the LLM movement\nWhat does this mean for the future?\nüí¨ Chat interfaces as a key UI paradigm\n‚öôÔ∏è Easily connect to the LLM ecosystem\n‚ùÑÔ∏è Unlock business insights with LLMs in Snowflake\nLet's build the future together\nContents\nShare this post\n‚Üê All posts\n\nFive years ago, we started Streamlit to help you share your work through beautiful web apps. Our vision was to create a thin wrapper around the Python ecosystem, allowing you to bring APIs, models, and business logic to life just by typing.\n\nToday, generative AI is exploding into the Python ecosystem, and Streamlit has been right there. In fact, it's so easy that‚Ä¶\n\n‚Ä¶LLMs can write a Streamlit app for you!\n\nStreamlit is the UI powering the LLM movement\n\nHow is it that LLMs are so good at writing Streamlit apps? It's because of all of the Streamlit code the community has shared!\n\nOver 190,000 snippets of Streamlit code (and counting) exist on GitHub alone, all of which have helped train GPT4 and other LLMs. This means that analysts, data scientists, and even students can quickly perform analyses, rough out new apps, and weave auto-generated Streamlit fragments throughout other apps.\nOver 5,000 LLM-powered Streamlit apps (and counting) have already been built on the Community Cloud alone. And these numbers are growing rapidly every day.\nWhat does this mean for the future?\n\nWith its simple API and design, Streamlit has surfed each wave of generative AI advances, helping you effortlessly bring LLM-powered apps to life.\n\nBut what is even more important is how the rest of the LLM ecosystem is using Streamlit and how we can weave this all together for even more powerful apps.\n\nüí¨ Chat interfaces as a key UI paradigm\n\nStreamlit is perfectly suited for generative AI with its simple API, real-time visualization, interactive abilities, and user-friendly interfaces.\n\nWe are releasing chat elements soon that will help you make all the LLM-powered apps you can dream up.\n\n‚öôÔ∏è Easily connect to the LLM ecosystem\n\nAs LLMs continue to gain popularity, a vast ecosystem of tools is emerging around them. Here are some of the tools that already work well with Streamlit.\n\nü¶úüîó LangChain's callback system to see what an LLM is thinking\n\nAs we move beyond simple chatbots to more complex applications, it'll be critical to understand how LLMs think and the steps they take to answer questions. We've enabled you to add intermediate step information to your Streamlit + LangChain app with just one command:\n\nIn addition to LangChain, here are some other tools that we're excited about:\n\nStable Diffusion: example apps from Stability AI\nMultiple BabyAGI apps, including AgentGPT\nLlamaIndex Term Extractor and Starter Pack\nSygil WebUI for Stable Diffusion\n\nWe'll be announcing more integrations soon, so stay tuned!\n\n‚ùÑÔ∏è Unlock business insights with LLMs in Snowflake\n\nSince our acquisition in 2022, we've been integrating Streamlit's functionality into Snowflake's enterprise-grade data solutions used by some of the largest companies in the world.\n\nWe'll share more at Snowflake Summit 2023, June 26-29 (you can still register here). Here are a few Streamlit + LLM sessions:\n\nUnleashing the Power of Large Language Models with Snowflake with Adrien Treuille, Streamlit co-founder, and Richard Meng, Senior Software Engineer.\nGPTZero: Idea to Iteration Powered by Streamlit and Snowflake with Caroline Frasca and Edward Tian.\nKeynote: Generative AI and LLMs in the enterprise\n\nCheck out more LLM talks here (you can watch them online after the Summit).\n\nLet's build the future together\n\nThis community's engagement, creativity, and innovation are absolutely incredible. Thank you for inspiring our roadmap with your contributions.\n\nYour support, combined with the rapid advancements coming from generative AI, will enable us to release new features that help shape the future of apps with LLMs. We want to hear from you!\n\nIn the meantime, check out our new generative AI hub and join us on the forum to share your ideas.\n\nLet's do this together!\n\nLove,\n\nAdrien, Amanda, Thiago, and everyone at Streamlit. üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Instant Insight: Generate data-driven presentations in a snap!",
    "url": "https://blog.streamlit.io/instant-insight-generate-data-driven-presentations-in-a-snap/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nInstant Insight: Generate data-driven presentations in a snap!\n\nCreate presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery\n\nBy Oleksandr Arsentiev\nPosted in LLMs, August 2 2023\nWhy Instant Insight?\nApp overview\n1. Connect your Streamlit app to Snowflake\n2. Create a UI with dynamic filters and an interactive table\n3. Fetch company data from Yahoo Finance\n4. Create graphs using Plotly\n5. Use Clearbit API to get company logos\n6. Use LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition\n7. Extract structured data from the GPT response\n8. Generate slides using python-pptx\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nHey there! üëã\n\nI'm Oleksandr, a Data Analyst at Workday. I wrote my first lines of code four years ago and saw the incredible potential of technology to simplify tasks and enhance productivity.\n\nIn this post, I'll tell you about my Instant Insight app, which can generate PowerPoint presentations for company research instantly. Forget the hassle of spending hours crafting slides. Now you can create sleek, data-driven presentations in just a few clicks!\n\nI'll show you how to:\n\nConnect your Streamlit app to Snowflake\nCreate a UI with dynamic filters and an interactive table\nFetch company data from Yahoo Finance\nCreate graphs using Plotly\nUse Clearbit API to get company logos\nUse LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition\nExtract structured data from the GPT response\nGenerate slides using python-pptx\n\nLet's get cracking!\n\n‚ö°\nTLDR: Here's the app‚Äôs URL and the GitHub repo. Enjoy!\nWhy Instant Insight?\n\nImagine working in sales for a B2B SaaS company with hundreds of prospects and offering the following products: Accounting and Planning Software, CRM, Chatbot, and Cloud Data Storage. Your task is to conduct prospect research, including financial and SWOT analysis, explore the competitive landscape, craft value propositions, and share a presentation with your team.\n\nThe prospects' data is stored in Snowflake, which feeds your CRM system. You can use the Instant Insight app to quickly filter the prospects by sector, industry, prospect status, and product. Next, select the prospect you want to include in the presentation and click the button to generate the presentation. And... that's it! ü§© You now have the slides ready to be shared with your team. üöÄ\n\nThe slides created by the app include the following:\n\nBasic company overview\nSWOT analysis\nFinancial charts\nValue proposition\nCompetitor analysis\nKey people\nNews and corporate events\nApp overview\n\nThe high-level architecture of the app looks like this:\n\nAnd here is a 3-minute video of the app in action:\n\nNow, let's dive into the code!\n\n1. Connect your Streamlit app to Snowflake\n\nFirst, obtain some data. To do this, use the Snowflake Connector:\n\nimport snowflake.connector\n\n# get Snowflake credentials from Streamlit secrets\nSNOWFLAKE_ACCOUNT = st.secrets[\"snowflake_credentials\"][\"SNOWFLAKE_ACCOUNT\"]\nSNOWFLAKE_USER = st.secrets[\"snowflake_credentials\"][\"SNOWFLAKE_USER\"]\nSNOWFLAKE_PASSWORD = st.secrets[\"snowflake_credentials\"][\"SNOWFLAKE_PASSWORD\"]\nSNOWFLAKE_DATABASE = st.secrets[\"snowflake_credentials\"][\"SNOWFLAKE_DATABASE\"]\nSNOWFLAKE_SCHEMA = st.secrets[\"snowflake_credentials\"][\"SNOWFLAKE_SCHEMA\"]\n\n@st.cache_resource\ndef get_database_session():\n    \"\"\"Returns a database session object.\"\"\"\n    return snowflake.connector.connect(\n        account=SNOWFLAKE_ACCOUNT,\n        user=SNOWFLAKE_USER,\n        password=SNOWFLAKE_PASSWORD,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA,\n    )\n\n@st.cache_data\ndef get_data():\n    \"\"\"Returns a pandas DataFrame with data from Snowflake.\"\"\"\n    query = 'SELECT * FROM us_prospects;'\n    cur = conn.cursor()\n    cur.execute(query)\n\n    # Fetch the result as a pandas DataFrame\n    column_names = [col[0] for col in cur.description]\n    data = cur.fetchall()\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Close the connection to Snowflake\n    cur.close()\n    conn.close()\n    return df\n\n# get the data from Snowflake\nconn = get_database_session()\ndf = get_data(conn)\n\n\nSensitive data such as your Snowflake account, username, password, database name, and schema are stored in secrets and retrieved by calling st.secrets (read more here).\n\nNext, define two functions:\n\nget_database_session() initializes a connection object\nget_data() executes a SQL query and returns a pandas DataFrame\n\nUse a simple SELECT * query to retrieve all the data from the us_prospects table.\n\n2. Create a UI with dynamic filters and an interactive table\n\nNow let's use some Streamlit magic to develop a front end for the app. Create a sidebar panel containing four dynamic multi-select filters and add a checkbox that lets users select all values.\n\nThe filters in your app work sequentially. Users are expected to apply them individually, from top to bottom. Once the first filter is applied, the second filter becomes available and contains only relevant labels. After each filter is applied, the underlying DataFrame is pre-filtered, and the num_of_pros variable is updated to reflect the number of selected prospects.\n\nSee the filters in action:\n\nHere is the code for the first two filters:\n\n# create sidebar filters\nst.sidebar.write('**Use filters to select prospects** üëá')\nsector_checkbox = st.sidebar.checkbox('All Sectors', help='Check this box to select all sectors')\nunique_sector = sorted(df['SECTOR'].unique())\n\n# if select all checkbox is checked then select all sectors\nif sector_checkbox:\n    selected_sector = st.sidebar.multiselect('Select Sector', unique_sector, unique_sector)\nelse:\n    selected_sector = st.sidebar.multiselect('Select Sector', unique_sector)\n\n# if a user selected sector then allow to check all industries checkbox\nif len(selected_sector) > 0:\n    industry_checkbox = st.sidebar.checkbox('All Industries', help='Check this box to select all industries')\n    # filtering data\n    df = df[(df['SECTOR'].isin(selected_sector))]\n    # show number of selected prospects\n    num_of_pros = str(df.shape[0])\nelse:\n    industry_checkbox = st.sidebar.checkbox('All Industries', help='Check this box to select all industries',\n                                           disabled=True)\n    # show number of selected prospects\n    num_of_pros = str(df.shape[0])\n\n# if select all checkbox is checked then select all industries\nunique_industry = sorted(df['INDUSTRY'].loc[df['SECTOR'].isin(selected_sector)].unique())\nif industry_checkbox:\n    selected_industry = st.sidebar.multiselect('Select Industry', unique_industry, unique_industry)\nelse:\n    selected_industry = st.sidebar.multiselect('Select Industry', unique_industry)\n\n# if a user selected industry then allow them to check all statuses checkbox\nif len(selected_industry) > 0:\n    status_checkbox = st.sidebar.checkbox('All Prospect Statuses', help='Check this box to select all prospect statuses')\n    # filtering data\n    df = df[(df['SECTOR'].isin(selected_sector)) & (df['INDUSTRY'].isin(selected_industry))]\n    # show number of selected prospects\n    num_of_pros = str(df.shape[0])\n\nelse:\n    status_checkbox = st.sidebar.checkbox('All Prospect Statuses', help='Check this box to select all prospect statuses', disabled=True)\n\n\nNext, use AgGrid to create an interactive table displaying your data, allowing users to select slide-generation prospects (read more here).\n\nPlace a checkbox on each table row, allowing users to select only one row. Additionally, set custom column width and table height.\n\nHere is what it'll look like:\n\nNeat, right?\n\nHere is the code to create this table:\n\nfrom st_aggrid import AgGrid\nfrom st_aggrid.grid_options_builder import GridOptionsBuilder\nfrom st_aggrid import GridUpdateMode, DataReturnMode\nimport pandas as pd\n\n# creating AgGrid dynamic table and setting configurations\ngb = GridOptionsBuilder.from_dataframe(df)\ngb.configure_selection(selection_mode=\"single\", use_checkbox=True)\ngb.configure_column(field='Company Name', width=270)\ngb.configure_column(field='Sector', width=260)\ngb.configure_column(field='Industry', width=350)\ngb.configure_column(field='Prospect Status', width=270)\ngb.configure_column(field='Product', width=240)\n\ngridOptions = gb.build()\n\nresponse = AgGrid(\n    df,\n    gridOptions=gridOptions,\n    height=600,\n    update_mode=GridUpdateMode.SELECTION_CHANGED,\n    data_return_mode=DataReturnMode.FILTERED_AND_SORTED,\n    fit_columns_on_grid_load=False,\n    theme='alpine',\n    allow_unsafe_jscode=True\n)\n\n# get selected rows\nresponse_df = pd.DataFrame(response[\"selected_rows\"])\n\n3. Fetch company data from Yahoo Finance\n\nLet's say the user has selected a company for research, and you need to gather some data on it. Your primary data source is Yahoo Finance, which you'll access with the yahooquery library‚Äîa Python interface to unofficial Yahoo Finance API endpoints. It allows users to retrieve almost all the visible data via the Yahoo Finance front-end.\n\nHere's the overview slide with Yahoo Finance data:\n\nUse the Ticker class of yahooquery to obtain quantitative and qualitative data about a selected company. Just pass the company's ticker symbol as an argument, call the required property, and retrieve the data from the returned dictionary.\n\nHere is the code that retrieves data for the company overview slide:\n\nfrom yahooquery import Ticker\n\nselected_ticker = 'ABC'\nticker = Ticker(selected_ticker)\n\n# get company info\nname = ticker.price[selected_ticker]['shortName']\nsector = ticker.summary_profile[selected_ticker]['sector']\nindustry = ticker.summary_profile[selected_ticker]['industry']\nemployees = ticker.summary_profile[selected_ticker]['fullTimeEmployees']\ncountry = ticker.summary_profile[selected_ticker]['country']\ncity = ticker.summary_profile[selected_ticker]['city']\nwebsite = ticker.summary_profile[selected_ticker]['website']\nsummary = ticker.summary_profile[selected_ticker]['longBusinessSummary']\n\n\nThe app utilizes Yahoo Finance data to create graphs illustrating financial performance over time. One slide displays basic financial metrics such as stock price, total debt, total revenue, and EBITDA over time.\n\nI'll touch on plotting later. For now, let's focus on obtaining financial data from Yahoo Finance. Functions get_stock() and get_financials() return dataframes with relevant financial metrics. The stock price data is stored separately from other financial metrics, which is why you call two properties:\n\nticker.history() to retrieve historical pricing data for given symbol(s) (read docs here)\nticker.all_financial_data() to retrieve all financial data, including income statement, balance sheet, cash flow, and valuation measures (read docs here)\n\nHere is the code used to generate four dataframes with historical stock price, revenue, total debt, and EBITDA:\n\nfrom yahooquery import Ticker\nimport pandas as pd\n\ndef get_stock(ticker, period, interval):\n    \"\"\"function to get stock data from Yahoo Finance. Takes ticker, period and interval as arguments and returns a DataFrame\"\"\"\n    hist_df = ticker.history(period=period, interval=interval)\n    hist_df = hist_df.reset_index()\n    # capitalize column names\n    hist_df.columns = [x.capitalize() for x in hist_df.columns]\n    return hist_df\n\ndef get_financials(df, col_name, metric_name):\n    \"\"\"function to get financial metrics from a DataFrame. Takes DataFrame, column name and metric name as arguments and returns a DataFrame\"\"\"\n    metric = df.loc[:, ['asOfDate', col_name]]\n    metric_df = pd.DataFrame(metric).reset_index()\n    metric_df.columns = ['Symbol', 'Year', metric_name]\n    return metric_df\n\nselected_ticker = 'ABC'\nticker = Ticker(selected_ticker)\n\n# get all financial data\nfin_df = ticker.all_financial_data()\n\n# create df's for each metric\nstock_df = get_stock(ticker=ticker, period='5y', interval='1mo')\nrev_df = get_financials(df=fin_df, col_name='TotalRevenue', metric_name='Total Revenue')\ndebt_df = get_financials(df=fin_df, col_name='TotalDebt', metric_name='Total Debt')\nebitda_df = get_financials(df=fin_df, col_name='NormalizedEBITDA', metric_name='EBITDA')\n\n\nThe data from Yahoo Finance is also used on another slide for competitor analysis, where a company's performance is compared to its peers. To perform it, use two metrics: total revenue and SG&A % of revenue. They're available in the income statement, so use the ticker.income_statement() property which returns a DataFrame (read more here).\n\nThe extract_comp_financials() function retrieves the total revenue and selling, general, and administrative expenses (SG&A) from the income statement DataFrame, and only keeps data from 2022. Since the SG&A as a percentage of revenue metric isn't readily available, calculate it manually by dividing SG&A by revenue and multiplying by 100.\n\nThe function operates on a nested dictionary with company names as keys and a dictionary with tickers as values, then appends new values to the existing dictionary:\n\nfrom yahooquery import Ticker\nimport pandas as pd\n\ndef extract_comp_financials(tkr, comp_name, comp_dict):\n    \"\"\"Function to extract financial metrics for competitors. \nTakes a ticker, company name and a nested dictionary with \ncompetitors as arguments and appends financial metrics to dict\"\"\"\n\n    ticker = Ticker(tkr)\n    income_df = ticker.income_statement(frequency='a', trailing=False)\n    subset = income_df.loc[:, ['asOfDate', 'TotalRevenue', 'SellingGeneralAndAdministration']].reset_index()\n\n    # keep only 2022 data\n    subset = subset[subset['asOfDate'].dt.year == 2022].sort_values(by='asOfDate', ascending=False).head(1)\n\n    # get values\n    total_revenue = subset['TotalRevenue'].values[0]\n    sg_and_a = subset['SellingGeneralAndAdministration'].values[0]\n\n    # calculate sg&a as a percentage of total revenue\n    sg_and_a_pct = round(sg_and_a / total_revenue * 100, 2)\n\n    # add values to dictionary\n    comp_dict[comp_name]['Total Revenue'] = total_revenue\n    comp_dict[comp_name]['SG&A % Of Revenue'] = sg_and_a_pct\n\n# sample dict\npeers_dict_nested = {'Company 1': {'Ticker': 'ABC'}, 'Company 2': {'Ticker': 'XYZ'}}\n\n# extract financial data for each competitor\nfor key, value in peers_dict_nested.items():\n    try:\n        extract_comp_financials(tkr=value['Ticker'], comp_name=key, dict=peers_dict_nested)\n    # if ticker is not found in Yahoo Finance, drop it from the peers dict and continue\n    except:\n        del peers_dict_nested[key]\n        continue\n\n\nAfter running the code above, get a nested dictionary that resembles the following structure:\n\n# sample output dict\n{'Company 1': {'Ticker': 'ABC', 'Total Revenue': '1234', 'SG&A % Of Revenue': '10'}, \n 'Company 2': {'Ticker': 'XYZ', 'Total Revenue': '5678', 'SG&A % Of Revenue': '20'}}\n\n\nNext, convert the nested dictionary to the DataFrame to pass it to a plotting function:\n\n# create a dataframe with peers financial data\npeers_df = pd.DataFrame.from_dict(peers_dict_nested, orient='index')\npeers_df = peers_df.reset_index().rename(columns={'index': 'Company Name'})\n\n\nThe resulting DataFrame should look something like this:\n\nCompany Name\tTicker\tTotal Revenue\tSG&A % Of Revenue\nCompany 1\tABC\t1234\t10\nCompany 2\tXYZ\t5678\t20\n4. Create graphs using Plotly\n\nYou've filtered the financial data‚Äînow it's time to plot it! Use Plotly Express to create simple yet visually appealing graphs (read more here).\n\nIn the previous section, you create a DataFrame and a variable for the company name. Use these in the plot_graph() function to take the dataframe, column names for the x and y axes, and the graph title as arguments:\n\nimport plotly.express as px\n\ndef plot_graph(df, x, y, title, name):\n    \"\"\"function to plot a line graph. Takes DataFrame, x and y axis, title and name as arguments and returns a Plotly figure\"\"\"\n    fig = px.line(df, x=x, y=y, template='simple_white',\n                        title='<b>{} {}</b>'.format(name, title))\n    fig.update_traces(line_color='#A27D4F')\n    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')\n    return fig\n\nstock_fig = plot_graph(df=stock_df, x='Date', y='Open', title='Stock Price USD', name=name)\nrev_fig = plot_graph(df=rev_df, x='Year', y='Total Revenue', title='Total Revenue USD', name=name)\ndebt_fig = plot_graph(df=debt_df, x='Year', y='Total Debt', title='Total Debt USD', name=name)\nebitda_fig = plot_graph(df=ebitda_df, x='Year', y='EBITDA', title='EBITDA USD', name=name)\n\n\nThe resulting graphs should look something like this:\n\nThe app also generates a slide containing a competitor analysis for a given company. To make it, use the peers_plot() function along with peers_df. This function generates a horizontal bar chart that compares the total revenue and SG&A % of revenue among competitors.\n\nHere is the code:\n\nimport plotly.express as px\nimport pandas as pd\n\ndef peers_plot(df, name, metric):\n    \"\"\"Function to plot a bar chart with peers. \nTakes DataFrame, name, metric and ticker as arguments and returns a Plotly figure\"\"\"\n\n    # drop rows with missing metrics\n    df.dropna(subset=[metric], inplace=True)\n    df_sorted = df.sort_values(metric, ascending=False)\n\n    # iterate over the labels and add the colors to the color mapping dictionary, hightlight the selected company\n    color_map = {}\n    for label in df_sorted['Company Name']:\n        if label == name:\n            color_map[label] = '#A27D4F'\n        else:\n            color_map[label] = '#D9D9D9'\n\n    fig = px.bar(df_sorted, y='Company Name', x=metric, \n                        template='simple_white', color='Company Name',\n                        color_discrete_map=color_map,\n                        orientation='h',\n                        title='<b>{} {} vs Peers FY22</b>'.format(name, metric))\n    \n    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend=False, yaxis_title='')\n    return fig\n\n# plot peers graphs\nrev_peers_fig = peers_plot(df=peers_df, name=name, metric='Total Revenue')\nsg_and_a_peers_fig = peers_plot(df=peers_df, name=name, metric='SG&A % Of Revenue')\n\n\nUsing custom colors makes your company stand out.\n\nThe bar charts should look something like this:\n\n5. Use Clearbit API to get company logos\n\nAs you saw on the company overview slide, there was a logo of the researched company. Logo URLs aren‚Äôt available on Yahoo Finance, so use Clearbit instead. Just connect the company's website to \"https://logo.clearbit.com/\" with a few lines of code:\n\nfrom yahooquery import Ticker\n\nselected_ticker = 'ABC'\nticker = Ticker(selected_ticker)\n\n# get website of the selected company\nwebsite = ticker.summary_profile[selected_ticker]['website']\n\n# get logo url of the selected company\nlogo_url = '<https://logo.clearbit.com/>' + website\n\n\nNow that you have the logo URL check if it works. If it does, adjust its size and position it on a slide. To do this, use the custom function resize_image(), which places a logo image inside a container and adjusts its size while maintaining its aspect ratio. This ensures that all logos look the same despite any initial differences in size.\n\nThen save the image object locally as \"logo.png\" and retrieve it to place on a slide as a picture. You can place Plotly figures on slides in a similar manner. Use the python-pptx library to manipulate PowerPoint slides and shapes programmatically (read more here).\n\nHere is the process:\n\nThe code below uses the logo_url variable (defined in the previous code snippet):\n\nfrom PIL import Image\nimport requests\nfrom pptx import Presentation\nfrom pptx.util import Inches\nimport os\n\ndef resize_image(url):\n    \"\"\"function to resize logos while keeping aspect ratio. Accepts URL as an argument and return image object\"\"\"\n    # open image from url\n    image = Image.open(requests.get(url, stream=True).raw)\n\n    # if a logo is too high or too wide then make the background container twice as big\n    if image.height > 140:\n        container_width = 220 * 2\n        container_height = 140 * 2\n\n    elif image.width > 220:\n        container_width = 220 * 2\n        container_height = 140 * 2\n    else:\n        container_width = 220\n        container_height = 140\n\n    # create a new image with the same aspect ratio as the original image\n    new_image = Image.new('RGBA', (container_width, container_height))\n\n    # calculate the position to paste the image so that it is centered\n    x = (container_width - image.width) // 2\n    y = (container_height - image.height) // 2\n\n    # paste the image onto the new image\n    new_image.paste(image, (x, y))\n    return new_image\n\n# create presentation object\nprs = Presentation('template.pptx')\n# select second slide\nslide = prs.slides[1]\n\n# check if a logo ulr returns code 200 (working link)\nif requests.get(logo_url).status_code == 200:\n    # create logo image object\n    logo = resize_image(logo_url)\n    logo.save('logo.png')\n    logo_im = 'logo.png'\n\n    # add logo to the slide\n    slide.shapes.add_picture(logo_im, left=Inches(1.2), top=Inches(0.5), width=Inches(2))\n    os.remove('logo.png')\n\n\nRunning the code above should place a logo on the slide:\n\n6. Use LangChain and GPT 3.5 LLM to write a SWOT analysis and value proposition\n\nIt's time to use AI for your company research! ü§ñ\n\nYou'll use LangChain, a popular framework designed to simplify the creation of applications using ChatOpenAI and Human/System Message LLMs (read more here).\n\nThe generate_gpt_response() function takes two arguments:\n\ngpt_input, a prompt that you'll pass to the model\nmax_tokens, which limits the number of tokens in the model's response\n\nYou'll use the gpt-3.5-turbo-0613 model in the arguments of ChatOpenAI and retrieve the OpenAI API key stored in Streamlit secrets. You'll also set the temperature to 0 to get more deterministic responses (read more here).\n\nTo improve the quality of GPT responses, pass the following text to the SystemMessage argument: \"You are a helpful expert in finance, market, and company research. You also have exceptional skills in selling B2B software products.\" It'll set the objectives for the AI to follow (read more here).\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\ndef generate_gpt_response(gpt_input, max_tokens):\n    \"\"\"Function to generate a response from GPT-3. Takes input and max tokens as arguments and returns a response\"\"\"\n    # Create an instance of the OpenAI class\n    chat = ChatOpenAI(openai_api_key=st.secrets[\"openai_credentials\"][\"API_KEY\"], model='gpt-3.5-turbo-0613',\n                      temperature=0, max_tokens=max_tokens)\n\n    # Generate a response from the model\n    response = chat.predict_messages(\n        [SystemMessage(content='You are a helpful expert in finance, market and company research.'\n                               'You also have exceptional skills in selling B2B software products.'),\n         HumanMessage(\n             content=gpt_input)])\n\n    return response.content.strip()\n\n\nNext, let's create a prompt for the model and invoke the generate_gpt_response() function.\n\nPrompt the model to create a SWOT analysis of a specific company and return the result as a Python dictionary with this code:\n\ninput_swot = \"\"\"Create a brief SWOT analysis of {name} company with ticker {ticker}?\nReturn output as a Python dictionary with the following keys: Strengths, Weaknesses, \nOpportunities, Threats as keys and analysis as values.\nDo not return anything else.\"\"\"\n\ninput_swot = input_swot.format(name='Company 1', ticker='ABC')\n\n# return response from GPT-3\ngpt_swot = generate_gpt_response(input_swot, 1000)\n\n\nThe resulting dictionary should look something like this:\n\n{\"Strengths\": \"text\", \"Weaknesses\": \"text\", \n\"Opportunities\": \"text\", \"Threats\": \"text\"}\n\n\nSimilarly, you can prompt the GPT model to write a value proposition for a specific product for a given company. The app uses the common value proposition framework of identifying customer pains and gains, as well as gain creators and pain relievers:\n\ninput_vp = \"\"\"\"Create a brief value proposition using Value Proposition Canvas framework for {product} for \n{name} company with ticker {ticker} that operates in {industry} industry.\nReturn output as a Python dictionary with the following keys: Pains, Gains, Gain Creators, \nPain Relievers as a keys and text as values. Be specific and concise. Do not return anything else.\"\"\"\n\ninput_vp = input_vp.format(product='Accounting software', name='Company 1', ticker='ABC', industry='Retail')\n\n# return response from GPT-3\ngpt_value_prop = generate_gpt_response(input_vp, 1000)\n\n# response looks like below: \n# {\"Pains\": \"text\", \"Gains\": \"text\", \"Gain Creators\": \"text\", \"Pain Relievers\": \"text\"}\n\n7. Extract structured data from the GPT response\n\nIn the previous step, you asked the GPT model for a Python dictionary of responses. But since LLMs can sometimes produce nonsensical responses, the returned string may not always contain just the necessary dictionary. In such cases, you may need to parse the response string to extract the dictionary and convert it to the Python dictionary type.\n\nTo accomplish this, you‚Äôll need two standard libraries: re and ast.\n\nThe dict_from_string() function takes the response string from the LLM and returns a dictionary in this workflow:\n\nHere is the code:\n\nimport re\nimport ast\n\ndef dict_from_string(gpt_response):\n    \"\"\"Function to parse GPT response and convert it to a dict\"\"\"\n    # Find a substring that starts with '{' and ends with '}', across multiple lines\n    match = re.search(r'\\\\{.*?\\\\}', gpt_response, re.DOTALL)\n    dictionary = None\n    if match:\n        try:\n            # Try to convert substring to dict\n            dictionary = ast.literal_eval(match.group())\n        except (ValueError, SyntaxError):\n            # Not a dictionary\n            return None\n    return dictionary\n\nswot_dict = dict_from_string(gpt_response=gpt_swot)\nvp_dict = dict_from_string(gpt_response=gpt_value_prop)\n\n8. Generate slides using python-pptx\n\nNow that you have the data, it's time to fill out the slides. Use a PowerPoint template and replace the placeholders with actual values using the python-pptx library.\n\nHere is what the SWOT slide template should look like:\n\nTo populate the slide with data, use the replace_text() function, which takes two arguments:\n\nA dictionary with placeholders as keys and replacement text as values\nA PowerPoint slide object\n\nUse the swot_dict variable defined in the previous step:\n\nfrom pptx import Presentation\n\ndef replace_text(replacements, slide):\n    \"\"\"function to replace text on a PowerPoint slide. Takes dict of {match: replacement, ... } and replaces all matches\"\"\"\n    # Iterate through all shapes in the slide\n    for shape in slide.shapes:\n        for match, replacement in replacements.items():\n            if shape.has_text_frame:\n                if (shape.text.find(match)) != -1:\n                    text_frame = shape.text_frame\n                    for paragraph in text_frame.paragraphs:\n                        whole_text = \"\".join(run.text for run in paragraph.runs)\n                        whole_text = whole_text.replace(str(match), str(replacement))\n                        for idx, run in enumerate(paragraph.runs):\n                            if idx != 0:\n                                p = paragraph._p\n                                p.remove(run._r)\n                        if bool(paragraph.runs):\n                            paragraph.runs[0].text = whole_text\n\nprs = Presentation(\"template.pptx\")\nswot_slide = prs.slides[2]\n\n# create title for the slide\nswot_title = 'SWOT Analysis of {}'.format('Company 1')\n\n# initiate a dictionary of placeholders and values to replace\nreplaces_dict = {\n    '{s}': swot_dict['Strengths'],\n    '{w}': swot_dict['Weaknesses'],\n    '{o}': swot_dict['Opportunities'],\n    '{t}': swot_dict['Threats'],\n    '{swot_title}': swot_title}\n\n# run the function to replace placeholders with values\nreplace_text(replacements=replaces_dict, slide=swot_slide)\n\n\nIn short, the replace_text() function iterates over all shapes on a slide, looking for placeholder values, and replaces them with values from the dictionary if found.\n\nOnce all the slides have been filled with data or images, the presentation object is saved as binary output and passed to st.download_button() so that a user can download a PowerPoint file (read more here).\n\nHere's what the download button should look like on the front end:\n\nDownload the resulting PPT here:\n\nNVIDIA Corporation 2023 08 02\nNVIDIA Corporation 2023-08-02.pptx 2 MB\n\nAnd here‚Äôs the code:\n\nfrom pptx import Presentation\nfrom io import BytesIO\nfrom datetime import date\nimport streamlit as st\n\n# create file name\nfilename = '{name} {date}.pptx'.format(name='Company 1', date=date.today())\n\n# save presentation as binary output\nbinary_output = BytesIO()\nprs.save(binary_output)\n\n# display success message and download button\nst.success('The slides have been generated! :tada:')\n\nst.download_button(label='Click to download PowerPoint',\n                   data=binary_output.getvalue(),\n                   file_name=filename)\n\nWrapping up\n\nThank you for reading to the end! Now you can develop a slide automation app for company research using Streamlit, Snowflake, YahooFinance, and LangChain. I hope you found something new and useful in this post.\n\nAs you can see, there are some limitations to the app. Firstly, it only generates research on public companies. Secondly, the GPT model uses general knowledge about a product, such as ChatBot or Accounting Software, to write a value proposition. In a more advanced app, the second constraint could be addressed by fine-tuning the model with your product data. This can be done by passing your product details in a prompt or storing this data as embeddings in a vector database (read more here).\n\nIf you have any questions or feedback, please post it in the comments below or contact me on GitHub, LinkedIn, or Twitter.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Announcing Streamlit for Data Science: Second Edition",
    "url": "https://blog.streamlit.io/streamlit-for-data-science-book/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nBy Tyler Richards\nPosted in Product, September 29 2023\nWhy I wrote it\nWho is it for?\nWhat is the book about?\nHow do I get a copy?\nContents\nShare this post\n‚Üê All posts\n\nDo you love diving into one subject in depth instead of switching between Medium, Twitter, and five different documentation sites? Then check out the 2nd version of my book Streamlit for Data Science I just published!\n\nWhy I wrote it\n\nIf you‚Äôre new to Streamlit, it can feel tricky to know where to start. Do you follow people on Twitter? Start with your own project? What do you need to get started with the right foundations? How do you know if you‚Äôre missing a crucial piece of knowledge? There are so many resources and so little time.\n\nTwo years ago, I tried to solve this problem by writing the book I wanted to read when I started as a Streamlit developer.\n\nI‚Äôm very proud of it! But‚Ä¶\n\nStreamlit kept putting out release after release, making amazing changes. We got past the famous 1.0 version barrier with the introduction of new features like data editors, caching and visualization improvements, and component upgrades. As a result of this, the original book became increasingly outdated.\n\nIn addition, I left my job at Meta Integrity to join Streamlit as a data scientist, and just two months later, Streamlit was acquired by Snowflake. It was quite a whirlwind, to say the least.\n\nSo I wrote an updated version of the book. It includes new interviews with creators, power users, and employees, along with the experience I gained from building Streamlit apps with the Streamlit data team over the last year and a half. It also incorporates examples using the latest tools and platforms that have gained popularity, such as OpenAI and HuggingFace. Since the first version, I‚Äôve created hundreds of apps, and this version contains all the lessons, both big and small, that I‚Äôve learned.\n\nA lot hasn't changed too. Streamlit is still my favorite Python library, my favorite way to show off data science work. I wrote this second version as a labor of love, and I hope that you find it as lovely to read as I did to write.\n\nWho is it for?\n\nThis book is for data scientists and ML enthusiasts, especially those who are new to Streamlit or data science in general. You‚Äôll get the most value from it if you already have knowledge of Python, and it‚Äôd also be helpful (though not necessary!) to have some familiarity with popular libraries like Pandas. But I highly recommend that you first explore the Streamlit documentation to see if it meets your requirements. It‚Äôs truly delightful, and I often refer to it myself. If you find all the information you need there, simply use the docs!\n\nWhat is the book about?\n\nRoughly the book contains three sections:\n\nThe first section gives an introduction to building basic Streamlit apps. It covers visualizations, understanding the execution model, and introduces popular libraries. By the end of this section, you‚Äôll have a working app and an ML model that you can share with anyone!\nThe second section focuses on beauty and complex use cases. It covers Streamlit components, databases, animations, and generative AI. By the end of this section, you‚Äôll have enough knowledge to create production-level Streamlit apps for work or for a large audience.\nThe final section is project-based and explores the use of Streamlit in a working environment. It includes interviews with power users, discusses using Streamlit for job applications, and gives you a better understanding of Streamlit‚Äôs long-term direction.\nHow do I get a copy?\n\nYou can always snag a copy on Amazon! Use this link to get 25% (it expires on October 31st). We're also giving away 10 copies on Twitter and LinkedIn. For a chance to win: follow @Streamlit, like this post, and tag a data nerd in the comments!\n\nAll proceeds for this copy are going to be donated to PyLadies. If you can‚Äôt afford it, shoot me a DM on Twitter and I‚Äôd be happy to send you a copy.\n\nCan‚Äôt wait to hear your thoughts about the book.\n\nHappy reading! üìï\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to improve Streamlit app loading speed",
    "url": "https://blog.streamlit.io/how-to-improve-streamlit-app-loading-speed/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nBy Zachary Blackwood\nPosted in Product, October 5 2023\nMy app is too slow\nProfiling the app\nHandling the slow query\nWhat‚Äôs the general pattern?\nWill this fix my slow app?\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüëâ\nTL;DR: The article describes my experience of optimizing a slow Streamlit app that pulled data from Snowflake. The primary issue was with a specific long-running query. To solve this, I shifted the heavy data processing from the app to dbt by creating a pre-processed model, which led to a query speed improvement by 100 times. The general advice is to identify and move slow parts outside the Streamlit app if possible and to utilize caching features like st.cache_data for better performance.\nMy app is too slow\n\nA few weeks ago, I used an internal Streamlit app to check the performance of a specific feature. But it took so long to load, that I often gave up before I actually got to use the app. So I checked the code. At first glance it looked all good. We were caching the results using st.cache_data. And the code itself wasn‚Äôt very complex. It was pulling data from a few tables in Snowflake, applying filters (some using Snowpark and some using Pandas) and displaying graphs and tables.\n\nSo why did it take so long to load the first time I visited it?\n\nProfiling the app\n\nSince the app used many queries, I first wanted to find which ones were slow.\n\nWe have a set of standard functions for querying and caching data from Snowflake, that look roughly like this:\n\nI modified them so that, in addition to running the queries and caching the results, they‚Äôd also tell me how long each query took to run.\n\nThe final result looked like this:\n\nAdding time() calls before and after the query and using st.write() to display the time difference helped me find the slow parts of the app. I could‚Äôve used python -m cProfile streamlit_app.py to dig deeper, but it wasn't necessary in this case. Now I could start investigating.\n\nHandling the slow query\n\nI identified the slow parts of the query with Snowflake's Query Profiler. As you can see below, a specific Aggregate step took up a lot of time time. By clicking on it, I can learn more about what‚Äôs happening.\n\nIn this case, the slow query was performing two tasks that required significant processing:\n\nRetrieving data from a large table\nPerforming complex transformations on the data\n\nThese transformations involved filtering the data based on multiple columns to obtain the latest rows and calculating statistics related to a Variant column. While the actual process was more intricate, this is a general overview of what was happening.\n\nTo speed up the query, I created a new model in dbt that:\n\nFiltered the source table to include only the latest values each day.\nAdded a new column that pre-computed the list of values for the specific \"key\" of interest.\n\nThe process of flattening the data looked roughly like this:\n\nwith flattened_keys as (\n    select\n        id,\n        flattened.value:\"KEY\"::string as value\n    from\n        BASE_TABLE as BT,\n        lateral flatten(input => parse_json(p.COLUMN)) as flattened\n    where\n        flattened.value:\"KEY\" is not null\n),\n\nflattened_array as (\n    select\n        id,\n        array_agg(value) as value_array\n    from\n        flattened_keys\n    group by\n        id\n),\n\nnew_table as (\n    select\n        f.value_array,\n        BT.*\n    from BASE_TABLE as BT\n        left join flattened_arrays as f\n        using (id)\n)\n\nselect * from new_table\n\n\nOnce I had the model and it was populated with data, I switched my app to pull from the new table, and I simplified the query that was performing computations by using this well-prepared table.\n\nThe app's query before:\n\nwith transformed as (\n\tselect \n\t\tid,\n\t\tcount(flattened.value:KEY) AS num_values,\n\t\tcount(distinct flattened.value:KEY) AS num_unique_values\n\tfrom BASE_TABLE,\n\t\tlateral flatten(BASE_TABLE.COLUMN) as flattened\n\tgroup by id\n\t{extra_filters}\n)\n\nselect \n\ttransformed.num_values,\n\ttransformed.num_unique_values\n\tBASE_TABLE.*,\nfrom BASE_TABLE\n\tinner join transformed \n\tusing(id)\n\n\nThe query after:\n\nselect\n    *,\n    array_size(value_array) as num_values,\n    array_size(array_distinct(value_array)) as num_unique_values,\nfrom new_table\n{extra_filters}\n\n\nThe result? The new query was 100 times faster than the old one! This improved the app‚Äôs usability and expanded the range of analyses that we could easily perform in a reasonable amount of time.\n\nWhat‚Äôs the general pattern?\n\nIf your Streamlit app is slow, try moving the slowest parts outside of the app‚Äîlike into a pre-processed table with dbt.\n\nWill this fix my slow app?\n\nPerhaps! It depends on the specific problem. Here are a few scenarios and the approaches you might take to resolve them:\n\nMy app is query-driven and the first run is very slow. Try pre-computing the table you will need outside of your app like I did above.\nMy app is slow every time I run it. Are you using st.cache_data or st.cache_resource to prevent rerunning slow processes? For more information, read the Streamlit documentation on caching.\nMy app pulls data from a CSV, not from a database. Consider performing a similar process to what I did with my data. Get the raw data, execute queries to process it, and save the pre-processed file for use in your app. For faster loading, use a Parquet file format instead of CSV.\nMy app is slow for a different reason. Try figuring out which part is the slowest, and if you can find a way to do the slowest part of the work outside of your app itself. For example, if your app trains a machine learning model, try moving the model training outside of the app itself, save the model once it‚Äôs trained, and then use the pre-trained model to make predictions within your app.\nWrapping up\n\nIf your Streamlit app is too slow, consider adding profiling to identify the slow parts, especially data queries. We sped up our apps by offloading resource-intensive tasks to dbt and optimizing the problematic query. By moving heavy workloads outside the app, you can create a faster and more user-friendly experience.\n\nIf you have any questions, please post them in the comments below or contact me on Twitter.\n\nHappy app-fixing! üõ†Ô∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tyler Richards - Streamlit",
    "url": "https://blog.streamlit.io/author/tyler/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Tyler Richards\n4 posts\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nTutorials\nby\nTyler Richards and¬†\n2\n¬†more,\nAugust 18 2022\nHow Streamlit uses Streamlit: Sharing contextual apps\n\nLearn about session state and query parameters!\n\nTutorials\nby\nTyler Richards\n,\nMay 26 2022\nDeploying Streamlit apps using Streamlit sharing\n\nA sneak peek into Streamlit's new deployment platform\n\nTutorials\nby\nTyler Richards\n,\nOctober 15 2020\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "AI Interviewer: Customized interview preparation with generative AI",
    "url": "https://blog.streamlit.io/ai-interviewer-customized-interview-preparation-with-generative-ai/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nAI Interviewer: Customized interview preparation with generative AI\n\nHow we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!\n\nBy Haoxiang Jia and Todd Wang\nPosted in LLMs, August 9 2023\nWhy AI Interviewer?\nApp overview\n1. Create a chatbot interviewer\n2. Create vector embeddings and initialize session state\n3. Generate interview questions with a similarity search\nOpenAI Whisper\nAmazon Polly\n4. Build your app!\nWrapping up\nContents\nShare this post\n‚Üê All posts\nü§ñ\nTL;DR: The AI Interviewer app generates job-specific interview questions, covers diverse interview types, and offers personalized evaluations. The development involved creating vector embeddings, establishing interview guidelines, managing session states, and integrating optional voice interactions. Here is the GitHub code. Have fun!\n\nHey, community! üëã\n\nWe‚Äôre two graduating MSBA students at Fordham University, Todd Wang (also known as Zicheng Wang) and Haoxiang Jia. Both of us struggle with interview preparations, so we discovered a shared goal: to make generative AI play an interviewer for us!\n\nIn this post, we‚Äôll show you how you can build your own AI Interviewer:\n\nCreate a chatbot interviewer\nCreate vector embeddings and initialize session state\nCreate a callback function that memorizes conversation history\nBuild your app!\n\n\nWhy AI Interviewer?\n\nWhen we started looking for jobs, we saw lots of job listings under the same title but with different descriptions. Many graduates resort to a \"one size fits all\" approach, but that meant potentially missing a particular job‚Äôs requirements.\n\nAfter much thought, we decided to build an app that could help with:\n\nCustomized interview preparation. Just type in a job description and get targeted interview questions that simulate a real-life interview (no need for prompt engineering). This helps you prepare for interviews perfectly aligned with every job posting.\n\nComprehensive coverage. Prepare for various types of interviews:\n\nProfessional interview‚Äîfocuses on technical skills and industry knowledge. For example, you may be asked to design and implement a feature.\nBehavioral interview‚Äîfocuses on how the candidate handles specific situations in the workplace. For example, you may be asked to describe how you dealt with a difficult coworker.\nResume interview‚Äîfocuses on the candidate's work experience, education, and skills as listed on their resume. For example, you may be asked to elaborate on past job responsibilities or explain a particular accomplishment.\n\nPersonalized evaluation and guidance. Get interview evaluations with feedback and actionable insights to refine your interview skills and improve your performance!\n\nApp overview\n\nBefore we dive into coding, let‚Äôs take a look at a two-step development process instead of a single RetrievalQA step (it leads to better performance):\n\nCreating vector embeddings. This is a technique of representing words as numbers to make them more computationally accessible.\nDeveloping an interviewer guideline. This requires careful planning to ensure that the questions asked are relevant and comprehensive. Once the guidelines are established, the actual interview can take place.\n\nNow, let‚Äôs get to coding!\n\n1. Create a chatbot interviewer\n\nTo start, create an efficient chatbot interviewer that can help you develop prompt templates and save them for later use.\n\nUse this template to construct an interview guide for behavioral screening:\n\nclass templates: \n\t\"\"\" store all prompts templates \"\"\"\n\tbehavioral_template = \"\"\" I want you to act as an interviewer. Remember, you are the interviewer not the candidate.   \n            Let's think step by step.\n            \n            Based on the keywords, \n            Create a guideline with the following topics for a behavioral interview to test the soft skills of the candidate. \n            \n            Do not ask the same question.\n            Do not repeat the question. \n            \n            Keywords: \n            {context}\n       \n            Question: {question}\n            Answer:\"\"\"\n\n\tconversation_template = \"\"\"I want you to act as an interviewer strictly following the guideline in the current conversation.\n                            Candidate has no idea what the guideline is.\n                            Ask me questions and wait for my answers. Do not write explanations.\n                            Ask each question like a real person, only one question at a time.\n                            Do not ask the same question.\n                            Do not repeat the question.\n                            Do ask follow-up questions if necessary. \n                            Your name is GPTInterviewer.\n                            I want you to only reply as an interviewer.\n                            Do not write all the conversation at once.\n                            If there is an error, point it out.\n\n                            Current Conversation:\n                            {history}\n\n                            Candidate: {input}\n                            AI: \"\"\")\n\n\n\nü§ñ\nNOTE: Context refers to the embeddings of keywords or job descriptions entered.\n2. Create vector embeddings and initialize session state\n\nNext, create a function that utilizes FAISS to generate vector embeddings. Since the job description or resume text isn‚Äôt long, use the NLTKTextSplitter instead of RecursiveCharacterTextSplitter for better results. For longer texts, split them into chunks and process them individually to avoid loss of information or context.\n\nWhen working with vector embeddings, use the appropriate text splitter and chunk size to get desired results:\n\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import NLTKTextSplitter\njd = st.text_area(\"Please enter the job description here ()\")\n\n'''The variable \"jd\" was passed to the embeddings function later on.'''\n\ndef embeddings(text):\n\ttext_splitter = NLTKTextSplitter()\n\ttexts = text_splitter.split_text(text)\n\tembeddings = OpenAIEmbeddings()\n\tdocsearch = FAISS.from_texts(texts, embeddings)\n\tretriever = docsearch.as_retriever(search_tupe='similarity search')\n\treturn retriever\n\n\nThe function returns a variable called \"retriever\", which users can later use to generate interview questions that are relevant, targeted, and effective.\n\nIt‚Äôs also important to distinguish between messages from AI and humans. Use specific markers or tags to identify the message source, properly attribute each message, and maintain clear communication with their interviewees:\n\nfrom dataclasses import dataclass\nfrom typing import Literal\n\n@dataclass\nclass Message:\n\torigin: Literal[\"human\", \"ai\"]\n\tmessage: str\n\n\nNow, let's write a function to initialize session states:\n\nfrom langchain\n\ndef initialize_session_state():\n    if \"retriever\" not in st.session_state:\n        st.session_state.retriever = embeddings(jd)\n\n    if \"chain_type_kwargs\" not in st.session_state:\n        Behavioral_Prompt = PromptTemplate(input_variables=[\"context\", \"question\"],\n                                          template=templates.behavioral_template)\n        st.session_state.chain_type_kwargs = {\"prompt\": Behavioral_Prompt}\n    # interview history\n    if \"history\" not in st.session_state:\n        st.session_state.history = []\n        st.session_state.history.append(Message(\"ai\", \"Hello there! I am your interviewer today. I will access your soft skills through a series of questions. Let's get started! Please start by saying hello or introducing yourself. Note: The maximum length of your answer is 4097 tokens!\"))\n\n    # token count\n    if \"token_count\" not in st.session_state:\n        st.session_state.token_count = 0\n    if \"memory\" not in st.session_state:\n        st.session_state.memory = ConversationBufferMemory()\n    if \"guideline\" not in st.session_state:\n        llm = ChatOpenAI(\n            model_name=\"gpt-3.5-turbo\",\n            temperature=0.8)\n        st.session_state.guideline = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type_kwargs=st.session_state.chain_type_kwargs, chain_type='stuff',\n            retriever=st.session_state.retriever, memory=st.session_state.memory).run(\n            \"Create an interview guideline and prepare total of 8 questions. Make sure the questions test the soft skills\")\n  \n    if \"conversation\" not in st.session_state:\n        llm = ChatOpenAI(\n        model_name = \"gpt-3.5-turbo\",\n        temperature = 0.8)\n        PROMPT = PromptTemplate(\n            input_variables=[\"history\", \"input\"],\n            template=templates.conversation_template)\n        st.session_state.conversation = ConversationChain(prompt=PROMPT, llm=llm,\n                                                       memory=st.session_state.memory)\n    if \"feedback\" not in st.session_state:\n        llm = ChatOpenAI(\n        model_name = \"gpt-3.5-turbo\",\n        temperature = 0.5)\n        st.session_state.feedback = ConversationChain(\n            prompt=PromptTemplate(input_variables = [\"history\", \"input\"], template = templates.feedback_template),\n            llm=llm,\n            memory = st.session_state.memory\n        )\n\n\nSession state has several features that provide a complete interview experience:\n\nChain_type_kwargs: responsible for creating a customizable prompt template for the RetrievalQA chain (enables the system to generate questions tailored to the specific job description).\nHistory: keeps track of the messages exchanged during the interview (helps the system to maintain context and avoid asking repetitive questions).\nToken_count: keeps track of the number of tokens consumed during the interview (measures the interview's duration and ensures that the system doesn‚Äôt ask too many questions).\nMemory: initializes a memory buffer to keep track of the context (helps the system to remember important details about the candidate's qualifications and experience).\nGuideline: uses the RetrievalQA chain to generate an interview guideline based on the job description (provides a framework for the interview and ensures that all relevant topics are covered).\nConversation: uses the conversation chain to conduct the interview based on the guideline (enables the system to ask follow-up questions and engage in a more natural conversation with the candidate).\nFeedback: uses the LLM chain to generate feedback based on the context (provides the candidate with valuable insights into their performance during the interview and helps them to improve their skills for future interviews).\n3. Generate interview questions with a similarity search\n\nNow that you‚Äôve completed the initialization steps for your conversation chain, let‚Äôs move on to the callback function‚Äîthe backbone of our chatbot (without it, our chatbot wouldn‚Äôt be able to talk to users).\n\nThe callback function takes the user's text input (the latest response saved to and pulled from st.session_state.history) and applies the necessary logic to generate an appropriate response. Once it‚Äôs generated, it gets appended to the conversation history. This allows the chatbot to remember the context of previous messages.\n\nBut that's not all!\n\nWe‚Äôve also added an optional voice interaction feature. It allows the user to listen to the chatbot's response and input their own responses using their voice. This can be incredibly helpful for users who wish to practice their speaking skills. The audio widget is returned by the callback function, giving the user the option to listen to the chatbot's response instead of reading it.\n\nfrom IPython.display import Audio\n\ndef answer_call_back():\n    with get_openai_callback() as cb:\n        # user input\n        human_answer = st.session_state.answer\n        # transcribe audio\n        if voice:\n            save_wav_file(\"temp/audio.wav\", human_answer)\n            try:\n                input = transcribe(\"temp/audio.wav\")\n                # save human_answer to history\n            except:\n                st.session_state.history.append(Message(\"ai\", \"Sorry, I didn't get that. Please try again.\"))\n        else:\n            input = human_answer\n\n        st.session_state.history.append(\n            Message(\"human\", input)\n        )\n        # OpenAI answer and save to history\n        llm_answer = st.session_state.conversation.run(input)\n        # speech synthesis and speak out\n        audio_file_path = synthesize_speech(llm_answer)\n        # create audio widget with autoplay\n        audio_widget = Audio(audio_file_path, autoplay=True)\n        # save audio data to history\n        st.session_state.history.append(\n            Message(\"ai\", llm_answer)\n        )\n        st.session_state.token_count += cb.total_tokens\n        return audio_widget\n\t\t\t\t\n\n\nWe‚Äôve defined the \"transcribe\" function for Speech-to-Text and \"Audio\" for Text-to-Speech. There are many APIs available for performing these tasks. Specifically, we used OpenAI Whisper for Speech-to-Text and Amazon Polly for Text-to-Speech.\n\nOpenAI Whisper\nimport wave\nimport os \nimport openai\nclass Config:\n    channels = 2\n    sample_width = 2\n    sample_rate = 44100\n\ndef save_wav_file(file_path, wav_bytes):\n    with wave.open(file_path, 'wb') as wav_file:\n        wav_file.setnchannels(Config.channels)\n        wav_file.setsampwidth(Config.sample_width)\n        wav_file.setframerate(Config.sample_rate)\n        wav_file.writeframes(wav_bytes)\n\ndef transcribe(file_path):\n    audio_file = open(file_path, 'rb')\n    transcription = openai.Audio.transcribe(\"whisper-1\", audio_file)\n    return transcription['text']\n\nAmazon Polly\nimport boto3\nfrom contextlib import closing\nimport sys\nfrom tempfile import gettempdir \n\nSession = boto3.Session(\n        region_name = \"us-east-1\"\n    )\n\ndef synthesize_speech(text):\n    Polly = Session.client(\"polly\")\n    response = Polly.synthesize_speech(\n        Text=text,\n        OutputFormat=\"mp3\",\n        VoiceId=\"Joanna\")\n    if \"AudioStream\" in response:\n        # Note: Closing the stream is important because the service throttles on the\n        # number of parallel connections. Here we are using contextlib.closing to\n        # ensure the close method of the stream object will be called automatically\n        # at the end of the with statement's scope.\n        with closing(response[\"AudioStream\"]) as stream:\n            output = os.path.join(gettempdir(), \"speech.mp3\")\n\n            try:\n                # Open a file for writing the output as a binary stream\n                with open(output, \"wb\") as file:\n                    file.write(stream.read())\n            except IOError as error:\n                # Could not write to file, exit gracefully\n                print(error)\n                sys.exit(-1)\n    else:\n        # The response didn't contain audio data, exit gracefully\n        print(\"Could not stream audio\")\n        sys.exit(-1)\n    '''\n    # Play the audio using the platform's default player\n    if sys.platform == \"win32\":\n        os.startfile(output)\n    else:\n        # The following works on macOS and Linux. (Darwin = mac, xdg-open = linux).\n        opener = \"open\" if sys.platform == \"darwin\" else \"xdg-open\"\n        subprocess.call([opener, output])\n\n\n\nü§ñ\nNOTE: You can store your API keys in .streamlit/secrets.toml. This is a convenient and secure way to manage your API keys.\n4. Build your app!\n\nWith the initialization function and callback function in place, you can build your app!\n\nHere is the code:\n\n# submit job description\njd = st.text_area(\"Please enter the job description here (If you don't have one, enter keywords, such as PostgreSQL or Python instead): \")\n# auto play audio\nauto_play = st.checkbox(\"Let AI interviewer speak! (Please don't switch during the interview)\")\n\nif jd:\n    # initialize session states\n    initialize_session_state()\n\t\t# feedback requested button \n\t\tfeedback = st.button(\"Get Interview Feedback\")\n\n    token_placeholder = st.empty()\n    chat_placeholder = st.container()\n    answer_placeholder = st.container()\n\n\t  # initialize an audio widget with None \n\t\taudio = None\n\t\t\n\t\t# if feedback button has been clicked, run the feedback chain and terminate the interview\n    if feedback:\n        evaluation = st.session_state.feedback.run(\"please give evalution regarding the interview\")\n        st.markdown(evaluation)\n        st.stop()\n    else:\n        with answer_placeholder:\n\t\t\t\t\t\t# choose the way of input \n            voice: bool = st.checkbox(\"I would like to speak with AI Interviewer\")\n            if voice:\n\t\t\t\t\t\t\t\t# audio input \n                answer = audio_recorder(pause_threshold = 2.5, sample_rate = 44100)\n            else:\n\t\t\t\t\t\t\t\t# message input\n                answer = st.chat_input(\"Your answer\")\n\t\t\t\t\t\t# run the callback function, generate response, and return a audio widget\n\t\t\t\t\t\tif answer:\n                st.session_state['answer'] = answer\n                audio = answer_call_back()\n\n        # chat_placeholder is use to display the chat history\n\t\t\t\twith chat_placeholder:\n            for answer in st.session_state.history:\n                if answer.origin == 'ai':\n\t\t\t\t\t\t\t\t\t\t# if user choose auto play, return both AI outputs and its audio\n                    if auto_play and audio:\n                        with st.chat_message(\"assistant\"):\n                            st.write(answer.message)\n                            st.write(audio)\n                    else:\n\t\t\t\t\t\t\t\t\t\t# only return AI outputs\n                        with st.chat_message(\"assistant\"):\n                            st.write(answer.message)\n                else:\n\t\t\t\t\t\t\t\t\t\t# user inputs \n                    with st.chat_message(\"user\"):\n                        st.write(answer.message)\n\t\t\t\t\n\t\t\t\t# keep track of token consumed \n\t\t    token_placeholder.caption(f\"\"\"\n        Used {st.session_state.token_count} tokens \"\"\")\nelse:\n    st.info(\"Please submit a job description to start the interview.\")\n\n\nAnd here is the app in action!\n\nWrapping up\n\nThank you for reading our post. We hope it has inspired you to make your own Streamlit app. If you have any questions, please post them in the comments below or contact us on LinkedIn or Twitter.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Zachary Blackwood - Streamlit",
    "url": "https://blog.streamlit.io/author/zachary_b/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Zachary Blackwood\n3 posts\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nHow to build your own Streamlit component\n\nLearn how to make a component from scratch!\n\nTutorials\nby\nZachary Blackwood\n,\nSeptember 15 2022\nAuto-generate a dataframe filtering UI in Streamlit with filter_dataframe!\n\nLearn how to add a UI to any dataframe\n\nTutorials\nby\nTyler Richards and¬†\n2\n¬†more,\nAugust 18 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts: Using LLMs with Streamlit",
    "url": "https://blog.streamlit.io/tag/llms/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in LLMs\n33 posts\nDecoding Warren Buffett with LLMs and Snowflake SQL\n\nHow I built Ask the Oracle of Omaha in less than a day!\n\nLLMs\nby\nRandy Pettus\n,\nAugust 1 2023\nChat2VIS: AI-driven visualisations with Streamlit and natural language\n\nLeverage ChatGPT for Python code generation using prompt engineering\n\nLLMs\nby\nPaula Maddigan\n,\nJuly 27 2023\nLangChain tutorial #5: Build an Ask the Data app\n\nLeverage Agents in LangChain to interact with pandas DataFrame\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\nHow to build a Llama 2 chatbot\n\nExperiment with this open-source LLM from Meta\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\nBeginner‚Äôs guide to OpenAI¬†API\n\nBuild your own LLM tool from¬†scratch\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 20 2023\nHow to build an interconnected multi-page Streamlit app\n\nFrom planning to execution‚Äîhow I built GPT lab\n\nLLMs\nby\nDave Lin\n,\nJuly 19 2023\nLangChain ü§ù Streamlit\n\nThe initial integration of Streamlit with LangChain and our future plans\n\nLLMs\nby\nJoshua Carroll\n,\nJuly 11 2023\nGenerate interview questions from a candidate‚Äôs tweets\n\nMake an AI assistant to prepare for interviews with LangChain and Streamlit\n\nLLMs\nby\nGreg Kamradt\n,\nJune 24 2023\nLangChain tutorial #4: Build an Ask the Doc app\n\nHow to get answers from documents using embeddings, a vector store, and a question-answering chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 20 2023\nBuilding a Streamlit and scikit-learn app with ChatGPT\n\nCatching up on coding skills with an AI assistant\n\nLLMs\nby\nMichael Hunger\n,\nJune 16 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Oleksandr Arsentiev - Streamlit",
    "url": "https://blog.streamlit.io/author/oleksandr/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Oleksandr Arsentiev\n1 post\nInstant Insight: Generate data-driven presentations in a snap!\n\nCreate presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery\n\nLLMs\nby\nOleksandr Arsentiev\n,\nAugust 2 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Exploring LLMs and prompts: A guide to the PromptTools Playground",
    "url": "https://blog.streamlit.io/exploring-llms-and-prompts-a-guide-to-the-prompttools-playground/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nExploring LLMs and prompts: A guide to the PromptTools Playground\n\nLearn how to build dynamic, stateful applications that harness multiple LLMs at once\n\nBy Steve Krawczyk and Kevin Tse\nPosted in LLMs, August 18 2023\nWhy PromptTools Playground?\nHow it can help you\nExample use case\n1. Creating a grid for inputs and outputs\n2. Building a dynamic sidebar for app configuration\n3. Creating shareable links\nBest practices for using PromptTools\nWrapping up\nContents\nShare this post\n‚Üê All posts\nü¶â\nTL;DR: The PromptTools Playground app allows developers to experiment with multiple prompts and models simultaneously using large language models (LLMs) and vector databases. The post details important features, such as creating grids for inputs and outputs, building dynamic sidebars for app configuration, and enabling shareable links. You can find the code on GitHub.\n\n\n\nHey Streamlit community! üëã\n\nI'm Steve Krawczyk, one of the co-founders of Hegel AI. We build open-source tools for working with large language models (LLMs) and vector databases. In this post, I'll provide insights into our PromptTools Playground, a project we've created to allow developers to experiment with multiple prompts and models simultaneously using our open-source library.\n\nI'll guide you through the app's features and the ways we've implemented them:\n\nCreating a grid for inputs and outputs\nBuilding a dynamic sidebar for app configuration\nCreating shareable links (experimental)\n\nWhy PromptTools Playground?\n\nPromptTools is a library designed for experimenting with, testing, and evaluating LLMs and VectorDBs. It provides a user-friendly interface for constructing and executing requests to LLMs. And it speeds up our work with LLMs and prompts for our applications and to compare new open-source models with GPT-3.5 and 4.\n\nHow it can help you\nFor some use cases, running an instance of Llama 2 on your laptop may suffice. But for others, you might need to use OpenAI. If you experiment early in development, you can determine the optimal fit to improve performance and save costs.\nOptimizing prompts can be challenging. You can make informed decisions quickly by experimenting with multiple prompts and comparing responses. We support various model providers, including OpenAI, Anthropic, HuggingFace, Google, and local models through LlamaCpp. Users can compare templates, instructions, and messages and evaluate models head-to-head in the same table.\nSetting up and running such experiments can require writing custom code for each LLM and prompting strategy you wish to try. PromptTools simplifies the process of testing LLMs and prompts without investing significant upfront time and effort‚Äîthey abstract the less relevant parts, allowing you to focus on what matters.\nExample use case\n\nSome of our users use LLMs to generate outbound sales emails. They experiment with various prompt templates that include variables, then pass them into an LLM to create the email. For example, they may use templates such as the following:\n\nYou are a sales development representative at {{firm_name}}, which sells {{product_description}}. Your job is to write outbound emails to prospects given some information about them. Using the following details, write an email to the given prospect: Name: {{prospect_name}} Role: {{prospect_role}} Company: {{prospect_company}}\n\nUsers typically test different versions of a template across many cases to determine which one will work best when they take the prompts into production.\n\nBefore PromptTools, they'd run the prompts one at a time on a single model, and the results had to be manually tracked. With PromptTools, they can run all prompts simultaneously, and the results can be viewed on a single page. This speeds up the iteration process, whether you're using LLMs for productivity and drafting or testing a prompt for a production system.\n\nNow let's get to building the app.\n\n1. Creating a grid for inputs and outputs\n\nThe core idea behind our UI is that users can iterate over two lists simultaneously to experiment with LLM inputs, such as system and user messages, prompt templates and variables, or models and prompts. We then utilize the Cartesian product of these lists to craft a set of requests that are sent and displayed for manual evaluation.\n\nAt first, we tried to use the dataframe component. But it was hard to align the dataframe with the input cells to create a \"table\" of inputs and outputs. So we switched to columns and placeholder cells for the output. By default, the components of a Streamlit app are displayed vertically. We used columns to arrange the components horizontally as well.\n\nHere is the code to display the outputs:\n\n    # Add placeholders for output\n    placeholders = [[st.empty() for _ in range(instruction_count + 1)] for _ in range(prompt_count)]\n\n    cols = st.columns(instruction_count + 1)\n\n    # Create top row for instructions or system messages\n    with cols[0]:\n        a = None\n    instructions = []\n    for j in range(1, instruction_count + 1):\n        with cols[j]:\n            instructions.append(\n                st.text_area(\n                    \"System Message\",\n                    value=\"You are a helpful AI assistant.\",\n                    key=f\"col_{j}\",\n                )\n            )\n\n    # Create rows for prompts, and output placeholders\n    prompts = []\n    for i in range(prompt_count):\n        cols = st.columns(instruction_count + 1)\n        with cols[0]:\n            prompts.append(\n                st.text_area(\n                    \"User Message\",\n                    key=f\"row_{i}\"\n                )\n            )\n        for j in range(1, instruction_count + 1):\n            with cols[j]:\n                placeholders[i][j] = st.empty()  # placeholders for the future output\n        st.divider()\n\n\nTo make a table listing instructions and prompts, we created a column for each instruction and another for prompts. Next, we iterated over the number of rows equal to the number of prompts. Each row contained a prompt input cell and as many output cells as there were instructions.\n\nHere is the resulting table:\n\nThis allows our users to easily visualize how a system and user message are mapped onto the outputs we create.\n\n2. Building a dynamic sidebar for app configuration\n\nStreamlit has a handy sidebar component where most configuration UI is kept separate from system and user messages. In our case, the inputs displayed on this sidebar are dynamic. When running the app in \"Instruction\" mode, you'll see something like this:\n\nBut, if you're running in \"Model Comparison\" mode, you'll need to enter all the API keys for the models you're testing on the sidebar and configure the individual models in the first row of the table.\n\nTo implement a sidebar with dynamic input fields, use this code:\n\nwith st.sidebar:\n    mode = st.radio(\"Choose a mode\", MODES)\n    if mode != \"Model Comparison\":\n        model_type = st.selectbox(\n            \"Model Type\", MODEL_TYPES\n        )\n        model, api_key = None, None\n        if model_type in {\"LlamaCpp Chat\", \"LlamaCpp Completion\"}:\n            model = st.text_input(\"Local Model Path\", key=\"llama_cpp_model_path\")\n        elif model_type == \"OpenAI Chat\":\n            model = st.selectbox(\n                \"Model\",\n                OPENAI_CHAT_MODELS\n            )\n            api_key = st.text_input(\"OpenAI API Key\", type='password')\n        elif model_type == \"OpenAI Completion\":\n            model = st.selectbox(\n                \"Model\",\n                OPENAI_COMPLETION_MODELS\n            )\n            api_key = st.text_input(\"OpenAI API Key\", type='password')\n    else:\n        model_count = st.number_input(\"Add Model\", step=1, min_value=1, max_value=5)\n        prompt_count = st.number_input(\"Add Prompt\", step=1, min_value=1, max_value=10)\n        openai_api_key = st.text_input(\"OpenAI API Key\", type='password')\n        anthropic_api_key = st.text_input(\"Anthropic Key\", type='password')\n        google_api_key = st.text_input(\"Google PaLM Key\", type='password')\n        hf_api_key = st.text_input(\"HuggingFace Hub Key\", type='password')\n\n\nIt's important to understand the Streamlit execution model: after every user interaction, the app reruns top-to-bottom. So you need to organize your UI components in order of conditional dependency, with components at the top determining which components we display later.\n\n3. Creating shareable links\n\nOur application also enables users to create and share links for their experiment setup with others. This is done by using the experimental_get_query_params() function from Streamlit:\n\nparams = st.experimental_get_query_params()\n\n\nNext, set defaults using the provided query parameters:\n\nif 'mode' not in st.session_state and 'mode' in params:\n        st.session_state.mode = unquote(params[\"mode\"][0])\nmode = st.radio(\"Choose a mode\", MODES, key=\"mode\")\n\n\nOne concern with using query parameters as defaults is that users may need to edit these inputs. Streamlit reruns every time a user interacts with the app, causing the query parameters to be reread and used to set input values every time. To prevent this, modify the app to clear the query parameters after reading them. This way, they're only used to populate the app once:\n\nparams = st.experimental_get_query_params()\nst.experimental_set_query_params()\n\n\nUsers can make changes by setting these values once without modifying the code repeatedly.\n\nü¶â\nIf you‚Äôre working locally, you can use pyperclip to copy the link to the clipboard. But if you‚Äôve deployed your app to Streamlit Community Cloud, note that pyperclip can‚Äôt directly copy to a clipboard from the app. Instead, use st.code to display the link, which users can then copy.\n\n\nif share:\n    try:\n        pyperclip.copy(link)\n    except pyperclip.PyperclipException:\n        st.write(\"Please copy the following link:\")\n        st.code(link)\n\n\nThis is the easiest way to make links shareable for locally and cloud-hosted app versions.\n\nBest practices for using PromptTools\n\nWhen writing and evaluating prompts for LLMs, there are a few things to keep in mind:\n\nThe same prompts can perform differently on different models. So it's worth trying multiple prompts when comparing models and checking where each model's strengths lie.\nParameters like temperature can have just as much of an impact on the model output as the prompt itself. If a model isn't giving you the desired results after trying multiple prompts, try tweaking other settings in the sidebar to see how they impact the model's response quality.\nThe evaluation depends on the specific use case. While benchmarks can generally indicate a model's power, you won't know if the model can perform well on your particular use case until you create a test set and decide on your evaluation criteria. If you want to run evaluations automatically, you can use our SDK to run experiments at scale.\n\nPrompt engineering still requires patience and experimentation, but with PromptTools, you can move faster from your first idea to a working prompt and model!\n\nWrapping up\n\nWe used a few Streamlit tricks to make our app and plan to introduce more features. We hope this will be of great benefit to the community, and we'll continue to develop new features, including support for experimentation with vector databases and chains, storing experiments in a database using st.experimental_connection, and loading test data from files or other sources.\n\nIf you found this interesting or helpful, check out the app, give our GitHub repo a star ‚≠ê, and join our Discord community. Refer to our notebook examples when you're ready to move from the playground to production code. We provide functionality to auto-evaluate and validate your outputs.\n\nHappy coding! üòé\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Build a chatbot with custom data sources, powered by LlamaIndex",
    "url": "https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nBy Caroline Frasca, Krista Muir and Yi Ding\nPosted in LLMs, August 23 2023\nWhat is LlamaIndex?\nHow to build a custom chatbot using LlamaIndex\nIn 43 lines of code, this app will:\n1. Configure app secrets\n2. Install dependencies\n2.1. Local development\n2.2. Cloud development\n3. Build the app\n3.1. Import libraries\n3.2. Initialize message history\n3.3. Load and index data\n3.4. Create the chat engine\n3.5. Prompt for user input and display message history\n3.6. Pass query to chat engine and display response\n4. Deploy the app!\nLlamaIndex helps prevent hallucinations\nWhat is Streamlit‚Äôs experimental connection feature?\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüéà\nTL;DR: Learn how LlamaIndex can enrich your LLM model with custom data sources through RAG pipelines. Build a chatbot app using LlamaIndex to augment GPT-3.5 with Streamlit documentation in just 43 lines of code.\n\nSo, you want to build a reliable chatbot using LLMs based on custom data sources?\n\nModels like GPT are excellent at answering general questions from public data sources but aren't perfect. Accuracy takes a nose dive when you need to access domain expertise, recent data, or proprietary data sources.\n\nEnhancing your LLM with custom data sources can feel overwhelming, especially when data is distributed across multiple (and siloed) applications, formats, and data stores.\n\nThis is where LlamaIndex comes in.\n\nLlamaIndex is a flexible framework that enables LLM applications to ingest, structure, access, and retrieve private data sources. The end result is that your model's responses will be more relevant and context-specific. Together with Streamlit, LlamaIndex empowers you to quickly create LLM-enabled apps enriched by your data. In fact, the LlamaIndex team used Streamlit to prototype and run experiments early in their journey, including their initial proofs of concept!\n\nIn this post, we'll show you how to build a chatbot using LlamaIndex to augment GPT-3.5 with Streamlit documentation in four simple steps:\n\nConfigure app secrets\nInstall dependencies\nBuild the app\nDeploy the app!\nWhat is LlamaIndex?\n\nBefore we get started, let's walk through the basics of LlamaIndex.\n\nBehind the scenes, LlamaIndex enriches your model with custom data sources through Retrieval Augmented Generation (RAG).\n\nOverly simplified, this process generally consists of two stages:\n\nAn indexing stage. LlamaIndex prepares the knowledge base by ingesting data and converting it into Documents. It parses metadata from those documents (text, relationships, and so on) into nodes and creates queryable indices from these chunks into the Knowledge Base.\nA querying stage. Relevant context is retrieved from the knowledge base to assist the model in responding to queries. The querying stage ensures the model can access data not included in its original training data.\n\nüí¨\nLlamaIndex for any level: Tasks like enriching models with contextual data and constructing RAG pipelines have typically been reserved for experienced engineers, but LlamaIndex enables developers of all experience levels to approach this work. Whether you‚Äôre a beginner looking to get started in three lines of code, LlamaIndex unlocks the ability to supercharge your apps with both AI and your own data. For more complex applications, check out Llama Lab.\n\nNo matter what your LLM data stack looks like, LlamaIndex and LlamaHub likely already have an integration, and new integrations are added daily. Integrations with LLM providers, vector stores, data loaders, evaluation providers, and agent tools are already built.\n\nLlamaIndex's Chat Engines pair nicely with Streamlit's chat elements, making building a contextually relevant chatbot fast and easy.\n\nLet's unpack how to build one.\n\nHow to build a custom chatbot using LlamaIndex\nü¶ô\nWant to jump right in? Check out the app and the code.\nIn 43 lines of code, this app will:\nUse LlamaIndex to load and index data. Specifically, we're using the markdown files that make up Streamlit's documentation (you can sub in your data if you want).\nCreate a chat UI with Streamlit's st.chat_input and st.chat_message methods\nStore and update the chatbot's message history using the session state\nAugment GPT-3.5 with the loaded, indexed data through LlamaIndex's chat engine interface so that the model provides relevant responses based on Streamlit's recent documentation\n\nTry the app for yourself:\n\n1. Configure app secrets\n\nThis app will use GPT-3.5, so you'll also need an OpenAI API key. Follow our instructions here if you don't already have one.\n\nCreate a secrets.toml file with the following contents.\n\nIf you're using Git, be sure to add the name of this file to your .gitignore so you don't accidentally expose your API key.\nIf you plan to deploy this app on Streamlit Community Cloud, the following contents should be added to your app's secrets via the Community Cloud modal.\n\n\nopenai_key = \"<your OpenAI API key here>\"\n\n2. Install dependencies\n2.1. Local development\n\nIf you're working on your local machine, install dependencies using pip:\n\npip install streamlit openai llama-index nltk\n\n2.2. Cloud development\n\nIf you're planning to deploy this app on Streamlit Community Cloud, create a requirements.txt file with the following contents:\n\nstreamlit\nopenai\nllama-index\nnltk\n\n3. Build the app\n\nThe full app is only 43 lines of code. Let's break down each section.\n\n3.1. Import libraries\n\nRequired Python libraries for this app: streamlit, llama_index, openai, and nltk.\n\nimport streamlit as st\nfrom llama_index import VectorStoreIndex, ServiceContext, Document\nfrom llama_index.llms import OpenAI\nimport openai\nfrom llama_index import SimpleDirectoryReader\n\n3.2. Initialize message history\nSet your OpenAI API key from the app's secrets.\nAdd a heading for your app.\nUse session state to keep track of your chatbot's message history.\nInitialize the value of st.session_state.messages to include the chatbot's starting message, such as, \"Ask me a question about Streamlit's open-source Python library!\"\n\n\nopenai.api_key = st.secrets.openai_key\nst.header(\"Chat with the Streamlit docs üí¨ üìö\")\n\nif \"messages\" not in st.session_state.keys(): # Initialize the chat message history\n    st.session_state.messages = [\n        {\"role\": \"assistant\", \"content\": \"Ask me a question about Streamlit's open-source Python library!\"}\n    ]\n\n3.3. Load and index data\n\nStore your Knowledge Base files in a folder called data within the app. But before you begin‚Ä¶\n\nDownload the markdown files for Streamlit's documentation from the data demo app's GitHub repository folder. Or use this link to download a .zip file for the repo. Add the data folder to the root level of your app. Alternatively, add your data.\n\nüéà\nIf you‚Äôre running your app locally, check out LlamaIndex‚Äôs library of data connectors, available via LlamaHub, which makes it fast and easy to retrieve data from a variety of sources (including GitHub repositories).\n\nDefine a function called load_data(), which will:\n\nUse LlamaIndex‚Äôs SimpleDirectoryReader to passLlamaIndex's the folder where you‚Äôve stored your data (in this case, it‚Äôs called data and sits at the base level of your repository).\nSimpleDirectoryReader will select the appropriate file reader based on the extensions of the files in that directory (.md files for this example) and will load all files recursively from that directory when we call reader.load_data().\nConstruct an instance of LlamaIndex‚Äôs ServiceContext, whichLlamaIndex'stion of resources used during a RAG pipeline's indexing and querying stages.\nServiceContext allows us to adjust settings such as the LLM and embedding model used.\nUse LlamaIndex‚Äôs VectorStoreIndex to creaLlamaIndex'sory SimpleVectorStore, which will structure your data in a way that helps your model quickly retrieve context from your data. Learn more about LlamaIndex‚Äôs Indices here. This function returns the VectorStoreIndex object.\n\nThis function is wrapped in Streamlit‚Äôs caching decorator st.cache_resource to minimize the number of times the data is loaded and indexed.\n\nFinally, call the load_data function, designating its returned VectorStoreIndex object to be called index.\n\n@st.cache_resource(show_spinner=False)\ndef load_data():\n    with st.spinner(text=\"Loading and indexing the Streamlit docs ‚Äì hang tight! This should take 1-2 minutes.\"):\n        reader = SimpleDirectoryReader(input_dir=\"./data\", recursive=True)\n        docs = reader.load_data()\n        service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, system_prompt=\"You are an expert on the Streamlit Python library and your job is to answer technical questions. Assume that all questions are related to the Streamlit Python library. Keep your answers technical and based on facts ‚Äì do not hallucinate features.\"))\n        index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n        return index\n\nindex = load_data()\n\n3.4. Create the chat engine\n\nLlamaIndex offers several different modes of chat engines. It can be helpful to test each mode with questions specific to your knowledge base and use case, comparing the response generated by the model in each mode.\n\nLlamaIndex has four different chat engines:\n\nCondense question engine: Always queries the knowledge base. Can have trouble with meta questions like ‚ÄúWhat did I previously ask you?‚Äù\nContext chat engin\": Always queries the knowledge base and uses retrieved text from the knowledge base as context for following queries. The retrieved context from previous queries can take up much of the available context for the current query.\nReAct agent: Chooses whether to query the knowledge base or not. Its performance is more dependent on the quality of the LLM. You may need to coerce the chat engine to correctly choose whether to query the knowledge base.\nOpenAI agent: Chooses whether to query the knowledge base or not‚Äîsimilar to ReAct agent mode, but uses OpenAI‚Äôs built-in fuOpenAI'salling capabilities.\n\nThis example uses the condense question mode because it always queries the knowledge base (files from the Streamlit docs) when generating a response. This mode is optimal because you want the model to keep its answers specific to the features mentioned in Streamlit‚Äôs documentation.\n\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n\n3.5. Prompt for user input and display message history\nUse Streamlit‚Äôs st.chat_input feature Streamlit'she user to enter a question.\nOnce the user has entered input, add that input to the message history by appending it st.session_state.messages.\nShow the message history of the chatbot by iterating through the content associated with the ‚Äúmessages‚Äù key in the session state and displaying each message using st.chat_message.\n\n\nif prompt := st.chat_input(\"Your question\"): # Prompt for user input and save to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\nfor message in st.session_state.messages: # Display the prior chat messages\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\n3.6. Pass query to chat engine and display response\n\nIf the last message in the message history is not from the chatbot, pass the message content to the chat engine via chat_engine.chat(), write the response to the UI using st.write and st.chat_message, and add the chat engine‚Äôs response to the message history.\n\n# If last message is not from assistant, generate a new response\nif st.session_state.messages[-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = chat_engine.chat(prompt)\n            st.write(response.response)\n            message = {\"role\": \"assistant\", \"content\": response.response}\n            st.session_state.messages.append(message) # Add response to message history\n\n4. Deploy the app!\n\nAfter building the app, deploy it on Streamlit Community Cloud:\n\nCreate a GitHub repository.\nNavigate to Streamlit Community Cloud, click New app, and pick the appropriate repository, branch, and file path.\nHit Deploy.\nLlamaIndex helps prevent hallucinations\n\nNow that you‚Äôve built a Streamlit docs chatbot using up-to-date markdown files, how do these results compare the results to ChatGPT? GPT-3.5 and 4 have only been trained on data up to September 2021. They‚Äôre missing three years of new releases! Augmenting your LLM with LlamaIndex ensures higher accuracy of the response.\n\nWhat is Streamlit‚Äôs experimental connection feature?\nWrapping up\n\nYou learned how the LlamaIndex framework can create RAG pipelines and supplement a model with your data.\n\nYou also built a chatbot app that uses LlamaIndex to augment GPT-3.5 in 43 lines of code. The Streamlit documentation can be substituted for any custom data source. The result is an app that yields far more accurate and up-to-date answers to questions about the Streamlit open-source Python library compared to ChatGPT or using GPT alone.\n\nCheck out our LLM gallery for inspiration to build even more LLM-powered apps, and share your questions in the comments.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Land your dream job: Build your portfolio with Streamlit",
    "url": "https://blog.streamlit.io/land-your-dream-job-build-your-portfolio-with-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nLand your dream job: Build your portfolio with Streamlit\n\nShowcase your coding skills to recruiters with a chatbot tailored to your resume\n\nBy Vicky Kuo\nOctober 13 2023\nIn this tutorial, you‚Äôll learn how to build a portfolio with:\nLet‚Äôs get started\nStep 1: Add an introduction file to your app\nStep 2: Build your chatbot\nStep 3: Import the main CSS file and Lottie animation\nStep 4: Design the layout using a sidebar, container, columns, and expander\nStep 5: Add a career timeline with vis.js\nStep 6: Add HTML and JavaScript with custom CSS\nStep 7: Add RSS feeds with components.html\nStep 8: Add images and a resume PDF\nStep 9: Add a slideshow of colleague endorsements\nStep 10: Add a contact form\nStep 11: Deploy on Streamlit Community Cloud\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüí°\nTL;DR: Make a portfolio website with a custom AI chatbot, animations, a career snapshot, RSS feeds, images, and a colleague endorsement slideshow! Check out the code and the demo app.\n\nHey, community! üëã\n\nI'm Vicky, currently working as a data scientist intern at IBM, and I'm thrilled to share my insights with all of you.\n\nIn today's competitive job market, having a portfolio is more than just a nice-to-have; it's a must. Your portfolio is the visual companion to your resume, helping you stand out and show what you're capable of. It's the proof behind the promises on your resume, making it an essential tool to make a strong impression on potential employers.\n\nWith Streamlit, you can take your resume to the next level, setting you apart from other candidates. Create an interactive app to showcase your own data work, achievements, and personality. Then, pair that with an AI-powered chatbot where recruiters can talk to your resume!? Step aside, ‚Äúother candidates!‚Äù\n\nI hope you find this guide helpful, and it helps you land your dream job!\n\nIn this tutorial, you‚Äôll learn how to build a portfolio with:\nAn AI-powered chatbot that includes custom data, such as a self-introduction\nAnimations\nCareer snapshot with a timeline\nHTML and JavaScript with custom CSS\nRSS feeds (such as Medium posts)\nImages and a resume PDF\nColleague endorsement slideshow\nContact form\nhttps://portfolio-template.streamlit.app/\nLet‚Äôs get started\n\nClone the GitHub repository for this project‚Äôs starter code by entering the following command in your terminal:\n\ngit clone <https://github.com/vicky-playground/portfolio-template/>\n\n\nBefore proceeding, it's crucial to personalize both the constant.py and images files with your own information. This step ensures that your portfolio accurately represents your unique profile and content. Please exercise caution and avoid altering the names of keys within the constant.py file and the names of files in the images folder.\n\nStep 1: Add an introduction file to your app\n\nInside the bio.txt file, I've included a sample self-introduction that will guide the chatbot's responses. Customize it with your own introduction to potential recruiters. The chatbot will base its answers on a combination of the info provided in bio.txt and the large language model of your choice.\n\nStep 2: Build your chatbot\n\nThis step will walk you through building an AI chatbot using LlamaIndex and OpenAI. LlamaIndex is your go-to tool for creating applications (like Q&A, chatbots, and agents) powered by large language models (LLMs) and tailored to your specific data. For this app, you‚Äôll use LlamaIndex to enable the chatbot to answer questions about your work history based on the info you provided in the bio.txt file.\n\nNow, let's navigate to the 1_Home.py file to see how this is achieved.\n\nSummary of steps:\n\nOpenAI API Key Input: The user is prompted to input their OpenAI API key, which can be obtained by creating a new API token on OpenAI's platform in the Streamlit sidebar. This key is required to interact with OpenAI's models.\nDocument Loading: The code loads a document (e.g., a file containing information about the user) using SimpleDirectoryReader.\nQuery Engine Setup: A query engine is set up to interact with LlamaIndex and OpenAI's GPT-3.5-turbo model.\nUser Input: The user can enter questions or queries related to the user's information or profile.\nChatbot Interaction: When the user enters a question, the chatbot (named Buddy) uses LlamaIndex and GPT-3.5-turbo to provide responses. The user's input is included in a prompt, and the chatbot generates a response based on the indexed documents and the user's query.\nDisplay Response: The chatbot's response is displayed in the Streamlit app.\nAPI Key Verification: If the user hasn't entered their OpenAI API key or has entered it incorrectly, appropriate warnings or information messages are displayed.\n\nHere's the code that performs these steps, along with the additional information provided above:\n\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\nimport openai\nfrom langchain.chat_models import ChatOpenAI\n\nopenai_api_key = st.sidebar.text_input('Enter your OpenAI API Key and hit Enter', type=\"password\")\nopenai.api_key = (openai_api_key)\n\n# load the file\ndocuments = SimpleDirectoryReader(input_files=[\"bio.txt\"]).load_data()\n\n# build a query engine\ndef ask_bot(input_text):\n    # define LLM\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0,\n        openai_api_key=openai.api_key,\n    )\n    llm_predictor = LLMPredictor(llm=llm)\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n    \n    # load index\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)    \n    \n    # query LlamaIndex and GPT-3.5 for the AI's response\n    PROMPT_QUESTION = \"\"\"You are an AI agent named Buddy helping answer questions about Vicky to recruiters. Introduce yourself when you are introducing who you are.\n    If you do not know the answer, politely admit it and let users know how to contact Vicky to get more information. \n    Human: {input}\n    \"\"\"\n    output = index.as_query_engine().query(PROMPT_QUESTION.format(input=input_text))\n    print(f\"output: {output}\")\n    return output.response\n\n# get the user's input by calling the get_text function\ndef get_text():\n    input_text = st.text_input(\"You can send your questions and hit Enter to know more about me from my AI agent, Buddy!\", key=\"input\")\n    return input_text\n\nuser_input = get_text()\n\nif user_input:\n  if not openai_api_key.startswith('sk-'):\n    st.warning('‚ö†Ô∏èPlease enter your OpenAI API key on the sidebar.', icon='‚ö†')\n  if openai_api_key.startswith('sk-'):\n    st.info(ask_bot(user_input))\n\nüí°\nPro Tip: If you're searching for a free LLM alternative, I suggest exploring the complimentary guided project I crafted on IBM‚Äôs Cognitive Class here. This project will guide you how to integrate an LLM from IBM‚Äôs watsonx into your web app, and it comes with special free access.\nStep 3: Import the main CSS file and Lottie animation\n\nIn this step, we will enhance the appearance of our website by incorporating a CSS file and integrating Lottie animation JSON URLs to Streamlit.\n\nHere's a visual guide for your reference:\n\nhttps://lottiefiles.com/featured\nimport requests\nfrom streamlit_lottie import st_lottie\n\n# Load a Lottie animation from a URL\ndef load_lottieurl(url: str):\n    r = requests.get(url)\n    if r.status_code != 200:\n        return None\n    return r.json()\n\n# Apply local CSS styles from a file\ndef local_css(file_name):\n    with open(file_name) as f:\n        st.markdown('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)\n\n# Apply local CSS styles from the \"style.css\" file   \nlocal_css(\"style/style.css\")\n\n# Load Lottie animations from various URLs\nlottie_gif = load_lottieurl(\"<https://assets9.lottiefiles.com/packages/lf20_x17ybolp.json>\")\nlottie_gif = load_lottieurl(\"<https://assets9.lottiefiles.com/packages/lf20_x17ybolp.json>\")\npython_lottie = load_lottieurl(\"<https://assets6.lottiefiles.com/packages/lf20_2znxgjyt.json>\")\njava_lottie = load_lottieurl(\"<https://assets9.lottiefiles.com/packages/lf20_zh6xtlj9.json>\")\nmy_sql_lottie = load_lottieurl(\"<https://assets4.lottiefiles.com/private_files/lf30_w11f2rwn.json>\")\ngit_lottie = load_lottieurl(\"<https://assets9.lottiefiles.com/private_files/lf30_03cuemhb.json>\")\ngithub_lottie = load_lottieurl(\"<https://assets8.lottiefiles.com/packages/lf20_6HFXXE.json>\")\ndocker_lottie = load_lottieurl(\"<https://assets4.lottiefiles.com/private_files/lf30_35uv2spq.json>\")\nfigma_lottie = load_lottieurl(\"<https://lottie.host/5b6292ef-a82f-4367-a66a-2f130beb5ee8/03Xm3bsVnM.json>\")\naws_lottie = load_lottieurl(\"<https://lottie.host/6eae8bdc-74d1-4b5d-9eb7-37662274cd19/Nduizk8IOf.json>\")\n\nStep 4: Design the layout using a sidebar, container, columns, and expander\nAdd a left panel sidebar using the streamlit.sidebar function.\nfrom constant import *\n\n# Display the photo in the sidebar using HTML content\nst.sidebar.markdown(info['Photo'],unsafe_allow_html=True) #info['Photo'] is a reference to the dictionary in constants.py\n\nDesign a self-introduction with columns and add it to a container:\n# Define a function named \"gradient\" that generates a colored gradient title with content\ndef gradient(color1, color2, color3, content1, content2):\n    # Create an HTML structure with styling for a gradient header\n    st.markdown(f'<h1 style=\"text-align:center;background-image: linear-gradient(to right,{color1}, {color2});font-size:60px;border-radius:2%;\">'\n                f'<span style=\"color:{color3};\">{content1}</span><br>'\n                f'<span style=\"color:white;font-size:17px;\">{content2}</span></h1>', \n                unsafe_allow_html=True)\n\n# Create a container to organize content using Streamlit's container feature\nwith st.container():\n    # Divide the container into two columns, with widths 8 and 3\n    col1, col2 = st.columns([8, 3])\n\n# Inside the first column (col1):\nfull_name = info['Full_Name']\nwith col1:\n    # Call the \"gradient\" function to display a gradient title\n    gradient('#FFD4DD','#000395','e0fbfc',f\"Hi, I'm {full_name}üëã\", info[\"Intro\"])\n    st.write(\"\")  # Add an empty line\n    st.write(info['About'])  # Display the 'About' information stored in the 'info' dictionary\n\n# Inside the second column (col2):\nwith col2:\n    # Display a Lottie animation using the st_lottie function\n    st_lottie(lottie_gif, height=280, key=\"data\")\n\nCreate a layout for displaying the buttons of skills with columns:\n# Create a container to organize content using Streamlit's container feature\nwith st.container():\n    st.subheader('‚öíÔ∏è Skills')\n    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])\n    with col1:\n        st_lottie(python_lottie, height=70,width=70, key=\"python\", speed=2.5)\n    with col2:\n        st_lottie(java_lottie, height=70,width=70, key=\"java\", speed=4)\n    with col3:\n        st_lottie(my_sql_lottie,height=70,width=70, key=\"mysql\", speed=2.5)\n    with col4:\n        st_lottie(git_lottie,height=70,width=70, key=\"git\", speed=2.5)\n    with col1:\n        st_lottie(github_lottie,height=50,width=50, key=\"github\", speed=2.5)\n    with col2:\n        st_lottie(docker_lottie,height=70,width=70, key=\"docker\", speed=2.5)\n    with col3:\n        st_lottie(figma_lottie,height=50,width=50, key=\"figma\", speed=2.5)\n    with col4:\n        st_lottie(js_lottie,height=50,width=50, key=\"js\", speed=1)\n\nStep 5: Add a career timeline with vis.js\n\nTo personalize the timeline, you can update the example.json file with your own timeline data. This allows you to showcase your professional journey in a visually engaging manner.\n\nfrom streamlit_timeline import timeline\n\nwith st.container():\n    st.markdown(\"\")\n    st.subheader('üìå Career Snapshot')\n    # Load data\n    with open('example.json', \"r\") as f:\n        data = f.read()\n    # Render timeline\n    timeline(data, height=400)\n\nStep 6: Add HTML and JavaScript with custom CSS\n\nWhen working with Streamlit, there are two ways to modify HTML and CSS:\n\nst.markdown: Write and alter HTML code directly within your app, typically within the <body> tag.\nst.components.v1.html: Embed custom HTML and HTML elements/snippets in your app's UI, with the additional capability to include JavaScript.\n\nTo illustrate, here's an example of embedding a Tableau dashboard and using st.expander to show/hide content with an expand/collapse widget:\n\nimport streamlit.components.v1 as components\n    \nwith st.container():\n\t\t# Display an empty markdown to add some spacing\n    st.markdown(\"\"\"\"\"\")\n\n    st.subheader(\"üìä Tableau\")\n    col1,col2 = st.columns([0.95, 0.05])\n    with col1:\n        with st.expander('See the work'):\n\t\t\t\t\t\t# Display Tableau visualization using the components.html method\n            components.html(\n                \"\"\"\n                <!DOCTYPE html>\n                <html>  \n                    <title>Basic HTML</title>  \n                    <body style=\"width:130%\">  \n                        <div class='tableauPlaceholder' id='viz1684205791200' style='position: static'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Su&#47;SunnybrookTeam&#47;Overview&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='SunnybrookTeam&#47;Overview' /><param name='tabs' value='yes' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Su&#47;SunnybrookTeam&#47;Overview&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1684205791200');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='1350px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1550px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='1350px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1550px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.minHeight='5750px';vizElement.style.maxHeight=(divElement.offsetWidth*1.77)+'px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = '<https://public.tableau.com/javascripts/api/viz_v1.js>';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n                    </body>  \n                </HTML>\n                \"\"\"\n            , height=400, scrolling=True\n            )\n    st.markdown(\"\"\" <a href={}> <em>üîó access to the link </a>\"\"\".format(info['Tableau']), unsafe_allow_html=True)\n\nStep 7: Add RSS feeds with components.html\n\nRSS feeds offer a dynamic way to showcase the latest updates and posts from blogs or other platforms directly in your application.\n\nStreamlit's st.components.v1.html() function simplifies the integration process, allowing for a seamless display of content, such as Medium articles. For the purpose of guiding you through this process, we've provided a sample RSS feed URL from Medium. This sample link can be located in the constant.py file under the variable embed_rss['rss'].\n\nwith st.container():\n    st.markdown(\"\"\"\"\"\")\n    st.subheader('‚úçÔ∏è Medium')\n    col1,col2 = st.columns([0.95, 0.05])\n    with col1:\n        with st.expander('Display my latest posts'):\n            components.html(embed_rss['medium'],height=400)\n            \n        st.markdown(\"\"\" <a href={}> <em>üîó access to the link </a>\"\"\".format(info['Medium']), unsafe_allow_html=True)\n\nStep 8: Add images and a resume PDF\n\nIn this step, we'll add images and a resume to our website. To display your resume as a PDF on the web, we'll use base64 encoding in the pages/2_Resume.py file.\n\nimport base64\n    \nwith open(\"images/resume.pdf\",\"rb\") as f:\n      base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n      pdf_display = F'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"1000mm\" height=\"1000mm\" type=\"application/pdf\"></iframe>'\n      st.markdown(pdf_display, unsafe_allow_html=True)\n\n\nAdditionally, to showcase your hobbies with images, you can include both JPG and PNG images in the pages/3_Hobbies.py file. To do this, you can leverage the Pillow library for image handling.\n\nfrom PIL import Image\n    \nimg_1 = Image.open(\"images/1.jpg\")\nimg_2 = Image.open(\"images/2.png\")\nimg_3 = Image.open(\"images/3.png\")\n\nStep 9: Add a slideshow of colleague endorsements\n\nNow, let's showcase endorsements from your coworkers in a slideshow. To do this, you'll need to replace the existing image URLs in the constant.py file with your own image URLs.\n\nwith st.container():\n\t\t# Divide the container into three columns\n    col1,col2,col3 = st.columns([0.475, 0.475, 0.05])\n    # In the first column (col1)    \n\t\twith col1:\n\t\t        # Add a subheader to introduce the coworker endorsement slideshow\n\t\t        st.subheader(\"üëÑ Coworker Endorsements\")\n\t\t        # Embed an HTML component to display the slideshow\n\t\t        components.html(\n\t\t        f\"\"\"\n\t\t        <!DOCTYPE html>\n\t\t        <html>\n\t\t        <head>\n\t\t        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\t\t        <!-- Styles for the slideshow -->\n\t\t        <style>\n\t\t            * {{box-sizing: border-box;}}\n\t\t            .mySlides {{display: none;}}\n\t\t            img {{vertical-align: middle;}}\n\t\t\n\t\t            /* Slideshow container */\n\t\t            .slideshow-container {{\n\t\t            position: relative;\n\t\t            margin: auto;\n\t\t            width: 100%;\n\t\t            }}\n\t\t\n\t\t            /* The dots/bullets/indicators */\n\t\t            .dot {{\n\t\t            height: 15px;\n\t\t            width: 15px;\n\t\t            margin: 0 2px;\n\t\t            background-color: #eaeaea;\n\t\t            border-radius: 50%;\n\t\t            display: inline-block;\n\t\t            transition: background-color 0.6s ease;\n\t\t            }}\n\t\t\n\t\t            .active {{\n\t\t            background-color: #6F6F6F;\n\t\t            }}\n\t\t\n\t\t            /* Fading animation */\n\t\t            .fade {{\n\t\t            animation-name: fade;\n\t\t            animation-duration: 1s;\n\t\t            }}\n\t\t\n\t\t            @keyframes fade {{\n\t\t            from {{opacity: .4}} \n\t\t            to {{opacity: 1}}\n\t\t            }}\n\t\t\n\t\t            /* On smaller screens, decrease text size */\n\t\t            @media only screen and (max-width: 300px) {{\n\t\t            .text {{font-size: 11px}}\n\t\t            }}\n\t\t            </style>\n\t\t        </head>\n\t\t        <body>\n\t\t            <!-- Slideshow container -->\n\t\t            <div class=\"slideshow-container\">\n\t\t                <div class=\"mySlides fade\">\n\t\t                <img src={endorsements[\"img1\"]} style=\"width:100%\">\n\t\t                </div>\n\t\t\n\t\t                <div class=\"mySlides fade\">\n\t\t                <img src={endorsements[\"img2\"]} style=\"width:100%\">\n\t\t                </div>\n\t\t\n\t\t                <div class=\"mySlides fade\">\n\t\t                <img src={endorsements[\"img3\"]} style=\"width:100%\">\n\t\t                </div>\n\t\t\n\t\t            </div>\n\t\t            <br>\n\t\t            <!-- Navigation dots -->\n\t\t            <div style=\"text-align:center\">\n\t\t                <span class=\"dot\"></span> \n\t\t                <span class=\"dot\"></span> \n\t\t                <span class=\"dot\"></span> \n\t\t            </div>\n\t\t\n\t\t            <script>\n\t\t            let slideIndex = 0;\n\t\t            showSlides();\n\t\t\n\t\t            function showSlides() {{\n\t\t            let i;\n\t\t            let slides = document.getElementsByClassName(\"mySlides\");\n\t\t            let dots = document.getElementsByClassName(\"dot\");\n\t\t            for (i = 0; i < slides.length; i++) {{\n\t\t                slides[i].style.display = \"none\";  \n\t\t            }}\n\t\t            slideIndex++;\n\t\t            if (slideIndex > slides.length) {{slideIndex = 1}}    \n\t\t            for (i = 0; i < dots.length; i++) {{\n\t\t                dots[i].className = dots[i].className.replace(\"active\", \"\");\n\t\t            }}\n\t\t            slides[slideIndex-1].style.display = \"block\";  \n\t\t            dots[slideIndex-1].className += \" active\";\n\t\t            }}\n\t\t\n\t\t            var interval = setInterval(showSlides, 2500); // Change image every 2.5 seconds\n\t\t\n\t\t            function pauseSlides(event)\n\t\t            {{\n\t\t                clearInterval(interval); // Clear the interval we set earlier\n\t\t            }}\n\t\t            function resumeSlides(event)\n\t\t            {{\n\t\t                interval = setInterval(showSlides, 2500);\n\t\t            }}\n\t\t            // Set up event listeners for the mySlides\n\t\t            var mySlides = document.getElementsByClassName(\"mySlides\");\n\t\t            for (i = 0; i < mySlides.length; i++) {{\n\t\t            mySlides[i].onmouseover = pauseSlides;\n\t\t            mySlides[i].onmouseout = resumeSlides;\n\t\t            }}\n\t\t            </script>\n\t\t\n\t\t            </body>\n\t\t            </html> \n\t\t\n\t\t            \"\"\",\n\t\t                height=270,\n\t\t    )\n\nStep 10: Add a contact form\n\nLet's create a contact form that allows visitors to get in touch with you and connect it with your email using FormSubmit.\n\nCopy and paste the provided code at the bottom of the 1_Home.py file. Ensure that your email address is stored in constant.py under the key Email in the info dictionary. This will ensure that messages submitted through the contact form are sent to your email.\n\nwith col2:\n        st.subheader(\"üì® Contact Me\")\n        email = info[\"Email\"]\n        contact_form = f\"\"\"\n        <form action=\"<https://formsubmit.co/{email}>\" method=\"POST\">\n            <input type=\"hidden\" name=\"_captcha value=\"false\">\n            <input type=\"text\" name=\"name\" placeholder=\"Your name\" required>\n            <input type=\"email\" name=\"email\" placeholder=\"Your email\" required>\n            <textarea name=\"message\" placeholder=\"Your message here\" required></textarea>\n            <button type=\"submit\">Send</button>\n        </form>\n        \"\"\"\n        st.markdown(contact_form, unsafe_allow_html=True)\n\nüí°\nNote: Submit the contact form once. This first-time-use will trigger an email requesting confirmation.\nStep 11: Deploy on Streamlit Community Cloud\n\nAfter building your app, deploy it on Streamlit Community Cloud:\n\nCreate a GitHub repository.\nNavigate to¬†Streamlit Community Cloud, click¬†New app, and pick the appropriate repository, branch, and file path.\nHit¬†Deploy\nShare with recruiters and hiring managers!\nWrapping up\n\nThank you for reading my post! I hope you enjoyed this tutorial and found it inspiring.\n\nI can‚Äôt wait to see the amazing portfolios you‚Äôll create. Once you have, tag me on LinkedIn. I'd love to celebrate your accomplishments and get inspired by your work! üéâüí°\n\nIf you have any feedback or questions, please feel free to post them in the comments below or contact me on LinkedIn.\n\nLet‚Äôs continue to learn and grow together! üë©‚Äçüíªüå±\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How in-app feedback can increase your chatbot‚Äôs performance",
    "url": "https://blog.streamlit.io/how-in-app-feedback-can-increase-your-chatbots-performance/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow in-app feedback can increase your chatbot‚Äôs performance\n\nA guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith\n\nBy Charly Wargnier\nPosted in Tutorials, October 6 2023\nWhat is LangChain?\n1. How to build a custom chatbot with LangChain\n1.1 Set up pre-requisites\n1.2 Create the RAG chain\n1.3 Create main.py\n1.4 Give your chatbot a memory\n1.5 Build the chat interface\n1.6 Test the custom chatbot\n2. How to implement Trubrics' Feedback\n3. How to connect to LangSmith\n4. How to turn user feedback into actionable insights\n4.1 Spot content inaccuracies\n4.2 Highlight slow response times\n4.3 Gauge users‚Äô satisfaction\nBONUS. Pinpoint library and API errors\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüëâ\nTL;DR: Learn how to build a RAG chatbot with LangChain, capture user feedback via Trubrics, and monitor it with LangSmith to gain actionable insights and improve chatbot performance. Check out the app and its code.\n\nIs your chatbot occasionally falling short? Whether it's providing incorrect answers, not meeting users' expectations, or not engaging them the way you want, implementing in-app user feedback can be a game-changer!\n\nGathering real-time feedback from your users enables you to:\n\nIdentify missing or incorrect content used to retrieve answers\nDetect technical issues (slow response times, library errors, API hiccups, etc.)\nGauge the overall user satisfaction\n\nHow do you start collecting user feedback? Simple. All you need is a Retrieval Augmented Generation (RAG) chatbot, Trubrics' Feedback component, and a connection to LangSmith.\n\nLet's get started!\n\nWhat is LangChain?\n\nFirst, let's walk through the basics of LangChain‚Äîthe foundation for your custom chatbot.\n\nLangChain is a versatile data framework designed for apps that utilize large language models (LLMs). It simplifies the process of creating robust chatbot apps by offering pre-built chains and modular components.\n\nFor this example, we'll be using LangChain's Retrieval-Augmented Generation (RAG) capabilities. Simply put, RAG is a technique that enhances the effectiveness of LLMs by connecting them to custom and up-to-date resources. This ensures that your chatbot provides accurate and precise answers to meet your user's requirements.\n\n1. How to build a custom chatbot with LangChain\n1.1 Set up pre-requisites\n\nTo get started:\n\nFollow the instructions here to obtain an OpenAI API key.\nSign up for LangSmith at https://smith.langchain.com/ and generate an API Key. To do this, click on the API Key icon and then click on Create API Key (make sure to copy it).\n\nInstall Streamlit, LangChain, LangSmith, the streamlit-feedback component, and the dependencies from the requirements.txt file‚Äîopenai, tiktoken, and chromadb. Make sure to run this command in the virtual environment where you want to run your app to avoid any package conflicts:\npip install -r requirements.txt\n\n1.2 Create the RAG chain\n\nNow, let's move to the core of your chatbot‚Äîthe essential_chain.py file. This file enables the vectorization of any documents (we'll use Streamlit's documentation as an example) to obtain up-to-date and context-aware responses.\n\nTo begin, import the required libraries and LangChain functions:\n\nimport os\nimport streamlit as st\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.docstore.document import Document\nfrom langchain.memory import ConversationBufferMemory\n\n\nThe main function in this file is initialize_chain, which sets up the chat model and loads the required documents.\n\nCreate a ChatOpenAI instance with the gpt-3.5-turbo model:\n@st.cache_resource(show_spinner=False)\ndef initialize_chain(system_prompt, _memory):\n    llm = ChatOpenAI(temperature=0, max_tokens=1000, model_name=\"gpt-3.5-turbo\", streaming=True)\n\nIterate over all files in the specified directory (./markdown_files), read the content of each markdown file, and store it as a Document object. This allows you to integrate Streamlit's up-to-date markdown documentation into the app:\ndocuments = []\nfolder_path = \"./markdown_files\"\n# ... (loop through and read each .md file, storing them as Document objects)\n\nüí°\nTIP: While this demo app queries our Streamlit docs, you can test it with your own docs or blog content in just a few clicks. Simply replace the Streamlit markdown files in the repository with your own markdown files, and you're done! üôå\n\nOnce loaded, the documents undergo a two-step process of splitting and embedding:\n\nSplitting: The content is divided into smaller chunks using RecursiveCharacterTextSplitter to facilitate efficient vectorization.\nEmbedding: These chunks are then vectorized using OpenAI embeddings and stored in a Chroma DB database.\nif documents:\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n    document_chunks = text_splitter.split_documents(documents)\n    embeddings = OpenAIEmbeddings()\n    vectorstore = Chroma.from_documents(document_chunks, embeddings)\n\n\nFinally, to make use of the vectorized data, initialize the ConversationalRetrievalChain:\n\nqa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=_memory)\n\n\nThat's all for essential_chain.py!\n\nNow, let's move on to the main.py file.\n\n1.3 Create main.py\n\nmain.py is the nucleus of your app, where you call essential_chain.py and displays the front-end UI, among other things.\n\nFirst, import the required modules into the file:\n\nimport streamlit as st\nfrom streamlit_feedback import streamlit_feedback\nfrom langchain.callbacks.tracers.run_collector import RunCollectorCallbackHandler\nfrom langchain.memory import StreamlitChatMessageHistory, ConversationBufferMemory\nfrom langchain.schema.runnable import RunnableConfig\nfrom langsmith import Client\nfrom langchain.callbacks.tracers.langchain import wait_for_all_tracers\nfrom essential_chain import initialize_chain\nimport os\n\n\nNext, configure the environment variables to establish connections with OpenAI and LangSmith:\n\n# Set LangSmith environment variables\nos.environ[\"OPENAI_API_KEY\"] = st.secrets[\"api_keys\"][\"OPENAI_API_KEY\"]\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"<https://api.smith.langchain.com>\"\n\n\nIn the app's sidebar, you will collect additional inputs from users to configure LangSmith:\n\n# Add the toggle for LangSmith API key source\nuse_secret_key = st.sidebar.toggle(label=\"Demo LangSmith API key\", value=True)\n\n# Conditionally set the project name based on the toggle\nif use_secret_key:\n    os.environ[\"LANGCHAIN_PROJECT\"] = \"Streamlit Demo\"\nelse:\n    project_name = st.sidebar.text_input(\n        \"Name your LangSmith Project:\", value=\"Streamlit Demo\"\n    )\n    os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n\n# Conditionally get the API key based on the toggle\nif use_secret_key:\n    langchain_api_key = st.secrets[\"api_keys\"][\n        \"LANGSMITH_API_KEY\"\n    ]  # assuming it's stored under this key in secrets\nelse:\n    langchain_api_key = st.sidebar.text_input(\n        \"üëá Add your LangSmith Key\",\n        value=\"\",\n        placeholder=\"Your_LangSmith_Key_Here\",\n        label_visibility=\"collapsed\",\n    )\nif langchain_api_key is not None:\n    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n\nYou can use the provided LangSmith API key, which is for demo purposes. By using it, you'll only have access to each run, and runs won't be stored in your account.\nIf you choose to add your own LangSmith API key (toggle OFF), you'll gain full access to your LangSmith account and have the ability to specify a name for each of your projects.\n1.4 Give your chatbot a memory\n\nThe chat memory is an essential component for maintaining a contextual conversation. Use LangChain's ConversationBufferMemory to store the chat history, simplifying access to previous interactions.\n\nmemory = ConversationBufferMemory(\n    chat_memory=StreamlitChatMessageHistory(key=\"langchain_messages\"),\n    return_messages=True,\n    memory_key=\"chat_history\",\n)\n\n\nNext, add a functionality to clear the message history:\n\nif st.sidebar.button(\"Clear message history\"):\n    print(\"Clearing message history\")\n    memory.clear()\n    st.session_state.trace_link = None\n    st.session_state.run_id = None\n\n1.5 Build the chat interface\n\nUse Streamlit's new st.chat_message command to create a chat user interface that displays messages from both the user and the assistant in separate containers.\n\nfor msg in st.session_state.langchain_messages:\n    streamlit_type = _get_openai_type(msg)\n    avatar = \"ü¶ú\" if streamlit_type == \"assistant\" else None\n    with st.chat_message(streamlit_type, avatar=avatar):\n        st.markdown(msg.content)\n\n\nüí°\nTIP: You can modify the avatar parameter to give the assistant's messages a personal touch. In this example, I added a parrot emoji as a reference to LangChain's logo.\n\nThen, the st.chat_input command provides a chat input widget for receiving user messages:\n\nif prompt := st.chat_input(placeholder=\"Ask a question about the Streamlit Docs!\"):\n    # ... (Handling chat input and generating responses)\n    # ... (Displaying thinking animation and updating session state)\n\n\n1.6 Test the custom chatbot\n\nNow that you have built a Streamlit docs chatbot using up-to-date markdown files, let's compare the results to ChatGPT. GPT-3.5 and 4 have only been trained on data up to September 2021, missing almost three years of new releases. So augmenting your LLM with Langchain ensures higher accuracy of the response.\n\nLet's ask a question about Streamlit's Chat elements, released in July 2023: \"Tell me about Streamlit's Chat elements.\"\n\nChatGPT can‚Äôt come up with the right answer, but your custom chatbot can!\n\nYou can compare the classic GPT 3.5 LLM with our custom LLM directly in the demo app by switching between these two options:\n\n2. How to implement Trubrics' Feedback\n\nTrubrics' Feedback is a Streamlit component that enables you to collect user feedback directly in your app. With this component, users can rate the responses of the chatbot, and each feedback will be sent and stored in LangSmith.\n\nTrubrics' Feedback offers two feedback mechanisms:\n\nthumbs style:\n\nfaces style:\n\nThe default feedback style is set to thumbs. To allow users to easily switch between the two options, let's use Streamlit's new st.toggle widget:\n\nfeedback_option = \"faces\" if st.toggle(label=\"`Thumbs` ‚áÑ `Faces`\", value=False) else \"thumbs\"\n\n\nThe selected feedback style is activated when a valid run_id is present:\n\nif st.session_state.get(\"run_id\"):\n    feedback = streamlit_feedback(\n        feedback_type=feedback_option,  # Apply the selected feedback style\n        optional_text_label=\"[Optional] Please provide an explanation\",  # Allow for additional comments\n        key=f\"feedback_{st.session_state.run_id}\",\n    )\n\n3. How to connect to LangSmith\n\nLangSmith seamlessly integrates with the LangChain library, which you used to build your custom chatbot. In this case, you‚Äôll primarily use it to store user feedback and gain insights from it.\n\nTo establish a connection to LangSmith and send both the chatbot outputs and user feedback, follow these steps:\n\nclient = Client(api_url=langchain_endpoint, api_key=langchain_api_key)\n\n\n\nüí°\nTIP: Remember to add the LangSmith API key you obtained in section 1.1 to the LangChain API Key field of the app.\n\nTo check if the connection is working, follow these steps:\n\nType a question in the chat. For example, ask, \"Tell me more about Streamlit Chat elements.\"\nProvide feedback by giving a score and comment. For example, use Thumbs up and Great answer!\n\nClick on the Latest Trace button in the chatbot app:\n\nYou‚Äôll be redirected to LangSmith, where you can view the details of your run.\nClick on the Feedback tab in LangSmith to access the user feedback for that run.\n\n4. How to turn user feedback into actionable insights\n\nLet me show you some examples of how user feedback can help improve your chatbot‚Äôs answers, spot errors, and enhance your app‚Äôs workflow.\n\nüí°\nNOTE: The examples below are taken from the chatbot we use at Streamlit, which is the one you just built in this tutorial. Remember, you can get similar insights with your own docs by swapping the Streamlit markdown files in the repository with your own!\n4.1 Spot content inaccuracies\n\nHere is how we spotted content inaccuracies in our internal user feedback for the Steamlit app. LangSmith enabled us to filter chatbot runs by feedback score using the filter panel:\n\nWe reviewed the comments related to negative feedback scores and found that some users expressed dissatisfaction with the chatbot's responses to the question, \"How can I connect DuckDB to Streamlit using st.experimental_connection()?\"\n\nDuckDB is an SQL database system. While it‚Äôs possible to connect it to Streamlit using st.experimental_connection(), users have reported that the returned answers are incorrect. We tested this and found that our chatbot suggested non-existent Streamlit functions:\n\nThen we reviewed our docs and found that there is no information on how to use st.experimental_connection() with DuckDB:\n\nOur chatbot currently lacks the ability to retrieve the relevant content that users are requesting from our docs. So we requested the content team to prioritize creating content specifically related to \"DuckDB + st.experimental_connection()\". Once it‚Äôs created and added to the docs, we‚Äôll integrate it into our custom chatbot to enhance its responses.\n\nThere are several benefits to this approach:\n\n‚úÖ It will provide improved and more accurate responses from the chatbot\n‚úÖ It will ensure that our docs remains current and relevant\n‚úÖ It will help us prioritize the most important content for our docs\n4.2 Highlight slow response times\n\nSlow chatbot response times can frustrate users and decrease user retention. Additionally, users frequently provide feedback about noticeable lags or delays during interactions.\n\nLet's review some of the user feedback that we collected in our internal app:\n\nIt‚Äôs clear that the chatbot‚Äôs responses could be sped up!\n\nIn LangSmith, we can identify chatbot runs with slow response times by using the Latency >= 10s tag. You can locate this tag in the filter panel of any project.\n\nAny run with high latency will be displayed:\n\nHere is how you can fix these latency issues:\n\nLangSmith can show you the latency associated with each chain of your chatbot. You can optimize response times by experimenting with different chains. For example, in some of our tests, we found that the ConversationSummaryBufferMemory chain had a significant impact on latency.\nYou can also try Zep, an open-source long-term memory store designed to improve the performance of chatbots, especially those built with LangChain.\nWe also recommend that you use [st.cache_resource](<https://docs.streamlit.io/library/api-reference/performance/st.cache_resource>) to cache the relevant functions of your app (if you haven't done so already).\n4.3 Gauge users‚Äô satisfaction\n\nIn addition to analyzing individual user feedback as demonstrated in sections 4.1 and 4.2, it‚Äôs important to consider the value of aggregated feedback.\n\nAggregated feedback can help broaden your approach and provide a measurable indicator of user satisfaction towards a model, content, or chatbot application. LangSmith provides data visualizations for all recorded metrics, including metrics derived from user feedback. Each metric is presented on a timeline chart, allowing you to track its progress over time.\n\nLet's explore!\n\nAfter you build the app by following the steps below, head to your LangSmith project and click on the Monitor tab:\n\nOne way to track user satisfaction is by checking the Avg. Score - faces chart. From this chart, we can observe that user satisfaction with our internal chatbot is improving. So we‚Äôll continue testing and refining it!\n\nBONUS. Pinpoint library and API errors\n\nWhile users can sometimes provide feedback on errors, there are instances where they can‚Äôt do so because these errors hinder the chatbot's proper functioning. The good news is that LangSmith logs all of these errors for you.\n\nTo view chatbot runs that contain errors, simply select the error tag in the filter panel:\n\nDuring our internal testing, we found the following two issues:\n\nLibrary installations issues\n\n‚úÖ To fix it, we went back to the app and installed the h2 package.\n\nIncorrect API keys\n\nWe spotted several types of API errors and warnings (you also get the timing of their occurrences in LangSmith). For example, we observed an error when our OpenAI key became invalid.\n\n‚úÖ To fix it, we updated the OpenAI API key in the app.\n\nAnd now, let's get to building!\n\nWrapping up\n\nWe‚Äôve embarked on a journey together, delving into the code to build a reliable and powerful RAG chatbot in Streamlit. Throughout this journey, I introduced you to the LangSmith platform and the Trubrics Feedback component. You‚Äôve witnessed the value of user feedback, whether it‚Äôs for refining documentation, enhancing app workflows, identifying errors, or understanding user sentiments.\n\nFeel free to explore our LLM gallery, learn, and draw inspiration from the amazing creations of the community. Soon enough, you'll be eager to create your own chatbot masterpiece! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Amjad Raza - Streamlit",
    "url": "https://blog.streamlit.io/author/amjad/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Amjad Raza\n1 post\nChat with pandas DataFrames using LLMs\n\nA step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI\n\nLLMs\nby\nAmjad Raza\n,\nAugust 31 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Chat with pandas DataFrames using LLMs",
    "url": "https://blog.streamlit.io/chat-with-pandas-dataframes-using-llms/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nChat with pandas DataFrames using LLMs\n\nA step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI\n\nBy Amjad Raza\nPosted in LLMs, August 31 2023\nApp overview\nTutorial\n1. Load data into pandas DataFrame\n2. LangChain and OpenAI as an LLM engine\n3. Use Streamlit for UI\n4. Use Docker for deployment\nPotential errors\nWrapping up\nContents\nShare this post\n‚Üê All posts\nTL;DR: In this post, I‚Äôll show you how to interact with pandas DataFrames, build an app powered by LangChain and OpenAI API, and set up the docker deployment for local or cloud deployments (grab the code here).\n\n\n\n\n‚ö†Ô∏è\nWARNING: This app uses LangChain's PythonAstREPLTool which is vulnerable to arbitrary code execution. Use caution in deploying and sharing this app.\n\nCommunicating with pandas DataFrames makes data analysis accessible to non-technical users. Using a chat-like interface, users can ask data-related questions, request insights, and navigate through data as if they were chatting with a friend.\n\nSince the preferred method for engaging with chatbots is through a question-and-answer format, let‚Äôs make an app that seamlessly integrates with user queries.\n\nBut first, we‚Äôll take a look at how it works.\n\nApp overview\n\nUsers can interact with pandas DataFrames by uploading CSV, Excel, or any other supported structured data files:\n\nTutorial\n\nTo start, make sure that you have the necessary key technologies installed, as well as a basic understanding of the application framework shown in the diagram below:\n\nPython, Streamlit, and Docker: Python and Docker are essential components for building and deploying the app. Having knowledge of building UI with Streamlit is a plus.\nVirtual Environment using Poetry: To create a virtual environment, install [Poetry](<https://python-poetry.org/>). It simplifies dependency management and environment setup.\nProject Dependencies: All the required dependencies for the project are specified in the pyproject.toml file. They ensure that your app has access to the necessary libraries and tools to function properly.\nOpenAI API Token: Get an OpenAI API token here.\n\n1. Load data into pandas DataFrame\n\nThe first step is to load and persist user data into a pandas DataFrame. For smaller datasets, it is good practice to persist the data. Users can upload files with various extensions from the list above. The data is cached for 2 hours using @st.cache_data(ttl=\"2h\") and destroyed after that time has elapsed to release resources.\n\nfile_formats = {\n    \"csv\": pd.read_csv,\n    \"xls\": pd.read_excel,\n    \"xlsx\": pd.read_excel,\n    \"xlsm\": pd.read_excel,\n    \"xlsb\": pd.read_excel,\n}\n@st.cache_data(ttl=\"2h\")\ndef load_data(uploaded_file):\n    try:\n        ext = os.path.splitext(uploaded_file.name)[1][1:].lower()\n    except:\n        ext = uploaded_file.split(\".\")[-1]\n    if ext in file_formats:\n        return file_formats[ext](uploaded_file)\n    else:\n        st.error(f\"Unsupported file format: {ext}\")\n        return None\n\n# Read the Pandas DataFrame\ndf = load_data(uploaded_file)\n\n2. LangChain and OpenAI as an LLM engine\n\nI have integrated LangChain's create_pandas_dataframe_agent to set up a pandas agent that interacts with df and the OpenAI API through the LLM model. This agent takes df, the ChatOpenAI model, and the user's question as arguments to generate a response. Under the hood, a Python code is generated based on the prompt and executed to summarize the data. The LLM model then converts the data into a conversational format for the final response.\n\nFor this example, I used the \"gpt-3.5-turbo-0613\" model, but users can choose GPT4 or any other model. Performance may vary depending on the model and dataset used.\n\nIn this code, the input questions are captured using the st.session_state.messages object from the Streamlit UI, and the response is passed back to the UI for display:\n\nfrom langchain.agents import AgentType\nfrom langchain.agents import create_pandas_dataframe_agent\nfrom langchain.callbacks import StreamlitCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(\n        temperature=0, model=\"gpt-3.5-turbo-0613\", openai_api_key=openai_api_key, streaming=True\n    )\n\n    pandas_df_agent = create_pandas_dataframe_agent(\n        llm,\n        df,\n        verbose=True,\n        agent_type=AgentType.OPENAI_FUNCTIONS,\n        handle_parsing_errors=True,\n    )\n\nresponse = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])\n\n\n\nNOTE: Make sure you have set up the OpenAI API Key in the sidebar.\n3. Use Streamlit for UI\n\nI chose Streamlit UI for its simplicity and recently released chat features, such as st.chat_message(\"assistant\"). It‚Äôs a lightweight and efficient method for building and sharing data apps.\n\nimport streamlit as st\nimport pandas as pd\nimport os\n\nst.set_page_config(page_title=\"LangChain: Chat with Pandas DataFrame\", page_icon=\"ü¶ú\")\nst.title(\"ü¶ú LangChain: Chat with Pandas DataFrame\")\n\nuploaded_file = st.file_uploader(\n    \"Upload a Data file\",\n    type=list(file_formats.keys()),\n    help=\"Various File formats are Support\",\n    on_change=clear_submit,\n)\n\nif uploaded_file:\n    df = load_data(uploaded_file)\n\n\n\nThe above code initializes the app and adds the Upload File Widget to the UI. You can upload data files using Streamlit's st.file_uploader component.\n\nopenai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\nif \"messages\" not in st.session_state or st.sidebar.button(\"Clear conversation history\"):\n    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n\nfor msg in st.session_state.messages:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n\nif prompt := st.chat_input(placeholder=\"What is this data about?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    st.chat_message(\"user\").write(prompt)\n\n    if not openai_api_key:\n        st.info(\"Please add your OpenAI API key to continue.\")\n        st.stop()\n\n    llm = ChatOpenAI(\n        temperature=0, model=\"gpt-3.5-turbo-0613\", openai_api_key=openai_api_key, streaming=True\n    )\n\n    pandas_df_agent = create_pandas_dataframe_agent(\n        llm,\n        df,\n        verbose=True,\n        agent_type=AgentType.OPENAI_FUNCTIONS,\n        handle_parsing_errors=True,\n    )\n\n\nThe user is prompted to provide their OpenAI API keys through a sidebar text widget. Use st.session_state to keep track of variables and chat history. The user's input query is obtained using st.chat_input(), which is then passed to pandas_df_agent as discussed in the previous section.\n\nwith st.chat_message(\"assistant\"):\n        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n        response = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n        st.write(response)\n\n\nThe code above interacts with the pandas_agent and captures its response‚Äîdisplayed and appended to the chat history.\n\n4. Use Docker for deployment\n\nAfter preparing and testing the app, deploy it on the Streamlit Community Cloud using the GitHub repository or on Google Cloud, Heroku, AWS, or Azure using a Docker configuration.\n\n4.1. Clone the GitHub repository\n\ngit clone https://github.com/langchain-ai/streamlit-agent.git\n\n4.2. Install requirements by creating a venv\n\n>> poetry install\n>> poetry shell\n\n\n4.3. Run the app locally\n\n$ streamlit run streamlit_agent/chat_pandas_df.py\n\n4.4. Run the app using Docker\n\nThe project includes the Dockerfile and docker-compose.yml. To build and run a Docker image:\n\nGenerate the image with DOCKER_BUILDKIT\n\nDOCKER_BUILDKIT=1 docker build --target=runtime . -t langchain-streamlit-agent:latest\n\nRun the Docker container directly\n\ndocker run -d --name langchain-streamlit-agent -p 8051:8051 langchain-streamlit-agent:latest\n\nRun the Docker container using docker-compose\n\nEdit the Command in docker-compose with the target Streamlit app docker-compose up. To deploy Streamlit apps using Google Cloud, follow this guide.\n\nPotential errors\n\nIf you choose to make a copy of and operate the application on your own computer or any online cloud system, you might come across the following problems:\n\nPreparing your personal environment and necessary components, since they are currently configured to utilize the most recent editions of LangChain & Streamlit.\nDue to our utilization of a mix of OpenAI & Langchain tools, there are instances where the model produces outcomes that aren't what we intended. In some cases, rephrasing your questions can help resolve this problem.\nWhen using Docker, it's important to have the latest version of Docker Desktop installed and sufficient storage space available for creating and running the image.\nWrapping up\n\nYou learned how to construct a generative AI application to talk with pandas DataFrames or CSV files by using LangChain's tools, and how to deploy and run your app locally or with Docker support.\n\nHere are the key takeaways:\n\nYou can seamlessly interact with business-specific data stored in Excel or CSV files, eliminating the need for complex setups or configurations.\nYou can transform DataFrames into conversational entities, similar to human conversations.\nYou can empower business users to pose relevant questions and engage with data, without requiring any prior knowledge of data processing or analysis.\n\nNow you can bridge the gap between data-driven insights and effortless interaction, enhancing the accessibility and usability of your data for a wider range of users. Let me know if you have any questions in the comments below or contact me on GitHub, LinkedIn, Twitter, or email.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Logan Vendrix - Streamlit",
    "url": "https://blog.streamlit.io/author/logan/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Logan Vendrix\n1 post\nBuild your own Notion chatbot\n\nA step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit\n\nLLMs\nby\nLogan Vendrix\n,\nSeptember 14 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit (Page 2)",
    "url": "https://blog.streamlit.io/page/2/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nDevelop Streamlit apps in-browser with GitHub Codespaces\n\nBuild anywhere without the hassle of a local Python environment\n\nGitHub Codespaces\nby\nBrian Holt\n,\nSeptember 14 2023\nBuild your own Notion chatbot\n\nA step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit\n\nLLMs\nby\nLogan Vendrix\n,\nSeptember 14 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nChat with pandas DataFrames using LLMs\n\nA step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI\n\nLLMs\nby\nAmjad Raza\n,\nAugust 31 2023\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nLLMs\nby\nCaroline Frasca and¬†\n2\n¬†more,\nAugust 23 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nExploring LLMs and prompts: A guide to the PromptTools Playground\n\nLearn how to build dynamic, stateful applications that harness multiple LLMs at once\n\nLLMs\nby\nSteve Krawczyk and¬†\n1\n¬†more,\nAugust 18 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\n‚Üê Previous page\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Simplifying generative AI workflows",
    "url": "https://blog.streamlit.io/simplifying-generative-ai-workflows/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nBy Filip Boltuzic, Ara Ghukasyan and Santosh Kumar Radha\nPosted in Advocate Posts, October 6 2023\nApp overview\n1. How to build and execute a Covalent workflow\n1.1 The electron decorator\n1.2 The executor objects\n1.3 The lattice decorator\n2. How to build a Streamlit UI to generate requests for news article summarization\n3. How to summarize news articles from Quanta\nWrapping up\nContents\nShare this post\n‚Üê All posts\nü§ñ\nTL;DR: Learn how to build complex generative AI apps using Covalent and Streamlit. Covalent simplifies resource management in the Python environment, while Streamlit enhances data visualization and user interaction. Together, they provide a transformative solution for efficient ML workflow management. Read more in our docs, and access the complete code here and the demo app here.\n\nWant to create a generative AI solution that combines LLMs, stable diffusion generation, and cloud-based computing for resource-heavy tasks? You‚Äôll need more than code. You‚Äôll need to act as a cloud architect, DevOps engineer, and financial analyst all in one, trapped in a cycle of provisioning resources, monitoring them, and scaling them up or down.\n\nThat‚Äôs where Covalent comes in.\n\nCovalent is a Pythonic workflow orchestration platform that scales workloads from your laptop to any compute backend. It lets you focus on what you do best‚Äîcoding, experimenting, and innovating‚Äîwithout the burden of operational complexity.\n\nIn this post, we‚Äôll show you how to build an interactive, generative AI app for summarizing news articles. You‚Äôll learn:\n\nHow to build and execute a Covalent workflow\nHow to build a Streamlit UI to generate requests for news article summarization\nHow to summarize news articles from Quanta\nApp overview\n\nLet's take a high-level look at the app:\n\nUsers create a Covalent generative AI workflow\nThey execute these workflows on the Covalent Server\nUsers trigger reruns and refinements of their Covalent workflows via a Streamlit app\n\n1. How to build and execute a Covalent workflow\n\nCovalent SDK is a Python framework that consists of three key components:\n\nThe electron decorator\nThe executor objects\nThe lattice decorator\n\nThese components help researchers define complex workflows in a lightweight and non-destructive manner, with minimal and non-intrusive code changes.\n\nIn the following sections, we‚Äôll explain how to use them.\n\n1.1 The electron decorator\n\nThe @ct.electron decorator converts a Python function into a remotely executable task that Covalent can use to deploy arbitrary resources. Users can specify resources and constraints for each task by passing different executor objects to electron decorators.\n\nHere is a simple Python function that generates an image from a text prompt using a stable diffusion model:\n\n@ct.electron\ndef generate_image_from_text(\n    summary, model_name=\"OFA-Sys/small-stable-diffusion-v0\", prompt=\"Impressionist image - \"\n):\n    model = DiffusionPipeline.from_pretrained(model_name)\n    model.enable_attention_slicing()\n\n    # Generate image using DiffusionPipeline\n    reduced_summary = prompt + summary\n    return model(reduced_summary).images[0]\n\n1.2 The executor objects\n\nCovalent executors define the environment in which the workflow will be executed. Each electron (task) can be assigned a different executor, allowing the workflow to be executed across multiple machines. Some tasks may require intensive and parallelizable CPU computation, while others can be completed using serverless, low-intensity CPU processing. This flexibility is crucial for optimizing task execution throughout the workflow, ensuring efficient use of time and resources.\n\nimport covalent as ct\n\n# requires installing library covalent-gcpbatch-plugin\ngcp_low_cpu_executor = ct.executor.GCPBatchExecutor(\n   vcpus = 2,  # Number of vCPUs to allocate\n   memory = 512,  # Memory in MB to allocate\n   time_limit = 60,  # Time limit of job in seconds\n   poll_freq = 1,  # Number of seconds to pause before polling for the job's status\n)\ngcp_high_cpu_executor = ct.executor.GCPBatchExecutor(\n   vcpus = 32,  # Number of vCPUs to allocate\n   memory = 32768,  # Memory in MB to allocate\n   time_limit = 60,  # Time limit of job in seconds\n   poll_freq = 1,  # Number of seconds to pause before polling for the job's status\n)\n\n@ct.electron(executor=gcp_low_cpu_executor)\ndef generate_title(\n    article, model_name=\"JulesBelveze/t5-small-headline-generator\",\n    max_tokens=84, temperature=1, no_repeat_ngram_size=2\n):\n    ...\n\n@ct.electron(executor=gcp_high_cpu_executor)\ndef generate_image_from_text(\n\t\treduced_summary, model_name=\"OFA-Sys/small-stable-diffusion-v0\", prompt=\"Impressionist image\"\n):\n    ...\n\n1.3 The lattice decorator\n\nThe @ct.lattice converts a function composed of electrons into a manageable workflow. You can transform a workflow into a lattice simply by adding this decorator to a function:\n\n@ct.lattice\ndef news_content_curator(\nnews_url, image_generation_prompt=\"Impressionist image\",\n    summarizer_model=\"t5-small\", summarizer_max_length=40,\n    title_generating_model=\"JulesBelveze/t5-small-headline-generator\",\n    image_generation_model=\"OFA-Sys/small-stable-diffusion-v0\",\n    temperature=1, max_tokens=64, no_repeat_ngram_size=2,\n    content_analysis_model=\"finiteautomata/bertweet-base-sentiment-analysis\"\n):\n\t\t...\n\n\nOnce a lattice (workflow) is defined, you need to dispatch it to run. You can do this using Covalent by calling ct.dispatch and providing the workflow name and parameters:\n\nnews_url = '<https://www.quantamagazine.org/math-proof-draws-new-boundaries-around-black-hole-formation-20230816/>'\ndispatch_id = ct.dispatch(news_content_curator)(news_url)\n\n\nBelow are the complete workflow steps (find the code here):\n\n@ct.lattice\ndef news_content_curator(\n    news_url, image_generation_prefix,\n    summarizer_model=\"t5-small\",\n    summarizer_max_length=40,\n    title_generating_model=\"JulesBelveze/t5-small-headline-generator\",\n    image_generation_model=\"OFA-Sys/small-stable-diffusion-v0\",\n    temperature=1, max_tokens=64, no_repeat_ngram_size=2,\n    content_analysis_model=\"finiteautomata/bertweet-base-sentiment-analysis\"\n):\n    article = extract_news_content(news_url)\n    content_property = sentiment_analysis(\n        article, model_name=content_analysis_model\n    )\n    reduced_summary = generate_reduced_summary(\n        article, model_name=summarizer_model, max_length=summarizer_max_length\n    )\n    title = generate_title(\n        article, model_name=title_generating_model,\n        temperature=temperature, max_tokens=max_tokens,\n        no_repeat_ngram_size=no_repeat_ngram_size\n    )\n    generated_image = generate_image_from_text(\n        reduced_summary, prompt=image_generation_prefix,\n        model_name=image_generation_model\n    )\n    image_with_title = add_title_to_image(generated_image, title)\n    url = save_image(image_with_title)\n    return {\n        \"content_property\": content_property, \"summary\": reduced_summary,\n        \"title\": title, \"image\": url,\n    }\n\n\nHere is the Covalent workflow for the News article AI summarization app as viewed through the Covalent UI:\n\nWhen a Covalent workflow is executed, a unique identifier called dispatch_id is generated. This ID serves two purposes:\n\nIt acts as a reference for the specific workflow.\nIt allows for the rerun of the entire workflow.\n\nCovalent keeps a record of all previously executed workflows in a scalable database, creating a comprehensive history that you can use to rerun workflows using their respective dispatch IDs.\n\nü§ñ\nNOTE: It‚Äôs important to distinguish between the dispatch (ct.dispatch) and redispatch (ct.redispatch) features. Dispatch is for creating brand-new workflows, whereas redispatch is for refining or duplicating existing workflows.\n\nYou can redispatch a workflow in three ways:\n\nProvide the dispatch_id to the redispatch method to summarize a different news article.\nRerun a workflow while reusing previously computed results. For example, if you want to experiment with a different prompt for generating images from the same news article, while keeping the summarization and headline generation unchanged, you can initiate the workflow again and preserve the use of previous results.\n\n\nredispatch_id = ct.redispatch(\n        dispatch_id, reuse_previous_results=True\n)(new_url, \"Cubistic image\")\n\n\n\nCustomize a previously executed workflow by replacing tasks and using the replace_electrons feature (learn more here):\n\n\ndef classify_news_genre(\n    article, model_name=\"abhishek/autonlp-bbc-news-classification-37229289\"\n):\n    ...\n\nreplace_electrons = {\n    \"sentiment_analysis\": ct.electron(classify_news_genre)\n}\nredispatch_id = ct.redispatch(\n    dispatch_id, replace_electrons=replace_electrons\n)(new_url, \"Cubistic image\", content_analysis_model=\"abhishek/autonlp-bbc-news-classification-37229289\")\n\n2. How to build a Streamlit UI to generate requests for news article summarization\n\nStreamlit lets users adjust parameters for the AI news summarization workflow and trigger previously executed workflows using their dispatch IDs.\n\nThe app sidebar contains the parameters with some proposed default values:\n\nimport streamlit as st\n\nwith st.sidebar:\n    server_location = st.text_input(\n        \"Remote server URL\", value=\"<http://localhost:8085>\"\n    )\n    news_article_url = st.text_input(\n        \"News article URL\",\n        value=\"<https://www.quantamagazine.org/math-proof-draws-new-boundaries-around-black-hole-formation-20230816/>\"\n    )\n    st.header(\"Parameters\")\n\n    # Title generation section\n    st.subheader(\"Title generation parameters\")\n    title_generating_model = headline_generation_models[0]\n    temperature = st.slider(\n        \"Temperature\", min_value=0.0, max_value=100.0, value=1.0,\n        step=0.1\n    )\n    max_tokens = st.slider(\n        \"Max tokens\", min_value=2, max_value=50, value=32,\n    )\n\n    # Image generation section\n    st.subheader(\"Image generation\")\n    image_generation_prefix = st.text_input(\n        \"Image generation prompt\",\n        value=\"industrial style\"\n    )\n    image_generation_model = stable_diffusion_models[0]\n\n    # Text summarization section\n    st.subheader(\"Text summarization\")\n    summarizer_model = news_summary_generation[0]\n    summarizer_max_length = st.slider(\n        \"Summarization text length\", min_value=2, max_value=50, value=20,\n    )\n\n    # Content analysis section\n    st.subheader(\"Content analysis\")\n    selected_content_analysis = st.selectbox(\n        \"Content analysis option\", options=[\n            \"sentiment analysis\",\n            \"genre classification\"\n        ]\n    )\n    if selected_content_analysis == \"sentiment analysis\":\n        content_analysis_model = sentiment_analysis_models[0]\n    else:\n        content_analysis_model = genre_analysis_models[0]\n\n\nThe main part of the app displays the results from the Covalent server (based on the parameters configured in the sidebar). This process generates an AI-generated summary of the news article, a proposed title, and an AI-generated image that represents the content of the news article.\n\nimport requests\n\nst.title(\"News article AI summarization\")\ndispatch_id = st.text_input(\"Dispatch ID\")\n\nif st.button(\"Generate image and text summary\"):\n    st.write(\"Generating...\")\n\n    container = st.container()\n\t\t\n\t\t# select either genre analysis or sentiment analysis\n    selected_content_analysis = parameters.pop('selected_content_analysis')\n    if selected_content_analysis != 'sentiment analysis':\n        replace_electrons = {\n            \"sentiment_analysis\": ct.electron(classify_news_genre)\n        }\n        parameters[\n            \"content_analysis_model\"\n        ] = \"abhishek/autonlp-bbc-news-classification-37229289\"\n    else:\n        replace_electrons = {}\n\n    redispatch_id = ct.redispatch(\n        dispatch_id, reuse_previous_results=True,\n        replace_electrons=replace_electrons\n    )(**parameters)\n\n    covalent_info = ct.get_config()['dispatcher']\n    address = covalent_info['address']\n    port = covalent_info['port']\n    covalent_url = f\"{address}:{port}/{redispatch_id}\"\n\n    st.write(f\"Covalent URL on remote server: http://{covalent_url}\")\n\n    with container:\n        result = ct.get_result(redispatch_id, wait=True).result\n        st.subheader(f\"Article generated title: {result['title']}\")\n        st.write(\n            \"In terms of \" +\n            selected_content_analysis +\n            \" content is: \" + str(result['content_property'])\n        )\n        st.image(result['image'])\n        st.text_area(\n            label=\"AI generated summary\",\n            key=\"summary\",\n            value=result['summary'], disabled=True\n        )\n\n\nLastly, within the Streamlit app, you have the option to start the Covalent server for complete automation. This only needs to be done once.\n\nIf you want to start the Covalent server and execute a predefined workflow (in a file named workflow_remote.py), just include this code:\n\ndef is_covalent_down():\n    out = check_output([\"covalent\", \"status\"])\n    if \"Covalent server is stopped\" in out.decode('utf-8'):\n        return True\n    return False\n\ndef start_covalent():\n    subprocess.run(\"covalent start --no-cluster\", shell=True)\n\ndef run_covalent_workflow(workflow_filename):\n    dispatch_id = check_output([\"python\", workflow_filename]).decode(\"utf-8\")\n    return dispatch_id.strip()\n\nif is_covalent_down():\n    st.write(\"Covalent is not up. Starting Covalent...\")\n    start_covalent()\n    \n\t\tdispatch_id = run_covalent_workflow(\"workflow_remote.py\")\n\n    # wait for result\n    ct.get_result(dispatch_id, wait=True)\n    st.session_state['dispatch_id'] = dispatch_id\n\n3. How to summarize news articles from Quanta\n\nOnce you construct the Covalent workflows, you can repeatedly execute them through Streamlit. This provides an interactive environment for easily running complex ML workflows and fine-tuning their parameters.\n\nTo get started, launch the app and copy the Covalent workflow dispatch IDs.\n\nü§ñ\nNOTE: To run everything, make sure you have both services running: the Covalent server (covalent start) and the Streamlit app (streamlit run streamlit_app.py).\nWrapping up\n\nYou‚Äôve learned how to build complex ML workflows using an example of a news summarization app. In this setup, a Covalent server handles the ML workflows, while a Streamlit interface manages user interactions. The communication between the two is facilitated through a single (dispatch) ID, which simplifies resource management, improves efficiency, and allows you to focus on the ML aspects.\n\nIf you found this interesting, please note that Covalent is a free and open-source tool. You can find more information and additional tutorials in our docs.\n\nHappy workflow building! ‚öôÔ∏è\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Advocate Posts...\n\nView even more ‚Üí\n\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts from Streamlit Advocates",
    "url": "https://blog.streamlit.io/tag/advocates/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Advocate Posts\n67 posts\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nDeep-learning apps for image processing made easy: A step-by-step guide\n\nLearn how to develop custom deep-learning apps using image processing models with Streamlit\n\nAdvocate Posts\nby\nMainak Chaudhuri\n,\nAugust 22 2023\nPureHuB: A search engine for your university\n\nA step-by-step guide to creating an inverted index search app using Python and Streamlit\n\nAdvocate Posts\nby\nMala Deep Upadhaya\n,\nAugust 10 2023\nData analysis with Mito: A powerful spreadsheet in Streamlit\n\nReplace st.dataframe or st.data_editor with the Mito spreadsheet to edit dataframes in your app\n\nAdvocate Posts\nby\nNate Rush\n,\nAugust 8 2023\nSimiLo: Find your best place to live\n\nA 5-step guide on how I built an app to relocate within the U.S.\n\nAdvocate Posts\nby\nKevin Soderholm\n,\nAugust 4 2023\nInstant Insight: Generate data-driven presentations in a snap!\n\nCreate presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery\n\nLLMs\nby\nOleksandr Arsentiev\n,\nAugust 2 2023\nTrubrics: A user feedback tool for your AI Streamlit apps\n\nA 3-step guide on collecting, analyzing, and managing AI model feedback\n\nAdvocate Posts\nby\nJeff Kayne\n,\nJuly 28 2023\nChat2VIS: AI-driven visualisations with Streamlit and natural language\n\nLeverage ChatGPT for Python code generation using prompt engineering\n\nLLMs\nby\nPaula Maddigan\n,\nJuly 27 2023\nImproving healthcare management with Streamlit\n\nHow to build an all-in-one analytics platform for small clinics\n\nAdvocate Posts\nby\nMatteo Ballabio and¬†\n1\n¬†more,\nJuly 17 2023\nStreamlit and iFood: Empowering the Monitor Rosa project\n\nHarnessing technology and corporate support for social impact\n\nAdvocate Posts\nby\nHeber Augusto Scachetti\n,\nJuly 14 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Comparing data visualisations from Code Llama, GPT-3.5, and GPT-4",
    "url": "https://blog.streamlit.io/comparing-code-llama-vs-gpt-3-5-and-gpt-4-to-generate-visualisations/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nBy Paula Maddigan\nPosted in Tutorials, October 16 2023\nWhy I chose to compare Code Llama and GPT models\nQuick overview of Chat2VIS\n6 case studies using Chat2VIS to compare Code Llama vs. GPT-3.5 Instruct and GPT-4\nCase study 1: Generate code for bar chart\nCase study 2: Generate code for time series\nCase study 3: Plotting request with an unspecified chart type\nCase study 4: Parsing complex requests\nCase study 5: Misspelled prompts\nCase study 6: Ambiguous prompts\nHow to improve success with open-source models\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nGenerative AI is moving at lightning speed ‚ö°Ô∏è, and you don't want to blink. New LLMs brimming with exciting features consistently seize the headlines of my news feeds.\n\nWith Chat2VIS, you can use natural language to prompt up to 5 LLMs to generate Python code that builds plots from a dataset. (Learn more about why I built Chat2VIS from my journal article.)\n\nI wanted to put 3 of the latest LLMs to the test, comparing their performance in generating code for various visualisations. From creating bar charts and time series data, to handling misspelled words and ambiguous prompts, I uncover how each model responds.\n\nThe results provide interesting insights into the strengths and limitations of these models, with a focus on Code Llama‚Äôs potential and the benefits of GPT-3.5 Instruct and GPT-4.\n\nIn this post, you'll discover:\n\nWhy I chose to compare Code Llama vs. OpenAI models\nSix scenarios where Chat2VIS compares Code Llama, GPT-3.5 Instruct, and GPT-4 models\nTip to improve success with open-source models\nWhy I chose to compare Code Llama and GPT models\n\nOpen source models, like Code Llama, are free to use, and easier to fine-tune on your own data. OpenAI models are easy to use ‚Äúout of the box‚Äù, with some now available to fine-tune, but come with a cost. Historically, I‚Äôve faced challenges using open source models, finding they often misunderstood the request or failed completely to generate accurate Python code.\n\nWhen Code Llama was released, an LLM tuned for code generation, I was keen to see how it compared to the OpenAI models.\n\nI've been impressed with Code Llama, which shows great potential for this task without incurring the cost associated with the OpenAI models (read more here).\n\nI have opted for the \"Instruct\" fine-tuned variation of Code Llama. It aligns well with the existing prompt style, which issues instructions in natural language followed by the beginning of a Python code script.\n\nLet's see how it stacks up against OpenAI's GPT-4 and their recent release of GPT-3.5 Instruct.\n\nQuick overview of Chat2VIS\n\nBefore we begin comparing LLMs, here‚Äôs a look again at how Chat2VIS works (check out the full blog post to learn more.)\n\nChat2VIS App Architecture\n6 case studies using Chat2VIS to compare Code Llama vs. GPT-3.5 Instruct and GPT-4\n\nUsing Chat2VIS, I tested how each model performed based on 6 different scenarios.\n\nüí°\nTo try for yourself: follow these instructions to generate your HuggingFace API token (no credit required) for Code Llama. Acquire an OpenAI API key here and add some credit to your account. I'll walk you through all the examples from the Chat2VIS published article, this time using GPT-4, the new GPT-3.5 Instruct model, and Code Llama.\n\nFor each example, choose the dataset from the sidebar radio button options, select the models using the checkboxes, and enter your API keys for OpenAI and HuggingFace.\n\nCase study 1: Generate code for bar chart\n\nThis example uses the pre-loaded \"Department Store\" dataset.\n\nRun the following query: \"What is the highest price of product, grouped by product type? Show a bar chart, and display by the names in desc.‚Äù\n\nKudos to all three models for producing the same results! (Even though they may have different labels and titles.)\n\nGPT-4 ‚úÖ\nCode Llama ‚úÖ\nGPT-3.5 Instruct ‚úÖ\nCase study 2: Generate code for time series\n\nUsing the \"Energy Production\" dataset, run the query: \"What is the trend of oil production since 2004?‚Äù\n\nResults\n\nImpressive! All three models generated almost identical plots, showing data from 2004 onwards.\n\nGPT-4 ‚úÖ\nCode Llama ‚úÖ\nGPT-3.5 Instruct ‚úÖ\nCase study 3: Plotting request with an unspecified chart type\n\nHere, I‚Äôm using the pre-loaded \"Colleges\" dataset in the sidebar radio button.\n\nRun the query: \"Show debt and earnings for Public and Private colleges.\"\n\nResults\n\nGPT-4 ‚úÖ\nCode Llama ü§î\nDuring the initial runs of this example, I discovered that Code Llama had some limitations similar to other legacy OpenAI models. It repeatedly attempted to generate scatter plot code assigning invalid values to the function‚Äôs c parameter, as also mentioned in this article. As a result, the code failed to execute. To improve its success rate, I made a slight adjustment to the prompt (for the exact wording, delve into the prompt engineering within this code).\nGPT-3.5 Instruct ü§î plotted average values, maybe not quite as informative as the other models.\nCase study 4: Parsing complex requests\n\nLet's examine a more complex example where the models need to select a subset of the data. Using the Customers & Products dataset, run the query: \"Show the number of products with price higher than 1000 or lower than 500 for each product name in a bar chart, and could you rank y-axis in descending order?‚Äù\n\nResults\n\nGPT-4 succeeded in this case ‚úÖ\nGPT-3.5 Instruct produced an empty plot ‚ùå\nIt's surprising that GPT-3.5 Instruct didn't succeed, as this query has previously worked for ChatGPT3.5, GPT-3, and Codex.\nCode Llama also failed ‚ùå¬†for several reasons.\nIt did not filter the data to include only prices higher than $1000 or lower than $500.\nIt didn‚Äôt sort the data as requested.\nI encountered these kinds of limitations frequently while exploring Code Llama's capabilities.\nCase study 5: Misspelled prompts\n\nReturning to the \"Movies\" dataset, let's see how Code Llama handles misspelled words. Run the query: ‚Äúdraw the numbr of movie by gener.‚Äù\n\nLook at that! Each model overlooked my spelling mistakes!\n\nGPT-4 ‚úÖ\nGPT-3.5 Instruct ‚úÖ\nCode Llama ‚úÖ\nWhile it didn‚Äôt sort the results in the same order as the OpenAI models, the prompt didn't specify any sorting.\nCode Llama, that uninformative legend is not very helpful!\nCase study 6: Ambiguous prompts\n\nContinuing with the \"Movies\" dataset, let's submit the single word ‚Äútomatoes‚Äù and observe how the models process it.\n\nResults\n\nGPT-4 ‚úÖ\nCode Llama ‚úÖ\nGPT-3.5 Instruct ‚ùå¬† This model did not identify a relevant ‚Äútomato‚Äù visualisation to the movie data set.\nHow to improve success with open-source models\n\nI have compared the performance of Code Llama, GPT-3.5 Instruct, and ChatGPT-4 using examples from published research previously showcasing ChatGPT-3.5, GPT-3, and Codex.\n\nInitial experiments show promise, but the OpenAI models still outperform Code Llama in several scenarios. I encourage you to experiment and share your opinions.\n\nIn the future, I plan to enhance the prompt further and explore various other prompting techniques to potentially improve Code Llama's accuracy. Although I want to avoid overcomplicating the instructions, I acknowledge its potential for improvement.\n\nFor this task, considering my prompting style, ChatGPT-4 is my preferred choice.\n\nHowever, taking into consideration the comparable results of ChatGPT-3.5 in the journal article and previous blog, together with the lower cost of the GPT-3.5 models (costs here), I would ultimately still choose ChatGPT-3.5. Nonetheless, it may be worthwhile to fine-tune a Code Llama model to further explore its capabilities, as it offers a cost-effective solution for Chat2VIS.\n\nWrapping up\n\nThank you for reading my post!\n\nI‚Äôd love to hear your opinions and the outcomes of your experiments. If you have any questions, please post them in the comments below or contact me on LinkedIn.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Vicky Kuo - Streamlit",
    "url": "https://blog.streamlit.io/author/vicky/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Vicky Kuo\n1 post\nLand your dream job: Build your portfolio with Streamlit\n\nShowcase your coding skills to recruiters with a chatbot tailored to your resume\n\nby\nVicky Kuo\n,\nOctober 13 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Paula Maddigan - Streamlit",
    "url": "https://blog.streamlit.io/author/paula/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Paula Maddigan\n2 posts\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nChat2VIS: AI-driven visualisations with Streamlit and natural language\n\nLeverage ChatGPT for Python code generation using prompt engineering\n\nLLMs\nby\nPaula Maddigan\n,\nJuly 27 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "How to build a real-time LLM app without vector databases",
    "url": "https://blog.streamlit.io/build-a-real-time-llm-app-without-vector-databases-using-pathway/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nBy Bobur Umurzokov\nPosted in LLMs, October 19 2023\nThe role of Pathway and LLM App\nOverall app architecture\nHow to build a real-time discount tracking app\nStep 1. Data collection (custom data ingestion)\nStep 2. Data loading and mapping\nStep 3. Data embedding\nStep 4. Data indexing\nStep 5. User query processing and indexing\nStep 6. Similarity search and prompt engineering\nStep 7. Return the response\nStep 8. Put everything together\nStep 9. Design the UI with Streamlit\nStep 10. Run the app\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüëâ\nTL;DR: Learn how to build a discount finder app without using vector databases, additional frameworks, and a complex stack. Use the project source code to clone the repo and run the code sample by following the instructions in the README.md file.\n\nEver tried asking ChatGPT a question about real-time discounts, deals, or coupons?\n\nFor example, ‚ÄúCan you give me discounts for Adidas men's shoes?‚Äù If you did, I‚Äôm sure you‚Äôve been frustrated by the generic response it gave you, ‚ÄúI‚Äôm sorry, but I don‚Äôt have real-time growing capabilities or access to current promotion.‚Äù\n\nWhy? Because GPT lacks specific information.\n\nChallenges of Existing Solutions\n\nYou could try typing in a single JSON item from the Amazon products deal, but you‚Äôll face two problems:\n\nText length. The text length is restricted‚Äîa big problem when dealing with thousands of sale items.\nUnusable data. The data may need to be cleaned and formatted.\n0:00\n/0:20\n1√ó\n\nYou could also try using the OpenAI Chat Completion endpoint or building custom plugins, but you‚Äôll face additional problems:\n\nCost. Providing more detailed information and examples to improve the model's performance can increase costs. For example, with GPT-4, the cost is $0.624 per prediction for an input of 10k tokens and an output of 200 tokens. Sending identical requests repeatedly can escalate costs unless you use a local cache system.\nLatency. Utilizing ChatGPT APIs for production, like those from OpenAI, can be unpredictable in terms of latency. There is no guarantee of consistent service provision.\nSecurity. Integrating custom plugins requires specifying every API endpoint in the OpenAPI spec for functionality. This means exposing your internal API setup to ChatGPT, which may be a risk that many enterprises are skeptical of.\nOffline evaluation. When you conduct offline tests on code and data output or replicate the data flow locally, each system request may yield varying responses.\n\nTo solve these challenges (and to buy cool Adidas shoes at a discount, of course! üëü), I built a custom Language Learning Model (LLM) discount finder app without using vector databases, additional frameworks, and a complex stack.\n\nThe same solution can be applied to develop production-ready AI apps that use real-time data available in your data sources.\n\nIn this post, I‚Äôll walk through 10 steps on how to develop and expose an AI-powered HTTP REST API using Pathway and LLM App and design the UI with Streamlit to consume the API data through REST.\n\nThe role of Pathway and LLM App\n\nPathway is a powerful data processing framework in Python that takes care of real-time data updates from various data sources using its built-in connectors for structured, unstructured, and live data. For the discount finder app, I used Pathway to ingest sales data as streams to app and make sure that the app detects every change in a data input that changes frequently.\n\nLLM App is a production Python framework for building and serving AI applications. LLM App uses Pathway libraries under the hood to achieve real-time data indexing and vector similarity search. Using a combination of these two tools, the app is not only aware of changes in the documents but also updates vector indexes in real time and uses this new knowledge to answer the next questions without the need for storing and retrieving vector indexes to/from a vector database.\n\nOverall app architecture\n\nLet‚Äôs take a look at the app‚Äôs overall architecture. I was inspired by this article and wanted my app to expose the HTTP REST API endpoint‚Äîso you could get the best deals by using CSVs, JSON Lines, APIs, message brokers, or databases.\n\nThe app supports two types of data sources (if you want, you can add custom input connectors):\n\nJSON Lines: The data source expects each line to contain a doc object. Make sure to convert your input data to the Jsonlines format. You can find a sample data file at discounts.jsonl.\nRainforest Product API: This API gives you the daily discount data from Amazon products.\n\nGo to the app and try typing in ‚ÄúShow me discounts‚Äù:\n\n0:00\n/0:23\n1√ó\n\nThe app will index Rainforest API¬†and an example¬†discounts.csv¬†file documents in real-time and use the data when processing queries.\n\nHow to build a real-time discount tracking app\nStep 1. Data collection (custom data ingestion)\n\nTo add custom data for ChatGPT, you need to build a data pipeline for ingesting, processing, and exposing data in real-time.\n\nFor simplicity, use any JSON Lines file as a data source. The app accepts files like discounts.jsonl and uses this data when processing user queries. Each line in the data source should contain a doc object. Make sure to convert your input data to JSON Lines format.\n\nHere is an example of a JSON Lines file with a single entry:\n\n{\"doc\": \"{'position': 1, 'link': '<https://www.amazon.com/deal/6123cc9f>', 'asin': 'B00QVKOT0U', 'is_lightning_deal': False, 'deal_type': 'DEAL_OF_THE_DAY', 'is_prime_exclusive': False, 'starts_at': '2023-08-15T00:00:01.665Z', 'ends_at': '2023-08-17T14:55:01.665Z', 'type': 'multi_item', 'title': 'Deal on Crocs, DUNLOP REFINED(\\\\u30c0\\\\u30f3\\\\u30ed\\\\u30c3\\\\u30d7\\\\u30ea\\\\u30d5\\\\u30a1\\\\u30a4\\\\u30f3\\\\u30c9)', 'image': '<https://m.media-amazon.com/images/I/41yFkNSlMcL.jpg>', 'deal_price_lower': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48'}, 'deal_price_upper': {'value': 52.14, 'currency': 'USD', 'symbol': '$', 'raw': '52.14'}, 'deal_price': 35.48, 'list_price_lower': {'value': 49.99, 'currency': 'USD', 'symbol': '$', 'raw': '49.99'}, 'list_price_upper': {'value': 59.99, 'currency': 'USD', 'symbol': '$', 'raw': '59.99'}, 'list_price': {'value': 49.99, 'currency': 'USD', 'symbol': '$', 'raw': '49.99 - 59.99', 'name': 'List Price'}, 'current_price_lower': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48'}, 'current_price_upper': {'value': 52.14, 'currency': 'USD', 'symbol': '$', 'raw': '52.14'}, 'current_price': {'value': 35.48, 'currency': 'USD', 'symbol': '$', 'raw': '35.48 - 52.14', 'name': 'Current Price'}, 'merchant_name': 'Amazon Japan', 'free_shipping': False, 'is_prime': False, 'is_map': False, 'deal_id': '6123cc9f', 'seller_id': 'A3GZEOQINOCL0Y', 'description': 'Deal on Crocs, DUNLOP REFINED(\\\\u30c0\\\\u30f3\\\\u30ed\\\\u30c3\\\\u30d7\\\\u30ea\\\\u30d5\\\\u30a1\\\\u30a4\\\\u30f3\\\\u30c9)', 'rating': 4.72, 'ratings_total': 6766, 'page': 1, 'old_price': 49.99, 'currency': 'USD'}\"}\n\n\nThe app is always aware of the changes in the data folder. If you add another JSON Lines file, it will automatically update the AI model's response.\n\nStep 2. Data loading and mapping\n\nUsing Pathway's JSON Lines input connector, read the local JSON Lines file, map data entries into a schema, and create a Pathway Table (see the full source code in app.py):\n\n...\nsales_data = pw.io.jsonlines.read(\n    \"./examples/data\",\n    schema=DataInputSchema,\n    mode=\"streaming\"\n)\n\n\nMap each data row into a structured document schema (see the full source code in¬†app.py):\n\nclass DataInputSchema(pw.Schema):\n    doc: str\n\nStep 3. Data embedding\n\nEach document is¬†embedded¬†with the OpenAI API and retrieves the embedded result (see the full source code in¬†embedder.py):\n\n...\nembedded_data = embeddings(context=sales_data, data_to_embed=sales_data.doc)\n\nStep 4. Data indexing\n\nConstruct an instant index on the generated embeddings:\n\nindex = index_embeddings(embedded_data)\n\nStep 5. User query processing and indexing\n\nCreate a REST endpoint, take a user query from the API request payload, and embed the user query with the OpenAI API.\n\n...\nquery, response_writer = pw.io.http.rest_connector(\n    host=host,\n    port=port,\n    schema=QueryInputSchema,\n    autocommit_duration_ms=50,\n)\n\nembedded_query = embeddings(context=query, data_to_embed=pw.this.query)\n\nStep 6. Similarity search and prompt engineering\n\nTo perform a similarity search, utilize the index to identify the most relevant matches for the query embedding. Then create a prompt that combines the user's query with the retrieved relevant data results. This prompt is then sent to the ChatGPT completion endpoint to generate a comprehensive and detailed response.\n\nresponses = prompt(index, embedded_query, pw.this.query)\n\n\nYou used the same in-context learning approach when creating the prompt and incorporated internal knowledge into ChatGPT in the prompt.py file.\n\nprompt = f\"Given the following discounts data: \\\\\\\\n {docs_str} \\\\\\\\nanswer this query: {query}\"\n\nStep 7. Return the response\n\nThe final step is just to return the API response to the user.\n\n# Build prompt using indexed data\nresponses = prompt(index, embedded_query, pw.this.query)\n\nStep 8. Put everything together\n\nCombine all the steps to get a Python API enabled with LLM for custom discount data. You can use it by referring to the implementation in the app.py Python script.\n\nimport pathway as pw\n\nfrom common.embedder import embeddings, index_embeddings\nfrom common.prompt import prompt\n\ndef run(host, port):\n    # Given a user question as a query from your API\n    query, response_writer = pw.io.http.rest_connector(\n        host=host,\n        port=port,\n        schema=QueryInputSchema,\n        autocommit_duration_ms=50,\n    )\n\n    # Real-time data coming from external data sources such as jsonlines file\n    sales_data = pw.io.jsonlines.read(\n        \"./examples/data\",\n        schema=DataInputSchema,\n        mode=\"streaming\"\n    )\n\n    # Compute embeddings for each document using the OpenAI Embeddings API\n    embedded_data = embeddings(context=sales_data, data_to_embed=sales_data.doc)\n\n    # Construct an index on the generated embeddings in real-time\n    index = index_embeddings(embedded_data)\n\n    # Generate embeddings for the query from the OpenAI Embeddings API\n    embedded_query = embeddings(context=query, data_to_embed=pw.this.query)\n\n    # Build prompt using indexed data\n    responses = prompt(index, embedded_query, pw.this.query)\n\n    # Feed the prompt to ChatGPT and obtain the generated answer.\n    response_writer(responses)\n\n    # Run the pipeline\n    pw.run()\n\nclass DataInputSchema(pw.Schema):\n    doc: str\n\nclass QueryInputSchema(pw.Schema):\n    query: str\n\nStep 9. Design the UI with Streamlit\n\nUse Streamlit to make your app more interactive (refer to the implementation in the app.py file). You can build UI for your backend services without having knowledge of front-end tools. The use of Streamlit's st.sidebar allows for the organization of secondary information, keeping the main area focused on the primary interaction. You create a sidebar to explain to users how to use the app:\n\nwith st.sidebar:\n    st.markdown(\n        \"## How to use\\\\n\"\n        \"1. Choose data sources.\\\\n\"\n        \"2. If CSV is chosen as a data source, upload a CSV file.\\\\n\"\n        \"3. Ask a question about the discounts.\\\\n\"\n    )\n\n\nUsers are presented with a multi-select dropdown to choose data sources, and if CSV is chosen, they can upload a CSV file via the st.file_uploader widget. Streamlit's declarative nature stands out in the code, with the interface updating based on the state of variables. For example, the file uploader's disabled state is linked to the selected data sources.\n\nuploaded_file = st.file_uploader(\n    \"Upload a CSV file\",\n    type=(\"csv\"),\n    disabled=(DataSource.CSV.value not in data_sources)\n)\n\n\nOnce a CSV file is uploaded, its content is processed and written into a jsonlines file format, displaying a progress bar to inform the user of the ongoing operation. and the progress bar offers real-time feedback while processing the uploaded CSV.\n\nif uploaded_file and DataSource.CSV.value in data_sources:\n    df = pd.read_csv(uploaded_file)\n\n    # Start progress bar\n    progress_bar = st.progress(0, \"Processing your file. Please wait.\")\n\n\nDepending on the selected data sources and the provided question, the application interfaces with a Discounts API to fetch relevant answers.\n\nquestion = st.text_input(\n    \"Search for something\",\n    placeholder=\"What discounts are looking for?\",\n    disabled=not data_sources\n)\n\n\nHere is the code that handles Discounts API requests when the user selects a data source and asks a question. Error messages and responses from the API are handled smoothly, giving direct feedback to the user through st.error and st.write methods.\n\nif data_sources and question:\n    if not os.path.exists(csv_path) and not os.path.exists(rainforest_path):\n        st.error(\"Failed to process discounts file\")\n\n    url = f'http://{api_host}:{api_port}/'\n    data = {\"query\": question}\n\n    response = requests.post(url, json=data)\n\n    if response.status_code == 200:\n        st.write(\"### Answer\")\n        st.write(response.json())\n    else:\n        st.error(f\"Failed to send data to Discounts API. Status code: {response.status_code}\")\n\nStep 10. Run the app\n\nFollow the instructions in the README.md file's How to run the project section to run the app. Note that you‚Äôll need to run the API and UI as separate processes. Streamlit will automatically connect to the Discounts backend API, and you‚Äôll see the UI frontend running in your browser.\n\nIn this tutorial, Pathway's LLM App and Streamlit communicate over HTTP REST API. You can run the app using Docker with a single docker compose up command (refer to the run with the Docker section in the README.md file). The inability to embed the LLM App into Streamlit as a single process is due to Streamlit having its own program lifecycle loop, which triggers a complete app rerun whenever there is a change. This behavior can disrupt the data flow, especially since Pathway operates in streaming mode. Considering the above, there are two more ways to integrate Pathway's LLM app with Streamlit:\n\nRun Pathway's LLM app as a subprocess and communicate with it over inter-process communications such as sockets or TCP/IP. This can involve using random ports or signals to trigger actions like state dumps that can be picked up or pickled. For example, you can leverage Python‚Äôs Subprocess module to achieve that.\nPathway's LLM App and Streamlit share the same file storage to communicate. For example, you upload documents with a user query to a folder on your local disk. LLM App can listen to every change in that folder and access the files to process them, answer user queries, and write responses back to the file.\nWrapping up\n\nI‚Äôve only scratched the surface of what you can do with an LLM app by incorporating domain-specific knowledge like discounts into ChatGPT. You can also:\n\nIncorporate additional data from external APIs, formats such as JSON Lines, PDF, Doc, HTML, or text, and databases like PostgreSQL or MySQL.\nStream data from platforms like Kafka, Redpanda, or Debedizum.\nEnhance the Streamlit UI to accept any deals API, not just the Rainforest API.\nMaintain a data snapshot to observe changes in sales prices over time. Pathway provides a built-in feature to calculate differences between two alterations.\nSend processed data to other downstream connectors, such as BI and analytics tools. For example, you can configure it to receive alerts when price shifts are detected.\n\nIf you have any questions, please leave them in the comments section below or contact me on LinkedIn and Twitter. Join the Discord channel to see how the AI ChatBot assistant works.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Bobur Umurzokov - Streamlit",
    "url": "https://blog.streamlit.io/author/bobur/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Bobur Umurzokov\n1 post\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Take your Streamlit apps to the next level with¬†GPT-4",
    "url": "https://blog.streamlit.io/take-your-streamlit-apps-to-the-next-level-with-gpt-4/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nBy Charly Wargnier\nPosted in LLMs, October 24 2023\nSome background\nThe rise of¬†ChatGPT\nGPT-4, the game¬†changer\nWhat about trustworthiness in¬†LLMs\n#1: Use GPT-4 for faster Streamlit app development\n1.1‚Ää‚Äî‚ÄäGPT-4 as a starting point for any¬†app\n1.2‚Ää‚Äî‚ÄäPrompting tips to streamline your app¬†design\n1.3‚Ää‚Äî‚ÄäConvert Jupyter notebooks to Streamlit apps in¬†minutes\n#2: Lightning fast debugging + enhanced codebase management\n2.1‚Ää‚Äî‚ÄäSwiftly analyze error log traces to identify¬†issues\n2.2‚Ää‚Äî‚ÄäOvercoming the context window limitations\n2.3‚Ää‚Äî‚ÄäReshape any code, anywhere in your¬†codebase\n#3: From sluggish to speedy, use GPT-4 to improve your app performance\n3.1‚Ää‚Äî‚ÄäAutomatically diagnose performance issues\n3.2‚Ää‚Äî‚ÄäDiagnosing caching issues in large codebases\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nDesigning and scaling a Streamlit app can be a daunting task! As developers, we often face challenges like designing good UIs, debugging our apps quickly, and making them fast.\n\nWhat if there was a tool to speed it all up?\n\nThis tool has a name, it is called GPT-4!\n\nIn this guide, we‚Äôll be taking a look at:\n\nThe evolution of ChatGPT, from its rise to understanding the trustworthiness of large language models.\nLightning-fast app development with GPT-4, including prompt tips and notebook-to-Streamlit app conversion.\nEfficient debugging and codebase changes with GPT-4.\nApp performance optimization with GPT-4, and without the stress!\n\nWhether you‚Äôre a seasoned Streamlit developer or just getting started, this guide will help you leverage GPT-4 to build better apps, faster.\n\nSo grab a cup of coffee (or tea, or whatever your favorite beverage is) and let‚Äôs get started!\n\nSome background\nThe rise of¬†ChatGPT\n\nIn November 2022, OpenAI released ChatGPT, and it immediately took the world by storm!\n\nFor the first time, people could have meaningful conversations with an AI on any topic and use it for tasks spanning education, creative writing, legal research, personal tutoring, code creation, and more.\n\nAs of January 2023, it has over 100 million users, making it the fastest-growing platform ever.\n\nBut you might be wondering, how did early-day ChatGPT, powered by GPT-3.5, go at designing Streamlit apps?\n\nIt was actually far from perfect, often inaccurate, and required a hefty amount of manual fine-tuning to get the apps right.\n\nGPT-4, the game¬†changer\n\nReleased in March 2023, GPT-4 significantly improved short-term memory, parameters, and creativity, leading to far more accurate and creative responses than GPT-3.5.\n\nAnother notable perk: GPT-4 was trained on more recent data (up to January 2022) compared to GPT-3.5‚Äôs September 2021 cut-off.\n\nSo even though GPT-4 may not be aware of Streamlit‚Äôs latest feats such as Chat elements or st.experimental_connection(), its newer training data and the aforementioned improvements have really bolstered its ability to create great Streamlit apps.\n\nWhat about trustworthiness in¬†LLMs\n\nAlthough GPT-4 has improved its reliability, like other LLMs, it can still produce misleading or fictional outputs known as hallucinations (here‚Äôs a good read about them).\n\nThese can be attributed to lack of recent data, biases in the training data, and unclear or ambiguous prompts.\n\nYou can usually address them by refining your prompts iteratively until you achieve the desired results. Later we‚Äôll cover prompting tips to reduce hallucinations. I‚Äôll also explain how to improve data robustness via GPT-4‚Äôs Code Interpreter.\n\n#1: Use GPT-4 for faster Streamlit app development\n1.1‚Ää‚Äî‚ÄäGPT-4 as a starting point for any¬†app\n\nThese days, I usually start with GPT-4 when designing any Streamlit app. I then iterate via the chat interface to quickly experiment with various prompt ideas.\n\nGetting started is easy as 1, 2, 3:\n\nIt starts with a good prompt!\n‚úèÔ∏è\nWrite a humorous Streamlit app about how Adrien Treuille, an LLM genius and developer extraordinaire at Snowflake, is an LLM genius.\n\nInclude an Altair bar chart and an Altair line chart. Include at least 2 numerical input sliders in the sidebar\n\n- 1 slider for the Altair bar chart at the top of the app\n- 1 slider for the Altair line chart at the bottom of the app\n\nThe app should allow Adrien to predict the rise of generative AI in the next 5 years. Will we reach singularity?\nGo to ChatGPT and select GPT-4. You'll need a ChatGPT Plus subscription to access it.\nPaste the prompt into the ChatGPT.\nTry the generated code on your local machine. If you‚Äôre new to Streamlit, follow the installation steps here.\n\nLet‚Äôs check the Streamlit app generated by GPT-4 below:\n\nVerdict: the app is impeccable! Not a single change had to be made to make the app work.\n\n1.2‚Ää‚Äî‚ÄäPrompting tips to streamline your app¬†design\n\n‚ÄúGarbage in, Garbage out‚Äù applies well to prompting. The code quality greatly depends on the prompts you put in!\n\nSo what makes a good prompt for designing Streamlit apps?\n\nIn the prompt above, my instructions are clear. I specified the tone, context, app features, and added the charts and widgets I wanted to see.\n\nHere are 5 prompting tips to help you get started:\n\nBy following these simple guidelines, you should be able to get the desired results faster, reducing the need for back-and-forth conversations in the ChatGPT interface.\n\nNow, let‚Äôs explore some more practical use cases!\n\n1.3‚Ää‚Äî‚ÄäConvert Jupyter notebooks to Streamlit apps in¬†minutes\n\nIf creating Streamlit apps from scratch is fun, converting your Jupyter notebooks into Streamlit apps is even funner!\n\nGPT-4 can seamlessly automate most of the these conversion steps, such as:\n\nReplacing traditional Python syntax with their Streamlit counterparts.\nRemove Jupyter-specific imports and replace them with Streamlit-specific ones.\nExcluding Jupyter-specific components like cell magics (%%) and comments.\n\nBelow is a Jupyter notebook. I‚Äôm asking GPT-4 to convert it into a Streamlit app.\n\nJupyter notebook‚Ää‚Äî‚Ääbefore GPT-4 processing:\n\n# [Cell 1]\nimport numpy as np\nimport matplotlib.pyplot as plt\n# [Cell 2]\n# Generate random data\ndata = np.random.randn(1000)\n [Cell 2]\n# Generate random data\ndata = np.random.randn(1000)\n# [Cell 3]\n# Cell magic for inline plots\n%%matplotlib inline\n# [Cell 4]\n# User input for number of bins\nbins = int(input(\"Enter number of bins for histogram: \"))\n# [Cell 5]\n# Plot histogram\nplt.hist(data, bins=bins)\nplt.title(\"Random Data Histogram\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nConverted Streamlit app‚Ää‚Äî‚Ääafter GPT-4 processing:\n\nimport streamlit as st\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Title for the Streamlit app\nst.title(\"Random Data Histogram\")\n# Generate random data\ndata = np.random.randn(1000)\n# User input for number of bins using Streamlit's slider\nbins = st.slider(\"Choose number of bins for histogram\", 10, 100, 50)\n# Plot histogram\nfig, ax = plt.subplots()\nax.hist(data, bins=bins)\nax.set_title(\"Random Data Histogram\")\nax.set_xlabel(\"Value\")\nax.set_ylabel(\"Frequency\")\nst.pyplot(fig)\n\nGPT-4 did it again!\n\nPython syntax has been converted to the equivalent Streamlit syntax.\nJupyter-specific imports were replaced with Streamlit ones.\nJupyter-specific components like cell magics and redundant comments were removed.\n\nOnward to debugging apps with GPT-4!\n\n#2: Lightning fast debugging + enhanced codebase management\n2.1‚Ää‚Äî‚ÄäSwiftly analyze error log traces to identify¬†issues\n\nAs a developer advocate, a big part of my job is about debugging Streamlit apps.\n\nI build demos, submit pull requests, and spend time on forums assisting users in our community.\n\nAnd while it‚Äôs crucial to always double-check the accuracy of its outputs, GPT-4 is still a tremendous tool to quickly scan through error logs and find the root causes of any bugs or issues.\n\nHere‚Äôs a simple example.\n\nPaste this cryptic error message from Streamlit‚Äôs error logs into ChatGPT‚Äôs interface and press send:\n\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py‚Äù, line 552, in _run_script\nexec(code, module.dict)\nFile ‚Äú/mount/src/stroke_probability/Stroke_Proba.py‚Äù, line 66, in\nsvm1, svm2, logit1, logit2, nbc1, nbc2, rf1, rf2, errGBR = loadAllModels(URL)\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py‚Äù, line 211, in wrapper\nreturn cached_func(*args, **kwargs)\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py‚Äù, line 240, in call\nreturn self._get_or_create_cached_value(args, kwargs)\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py‚Äù, line 266, in _get_or_create_cached_value\nreturn self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py‚Äù, line 320, in _handle_cache_miss\ncomputed_value = self._info.func(*func_args, **func_kwargs)\nFile ‚Äú/mount/src/stroke_probability/Stroke_Proba.py‚Äù, line 58, in loadAllModels\njoblib.load(\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/joblib/numpy_pickle.py‚Äù, line 577, in load\nobj = _unpickle(fobj)\nFile ‚Äú/home/adminuser/venv/lib/python3.9/site-packages/joblib/numpy_pickle.py‚Äù, line 506, in _unpickle\nobj = unpickler.load()\nFile ‚Äú/usr/local/lib/python3.9/pickle.py‚Äù, line 1212, in load\ndispatchkey[0]\n\n\nGPT-4 will analyze the error trace and provide relevant recommendations in seconds:\n\n(Link to the chat)\n\nFast. Efficient. And not a single StackO or Google search in sight! Hallelujah!\n\nBut what if we need to go beyond simple error log trace debugging and review bugs and code changes not only for a single file but multiple files?\n\nLet‚Äôs see how GPT-4 can help us.\n\n2.2‚Ää‚Äî‚ÄäOvercoming the context window limitations\n\nIn LLM terminology, the context window refers to the maximum number of tokens (words or characters) that a language model can ‚Äúsee‚Äù at once when generating a response.\n\nThe GPT-4 model available in ChatGPT Plus has a context window limit of 8,192 tokens, which is twice what GPT-3.5 currently provides.\n\nHowever, even with 8,192 tokens, it may not be sufficient to analyze most codebases.\n\nEnter GPT-4‚Äôs Code Interpreter!\n\nGPT-4‚Äôs Code Interpreter lets you upload files in various formats (zip, txt, pdf, doc, excel, and more) and securely run code or analyze data right in the ChatGPT interface!\n\nIt also runs separately from the chat in a sandboxed environment, enabling analysis of large codebases without that context window constraint.\n\nSo let‚Äôs go ahead and give it a try in the section below‚Ää‚Äî‚Ääyou‚Äôll be amazed at what it can do!\n\n2.3‚Ää‚Äî‚ÄäReshape any code, anywhere in your¬†codebase\n\nLet‚Äôs use one of my Streamlit apps as an example.\n\nI created the CodeLlama Playground app to showcase the capabilities of Meta‚Äôs new CodeLlama model. You can get the repo here.\n\nDeepInfra, the company hosting the CodeLlama model, has recently introduced Mistral 7-B, a new open-source LLM that competes with CodeLlama in terms of performance.\n\nI want to update my Streamlit app with the new Mistral algorithm. Let‚Äôs see how GPT-4 can help us.\n\nHead to the GitHub repo and download it by clicking on Download ZIP.\nOpen a ChatGPT session and choose the ‚ÄòAdvanced Data Analysis‚Äô option to enable GPT-4‚Äôs Code Interpreter.\nUpload the repo.\nAdd this prompt:\n‚úèÔ∏è\nReview the repo of my CodeLlama Playground Streamlit app I just uploaded. Modify its contents to integrate the Mistral-7B code below into the app.\n\nimport openai\nConfigure OpenAI client to use our endpoint\nopenai.api_key = ‚Äú<YOUR DEEPINFRA TOKEN: deepctl auth token>‚Äù openai.api_base = ‚Äúhttps://api.deepinfra.com/v1/openai\"\nchat_completion = openai.ChatCompletion.create( model=‚Äùmistralai/Mistral-7B-Instruct-v0.1\", messages=[{‚Äúrole‚Äù: ‚Äúuser‚Äù, ‚Äúcontent‚Äù: ‚ÄúHello‚Äù}], )\nprint(chat_completion.choices[0].message.content)`\n\nI want the Mistral model to be selectable from a radio menu in the app‚Äôs sidebar, along with other models\nGPT-4 will scan the entire codebase, analyze the files, integrate the Mistral model into the Streamlit app, and make the necessary modifications throughout the repo.\nOnce that‚Äôs done, ask ChatGPT for permission to download the edited repo.\nReplace your local repo with the updated one and check the app.\n\nThe app is working well! The Mistral 7b can be selected from the radio selector in the top left section, along with other Llama models. I added the cyclone emoji manually, but the rest of the content was generated by GPT-4.\n\nThe entire process, from prompting to updating the app, was done in less than 2 minutes. Impressive!\n\n#3: From sluggish to speedy, use GPT-4 to improve your app performance\n3.1‚Ää‚Äî‚ÄäAutomatically diagnose performance issues\n\nWeb apps need to be fast. A fast web app keep users happy and coming back for more.\n\nBelow I have created an app with suboptimal coding, causing performance issues:\n\nimport streamlit as st\nimport pandas as pd\nimport sqlalchemy as db\nimport requests\nimport tensorflow as tf\n\nprice = st.number_input(\"House Price\", min_value=100000, max_value=10000000, step=100000)\ndef load_data():\n    return pd.read_csv('large_dataset.csv')\ndata = load_data()\nused_columns = ['Column1', 'Column2', 'Column3']\ndata = data[used_columns]\ndef download_model():\n    url = \"<https://example.com/large-model.h5>\"\n    response = requests.get(url)\n    with open('large-model.h5', 'wb') as f:\n        f.write(response.content)\ndownload_model()\nmodel = tf.keras.models.load_model('large-model.h5')\ndef load_efficient_data():\n    return pd.read_parquet('efficient_data.parquet')\nefficient_data = load_efficient_data()\ndatabase_uri = 'sqlite:///example.db'\nengine = db.create_engine(database_uri)\nquery = \"SELECT * FROM large_table WHERE condition = true\"\ndb_data = pd.read_sql_query(query, con=engine)\n\nIf you paste this code into ChatGPT and ask for performance improvements‚Ää‚Äî‚ÄäGPT-4 can identify areas for improvement in seconds!\n\nHere‚Äôs a breakdown of how GPT-4 would address each issue:\n\nRecommendation 01: Add caching\n\nThe load_data function is called without caching, causing it to read the CSV file from the disk every time it's called, impacting performance. To mitigate this, use @st.cache to cache function results, reducing the need for recomputing data with each interaction.\n\nPre (no caching):\n\ndef load_data():\n    return pd.read_csv('precalculated_data.csv')\n    data = load_data()\n\nPost (with caching):\n\n@st.cache(ttl=24*60*60)  # Cache the data with a time-to-live of 24 hours\ndef load_data():\n    return pd.read_csv('precalculated_data.csv')\n    data = load_data()\n\nRecommendation 02: Avoid downloading large, static models\n\nThe download_model function downloads the large model file every time the app is run. This can impact the app's performance, especially if the model file is very large. To avoid this, store the model on the production machine or use Git LFS to manage large files in your repository.\n\nPre (without efficient model management):\n\nimport requests\nimport tensorflow as tf\ndef download_model():\n    url = \"<https://example.com/large-model.h5>\"\n    response = requests.get(url)\n    with open('large-model.h5', 'wb') as f:\n        f.write(response.content)\ndownload_model()  # This will download the model every time the app runs\nmodel = tf.keras.models.load_model('large-model.h5')\n\n\nPost (with efficient model management):\n\nimport tensorflow as tf\nimport os\nmodel_path = 'models/large-model.h5'\nif not os.path.exists(model_path):\n    # Code to download the model\n    import requests\n    url = \"<https://example.com/large-model.h5>\"\n    response = requests.get(url)\n    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n    with open(model_path, 'wb') as f:\n        f.write(response.content)\nmodel = tf.keras.models.load_model(model_path)\n\nRecommendation 03: Use efficient binary-serialized formats\n\nReading data from a CSV file can be slow, especially for large datasets. You should use efficient binary-serialized formats like Apache Parquet or Apache Arrow for storing and reading large datasets.\n\nPre (without Parquet):\n\ndef load_data():\n    return pd.read_csv('large_dataset.csv')  # Reading data from a CSV file\n    data = load_data()\n\nPost (with Parquet):\n\ndef load_efficient_data():\n    return pd.read_parquet('large_dataset.parquet')  # Reading data from a Parquet file\n    efficient_data = load_efficient_data()\n\n\nYet again, in merely a few seconds, GPT-4 was able to propose these significant performance enhancements, a remarkable feat indeed!\n\nSince these changes were made in one file, let‚Äôs spice things up a bit and test GPT-4‚Äôs ability to diagnose similar performance issues on a larger codebase.\n\n3.2‚Ää‚Äî‚ÄäDiagnosing caching issues in large codebases\n\nOne of the best ways to speed up your Streamlit app is to leverage caching methods. Put simply, these methods optimize performance by eliminating redundant recalculations.\n\nStreamlit offers two caching mechanisms, @st.cache_data and @st.cache_resource:\n\n@st.cache_data is ideal for functions that return data types such as DataFrames and arrays. It addresses mutation and concurrency issues via serialization.\n@st.cache_resource is tailored for caching global resources, for instance, ML models or database connections.\n\nAdding caching methods to functions in a single Python file is ususally straightforward, but it can be tricky to figure out which functions to cache and which caching method to choose when you‚Äôre working with multiple files and a larger codebase.\n\nGPT-4‚Äôs Code Interpreter to the rescue, yet again!\n\nI‚Äôve created a demo app with multiple Python functions, spread across different files. You can download it here.\n\n/my_streamlit_app/\n|‚Äì data/\n|   |‚Äì large_dataset.csv\n|-- models/\n|   |-- heavy_model.pkl\n|-- src/\n|   |-- data_loader.py\n|   |-- model_loader.py\n|   |-- predictor.py\n|   |-- transformer.py\n|   |-- analyser.py\n|   |-- forecast.py\n|-- streamlit_app.py\n|-- requirements.txt\n\n\nSome of these functions either involve heavy I/O or compute tasks and currently lack caching. As a result, the app is loading slowly and each operation is sluggish.\n\nWe want the Code Interpreter to inspect the entire codebase and do the following:\n\nIdentify the functions that would benefit from caching.\nSuggest the best caching method based on what each function does.\nImplement those changes for us.\n\nHere‚Äôs the process:\n\nUpload the repo to ChatGPT.\nAdd this prompt:\n‚úèÔ∏è\nReview the repository of the Streamlit app I just uploaded via the Code Interpreter.\n\nIdentify functions in the codebase that would benefit from caching.\n\nRecommend appropriate caching techniques, either @st.cache_data or @st.cache_resource decorators.\n\nCreate a markdown table with the following columns:\n- Column 1: file name where we need the caching methods.\n- Column 2: function name to add the cache to.\n- Column 3: recommended caching methods.\n- Column 4: reason for using that cache method.\nVisit this URL and copy the body text from each page. This will help GPT-4 learn about our newest caching methods that were not available during its training. It will improve the quality of ChatGPT‚Äôs answers.\nPress send message.\nCheck GPT-4‚Äôs output:\nAsk ChatGPT to implement these changes and give us a downloadable copy of the edited repo.\n\nHooray! GPT-4 has not only provided us with a clear recommendation sheet that we can share with our peers and colleagues, but it has also implemented all of those changes for us across the entire codebase. The code is now ready to be copied into your local or deployed environment!\n\nIsn‚Äôt that amazing?\n\nWrapping up\n\nWe‚Äôve covered a lot in this guide!\n\nAfter a preamble to ChatGPT, GPT-4, and LLM trustworthiness, we provided prompting tips for various use cases of GPT-4 in app design and debugging.\n\nWe have then gone beyond single-file analysis and discussed ways to automatically refactor your apps, codebase, and optimize performance at scale, all made possible with GPT-4‚Äôs Code Interpreter.\n\nKeep in mind that we‚Äôve barely scratched the surface of what‚Äôs possible with GPT-4 in this post. We have yet to explore ChatGPT‚Äôs plugin ecosystem, its browsing or vision capabilities! (Psst... I will cover great use cases leveraging ChatGPT Vision in Part 2!).\n\nAs a final word, I would encourage you to think outside the box and embrace creativity in your prompts, you may just be blown away by the convos that will follow!\n\nPlease share your comments, use cases and tips below. Also, keep an eye on my Twitter/X feed, where I regularly share cool LLM use cases!\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "AppTest: test & build interactive Python data apps faster",
    "url": "https://blog.streamlit.io/apptest-faster-automated-testing/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nBy Joshua Carroll\nPosted in Product, October 31 2023\nIntroducing: AppTest\nHow AppTest works\nAppTest in action\nReady, set, test!\nShow off what you have built!\nContents\nShare this post\n‚Üê All posts\n\nEvery data app builder wants to build a flawless app in record time, but speed and quality may feel at odds with each other. Imagine pouring your heart into coding a beautiful new Python app, eager to share it with the world. However, before you can deploy, you can‚Äôt rush testing, else you‚Äôll risk a code error tarnishing your app experience.¬†\n\nWe've heard your pain and felt this ourselves. In practice, you probably do some manual sanity tests of your app changes and hope for the best. Yesterday‚Äôs automated test options are usually complex and hard to maintain:\n\nConduct unit tests on the backend logic by factoring it out from the UI\nSet up a heavyweight browser testing framework like Selenium, Playwright, or Cypress for end-to-end testing\n\nLuckily, you don‚Äôt have to live in this reality anymore. You can develop faster and ensure high quality!\n\nIntroducing: AppTest\n\nAppTest is a new automated way to write and execute tests natively in Streamlit. Developers can use this API to confirm that all aspects of their app are working correctly.¬†\n\nWith this automated testing framework you can:¬†\n\nCode with confidence: Run all your tests with a single command with Pytest. You no longer need to factor out your unit testable code or do extensive manual testing. Dealing with heavy end-to-end testing frameworks can be a thing of the past.¬†\n\nCollaborate seamlessly: Build apps with your team without worrying about breaking existing workflows. By connecting Streamlit to tools like GitHub Actions you can build a continuous integration pipeline that automatically runs tests after each commit.\n\nFeel more comfortable with complexity. Go beyond prototypes and build more powerful apps to take your data apps to the next level.\n\nSimple, powerful, and all in Python. üí™\n\nHow AppTest works¬†\n\nNow you can test each feature, interaction, or app logic headlessly via API. By headlessly, we mean that you can test the result without having to preview in the browser.¬† You can use the API reference docs to build out different scenarios you want to test.¬†\n\nWhen you are ready, test everything with Pytest, locally and/or with GitHub Actions. View the results that will confirm that your features are all working correctly (or not).¬†\n\nAppTest in action¬†\n\nWatch the video below to take a tour of AppTest. In the demo video we will cover:¬†\n\nSimple example - writing tests as you build your app\nReference API overview\nExamples from Sophisticated Palette and LLM-Examples apps\nIntegrating with GitHub Actions\nReady, set, test!¬†\n\nStart building and executing tests faster with AppTest to have more control over your app experiences. Check out the docs to get started.¬†\n\nShow off what you have built!¬†\n\nShare a link to tests you built for your Community Cloud app and show them successfully running in GitHub Actions.¬†\n\nYou can share an example by posting a link to your test file like this, and then share a successful run in a link like this. To get started setting up GitHub Actions, take a look at GitHub‚Äôs tutorial or use our sample workflow file.\n\nIf you provide your email in the comment with the two links, we will send the first 10 examples a Streamlit t-shirt!¬†\n\nHappy Streamlit-ing üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Product...\n\nView even more ‚Üí\n\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Joshua Carroll - Streamlit",
    "url": "https://blog.streamlit.io/author/joshua/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Joshua Carroll\n4 posts\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nLangChain ü§ù Streamlit\n\nThe initial integration of Streamlit with LangChain and our future plans\n\nLLMs\nby\nJoshua Carroll\n,\nJuly 11 2023\nIntroducing st.connection!\n\nQuickly and easily connect your app to data and APIs\n\nProduct\nby\nJoshua Carroll and¬†\n1\n¬†more,\nMay 2 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "7 ways GPT-4 with Vision can uplevel your Streamlit apps",
    "url": "https://blog.streamlit.io/7-ways-gpt-4-vision-can-uplevel-your-streamlit-apps/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nBy Charly Wargnier\nPosted in LLMs, November 15 2023\nA brief history of multi-modality in AI\n7 practical use cases for GPT-4 Vision\n1. Sketch your app and watch it come to life\n2. Turn any static chart into a dynamic data visualization\n3. Convert tabular data from images into fully editable tables\n4. Enhance your app's UX with tailored recommendations\n5. Conquer LLM hallucinations\n6. Debug any app, even when textual logs are missing\n7. Document your apps fast\nWhat will you build with GPT-4 Vision?\nContents\nShare this post\n‚Üê All posts\n\nIn my previous article, I explored how GPT-4 has transformed the way you can develop, debug, and optimize Streamlit apps.\n\nWith OpenAI‚Äôs latest advancements in multi-modality, imagine combining that power with visual understanding.\n\nNow, you can use GPT-4 with Vision in your Streamlit apps to:\n\nBuild Streamlit apps from sketches and static images.\nHelp you refine your apps' user experience, including debugging and documentation.\nOvercome LLMs limitations and hallucinations.\n\nIn this article, I'll walk you through 8 practical use cases that exemplify new possibilities using GPT-4 with Vision!\n\nA brief history of multi-modality in AI\n\nBefore we dive into various use cases, it's important to lay some conceptual foundations for multimodality, discuss pioneering models, and explore currently available multi-modal models.\n\nMulti-modal LLMs are an AI systems trained on multiple types of data such as text, images, and audio, as opposed to traditional models that focus on a single modality.\n\nThe journey towards multi-modality has seen significant strides over the recent years, with various models paving the way:\n\nCLIP, the OG model introduced by OpenAI in 2021, emerged as a pioneering model capable of generalizing to multiple image classification tasks with zero and few-shot learning.\nFlamingo, released in 2022, was notable for its strong performance in generating open-ended responses in a multimodal domain.\nSalesforce's BLIP model was a framework for unified vision-language understanding and generation, enhancing performance across a range of vision-language tasks.\n\nGPT-4 with Vision builds on pioneering models to advance the integration of visual and textual modalities. However, it's not the only multi-modal model vying for attention nowadays; Microsoft and Google are also gaining traction:\n\nMicrosoft's LLaVA, using a pre-trained CLIP visual encoder, offers similar performance to GPT-4 despite a smaller dataset.\nGemini is Google‚Äôs multimodal model, which stands out because it is fundamentally designed to be multimodal from the ground up.\n\nNow, to the fun part!\n\n7 practical use cases for GPT-4 Vision\n\nüí°\nPre-requisites:\n1) You'll need a¬†ChatGPT Plus subscription¬†to access GPT-4 Vision.\n2) If you‚Äôre new to Streamlit, follow the installation steps¬†here.\n1. Sketch your app and watch it come to life\n\n‚Ä¶ as my drawing skills are comparable to a cat chasing a laser pointer, I'll use Balsamiq to achieve that hand-drawn mockup feel.\n\nThis mockup consists of a two-panel UI. The left panel includes a logo, three navigation links, two checkboxes, and a dropdown. The right panel showcases the app name, a text area, two line charts, and a \"Download Chart data\" button:\n\nPaste this mock-up image into the ChatGPT interface:\n\nInclude the following prompt:\n\nCreate a Streamlit app from this mock-up. You will use the Echarts library.\nüí°\nIt's good practice to specify the data visualization you would like to use. When it's not specified, the default will be matplotlib.\n\nEach data visualization library will have to be installed via pip install into your virtual environment to work.\n\nCheck the results:\n\nBy simply uploading a mockup, Vision generated a fully functional Streamlit prototype: an app with a logo placeholder, navigation links, checkboxes, a combo box, some text, a chart, and a download button. üôå\n\n2. Turn any static chart into a dynamic data visualization\n\nChatGPT Vision doesn't just turn scribbles into fully functional Streamlit apps, it can also transform any static visual into a beautiful, interactive, dynamic data visualization.\n\nPaste this screenshot of a Nightingale rose chart from the Echarts library:\n\nInclude this prompt:\n\nCreate a nightingale/rose chart in Streamlit. The chart should be a dynamic ECharts chart, via the streamlit-echarts library. Each section should be toggleable. The color scheme of the chart should match the one in the image.\n\nCopy the code into your editor, and voil√†!\n\nThe app displays a dynamic rose chart in Streamlit, with legends, toggleable petals/sections, and a color palette that is a faithful replica of the original!\n\n3. Convert tabular data from images into fully editable tables\n\nChatGPT Vision is also incredibly useful when you need to extract data from a table that is not copyable nor downloadable.\n\nIn this example, we will ask Vision to make this image of tabular data fully editable using Streamlit‚Äôs data editor.\n\nPaste this image of a ReactJS table into ChatGPT:\n\nInclude this prompt:\n\nCode this table in Streamlit. We want the tabular data fully editable via Streamlit‚Äôs data editor.\n\nPaste the code into your editor and review the results:\n\nGPT-4 Vision crushed it!\n\nThe table is fully functional with the correct headers, each row is flawlessly reproduced, and the data is fully editable. As an added bonus, ChatGPT includes a function to find the highest sale amount and display the associated order!\n\n4. Enhance your app's UX with tailored recommendations\n\nGPT-4 Vision can also help you improve your app's UX and ease the design process for multi-page apps.\n\nPaste a screenshot of complex dashboard app into ChatGPT.\n\nInclude this prompt:\n\nProvide 8 suggestions to enhance the usability of this Streamlit app.\n\nChatGPT's recommendations are pretty spot on!\n\nGroup related controls into visually distinct sections.\nStandardize the UI's color scheme.\nImplement interactive tooltips on various sections and controls.\nIncrease the font size of axis labels and graph titles.\nHighlight the final decision output with a colored background.\nIncorporate a feedback mechanism.\nInclude a legend for multi-color or line graphs.\nEnsure consistent spacing and alignment between graphs and sections.\n5. Conquer LLM hallucinations\n\nThere's no doubt that GPT-4 is a significant improvement over its predecessors.\n\nLike all LLMs, it can produce misleading or fictional outputs, known as hallucinations. This can be due to biases in the training data, unclear prompts, or the fact that GPT-4 may not include the most up-to-date data.\n\nThis is when Retrieval Augmented Generation (or RAG) comes into play. RAG is a technique that improves chatbots by incorporating external data, ensuring more relevant and up-to-date responses.\n\nFor example, GPT-4 is not aware of Streamlit's new colorful headers, as they were not available when it was trained.\n\nWe'll start by pasting a screenshot of the new st.header() documentation, which includes our new API parameter for coloring headers:\n\nInclude this prompt:\n\nBuild a Streamlit app featuring various st.header() in different colors, using the new divider argument.Include a brief humorous data science pun for each header.Add a corresponding emoji to each header.\n\nLet's look at the results:\n\nVision did a great job of displaying the rainbow header seamlessly.\nüí°\nFun fact: uploading documentation screenshots with Vision resulted in better chat conversations than uploading PDFs through the Code Interpreter.\n6. Debug any app, even when textual logs are missing\n\nAs a developer advocate for Streamlit, I spend a big part of my time on forums helping our community users debug their Streamlit apps.\n\nWhile GPT-4 is an incredibly effective tool for quickly reviewing error logs to find the source of a bug, sometimes, users cannot provide error log traces for various reasons:\n\nThe log trace may contain private data that cannot be shared\nThe user may not be able to access the log trace at a specific time.\n\nWe may only be given a screenshot of the error callout from the Streamlit front-end, such as the one below:\n\nThis can make it difficult to debug the issue, as we do not have access to the full log trace.\n\nFear not! ChatGPT Vision can still assist you by providing useful debugging hints, by extracting relevant information from the screenshot.\n\nPaste the above image with the following prompt:\n\nGive me a clue on the error.\n\nLet‚Äôs review ChatGPT's answer:\n\nVerdict ‚úÖ\nEven though ChatGPT Vision only had access to a partly displayed screenshot of the error and did not have the full textual log trace, it was still able to infer the full error and retrieve the correct answer.\n\n7. Document your apps fast\n\nOnce you build your web app, it needs clear documentation to help users get started, understand its features, and learn how to use it. Writing documentation can be time-consuming, but ChatGPT Vision can help streamline the process.\n\nSimply provide a snapshot of your app, and ChatGPT Vision will generate tailored descriptive content that you can use in a document, README, social post, or anywhere else you need it. This not only saves time, but it also ensures that all of the visual details of your app are captured and explained.\n\nPaste a screenshot of my CodeLlama Playground app:\n\nAdd this prompt:\n\nAnalyze the image I've uploaded, which displays my CodeLlama Playground app. Create a README about it, in Markdown syntax. Add a prerequisite on how to install Streamlit (either locally or on Streamlit Community Cloud).\nüí°\nChatGPT Vision can only infer information from from the elements present in a given UI screenshot. Thus, for documentation purposes, it is always recommended to:\n\n1) Display all pages in a multi-page app\n2) Aid the inference by including any additional descriptive elements (in image or text) if the app's layout lacks sufficient detail.\n\nLet's look at the generated README from markdown:\n\nIn a matter of seconds, by merely examining the app's UI, ChatGPT Vision generated a ready-to-use README for my CodeLlama Playground app. It accurately listed its features, provided installation instructions for Streamlit both locally and via the Cloud, and offered a quick start guide to launch the app. ü§Ø\n\nWhat will you build with GPT-4 Vision?\n\nThe OpenAI Vision API also opens up new possibilities and creative combinations. At the time of this writing, GPT-4 with vision is currently only available to¬†developers with access to GPT-4¬†via the¬†gpt-4-vision-preview.\n\nUntil it becomes available world-wide, check out the art of the possible with some creations from the Streamlit community:\n\nTry out UI Auditor, from Streamlit community member, Kartik. Upload a screenshot of your app's UI, and GPT will tell you how to improve it ü§ñ\nIn this app tease from our Streamlit Creator, Avra, you can upload screenshots (in this case, from scientific publications) to get spot-on analyses.\nPeter Wang, another Streamlit Creator, built a image-to-text-to-speech app to commentate a League of Legends game!\n\nLet your imagination run wild with your prompts, and share what you discover in the comments below!\n\nAlso, keep an eye on my¬†Twitter/X feed, where I regularly share cool LLM use cases.\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in LLMs...\n\nView even more ‚Üí\n\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Charly Wargnier - Streamlit",
    "url": "https://blog.streamlit.io/author/charly/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Charly Wargnier\n4 posts\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow in-app feedback can increase your chatbot‚Äôs performance\n\nA guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith\n\nTutorials\nby\nCharly Wargnier\n,\nOctober 6 2023\nHow to enhance Google Search Console data exports with Streamlit\n\nConnect to the GSC API in one click and go beyond the 1,000-row UI limit!\n\nTutorials\nby\nCharly Wargnier\n,\nJuly 28 2022\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Building a dashboard in Python using Streamlit",
    "url": "https://blog.streamlit.io/crafting-a-dashboard-app-in-python-using-streamlit/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nBy Chanin Nantasenamat\nPosted in Tutorials, January 22 2024\nWhat‚Äôs inside the dashboard?\n1. Define key metrics\n1.1 Overview of key metrics\n1.2 Key metrics selected for this app\n2. Perform EDA analysis\n2.1 What data is available to you?\n2.2 Prepare the data\n2.3 Select charts to best visualize our key metrics\n3. Build your dashboard with Streamlit\n3.1 Import libraries\n3.2 Page configuration\n3.3 Apply CSS styling\n3.4 Load data\n3.5 Add a sidebar\n3.6 Plot and chart types\n3.7 App layout\n3.8 Deploying the Dashboard app to the cloud\nBONUS: 5 reminders when building dashboards\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nYou may be familiar with the phrase ‚ÄúA picture is worth a thousand words‚Äù and, in the context of data science, a visualized plot provides just as much value. It does this by providing a different look at tabular data, perhaps in the form of simple line charts, histogram distribution and more elaborate pivot charts.\n\nAs useful as these can be, a typical chart that we may see in print or on the web are most likely static. Imagine how much more engaging it would be to manipulate these static variables in an interactive dashboard?\n\nüèÇ\nReady to jump right in? Here‚Äôs the dashboard app and GitHub repo.\n\nIn this blog, you‚Äôll learn how to build a Population Dashboard app that displays data and visualizations of the US population for the years 2010-2019 as obtained from the US Census Bureau.\n\nI'll guide you through the process of building this interactive dashboard app from scratch using Streamlit for the frontend. Our backend muscle comes from PyData heavyweights like NumPy, Pandas, Scikit-Learn, and Altair, ensuring robust data processing and analytics.\n\nYou‚Äôll learn how to:\n\nDefine key metrics\nPerform EDA analysis\nBuild the dashboard app with Streamlit\nWhat‚Äôs inside the dashboard?\n\nHere‚Äôs a visual breakdown of the components that make up this population dashboard:\n\nLet‚Äôs get started!\n\n1. Define key metrics\n\nBefore we dive into actually building the dashboard, we need to first come up with well-defined metrics to measure what matters.\n\n1.1 Overview of key metrics\n\nThe goal of any dashboard is to surface insights that provide the basis for data-driven decision making. What is the primary purpose of the dashboard? This will guide the subsequent questions you want the dashboard to answer in the form of key metrics.\n\nFor example:\n\nIn sales, the primary goal may be to understand: ‚ÄúHow are sales teams performing?‚Äù Metrics may include total revenue by sales rep, units sold by territory, or new leads generated over time.\nIn marketing, the primary goal may be to understand ‚ÄúHow is my campaign performing?‚Äù this may include measuring leading indicators such as response rates or click-through rate, and lagging indicators such as revenue conversion rate or customer acquisition costs.\nIn finance, the dashboard may need to answer ‚ÄúHow profitable is our business?‚Äù this might include gross profit, operating margin, and return on assets.\n1.2 Key metrics selected for this app\n\nThe primary question this population dashboard aims to answer is: how do US state populations change over time?\n\nWhat questions do we need to ask that will help us answer this dashboard goal?\n\nHow do total populations compare among different states?\nHow do state populations evolve over time and how do they compare to each other?\nIn a given year, which states experienced more than 50,000 people moving in, or out? We'll label these inbound and outbound migration metrics.\n2. Perform EDA analysis\n\nOnce we have our key metrics, we will then need to collect and gain a solid understanding about the data available before we can present it in a visually aesthetic way in our dashboard.\n\nExploratory data analysis (EDA) can be defined as an iterative process for data understanding that entails asking and answering questions through investigative work of analyzing the data. In essence, your dashboard starts out as a blank canvas and EDA provides a pragmatic approach for coming up with compelling data visuals that tells a story.\n\nJohn Tukey's seminal work on EDA in 1977 meticulously sets the stage for effective data communication. Here are some notable key takeaways:\n\n‚ÄúThe greatest value of a graph is when it forces us to see what we never expected.‚Äù In fact Tukey, introduced the Box and whisker plot (aka box plots).\nHaving a flexible and open mindset when approaching data, hence the ‚Äúexploratory‚Äù nature of EDA.\n2.1 What data is available to you?\n\nHere‚Äôs a sample of the dataset from the US Census Bureau we‚Äôre using for our population dashboard. There are 3 potential variables (states, year and population) that will serve as the basis for our metrics.\n\nstates, states_code, id, year, population\nAlabama, AL, 1, 2010, 4785437\nAlaska, AK, 2, 2010, 713910\nArizona, AZ, 4, 2010, 6407172\nArkansas, AR, 5, 2010, 2921964\nCalifornia, CA, 6, 2010, 37319502\n\n2.2 Prepare the data\n\nConsolidate the year columns into a single unified column.\n\nThe advantage of subsetting the data by year, will provide the necessary format for generating possible visualizations (e.g. a heatmap, choropleth map, etc.) and a sortable dataframe.\n\n2.3 Select charts to best visualize our key metrics\n\nNow that we have a better understanding of the data at our fingertips, and the key metrics to measure, it‚Äôs time to decide how to visualize the results on our dashboard. There are countless ways to visualize your datasets, here‚Äôs what was selected for our population dashboard app.\n\nWhat is the comparison of total populations among different states?\nA choropleth map adds a geospatial dimension to highlight the most and least populated states.\nHow do populations of different states evolve over time and how do they compare to each other?\nA heatmap offers a comprehensive overview of states with the highest and lowest population values by presenting this information across different years\nSorting the dataframe provides a quick and direct comparison of the most and least populated states, thereby eliminating the need to wander through different sections of the charts.\nIn a given year, what percent of all states experience inbound/outbound migration >50,000 people?\nA donut chart is a pie chart with an empty inner arc and we‚Äôre using this to visualize the percentage of inbound and outbound state migration.\n\nThere are countless ways to visualize your datasets!\n\nYou can discover even more visualization options from the growing collection of custom components that the community. Here‚Äôs a few that you can try out:\n\nstreamlit-extras affords a wide range of widgets that extends the native functionality of Streamlit.\nstreamlit-shadcn-ui provides several UI frontend components (modal, hovercard, badges, etc.) that can be incorporated into the dashboard app.\nstreamlit-element allows the creation of draggable and resizable dashboard components.\n3. Build your dashboard with Streamlit\nüí°\nHere‚Äôs the dashboard app and the GitHub repo.\n3.1 Import libraries\n\nFirst, we‚Äôll start by importing the prerequisite libraries:\n\nStreamlit - a low-code web framework\nPandas - a data analysis and wrangling tool\nAltair - a data visualization library\nPlotly Express - a terse and high-level API for creating figures\n\nimport streamlit as st\nimport pandas as pd\nimport altair as alt\nimport plotly.express as px\n\n3.2 Page configuration\n\nNext, we‚Äôll define settings for the app by giving it a page title and icon that are displayed on the browser. This also defines the page content to be displayed in a wide layout that fits the page‚Äôs width as well as showing the sidebar in the expanded state.\n\nHere, we also set the color theme for the Altair plot to be dark in order to accompany dark color theme of the app.\n\nst.set_page_config(\n    page_title=\"US Population Dashboard\",\n    page_icon=\"üèÇ\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\")\n\nalt.themes.enable(\"dark\")\n\n3.3 Apply CSS styling\n\nTo obtain the desired visual display of the page elements, we‚Äôre using a little CSS magic to center the text and icons of the metrics widget (via the use of data-testid of stMetric, stMetricLabel, stMetricDeltaIcon-Up, and stMetricDeltaIcon-Down), to center the page contents (via the use of data-testid of block-container), and to reduce the sidebar padding (via the use of data-testid of stVerticalBlock).\n\nst.markdown(\"\"\"\n<style>\n\n[data-testid=\"block-container\"] {\n    padding-left: 2rem;\n    padding-right: 2rem;\n    padding-top: 1rem;\n    padding-bottom: 0rem;\n    margin-bottom: -7rem;\n}\n\n[data-testid=\"stVerticalBlock\"] {\n    padding-left: 0rem;\n    padding-right: 0rem;\n}\n\n[data-testid=\"stMetric\"] {\n    background-color: #393939;\n    text-align: center;\n    padding: 15px 0;\n}\n\n[data-testid=\"stMetricLabel\"] {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n\n[data-testid=\"stMetricDeltaIcon-Up\"] {\n    position: relative;\n    left: 38%;\n    -webkit-transform: translateX(-50%);\n    -ms-transform: translateX(-50%);\n    transform: translateX(-50%);\n}\n\n[data-testid=\"stMetricDeltaIcon-Down\"] {\n    position: relative;\n    left: 38%;\n    -webkit-transform: translateX(-50%);\n    -ms-transform: translateX(-50%);\n    transform: translateX(-50%);\n}\n\n</style>\n\"\"\", unsafe_allow_html=True)\n\n3.4 Load data\n\nNext, we‚Äôll load data into the app using Pandas‚Äô read_csv() function as follows:\n\ndf_reshaped = pd.read_csv('data/us-population-2010-2019-reshaped.csv')\n\n3.5 Add a sidebar\n\nWe‚Äôre now going to create the app title via st.title() and create drop-down widgets for allowing users to select the specific year and color theme via st.selectbox().\n\nThe selected_year (from the available years from 2010-2019) will then be used to subset the data for that year, which is then displayed in-app.\n\nThe selected_color_theme will allow the choropleth map and heatmap to be colored according to the selected color specified by the aforementioned widget.\n\nwith st.sidebar:\n    st.title('üèÇ US Population Dashboard')\n    \n    year_list = list(df_reshaped.year.unique())[::-1]\n    \n    selected_year = st.selectbox('Select a year', year_list, index=len(year_list)-1)\n    df_selected_year = df_reshaped[df_reshaped.year == selected_year]\n    df_selected_year_sorted = df_selected_year.sort_values(by=\"population\", ascending=False)\n\n    color_theme_list = ['blues', 'cividis', 'greens', 'inferno', 'magma', 'plasma', 'reds', 'rainbow', 'turbo', 'viridis']\n    selected_color_theme = st.selectbox('Select a color theme', color_theme_list)\n\n3.6 Plot and chart types\n\nNext, we‚Äôre going to define custom functions to create the various plots displayed in the dashboard.\n\nHeatmap\n\nA heatmap will allow us to see the population growth over the years from 2010-2019 for the 52 states.\n\nHeatmap of the US population for 2010-2019.\ndef make_heatmap(input_df, input_y, input_x, input_color, input_color_theme):\n    heatmap = alt.Chart(input_df).mark_rect().encode(\n            y=alt.Y(f'{input_y}:O', axis=alt.Axis(title=\"Year\", titleFontSize=18, titlePadding=15, titleFontWeight=900, labelAngle=0)),\n            x=alt.X(f'{input_x}:O', axis=alt.Axis(title=\"\", titleFontSize=18, titlePadding=15, titleFontWeight=900)),\n            color=alt.Color(f'max({input_color}):Q',\n                             legend=None,\n                             scale=alt.Scale(scheme=input_color_theme)),\n            stroke=alt.value('black'),\n            strokeWidth=alt.value(0.25),\n        ).properties(width=900\n        ).configure_axis(\n        labelFontSize=12,\n        titleFontSize=12\n        ) \n    # height=300\n    return heatmap\n\n\nChoropleth map\n\nNext, a colored map of the 52 US states for the selected year is depicted by the choropleth map.\n\nChoropleth map of the US population for a selected year of interest (2019 in this case).\ndef make_choropleth(input_df, input_id, input_column, input_color_theme):\n    choropleth = px.choropleth(input_df, locations=input_id, color=input_column, locationmode=\"USA-states\",\n                               color_continuous_scale=input_color_theme,\n                               range_color=(0, max(df_selected_year.population)),\n                               scope=\"usa\",\n                               labels={'population':'Population'}\n                              )\n    choropleth.update_layout(\n        template='plotly_dark',\n        plot_bgcolor='rgba(0, 0, 0, 0)',\n        paper_bgcolor='rgba(0, 0, 0, 0)',\n        margin=dict(l=0, r=0, t=0, b=0),\n        height=350\n    )\n    return choropleth\n\n\nDonut chart\n\nNext, we‚Äôre going to create a donut chart for the states migration in percentage.\n\nDonut chart of the percentage of states with annual inbound and outbound migration greater than 50,000 in 2019.\n\nParticularly, this represents the percentage of states with annual inbound or outbound migration > 50,000 people. For instance, in 2019, there were 12 out of 52 states and this corresponds to 23%.\n\nBefore creating the donut chart, we‚Äôll need to calculate the year-over-year population migrations.\n\ndef calculate_population_difference(input_df, input_year):\n  selected_year_data = input_df[input_df['year'] == input_year].reset_index()\n  previous_year_data = input_df[input_df['year'] == input_year - 1].reset_index()\n  selected_year_data['population_difference'] = selected_year_data.population.sub(previous_year_data.population, fill_value=0)\n  return pd.concat([selected_year_data.states, selected_year_data.id, selected_year_data.population, selected_year_data.population_difference], axis=1).sort_values(by=\"population_difference\", ascending=False)\n\n\nThe donut chart is then created from the aforementioned percentage value for states migration.\n\ndef make_donut(input_response, input_text, input_color):\n  if input_color == 'blue':\n      chart_color = ['#29b5e8', '#155F7A']\n  if input_color == 'green':\n      chart_color = ['#27AE60', '#12783D']\n  if input_color == 'orange':\n      chart_color = ['#F39C12', '#875A12']\n  if input_color == 'red':\n      chart_color = ['#E74C3C', '#781F16']\n    \n  source = pd.DataFrame({\n      \"Topic\": ['', input_text],\n      \"% value\": [100-input_response, input_response]\n  })\n  source_bg = pd.DataFrame({\n      \"Topic\": ['', input_text],\n      \"% value\": [100, 0]\n  })\n    \n  plot = alt.Chart(source).mark_arc(innerRadius=45, cornerRadius=25).encode(\n      theta=\"% value\",\n      color= alt.Color(\"Topic:N\",\n                      scale=alt.Scale(\n                          #domain=['A', 'B'],\n                          domain=[input_text, ''],\n                          # range=['#29b5e8', '#155F7A']),  # 31333F\n                          range=chart_color),\n                      legend=None),\n  ).properties(width=130, height=130)\n    \n  text = plot.mark_text(align='center', color=\"#29b5e8\", font=\"Lato\", fontSize=32, fontWeight=700, fontStyle=\"italic\").encode(text=alt.value(f'{input_response} %'))\n  plot_bg = alt.Chart(source_bg).mark_arc(innerRadius=45, cornerRadius=20).encode(\n      theta=\"% value\",\n      color= alt.Color(\"Topic:N\",\n                      scale=alt.Scale(\n                          # domain=['A', 'B'],\n                          domain=[input_text, ''],\n                          range=chart_color),  # 31333F\n                      legend=None),\n  ).properties(width=130, height=130)\n  return plot_bg + plot + text\n\n\nConvert population to text\n\nNext, we‚Äôll going to create a custom function for making population values more concise as well as improving the aesthetics. Particularly, instead of being displayed as numerical values of 28,995,881 in the metrics card to a more concised form as 29.0 M. This was also applied to numerical values in the thousand range.\n\nMetrics cards showing states with high inbound/outbound migration in the selected year of interest (2019 in this case).\ndef format_number(num):\n    if num > 1000000:\n        if not num % 1000000:\n            return f'{num // 1000000} M'\n        return f'{round(num / 1000000, 1)} M'\n    return f'{num // 1000} K'\n\n3.7 App layout\n\nFinally, it‚Äôs time to put everything together in the app.\n\nDefine the layout\n\nBegin by creating 3 columns:\n\ncol = st.columns((1.5, 4.5, 2), gap='medium')\n\n\nParticularly, the input argument (1.5, 4.5, 2) indicated that the second column has a width of about three times that of the first column and that the third column has a width about twice less than that of the second column.\n\nColumn 1\n\nThe Gain/Loss section is shown where metrics card are displaying states with the highest inbound and outbound migration for the selected year (specified via the Select a year drop-down widget created via st.selectbox).\n\nThe States migration section shows a donut chart where the percentage of states with annual inbound or outbound migration > 50,000 are displayed.\n\nwith col[0]:\n    st.markdown('#### Gains/Losses')\n\n    df_population_difference_sorted = calculate_population_difference(df_reshaped, selected_year)\n\n    if selected_year > 2010:\n        first_state_name = df_population_difference_sorted.states.iloc[0]\n        first_state_population = format_number(df_population_difference_sorted.population.iloc[0])\n        first_state_delta = format_number(df_population_difference_sorted.population_difference.iloc[0])\n    else:\n        first_state_name = '-'\n        first_state_population = '-'\n        first_state_delta = ''\n    st.metric(label=first_state_name, value=first_state_population, delta=first_state_delta)\n\n    if selected_year > 2010:\n        last_state_name = df_population_difference_sorted.states.iloc[-1]\n        last_state_population = format_number(df_population_difference_sorted.population.iloc[-1])   \n        last_state_delta = format_number(df_population_difference_sorted.population_difference.iloc[-1])   \n    else:\n        last_state_name = '-'\n        last_state_population = '-'\n        last_state_delta = ''\n    st.metric(label=last_state_name, value=last_state_population, delta=last_state_delta)\n\n    \n    st.markdown('#### States Migration')\n\n    if selected_year > 2010:\n        # Filter states with population difference > 50000\n        # df_greater_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference_absolute > 50000]\n        df_greater_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference > 50000]\n        df_less_50000 = df_population_difference_sorted[df_population_difference_sorted.population_difference < -50000]\n        \n        # % of States with population difference > 50000\n        states_migration_greater = round((len(df_greater_50000)/df_population_difference_sorted.states.nunique())*100)\n        states_migration_less = round((len(df_less_50000)/df_population_difference_sorted.states.nunique())*100)\n        donut_chart_greater = make_donut(states_migration_greater, 'Inbound Migration', 'green')\n        donut_chart_less = make_donut(states_migration_less, 'Outbound Migration', 'red')\n    else:\n        states_migration_greater = 0\n        states_migration_less = 0\n        donut_chart_greater = make_donut(states_migration_greater, 'Inbound Migration', 'green')\n        donut_chart_less = make_donut(states_migration_less, 'Outbound Migration', 'red')\n\n    migrations_col = st.columns((0.2, 1, 0.2))\n    with migrations_col[1]:\n        st.write('Inbound')\n        st.altair_chart(donut_chart_greater)\n        st.write('Outbound')\n        st.altair_chart(donut_chart_less)\n\n\nColumn 2\n\nNext, the second column displays the choropleth map and heatmap using custom functions created earlier.\n\nwith col[1]:\n    st.markdown('#### Total Population')\n    \n    choropleth = make_choropleth(df_selected_year, 'states_code', 'population', selected_color_theme)\n    st.plotly_chart(choropleth, use_container_width=True)\n    \n    heatmap = make_heatmap(df_reshaped, 'year', 'states', 'population', selected_color_theme)\n    st.altair_chart(heatmap, use_container_width=True)\n\n\nColumn 3\n\nFinally, the third column shows the top states via a dataframe whereby the population are shown as a colored progress bar via the column_config parameter of st.dataframe.\n\nAn About section is displayed via the st.expander() container to provide information on the data source and definitions for terminologies used in the dashboard.\n\nwith col[2]:\n    st.markdown('#### Top States')\n\n    st.dataframe(df_selected_year_sorted,\n                 column_order=(\"states\", \"population\"),\n                 hide_index=True,\n                 width=None,\n                 column_config={\n                    \"states\": st.column_config.TextColumn(\n                        \"States\",\n                    ),\n                    \"population\": st.column_config.ProgressColumn(\n                        \"Population\",\n                        format=\"%f\",\n                        min_value=0,\n                        max_value=max(df_selected_year_sorted.population),\n                     )}\n                 )\n    \n    with st.expander('About', expanded=True):\n        st.write('''\n            - Data: [U.S. Census Bureau](<https://www.census.gov/data/datasets/time-series/demo/popest/2010s-state-total.html>).\n            - :orange[**Gains/Losses**]: states with high inbound/ outbound migration for selected year\n            - :orange[**States Migration**]: percentage of states with annual inbound/ outbound migration > 50,000\n            ''')\n\n3.8 Deploying the Dashboard app to the cloud\n\nFor a video walkthrough on deploying a Streamlit app, check out this tutorial on YouTube.\n\nBONUS: 5 reminders when building dashboards\nPerform EDA to gain data understanding\nIdentify key metrics for tracking what matters\nDecide on charts to best visualize key metrics\nGroup related metrics together\nUse clear and concise labels to describe metrics\nWrapping up\n\nIn summary, Streamlit offers a quick, efficient, and code-friendly way to build interactive dashboard apps in Python, making it a go-to tool for data scientists and developers working with data visualization.\n\nOne of the key features of Streamlit is its ability to automatically update and re-render the app based on incremental changes in the data or input parameters, which makes it highly suitable for real-time data visualization tasks.\n\nCheck out this tutorial video to follow along:\n\nWhat dashboards are you building? In the comments below, share your dashboard below to inspire the community, or ask for feedback!\n\nFollow me on X at¬†@thedataprof, on LinkedIn at¬†Chanin Nantasenamat, or subscribe to my Data Professor channel on Youtube!\n\nHappy Streamlit-ing! üìä\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nAlso in Tutorials...\n\nView even more ‚Üí\n\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Avthar Sewrathan - Streamlit",
    "url": "https://blog.streamlit.io/author/avthar/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Avthar Sewrathan\n1 post\nWebsite\nTwitter\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Using time-based RAG in LLM apps",
    "url": "https://blog.streamlit.io/using-time-based-rag-llm-apps-with-timescale-vector/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nBy Avthar Sewrathan\nPosted in LLMs, December 15 2023\nBefore vs. after time-aware retrieval\nTutorial: Build a chatbot about GitHub commit history\nOverview of TSV Time Machine app\nPart 1: Load time-based data with Timescale Vector and LlamaIndex\nPart 2: Build the chatbot\nDeployment\n‚òÅÔ∏è¬†On Streamlit Community Cloud\nüíª¬†On your local machine\nHow will you use time-based retrieval?\nWrapping up\nContents\nShare this post\n‚Üê All posts\n\nRetrieval Augmented Generation (RAG) is essential for enhancing large language models (LLMs) in app development. It supplements LLMs with external data sources, helping arrive at more relevant responses by reducing errors or hallucinations. RAG determines what info is relevant to the user‚Äôs query through semantic search, which searches data by meaning (rather than just looking for literal matches of search terms). RAG is particularly effective for LLM apps that need to access domain-specific or proprietary data.\n\nHowever, RAG alone isn‚Äôt always enough to build powerful, context-aware LLM apps. Enhancing RAG with time-aware retrieval, which finds semantically relevant vectors within specific time and date ranges, can significantly improve its effectiveness.\n\nBefore vs. after time-aware retrieval\n\nWhen dealing with a date-heavy knowledge base, time-aware RAG can help you build LLM apps that excel at generating relevant answers to user queries.\n\nIn this example, see how time-aware retrieval improves the quality of LLM responses:\n\nAlice is a developer that wants to learn about specific changes to a GitHub repo (in this case, the TimescaleDB repo). Rather than manually inspect the code or commit history, she finds a ‚ÄúChat with my GitHub repo‚Äù application and uses it to expedite her research. Alice wants to learn about recent changes to a feature named ‚Äúcontinuous aggregates‚Äù and asks the following question on 29 November 2023:\n\nWhat's new with the continuous aggregates feature in the past 6 months?\n\nLLM response without time-aware RAG üëé\n\nThe feature named continuous aggregates has undergone some changes. The most recent change was made by Sven Klemm on January 24, 2020. This change added a new mode called real-time aggregates, which combines materialized data with new data received after the last refresh. This new mode is now the default behavior for newly created continuous aggregates. To upgrade existing continuous aggregates to the new behavior, a specific command needs to be run.\n\nLLM response with time-aware RAG ‚úÖ\n\nIn the past 6 months, there have been two changes made to the feature named continuous aggregates. The first change was made on August 7, 2023, by Fabr√≠zio de Royes Mello. This change relaxed the strong table lock when refreshing a continuous aggregate, allowing the refresh procedure to be executed in multiple sessions with fewer locks. The second change was made on August 29, 2023, by Jan Nidzwetzki. This change made the up/downgrade test deterministic by adding an ORDER BY specification to two queries in post.continuous_aggs.v3.sql.\n\nThe response using time-aware RAG is much more useful ‚Äî it‚Äôs within Alice‚Äôs specified time period and is topically relevant. The difference between the two responses is the retrieval step.\n\nTutorial: Build a chatbot about GitHub commit history\n\nNow it's your turn! In the rest of this post, we'll walk through the process of building TSV Time Machine: a time-aware RAG chatbot that enables you to go back in time and chat with the commit history of any GitHub repository.\n\nEach Git commit has an associated timestamp, a natural language message, and other metadata, meaning that both semantic and time-based search are needed to answer questions about the commit history.\n\nüë©‚Äçüíª\nWant to jump right in? Check out the app and the code.\nScreenshot of the TSV Time Machine app, showing a user chatting with the PostgreSQL project GitHub commit history.\nOverview of TSV Time Machine app\n\nTo power TSV Time Machine, we use the following:\n\nLlamaIndex is a powerful LLM data framework for RAG applications. LlamaIndex ingests, processes, and retrieves data. We‚Äôll use the LlamaIndex autoretriever to infer the right query to run on the vector database, including both the query string and metadata filters.\nTimescale Vector is our vector database. Timescale Vector has optimizations for similarity and time-based search, making it ideal to power the time-aware RAG. It does this through automatic table partitioning to isolate data for particular time ranges. We will access it through LlamaIndex‚Äôs Timescale Vector Store.\nüéâ\nStreamlit Users Get 3 Months of Timescale Vector for Free! Timescale‚Äôs cloud hosted vector databases make it easy to test and develop your Streamlit applications.\n\nThe TSV Time Machine sample app has three pages:\n\nHome: homepage of the app that provides instructions on how to use the app.\nLoad Data: page to load the Git commit history of the repo of your choice.\nTime Machine Demo: interface with chat with any of the GitHub repositories loaded.\n\nSince the app is ~600 lines of code, we won‚Äôt unpack it line by line (although you can ask ChatGPT to explain any tricky parts to you!). Let‚Äôs take a look at the key code snippets involved in:\n\nLoading data from the GitHub repo you want to chat with\nPowering chat via time-aware retrieval augmented generation\nPart 1: Load time-based data with Timescale Vector and LlamaIndex\n\nInput the URL of a GitHub repo you want to load data for and TSV Time Machine uses LlamaIndex to load the data, create vector embeddings for it, and store it in Timescale Vector.\n\nIn the file 0_LoadData.py , we fetch data from a GitHub repository of your choice, create embeddings for it using OpenAI‚Äôs text-embedding-ada-002 model and LlamaIndex, and store it in tables in Timescale Vector. The tables contain the vector embedding, the original text, and metadata associated with the Git commit, including a UUID which reflects the timestamp of the commit.\n\nFirst, we define a load_git_history() function. This function will ask the user to input the GitHub repo URL, branch, and number of commits to load via a st.text_input element. Then it will will fetch the Git commit history for the repo, use LlamaIndex to embed the commit history text and turn them into LlamaIndex nodes, and insert the embeddings and metadata into Timescale Vector:\n\n# Load git history into the database using LlamaIndex\ndef load_git_history():\n   repo = st.text_input(\"Repo\", \"<https://github.com/postgres/postgres>\")\n   branch = st.text_input(\"Branch\", \"master\")\n   limit = int(st.text_input(\"Limit number commits (0 for no limit)\", \"1000\"))\n   if st.button(\"Load data into the database\"):\n       df = get_history(repo, branch, limit)\n       table_name = record_catalog_info(repo)\n       load_into_db(table_name, df)\n\n\nFunction for loading Git history from a user defined URL. Defaults to the PostgreSQL project.\n\nWhile the full code for the helper functions get_history(), record_catalog_info(), and load_into_db() is in the sample app repo, here‚Äôs an overview:\n\nget_history(): fetches the repo‚Äôs Git history and stores it in a Pandas DataFrame. We fetch the commit hash, author name, date of commit, and commit message.\nrecord_catalog_info(): creates a relational table in our Timescale Vector database to store the information of the loaded GitHub repositories. The repo URL and the name of the table commits are stored in the database.\nload_into_db(): creates a TimescaleVectorStore in LlamaIndex to store our embeddings and metadata for the commit data.\nWe set the time_partition_interval parameter to 365 days. This parameter represents the length of each interval for partitioning the data by time. Each partition will consist of data for the specified length of time.\n# Create Timescale Vectorstore with partition interval of 1 year\n   ts_vector_store = TimescaleVectorStore.from_params(\n       service_url=st.secrets[\"TIMESCALE_SERVICE_URL\"],\n       table_name=table_name,\n       time_partition_interval=timedelta(days=365),\n   )\nüí°\nChoosing the right partition interval\nThis example uses 365 days as the partition interval, but pick the value that makes sense for your app‚Äôs queries.\n\nFor example, if you frequently query recent vectors, use a smaller time interval (e.g. one day). If you query vectors over a decade-long time period, use a larger interval (e.g. six months or one year).\n\nMost queries should touch only a couple of partitions and your full dataset should fit within 1,000 partitions.\n\nOnce we‚Äôve created our TimescaleVectorStore, we create LlamaIndex TextNodes for each commit and create embeddings for the content of each node in batches.\n\ncreate_uuid() creates a UUID v1 from the commit‚Äôs datetime associated with the node and vector embedding. This UUID enables us to efficiently store the nodes in partitions by time and query embeddings according to their partition.\n\n# Create UUID based on time\ndef create_uuid(date_string: str):\n   datetime_obj = datetime.fromisoformat(date_string)\n   uuid = client.uuid_from_time(datetime_obj)\n   return str(uuid)\n\nCreating a UUID v1 with a time component helps power similarity search on time. We use the uuid_from_time() function found in the Timescale Vector Python client to help us.\n\nThe Load Data page showing the progress of loading and vectoring data from a user specified GitHub URL.\n\nFinally, we create a TimescaleVectorIndex, which will allow us to do fast similarity search and time-based search for time-aware RAG. We use st.spinner and st.progress to show load progress.\n\nst.spinner(\"Creating the index...\")\nprogress = st.progress(0, \"Creating the index\")\nstart = time.time()\nts_vector_store.create_index()\nduration = time.time()-start\nprogress.progress(100, f\"Creating the index took {duration} seconds\")\nst.success(\"Done\")\nPart 2: Build the chatbot\n\nIn the file 1_TimeMachineDemo.py, we use LlamaIndex‚Äôs auto-retriever to answer user questions by fetching data from Timescale Vector to use as context for GPT-4.\n\nTime Machine Demo page showing a sample user conversation.\n\nHere‚Äôs an overview of the key functions:\n\nget_repos(): Fetches list of available GitHub repos you‚Äôve loaded that you can chat with, so you can easily switch between them in the side bar.\nget_autoretriever(): Creates a LlamaIndex auto-retriever from the TimescaleVectorStore, which gives GPT-4 the ability to form vector store queries with metadata filters. This enables the LLM to limit answers to user queries to a specific timeframe.\nFor example, the query: ‚ÄúWhat new features were released in the past 6 months‚Äù will only search partitions in Timescale Vector that contain data between now and 6 months ago, and fetch the most relevant vectors to be used as context for RAG.\n# Creates a LlamaIndex auto-retriever interface with the TimescaleVector database\ndef get_auto_retriever(index, retriever_args):\n   vector_store_info = VectorStoreInfo(\n       # Note: Modify this to match the metadata of your data\n       content_info=\"Description of the commits to PostgreSQL. Describes changes made to Postgres\",\n       metadata_info=[\n           MetadataInfo(\n               name=\"commit_hash\",\n               type=\"str\",\n               description=\"Commit Hash\",\n           ),\n           MetadataInfo(\n               name=\"author\",\n               type=\"str\",\n               description=\"Author of the commit\",\n           ),\n           MetadataInfo(\n               name=\"__start_date\",\n               type=\"datetime in iso format\",\n               description=\"All results will be after this datetime\",\n  \n           ),\n           MetadataInfo(\n               name=\"__end_date\",\n               type=\"datetime in iso format\",\n               description=\"All results will be before this datetime\",\n  \n           )\n       ],\n   )\n   retriever = VectorIndexAutoRetriever(index,\n                                      vector_store_info=vector_store_info,\n                                      service_context=index.service_context,\n                                      **retriever_args)\n  \n   # build query engine\n   query_engine = RetrieverQueryEngine.from_args(\n       retriever=retriever, service_context=index.service_context\n   )\n   \n   # convert query engine to tool\n   query_engine_tool = QueryEngineTool.from_defaults(query_engine=query_engine)\n\n   chat_engine = OpenAIAgent.from_tools(\n       tools=[query_engine_tool],\n       llm=index.service_context.llm,\n       verbose=True,\n       service_context=index.service_context\n   )\n   return chat_engine\nvector_store_info provides the LLM with info about the metadata so that it can create valid filters for fetching data in response to user questions. If you‚Äôre using your own data (different from Git commit histories), you‚Äôll need to modify this method to match your metadata.\n__start_date and __end_date are special filter names used by Timescale Vector to support time-based search. If you‚Äôve enabled time partitioning in Timescale Vector (by specifying the time_partition_interval argument when creating the TimescaleVectorStore), you can specify these fields in the VectorStoreInfo to enable the LLM to perform time-based search on each LlamaIndex Node‚Äôs UUID.\nThe chat_engine returned is an OpenAIAgent which can use the QueryEngine tool to perform tasks ‚Äî in this case, answer questions about GitHub repo commits.\ntm_demo() handles the chat interaction between the user and LLM. It provides an st.slider element for the user to specify the time period and number of commits to fetch. It then prompts the user for input, processes that input using get_autoretriever(), and displays the chat messages. Check out this method in the GitHub repo.\nDeployment\n‚òÅÔ∏è¬†On Streamlit Community Cloud\nFork and clone this repository.\nCreate a new cloud PostgreSQL database with Timescale Vector (sign up for an account here). Download the cheatsheet or .env file containing the database connection string.\nCreate a new OpenAI API key to use in this project, or follow these instructions to sign up for an OpenAI developer account to obtain one. We‚Äôll use OpenAI‚Äôs embedding model to generate text embeddings and GPT-4 as the LLM to power our chat engine.\nIn Streamlit Community Cloud:\nClick¬†New app, and pick the appropriate repository, branch, and file path.\nClick Advanced Settings and set the following secrets:\nOPENAI_API_KEY=‚ÄùYOUR_OPENAI_API KEY‚Äù\n\nTIMESCALE_SERVICE_URL=‚ÄùYOUR_TIMESCALE_SERVICE_URL‚Äù\n\nENABLE_LOAD=1\n\nHit¬†Deploy.\n\nAnd you‚Äôre off to the races!\n\nüíª¬†On your local machine\nCreate a new folder for your project, then follow steps 1-3 above.\nInstall dependencies. Navigate to the tsv-timemachine directory and run the following command in your terminal, which will install the python libraries needed:\npip install -r requirements.txt\n\nIn the tsv-timemachine directory, create a new .streamlit folder and create a secrets.toml file that includes the following:\nOPENAI_API_KEY=‚ÄùYOUR_OPENAI_API KEY‚Äù\n\nTIMESCALE_SERVICE_URL=‚ÄùYOUR_TIMESCALE_SERVICE_URL‚Äù\n\nENABLE_LOAD=1\n\n\nExample TOML file with Streamlit secrets. You‚Äôll need to set these to embed and store data using OpenAI and Timescale Vector.\n\nTo run the application locally, enter the following in the command line:\nstreamlit run Home.py\n\n\nüéâ Congrats! Now you can load and chat with the Git commit history of any repo, using LlamaIndex as the data framework, and Timescale Vector as the vector database.\n\nThe Git commit history can be substituted for any time-based data of your choice. The result is an application that can efficiently perform RAG on time-based data and answer user questions with data from specific time periods.\n\nHow will you use time-based retrieval?\n\nBuilding on the example above, here are a few examples of use cases unlocked by time-aware RAG:\n\nSimilarity search within a time range: filter documents by create date, publish date, or update date when chatting with a corpus of documents.\nFind the most recent embeddings: find the most relevant and recent news or social media posts about a specific topic.\nGive LLMs a sense of time: leverage tools like LangChain‚Äôs self-query retriever or LlamaIndex‚Äôs auto-retriever to ask time-based and semantic questions about a knowledge base of documents.\nSearch and retrieve chat history: search a user‚Äôs prior conversations to retrieve details relevant to the current chat.\nWrapping up\n\nYou learned that time-aware RAG is crucial to build powerful LLM apps that deal with time-based data. You also used Timescale Vector and LlamaIndex to construct a time-based RAG pipeline, resulting in a Streamlit chatbot capable of answering questions about a GitHub commit history or any other time-based knowledge base.\n\nLearn more about Timescale Vector from our blog and dive even deeper on time-aware RAG in our webinar with LlamaIndex. Take your skills to the next level by creating your own Streamlit chatbot using time-based RAG and your data stored in Timescale.\n\nDon't forget to share your creations in the Streamlit forum and on social media.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Siddhant Sadangi - Streamlit",
    "url": "https://blog.streamlit.io/author/siddhant/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Siddhant Sadangi\n1 post\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Connect your Streamlit apps to Supabase",
    "url": "https://blog.streamlit.io/connect-your-streamlit-apps-to-supabase/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nBy Siddhant Sadangi\nDecember 20 2023\nWhy I built a Streamlit connection to Supabase\nHow to use the app\nStep 1: Initialize the client for the demo project\nStep 2: Explore the database, storage, or auth\nStep 3: Click on ‚ÄúRun query üèÉ‚Äù to get the results\nWhat I learned while building this connection\nReusing Supabase methods\nSingle-sourcing versioning for consistency\nWhat I learned while building the Streamlit app\nReusing components\nDemo project for unregistered users\nConstructing valid code snippets that can be copied by the user\nWrapping up\nContents\nShare this post\n‚Üê All posts\nüëâ\nTL;DR: st-supabase-connection is a Supabase connection for Streamlit that caches API calls and simplifies the needed code. I‚Äôll walk you through how to:\n1. Install and configure the connection\n2. Use the tutorial app to explore Supabase storage, database, and auth features\n3. Construct reusable code snippets to interact with Supabase\n\nWant to dive right in? Check out the GitHub repo and demo app.\n\nI‚Äôve been building Streamlit apps for a while now, and a common requirement is to connect to a database. That‚Äôs when I stumbled upon Supabase.\n\nTheir web interface was powerful yet user-friendly, and when Streamlit launched their Connections Hackathon, I saw an opportunity to make it even easier to use Supabase with Streamlit. My aim was to help Streamlit users and build on the functionality of Supabase‚Äôs existing Python SDK.\n\nIn this article, you‚Äôll learn:\n\nWhy I chose Supabase\nHow to install and set up st-supabase-connection\nHow to use the tutorial app provided with the connection\nMy learnings while working on the connection and the tutorial app\nWhy I built a Streamlit connection to Supabase\n\nI was introduced to Supabase by a friend. I found the docs and the user interface so intuitive that I was able to have a hosted database up and running in under 10 minutes. I didn‚Äôt feel the need to research alternatives as Supabase offered everything I was looking for: the ability to perform CRUD operations from the UI and an open-source Python SDK that I could build upon.\n\nMy connection offers three advantages that improve the experience of building with Supabase and Streamlit:\n\nIt leverages Streamlit's caching feature to store API responses. This speeds up subsequent similar requests, reducing API calls, which ultimately decreases the cost of using Supabase.\nIt simplifies the boilerplate code needed to connect your Streamlit apps to Supabase.\nIt exposes more storage methods than Supabase‚Äôs Python client.\nHow to use the app\n\nThis app is designed to showcase and teach you how to use the connection's features. The connection currently works with Supabase Storage, Database, and Auth.\n\nüí°\nSupabase offers a free tier. You need to sign up and get a Supabase API key to use it. For this app, I made it possible for you to demo the app without signing up for Supabase. If you like what you see and want to use it for your own project, sign up and enter your own API key to unlock all the features of the app and connect it with your own Supabase project.\n\nIf you are new to Supabase and don‚Äôt have a Supabase API key, you can follow the below quickstart to get acquainted with Supabase, the connection, and the app.\n\nTo get started, head over to the app hosted on Streamlit Community Cloud.\n\nStep 1: Initialize the client for the demo project\n‚úÖ\nSince the connection is built on top of st.connection(), the client is cached so that it can handle network interruptions and does not need to be initialized frequently. That said, the tutorial app overrides this caching to show you the latest results.\n\nThe first thing you‚Äôll need to do is initialize the connection to Supabase ‚Äî you can do this by clicking ‚ÄúInitialize client‚Äù and you can either use a demo project or connect to your own Supabase instance.\n\nOnce the client is initialized, you‚Äôll see the demo database or storage bucket. There are some tables and files that you can use for testing, and the app provides example queries.\n\nYou can also choose to explore storage, the database, or authentication. For each option, you‚Äôll be able to test the functionality of the Supabase connection, as well as copy a code snippet that can be used in your Streamlit app to perform that operation.\n\nStep 2: Explore the database, storage, or auth\n\nExploring the database\n\nBy default, database queries are not cached, so they always show the most recent data.\n\nExploring storage\n\nStorage queries are cached forever, so they are very fast. You can change this by using the ‚ÄúResults cache duration‚Äù option, which affects the ttl parameter of the query.\n\nExploring auth\n\nauth methods don‚Äôt have a cache, so you always get the latest results.\n\nStep 3: Click on ‚ÄúRun query üèÉ‚Äù to get the results\n\nClick ‚ÄúRun query‚Äù or ‚ÄúExecute‚Äù to execute the specific command against the demo Supabase instance, and the results will be displayed in the app.\n\nExample database view\n\nExample storage view\n\nExample auth view\n\nOnce you feel that Supabase would add value to your Streamlit apps, you can start using the connection as described in the Streamlit docs!\n\nWhat I learned while building this connection\n\nThis was the first time I built something like this, so it was a great learning experience for me, especially around things experienced folks might take for granted. I‚Äôve tried to mention a few of them below.\n\nReusing Supabase methods\n\nOne of the challenges I faced while developing this project was how to reuse the existing methods from the Supabase Python API, yet add caching functionality on top of them. I was not very familiar with Object Oriented Programming in Python, so I had to learn some concepts along the way.\n\nFor example, for the database operations, I assigned self.table to self.client.table, so that I could access all the methods Supabase provides, and benefit from their method chaining feature, which is very convenient and elegant. However, I also wanted to cache the results of the select() method without losing the ability to chain other methods. I could not find a way to do this while still using self.client.table, so I decided to create a new query() function that works like select(), but also stores the results in a cache. This way, you can use query() if you want to use the cache, or select() if you don't.\n\nFor some methods that modify or delete data, such as delete_bucket() and empty_bucket(), I did not need to add any caching functionality, so I used the Supabase Python client‚Äôs methods as-is.\n\nself.delete_bucket = self.client.storage.delete_bucket\nself.empty_bucket = self.client.storage.empty_bucket\n\nFor others, I wrapped Supabase‚Äôs methods around a function to add the st.cache_resource decorator. For example:\n\ndef get_bucket(\n    self,\n    bucket_id: str,\n    ttl: Optional[Union[float, timedelta, str]] = None,\n):\n    \"\"\"Retrieves the details of an existing storage bucket.\n\n    Parameters\n    ----------\n    bucket_id : str\n        Unique identifier of the bucket you would like to retrieve.\n    ttl : float, timedelta, str, or None\n        The maximum time to keep an entry in the cache. Defaults to `None` (cache never expires).\n    \"\"\"\n\n    @cache_resource(ttl=ttl)\n    def _get_bucket(_self, bucket_id):\n        return _self.client.storage.get_bucket(bucket_id)\n\n    return _get_bucket(self, bucket_id)\nSingle-sourcing versioning for consistency\n\nst-supabase-connection follows Semantic Versioning (as everyone should).\n\nThe version number is needed by [setuptools](<https://pypi.org/project/setuptools/>) to install the library using pip install. Besides the library, this package also includes an application that demonstrates its usage. I wanted to display the current library version in the application as well.\n\nAdditionally, I like to mention the version number that corresponds to a change in the commit message. This implies that I have to keep track of the version number in at least three different places‚Äìsetup.py, the demo application, and the commits.\n\nTo simplify this, I decided to use a __version__ attribute in the package itself. This attribute is imported to the Streamlit app as from st_supabase_connection import __version__.\n\nTo use this version in setup.py, I use the following function that reads the script and extracts the version:\n\ndef get_version(rel_path):\n    with open(rel_path, \"r\", encoding=\"UTF-8\") as f:\n        for line in f:\n            if line.startswith(\"__version__\"):\n                delim = '\"' if '\"' in line else \"'\"\n                return line.split(delim)[1]\n    raise RuntimeError(\"Unable to find version string.\")\n\nNow I only need to update the version in the script, and both the app and the setup script use the same version. Adding the version manually to commit messages doesn‚Äôt take enough time to warrant automation, but I am sure you could do that if you wanted to.\n\nWhat I learned while building the Streamlit app\nReusing components\n\nMost of my apps use the same template. I have a sidebar.html that populates the sidebar with a few sections that are present in all my apps. I just need to update a few links and captions.\n\nAny additional sections can be added in the with sidebar() context:\n\nwith open(\"demo/sidebar.html\", \"r\", encoding=\"UTF-8\") as sidebar_file:\n    # Replaces the \"VERSION\" placeholder with the current version from the script\n\t\tsidebar_html = sidebar_file.read().replace(\"{VERSION}\", VERSION)\n\nwith st.sidebar:\n\t\t# Additional sections\n    with st.expander(\"üí°**How to use**\", expanded=True):\n        st.info(\n            \"\"\"\n                1. Select a project and initialize client\n                2. Select the relevant DB or Storage operation and options\n                3. Run the query to get the results \n                4. Copy the constructed statement to use in your own app.\n                \"\"\"\n        )\n\n    if st.button(\n        \"Clear the cache to fetch latest dataüßπ\",\n        use_container_width=True,\n        type=\"primary\",\n    ):\n        st.cache_data.clear()\n        st.cache_resource.clear()\n        st.success(\"Cache cleared\")\n\t\t\n\t\t# Reused sidebar template\n    st.components.v1.html(sidebar_html, height=600)\nDemo project for unregistered users\n\nThe company I currently work for, neptune.ai, allows users to test our product without signing up. This is a user-friendly way to let potential customers try the product before they decide to share their personal information and create an account. That's why I decided to implement a similar feature in my own app.\n\nI created a project with sample data and files that users can explore in my Supabase org. Then I added my own Supabase keys as Streamlit secrets, so that users can access the project with my credentials if they don't have or want their own Supabase account.\n\nHowever, I also had to limit some actions that could affect the sample project for these users, so I used a Streamlit session_state variable to check if the user was using the demo project or their own project, and enable or disable functions accordingly.\n\nFor example, if the user is using the demo project, the session_state variable \"project\" is set to \"demo\", and some buttons are greyed out:\n\nif st.session_state[\"project\"] == \"demo\" and operation in RESTRICTED_STORAGE_OPERATORS:\n  help = f\"'{selected_operation.capitalize()}' not allowed in demo project\"\nConstructing valid code snippets that can be copied by the user\n\nOne of the features of the app is that it constructs code-snippets based on your inputs that you can then copy and paste into your own app.\n\nThis had the added benefit of helping me debug if I was doing something wrong while building the app and passing values from widgets to the connector.\n\nI used formatted strings with placeholders that would be filled based on the chosen operation and options.\n\nFor example, this is the template I used for creating a new bucket:\n\nconstructed_storage_query = f\"\"\"st_supabase.create_bucket('{bucket_id}',{name=},{file_size_limit=},allowed_mime_types={allowed_mime_types},{public=})\"\"\"\n\nI then display this snippet using Streamlit code(), and to make sure that this is the same statement that will be executed in the backend to perform the operation, I use Python‚Äôs eval() function to get results:\n\nst.code(constructed_storage_query)\nresponse = eval(constructed_storage_query)\nWrapping up\n\nIn this blog post, I have shared my experience and insights on how to use st-supabase-connection and build a tutorial app with it. I hope this has given you some guidance on how to install and configure the connection, how to use the tutorial app to learn about Supabase storage and database features, and how to use the connection in other Streamlit apps.\n\nI would love to hear your feedback and suggestions on how to improve the connection or the app. You can reach me on GitHub, LinkedIn, or email.\n\nHappy Streamlit-ing! üéà\n\nShare this post\nFacebook\nTwitter\nLinkedIn\nComments\n\nContinue the conversation in our forums ‚Üí\n\nSign up for our newsletter\n\nStay in the loop with our announcements and updates ü§ì\n\nFirst name\nLast name\nEmail*\n\nYou may unsubscribe from Streamlit communications at anytime. Check out our Privacy Policy.\n\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Chanin Nantasenamat - Streamlit",
    "url": "https://blog.streamlit.io/author/chanin/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts by Chanin Nantasenamat\nSenior Developer Advocate at Streamlit\n16 posts\nWebsite\nTwitter\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nLangChain tutorial #5: Build an Ask the Data app\n\nLeverage Agents in LangChain to interact with pandas DataFrame\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\nHow to build a Llama 2 chatbot\n\nExperiment with this open-source LLM from Meta\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 21 2023\nBeginner‚Äôs guide to OpenAI¬†API\n\nBuild your own LLM tool from¬†scratch\n\nLLMs\nby\nChanin Nantasenamat\n,\nJuly 20 2023\nLangChain tutorial #4: Build an Ask the Doc app\n\nHow to get answers from documents using embeddings, a vector store, and a question-answering chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 20 2023\nLangChain tutorial #3: Build a Text Summarization app\n\nExplore the use of the document loader, text splitter, and summarization chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 13 2023\nLangChain tutorial #2: Build a blog outline generator app in 25 lines of code\n\nA guide on conquering writer‚Äôs block with a Streamlit app\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 7 2023\nLangChain tutorial #1: Build an LLM-powered app in 18 lines of code\n\nA step-by-step guide using OpenAI, LangChain, and Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 31 2023\n8 tips for securely using API keys\n\nHow to safely navigate the turbulent landscape of LLM-powered apps\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 19 2023\nHow to build an LLM-powered ChatBot with Streamlit\n\nA step-by-step guide using the unofficial HuggingChat API\n\nLLMs\nby\nChanin Nantasenamat\n,\nMay 10 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Tutorials on Building, Managing & Deploying Apps | Streamlit",
    "url": "https://blog.streamlit.io/tag/tutorials/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Tutorials\n57 posts\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nHow in-app feedback can increase your chatbot‚Äôs performance\n\nA guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith\n\nTutorials\nby\nCharly Wargnier\n,\nOctober 6 2023\nLangChain tutorial #4: Build an Ask the Doc app\n\nHow to get answers from documents using embeddings, a vector store, and a question-answering chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 20 2023\nBuilding a Streamlit and scikit-learn app with ChatGPT\n\nCatching up on coding skills with an AI assistant\n\nLLMs\nby\nMichael Hunger\n,\nJune 16 2023\nLangChain tutorial #3: Build a Text Summarization app\n\nExplore the use of the document loader, text splitter, and summarization chain\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 13 2023\nLangChain tutorial #2: Build a blog outline generator app in 25 lines of code\n\nA guide on conquering writer‚Äôs block with a Streamlit app\n\nLLMs\nby\nChanin Nantasenamat\n,\nJune 7 2023\nLangChain tutorial #1: Build an LLM-powered app in 18 lines of code\n\nA step-by-step guide using OpenAI, LangChain, and Streamlit\n\nTutorials\nby\nChanin Nantasenamat\n,\nMay 31 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Streamlit Product Announcements",
    "url": "https://blog.streamlit.io/tag/product/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in Product\n36 posts\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nHow to improve Streamlit app loading speed\n\nSpeed up loading times by optimizing slow queries and moving resource-intensive transformations outside your app\n\nProduct\nby\nZachary Blackwood\n,\nOctober 5 2023\nAnnouncing Streamlit for Data Science: Second Edition\n\nYour step-by-step guide to building interactive data apps with Streamlit. Check out the latest power user insights, practical tips, and interviews from expert creators!\n\nProduct\nby\nTyler Richards\n,\nSeptember 29 2023\nst.status: Visualize your app‚Äôs processes\n\nRich context for users and more control for developers\n\nProduct\nby\nJoshua Carroll\n,\nSeptember 7 2023\nIntroducing column config ‚öôÔ∏è\n\nTake st.dataframe and st.data_editor to the next level!\n\nProduct\nby\nLukas Masuch and¬†\n1\n¬†more,\nJune 22 2023\nGenerative AI and Streamlit: A perfect match\n\nThe future is about to get interesting‚Ä¶\n\nLLMs\nby\nAdrien Treuille and¬†\n1\n¬†more,\nJune 15 2023\nIntroducing st.connection!\n\nQuickly and easily connect your app to data and APIs\n\nProduct\nby\nJoshua Carroll and¬†\n1\n¬†more,\nMay 2 2023\nIntroducing a chemical molecule component for your Streamlit apps\n\nIntegrate a fully featured molecule editor with just a few lines of code!\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 13 2023\nIntroducing Streamlit to the Polish Python community\n\nMy Streamlit presentation at PyWaW #103\n\nProduct\nby\nMicha≈Ç Nowotka\n,\nApril 4 2023\nEditable dataframes are here! ‚úçÔ∏è\n\nTake interactivity to the next level with st.experimental_data_editor\n\nProduct\nby\nLukas Masuch and¬†\n2\n¬†more,\nFebruary 28 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog Posts: Using LLMs with Streamlit",
    "url": "https://blog.streamlit.io/tag/llms/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\n‚Üê All posts\nPosts in LLMs\n33 posts\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nBuild your own Notion chatbot\n\nA step-by-step guide on building a Notion chatbot using LangChain, OpenAI, and Streamlit\n\nLLMs\nby\nLogan Vendrix\n,\nSeptember 14 2023\nChat with pandas DataFrames using LLMs\n\nA step-by-step guide on how to build a data analysis chatbot powered by LangChain and OpenAI\n\nLLMs\nby\nAmjad Raza\n,\nAugust 31 2023\nBuild a chatbot with custom data sources, powered by LlamaIndex\n\nAugment any LLM with your own data in 43 lines of code!\n\nLLMs\nby\nCaroline Frasca and¬†\n2\n¬†more,\nAugust 23 2023\nExploring LLMs and prompts: A guide to the PromptTools Playground\n\nLearn how to build dynamic, stateful applications that harness multiple LLMs at once\n\nLLMs\nby\nSteve Krawczyk and¬†\n1\n¬†more,\nAugust 18 2023\nAI Interviewer: Customized interview preparation with generative AI\n\nHow we built an app to generate job-specific interview questions, offers personalized evaluations, and even support voice interaction!\n\nLLMs\nby\nHaoxiang Jia and¬†\n1\n¬†more,\nAugust 9 2023\nInstant Insight: Generate data-driven presentations in a snap!\n\nCreate presentations with Streamlit, Snowflake, Plotly, python-pptx, LangChain, and yahooquery\n\nLLMs\nby\nOleksandr Arsentiev\n,\nAugust 2 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  },
  {
    "title": "Blog on Building & Sharing Data Apps | Streamlit Blog",
    "url": "https://blog.streamlit.io/",
    "html": "Blog\nLLMs\nProduct\nTutorials\nWrite for Streamlit!\nStreamlit Home\nMore\nSearch posts\nBuilding a dashboard in Python using Streamlit\n\nUsing pandas for data wrangling, Altair/Plotly for data visualization, and Streamlit as your frontend\n\nTutorials\nby\nChanin Nantasenamat\n,\nJanuary 22 2024\nConnect your Streamlit apps to Supabase\n\nLearn how to connect your Streamlit apps to Supabase with the st-supabase-connection component\n\nby\nSiddhant Sadangi\n,\nDecember 20 2023\nUsing time-based RAG in LLM apps\n\nBuild a GitHub commit chatbot using Timescale Vector, pgvector, and LlamaIndex\n\nLLMs\nby\nAvthar Sewrathan\n,\nDecember 15 2023\n7 ways GPT-4 with Vision can uplevel your Streamlit apps\n\nYour AI coach to design and debug interactive Streamlit apps from static images\n\nLLMs\nby\nCharly Wargnier\n,\nNovember 15 2023\nIntroducing AppTest: a faster way to build quality Streamlit apps\n\nA native framework for automated app testing\n\nProduct\nby\nJoshua Carroll\n,\nOctober 31 2023\nTake your Streamlit apps to the next level with¬†GPT-4\n\nPro tips to design, debug, and optimize your Streamlit apps faster\n\nLLMs\nby\nCharly Wargnier\n,\nOctober 24 2023\nHow to build a real-time LLM app without vector databases\n\nCreate a discount finder app using Pathway and Streamlit in 10 steps\n\nLLMs\nby\nBobur Umurzokov\n,\nOctober 19 2023\nComparing data visualisations from Code Llama, GPT-3.5, and GPT-4\n\n6 case studies that compare data chart outputs from LLMs using Chat2VIS\n\nTutorials\nby\nPaula Maddigan\n,\nOctober 16 2023\nLand your dream job: Build your portfolio with Streamlit\n\nShowcase your coding skills to recruiters with a chatbot tailored to your resume\n\nby\nVicky Kuo\n,\nOctober 13 2023\nSimplifying generative AI workflows\n\nA step-by-step tutorial to building complex ML workflows with Covalent and Streamlit\n\nAdvocate Posts\nby\nFilip Boltuzic and¬†\n2\n¬†more,\nOctober 6 2023\nHow in-app feedback can increase your chatbot‚Äôs performance\n\nA guide to building a RAG chatbot with LangChain, Trubrics' Feedback component, and LangSmith\n\nTutorials\nby\nCharly Wargnier\n,\nOctober 6 2023\nNext page ‚Üí\nCopyright ¬© Streamlit 2024\nCookie settings\nHello there üëãüèª\n\nThanks for stopping by! We use cookies to help us understand how you interact with our website.\nBy clicking ‚ÄúAccept all‚Äù, you consent to our use of cookies. For more information, please see our privacy policy.\n\nCookie settings\nReject all\nAccept all"
  }
]