[
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.0/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.1/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.3/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.2/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.5/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.26.4/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.1/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.2/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.0/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.3/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.4/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.9/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.5/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.6/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.8/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.7/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.27.10/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.28.0/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.0.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.0.2\n\npip install openai==0.0.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Feb 18, 2020\n\nPlaceholder package\n\nNavigation\n Project description\n Release history\n Download files\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "Page Not Found (404) · PyPI",
    "url": "https://pypi.org/project/openai/0.28.1/openai/embeddings_utils.py",
    "html": "We looked everywhere but couldn't find this page\nSearch PyPI\nSearch\n\nError code 404\nBack to the homepage\n\nAnd now for something\ncompletely different\n\nView video transcript"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.1.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.1.3\n\npip install openai==0.1.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.1.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.1.0\n\npip install openai==0.1.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.1.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.1.1\n\npip install openai==0.1.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.1.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.1.2\n\npip install openai==0.1.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.0\n\npip install openai==0.2.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.3\n\npip install openai==0.2.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 7, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.1\n\npip install openai==0.2.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 13, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.4\n\npip install openai==0.2.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 12, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.5\n\npip install openai==0.2.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 5, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.2.6/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.2.6\n\npip install openai==0.2.6\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 24, 2020\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.3.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.3.0\n\npip install openai==0.3.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 27, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.4.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.4.0\n\npip install openai==0.4.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 4, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.6.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.6.0\n\npip install openai==0.6.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 18, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.6.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.6.1\n\npip install openai==0.6.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 19, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.6.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.6.2\n\npip install openai==0.6.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 20, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.6.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.6.3\n\npip install openai==0.6.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 12, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.8.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.8.0\n\npip install openai==0.8.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 17, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.6.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.6.4\n\npip install openai==0.6.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: May 21, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.7.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.7.0\n\npip install openai==0.7.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 12, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.9.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.9.0\n\npip install openai==0.9.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 29, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.9.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.9.1\n\npip install openai==0.9.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 30, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.9.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.9.2\n\npip install openai==0.9.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 30, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.9.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.9.4\n\npip install openai==0.9.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 13, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: all systems operational\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.9.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.9.3\n\npip install openai==0.9.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 30, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.0/",
    "html": "Skip to main content\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.0\n\npip install openai==0.10.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 14, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: all systems operational\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.1\n\npip install openai==0.10.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 14, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.2\n\npip install openai==0.10.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 29, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.3/",
    "html": "Skip to main content\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.3\n\npip install openai==0.10.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Aug 31, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.4\n\npip install openai==0.10.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Sep 9, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.10.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.10.5\n\npip install openai==0.10.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 1, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.6\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.0\n\npip install openai==0.11.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 27, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.1\n\npip install openai==0.11.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 2, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.2\n\npip install openai==0.11.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 4, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.3\n\npip install openai==0.11.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 4, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.4\n\npip install openai==0.11.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 14, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.6/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.6\n\npip install openai==0.11.6\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 21, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.11.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.11.5\n\npip install openai==0.11.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 21, 2021\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.12.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.12.0\n\npip install openai==0.12.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 22, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.15.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.15.0\n\npip install openai==0.15.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Feb 24, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.16.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.16.0\n\npip install openai==0.16.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 17, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.13.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.13.0\n\npip install openai==0.13.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 25, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.14.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.14.0\n\npip install openai==0.14.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Feb 2, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.18.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.18.0\n\npip install openai==0.18.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 8, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.18.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.18.1\n\npip install openai==0.18.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 15, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.19.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.19.0\n\npip install openai==0.19.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: May 25, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.20.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.20.0\n\npip install openai==0.20.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 16, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.22.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.22.0\n\npip install openai==0.22.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jul 26, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.22.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.22.1\n\npip install openai==0.22.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Aug 3, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.23.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.23.0\n\npip install openai==0.23.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Aug 24, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.23.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.23.1\n\npip install openai==0.23.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Sep 28, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.0\n\npip install openai==0.26.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 7, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nDocumentation\n\nSee the OpenAI API docs.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openapi.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise a openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine tuning\n\nFine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine tuning are shared in the following Jupyter notebooks:\n\nClassification with fine tuning (a simple notebook that shows the steps required for fine tuning)\nFine tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.24.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.24.0\n\npip install openai==0.24.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 21, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.25.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.25.0\n\npip install openai==0.25.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 2, 2022\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\n\nThe author of this package has not provided a project description\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.1\n\npip install openai==0.26.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 13, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nDocumentation\n\nSee the OpenAI API docs.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise a openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine tuning\n\nFine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine tuning are shared in the following Jupyter notebooks:\n\nClassification with fine tuning (a simple notebook that shows the steps required for fine tuning)\nFine tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.3\n\npip install openai==0.26.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 25, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.2\n\npip install openai==0.26.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 24, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nDocumentation\n\nSee the OpenAI API docs.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.5\n\npip install openai==0.26.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Feb 7, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.26.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.26.4\n\npip install openai==0.26.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 26, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list engines\nengines = openai.Engine.list()\n\n# print the first engine's id\nprint(engines.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list engines\nopenai api engines.list\n\n# create a completion\nopenai api completions.create -e ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, engine=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", engine=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at support@openai.com.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.1\n\npip install openai==0.27.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 8, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.2\n\npip install openai==0.27.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 11, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.0\n\npip install openai==0.27.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Mar 1, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.TimeoutError error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2022-12-01\"\n\n# create a completion\ncompletion = openai.Completion.create(engine=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2022-12-01\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.3\n\npip install openai==0.27.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 3, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# create a completion\ncompletion = openai.Completion.create(deployment_id=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.4\n\npip install openai==0.27.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 4, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# create a completion\ncompletion = openai.Completion.create(deployment_id=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.5\n\npip install openai==0.27.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Apr 27, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# create a completion\ncompletion = openai.Completion.create(deployment_id=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.6/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.6\n\npip install openai==0.27.6\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: May 2, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a completion\ncompletion = openai.Completion.create(model=\"ada\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# create a completion\ncompletion = openai.Completion.create(deployment_id=\"deployment-name\", prompt=\"Hello world\")\n\n# print the completion\nprint(completion.choices[0].text)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-03-15-preview\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a completion\nopenai api completions.create -m ada -p \"Hello world\"\n\n# create a chat completion\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_completion():\n    completion_resp = await openai.Completion.acreate(prompt=\"This is a test\", model=\"davinci\")\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.7/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.7\n\npip install openai==0.27.7\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: May 19, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the chat completion\nprint(chat_completion.choices[0].message.content)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat Completions\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\nCompletions\n\nText models such as text-davinci-003, text-davinci-002 and earlier (ada, babbage, curie, davinci, etc.) can be called using the completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.8/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.8\n\npip install openai==0.27.8\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jun 7, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the chat completion\nprint(chat_completion.choices[0].message.content)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat Completions\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\nCompletions\n\nText models such as text-davinci-003, text-davinci-002 and earlier (ada, babbage, curie, davinci, etc.) can be called using the completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-similarity-davinci-001\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this get embeddings notebook.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.9/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.9\n\npip install openai==0.27.9\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Aug 22, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the chat completion\nprint(chat_completion.choices[0].message.content)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(chat_completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat Completions\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\nCompletions\n\nText models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.Completion.create(model=\"davinci-002\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-embedding-ada-002\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this embeddings guide.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).\n\nExamples of fine-tuning are shared in the following Jupyter notebooks:\n\nClassification with fine-tuning (a simple notebook that shows the steps required for fine-tuning)\nFine-tuning a model that answers questions about the 2020 Olympics\nStep 1: Collecting data\nStep 2: Creating a synthetic Q&A dataset\nStep 3: Train a fine-tuning model specialized for Q&A\n\nSync your fine-tunes to Weights & Biases to track experiments, models, and datasets in your central dashboard with:\n\nopenai wandb sync\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.27.10/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.27.10\n\npip install openai==0.27.10\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Aug 30, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the chat completion\nprint(chat_completion.choices[0].message.content)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(chat_completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat Completions\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\nCompletions\n\nText models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.Completion.create(model=\"davinci-002\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-embedding-ada-002\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this embeddings guide.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.\n\n# Create a fine-tuning job with an already uploaded file\nopenai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n\n# List 10 fine-tuning jobs\nopenai.FineTuningJob.list(limit=10)\n\n# Retrieve the state of a fine-tune\nopenai.FineTuningJob.retrieve(\"ft-abc123\")\n\n# Cancel a job\nopenai.FineTuningJob.cancel(\"ft-abc123\")\n\n# List up to 10 events from a fine-tuning job\nopenai.FineTuningJob.list_events(id=\"ft-abc123\", limit=10)\n\n# Delete a fine-tuned model (must be an owner of the org the model was created in)\nopenai.Model.delete(\"ft-abc123\")\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.28.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.28.0\n\npip install openai==0.28.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Sep 1, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\npip install --upgrade openai\n\n\nInstall from source with:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nimport openai\nopenai.api_key = \"sk-...\"\n\n# list models\nmodels = openai.Model.list()\n\n# print the first model's id\nprint(models.data[0].id)\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the chat completion\nprint(chat_completion.choices[0].message.content)\n\nParams\n\nAll endpoints have a .create method that supports a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(chat_completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure fine-tuning\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\n# ...\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nExample code\n\nExamples of how to use this Python library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for:\n\nClassification using fine-tuning\nClustering\nCode search\nCustomizing embeddings\nQuestion answering from a corpus of documents\nRecommendations\nVisualization of embeddings\nAnd more\n\nPrior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the OpenAI Cookbook.\n\nChat Completions\n\nConversational models such as gpt-3.5-turbo can be called using the chat completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\nCompletions\n\nText models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\ncompletion = openai.Completion.create(model=\"davinci-002\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\nEmbeddings\n\nIn the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.\n\nTo get an embedding for a text string, you can use the embeddings method as follows in Python:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\n# choose text to embed\ntext_string = \"sample text\"\n\n# choose an embedding\nmodel_id = \"text-embedding-ada-002\"\n\n# compute the embedding of the text\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nAn example of how to call the embeddings method is shown in this embeddings guide.\n\nExamples of how to use embeddings are shared in the following Jupyter notebooks:\n\nClassification using embeddings\nClustering using embeddings\nCode search using embeddings\nSemantic text search using embeddings\nUser and product embeddings\nZero-shot classification using embeddings\nRecommendation using embeddings\n\nFor more information on embeddings and the types of embeddings OpenAI offers, read the embeddings guide in the OpenAI documentation.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.\n\n# Create a fine-tuning job with an already uploaded file\nopenai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n\n# List 10 fine-tuning jobs\nopenai.FineTuningJob.list(limit=10)\n\n# Retrieve the state of a fine-tune\nopenai.FineTuningJob.retrieve(\"ft-abc123\")\n\n# Cancel a job\nopenai.FineTuningJob.cancel(\"ft-abc123\")\n\n# List up to 10 events from a fine-tuning job\nopenai.FineTuningJob.list_events(id=\"ft-abc123\", limit=10)\n\n# Delete a fine-tuned model (must be an owner of the org the model was created in)\nopenai.Model.delete(\"ft-abc123\")\n\n\nFor more information on fine-tuning, read the fine-tuning guide in the OpenAI documentation.\n\nModeration\n\nOpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI content policy\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nSee the moderation guide for more details.\n\nImage generation (DALL·E)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\nAudio transcription (Whisper)\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nimport openai\nopenai.api_key = \"sk-...\"  # supply your API key however you choose\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nimport openai\nfrom aiohttp import ClientSession\n\nopenai.aiosession.set(ClientSession())\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\n\nSee the usage guide for more details.\n\nRequirements\nPython 3.7.1+\n\nIn general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know on our support page.\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/0.28.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 0.28.1\n\npip install openai==0.28.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Sep 26, 2023\n\nPython client library for the OpenAI API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: MIT License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: MIT License\nOperating System\nOS Independent\nProgramming Language\nPython :: 3\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python Library\n\nThe OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.\n\nYou can find usage examples for the OpenAI Python library in our API reference and the OpenAI Cookbook.\n\nInstallation\n\nTo start, ensure you have Python 3.7.1 or newer. If you just want to use the package, run:\n\npip install --upgrade openai\n\n\nAfter you have installed the package, import it at the top of a file:\n\nimport openai\n\n\nTo install this package from source to make modifications to it, run the following command from the root of the repository:\n\npython setup.py install\n\nOptional dependencies\n\nInstall dependencies for openai.embeddings_utils:\n\npip install openai[embeddings]\n\n\nInstall support for Weights & Biases:\n\npip install openai[wandb]\n\n\nData libraries like numpy and pandas are not installed by default due to their size. They’re needed for some functionality of this library, but generally not for talking to the API. If you encounter a MissingDependencyError, install them with:\n\npip install openai[datalib]\n\nUsage\n\nThe library needs to be configured with your account's secret key which is available on the website. Either set it as the OPENAI_API_KEY environment variable before using the library:\n\nexport OPENAI_API_KEY='sk-...'\n\n\nOr set openai.api_key to its value:\n\nopenai.api_key = \"sk-...\"\n\n\nExamples of how to use this library to accomplish various tasks can be found in the OpenAI Cookbook. It contains code examples for: classification using fine-tuning, clustering, code search, customizing embeddings, question answering from a corpus of documents. recommendations, visualization of embeddings, and more.\n\nMost endpoints support a request_timeout param. This param takes a Union[float, Tuple[float, float]] and will raise an openai.error.Timeout error if the request exceeds that time in seconds (See: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts).\n\nChat completions\n\nChat models such as gpt-3.5-turbo and gpt-4 can be called using the chat completions endpoint.\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n\n\nYou can learn more in our chat completions guide.\n\nCompletions\n\nText models such as babbage-002 or davinci-002 (and our legacy completions models) can be called using the completions endpoint.\n\ncompletion = openai.Completion.create(model=\"davinci-002\", prompt=\"Hello world\")\nprint(completion.choices[0].text)\n\n\nYou can learn more in our completions guide.\n\nEmbeddings\n\nEmbeddings are designed to measure the similarity or relevance between text strings. To get an embedding for a text string, you can use following:\n\ntext_string = \"sample text\"\n\nmodel_id = \"text-embedding-ada-002\"\n\nembedding = openai.Embedding.create(input=text_string, model=model_id)['data'][0]['embedding']\n\n\nYou can learn more in our embeddings guide.\n\nFine-tuning\n\nFine-tuning a model on training data can both improve the results (by giving the model more examples to learn from) and lower the cost/latency of API calls by reducing the need to include training examples in prompts.\n\n# Create a fine-tuning job with an already uploaded file\nopenai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n\n# List 10 fine-tuning jobs\nopenai.FineTuningJob.list(limit=10)\n\n# Retrieve the state of a fine-tune\nopenai.FineTuningJob.retrieve(\"ft-abc123\")\n\n# Cancel a job\nopenai.FineTuningJob.cancel(\"ft-abc123\")\n\n# List up to 10 events from a fine-tuning job\nopenai.FineTuningJob.list_events(id=\"ft-abc123\", limit=10)\n\n# Delete a fine-tuned model (must be an owner of the org the model was created in)\nopenai.Model.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")\n\n\nYou can learn more in our fine-tuning guide.\n\nTo log the training results from fine-tuning to Weights & Biases use:\n\nopenai wandb sync\n\n\nFor more information, read the wandb documentation on Weights & Biases.\n\nModeration\n\nOpenAI provides a free Moderation endpoint that can be used to check whether content complies with the OpenAI content policy.\n\nmoderation_resp = openai.Moderation.create(input=\"Here is some perfectly innocuous text that follows all OpenAI content policies.\")\n\n\nYou can learn more in our moderation guide.\n\nImage generation (DALL·E)\n\nDALL·E is a generative image model that can create new images based on a prompt.\n\nimage_resp = openai.Image.create(prompt=\"two dogs playing chess, oil painting\", n=4, size=\"512x512\")\n\n\nYou can learn more in our image generation guide.\n\nAudio (Whisper)\n\nThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model.\n\nf = open(\"path/to/file.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", f)\n\ntranscript = openai.Audio.translate(\"whisper-1\", f)\n\n\nYou can learn more in our speech to text guide.\n\nAsync API\n\nAsync support is available in the API by prepending a to a network-bound method:\n\nasync def create_chat_completion():\n    chat_completion_resp = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n\nTo make async requests more efficient, you can pass in your own aiohttp.ClientSession, but you must manually close the client session at the end of your program/event loop:\n\nfrom aiohttp import ClientSession\nopenai.aiosession.set(ClientSession())\n\n# At the end of your program, close the http session\nawait openai.aiosession.get().close()\n\nCommand-line interface\n\nThis library additionally provides an openai command-line utility which makes it easy to interact with the API from your terminal. Run openai api -h for usage.\n\n# list models\nopenai api models.list\n\n# create a chat completion (gpt-3.5-turbo, gpt-4, etc.)\nopenai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n\n# create a completion (text-davinci-003, text-davinci-002, ada, babbage, curie, davinci, etc.)\nopenai api completions.create -m ada -p \"Hello world\"\n\n# generate images via DALL·E API\nopenai api image.create -p \"two dogs playing chess, cartoon\" -n 1\n\n# using openai through a proxy\nopenai --proxy=http://proxy.com api models.list\n\nMicrosoft Azure Endpoints\n\nIn order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.\n\nimport openai\nopenai.api_type = \"azure\"\nopenai.api_key = \"...\"\nopenai.api_base = \"https://example-endpoint.openai.azure.com\"\nopenai.api_version = \"2023-05-15\"\n\n# create a chat completion\nchat_completion = openai.ChatCompletion.create(deployment_id=\"deployment-name\", model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n\n# print the completion\nprint(chat_completion.choices[0].message.content)\n\n\nPlease note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example of how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:\n\nUsing Azure completions\nUsing Azure chat\nUsing Azure embeddings\nMicrosoft Azure Active Directory Authentication\n\nIn order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to \"azure_ad\" and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.\n\nfrom azure.identity import DefaultAzureCredential\nimport openai\n\n# Request credential\ndefault_credential = DefaultAzureCredential()\ntoken = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Setup parameters\nopenai.api_type = \"azure_ad\"\nopenai.api_key = token.token\nopenai.api_base = \"https://example-endpoint.openai.azure.com/\"\nopenai.api_version = \"2023-05-15\"\n\nCredit\n\nThis library is forked from the Stripe Python Library.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0b1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0b1\n\npip install openai==1.0.0b1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Sep 29, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License (Apache-2.0)\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1, <4.0.0\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: Apache Software License\nProgramming Language\nPython :: 3\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API Library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. It includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"my api key\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n)\nprint(completion.choices)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv and adding OPENAI_API_KEY=\"my api key\" to your .env file so that your API Key is not stored in source control.\n\nAsync Usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"my api key\",\n)\n\n\nasync def main():\n    completion = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n    )\n    print(completion.choices)\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT]\nWe highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing Types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into json (v1, v2). To get a dictionary, you can call dict(model).\n\nThis helps provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to \"basic\".\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncontents = Path(\"input.jsonl\").read_bytes()\nclient.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.\n\nimport aiofiles\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync with aiofiles.open(\"input.jsonl\", mode=\"rb\") as f:\n    contents = await f.read()\n\nawait client.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\nHandling errors\n\nWhen the library is unable to connect to the API (e.g., due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (i.e., 4xx or 5xx response), a subclass of openai.APIStatusError will be raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors will be automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors will all be retried by default.\n\nYou can use the max_retries option to configure or disable this:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n)\n\nTimeouts\n\nRequests time out after 10 minutes by default. You can configure this with a timeout option, which accepts a float or an httpx.Timeout:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests which time out will be retried twice by default.\n\nAdvanced\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nConfiguring custom URLs, proxies, and transports\n\nYou can configure the following keyword arguments when instantiating the client:\n\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Use a custom base URL\n    base_url=\"http://my.test.server.example.com:8083\",\n    proxies=\"http://my.test.proxy.example.com\",\n    transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n)\n\n\nSee the httpx documentation for information about the proxies and transport keyword arguments.\n\nManaging HTTP resources\n\nBy default we will close the underlying HTTP connections whenever the client is garbage collected is called but you can also manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nVersioning\n\nThis package generally attempts to follow SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0b2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0b2\n\npip install openai==1.0.0b2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 12, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License (Apache-2.0)\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1, <4.0.0\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: Apache Software License\nProgramming Language\nPython :: 3\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n)\nprint(completion.choices)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main():\n    completion = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n    )\n    print(completion.choices)\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call dict(model).\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncontents = Path(\"input.jsonl\").read_bytes()\nclient.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.\n\nimport aiofiles\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync with aiofiles.open(\"input.jsonl\", mode=\"rb\") as f:\n    contents = await f.read()\n\nawait client.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0b3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0b3\n\npip install openai==1.0.0b3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 17, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License (Apache-2.0)\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1, <4.0.0\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nLicense\nOSI Approved :: Apache Software License\nProgramming Language\nPython :: 3\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n)\nprint(completion.choices)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main():\n    completion = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n    )\n    print(completion.choices)\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call dict(model).\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncontents = Path(\"input.jsonl\").read_bytes()\nclient.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. This example uses aiofiles to asynchronously read the file contents but you can use whatever method you would like.\n\nimport aiofiles\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync with aiofiles.open(\"input.jsonl\", mode=\"rb\") as f:\n    contents = await f.read()\n\nawait client.files.create(\n    file=contents,\n    purpose=\"fine-tune\",\n)\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0rc1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0rc1\n\npip install openai==1.0.0rc1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Oct 28, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n)\nprint(completion.choices)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main():\n    completion = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n    )\n    print(completion.choices)\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"content\": \"string\",\n        \"role\": \"system\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion.choices)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT]\nThe Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0rc2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0rc2\n\npip install openai==1.0.0rc2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 3, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n)\nprint(completion.choices)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main():\n    completion = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n    )\n    print(completion.choices)\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"content\": \"string\",\n        \"role\": \"system\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion.choices)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0rc3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0rc3\n\npip install openai==1.0.0rc3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 6, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.completions.create(\n    prompt=\"Say this is a test\",\n    model=\"text-davinci-003\",\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.0\n\npip install openai==1.0.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 6, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nBeta Release\n\n[!IMPORTANT]\nWe're preparing to release version 1.0 of the OpenAI Python library.\n\nThis new version will be a major release and will include breaking changes. We're releasing this beta version to give you a chance to try out the new features and provide feedback before the official release. You can install the beta version with:\n\npip install --pre openai\n\n\nAnd follow along with the beta release notes.\n\nInstallation\npip install --pre openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.0.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.0.1\n\npip install openai==1.0.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 6, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.1.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.1.0\n\npip install openai==1.1.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 6, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.1.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.1.1\n\npip install openai==1.1.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 6, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.1.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.1.2\n\npip install openai==1.1.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 8, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.2.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.2.0\n\npip install openai==1.2.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 9, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\npage = client.files.list()\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.2.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.2.1\n\npip install openai==1.2.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 9, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.2.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.2.2\n\npip install openai==1.2.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 9, 2023\n\nClient library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.2.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.2.3\n\npip install openai==1.2.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 10, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.0\n\npip install openai==1.3.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 15, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.2.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.2.4\n\npip install openai==1.2.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 13, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint\nazure_deployment\napi_version\nazure_ad_token\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.1\n\npip install openai==1.3.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 16, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like serializing back into JSON (v1, v2). To get a dictionary, call model.model_dump().\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.2\n\npip install openai==1.3.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 16, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.4/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.4\n\npip install openai==1.3.4\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 21, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.3/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.3\n\npip install openai==1.3.3\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 18, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.5/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.5\n\npip install openai==1.3.5\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 22, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor part in stream:\n    print(part.choices[0].delta.content or \"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for part in stream:\n    print(part.choices[0].delta.content or \"\")\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.6/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.6\n\npip install openai==1.3.6\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Nov 29, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"My API Key\",\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(part.choices[0].delta.content)\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(part.choices[0].delta.content)\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.7/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.7\n\npip install openai==1.3.7\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 1, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    prompt=\"Say this is a test\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.8/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.8\n\npip install openai==1.3.8\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 9, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nstream = await client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nasync for chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 60s\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.3.9/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.3.9\n\npip install openai==1.3.9\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 13, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.4.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.4.0\n\npip install openai==1.4.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 15, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.5.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.5.0\n\npip install openai==1.5.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 17, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nBloomberg is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.6.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.6.0\n\npip install openai==1.6.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 20, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\"\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.7.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.7.0\n\npip install openai==1.7.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 9, 2024\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nCapital One is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.6.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.6.1\n\npip install openai==1.6.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Dec 22, 2023\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.7.1/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.7.1\n\npip install openai==1.7.1\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 10, 2024\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nRed Hat is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe API documentation can be found here.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-XGinujblHPwGLSztz8cPS8XY\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.7.2/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.7.2\n\npip install openai==1.7.2\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 12, 2024\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an APIResponse object.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.8.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.8.0\n\npip install openai==1.8.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 16, 2024\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nGoogle is a Visionary sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.\n\nA migration script will be provided & the migration in general should be smooth.\n\n.with_streaming_response\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.\n\nAs such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.\n\nwith client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n\n\nThe context manager is required so that the response will reliably be closed.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.9.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.9.0\n\npip install openai==1.9.0\nCopy PIP instructions\n\nNewer version available (1.10.0)\n\nReleased: Jan 21, 2024\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nQube Research & Technologies Limited is a Maintaining sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.\n\nA migration script will be provided & the migration in general should be smooth.\n\n.with_streaming_response\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.\n\nAs such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.\n\nwith client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n\n\nThe context manager is required so that the response will reliably be closed.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/1.10.0/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.10.0\n\npip install openai==1.10.0\nCopy PIP instructions\n\nLatest version\n\nReleased: about 4 hours ago\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nOpenEDG Python Institute is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.\n\nA migration script will be provided & the migration in general should be smooth.\n\n.with_streaming_response\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.\n\nAs such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.\n\nwith client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n\n\nThe context manager is required so that the response will reliably be closed.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  },
  {
    "title": "openai · PyPI",
    "url": "https://pypi.org/project/openai/",
    "html": "Skip to main content\n 2FA is now required on PyPI  Read more\nSearch PyPI\nSearch\nHelp Sponsors Log in Register\nopenai 1.10.0\n\npip install openai\nCopy PIP instructions\n\nLatest version\n\nReleased: about 4 hours ago\n\nThe official Python library for the openai API\n\nNavigation\n Project description\n Release history\n Download files\nProject links\nHomepage\nRepository\nStatistics\nGitHub statistics:\n Stars: 17970\n Forks: 2459\n Open issues: 48\n Open PRs: 6\n\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\n\nMeta\n\nLicense: Apache Software License\n\nAuthor: OpenAI\n\nRequires: Python >=3.7.1\n\nMaintainers\n atty-openai\n borispower\n christinakim\n ddeville\n dschnurr-openai\n emorikawa-openai\n gdb\n hallacy-openai\n hponde_oai\n jhallard\n kennyhsu\n michelle-openai\n mikaell-openai\n peterz-openai\n rachel-openai\n tomerkOpenAI\nClassifiers\nIntended Audience\nDevelopers\nLicense\nOSI Approved :: Apache Software License\nOperating System\nMacOS\nMicrosoft :: Windows\nOS Independent\nPOSIX\nPOSIX :: Linux\nProgramming Language\nPython :: 3.7\nPython :: 3.8\nPython :: 3.9\nPython :: 3.10\nPython :: 3.11\nPython :: 3.12\nTopic\nSoftware Development :: Libraries :: Python Modules\nTyping\nTyped\nIndeed is a Contributing sponsor of the Python Software Foundation.\nPSF Sponsor · Served ethically\nProject description\nOpenAI Python API library\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n\nIt is generated from our OpenAPI specification with Stainless.\n\nDocumentation\n\nThe REST API documentation can be found on platform.openai.com. The full API of this library can be found in api.md.\n\nInstallation\n\n[!IMPORTANT] The SDK was rewritten in v1, which was released November 6th 2023. See the v1 migration guide, which includes scripts to automatically update your code.\n\npip install openai\n\nUsage\n\nThe full API of this library can be found in api.md.\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nWhile you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n\nAsync usage\n\nSimply import AsyncOpenAI instead of OpenAI and use await with each API call:\n\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\nStreaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nThe async client uses the exact same interface.\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n\nModule-level client\n\n[!IMPORTANT] We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, not in application code.\n\nWe recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:\n\nIt can be difficult to reason about where client options are configured\nIt's not possible to change certain client options without potentially causing race conditions\nIt's harder to mock for testing purposes\nIt's not possible to control cleanup of network connections\nUsing types\n\nNested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:\n\nSerializing back into JSON, model.model_dump_json(indent=2, exclude_unset=True)\nConverting to a dictionary, model.model_dump(exclude_unset=True)\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set python.analysis.typeCheckingMode to basic.\n\nPagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n\n\nOr, asynchronously:\n\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n\n\nAlternatively, you can use the .has_next_page(), .next_page_info(), or .get_next_page() methods for more granular control working with pages:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n\n\nOr just work directly with the returned data:\n\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n\nNested params\n\nNested parameters are dictionaries, typed using TypedDict, for example:\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n\nFile Uploads\n\nRequest parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n\n\nThe async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n\nHandling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of openai.APIConnectionError is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of openai.APIStatusError is raised, containing status_code and response properties.\n\nAll errors inherit from openai.APIError.\n\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n\n\nError codes are as followed:\n\nStatus Code\tError Type\n400\tBadRequestError\n401\tAuthenticationError\n403\tPermissionDeniedError\n404\tNotFoundError\n422\tUnprocessableEntityError\n429\tRateLimitError\n>=500\tInternalServerError\nN/A\tAPIConnectionError\nRetries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the max_retries option to configure or disable retry settings:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nTimeouts\n\nBy default requests time out after 10 minutes. You can configure this with a timeout option, which accepts a float or an httpx.Timeout object:\n\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\n\nOn timeout, an APITimeoutError is thrown.\n\nNote that requests that time out are retried twice by default.\n\nAdvanced\nLogging\n\nWe use the standard library logging module.\n\nYou can enable logging by setting the environment variable OPENAI_LOG to debug.\n\n$ export OPENAI_LOG=debug\n\nHow to tell whether None means null or missing\n\nIn an API response, a field may be explicitly null, or missing entirely; in either case, its value is None in this library. You can differentiate the two cases with .model_fields_set:\n\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n\nAccessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing .with_raw_response. to any HTTP method call, e.g.,\n\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n\n\nThese methods return an LegacyAPIResponse object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception of content & text will be methods instead of properties. In the async client, all methods will be async.\n\nA migration script will be provided & the migration in general should be smooth.\n\n.with_streaming_response\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use .with_streaming_response instead, which requires a context manager and only reads the response body once you call .read(), .text(), .json(), .iter_bytes(), .iter_text(), .iter_lines() or .parse(). In the async client, these are async methods.\n\nAs such, .with_streaming_response methods return a different APIResponse object, and the async client returns an AsyncAPIResponse object.\n\nwith client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n\n\nThe context manager is required so that the response will reliably be closed.\n\nConfiguring the HTTP client\n\nYou can directly override the httpx client to customize it for your use case, including:\n\nSupport for proxies\nCustom transports\nAdditional advanced functionality\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n\nManaging HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the .close() method if desired, or with a context manager that closes when exiting.\n\nMicrosoft Azure OpenAI\n\nTo use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.\n\n[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.\n\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n\n\nIn addition to the options provided in the base OpenAI client, the following options are provided:\n\nazure_endpoint (or the AZURE_OPENAI_ENDPOINT environment variable)\nazure_deployment\napi_version (or the OPENAI_API_VERSION environment variable)\nazure_ad_token (or the AZURE_OPENAI_AD_TOKEN environment variable)\nazure_ad_token_provider\n\nAn example of using the client with Azure Active Directory can be found here.\n\nVersioning\n\nThis package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:\n\nChanges that only affect static types, without breaking runtime behavior.\nChanges to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).\nChanges that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an issue with questions, bugs, or suggestions.\n\nRequirements\n\nPython 3.7 or higher.\n\nHelp\nInstalling packages\nUploading packages\nUser guide\nProject name retention\nFAQs\nAbout PyPI\nPyPI on Twitter\nInfrastructure dashboard\nStatistics\nLogos & trademarks\nOur sponsors\nContributing to PyPI\nBugs and feedback\nContribute on GitHub\nTranslate PyPI\nSponsor PyPI\nDevelopment credits\nUsing PyPI\nCode of conduct\nReport security issue\nPrivacy policy\nTerms of use\nAcceptable Use Policy\n\nStatus: Service Under Maintenance\n\nDeveloped and maintained by the Python community, for the Python community.\nDonate today!\n\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation.\n\n\n© 2024 Python Software Foundation\nSite map\n\n English español français 日本語 português (Brasil) українська Ελληνικά Deutsch 中文 (简体) 中文 (繁體) русский עברית esperanto\nAWS\nCloud computing and Security Sponsor\nDatadog\nMonitoring\nFastly\nCDN\nGoogle\nDownload Analytics\nMicrosoft\nPSF Sponsor\nPingdom\nMonitoring\nSentry\nError logging\nStatusPage\nStatus page"
  }
]